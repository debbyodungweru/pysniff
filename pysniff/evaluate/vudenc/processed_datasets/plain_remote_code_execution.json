[
    {
        "html_url": " https://github.com/Internet-of-People/titania-os/blob/8ecb42ffb736232f6e84fae016498c431e7ae394",
        "file_path": "/vuedj/configtitania/views.py",
        "source": "from django.shortcuts import render\nfrom django.http import HttpResponse, JsonResponse\nfrom django.views.decorators.csrf import csrf_exempt\n\nfrom rest_framework.renderers import JSONRenderer\nfrom rest_framework.parsers import JSONParser\nfrom rest_framework.response import Response\nfrom rest_framework import viewsets\nfrom rest_framework.decorators import list_route\nfrom flask import escape\n\nfrom .models import BoxDetails, RegisteredServices\nfrom .serializers import BoxDetailsSerializer, RegisteredServicesSerializer\n\nimport common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd \n\n# fetch network AP details\nnm = NetworkManager.NetworkManager\nwlans = [d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)]\n\ndef get_osversion():\n    \"\"\"\n    PRETTY_NAME of your Titania os (in lowercase).\n    \"\"\"\n    with open(\"/etc/os-release\") as f:\n        osfilecontent = f.read().split(\"\\n\")\n        # $PRETTY_NAME is at the 5th position\n        version = osfilecontent[4].split('=')[1].strip('\\\"')\n        return version\n\ndef get_allconfiguredwifi():\n    \"\"\"\n    nmcli con | grep 802-11-wireless\n    \"\"\"\n    ps = subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0]\n    wifirows = ps.split('\\n')\n    wifi = []\n    for row in wifirows:\n        name = row.split(':')\n        print(name)\n        wifi.append(name[0])\n    return wifi\n\ndef get_allAPs():\n    \"\"\"\n    nmcli con | grep 802-11-wireless\n    \"\"\"\n    ps = subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0]\n    wifirows = ps.split('\\n')\n    wifi = []\n    for row in wifirows:\n        entry = row.split(':')\n        print(entry)\n        wifi.append(entry)\n    return wifi\n    # wifi_aps = []   \n    # for dev in wlans:\n    #     for ap in dev.AccessPoints:\n    #         wifi_aps.append(ap.Ssid)\n    # return wifi_aps\n\ndef add_user(username, password):\n    encPass = crypt.crypt(password,\"22\")\n    os.system(\"useradd -G docker,wheel -p \"+encPass+\" \"+username)\n\ndef add_newWifiConn(wifiname, wifipass):\n    print(wlans)\n    wlan0 = wlans[0]\n    print(wlan0)\n    print(wifiname)\n    # get selected ap as currentwifi\n    for dev in wlans:\n        for ap in dev.AccessPoints:\n            if ap.Ssid == wifiname:\n                currentwifi = ap\n    print(currentwifi)\n    # params to set password\n    params = {\n            \"802-11-wireless\": {\n                \"security\": \"802-11-wireless-security\",\n            },\n            \"802-11-wireless-security\": {\n                \"key-mgmt\": \"wpa-psk\",\n                \"psk\": wifipass\n            },\n        }\n    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi)        \n\ndef delete_WifiConn(wifiap):\n    \"\"\"\n    nmcli connection delete id <connection name>\n    \"\"\"\n    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE)\n    print(ps)\n\ndef edit_WifiConn(wifiname, wifipass):\n    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE)\n    print(ps)\n    print(wlans)\n    wlan0 = wlans[0]\n    print(wlan0)\n    print(wifiname)\n    # get selected ap as currentwifi\n    for dev in wlans:\n        for ap in dev.AccessPoints:\n            if ap.Ssid == wifiname:\n                currentwifi = ap\n    # params to set password\n    params = {\n            \"802-11-wireless\": {\n                \"security\": \"802-11-wireless-security\",\n            },\n            \"802-11-wireless-security\": {\n                \"key-mgmt\": \"wpa-psk\",\n                \"psk\": wifipass\n            },\n        }\n    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi) \n    return       \n\n@csrf_exempt\ndef handle_config(request):\n    \"\"\"\n    List all code snippets, or create a new snippet.\n    \"\"\" \n    if request.method == 'POST':\n        action = request.POST.get(\"_action\")\n        print(action)\n        if action == 'registerService':\n            request_name = request.POST.get(\"name\")\n            request_address = request.POST.get(\"address\")\n            request_icon = request.POST.get(\"icon\")\n            print(request_name)\n            print(request_address)\n            print(request_icon)\n            setServiceDetails = RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon)\n            return JsonResponse({\"STATUS\":\"SUCCESS\"}, safe=False)\n        elif action == 'getSchema':\n            schema = get_osversion()\n            return JsonResponse({\"version_info\":schema}, safe=False)\n        elif action == 'getIfConfigured':\n            print(action)\n            queryset = BoxDetails.objects.all()\n            serializer = BoxDetailsSerializer(queryset, many=True)\n            return JsonResponse(serializer.data, safe=False)\n        elif action == 'loadDependencies':\n            print(action)\n            queryset = RegisteredServices.objects.all()\n            serializer = RegisteredServicesSerializer(queryset, many=True)\n            return JsonResponse(serializer.data, safe=False)\n        elif action == 'getAllAPs':\n            wifi_aps = get_allAPs()\n            return JsonResponse(wifi_aps, safe=False)\n        elif action == 'saveUserDetails':\n            print(action)\n            boxname = escape(request.POST.get(\"boxname\"))\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            print(username)\n            add_user(username,password)\n            setBoxName = BoxDetails(boxname=boxname)\n            setBoxName.save()\n            # connect to wifi ap user selected\n            wifi_pass = request.POST.get(\"wifi_password\")\n            wifi_name = request.POST.get(\"wifi_ap\")\n            if len(wifi_name) > 0:\n                add_newWifiConn(wifi_name,wifi_pass)\n            return JsonResponse({\"STATUS\":\"SUCCESS\"}, safe=False)\n        elif action == 'login':\n            print(action)\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            output=''\n            \"\"\"Tries to authenticate a user.\n            Returns True if the authentication succeeds, else the reason\n            (string) is returned.\"\"\"\n            try:\n                enc_pwd = spwd.getspnam(username)[1]\n                if enc_pwd in [\"NP\", \"!\", \"\", None]:\n                    output = \"User '%s' has no password set\" % username\n                if enc_pwd in [\"LK\", \"*\"]:\n                    output = \"account is locked\"\n                if enc_pwd == \"!!\":\n                    output = \"password has expired\"\n                # Encryption happens here, the hash is stripped from the\n                # enc_pwd and the algorithm id and salt are used to encrypt\n                # the password.\n                if crypt.crypt(password, enc_pwd) == enc_pwd:\n                    output = ''\n                else:\n                    output = \"incorrect password\"\n            except KeyError:\n                output = \"User '%s' not found\" % username\n            if len(output) == 0:\n                return JsonResponse({\"username\":username}, safe=False)\n            else:\n                return JsonResponse(output, safe=False)\n        elif action == 'logout':\n            print(action)\n            username = request.POST.get(\"username\")\n            print(username+' ')\n            queryset = User.objects.all().first()\n            if username == queryset.username:\n                return JsonResponse({\"STATUS\":\"SUCCESS\", \"username\":queryset.username}, safe=False)\n        elif action == 'getDashboardCards':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_DASHBOARD_CARDS)\n            rows = cursor.fetchall()\n            print(rows)\n            return JsonResponse(rows, safe=False)\n        elif action == 'getDashboardChart':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            for row in rows:\n                cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],])\n                datasets = cursor.fetchall()\n                print(datasets)\n                data = {'container_name' : row[1], 'data': datasets}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getDockerOverview':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_DOCKER_OVERVIEW)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            for row in rows:\n                data = {'state': row[0], 'container_id': row[1], 'name': row[2],\n                        'image': row[3], 'running_for': row[4],\n                        'command': row[5], 'ports': row[6],\n                        'status': row[7], 'networks': row[8]}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getContainerStats':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            datasets_io = []\n            datasets_mem = []\n            datasets_perc = []\n            for row in rows:\n                datasets_io = []\n                datasets_mem = []\n                datasets_perc = []\n                # values with % appended to them\n                for iter in range(0,2):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_perc.append(counter_val)\n                # values w/o % appended to them\n                for iter in range(2,4):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_mem.append(counter_val)\n                # values w/o % appended to them\n                for iter in range(4,8):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_io.append(counter_val)\n                data = {'container_id': row[0], 'container_name' : row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getThreads':\n            print(action)\n            rows = []\n            ps = subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0]\n            processes = ps.decode().split('\\n')\n            # this specifies the number of splits, so the splitted lines\n            # will have (nfields+1) elements\n            nfields = len(processes[0].split()) - 1\n            for row in processes[4:]:\n                rows.append(row.split(None, nfields))\n            return JsonResponse(rows, safe=False)\n        elif action == 'getContainerTop':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            resultset = []\n            for i in rows:\n                data = {}\n                datasets = []\n                ps = subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0]\n                processes = ps.decode().split('\\n')\n                # this specifies the number of splits, so the splitted lines\n                # will have (nfields+1) elements\n                nfields = len(processes[0].split()) - 1\n                for p in processes[1:]:\n                    datasets.append(p.split(None, nfields))\n                data = {'container_id': i[0], 'container_name' : i[1], 'data': datasets}\n                resultset.append(data)\n            return JsonResponse(resultset, safe=False)\n        elif action == 'getSettings':\n            print(action)\n            ps = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = ps.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False)\n        elif action == 'deleteUser':\n            print(action)\n            username = escape(request.POST.get(\"user\"))\n            ps = subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate()\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False)\n        elif action == 'addNewUser':\n            print(action)\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            add_user(username,password)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False)\n        elif action == 'addWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_pass = escape(request.POST.get(\"wifi_password\"))\n            wifi_name = request.POST.get(\"wifi_ap\")\n            if len(wifi_name) > 0:\n                add_newWifiConn(wifi_name,wifi_pass)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False)\n        elif action == 'deleteWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_name = request.POST.get(\"wifi\")\n            delete_WifiConn(wifi_name)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False)\n        elif action == 'editWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_name = request.POST.get(\"wifi_ap\")\n            wifi_pass = escape(request.POST.get(\"wifi_password\"))\n            edit_WifiConn(wifi_name,wifi_pass)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False)\n        return JsonResponse(serializer.errors, status=400)\n\ndef index(request):\n    return render(request, 'index.html')\n\nclass BoxDetailsViewSet(viewsets.ModelViewSet):\n    queryset = BoxDetails.objects.all()\n    serializer_class = BoxDetailsSerializer\n\nclass RegisteredServicesViewSet(viewsets.ModelViewSet):\n    queryset = RegisteredServices.objects.all()\n    serializer_class = RegisteredServicesSerializer    \n\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/Internet-of-People/titania-ux/blob/8ecb42ffb736232f6e84fae016498c431e7ae394",
        "file_path": "/vuedj/configtitania/views.py",
        "source": "from django.shortcuts import render\nfrom django.http import HttpResponse, JsonResponse\nfrom django.views.decorators.csrf import csrf_exempt\n\nfrom rest_framework.renderers import JSONRenderer\nfrom rest_framework.parsers import JSONParser\nfrom rest_framework.response import Response\nfrom rest_framework import viewsets\nfrom rest_framework.decorators import list_route\nfrom flask import escape\n\nfrom .models import BoxDetails, RegisteredServices\nfrom .serializers import BoxDetailsSerializer, RegisteredServicesSerializer\n\nimport common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd \n\n# fetch network AP details\nnm = NetworkManager.NetworkManager\nwlans = [d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)]\n\ndef get_osversion():\n    \"\"\"\n    PRETTY_NAME of your Titania os (in lowercase).\n    \"\"\"\n    with open(\"/etc/os-release\") as f:\n        osfilecontent = f.read().split(\"\\n\")\n        # $PRETTY_NAME is at the 5th position\n        version = osfilecontent[4].split('=')[1].strip('\\\"')\n        return version\n\ndef get_allconfiguredwifi():\n    \"\"\"\n    nmcli con | grep 802-11-wireless\n    \"\"\"\n    ps = subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0]\n    wifirows = ps.split('\\n')\n    wifi = []\n    for row in wifirows:\n        name = row.split(':')\n        print(name)\n        wifi.append(name[0])\n    return wifi\n\ndef get_allAPs():\n    \"\"\"\n    nmcli con | grep 802-11-wireless\n    \"\"\"\n    ps = subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0]\n    wifirows = ps.split('\\n')\n    wifi = []\n    for row in wifirows:\n        entry = row.split(':')\n        print(entry)\n        wifi.append(entry)\n    return wifi\n    # wifi_aps = []   \n    # for dev in wlans:\n    #     for ap in dev.AccessPoints:\n    #         wifi_aps.append(ap.Ssid)\n    # return wifi_aps\n\ndef add_user(username, password):\n    encPass = crypt.crypt(password,\"22\")\n    os.system(\"useradd -G docker,wheel -p \"+encPass+\" \"+username)\n\ndef add_newWifiConn(wifiname, wifipass):\n    print(wlans)\n    wlan0 = wlans[0]\n    print(wlan0)\n    print(wifiname)\n    # get selected ap as currentwifi\n    for dev in wlans:\n        for ap in dev.AccessPoints:\n            if ap.Ssid == wifiname:\n                currentwifi = ap\n    print(currentwifi)\n    # params to set password\n    params = {\n            \"802-11-wireless\": {\n                \"security\": \"802-11-wireless-security\",\n            },\n            \"802-11-wireless-security\": {\n                \"key-mgmt\": \"wpa-psk\",\n                \"psk\": wifipass\n            },\n        }\n    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi)        \n\ndef delete_WifiConn(wifiap):\n    \"\"\"\n    nmcli connection delete id <connection name>\n    \"\"\"\n    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE)\n    print(ps)\n\ndef edit_WifiConn(wifiname, wifipass):\n    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE)\n    print(ps)\n    print(wlans)\n    wlan0 = wlans[0]\n    print(wlan0)\n    print(wifiname)\n    # get selected ap as currentwifi\n    for dev in wlans:\n        for ap in dev.AccessPoints:\n            if ap.Ssid == wifiname:\n                currentwifi = ap\n    # params to set password\n    params = {\n            \"802-11-wireless\": {\n                \"security\": \"802-11-wireless-security\",\n            },\n            \"802-11-wireless-security\": {\n                \"key-mgmt\": \"wpa-psk\",\n                \"psk\": wifipass\n            },\n        }\n    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi) \n    return       \n\n@csrf_exempt\ndef handle_config(request):\n    \"\"\"\n    List all code snippets, or create a new snippet.\n    \"\"\" \n    if request.method == 'POST':\n        action = request.POST.get(\"_action\")\n        print(action)\n        if action == 'registerService':\n            request_name = request.POST.get(\"name\")\n            request_address = request.POST.get(\"address\")\n            request_icon = request.POST.get(\"icon\")\n            print(request_name)\n            print(request_address)\n            print(request_icon)\n            setServiceDetails = RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon)\n            return JsonResponse({\"STATUS\":\"SUCCESS\"}, safe=False)\n        elif action == 'getSchema':\n            schema = get_osversion()\n            return JsonResponse({\"version_info\":schema}, safe=False)\n        elif action == 'getIfConfigured':\n            print(action)\n            queryset = BoxDetails.objects.all()\n            serializer = BoxDetailsSerializer(queryset, many=True)\n            return JsonResponse(serializer.data, safe=False)\n        elif action == 'loadDependencies':\n            print(action)\n            queryset = RegisteredServices.objects.all()\n            serializer = RegisteredServicesSerializer(queryset, many=True)\n            return JsonResponse(serializer.data, safe=False)\n        elif action == 'getAllAPs':\n            wifi_aps = get_allAPs()\n            return JsonResponse(wifi_aps, safe=False)\n        elif action == 'saveUserDetails':\n            print(action)\n            boxname = escape(request.POST.get(\"boxname\"))\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            print(username)\n            add_user(username,password)\n            setBoxName = BoxDetails(boxname=boxname)\n            setBoxName.save()\n            # connect to wifi ap user selected\n            wifi_pass = request.POST.get(\"wifi_password\")\n            wifi_name = request.POST.get(\"wifi_ap\")\n            if len(wifi_name) > 0:\n                add_newWifiConn(wifi_name,wifi_pass)\n            return JsonResponse({\"STATUS\":\"SUCCESS\"}, safe=False)\n        elif action == 'login':\n            print(action)\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            output=''\n            \"\"\"Tries to authenticate a user.\n            Returns True if the authentication succeeds, else the reason\n            (string) is returned.\"\"\"\n            try:\n                enc_pwd = spwd.getspnam(username)[1]\n                if enc_pwd in [\"NP\", \"!\", \"\", None]:\n                    output = \"User '%s' has no password set\" % username\n                if enc_pwd in [\"LK\", \"*\"]:\n                    output = \"account is locked\"\n                if enc_pwd == \"!!\":\n                    output = \"password has expired\"\n                # Encryption happens here, the hash is stripped from the\n                # enc_pwd and the algorithm id and salt are used to encrypt\n                # the password.\n                if crypt.crypt(password, enc_pwd) == enc_pwd:\n                    output = ''\n                else:\n                    output = \"incorrect password\"\n            except KeyError:\n                output = \"User '%s' not found\" % username\n            if len(output) == 0:\n                return JsonResponse({\"username\":username}, safe=False)\n            else:\n                return JsonResponse(output, safe=False)\n        elif action == 'logout':\n            print(action)\n            username = request.POST.get(\"username\")\n            print(username+' ')\n            queryset = User.objects.all().first()\n            if username == queryset.username:\n                return JsonResponse({\"STATUS\":\"SUCCESS\", \"username\":queryset.username}, safe=False)\n        elif action == 'getDashboardCards':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_DASHBOARD_CARDS)\n            rows = cursor.fetchall()\n            print(rows)\n            return JsonResponse(rows, safe=False)\n        elif action == 'getDashboardChart':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            for row in rows:\n                cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],])\n                datasets = cursor.fetchall()\n                print(datasets)\n                data = {'container_name' : row[1], 'data': datasets}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getDockerOverview':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_DOCKER_OVERVIEW)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            for row in rows:\n                data = {'state': row[0], 'container_id': row[1], 'name': row[2],\n                        'image': row[3], 'running_for': row[4],\n                        'command': row[5], 'ports': row[6],\n                        'status': row[7], 'networks': row[8]}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getContainerStats':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            datasets_io = []\n            datasets_mem = []\n            datasets_perc = []\n            for row in rows:\n                datasets_io = []\n                datasets_mem = []\n                datasets_perc = []\n                # values with % appended to them\n                for iter in range(0,2):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_perc.append(counter_val)\n                # values w/o % appended to them\n                for iter in range(2,4):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_mem.append(counter_val)\n                # values w/o % appended to them\n                for iter in range(4,8):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_io.append(counter_val)\n                data = {'container_id': row[0], 'container_name' : row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getThreads':\n            print(action)\n            rows = []\n            ps = subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0]\n            processes = ps.decode().split('\\n')\n            # this specifies the number of splits, so the splitted lines\n            # will have (nfields+1) elements\n            nfields = len(processes[0].split()) - 1\n            for row in processes[4:]:\n                rows.append(row.split(None, nfields))\n            return JsonResponse(rows, safe=False)\n        elif action == 'getContainerTop':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            resultset = []\n            for i in rows:\n                data = {}\n                datasets = []\n                ps = subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0]\n                processes = ps.decode().split('\\n')\n                # this specifies the number of splits, so the splitted lines\n                # will have (nfields+1) elements\n                nfields = len(processes[0].split()) - 1\n                for p in processes[1:]:\n                    datasets.append(p.split(None, nfields))\n                data = {'container_id': i[0], 'container_name' : i[1], 'data': datasets}\n                resultset.append(data)\n            return JsonResponse(resultset, safe=False)\n        elif action == 'getSettings':\n            print(action)\n            ps = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = ps.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False)\n        elif action == 'deleteUser':\n            print(action)\n            username = escape(request.POST.get(\"user\"))\n            ps = subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate()\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False)\n        elif action == 'addNewUser':\n            print(action)\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            add_user(username,password)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False)\n        elif action == 'addWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_pass = escape(request.POST.get(\"wifi_password\"))\n            wifi_name = request.POST.get(\"wifi_ap\")\n            if len(wifi_name) > 0:\n                add_newWifiConn(wifi_name,wifi_pass)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False)\n        elif action == 'deleteWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_name = request.POST.get(\"wifi\")\n            delete_WifiConn(wifi_name)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False)\n        elif action == 'editWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_name = request.POST.get(\"wifi_ap\")\n            wifi_pass = escape(request.POST.get(\"wifi_password\"))\n            edit_WifiConn(wifi_name,wifi_pass)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False)\n        return JsonResponse(serializer.errors, status=400)\n\ndef index(request):\n    return render(request, 'index.html')\n\nclass BoxDetailsViewSet(viewsets.ModelViewSet):\n    queryset = BoxDetails.objects.all()\n    serializer_class = BoxDetailsSerializer\n\nclass RegisteredServicesViewSet(viewsets.ModelViewSet):\n    queryset = RegisteredServices.objects.all()\n    serializer_class = RegisteredServicesSerializer    \n\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/DavidPL1/Hyperion/blob/9940b677381fcc38595a1452b1586480fd8a6146",
        "file_path": "/hyperion/hyperion.py",
        "source": "#! /usr/bin/env python\nfrom libtmux import Server\nfrom yaml import load, dump\nfrom setupParser import Loader\nfrom DepTree import Node, dep_resolve, CircularReferenceException\nimport logging\nimport os\nimport socket\nimport argparse\nfrom psutil import Process\nfrom subprocess import call\nfrom graphviz import Digraph\nfrom enum import Enum\nfrom time import sleep\n\nimport sys\nfrom PyQt4 import QtGui\nimport hyperGUI\n\nFORMAT = \"%(asctime)s: %(name)s [%(levelname)s]:\\t%(message)s\"\n\nlogging.basicConfig(level=logging.WARNING, format=FORMAT, datefmt='%I:%M:%S')\nTMP_SLAVE_DIR = \"/tmp/Hyperion/slave/components\"\nTMP_COMP_DIR = \"/tmp/Hyperion/components\"\nTMP_LOG_PATH = \"/tmp/Hyperion/log\"\n\nBASE_DIR = os.path.dirname(__file__)\nSCRIPT_CLONE_PATH = (\"%s/scripts/start_named_clone_session.sh\" % BASE_DIR)\n\n\nclass CheckState(Enum):\n    RUNNING = 0\n    STOPPED = 1\n    STOPPED_BUT_SUCCESSFUL = 2\n    STARTED_BY_HAND = 3\n    DEP_FAILED = 4\n\n\nclass ControlCenter:\n\n    def __init__(self, configfile=None):\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.DEBUG)\n        self.configfile = configfile\n        self.nodes = {}\n        self.server = []\n        self.host_list = []\n\n        if configfile:\n            self.load_config(configfile)\n            self.session_name = self.config[\"name\"]\n\n            # Debug write resulting yaml file\n            with open('debug-result.yml', 'w') as outfile:\n                dump(self.config, outfile, default_flow_style=False)\n            self.logger.debug(\"Loading config was successful\")\n\n            self.server = Server()\n\n            if self.server.has_session(self.session_name):\n                self.session = self.server.find_where({\n                    \"session_name\": self.session_name\n                })\n\n                self.logger.info('found running session by name \"%s\" on server' % self.session_name)\n            else:\n                self.logger.info('starting new session by name \"%s\" on server' % self.session_name)\n                self.session = self.server.new_session(\n                    session_name=self.session_name,\n                    window_name=\"Main\"\n                )\n        else:\n            self.config = None\n\n    ###################\n    # Setup\n    ###################\n    def load_config(self, filename=\"default.yaml\"):\n        with open(filename) as data_file:\n            self.config = load(data_file, Loader)\n\n    def init(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n\n        else:\n            for group in self.config['groups']:\n                for comp in group['components']:\n                    self.logger.debug(\"Checking component '%s' in group '%s' on host '%s'\" %\n                                      (comp['name'], group['name'], comp['host']))\n\n                    if comp['host'] != \"localhost\" and not self.run_on_localhost(comp):\n                        self.copy_component_to_remote(comp, comp['name'], comp['host'])\n\n            # Remove duplicate hosts\n            self.host_list = list(set(self.host_list))\n\n            self.set_dependencies(True)\n\n    def set_dependencies(self, exit_on_fail):\n        for group in self.config['groups']:\n            for comp in group['components']:\n                self.nodes[comp['name']] = Node(comp)\n\n        # Add a pseudo node that depends on all other nodes, to get a starting point to be able to iterate through all\n        # nodes with simple algorithms\n        master_node = Node({'name': 'master_node'})\n        for name in self.nodes:\n            node = self.nodes.get(name)\n\n            # Add edges from each node to pseudo node\n            master_node.addEdge(node)\n\n            # Add edges based on dependencies specified in the configuration\n            if \"depends\" in node.component:\n                for dep in node.component['depends']:\n                    if dep in self.nodes:\n                        node.addEdge(self.nodes[dep])\n                    else:\n                        self.logger.error(\"Unmet dependency: '%s' for component '%s'!\" % (dep, node.comp_name))\n                        if exit_on_fail:\n                            exit(1)\n        self.nodes['master_node'] = master_node\n\n        # Test if starting all components is possible\n        try:\n            node = self.nodes.get('master_node')\n            res = []\n            unres = []\n            dep_resolve(node, res, unres)\n            dep_string = \"\"\n            for node in res:\n                if node is not master_node:\n                    dep_string = \"%s -> %s\" % (dep_string, node.comp_name)\n            self.logger.debug(\"Dependency tree for start all: %s\" % dep_string)\n        except CircularReferenceException as ex:\n            self.logger.error(\"Detected circular dependency reference between %s and %s!\" % (ex.node1, ex.node2))\n            if exit_on_fail:\n                exit(1)\n\n    def copy_component_to_remote(self, infile, comp, host):\n        self.host_list.append(host)\n\n        self.logger.debug(\"Saving component to tmp\")\n        tmp_comp_path = ('%s/%s.yaml' % (TMP_COMP_DIR, comp))\n        ensure_dir(tmp_comp_path)\n        with open(tmp_comp_path, 'w') as outfile:\n            dump(infile, outfile, default_flow_style=False)\n\n        self.logger.debug('Copying component \"%s\" to remote host \"%s\"' % (comp, host))\n        cmd = (\"ssh %s 'mkdir -p %s' & scp %s %s:%s/%s.yaml\" %\n               (host, TMP_SLAVE_DIR, tmp_comp_path, host, TMP_SLAVE_DIR, comp))\n        self.logger.debug(cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Stop\n    ###################\n    def stop_component(self, comp):\n        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):\n            self.logger.debug(\"Stopping remote component '%s' on host '%s'\" % (comp['name'], comp['host']))\n            self.stop_remote_component(comp['name'], comp['host'])\n        else:\n            window = find_window(self.session, comp['name'])\n\n            if window:\n                self.logger.debug(\"window '%s' found running\" % comp['name'])\n                self.logger.info(\"Shutting down window...\")\n                kill_window(window)\n                self.logger.info(\"... done!\")\n\n    def stop_remote_component(self, comp_name, host):\n        # invoke Hyperion in slave mode on each remote host\n        cmd = (\"ssh %s 'hyperion --config %s/%s.yaml slave --kill'\" % (host, TMP_SLAVE_DIR, comp_name))\n        self.logger.debug(\"Run cmd:\\n%s\" % cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Start\n    ###################\n    def start_component(self, comp):\n\n        node = self.nodes.get(comp['name'])\n        res = []\n        unres = []\n        dep_resolve(node, res, unres)\n        for node in res:\n            self.logger.debug(\"node name '%s' vs. comp name '%s'\" % (node.comp_name, comp['name']))\n            if node.comp_name != comp['name']:\n                self.logger.debug(\"Checking and starting %s\" % node.comp_name)\n                state = self.check_component(node.component)\n                if (state is CheckState.STOPPED_BUT_SUCCESSFUL or\n                        state is CheckState.STARTED_BY_HAND or\n                        state is CheckState.RUNNING):\n                    self.logger.debug(\"Component %s is already running, skipping to next in line\" % comp['name'])\n                else:\n                    self.logger.debug(\"Start component '%s' as dependency of '%s'\" % (node.comp_name, comp['name']))\n                    self.start_component_without_deps(node.component)\n\n                    tries = 0\n                    while True:\n                        self.logger.debug(\"Checking %s resulted in checkstate %s\" % (node.comp_name, state))\n                        state = self.check_component(node.component)\n                        if (state is not CheckState.RUNNING or\n                           state is not CheckState.STOPPED_BUT_SUCCESSFUL):\n                            break\n                        if tries > 100:\n                            return False\n                        tries = tries + 1\n                        sleep(.5)\n\n        self.logger.debug(\"All dependencies satisfied, starting '%s'\" % (comp['name']))\n        state = self.check_component(node.component)\n        if (state is CheckState.STARTED_BY_HAND or\n                state is CheckState.RUNNING):\n            self.logger.debug(\"Component %s is already running. Skipping start\" % comp['name'])\n        else:\n            self.start_component_without_deps(comp)\n        return True\n\n    def start_component_without_deps(self, comp):\n        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):\n            self.logger.debug(\"Starting remote component '%s' on host '%s'\" % (comp['name'], comp['host']))\n            self.start_remote_component(comp['name'], comp['host'])\n        else:\n            log_file = (\"%s/%s\" % (TMP_LOG_PATH, comp['name']))\n            window = find_window(self.session, comp['name'])\n\n            if window:\n                self.logger.debug(\"Restarting '%s' in old window\" % comp['name'])\n                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])\n            else:\n                self.logger.info(\"creating window '%s'\" % comp['name'])\n                window = self.session.new_window(comp['name'])\n                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])\n\n    def start_remote_component(self, comp_name, host):\n        # invoke Hyperion in slave mode on each remote host\n        cmd = (\"ssh %s 'hyperion --config %s/%s.yaml slave'\" % (host, TMP_SLAVE_DIR, comp_name))\n        self.logger.debug(\"Run cmd:\\n%s\" % cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Check\n    ###################\n    def check_component(self, comp):\n        return check_component(comp, self.session, self.logger)\n\n    ###################\n    # Dependency management\n    ###################\n    def get_dep_list(self, comp):\n        node = self.nodes.get(comp['name'])\n        res = []\n        unres = []\n        dep_resolve(node, res, unres)\n        res.remove(node)\n\n        return res\n\n    ###################\n    # Host related checks\n    ###################\n    def is_localhost(self, hostname):\n        try:\n            hn_out = socket.gethostbyname(hostname)\n            if hn_out == '127.0.0.1' or hn_out == '::1':\n                self.logger.debug(\"Host '%s' is localhost\" % hostname)\n                return True\n            else:\n                self.logger.debug(\"Host '%s' is not localhost\" % hostname)\n                return False\n        except socket.gaierror:\n            sys.exit(\"Host '%s' is unknown! Update your /etc/hosts file!\" % hostname)\n\n    def run_on_localhost(self, comp):\n        return self.is_localhost(comp['host'])\n\n    ###################\n    # TMUX\n    ###################\n    def kill_remote_session_by_name(self, name, host):\n        cmd = \"ssh -t %s 'tmux kill-session -t %s'\" % (host, name)\n        send_main_session_command(self.session, cmd)\n\n    def start_clone_session(self, comp_name, session_name):\n        cmd = \"%s '%s' '%s'\" % (SCRIPT_CLONE_PATH, session_name, comp_name)\n        send_main_session_command(self.session, cmd)\n\n    def start_remote_clone_session(self, comp_name, session_name, hostname):\n        remote_cmd = (\"%s '%s' '%s'\" % (SCRIPT_CLONE_PATH, session_name, comp_name))\n        cmd = \"ssh %s 'bash -s' < %s\" % (hostname, remote_cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Visualisation\n    ###################\n    def draw_graph(self):\n        deps = Digraph(\"Deps\", strict=True)\n        deps.graph_attr.update(rankdir=\"BT\")\n        try:\n            node = self.nodes.get('master_node')\n\n            for current in node.depends_on:\n                deps.node(current.comp_name)\n\n                res = []\n                unres = []\n                dep_resolve(current, res, unres)\n                for node in res:\n                    if \"depends\" in node.component:\n                        for dep in node.component['depends']:\n                            if dep not in self.nodes:\n                                deps.node(dep, color=\"red\")\n                                deps.edge(node.comp_name, dep, \"missing\", color=\"red\")\n                            elif node.comp_name is not \"master_node\":\n                                deps.edge(node.comp_name, dep)\n\n        except CircularReferenceException as ex:\n            self.logger.error(\"Detected circular dependency reference between %s and %s!\" % (ex.node1, ex.node2))\n            deps.edge(ex.node1, ex.node2, \"circular error\", color=\"red\")\n            deps.edge(ex.node2, ex.node1, color=\"red\")\n\n        deps.view()\n\n\nclass SlaveLauncher:\n\n    def __init__(self, configfile=None, kill_mode=False, check_mode=False):\n        self.kill_mode = kill_mode\n        self.check_mode = check_mode\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.DEBUG)\n        self.config = None\n        self.session = None\n        if kill_mode:\n            self.logger.info(\"started slave with kill mode\")\n        if check_mode:\n            self.logger.info(\"started slave with check mode\")\n        self.server = Server()\n\n        if self.server.has_session(\"slave-session\"):\n            self.session = self.server.find_where({\n                \"session_name\": \"slave-session\"\n            })\n\n            self.logger.info('found running slave session on server')\n        elif not kill_mode and not check_mode:\n            self.logger.info('starting new slave session on server')\n            self.session = self.server.new_session(\n                session_name=\"slave-session\"\n            )\n\n        else:\n            self.logger.info(\"No slave session found on server. Aborting\")\n            exit(CheckState.STOPPED)\n\n        if configfile:\n            self.load_config(configfile)\n            self.window_name = self.config['name']\n            self.flag_path = (\"/tmp/Hyperion/slaves/%s\" % self.window_name)\n            self.log_file = (\"/tmp/Hyperion/log/%s\" % self.window_name)\n            ensure_dir(self.log_file)\n        else:\n            self.logger.error(\"No slave component config provided\")\n\n    def load_config(self, filename=\"default.yaml\"):\n        with open(filename) as data_file:\n            self.config = load(data_file, Loader)\n\n    def init(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n        elif not self.session:\n            self.logger.error(\" Init aborted. No session was found!\")\n        else:\n            self.logger.debug(self.config)\n            window = find_window(self.session, self.window_name)\n\n            if window:\n                self.logger.debug(\"window '%s' found running\" % self.window_name)\n                if self.kill_mode:\n                    self.logger.info(\"Shutting down window...\")\n                    kill_window(window)\n                    self.logger.info(\"... done!\")\n            elif not self.kill_mode:\n                self.logger.info(\"creating window '%s'\" % self.window_name)\n                window = self.session.new_window(self.window_name)\n                start_window(window, self.config['cmd'][0]['start'], self.log_file, self.window_name)\n\n            else:\n                self.logger.info(\"There is no component running by the name '%s'. Exiting kill mode\" %\n                                 self.window_name)\n\n    def run_check(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n            exit(CheckState.STOPPED.value)\n        elif not self.session:\n            self.logger.error(\" Init aborted. No session was found!\")\n            exit(CheckState.STOPPED.value)\n\n        check_state = check_component(self.config, self.session, self.logger)\n        exit(check_state.value)\n\n###################\n# Component Management\n###################\ndef run_component_check(comp):\n    if call(comp['cmd'][1]['check'], shell=True) == 0:\n        return True\n    else:\n        return False\n\n\ndef check_component(comp, session, logger):\n    logger.debug(\"Running component check for %s\" % comp['name'])\n    check_available = len(comp['cmd']) > 1 and 'check' in comp['cmd'][1]\n    window = find_window(session, comp['name'])\n    if window:\n        pid = get_window_pid(window)\n        logger.debug(\"Found window pid: %s\" % pid)\n\n        # May return more child pids if logging is done via tee (which then was started twice in the window too)\n        procs = []\n        for entry in pid:\n            procs.extend(Process(entry).children(recursive=True))\n        pids = [p.pid for p in procs]\n        logger.debug(\"Window is running %s child processes\" % len(pids))\n\n        # Two processes are tee logging\n        # TODO: Change this when more logging options are introduced\n        if len(pids) < 3:\n            logger.debug(\"Main window process has finished. Running custom check if available\")\n            if check_available and run_component_check(comp):\n                logger.debug(\"Process terminated but check was successful\")\n                return CheckState.STOPPED_BUT_SUCCESSFUL\n            else:\n                logger.debug(\"Check failed or no check available: returning false\")\n                return CheckState.STOPPED\n        elif check_available and run_component_check(comp):\n            logger.debug(\"Check succeeded\")\n            return CheckState.RUNNING\n        elif not check_available:\n            logger.debug(\"No custom check specified and got sufficient pid amount: returning true\")\n            return CheckState.RUNNING\n        else:\n            logger.debug(\"Check failed: returning false\")\n            return CheckState.STOPPED\n    else:\n        logger.debug(\"%s window is not running. Running custom check\" % comp['name'])\n        if check_available and run_component_check(comp):\n            logger.debug(\"Component was not started by Hyperion, but the check succeeded\")\n            return CheckState.STARTED_BY_HAND\n        else:\n            logger.debug(\"Window not running and no check command is available or it failed: returning false\")\n            return CheckState.STOPPED\n\n\ndef get_window_pid(window):\n    r = window.cmd('list-panes',\n                   \"-F #{pane_pid}\")\n    return [int(p) for p in r.stdout]\n\n###################\n# TMUX\n###################\ndef kill_session_by_name(server, name):\n    session = server.find_where({\n        \"session_name\": name\n    })\n    session.kill_session()\n\n\ndef kill_window(window):\n    window.cmd(\"send-keys\", \"\", \"C-c\")\n    window.kill_window()\n\n\ndef start_window(window, cmd, log_file, comp_name):\n    setup_log(window, log_file, comp_name)\n    window.cmd(\"send-keys\", cmd, \"Enter\")\n\n\ndef find_window(session, window_name):\n    window = session.find_where({\n        \"window_name\": window_name\n    })\n    return window\n\n\ndef send_main_session_command(session, cmd):\n    window = find_window(session, \"Main\")\n    window.cmd(\"send-keys\", cmd, \"Enter\")\n\n\n###################\n# Logging\n###################\ndef setup_log(window, file, comp_name):\n    clear_log(file)\n    # Reroute stderr to log file\n    window.cmd(\"send-keys\", \"exec 2> >(exec tee -i -a '%s')\" % file, \"Enter\")\n    # Reroute stdin to log file\n    window.cmd(\"send-keys\", \"exec 1> >(exec tee -i -a '%s')\" % file, \"Enter\")\n    window.cmd(\"send-keys\", ('echo \"#Hyperion component start: %s\\n$(date)\"' % comp_name), \"Enter\")\n\n\ndef clear_log(file_path):\n    if os.path.isfile(file_path):\n        os.remove(file_path)\n\n\ndef ensure_dir(file_path):\n    directory = os.path.dirname(file_path)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n###################\n# Startup\n###################\ndef main():\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n    parser = argparse.ArgumentParser()\n\n    # Create top level parser\n    parser.add_argument(\"--config\", '-c', type=str,\n                        default='test.yaml',\n                        help=\"YAML config file. see sample-config.yaml. Default: test.yaml\")\n    subparsers = parser.add_subparsers(dest=\"cmd\")\n\n    # Create parser for the editor command\n    subparser_editor = subparsers.add_parser('edit', help=\"Launches the editor to edit or create new systems and \"\n                                                          \"components\")\n    # Create parser for the run command\n    subparser_run = subparsers.add_parser('run', help=\"Launches the setup specified by the --config argument\")\n    # Create parser for validator\n    subparser_val = subparsers.add_parser('validate', help=\"Validate the setup specified by the --config argument\")\n\n    subparser_remote = subparsers.add_parser('slave', help=\"Run a component locally without controlling it. The \"\n                                                           \"control is taken care of the remote master invoking \"\n                                                           \"this command.\\nIf run with the --kill flag, the \"\n                                                           \"passed component will be killed\")\n\n    subparser_val.add_argument(\"--visual\", help=\"Generate and show a graph image\", action=\"store_true\")\n\n    remote_mutex = subparser_remote.add_mutually_exclusive_group(required=False)\n\n    remote_mutex.add_argument('-k', '--kill', help=\"switch to kill mode\", action=\"store_true\")\n    remote_mutex.add_argument('-c', '--check', help=\"Run a component check\", action=\"store_true\")\n\n    args = parser.parse_args()\n    logger.debug(args)\n\n    if args.cmd == 'edit':\n        logger.debug(\"Launching editor mode\")\n\n    elif args.cmd == 'run':\n        logger.debug(\"Launching runner mode\")\n\n        cc = ControlCenter(args.config)\n        cc.init()\n        start_gui(cc)\n\n    elif args.cmd == 'validate':\n        logger.debug(\"Launching validation mode\")\n        cc = ControlCenter(args.config)\n        if args.visual:\n            cc.set_dependencies(False)\n            cc.draw_graph()\n        else:\n            cc.set_dependencies(True)\n\n    elif args.cmd == 'slave':\n        logger.debug(\"Launching slave mode\")\n        sl = SlaveLauncher(args.config, args.kill, args.check)\n\n        if args.check:\n            sl.run_check()\n        else:\n            sl.init()\n\n\n###################\n# GUI\n###################\ndef start_gui(control_center):\n    app = QtGui.QApplication(sys.argv)\n    main_window = QtGui.QMainWindow()\n    ui = hyperGUI.UiMainWindow()\n    ui.ui_init(main_window, control_center)\n    main_window.show()\n    sys.exit(app.exec_())\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/nakajima-hiro/invenio/blob/d64fffea5a05775ba2db65cba5408c2e9635e354",
        "file_path": "/invenio/legacy/bibclassify/engine.py",
        "source": "# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.\n#\n# Invenio is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\"\"\"\nBibClassify engine.\n\nThis module is the main module of BibClassify. its two main methods are\noutput_keywords_for_sources and get_keywords_from_text. The first one output\nkeywords for a list of sources (local files or URLs, PDF or text) while the\nsecond one outputs the keywords for text lines (which are obtained using the\nmodule bibclassify_text_normalizer).\n\nThis module also takes care of the different outputs (text, MARCXML or HTML).\nBut unfortunately there is a confusion between running in a standalone mode\nand producing output suitable for printing, and running in a web-based\nmode where the webtemplate is used. For the moment the pieces of the representation\ncode are left in this module.\n\"\"\"\n\nfrom __future__ import print_function\n\nimport os\nfrom six import iteritems\nimport config as bconfig\n\nfrom invenio.legacy.bibclassify import ontology_reader as reader\nimport text_extractor as extractor\nimport text_normalizer as normalizer\nimport keyword_analyzer as keyworder\nimport acronym_analyzer as acronymer\n\nfrom invenio.utils.url import make_user_agent_string\nfrom invenio.utils.text import encode_for_xml\n\nlog = bconfig.get_logger(\"bibclassify.engine\")\n\n# ---------------------------------------------------------------------\n#                          API\n# ---------------------------------------------------------------------\n\n\ndef output_keywords_for_sources(input_sources, taxonomy_name, output_mode=\"text\",\n                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,\n                                match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,\n                                api=False, **kwargs):\n    \"\"\"Output the keywords for each source in sources.\"\"\"\n\n    # Inner function which does the job and it would be too much work to\n    # refactor the call (and it must be outside the loop, before it did\n    # not process multiple files)\n    def process_lines():\n        if output_mode == \"text\":\n            print(\"Input file: %s\" % source)\n\n        output = get_keywords_from_text(\n            text_lines,\n            taxonomy_name,\n            output_mode=output_mode,\n            output_limit=output_limit,\n            spires=spires,\n            match_mode=match_mode,\n            no_cache=no_cache,\n            with_author_keywords=with_author_keywords,\n            rebuild_cache=rebuild_cache,\n            only_core_tags=only_core_tags,\n            extract_acronyms=extract_acronyms\n        )\n        if api:\n            return output\n        else:\n            if isinstance(output, dict):\n                for i in output:\n                    print(output[i])\n\n    # Get the fulltext for each source.\n    for entry in input_sources:\n        log.info(\"Trying to read input file %s.\" % entry)\n        text_lines = None\n        source = \"\"\n        if os.path.isdir(entry):\n            for filename in os.listdir(entry):\n                if filename.startswith('.'):\n                    continue\n                filename = os.path.join(entry, filename)\n                if os.path.isfile(filename):\n                    text_lines = extractor.text_lines_from_local_file(filename)\n                    if text_lines:\n                        source = filename\n                        process_lines()\n        elif os.path.isfile(entry):\n            text_lines = extractor.text_lines_from_local_file(entry)\n            if text_lines:\n                source = os.path.basename(entry)\n                process_lines()\n        else:\n            # Treat as a URL.\n            text_lines = extractor.text_lines_from_url(entry,\n                                                       user_agent=make_user_agent_string(\"BibClassify\"))\n            if text_lines:\n                source = entry.split(\"/\")[-1]\n                process_lines()\n\n\ndef get_keywords_from_local_file(local_file, taxonomy_name, output_mode=\"text\",\n                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,\n                                 match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,\n                                 **kwargs):\n    \"\"\"Outputs keywords reading a local file. Arguments and output are the same\n    as for :see: get_keywords_from_text() \"\"\"\n\n    log.info(\"Analyzing keywords for local file %s.\" % local_file)\n    text_lines = extractor.text_lines_from_local_file(local_file)\n\n    return get_keywords_from_text(text_lines,\n                                  taxonomy_name,\n                                  output_mode=output_mode,\n                                  output_limit=output_limit,\n                                  spires=spires,\n                                  match_mode=match_mode,\n                                  no_cache=no_cache,\n                                  with_author_keywords=with_author_keywords,\n                                  rebuild_cache=rebuild_cache,\n                                  only_core_tags=only_core_tags,\n                                  extract_acronyms=extract_acronyms)\n\n\ndef get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\",\n                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,\n                           spires=False, match_mode=\"full\", no_cache=False,\n                           with_author_keywords=False, rebuild_cache=False,\n                           only_core_tags=False, extract_acronyms=False,\n                           **kwargs):\n    \"\"\"Extract keywords from the list of strings\n\n    :param text_lines: list of strings (will be normalized before being\n        joined into one string)\n    :param taxonomy_name: string, name of the taxonomy_name\n    :param output_mode: string - text|html|marcxml|raw\n    :param output_limit: int\n    :param spires: boolean, if True marcxml output reflect spires codes.\n    :param match_mode: str - partial|full; in partial mode only\n        beginning of the fulltext is searched.\n    :param no_cache: boolean, means loaded definitions will not be saved.\n    :param with_author_keywords: boolean, extract keywords from the pdfs.\n    :param rebuild_cache: boolean\n    :param only_core_tags: boolean\n    :return: if output_mode=raw, it will return\n        (single_keywords, composite_keywords, author_keywords, acronyms)\n        for other output modes it returns formatted string\n    \"\"\"\n\n    cache = reader.get_cache(taxonomy_name)\n    if not cache:\n        reader.set_cache(taxonomy_name,\n                         reader.get_regular_expressions(taxonomy_name,\n                                                        rebuild=rebuild_cache,\n                                                        no_cache=no_cache))\n        cache = reader.get_cache(taxonomy_name)\n    _skw = cache[0]\n    _ckw = cache[1]\n    text_lines = normalizer.cut_references(text_lines)\n    fulltext = normalizer.normalize_fulltext(\"\\n\".join(text_lines))\n\n    if match_mode == \"partial\":\n        fulltext = _get_partial_text(fulltext)\n    author_keywords = None\n    if with_author_keywords:\n        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)\n    acronyms = {}\n    if extract_acronyms:\n        acronyms = extract_abbreviations(fulltext)\n\n    single_keywords = extract_single_keywords(_skw, fulltext)\n    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)\n\n    if only_core_tags:\n        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))\n        composite_keywords = _filter_core_keywors(composite_keywords)\n    else:\n        # Filter out the \"nonstandalone\" keywords\n        single_keywords = clean_before_output(single_keywords)\n    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,\n                               author_keywords, acronyms, output_mode, output_limit,\n                               spires, only_core_tags)\n\n\ndef extract_single_keywords(skw_db, fulltext):\n    \"\"\"Find single keywords in the fulltext\n    :var skw_db: list of KeywordToken objects\n    :var fulltext: string, which will be searched\n    :return : dictionary of matches in a format {\n            <keyword object>, [[position, position...], ],\n            ..\n            }\n            or empty {}\n    \"\"\"\n    return keyworder.get_single_keywords(skw_db, fulltext) or {}\n\n\ndef extract_composite_keywords(ckw_db, fulltext, skw_spans):\n    \"\"\"Returns a list of composite keywords bound with the number of\n    occurrences found in the text string.\n    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)\n    :var fulltext: string to search in\n    :skw_spans: dictionary of already identified single keywords\n    :return : dictionary of matches in a format {\n            <keyword object>, [[position, position...], [info_about_matches] ],\n            ..\n            }\n            or empty {}\n    \"\"\"\n    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}\n\n\ndef extract_abbreviations(fulltext):\n    \"\"\"Extract acronyms from the fulltext\n    :var fulltext: utf-8 string\n    :return: dictionary of matches in a formt {\n          <keyword object>, [matched skw or ckw object, ....]\n          }\n          or empty {}\n    \"\"\"\n    acronyms = {}\n    K = reader.KeywordToken\n    for k, v in acronymer.get_acronyms(fulltext).items():\n        acronyms[K(k, type='acronym')] = v\n    return acronyms\n\n\ndef extract_author_keywords(skw_db, ckw_db, fulltext):\n    \"\"\"Finds out human defined keyowrds in a text string. Searches for\n    the string \"Keywords:\" and its declinations and matches the\n    following words.\n\n    :var skw_db: list single kw object\n    :var ckw_db: list of composite kw objects\n    :var fulltext: utf-8 string\n    :return: dictionary of matches in a formt {\n          <keyword object>, [matched skw or ckw object, ....]\n          }\n          or empty {}\n    \"\"\"\n    akw = {}\n    K = reader.KeywordToken\n    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():\n        akw[K(k, type='author-kw')] = v\n    return akw\n\n\n# ---------------------------------------------------------------------\n#                          presentation functions\n# ---------------------------------------------------------------------\n\n\ndef get_keywords_output(single_keywords, composite_keywords, taxonomy_name,\n                        author_keywords=None, acronyms=None, style=\"text\", output_limit=0,\n                        spires=False, only_core_tags=False):\n    \"\"\"Returns a formatted string representing the keywords according\n    to the chosen style. This is the main routing call, this function will\n    also strip unwanted keywords before output and limits the number\n    of returned keywords\n    :var single_keywords: list of single keywords\n    :var composite_keywords: list of composite keywords\n    :var taxonomy_name: string, taxonomy name\n    :keyword author_keywords: dictionary of author keywords extracted from fulltext\n    :keyword acronyms: dictionary of extracted acronyms\n    :keyword style: text|html|marc\n    :keyword output_limit: int, number of maximum keywords printed (it applies\n            to single and composite keywords separately)\n    :keyword spires: boolen meaning spires output style\n    :keyword only_core_tags: boolean\n    \"\"\"\n    categories = {}\n    # sort the keywords, but don't limit them (that will be done later)\n    single_keywords_p = _sort_kw_matches(single_keywords)\n\n    composite_keywords_p = _sort_kw_matches(composite_keywords)\n\n    for w in single_keywords_p:\n        categories[w[0].concept] = w[0].type\n    for w in single_keywords_p:\n        categories[w[0].concept] = w[0].type\n\n    complete_output = _output_complete(single_keywords_p, composite_keywords_p,\n                                       author_keywords, acronyms, spires,\n                                       only_core_tags, limit=output_limit)\n    functions = {\"text\": _output_text, \"marcxml\": _output_marc, \"html\":\n                 _output_html, \"dict\": _output_dict}\n    my_styles = {}\n\n    for s in style:\n        if s != \"raw\":\n            my_styles[s] = functions[s](complete_output, categories)\n        else:\n            if output_limit > 0:\n                my_styles[\"raw\"] = (_kw(_sort_kw_matches(single_keywords, output_limit)),\n                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),\n                                    author_keywords,  # this we don't limit (?)\n                                    _kw(_sort_kw_matches(acronyms, output_limit)))\n            else:\n                my_styles[\"raw\"] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)\n\n    return my_styles\n\n\ndef build_marc(recid, single_keywords, composite_keywords,\n               spires=False, author_keywords=None, acronyms=None):\n    \"\"\"Create xml record.\n\n    :var recid: ingeter\n    :var single_keywords: dictionary of kws\n    :var composite_keywords: dictionary of kws\n    :keyword spires: please don't use, left for historical\n        reasons\n    :keyword author_keywords: dictionary of extracted keywords\n    :keyword acronyms: dictionary of extracted acronyms\n    :return: str, marxml\n    \"\"\"\n    output = ['<collection><record>\\n'\n              '<controlfield tag=\"001\">%s</controlfield>' % recid]\n\n    # no need to sort\n    single_keywords = single_keywords.items()\n    composite_keywords = composite_keywords.items()\n\n    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))\n\n    output.append('</record></collection>')\n\n    return '\\n'.join(output)\n\n\ndef _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,\n                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,\n                 provenience='BibClassify'):\n    \"\"\"Output the keywords in the MARCXML format.\n\n    :var skw_matches: list of single keywords\n    :var ckw_matches: list of composite keywords\n    :var author_keywords: dictionary of extracted author keywords\n    :var acronyms: dictionary of acronyms\n    :var spires: boolean, True=generate spires output - BUT NOTE: it is\n            here only not to break compatibility, in fact spires output\n            should never be used for xml because if we read marc back\n            into the KeywordToken objects, we would not find them\n    :keyword provenience: string that identifies source (authority) that\n        assigned the contents of the field\n    :return: string, formatted MARC\"\"\"\n\n    kw_template = ('<datafield tag=\"%s\" ind1=\"%s\" ind2=\"%s\">\\n'\n                   '    <subfield code=\"2\">%s</subfield>\\n'\n                   '    <subfield code=\"a\">%s</subfield>\\n'\n                   '    <subfield code=\"n\">%s</subfield>\\n'\n                   '    <subfield code=\"9\">%s</subfield>\\n'\n                   '</datafield>\\n')\n\n    output = []\n\n    tag, ind1, ind2 = _parse_marc_code(kw_field)\n    for keywords in (output_complete[\"Single keywords\"], output_complete[\"Core keywords\"]):\n        for kw in keywords:\n            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),\n                                         encode_for_xml(kw), keywords[kw],\n                                         encode_for_xml(categories[kw])))\n\n    for field, keywords in ((auth_field, output_complete[\"Author keywords\"]),\n                            (acro_field, output_complete[\"Acronyms\"])):\n        if keywords and len(keywords) and field:  # field='' we shall not save the keywords\n            tag, ind1, ind2 = _parse_marc_code(field)\n            for kw, info in keywords.items():\n                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),\n                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))\n\n    return \"\".join(output)\n\n\ndef _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,\n                     acronyms=None, spires=False, only_core_tags=False,\n                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):\n\n    if limit:\n        resized_skw = skw_matches[0:limit]\n        resized_ckw = ckw_matches[0:limit]\n    else:\n        resized_skw = skw_matches\n        resized_ckw = ckw_matches\n\n    results = {\"Core keywords\": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}\n\n    if not only_core_tags:\n        results[\"Author keywords\"] = _get_author_keywords(author_keywords, spires=spires)\n        results[\"Composite keywords\"] = _get_compositekws(resized_ckw, spires=spires)\n        results[\"Single keywords\"] = _get_singlekws(resized_skw, spires=spires)\n        results[\"Field codes\"] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)\n        results[\"Acronyms\"] = _get_acronyms(acronyms)\n\n    return results\n\n\ndef _output_dict(complete_output, categories):\n    return {\n        \"complete_output\": complete_output,\n        \"categories\": categories\n    }\n\n\ndef _output_text(complete_output, categories):\n    \"\"\"Output the results obtained in text format.\n\n\n    :return: str, html formatted output\n    \"\"\"\n    output = \"\"\n\n    for result in complete_output:\n        list_result = complete_output[result]\n        if list_result:\n            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],\n                                        reverse=True)\n            output += \"\\n\\n{0}:\\n\".format(result)\n            for element in list_result_sorted:\n                output += \"\\n{0} {1}\".format(list_result[element], element)\n\n    output += \"\\n--\\n{0}\".format(_signature())\n\n    return output\n\n\ndef _output_html(complete_output, categories):\n    \"\"\"Output the same as txt output does, but HTML formatted.\n\n    :var skw_matches: sorted list of single keywords\n    :var ckw_matches: sorted list of composite keywords\n    :var author_keywords: dictionary of extracted author keywords\n    :var acronyms: dictionary of acronyms\n    :var spires: boolean\n    :var only_core_tags: boolean\n    :keyword limit: int, number of printed keywords\n    :return: str, html formatted output\n    \"\"\"\n    return \"\"\"<html>\n    <head>\n      <title>Automatically generated keywords by bibclassify</title>\n    </head>\n    <body>\n    {0}\n    </body>\n    </html>\"\"\".format(\n        _output_text(complete_output).replace('\\n', '<br>')\n    ).replace('\\n', '')\n\n\ndef _get_singlekws(skw_matches, spires=False):\n    \"\"\"\n    :var skw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: list of formatted keywords\n    \"\"\"\n    output = {}\n    for single_keyword, info in skw_matches:\n        output[single_keyword.output(spires)] = len(info[0])\n    return output\n\n\ndef _get_compositekws(ckw_matches, spires=False):\n    \"\"\"\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: list of formatted keywords\n    \"\"\"\n    output = {}\n    for composite_keyword, info in ckw_matches:\n        output[composite_keyword.output(spires)] = {\"numbers\": len(info[0]),\n                                                    \"details\": info[1]}\n    return output\n\n\ndef _get_acronyms(acronyms):\n    \"\"\"Return a formatted list of acronyms.\"\"\"\n    acronyms_str = {}\n    if acronyms:\n        for acronym, expansions in iteritems(acronyms):\n            expansions_str = \", \".join([\"%s (%d)\" % expansion\n                                        for expansion in expansions])\n            acronyms_str[acronym] = expansions_str\n\n    return acronyms\n\n\ndef _get_author_keywords(author_keywords, spires=False):\n    \"\"\"Format the output for the author keywords.\n\n    :return: list of formatted author keywors\n    \"\"\"\n    out = {}\n    if author_keywords:\n        for keyword, matches in author_keywords.items():\n            skw_matches = matches[0]  # dictionary of single keywords\n            ckw_matches = matches[1]  # dict of composite keywords\n            matches_str = []\n            for ckw, spans in ckw_matches.items():\n                matches_str.append(ckw.output(spires))\n            for skw, spans in skw_matches.items():\n                matches_str.append(skw.output(spires))\n            if matches_str:\n                out[keyword] = matches_str\n            else:\n                out[keyword] = 0\n\n    return out\n\n\ndef _get_fieldcodes(skw_matches, ckw_matches, spires=False):\n    \"\"\"Return the output for the field codes.\n\n    :var skw_matches: dict of {keyword: [info,...]}\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: string\"\"\"\n    fieldcodes = {}\n    output = {}\n\n    for skw, _ in skw_matches:\n        for fieldcode in skw.fieldcodes:\n            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))\n    for ckw, _ in ckw_matches:\n\n        if len(ckw.fieldcodes):\n            for fieldcode in ckw.fieldcodes:\n                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))\n        else:  # inherit field-codes from the composites\n            for kw in ckw.getComponents():\n                for fieldcode in kw.fieldcodes:\n                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))\n                    fieldcodes.setdefault('*', set()).add(kw.output(spires))\n\n    for fieldcode, keywords in fieldcodes.items():\n        output[fieldcode] = ', '.join(keywords)\n\n    return output\n\n\ndef _get_core_keywords(skw_matches, ckw_matches, spires=False):\n    \"\"\"Return the output for the field codes.\n\n    :var skw_matches: dict of {keyword: [info,...]}\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: set of formatted core keywords\n    \"\"\"\n    output = {}\n    category = {}\n\n    def _get_value_kw(kw):\n        \"\"\"Help to sort the Core keywords.\"\"\"\n        i = 0\n        while kw[i].isdigit():\n            i += 1\n        if i > 0:\n            return int(kw[:i])\n        else:\n            return 0\n\n    for skw, info in skw_matches:\n        if skw.core:\n            output[skw.output(spires)] = len(info[0])\n            category[skw.output(spires)] = skw.type\n    for ckw, info in ckw_matches:\n        if ckw.core:\n            output[ckw.output(spires)] = len(info[0])\n        else:\n            #test if one of the components is  not core\n            i = 0\n            for c in ckw.getComponents():\n                if c.core:\n                    output[c.output(spires)] = info[1][i]\n                i += 1\n    return output\n\n\ndef _filter_core_keywors(keywords):\n    matches = {}\n    for kw, info in keywords.items():\n        if kw.core:\n            matches[kw] = info\n    return matches\n\n\ndef _signature():\n    \"\"\"Print out the bibclassify signature.\n\n    #todo: add information about taxonomy, rdflib\"\"\"\n\n    return 'bibclassify v%s' % (bconfig.VERSION,)\n\n\ndef clean_before_output(kw_matches):\n    \"\"\"Return a clean copy of the keywords data structure.\n\n    Stripped off the standalone and other unwanted elements\"\"\"\n    filtered_kw_matches = {}\n\n    for kw_match, info in iteritems(kw_matches):\n        if not kw_match.nostandalone:\n            filtered_kw_matches[kw_match] = info\n\n    return filtered_kw_matches\n\n# ---------------------------------------------------------------------\n#                          helper functions\n# ---------------------------------------------------------------------\n\n\ndef _skw_matches_comparator(kw0, kw1):\n    \"\"\"\n    Compare 2 single keywords objects.\n\n    First by the number of their spans (ie. how many times they were found),\n    if it is equal it compares them by lenghts of their labels.\n    \"\"\"\n    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))\n    if list_comparison:\n        return list_comparison\n\n    if kw0[0].isComposite() and kw1[0].isComposite():\n        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])\n        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])\n        component_comparison = cmp(component_avg1, component_avg0)\n        if component_comparison:\n            return component_comparison\n\n    return cmp(len(str(kw1[0])), len(str(kw0[0])))\n\n\ndef _kw(keywords):\n    \"\"\"Turn list of keywords into dictionary.\"\"\"\n    r = {}\n    for k, v in keywords:\n        r[k] = v\n    return r\n\n\ndef _sort_kw_matches(skw_matches, limit=0):\n    \"\"\"Return a resized version of keywords to the given length.\"\"\"\n    sorted_keywords = list(skw_matches.items())\n    sorted_keywords.sort(_skw_matches_comparator)\n    return limit and sorted_keywords[:limit] or sorted_keywords\n\n\ndef _get_partial_text(fulltext):\n    \"\"\"\n    Return a short version of the fulltext used with the partial matching mode.\n\n    The version is composed of 20% in the beginning and 20% in the middle of the\n    text.\"\"\"\n    length = len(fulltext)\n\n    get_index = lambda x: int(float(x) / 100 * length)\n\n    partial_text = [fulltext[get_index(start):get_index(end)]\n                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]\n\n    return \"\\n\".join(partial_text)\n\n\ndef save_keywords(filename, xml):\n    tmp_dir = os.path.dirname(filename)\n    if not os.path.isdir(tmp_dir):\n        os.mkdir(tmp_dir)\n\n    file_desc = open(filename, \"w\")\n    file_desc.write(xml)\n    file_desc.close()\n\n\ndef get_tmp_file(recid):\n    tmp_directory = \"%s/bibclassify\" % bconfig.CFG_TMPDIR\n    if not os.path.isdir(tmp_directory):\n        os.mkdir(tmp_directory)\n    filename = \"bibclassify_%s.xml\" % recid\n    abs_path = os.path.join(tmp_directory, filename)\n    return abs_path\n\n\ndef _parse_marc_code(field):\n    \"\"\"Parse marc field and return default indicators if not filled in.\"\"\"\n    field = str(field)\n    if len(field) < 4:\n        raise Exception('Wrong field code: %s' % field)\n    else:\n        field += '__'\n    tag = field[0:3]\n    ind1 = field[3].replace('_', '')\n    ind2 = field[4].replace('_', '')\n    return tag, ind1, ind2\n\n\nif __name__ == \"__main__\":\n    log.error(\"Please use bibclassify_cli from now on.\")\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/nakajima-hiro/invenio/blob/d64fffea5a05775ba2db65cba5408c2e9635e354",
        "file_path": "/invenio/legacy/bibclassify/ontology_reader.py",
        "source": "# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015 CERN.\n#\n# Invenio is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\n\"\"\"BibClassify ontology reader.\n\nThe ontology reader reads currently either a RDF/SKOS taxonomy or a\nsimple controlled vocabulary file (1 word per line). The first role of\nthis module is to manage the cached version of the ontology file. The\nsecond role is to hold all methods responsible for the creation of\nregular expressions. These methods are grammatically related as we take\ncare of different forms of the same words.  The grammatical rules can be\nconfigured via the configuration file.\n\nThe main method from this module is get_regular_expressions.\n\"\"\"\n\nfrom __future__ import print_function\n\nfrom datetime import datetime, timedelta\nfrom six import iteritems\nfrom six.moves import cPickle\n\nimport os\nimport re\nimport sys\nimport tempfile\nimport time\nimport urllib2\nimport traceback\nimport xml.sax\nimport thread\nimport rdflib\n\nfrom invenio.legacy.bibclassify import config as bconfig\nfrom invenio.modules.classifier.errors import TaxonomyError\n\nlog = bconfig.get_logger(\"bibclassify.ontology_reader\")\nfrom invenio import config\n\nfrom invenio.modules.classifier.registry import taxonomies\n\n# only if not running in a stanalone mode\nif bconfig.STANDALONE:\n    dbquery = None\n    from urllib2 import urlopen\nelse:\n    from invenio.legacy import dbquery\n    from invenio.utils.url import make_invenio_opener\n\n    urlopen = make_invenio_opener('BibClassify').open\n\n_contains_digit = re.compile(\"\\d\")\n_starts_with_non = re.compile(\"(?i)^non[a-z]\")\n_starts_with_anti = re.compile(\"(?i)^anti[a-z]\")\n_split_by_punctuation = re.compile(\"(\\W+)\")\n\n_CACHE = {}\n\n\ndef get_cache(taxonomy_id):\n    \"\"\"Return thread-safe cache for the given taxonomy id.\n\n    :param taxonomy_id: identifier of the taxonomy\n    :type taxonomy_id: str\n\n    :return: dictionary object (empty if no taxonomy_id\n        is found), you must not change anything inside it.\n        Create a new dictionary and use set_cache if you want\n        to update the cache!\n    \"\"\"\n    # Because of a standalone mode, we don't use the\n    # invenio.data_cacher.DataCacher, but it has no effect\n    # on proper functionality.\n\n    if taxonomy_id in _CACHE:\n        ctime, taxonomy = _CACHE[taxonomy_id]\n\n        # check it is fresh version\n        onto_name, onto_path, onto_url = _get_ontology(taxonomy_id)\n        cache_path = _get_cache_path(onto_name)\n\n        # if source exists and is newer than the cache hold in memory\n        if os.path.isfile(onto_path) and os.path.getmtime(onto_path) > ctime:\n            log.info('Forcing taxonomy rebuild as cached'\n                     ' version is newer/updated.')\n            return {}  # force cache rebuild\n\n        # if cache exists and is newer than the cache hold in memory\n        if os.path.isfile(cache_path) and os.path.getmtime(cache_path) > ctime:\n            log.info('Forcing taxonomy rebuild as source'\n                     ' file is newer/updated.')\n            return {}\n        log.info('Taxonomy retrieved from cache')\n        return taxonomy\n    return {}\n\n\ndef set_cache(taxonomy_id, contents):\n    \"\"\"Update cache in a thread-safe manner.\"\"\"\n    lock = thread.allocate_lock()\n    lock.acquire()\n    try:\n        _CACHE[taxonomy_id] = (time.time(), contents)\n    finally:\n        lock.release()\n\n\ndef get_regular_expressions(taxonomy_name, rebuild=False, no_cache=False):\n    \"\"\"Return a list of patterns compiled from the RDF/SKOS ontology.\n\n    Uses cache if it exists and if the taxonomy hasn't changed.\n    \"\"\"\n    # Translate the ontology name into a local path. Check if the name\n    # relates to an existing ontology.\n    onto_name, onto_path, onto_url = _get_ontology(taxonomy_name)\n    if not onto_path:\n        raise TaxonomyError(\"Unable to locate the taxonomy: '%s'.\"\n                            % taxonomy_name)\n\n    cache_path = _get_cache_path(onto_name)\n    log.debug('Taxonomy discovered, now we load it '\n              '(from cache: %s, onto_path: %s, cache_path: %s)'\n              % (not no_cache, onto_path, cache_path))\n\n    if os.access(cache_path, os.R_OK):\n        if os.access(onto_path, os.R_OK):\n            if rebuild or no_cache:\n                log.debug(\"Cache generation was manually forced.\")\n                return _build_cache(onto_path, skip_cache=no_cache)\n        else:\n            # ontology file not found. Use the cache instead.\n            log.warning(\"The ontology couldn't be located. However \"\n                        \"a cached version of it is available. Using it as a \"\n                        \"reference.\")\n            return _get_cache(cache_path, source_file=onto_path)\n\n        if (os.path.getmtime(cache_path) >\n                os.path.getmtime(onto_path)):\n            # Cache is more recent than the ontology: use cache.\n            log.debug(\"Normal situation, cache is older than ontology,\"\n                      \" so we load it from cache\")\n            return _get_cache(cache_path, source_file=onto_path)\n        else:\n            # Ontology is more recent than the cache: rebuild cache.\n            log.warning(\"Cache '%s' is older than '%s'. \"\n                        \"We will rebuild the cache\" %\n                        (cache_path, onto_path))\n            return _build_cache(onto_path, skip_cache=no_cache)\n\n    elif os.access(onto_path, os.R_OK):\n        if not no_cache and\\\n                os.path.exists(cache_path) and\\\n                not os.access(cache_path, os.W_OK):\n            raise TaxonomyError('We cannot read/write into: %s. '\n                                'Aborting!' % cache_path)\n        elif not no_cache and os.path.exists(cache_path):\n            log.warning('Cache %s exists, but is not readable!' % cache_path)\n        log.info(\"Cache not available. Building it now: %s\" % onto_path)\n        return _build_cache(onto_path, skip_cache=no_cache)\n\n    else:\n        raise TaxonomyError(\"We miss both source and cache\"\n                            \" of the taxonomy: %s\" % taxonomy_name)\n\n\ndef _get_remote_ontology(onto_url, time_difference=None):\n    \"\"\"Check if the online ontology is more recent than the local ontology.\n\n    If yes, try to download and store it in Invenio's cache directory.\n\n    Return a boolean describing the success of the operation.\n\n    :return: path to the downloaded ontology.\n    \"\"\"\n    if onto_url is None:\n        return False\n\n    dl_dir = ((config.CFG_CACHEDIR or tempfile.gettempdir()) + os.sep +\n              \"bibclassify\" + os.sep)\n    if not os.path.exists(dl_dir):\n        os.mkdir(dl_dir)\n\n    local_file = dl_dir + os.path.basename(onto_url)\n    remote_modif_time = _get_last_modification_date(onto_url)\n    try:\n        local_modif_seconds = os.path.getmtime(local_file)\n    except OSError:\n        # The local file does not exist. Download the ontology.\n        download = True\n        log.info(\"The local ontology could not be found.\")\n    else:\n        local_modif_time = datetime(*time.gmtime(local_modif_seconds)[0:6])\n        # Let's set a time delta of 1 hour and 10 minutes.\n        time_difference = time_difference or timedelta(hours=1, minutes=10)\n        download = remote_modif_time > local_modif_time + time_difference\n        if download:\n            log.info(\"The remote ontology '%s' is more recent \"\n                     \"than the local ontology.\" % onto_url)\n\n    if download:\n        if not _download_ontology(onto_url, local_file):\n            log.warning(\"Error downloading the ontology from: %s\" % onto_url)\n\n    return local_file\n\n\ndef _get_ontology(ontology):\n    \"\"\"Return the (name, path, url) to the short ontology name.\n\n    :param ontology: name of the ontology or path to the file or url.\n    \"\"\"\n    onto_name = onto_path = onto_url = None\n\n    # first assume we got the path to the file\n    if os.path.exists(ontology):\n        onto_name = os.path.split(os.path.abspath(ontology))[1]\n        onto_path = os.path.abspath(ontology)\n        onto_url = \"\"\n    else:\n        # if not, try to find it in a known locations\n        discovered_file = _discover_ontology(ontology)\n        if discovered_file:\n            onto_name = os.path.split(discovered_file)[1]\n            onto_path = discovered_file\n            # i know, this sucks\n            x = ontology.lower()\n            if \"http:\" in x or \"https:\" in x or \"ftp:\" in x or \"file:\" in x:\n                onto_url = ontology\n            else:\n                onto_url = \"\"\n        else:\n            # not found, look into a database\n            # (it is last because when bibclassify\n            # runs in a standalone mode,\n            # it has no database - [rca, old-heritage]\n            if not bconfig.STANDALONE:\n                result = dbquery.run_sql(\"SELECT name, location from clsMETHOD WHERE name LIKE %s\",\n                                         ('%' + ontology + '%',))\n                for onto_short_name, url in result:\n                    onto_name = onto_short_name\n                    onto_path = _get_remote_ontology(url)\n                    onto_url = url\n\n    return (onto_name, onto_path, onto_url)\n\n\ndef _discover_ontology(ontology_name):\n    \"\"\"Look for the file in a known places.\n\n    Inside invenio/etc/bibclassify and a few other places\n    like current directory.\n\n    :param ontology: name or path name or url\n    :type ontology: str\n\n    :return: absolute path of a file if found, or None\n    \"\"\"\n    last_part = os.path.split(os.path.abspath(ontology_name))[1]\n    if last_part in taxonomies:\n        return taxonomies.get(last_part)\n    elif last_part + \".rdf\" in taxonomies:\n        return taxonomies.get(last_part + \".rdf\")\n    else:\n        log.debug(\"No taxonomy with pattern '%s' found\" % ontology_name)\n\n    # LEGACY\n    possible_patterns = [last_part, last_part.lower()]\n    if not last_part.endswith('.rdf'):\n        possible_patterns.append(last_part + '.rdf')\n    places = [config.CFG_CACHEDIR,\n              config.CFG_ETCDIR,\n              os.path.join(config.CFG_CACHEDIR, \"bibclassify\"),\n              os.path.join(config.CFG_ETCDIR, \"bibclassify\"),\n              os.path.abspath('.'),\n              os.path.abspath(os.path.join(os.path.dirname(__file__),\n                                           \"../../../etc/bibclassify\")),\n              os.path.join(os.path.dirname(__file__), \"bibclassify\"),\n              config.CFG_WEBDIR]\n\n    log.debug(\"Searching for taxonomy using string: %s\" % last_part)\n    log.debug(\"Possible patterns: %s\" % possible_patterns)\n    for path in places:\n\n        try:\n            if os.path.isdir(path):\n                log.debug(\"Listing: %s\" % path)\n                for filename in os.listdir(path):\n                    #log.debug('Testing: %s' % filename)\n                    for pattern in possible_patterns:\n                        filename_lc = filename.lower()\n                        if pattern == filename_lc and\\\n                                os.path.exists(os.path.join(path, filename)):\n                            filepath = os.path.abspath(os.path.join(path,\n                                                                    filename))\n                            if (os.access(filepath, os.R_OK)):\n                                log.debug(\"Found taxonomy at: %s\" % filepath)\n                                return filepath\n                            else:\n                                log.warning('Found taxonony at: %s, but it is'\n                                            ' not readable. '\n                                            'Continue searching...'\n                                            % filepath)\n        except OSError, os_error_msg:\n            log.warning('OS Error when listing path '\n                        '\"%s\": %s' % (str(path), str(os_error_msg)))\n    log.debug(\"No taxonomy with pattern '%s' found\" % ontology_name)\n\n\nclass KeywordToken:\n\n    \"\"\"KeywordToken is a class used for the extracted keywords.\n\n    It can be initialized with values from RDF store or from\n    simple strings. Specialty of this class is that objects are\n    hashable by subject - so in the dictionary two objects with the\n    same subject appears as one -- :see: self.__hash__ and self.__cmp__.\n    \"\"\"\n\n    def __init__(self, subject, store=None, namespace=None, type='HEP'):\n        \"\"\"Initialize KeywordToken with a subject.\n\n        :param subject: string or RDF object\n        :param store: RDF graph object\n                      (will be used to get info about the subject)\n        :param namespace: RDF namespace object, used together with store\n        :param type: type of this keyword.\n        \"\"\"\n        self.id = subject\n        self.type = type\n        self.short_id = subject\n        self.concept = \"\"\n        self.regex = []\n        self.nostandalone = False\n        self.spires = False\n        self.fieldcodes = []\n        self.compositeof = []\n        self.core = False\n        # True means composite keyword\n        self._composite = '#Composite' in subject\n        self.__hash = None\n\n        # the tokens are coming possibly from a normal text file\n        if store is None:\n            subject = subject.strip()\n            self.concept = subject\n            self.regex = _get_searchable_regex(basic=[subject])\n            self.nostandalone = False\n            self.fieldcodes = []\n            self.core = False\n            if subject.find(' ') > -1:\n                self._composite = True\n\n        # definitions from rdf\n        else:\n            self.short_id = self.short_id.split('#')[-1]\n\n            # find alternate names for this label\n            basic_labels = []\n\n            # turn those patterns into regexes only for simple keywords\n            if self._composite is False:\n                try:\n                    for label in store.objects(subject,\n                                               namespace[\"prefLabel\"]):\n                        # XXX shall i make it unicode?\n                        basic_labels.append(str(label))\n                except TypeError:\n                    pass\n                self.concept = basic_labels[0]\n            else:\n                try:\n                    self.concept = str(store.value(subject,\n                                                   namespace[\"prefLabel\"],\n                                                   any=True))\n                except KeyError:\n                    log.warning(\"Keyword with subject %s has no prefLabel.\"\n                                \" We use raw name\" %\n                                self.short_id)\n                    self.concept = self.short_id\n\n            # this is common both to composite and simple keywords\n            try:\n                for label in store.objects(subject, namespace[\"altLabel\"]):\n                    basic_labels.append(str(label))\n            except TypeError:\n                pass\n\n            # hidden labels are special (possibly regex) codes\n            hidden_labels = []\n            try:\n                for label in store.objects(subject, namespace[\"hiddenLabel\"]):\n                    hidden_labels.append(unicode(label))\n            except TypeError:\n                pass\n\n            # compile regular expression that will identify this token\n            self.regex = _get_searchable_regex(basic_labels, hidden_labels)\n\n            try:\n                for note in map(lambda s: str(s).lower().strip(),\n                                store.objects(subject, namespace[\"note\"])):\n                    if note == 'core':\n                        self.core = True\n                    elif note in (\"nostandalone\", \"nonstandalone\"):\n                        self.nostandalone = True\n                    elif 'fc:' in note:\n                        self.fieldcodes.append(note[3:].strip())\n            except TypeError:\n                pass\n\n            # spiresLabel does not have multiple values\n            spires_label = store.value(subject, namespace[\"spiresLabel\"])\n            if spires_label:\n                self.spires = str(spires_label)\n\n        # important for comparisons\n        self.__hash = hash(self.short_id)\n\n        # extract composite parts ids\n        if store is not None and self.isComposite():\n            small_subject = self.id.split(\"#Composite.\")[-1]\n            component_positions = []\n            for label in store.objects(self.id, namespace[\"compositeOf\"]):\n                strlabel = str(label).split(\"#\")[-1]\n                component_name = label.split(\"#\")[-1]\n                component_positions.append((small_subject.find(component_name),\n                                            strlabel))\n            component_positions.sort()\n            if not component_positions:\n                log.error(\"Keyword is marked as composite, \"\n                          \"but no composite components refs found: %s\"\n                          % self.short_id)\n            else:\n                self.compositeof = map(lambda x: x[1], component_positions)\n\n    def refreshCompositeOf(self, single_keywords, composite_keywords,\n                           store=None, namespace=None):\n        \"\"\"Re-check sub-parts of this keyword.\n\n        This should be called after the whole RDF was processed, because\n        it is using a cache of single keywords and if that\n        one is incomplete, you will not identify all parts.\n        \"\"\"\n        def _get_ckw_components(new_vals, label):\n            if label in single_keywords:\n                new_vals.append(single_keywords[label])\n            elif ('Composite.%s' % label) in composite_keywords:\n                for l in composite_keywords['Composite.%s' % label].compositeof:\n                    _get_ckw_components(new_vals, l)\n            elif label in composite_keywords:\n                for l in composite_keywords[label].compositeof:\n                    _get_ckw_components(new_vals, l)\n            else:\n                # One single or composite keyword is missing from the taxonomy.\n                # This is due to an error in the taxonomy description.\n                message = \"The composite term \\\"%s\\\"\"\\\n                          \" should be made of single keywords,\"\\\n                          \" but at least one is missing.\" % self.id\n                if store is not None:\n                    message += \"Needed components: %s\"\\\n                               % list(store.objects(self.id,\n                                      namespace[\"compositeOf\"]))\n                message += \" Missing is: %s\" % label\n                raise TaxonomyError(message)\n\n        if self.compositeof:\n            new_vals = []\n            try:\n                for label in self.compositeof:\n                    _get_ckw_components(new_vals, label)\n                self.compositeof = new_vals\n            except TaxonomyError:\n                # the composites will be empty\n                # (better than to have confusing, partial matches)\n                self.compositeof = []\n                log.error(\n                    'We reset this composite keyword, so that it does not match anything. Please fix the taxonomy.')\n\n    def isComposite(self):\n        \"\"\"Return value of _composite.\"\"\"\n        return self._composite\n\n    def getComponents(self):\n        \"\"\"Return value of compositeof.\"\"\"\n        return self.compositeof\n\n    def getType(self):\n        \"\"\"Return value of type.\"\"\"\n        return self.type\n\n    def setType(self, value):\n        \"\"\"Set value of value.\"\"\"\n        self.type = value\n\n    def __hash__(self):\n        \"\"\"Return _hash.\n\n        This might change in the future but for the moment we want to\n        think that if the concept is the same, then it is the same\n        keyword - this sucks, but it is sort of how it is necessary\n        to use now.\n        \"\"\"\n        return self.__hash\n\n    def __cmp__(self, other):\n        \"\"\"Compare objects using _hash.\"\"\"\n        if self.__hash < other.__hash__():\n            return -1\n        elif self.__hash == other.__hash__():\n            return 0\n        else:\n            return 1\n\n    def __str__(self, spires=False):\n        \"\"\"Return the best output for the keyword.\"\"\"\n        if spires:\n            if self.spires:\n                return self.spires\n            elif self._composite:\n                return self.concept.replace(':', ',')\n            # default action\n        return self.concept\n\n    def output(self, spires=False):\n        \"\"\"Return string representation with spires value.\"\"\"\n        return self.__str__(spires=spires)\n\n    def __repr__(self):\n        \"\"\"Class representation.\"\"\"\n        return \"<KeywordToken: %s>\" % self.short_id\n\n\ndef _build_cache(source_file, skip_cache=False):\n    \"\"\"Build the cached data.\n\n    Either by parsing the RDF taxonomy file or a vocabulary file.\n\n    :param source_file: source file of the taxonomy, RDF file\n    :param skip_cache: if True, build cache will not be\n        saved (pickled) - it is saved as <source_file.db>\n    \"\"\"\n    store = rdflib.ConjunctiveGraph()\n\n    if skip_cache:\n        log.info(\"You requested not to save the cache to disk.\")\n    else:\n        cache_path = _get_cache_path(source_file)\n        cache_dir = os.path.dirname(cache_path)\n        # Make sure we have a cache_dir readable and writable.\n        try:\n            os.makedirs(cache_dir)\n        except:\n            pass\n        if os.access(cache_dir, os.R_OK):\n            if not os.access(cache_dir, os.W_OK):\n                raise TaxonomyError(\"Cache directory exists but is not\"\n                                    \" writable. Check your permissions\"\n                                    \" for: %s\" % cache_dir)\n        else:\n            raise TaxonomyError(\"Cache directory does not exist\"\n                                \" (and could not be created): %s\" % cache_dir)\n\n    timer_start = time.clock()\n\n    namespace = None\n    single_keywords, composite_keywords = {}, {}\n\n    try:\n        log.info(\"Building RDFLib's conjunctive graph from: %s\" % source_file)\n        try:\n            store.parse(source_file)\n        except urllib2.URLError:\n            if source_file[0] == '/':\n                store.parse(\"file://\" + source_file)\n            else:\n                store.parse(\"file:///\" + source_file)\n\n    except rdflib.exceptions.Error as e:\n        log.error(\"Serious error reading RDF file\")\n        log.error(e)\n        log.error(traceback.format_exc())\n        raise rdflib.exceptions.Error(e)\n\n    except (xml.sax.SAXParseException, ImportError) as e:\n        # File is not a RDF file. We assume it is a controlled vocabulary.\n        log.error(e)\n        log.warning(\"The ontology file is probably not a valid RDF file. \\\n            Assuming it is a controlled vocabulary file.\")\n\n        filestream = open(source_file, \"r\")\n        for line in filestream:\n            keyword = line.strip()\n            kt = KeywordToken(keyword)\n            single_keywords[kt.short_id] = kt\n        if not len(single_keywords):\n            raise TaxonomyError('The ontology file is not well formated')\n\n    else:  # ok, no exception happened\n        log.info(\"Now building cache of keywords\")\n        # File is a RDF file.\n        namespace = rdflib.Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n\n        single_count = 0\n        composite_count = 0\n\n        subject_objects = store.subject_objects(namespace[\"prefLabel\"])\n        for subject, pref_label in subject_objects:\n            kt = KeywordToken(subject, store=store, namespace=namespace)\n            if kt.isComposite():\n                composite_count += 1\n                composite_keywords[kt.short_id] = kt\n            else:\n                single_keywords[kt.short_id] = kt\n                single_count += 1\n\n    cached_data = {}\n    cached_data[\"single\"] = single_keywords\n    cached_data[\"composite\"] = composite_keywords\n    cached_data[\"creation_time\"] = time.gmtime()\n    cached_data[\"version_info\"] = {'rdflib': rdflib.__version__,\n                                   'bibclassify': bconfig.VERSION}\n    log.debug(\"Building taxonomy... %d terms built in %.1f sec.\" %\n              (len(single_keywords) + len(composite_keywords),\n               time.clock() - timer_start))\n\n    log.info(\"Total count of single keywords: %d \"\n             % len(single_keywords))\n    log.info(\"Total count of composite keywords: %d \"\n             % len(composite_keywords))\n\n    if not skip_cache:\n        cache_path = _get_cache_path(source_file)\n        cache_dir = os.path.dirname(cache_path)\n        log.debug(\"Writing the cache into: %s\" % cache_path)\n        # test again, it could have changed\n        if os.access(cache_dir, os.R_OK):\n            if os.access(cache_dir, os.W_OK):\n                # Serialize.\n                filestream = None\n                try:\n                    filestream = open(cache_path, \"wb\")\n                except IOError as msg:\n                    # Impossible to write the cache.\n                    log.error(\"Impossible to write cache to '%s'.\"\n                              % cache_path)\n                    log.error(msg)\n                else:\n                    log.debug(\"Writing cache to file %s\" % cache_path)\n                    cPickle.dump(cached_data, filestream, 1)\n                if filestream:\n                    filestream.close()\n\n            else:\n                raise TaxonomyError(\"Cache directory exists but is not \"\n                                    \"writable. Check your permissions \"\n                                    \"for: %s\" % cache_dir)\n        else:\n            raise TaxonomyError(\"Cache directory does not exist\"\n                                \" (and could not be created): %s\" % cache_dir)\n\n    # now when the whole taxonomy was parsed,\n    # find sub-components of the composite kws\n    # it is important to keep this call after the taxonomy was saved,\n    # because we don't  want to pickle regexes multiple times\n    # (as they are must be re-compiled at load time)\n    for kt in composite_keywords.values():\n        kt.refreshCompositeOf(single_keywords, composite_keywords,\n                              store=store, namespace=namespace)\n\n    # house-cleaning\n    if store:\n        store.close()\n\n    return (single_keywords, composite_keywords)\n\n\ndef _capitalize_first_letter(word):\n    \"\"\"Return a regex pattern with the first letter.\n\n    Accepts both lowercase and uppercase.\n    \"\"\"\n    if word[0].isalpha():\n        # These two cases are necessary in order to get a regex pattern\n        # starting with '[xX]' and not '[Xx]'. This allows to check for\n        # colliding regex afterwards.\n        if word[0].isupper():\n            return \"[\" + word[0].swapcase() + word[0] + \"]\" + word[1:]\n        else:\n            return \"[\" + word[0] + word[0].swapcase() + \"]\" + word[1:]\n    return word\n\n\ndef _convert_punctuation(punctuation, conversion_table):\n    \"\"\"Return a regular expression for a punctuation string.\"\"\"\n    if punctuation in conversion_table:\n        return conversion_table[punctuation]\n    return re.escape(punctuation)\n\n\ndef _convert_word(word):\n    \"\"\"Return the plural form of the word if it exists.\n\n    Otherwise return the word itself.\n    \"\"\"\n    out = None\n\n    # Acronyms.\n    if word.isupper():\n        out = word + \"s?\"\n    # Proper nouns or word with digits.\n    elif word.istitle():\n        out = word + \"('?s)?\"\n    elif _contains_digit.search(word):\n        out = word\n\n    if out is not None:\n        return out\n\n    # Words with non or anti prefixes.\n    if _starts_with_non.search(word):\n        word = \"non-?\" + _capitalize_first_letter(_convert_word(word[3:]))\n    elif _starts_with_anti.search(word):\n        word = \"anti-?\" + _capitalize_first_letter(_convert_word(word[4:]))\n\n    if out is not None:\n        return _capitalize_first_letter(out)\n\n    # A few invariable words.\n    if word in bconfig.CFG_BIBCLASSIFY_INVARIABLE_WORDS:\n        return _capitalize_first_letter(word)\n\n    # Some exceptions that would not produce good results with the set of\n    # general_regular_expressions.\n    regexes = bconfig.CFG_BIBCLASSIFY_EXCEPTIONS\n    if word in regexes:\n        return _capitalize_first_letter(regexes[word])\n\n    regexes = bconfig.CFG_BIBCLASSIFY_UNCHANGE_REGULAR_EXPRESSIONS\n    for regex in regexes:\n        if regex.search(word) is not None:\n            return _capitalize_first_letter(word)\n\n    regexes = bconfig.CFG_BIBCLASSIFY_GENERAL_REGULAR_EXPRESSIONS\n    for regex, replacement in regexes:\n        stemmed = regex.sub(replacement, word)\n        if stemmed != word:\n            return _capitalize_first_letter(stemmed)\n\n    return _capitalize_first_letter(word + \"s?\")\n\n\ndef _get_cache(cache_file, source_file=None):\n    \"\"\"Get cached taxonomy using the cPickle module.\n\n    No check is done at that stage.\n\n    :param cache_file: full path to the file holding pickled data\n    :param source_file: if we discover the cache is obsolete, we\n        will build a new cache, therefore we need the source path\n        of the cache\n    :return: (single_keywords, composite_keywords).\n    \"\"\"\n    timer_start = time.clock()\n\n    filestream = open(cache_file, \"rb\")\n    try:\n        cached_data = cPickle.load(filestream)\n        version_info = cached_data['version_info']\n        if version_info['rdflib'] != rdflib.__version__\\\n                or version_info['bibclassify'] != bconfig.VERSION:\n            raise KeyError\n    except (cPickle.UnpicklingError, ImportError,\n            AttributeError, DeprecationWarning, EOFError):\n        log.warning(\"The existing cache in %s is not readable. \"\n                    \"Removing and rebuilding it.\" % cache_file)\n        filestream.close()\n        os.remove(cache_file)\n        return _build_cache(source_file)\n    except KeyError:\n        log.warning(\"The existing cache %s is not up-to-date. \"\n                    \"Removing and rebuilding it.\" % cache_file)\n        filestream.close()\n        os.remove(cache_file)\n        if source_file and os.path.exists(source_file):\n            return _build_cache(source_file)\n        else:\n            log.error(\"The cache contains obsolete data (and it was deleted), \"\n                      \"however I can't build a new cache, the source does not \"\n                      \"exist or is inaccessible! - %s\" % source_file)\n    filestream.close()\n\n    single_keywords = cached_data[\"single\"]\n    composite_keywords = cached_data[\"composite\"]\n\n    # the cache contains only keys of the composite keywords, not the objects\n    # so now let's resolve them into objects\n    for kw in composite_keywords.values():\n        kw.refreshCompositeOf(single_keywords, composite_keywords)\n\n    log.debug(\"Retrieved taxonomy from cache %s created on %s\" %\n              (cache_file, time.asctime(cached_data[\"creation_time\"])))\n\n    log.debug(\"%d terms read in %.1f sec.\" %\n              (len(single_keywords) + len(composite_keywords),\n               time.clock() - timer_start))\n\n    return (single_keywords, composite_keywords)\n\n\ndef _get_cache_path(source_file):\n    \"\"\"Return the path where the cache should be written/located.\n\n    :param onto_name: name of the ontology or the full path\n    :return: string, abs path to the cache file in the tmpdir/bibclassify\n    \"\"\"\n    local_name = os.path.basename(source_file)\n    cache_name = local_name + \".db\"\n    cache_dir = os.path.join(config.CFG_CACHEDIR, \"bibclassify\")\n\n    if not os.path.isdir(cache_dir):\n        os.makedirs(cache_dir)\n\n    return os.path.abspath(os.path.join(cache_dir, cache_name))\n\n\ndef _get_last_modification_date(url):\n    \"\"\"Get the last modification date of the ontology.\"\"\"\n    request = urllib2.Request(url)\n    request.get_method = lambda: \"HEAD\"\n    http_file = urlopen(request)\n    date_string = http_file.headers[\"last-modified\"]\n    parsed = time.strptime(date_string, \"%a, %d %b %Y %H:%M:%S %Z\")\n    return datetime(*(parsed)[0:6])\n\n\ndef _download_ontology(url, local_file):\n    \"\"\"Download the ontology and stores it in CFG_CACHEDIR.\"\"\"\n    log.debug(\"Copying remote ontology '%s' to file '%s'.\" % (url,\n                                                              local_file))\n    try:\n        url_desc = urlopen(url)\n        file_desc = open(local_file, 'w')\n        file_desc.write(url_desc.read())\n        file_desc.close()\n    except IOError as e:\n        print(e)\n        return False\n    except:\n        log.warning(\"Unable to download the ontology. '%s'\" %\n                    sys.exc_info()[0])\n        return False\n    else:\n        log.debug(\"Done copying.\")\n        return True\n\n\ndef _get_searchable_regex(basic=None, hidden=None):\n    \"\"\"Return the searchable regular expressions for the single keyword.\"\"\"\n    # Hidden labels are used to store regular expressions.\n    basic = basic or []\n    hidden = hidden or []\n\n    hidden_regex_dict = {}\n    for hidden_label in hidden:\n        if _is_regex(hidden_label):\n            hidden_regex_dict[hidden_label] = \\\n                re.compile(\n                    bconfig.CFG_BIBCLASSIFY_WORD_WRAP % hidden_label[1:-1]\n                )\n        else:\n            pattern = _get_regex_pattern(hidden_label)\n            hidden_regex_dict[hidden_label] = re.compile(\n                bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern\n            )\n\n    # We check if the basic label (preferred or alternative) is matched\n    # by a hidden label regex. If yes, discard it.\n    regex_dict = {}\n    # Create regex for plural forms and add them to the hidden labels.\n    for label in basic:\n        pattern = _get_regex_pattern(label)\n        regex_dict[label] = re.compile(\n            bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern\n        )\n\n    # Merge both dictionaries.\n    regex_dict.update(hidden_regex_dict)\n\n    return regex_dict.values()\n\n\ndef _get_regex_pattern(label):\n    \"\"\"Return a regular expression of the label.\n\n    This takes care of plural and different kinds of separators.\n    \"\"\"\n    parts = _split_by_punctuation.split(label)\n\n    for index, part in enumerate(parts):\n        if index % 2 == 0:\n            # Word\n            if not parts[index].isdigit() and len(parts[index]) > 1:\n                parts[index] = _convert_word(parts[index])\n        else:\n            # Punctuation\n            if not parts[index + 1]:\n                # The separator is not followed by another word. Treat\n                # it as a symbol.\n                parts[index] = _convert_punctuation(\n                    parts[index],\n                    bconfig.CFG_BIBCLASSIFY_SYMBOLS\n                )\n            else:\n                parts[index] = _convert_punctuation(\n                    parts[index],\n                    bconfig.CFG_BIBCLASSIFY_SEPARATORS\n                )\n\n    return \"\".join(parts)\n\n\ndef _is_regex(string):\n    \"\"\"Check if a concept is a regular expression.\"\"\"\n    return string[0] == \"/\" and string[-1] == \"/\"\n\n\ndef check_taxonomy(taxonomy):\n    \"\"\"Check the consistency of the taxonomy.\n\n    Outputs a list of errors and warnings.\n    \"\"\"\n    log.info(\"Building graph with Python RDFLib version %s\" %\n             rdflib.__version__)\n\n    store = rdflib.ConjunctiveGraph()\n\n    try:\n        store.parse(taxonomy)\n    except:\n        log.error(\"The taxonomy is not a valid RDF file. Are you \"\n                  \"trying to check a controlled vocabulary?\")\n        raise TaxonomyError('Error in RDF file')\n\n    log.info(\"Graph was successfully built.\")\n\n    prefLabel = \"prefLabel\"\n    hiddenLabel = \"hiddenLabel\"\n    altLabel = \"altLabel\"\n    composite = \"composite\"\n    compositeOf = \"compositeOf\"\n    note = \"note\"\n\n    both_skw_and_ckw = []\n\n    # Build a dictionary we will reason on later.\n    uniq_subjects = {}\n    for subject in store.subjects():\n        uniq_subjects[subject] = None\n\n    subjects = {}\n    for subject in uniq_subjects:\n        strsubject = str(subject).split(\"#Composite.\")[-1]\n        strsubject = strsubject.split(\"#\")[-1]\n        if (strsubject == \"http://cern.ch/thesauri/HEPontology.rdf\" or\n           strsubject == \"compositeOf\"):\n            continue\n        components = {}\n        for predicate, value in store.predicate_objects(subject):\n            strpredicate = str(predicate).split(\"#\")[-1]\n            strobject = str(value).split(\"#Composite.\")[-1]\n            strobject = strobject.split(\"#\")[-1]\n            components.setdefault(strpredicate, []).append(strobject)\n        if strsubject in subjects:\n            both_skw_and_ckw.append(strsubject)\n        else:\n            subjects[strsubject] = components\n\n    log.info(\"Taxonomy contains %s concepts.\" % len(subjects))\n\n    no_prefLabel = []\n    multiple_prefLabels = []\n    bad_notes = []\n    # Subjects with no composite or compositeOf predicate\n    lonely = []\n    both_composites = []\n    bad_hidden_labels = {}\n    bad_alt_labels = {}\n    # Problems with composite keywords\n    composite_problem1 = []\n    composite_problem2 = []\n    composite_problem3 = []\n    composite_problem4 = {}\n    composite_problem5 = []\n    composite_problem6 = []\n\n    stemming_collisions = []\n    interconcept_collisions = {}\n\n    for subject, predicates in iteritems(subjects):\n        # No prefLabel or multiple prefLabels\n        try:\n            if len(predicates[prefLabel]) > 1:\n                multiple_prefLabels.append(subject)\n        except KeyError:\n            no_prefLabel.append(subject)\n\n        # Lonely and both composites.\n        if composite not in predicates and compositeOf not in predicates:\n            lonely.append(subject)\n        elif composite in predicates and compositeOf in predicates:\n            both_composites.append(subject)\n\n        # Multiple or bad notes\n        if note in predicates:\n            bad_notes += [(subject, n) for n in predicates[note]\n                          if n not in ('nostandalone', 'core')]\n\n        # Bad hidden labels\n        if hiddenLabel in predicates:\n            for lbl in predicates[hiddenLabel]:\n                if lbl.startswith(\"/\") ^ lbl.endswith(\"/\"):\n                    bad_hidden_labels.setdefault(subject, []).append(lbl)\n\n        # Bad alt labels\n        if altLabel in predicates:\n            for lbl in predicates[altLabel]:\n                if len(re.findall(\"/\", lbl)) >= 2 or \":\" in lbl:\n                    bad_alt_labels.setdefault(subject, []).append(lbl)\n\n        # Check composite\n        if composite in predicates:\n            for ckw in predicates[composite]:\n                if ckw in subjects:\n                    if compositeOf in subjects[ckw]:\n                        if subject not in subjects[ckw][compositeOf]:\n                            composite_problem3.append((subject, ckw))\n                    else:\n                        if ckw not in both_skw_and_ckw:\n                            composite_problem2.append((subject, ckw))\n                else:\n                    composite_problem1.append((subject, ckw))\n\n        # Check compositeOf\n        if compositeOf in predicates:\n            for skw in predicates[compositeOf]:\n                if skw in subjects:\n                    if composite in subjects[skw]:\n                        if subject not in subjects[skw][composite]:\n                            composite_problem6.append((subject, skw))\n                    else:\n                        if skw not in both_skw_and_ckw:\n                            composite_problem5.append((subject, skw))\n                else:\n                    composite_problem4.setdefault(skw, []).append(subject)\n\n        # Check for stemmed labels\n        if compositeOf in predicates:\n            labels = (altLabel, hiddenLabel)\n        else:\n            labels = (prefLabel, altLabel, hiddenLabel)\n\n        patterns = {}\n        for label in [lbl for lbl in labels if lbl in predicates]:\n            for expression in [expr for expr in predicates[label]\n                               if not _is_regex(expr)]:\n                pattern = _get_regex_pattern(expression)\n                interconcept_collisions.setdefault(pattern, []).\\\n                    append((subject, label))\n                if pattern in patterns:\n                    stemming_collisions.append(\n                        (subject,\n                         patterns[pattern],\n                         (label, expression)\n                         )\n                    )\n                else:\n                    patterns[pattern] = (label, expression)\n\n    print(\"\\n==== ERRORS ====\")\n\n    if no_prefLabel:\n        print(\"\\nConcepts with no prefLabel: %d\" % len(no_prefLabel))\n        print(\"\\n\".join([\"   %s\" % subj for subj in no_prefLabel]))\n    if multiple_prefLabels:\n        print((\"\\nConcepts with multiple prefLabels: %d\" %\n               len(multiple_prefLabels)))\n        print(\"\\n\".join([\"   %s\" % subj for subj in multiple_prefLabels]))\n    if both_composites:\n        print((\"\\nConcepts with both composite properties: %d\" %\n               len(both_composites)))\n        print(\"\\n\".join([\"   %s\" % subj for subj in both_composites]))\n    if bad_hidden_labels:\n        print(\"\\nConcepts with bad hidden labels: %d\" % len(bad_hidden_labels))\n        for kw, lbls in iteritems(bad_hidden_labels):\n            print(\"   %s:\" % kw)\n            print(\"\\n\".join([\"      '%s'\" % lbl for lbl in lbls]))\n    if bad_alt_labels:\n        print(\"\\nConcepts with bad alt labels: %d\" % len(bad_alt_labels))\n        for kw, lbls in iteritems(bad_alt_labels):\n            print(\"   %s:\" % kw)\n            print(\"\\n\".join([\"      '%s'\" % lbl for lbl in lbls]))\n    if both_skw_and_ckw:\n        print((\"\\nKeywords that are both skw and ckw: %d\" %\n               len(both_skw_and_ckw)))\n        print(\"\\n\".join([\"   %s\" % subj for subj in both_skw_and_ckw]))\n\n    print()\n\n    if composite_problem1:\n        print(\"\\n\".join([\"SKW '%s' references an unexisting CKW '%s'.\" %\n                         (skw, ckw) for skw, ckw in composite_problem1]))\n    if composite_problem2:\n        print(\"\\n\".join([\"SKW '%s' references a SKW '%s'.\" %\n                         (skw, ckw) for skw, ckw in composite_problem2]))\n    if composite_problem3:\n        print(\"\\n\".join([\"SKW '%s' is not composite of CKW '%s'.\" %\n                         (skw, ckw) for skw, ckw in composite_problem3]))\n    if composite_problem4:\n        for skw, ckws in iteritems(composite_problem4):\n            print(\"SKW '%s' does not exist but is \" \"referenced by:\" % skw)\n            print(\"\\n\".join([\"    %s\" % ckw for ckw in ckws]))\n    if composite_problem5:\n        print(\"\\n\".join([\"CKW '%s' references a CKW '%s'.\" % kw\n                         for kw in composite_problem5]))\n    if composite_problem6:\n        print(\"\\n\".join([\"CKW '%s' is not composed by SKW '%s'.\" % kw\n                         for kw in composite_problem6]))\n\n    print(\"\\n==== WARNINGS ====\")\n\n    if bad_notes:\n        print((\"\\nConcepts with bad notes: %d\" % len(bad_notes)))\n        print(\"\\n\".join([\"   '%s': '%s'\" % _note for _note in bad_notes]))\n    if stemming_collisions:\n        print(\"\\nFollowing keywords have unnecessary labels that have \"\n              \"already been generated by BibClassify.\")\n        for subj in stemming_collisions:\n            print(\"   %s:\\n     %s\\n     and %s\" % subj)\n\n    print(\"\\nFinished.\")\n    sys.exit(0)\n\n\ndef test_cache(taxonomy_name='HEP', rebuild_cache=False, no_cache=False):\n    \"\"\"Test the cache lookup.\"\"\"\n    cache = get_cache(taxonomy_name)\n    if not cache:\n        set_cache(taxonomy_name, get_regular_expressions(taxonomy_name,\n                                                         rebuild=rebuild_cache,\n                                                         no_cache=no_cache))\n        cache = get_cache(taxonomy_name)\n    return (thread.get_ident(), cache)\n\n\nlog.info('Loaded ontology reader')\n\nif __name__ == '__main__':\n    test_cache()\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/nakajima-hiro/invenio/blob/d64fffea5a05775ba2db65cba5408c2e9635e354",
        "file_path": "/invenio/legacy/bibclassify/text_extractor.py",
        "source": "# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014 CERN.\n#\n# Invenio is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\n\"\"\"\nBibClassify text extractor.\n\nThis module provides method to extract the fulltext from local or remote\ndocuments. Currently 2 formats of documents are supported: PDF and text\ndocuments.\n\n2 methods provide the functionality of the module: text_lines_from_local_file\nand text_lines_from_url.\n\nThis module also provides the utility 'is_pdf' that uses GNU file in order to\ndetermine if a local file is a PDF file.\n\nThis module is STANDALONE safe\n\"\"\"\n\nimport os\nimport re\nimport tempfile\nimport urllib2\nfrom invenio.legacy.bibclassify import config as bconfig\n\nif bconfig.STANDALONE:\n    from urllib2 import urlopen\nelse:\n    from invenio.utils.url import make_invenio_opener\n\n    urlopen = make_invenio_opener('BibClassify').open\n\nlog = bconfig.get_logger(\"bibclassify.text_extractor\")\n\n_ONE_WORD = re.compile(\"[A-Za-z]{2,}\")\n\n\ndef is_pdf(document):\n    \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\"\n    if not executable_exists('pdftotext'):\n        log.warning(\"GNU file was not found on the system. \"\n                    \"Switching to a weak file extension test.\")\n        if document.lower().endswith(\".pdf\"):\n            return True\n        return False\n        # Tested with file version >= 4.10. First test is secure and works\n    # with file version 4.25. Second condition is tested for file\n    # version 4.10.\n    file_output = os.popen('file ' + re.escape(document)).read()\n    try:\n        filetype = file_output.split(\":\")[1]\n    except IndexError:\n        log.error(\"Your version of the 'file' utility seems to \"\n                  \"be unsupported. Please report this to cds.support@cern.ch.\")\n        raise Exception('Incompatible pdftotext')\n\n    pdf = filetype.find(\"PDF\") > -1\n    # This is how it should be done however this is incompatible with\n    # file version 4.10.\n    #os.popen('file -bi ' + document).read().find(\"application/pdf\")\n    return pdf\n\n\ndef text_lines_from_local_file(document, remote=False):\n    \"\"\"Returns the fulltext of the local file.\n    @var document: fullpath to the file that should be read\n    @var remote: boolean, if True does not count lines (gosh!)\n    @return: list of lines if st was read or an empty list\"\"\"\n\n    try:\n        if is_pdf(document):\n            if not executable_exists(\"pdftotext\"):\n                log.error(\"pdftotext is not available on the system.\")\n            cmd = \"pdftotext -q -enc UTF-8 %s -\" % re.escape(document)\n            filestream = os.popen(cmd)\n        else:\n            filestream = open(document, \"r\")\n    except IOError as ex1:\n        log.error(\"Unable to read from file %s. (%s)\" % (document, ex1.strerror))\n        return []\n\n    # FIXME - we assume it is utf-8 encoded / that is not good\n    lines = [line.decode(\"utf-8\", 'replace') for line in filestream]\n    filestream.close()\n\n    if not _is_english_text('\\n'.join(lines)):\n        log.warning(\"It seems the file '%s' is unvalid and doesn't \"\n                    \"contain text. Please communicate this file to the Invenio \"\n                    \"team.\" % document)\n\n    line_nb = len(lines)\n    word_nb = 0\n    for line in lines:\n        word_nb += len(re.findall(\"\\S+\", line))\n\n    # Discard lines that do not contain at least one word.\n    lines = [line for line in lines if _ONE_WORD.search(line) is not None]\n\n    if not remote:\n        log.info(\"Local file has %d lines and %d words.\" % (line_nb, word_nb))\n\n    return lines\n\n\ndef _is_english_text(text):\n    \"\"\"\n    Checks if a text is correct english.\n    Computes the number of words in the text and compares it to the\n    expected number of words (based on an average size of words of 5.1\n    letters).\n\n    @param text_lines: the text to analyze\n    @type text_lines:  string\n    @return:           True if the text is English, False otherwise\n    @rtype:            Boolean\n    \"\"\"\n    # Consider one word and one space.\n    avg_word_length = 2.55 + 1\n    expected_word_number = float(len(text)) / avg_word_length\n\n    words = [word\n             for word in re.split('\\W', text)\n             if word.isalpha()]\n\n    word_number = len(words)\n\n    return word_number > expected_word_number\n\n\ndef text_lines_from_url(url, user_agent=\"\"):\n    \"\"\"Returns the fulltext of the file found at the URL.\"\"\"\n    request = urllib2.Request(url)\n    if user_agent:\n        request.add_header(\"User-Agent\", user_agent)\n    try:\n        distant_stream = urlopen(request)\n        # Write the URL content to a temporary file.\n        local_file = tempfile.mkstemp(prefix=\"bibclassify.\")[1]\n        local_stream = open(local_file, \"w\")\n        local_stream.write(distant_stream.read())\n        local_stream.close()\n    except:\n        log.error(\"Unable to read from URL %s.\" % url)\n        return None\n    else:\n        # Read lines from the temporary file.\n        lines = text_lines_from_local_file(local_file, remote=True)\n        os.remove(local_file)\n\n        line_nb = len(lines)\n        word_nb = 0\n        for line in lines:\n            word_nb += len(re.findall(\"\\S+\", line))\n\n        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))\n\n        return lines\n\n\ndef executable_exists(executable):\n    \"\"\"Tests if an executable is available on the system.\"\"\"\n    for directory in os.getenv(\"PATH\").split(\":\"):\n        if os.path.exists(os.path.join(directory, executable)):\n            return True\n    return False\n\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/chokribr/invenio/blob/d64fffea5a05775ba2db65cba5408c2e9635e354",
        "file_path": "/invenio/legacy/bibclassify/engine.py",
        "source": "# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.\n#\n# Invenio is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\"\"\"\nBibClassify engine.\n\nThis module is the main module of BibClassify. its two main methods are\noutput_keywords_for_sources and get_keywords_from_text. The first one output\nkeywords for a list of sources (local files or URLs, PDF or text) while the\nsecond one outputs the keywords for text lines (which are obtained using the\nmodule bibclassify_text_normalizer).\n\nThis module also takes care of the different outputs (text, MARCXML or HTML).\nBut unfortunately there is a confusion between running in a standalone mode\nand producing output suitable for printing, and running in a web-based\nmode where the webtemplate is used. For the moment the pieces of the representation\ncode are left in this module.\n\"\"\"\n\nfrom __future__ import print_function\n\nimport os\nfrom six import iteritems\nimport config as bconfig\n\nfrom invenio.legacy.bibclassify import ontology_reader as reader\nimport text_extractor as extractor\nimport text_normalizer as normalizer\nimport keyword_analyzer as keyworder\nimport acronym_analyzer as acronymer\n\nfrom invenio.utils.url import make_user_agent_string\nfrom invenio.utils.text import encode_for_xml\n\nlog = bconfig.get_logger(\"bibclassify.engine\")\n\n# ---------------------------------------------------------------------\n#                          API\n# ---------------------------------------------------------------------\n\n\ndef output_keywords_for_sources(input_sources, taxonomy_name, output_mode=\"text\",\n                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,\n                                match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,\n                                api=False, **kwargs):\n    \"\"\"Output the keywords for each source in sources.\"\"\"\n\n    # Inner function which does the job and it would be too much work to\n    # refactor the call (and it must be outside the loop, before it did\n    # not process multiple files)\n    def process_lines():\n        if output_mode == \"text\":\n            print(\"Input file: %s\" % source)\n\n        output = get_keywords_from_text(\n            text_lines,\n            taxonomy_name,\n            output_mode=output_mode,\n            output_limit=output_limit,\n            spires=spires,\n            match_mode=match_mode,\n            no_cache=no_cache,\n            with_author_keywords=with_author_keywords,\n            rebuild_cache=rebuild_cache,\n            only_core_tags=only_core_tags,\n            extract_acronyms=extract_acronyms\n        )\n        if api:\n            return output\n        else:\n            if isinstance(output, dict):\n                for i in output:\n                    print(output[i])\n\n    # Get the fulltext for each source.\n    for entry in input_sources:\n        log.info(\"Trying to read input file %s.\" % entry)\n        text_lines = None\n        source = \"\"\n        if os.path.isdir(entry):\n            for filename in os.listdir(entry):\n                if filename.startswith('.'):\n                    continue\n                filename = os.path.join(entry, filename)\n                if os.path.isfile(filename):\n                    text_lines = extractor.text_lines_from_local_file(filename)\n                    if text_lines:\n                        source = filename\n                        process_lines()\n        elif os.path.isfile(entry):\n            text_lines = extractor.text_lines_from_local_file(entry)\n            if text_lines:\n                source = os.path.basename(entry)\n                process_lines()\n        else:\n            # Treat as a URL.\n            text_lines = extractor.text_lines_from_url(entry,\n                                                       user_agent=make_user_agent_string(\"BibClassify\"))\n            if text_lines:\n                source = entry.split(\"/\")[-1]\n                process_lines()\n\n\ndef get_keywords_from_local_file(local_file, taxonomy_name, output_mode=\"text\",\n                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,\n                                 match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,\n                                 **kwargs):\n    \"\"\"Outputs keywords reading a local file. Arguments and output are the same\n    as for :see: get_keywords_from_text() \"\"\"\n\n    log.info(\"Analyzing keywords for local file %s.\" % local_file)\n    text_lines = extractor.text_lines_from_local_file(local_file)\n\n    return get_keywords_from_text(text_lines,\n                                  taxonomy_name,\n                                  output_mode=output_mode,\n                                  output_limit=output_limit,\n                                  spires=spires,\n                                  match_mode=match_mode,\n                                  no_cache=no_cache,\n                                  with_author_keywords=with_author_keywords,\n                                  rebuild_cache=rebuild_cache,\n                                  only_core_tags=only_core_tags,\n                                  extract_acronyms=extract_acronyms)\n\n\ndef get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\",\n                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,\n                           spires=False, match_mode=\"full\", no_cache=False,\n                           with_author_keywords=False, rebuild_cache=False,\n                           only_core_tags=False, extract_acronyms=False,\n                           **kwargs):\n    \"\"\"Extract keywords from the list of strings\n\n    :param text_lines: list of strings (will be normalized before being\n        joined into one string)\n    :param taxonomy_name: string, name of the taxonomy_name\n    :param output_mode: string - text|html|marcxml|raw\n    :param output_limit: int\n    :param spires: boolean, if True marcxml output reflect spires codes.\n    :param match_mode: str - partial|full; in partial mode only\n        beginning of the fulltext is searched.\n    :param no_cache: boolean, means loaded definitions will not be saved.\n    :param with_author_keywords: boolean, extract keywords from the pdfs.\n    :param rebuild_cache: boolean\n    :param only_core_tags: boolean\n    :return: if output_mode=raw, it will return\n        (single_keywords, composite_keywords, author_keywords, acronyms)\n        for other output modes it returns formatted string\n    \"\"\"\n\n    cache = reader.get_cache(taxonomy_name)\n    if not cache:\n        reader.set_cache(taxonomy_name,\n                         reader.get_regular_expressions(taxonomy_name,\n                                                        rebuild=rebuild_cache,\n                                                        no_cache=no_cache))\n        cache = reader.get_cache(taxonomy_name)\n    _skw = cache[0]\n    _ckw = cache[1]\n    text_lines = normalizer.cut_references(text_lines)\n    fulltext = normalizer.normalize_fulltext(\"\\n\".join(text_lines))\n\n    if match_mode == \"partial\":\n        fulltext = _get_partial_text(fulltext)\n    author_keywords = None\n    if with_author_keywords:\n        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)\n    acronyms = {}\n    if extract_acronyms:\n        acronyms = extract_abbreviations(fulltext)\n\n    single_keywords = extract_single_keywords(_skw, fulltext)\n    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)\n\n    if only_core_tags:\n        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))\n        composite_keywords = _filter_core_keywors(composite_keywords)\n    else:\n        # Filter out the \"nonstandalone\" keywords\n        single_keywords = clean_before_output(single_keywords)\n    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,\n                               author_keywords, acronyms, output_mode, output_limit,\n                               spires, only_core_tags)\n\n\ndef extract_single_keywords(skw_db, fulltext):\n    \"\"\"Find single keywords in the fulltext\n    :var skw_db: list of KeywordToken objects\n    :var fulltext: string, which will be searched\n    :return : dictionary of matches in a format {\n            <keyword object>, [[position, position...], ],\n            ..\n            }\n            or empty {}\n    \"\"\"\n    return keyworder.get_single_keywords(skw_db, fulltext) or {}\n\n\ndef extract_composite_keywords(ckw_db, fulltext, skw_spans):\n    \"\"\"Returns a list of composite keywords bound with the number of\n    occurrences found in the text string.\n    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)\n    :var fulltext: string to search in\n    :skw_spans: dictionary of already identified single keywords\n    :return : dictionary of matches in a format {\n            <keyword object>, [[position, position...], [info_about_matches] ],\n            ..\n            }\n            or empty {}\n    \"\"\"\n    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}\n\n\ndef extract_abbreviations(fulltext):\n    \"\"\"Extract acronyms from the fulltext\n    :var fulltext: utf-8 string\n    :return: dictionary of matches in a formt {\n          <keyword object>, [matched skw or ckw object, ....]\n          }\n          or empty {}\n    \"\"\"\n    acronyms = {}\n    K = reader.KeywordToken\n    for k, v in acronymer.get_acronyms(fulltext).items():\n        acronyms[K(k, type='acronym')] = v\n    return acronyms\n\n\ndef extract_author_keywords(skw_db, ckw_db, fulltext):\n    \"\"\"Finds out human defined keyowrds in a text string. Searches for\n    the string \"Keywords:\" and its declinations and matches the\n    following words.\n\n    :var skw_db: list single kw object\n    :var ckw_db: list of composite kw objects\n    :var fulltext: utf-8 string\n    :return: dictionary of matches in a formt {\n          <keyword object>, [matched skw or ckw object, ....]\n          }\n          or empty {}\n    \"\"\"\n    akw = {}\n    K = reader.KeywordToken\n    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():\n        akw[K(k, type='author-kw')] = v\n    return akw\n\n\n# ---------------------------------------------------------------------\n#                          presentation functions\n# ---------------------------------------------------------------------\n\n\ndef get_keywords_output(single_keywords, composite_keywords, taxonomy_name,\n                        author_keywords=None, acronyms=None, style=\"text\", output_limit=0,\n                        spires=False, only_core_tags=False):\n    \"\"\"Returns a formatted string representing the keywords according\n    to the chosen style. This is the main routing call, this function will\n    also strip unwanted keywords before output and limits the number\n    of returned keywords\n    :var single_keywords: list of single keywords\n    :var composite_keywords: list of composite keywords\n    :var taxonomy_name: string, taxonomy name\n    :keyword author_keywords: dictionary of author keywords extracted from fulltext\n    :keyword acronyms: dictionary of extracted acronyms\n    :keyword style: text|html|marc\n    :keyword output_limit: int, number of maximum keywords printed (it applies\n            to single and composite keywords separately)\n    :keyword spires: boolen meaning spires output style\n    :keyword only_core_tags: boolean\n    \"\"\"\n    categories = {}\n    # sort the keywords, but don't limit them (that will be done later)\n    single_keywords_p = _sort_kw_matches(single_keywords)\n\n    composite_keywords_p = _sort_kw_matches(composite_keywords)\n\n    for w in single_keywords_p:\n        categories[w[0].concept] = w[0].type\n    for w in single_keywords_p:\n        categories[w[0].concept] = w[0].type\n\n    complete_output = _output_complete(single_keywords_p, composite_keywords_p,\n                                       author_keywords, acronyms, spires,\n                                       only_core_tags, limit=output_limit)\n    functions = {\"text\": _output_text, \"marcxml\": _output_marc, \"html\":\n                 _output_html, \"dict\": _output_dict}\n    my_styles = {}\n\n    for s in style:\n        if s != \"raw\":\n            my_styles[s] = functions[s](complete_output, categories)\n        else:\n            if output_limit > 0:\n                my_styles[\"raw\"] = (_kw(_sort_kw_matches(single_keywords, output_limit)),\n                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),\n                                    author_keywords,  # this we don't limit (?)\n                                    _kw(_sort_kw_matches(acronyms, output_limit)))\n            else:\n                my_styles[\"raw\"] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)\n\n    return my_styles\n\n\ndef build_marc(recid, single_keywords, composite_keywords,\n               spires=False, author_keywords=None, acronyms=None):\n    \"\"\"Create xml record.\n\n    :var recid: ingeter\n    :var single_keywords: dictionary of kws\n    :var composite_keywords: dictionary of kws\n    :keyword spires: please don't use, left for historical\n        reasons\n    :keyword author_keywords: dictionary of extracted keywords\n    :keyword acronyms: dictionary of extracted acronyms\n    :return: str, marxml\n    \"\"\"\n    output = ['<collection><record>\\n'\n              '<controlfield tag=\"001\">%s</controlfield>' % recid]\n\n    # no need to sort\n    single_keywords = single_keywords.items()\n    composite_keywords = composite_keywords.items()\n\n    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))\n\n    output.append('</record></collection>')\n\n    return '\\n'.join(output)\n\n\ndef _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,\n                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,\n                 provenience='BibClassify'):\n    \"\"\"Output the keywords in the MARCXML format.\n\n    :var skw_matches: list of single keywords\n    :var ckw_matches: list of composite keywords\n    :var author_keywords: dictionary of extracted author keywords\n    :var acronyms: dictionary of acronyms\n    :var spires: boolean, True=generate spires output - BUT NOTE: it is\n            here only not to break compatibility, in fact spires output\n            should never be used for xml because if we read marc back\n            into the KeywordToken objects, we would not find them\n    :keyword provenience: string that identifies source (authority) that\n        assigned the contents of the field\n    :return: string, formatted MARC\"\"\"\n\n    kw_template = ('<datafield tag=\"%s\" ind1=\"%s\" ind2=\"%s\">\\n'\n                   '    <subfield code=\"2\">%s</subfield>\\n'\n                   '    <subfield code=\"a\">%s</subfield>\\n'\n                   '    <subfield code=\"n\">%s</subfield>\\n'\n                   '    <subfield code=\"9\">%s</subfield>\\n'\n                   '</datafield>\\n')\n\n    output = []\n\n    tag, ind1, ind2 = _parse_marc_code(kw_field)\n    for keywords in (output_complete[\"Single keywords\"], output_complete[\"Core keywords\"]):\n        for kw in keywords:\n            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),\n                                         encode_for_xml(kw), keywords[kw],\n                                         encode_for_xml(categories[kw])))\n\n    for field, keywords in ((auth_field, output_complete[\"Author keywords\"]),\n                            (acro_field, output_complete[\"Acronyms\"])):\n        if keywords and len(keywords) and field:  # field='' we shall not save the keywords\n            tag, ind1, ind2 = _parse_marc_code(field)\n            for kw, info in keywords.items():\n                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),\n                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))\n\n    return \"\".join(output)\n\n\ndef _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,\n                     acronyms=None, spires=False, only_core_tags=False,\n                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):\n\n    if limit:\n        resized_skw = skw_matches[0:limit]\n        resized_ckw = ckw_matches[0:limit]\n    else:\n        resized_skw = skw_matches\n        resized_ckw = ckw_matches\n\n    results = {\"Core keywords\": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}\n\n    if not only_core_tags:\n        results[\"Author keywords\"] = _get_author_keywords(author_keywords, spires=spires)\n        results[\"Composite keywords\"] = _get_compositekws(resized_ckw, spires=spires)\n        results[\"Single keywords\"] = _get_singlekws(resized_skw, spires=spires)\n        results[\"Field codes\"] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)\n        results[\"Acronyms\"] = _get_acronyms(acronyms)\n\n    return results\n\n\ndef _output_dict(complete_output, categories):\n    return {\n        \"complete_output\": complete_output,\n        \"categories\": categories\n    }\n\n\ndef _output_text(complete_output, categories):\n    \"\"\"Output the results obtained in text format.\n\n\n    :return: str, html formatted output\n    \"\"\"\n    output = \"\"\n\n    for result in complete_output:\n        list_result = complete_output[result]\n        if list_result:\n            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],\n                                        reverse=True)\n            output += \"\\n\\n{0}:\\n\".format(result)\n            for element in list_result_sorted:\n                output += \"\\n{0} {1}\".format(list_result[element], element)\n\n    output += \"\\n--\\n{0}\".format(_signature())\n\n    return output\n\n\ndef _output_html(complete_output, categories):\n    \"\"\"Output the same as txt output does, but HTML formatted.\n\n    :var skw_matches: sorted list of single keywords\n    :var ckw_matches: sorted list of composite keywords\n    :var author_keywords: dictionary of extracted author keywords\n    :var acronyms: dictionary of acronyms\n    :var spires: boolean\n    :var only_core_tags: boolean\n    :keyword limit: int, number of printed keywords\n    :return: str, html formatted output\n    \"\"\"\n    return \"\"\"<html>\n    <head>\n      <title>Automatically generated keywords by bibclassify</title>\n    </head>\n    <body>\n    {0}\n    </body>\n    </html>\"\"\".format(\n        _output_text(complete_output).replace('\\n', '<br>')\n    ).replace('\\n', '')\n\n\ndef _get_singlekws(skw_matches, spires=False):\n    \"\"\"\n    :var skw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: list of formatted keywords\n    \"\"\"\n    output = {}\n    for single_keyword, info in skw_matches:\n        output[single_keyword.output(spires)] = len(info[0])\n    return output\n\n\ndef _get_compositekws(ckw_matches, spires=False):\n    \"\"\"\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: list of formatted keywords\n    \"\"\"\n    output = {}\n    for composite_keyword, info in ckw_matches:\n        output[composite_keyword.output(spires)] = {\"numbers\": len(info[0]),\n                                                    \"details\": info[1]}\n    return output\n\n\ndef _get_acronyms(acronyms):\n    \"\"\"Return a formatted list of acronyms.\"\"\"\n    acronyms_str = {}\n    if acronyms:\n        for acronym, expansions in iteritems(acronyms):\n            expansions_str = \", \".join([\"%s (%d)\" % expansion\n                                        for expansion in expansions])\n            acronyms_str[acronym] = expansions_str\n\n    return acronyms\n\n\ndef _get_author_keywords(author_keywords, spires=False):\n    \"\"\"Format the output for the author keywords.\n\n    :return: list of formatted author keywors\n    \"\"\"\n    out = {}\n    if author_keywords:\n        for keyword, matches in author_keywords.items():\n            skw_matches = matches[0]  # dictionary of single keywords\n            ckw_matches = matches[1]  # dict of composite keywords\n            matches_str = []\n            for ckw, spans in ckw_matches.items():\n                matches_str.append(ckw.output(spires))\n            for skw, spans in skw_matches.items():\n                matches_str.append(skw.output(spires))\n            if matches_str:\n                out[keyword] = matches_str\n            else:\n                out[keyword] = 0\n\n    return out\n\n\ndef _get_fieldcodes(skw_matches, ckw_matches, spires=False):\n    \"\"\"Return the output for the field codes.\n\n    :var skw_matches: dict of {keyword: [info,...]}\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: string\"\"\"\n    fieldcodes = {}\n    output = {}\n\n    for skw, _ in skw_matches:\n        for fieldcode in skw.fieldcodes:\n            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))\n    for ckw, _ in ckw_matches:\n\n        if len(ckw.fieldcodes):\n            for fieldcode in ckw.fieldcodes:\n                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))\n        else:  # inherit field-codes from the composites\n            for kw in ckw.getComponents():\n                for fieldcode in kw.fieldcodes:\n                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))\n                    fieldcodes.setdefault('*', set()).add(kw.output(spires))\n\n    for fieldcode, keywords in fieldcodes.items():\n        output[fieldcode] = ', '.join(keywords)\n\n    return output\n\n\ndef _get_core_keywords(skw_matches, ckw_matches, spires=False):\n    \"\"\"Return the output for the field codes.\n\n    :var skw_matches: dict of {keyword: [info,...]}\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: set of formatted core keywords\n    \"\"\"\n    output = {}\n    category = {}\n\n    def _get_value_kw(kw):\n        \"\"\"Help to sort the Core keywords.\"\"\"\n        i = 0\n        while kw[i].isdigit():\n            i += 1\n        if i > 0:\n            return int(kw[:i])\n        else:\n            return 0\n\n    for skw, info in skw_matches:\n        if skw.core:\n            output[skw.output(spires)] = len(info[0])\n            category[skw.output(spires)] = skw.type\n    for ckw, info in ckw_matches:\n        if ckw.core:\n            output[ckw.output(spires)] = len(info[0])\n        else:\n            #test if one of the components is  not core\n            i = 0\n            for c in ckw.getComponents():\n                if c.core:\n                    output[c.output(spires)] = info[1][i]\n                i += 1\n    return output\n\n\ndef _filter_core_keywors(keywords):\n    matches = {}\n    for kw, info in keywords.items():\n        if kw.core:\n            matches[kw] = info\n    return matches\n\n\ndef _signature():\n    \"\"\"Print out the bibclassify signature.\n\n    #todo: add information about taxonomy, rdflib\"\"\"\n\n    return 'bibclassify v%s' % (bconfig.VERSION,)\n\n\ndef clean_before_output(kw_matches):\n    \"\"\"Return a clean copy of the keywords data structure.\n\n    Stripped off the standalone and other unwanted elements\"\"\"\n    filtered_kw_matches = {}\n\n    for kw_match, info in iteritems(kw_matches):\n        if not kw_match.nostandalone:\n            filtered_kw_matches[kw_match] = info\n\n    return filtered_kw_matches\n\n# ---------------------------------------------------------------------\n#                          helper functions\n# ---------------------------------------------------------------------\n\n\ndef _skw_matches_comparator(kw0, kw1):\n    \"\"\"\n    Compare 2 single keywords objects.\n\n    First by the number of their spans (ie. how many times they were found),\n    if it is equal it compares them by lenghts of their labels.\n    \"\"\"\n    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))\n    if list_comparison:\n        return list_comparison\n\n    if kw0[0].isComposite() and kw1[0].isComposite():\n        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])\n        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])\n        component_comparison = cmp(component_avg1, component_avg0)\n        if component_comparison:\n            return component_comparison\n\n    return cmp(len(str(kw1[0])), len(str(kw0[0])))\n\n\ndef _kw(keywords):\n    \"\"\"Turn list of keywords into dictionary.\"\"\"\n    r = {}\n    for k, v in keywords:\n        r[k] = v\n    return r\n\n\ndef _sort_kw_matches(skw_matches, limit=0):\n    \"\"\"Return a resized version of keywords to the given length.\"\"\"\n    sorted_keywords = list(skw_matches.items())\n    sorted_keywords.sort(_skw_matches_comparator)\n    return limit and sorted_keywords[:limit] or sorted_keywords\n\n\ndef _get_partial_text(fulltext):\n    \"\"\"\n    Return a short version of the fulltext used with the partial matching mode.\n\n    The version is composed of 20% in the beginning and 20% in the middle of the\n    text.\"\"\"\n    length = len(fulltext)\n\n    get_index = lambda x: int(float(x) / 100 * length)\n\n    partial_text = [fulltext[get_index(start):get_index(end)]\n                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]\n\n    return \"\\n\".join(partial_text)\n\n\ndef save_keywords(filename, xml):\n    tmp_dir = os.path.dirname(filename)\n    if not os.path.isdir(tmp_dir):\n        os.mkdir(tmp_dir)\n\n    file_desc = open(filename, \"w\")\n    file_desc.write(xml)\n    file_desc.close()\n\n\ndef get_tmp_file(recid):\n    tmp_directory = \"%s/bibclassify\" % bconfig.CFG_TMPDIR\n    if not os.path.isdir(tmp_directory):\n        os.mkdir(tmp_directory)\n    filename = \"bibclassify_%s.xml\" % recid\n    abs_path = os.path.join(tmp_directory, filename)\n    return abs_path\n\n\ndef _parse_marc_code(field):\n    \"\"\"Parse marc field and return default indicators if not filled in.\"\"\"\n    field = str(field)\n    if len(field) < 4:\n        raise Exception('Wrong field code: %s' % field)\n    else:\n        field += '__'\n    tag = field[0:3]\n    ind1 = field[3].replace('_', '')\n    ind2 = field[4].replace('_', '')\n    return tag, ind1, ind2\n\n\nif __name__ == \"__main__\":\n    log.error(\"Please use bibclassify_cli from now on.\")\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/chokribr/invenio/blob/d64fffea5a05775ba2db65cba5408c2e9635e354",
        "file_path": "/invenio/legacy/bibclassify/ontology_reader.py",
        "source": "# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015 CERN.\n#\n# Invenio is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\n\"\"\"BibClassify ontology reader.\n\nThe ontology reader reads currently either a RDF/SKOS taxonomy or a\nsimple controlled vocabulary file (1 word per line). The first role of\nthis module is to manage the cached version of the ontology file. The\nsecond role is to hold all methods responsible for the creation of\nregular expressions. These methods are grammatically related as we take\ncare of different forms of the same words.  The grammatical rules can be\nconfigured via the configuration file.\n\nThe main method from this module is get_regular_expressions.\n\"\"\"\n\nfrom __future__ import print_function\n\nfrom datetime import datetime, timedelta\nfrom six import iteritems\nfrom six.moves import cPickle\n\nimport os\nimport re\nimport sys\nimport tempfile\nimport time\nimport urllib2\nimport traceback\nimport xml.sax\nimport thread\nimport rdflib\n\nfrom invenio.legacy.bibclassify import config as bconfig\nfrom invenio.modules.classifier.errors import TaxonomyError\n\nlog = bconfig.get_logger(\"bibclassify.ontology_reader\")\nfrom invenio import config\n\nfrom invenio.modules.classifier.registry import taxonomies\n\n# only if not running in a stanalone mode\nif bconfig.STANDALONE:\n    dbquery = None\n    from urllib2 import urlopen\nelse:\n    from invenio.legacy import dbquery\n    from invenio.utils.url import make_invenio_opener\n\n    urlopen = make_invenio_opener('BibClassify').open\n\n_contains_digit = re.compile(\"\\d\")\n_starts_with_non = re.compile(\"(?i)^non[a-z]\")\n_starts_with_anti = re.compile(\"(?i)^anti[a-z]\")\n_split_by_punctuation = re.compile(\"(\\W+)\")\n\n_CACHE = {}\n\n\ndef get_cache(taxonomy_id):\n    \"\"\"Return thread-safe cache for the given taxonomy id.\n\n    :param taxonomy_id: identifier of the taxonomy\n    :type taxonomy_id: str\n\n    :return: dictionary object (empty if no taxonomy_id\n        is found), you must not change anything inside it.\n        Create a new dictionary and use set_cache if you want\n        to update the cache!\n    \"\"\"\n    # Because of a standalone mode, we don't use the\n    # invenio.data_cacher.DataCacher, but it has no effect\n    # on proper functionality.\n\n    if taxonomy_id in _CACHE:\n        ctime, taxonomy = _CACHE[taxonomy_id]\n\n        # check it is fresh version\n        onto_name, onto_path, onto_url = _get_ontology(taxonomy_id)\n        cache_path = _get_cache_path(onto_name)\n\n        # if source exists and is newer than the cache hold in memory\n        if os.path.isfile(onto_path) and os.path.getmtime(onto_path) > ctime:\n            log.info('Forcing taxonomy rebuild as cached'\n                     ' version is newer/updated.')\n            return {}  # force cache rebuild\n\n        # if cache exists and is newer than the cache hold in memory\n        if os.path.isfile(cache_path) and os.path.getmtime(cache_path) > ctime:\n            log.info('Forcing taxonomy rebuild as source'\n                     ' file is newer/updated.')\n            return {}\n        log.info('Taxonomy retrieved from cache')\n        return taxonomy\n    return {}\n\n\ndef set_cache(taxonomy_id, contents):\n    \"\"\"Update cache in a thread-safe manner.\"\"\"\n    lock = thread.allocate_lock()\n    lock.acquire()\n    try:\n        _CACHE[taxonomy_id] = (time.time(), contents)\n    finally:\n        lock.release()\n\n\ndef get_regular_expressions(taxonomy_name, rebuild=False, no_cache=False):\n    \"\"\"Return a list of patterns compiled from the RDF/SKOS ontology.\n\n    Uses cache if it exists and if the taxonomy hasn't changed.\n    \"\"\"\n    # Translate the ontology name into a local path. Check if the name\n    # relates to an existing ontology.\n    onto_name, onto_path, onto_url = _get_ontology(taxonomy_name)\n    if not onto_path:\n        raise TaxonomyError(\"Unable to locate the taxonomy: '%s'.\"\n                            % taxonomy_name)\n\n    cache_path = _get_cache_path(onto_name)\n    log.debug('Taxonomy discovered, now we load it '\n              '(from cache: %s, onto_path: %s, cache_path: %s)'\n              % (not no_cache, onto_path, cache_path))\n\n    if os.access(cache_path, os.R_OK):\n        if os.access(onto_path, os.R_OK):\n            if rebuild or no_cache:\n                log.debug(\"Cache generation was manually forced.\")\n                return _build_cache(onto_path, skip_cache=no_cache)\n        else:\n            # ontology file not found. Use the cache instead.\n            log.warning(\"The ontology couldn't be located. However \"\n                        \"a cached version of it is available. Using it as a \"\n                        \"reference.\")\n            return _get_cache(cache_path, source_file=onto_path)\n\n        if (os.path.getmtime(cache_path) >\n                os.path.getmtime(onto_path)):\n            # Cache is more recent than the ontology: use cache.\n            log.debug(\"Normal situation, cache is older than ontology,\"\n                      \" so we load it from cache\")\n            return _get_cache(cache_path, source_file=onto_path)\n        else:\n            # Ontology is more recent than the cache: rebuild cache.\n            log.warning(\"Cache '%s' is older than '%s'. \"\n                        \"We will rebuild the cache\" %\n                        (cache_path, onto_path))\n            return _build_cache(onto_path, skip_cache=no_cache)\n\n    elif os.access(onto_path, os.R_OK):\n        if not no_cache and\\\n                os.path.exists(cache_path) and\\\n                not os.access(cache_path, os.W_OK):\n            raise TaxonomyError('We cannot read/write into: %s. '\n                                'Aborting!' % cache_path)\n        elif not no_cache and os.path.exists(cache_path):\n            log.warning('Cache %s exists, but is not readable!' % cache_path)\n        log.info(\"Cache not available. Building it now: %s\" % onto_path)\n        return _build_cache(onto_path, skip_cache=no_cache)\n\n    else:\n        raise TaxonomyError(\"We miss both source and cache\"\n                            \" of the taxonomy: %s\" % taxonomy_name)\n\n\ndef _get_remote_ontology(onto_url, time_difference=None):\n    \"\"\"Check if the online ontology is more recent than the local ontology.\n\n    If yes, try to download and store it in Invenio's cache directory.\n\n    Return a boolean describing the success of the operation.\n\n    :return: path to the downloaded ontology.\n    \"\"\"\n    if onto_url is None:\n        return False\n\n    dl_dir = ((config.CFG_CACHEDIR or tempfile.gettempdir()) + os.sep +\n              \"bibclassify\" + os.sep)\n    if not os.path.exists(dl_dir):\n        os.mkdir(dl_dir)\n\n    local_file = dl_dir + os.path.basename(onto_url)\n    remote_modif_time = _get_last_modification_date(onto_url)\n    try:\n        local_modif_seconds = os.path.getmtime(local_file)\n    except OSError:\n        # The local file does not exist. Download the ontology.\n        download = True\n        log.info(\"The local ontology could not be found.\")\n    else:\n        local_modif_time = datetime(*time.gmtime(local_modif_seconds)[0:6])\n        # Let's set a time delta of 1 hour and 10 minutes.\n        time_difference = time_difference or timedelta(hours=1, minutes=10)\n        download = remote_modif_time > local_modif_time + time_difference\n        if download:\n            log.info(\"The remote ontology '%s' is more recent \"\n                     \"than the local ontology.\" % onto_url)\n\n    if download:\n        if not _download_ontology(onto_url, local_file):\n            log.warning(\"Error downloading the ontology from: %s\" % onto_url)\n\n    return local_file\n\n\ndef _get_ontology(ontology):\n    \"\"\"Return the (name, path, url) to the short ontology name.\n\n    :param ontology: name of the ontology or path to the file or url.\n    \"\"\"\n    onto_name = onto_path = onto_url = None\n\n    # first assume we got the path to the file\n    if os.path.exists(ontology):\n        onto_name = os.path.split(os.path.abspath(ontology))[1]\n        onto_path = os.path.abspath(ontology)\n        onto_url = \"\"\n    else:\n        # if not, try to find it in a known locations\n        discovered_file = _discover_ontology(ontology)\n        if discovered_file:\n            onto_name = os.path.split(discovered_file)[1]\n            onto_path = discovered_file\n            # i know, this sucks\n            x = ontology.lower()\n            if \"http:\" in x or \"https:\" in x or \"ftp:\" in x or \"file:\" in x:\n                onto_url = ontology\n            else:\n                onto_url = \"\"\n        else:\n            # not found, look into a database\n            # (it is last because when bibclassify\n            # runs in a standalone mode,\n            # it has no database - [rca, old-heritage]\n            if not bconfig.STANDALONE:\n                result = dbquery.run_sql(\"SELECT name, location from clsMETHOD WHERE name LIKE %s\",\n                                         ('%' + ontology + '%',))\n                for onto_short_name, url in result:\n                    onto_name = onto_short_name\n                    onto_path = _get_remote_ontology(url)\n                    onto_url = url\n\n    return (onto_name, onto_path, onto_url)\n\n\ndef _discover_ontology(ontology_name):\n    \"\"\"Look for the file in a known places.\n\n    Inside invenio/etc/bibclassify and a few other places\n    like current directory.\n\n    :param ontology: name or path name or url\n    :type ontology: str\n\n    :return: absolute path of a file if found, or None\n    \"\"\"\n    last_part = os.path.split(os.path.abspath(ontology_name))[1]\n    if last_part in taxonomies:\n        return taxonomies.get(last_part)\n    elif last_part + \".rdf\" in taxonomies:\n        return taxonomies.get(last_part + \".rdf\")\n    else:\n        log.debug(\"No taxonomy with pattern '%s' found\" % ontology_name)\n\n    # LEGACY\n    possible_patterns = [last_part, last_part.lower()]\n    if not last_part.endswith('.rdf'):\n        possible_patterns.append(last_part + '.rdf')\n    places = [config.CFG_CACHEDIR,\n              config.CFG_ETCDIR,\n              os.path.join(config.CFG_CACHEDIR, \"bibclassify\"),\n              os.path.join(config.CFG_ETCDIR, \"bibclassify\"),\n              os.path.abspath('.'),\n              os.path.abspath(os.path.join(os.path.dirname(__file__),\n                                           \"../../../etc/bibclassify\")),\n              os.path.join(os.path.dirname(__file__), \"bibclassify\"),\n              config.CFG_WEBDIR]\n\n    log.debug(\"Searching for taxonomy using string: %s\" % last_part)\n    log.debug(\"Possible patterns: %s\" % possible_patterns)\n    for path in places:\n\n        try:\n            if os.path.isdir(path):\n                log.debug(\"Listing: %s\" % path)\n                for filename in os.listdir(path):\n                    #log.debug('Testing: %s' % filename)\n                    for pattern in possible_patterns:\n                        filename_lc = filename.lower()\n                        if pattern == filename_lc and\\\n                                os.path.exists(os.path.join(path, filename)):\n                            filepath = os.path.abspath(os.path.join(path,\n                                                                    filename))\n                            if (os.access(filepath, os.R_OK)):\n                                log.debug(\"Found taxonomy at: %s\" % filepath)\n                                return filepath\n                            else:\n                                log.warning('Found taxonony at: %s, but it is'\n                                            ' not readable. '\n                                            'Continue searching...'\n                                            % filepath)\n        except OSError, os_error_msg:\n            log.warning('OS Error when listing path '\n                        '\"%s\": %s' % (str(path), str(os_error_msg)))\n    log.debug(\"No taxonomy with pattern '%s' found\" % ontology_name)\n\n\nclass KeywordToken:\n\n    \"\"\"KeywordToken is a class used for the extracted keywords.\n\n    It can be initialized with values from RDF store or from\n    simple strings. Specialty of this class is that objects are\n    hashable by subject - so in the dictionary two objects with the\n    same subject appears as one -- :see: self.__hash__ and self.__cmp__.\n    \"\"\"\n\n    def __init__(self, subject, store=None, namespace=None, type='HEP'):\n        \"\"\"Initialize KeywordToken with a subject.\n\n        :param subject: string or RDF object\n        :param store: RDF graph object\n                      (will be used to get info about the subject)\n        :param namespace: RDF namespace object, used together with store\n        :param type: type of this keyword.\n        \"\"\"\n        self.id = subject\n        self.type = type\n        self.short_id = subject\n        self.concept = \"\"\n        self.regex = []\n        self.nostandalone = False\n        self.spires = False\n        self.fieldcodes = []\n        self.compositeof = []\n        self.core = False\n        # True means composite keyword\n        self._composite = '#Composite' in subject\n        self.__hash = None\n\n        # the tokens are coming possibly from a normal text file\n        if store is None:\n            subject = subject.strip()\n            self.concept = subject\n            self.regex = _get_searchable_regex(basic=[subject])\n            self.nostandalone = False\n            self.fieldcodes = []\n            self.core = False\n            if subject.find(' ') > -1:\n                self._composite = True\n\n        # definitions from rdf\n        else:\n            self.short_id = self.short_id.split('#')[-1]\n\n            # find alternate names for this label\n            basic_labels = []\n\n            # turn those patterns into regexes only for simple keywords\n            if self._composite is False:\n                try:\n                    for label in store.objects(subject,\n                                               namespace[\"prefLabel\"]):\n                        # XXX shall i make it unicode?\n                        basic_labels.append(str(label))\n                except TypeError:\n                    pass\n                self.concept = basic_labels[0]\n            else:\n                try:\n                    self.concept = str(store.value(subject,\n                                                   namespace[\"prefLabel\"],\n                                                   any=True))\n                except KeyError:\n                    log.warning(\"Keyword with subject %s has no prefLabel.\"\n                                \" We use raw name\" %\n                                self.short_id)\n                    self.concept = self.short_id\n\n            # this is common both to composite and simple keywords\n            try:\n                for label in store.objects(subject, namespace[\"altLabel\"]):\n                    basic_labels.append(str(label))\n            except TypeError:\n                pass\n\n            # hidden labels are special (possibly regex) codes\n            hidden_labels = []\n            try:\n                for label in store.objects(subject, namespace[\"hiddenLabel\"]):\n                    hidden_labels.append(unicode(label))\n            except TypeError:\n                pass\n\n            # compile regular expression that will identify this token\n            self.regex = _get_searchable_regex(basic_labels, hidden_labels)\n\n            try:\n                for note in map(lambda s: str(s).lower().strip(),\n                                store.objects(subject, namespace[\"note\"])):\n                    if note == 'core':\n                        self.core = True\n                    elif note in (\"nostandalone\", \"nonstandalone\"):\n                        self.nostandalone = True\n                    elif 'fc:' in note:\n                        self.fieldcodes.append(note[3:].strip())\n            except TypeError:\n                pass\n\n            # spiresLabel does not have multiple values\n            spires_label = store.value(subject, namespace[\"spiresLabel\"])\n            if spires_label:\n                self.spires = str(spires_label)\n\n        # important for comparisons\n        self.__hash = hash(self.short_id)\n\n        # extract composite parts ids\n        if store is not None and self.isComposite():\n            small_subject = self.id.split(\"#Composite.\")[-1]\n            component_positions = []\n            for label in store.objects(self.id, namespace[\"compositeOf\"]):\n                strlabel = str(label).split(\"#\")[-1]\n                component_name = label.split(\"#\")[-1]\n                component_positions.append((small_subject.find(component_name),\n                                            strlabel))\n            component_positions.sort()\n            if not component_positions:\n                log.error(\"Keyword is marked as composite, \"\n                          \"but no composite components refs found: %s\"\n                          % self.short_id)\n            else:\n                self.compositeof = map(lambda x: x[1], component_positions)\n\n    def refreshCompositeOf(self, single_keywords, composite_keywords,\n                           store=None, namespace=None):\n        \"\"\"Re-check sub-parts of this keyword.\n\n        This should be called after the whole RDF was processed, because\n        it is using a cache of single keywords and if that\n        one is incomplete, you will not identify all parts.\n        \"\"\"\n        def _get_ckw_components(new_vals, label):\n            if label in single_keywords:\n                new_vals.append(single_keywords[label])\n            elif ('Composite.%s' % label) in composite_keywords:\n                for l in composite_keywords['Composite.%s' % label].compositeof:\n                    _get_ckw_components(new_vals, l)\n            elif label in composite_keywords:\n                for l in composite_keywords[label].compositeof:\n                    _get_ckw_components(new_vals, l)\n            else:\n                # One single or composite keyword is missing from the taxonomy.\n                # This is due to an error in the taxonomy description.\n                message = \"The composite term \\\"%s\\\"\"\\\n                          \" should be made of single keywords,\"\\\n                          \" but at least one is missing.\" % self.id\n                if store is not None:\n                    message += \"Needed components: %s\"\\\n                               % list(store.objects(self.id,\n                                      namespace[\"compositeOf\"]))\n                message += \" Missing is: %s\" % label\n                raise TaxonomyError(message)\n\n        if self.compositeof:\n            new_vals = []\n            try:\n                for label in self.compositeof:\n                    _get_ckw_components(new_vals, label)\n                self.compositeof = new_vals\n            except TaxonomyError:\n                # the composites will be empty\n                # (better than to have confusing, partial matches)\n                self.compositeof = []\n                log.error(\n                    'We reset this composite keyword, so that it does not match anything. Please fix the taxonomy.')\n\n    def isComposite(self):\n        \"\"\"Return value of _composite.\"\"\"\n        return self._composite\n\n    def getComponents(self):\n        \"\"\"Return value of compositeof.\"\"\"\n        return self.compositeof\n\n    def getType(self):\n        \"\"\"Return value of type.\"\"\"\n        return self.type\n\n    def setType(self, value):\n        \"\"\"Set value of value.\"\"\"\n        self.type = value\n\n    def __hash__(self):\n        \"\"\"Return _hash.\n\n        This might change in the future but for the moment we want to\n        think that if the concept is the same, then it is the same\n        keyword - this sucks, but it is sort of how it is necessary\n        to use now.\n        \"\"\"\n        return self.__hash\n\n    def __cmp__(self, other):\n        \"\"\"Compare objects using _hash.\"\"\"\n        if self.__hash < other.__hash__():\n            return -1\n        elif self.__hash == other.__hash__():\n            return 0\n        else:\n            return 1\n\n    def __str__(self, spires=False):\n        \"\"\"Return the best output for the keyword.\"\"\"\n        if spires:\n            if self.spires:\n                return self.spires\n            elif self._composite:\n                return self.concept.replace(':', ',')\n            # default action\n        return self.concept\n\n    def output(self, spires=False):\n        \"\"\"Return string representation with spires value.\"\"\"\n        return self.__str__(spires=spires)\n\n    def __repr__(self):\n        \"\"\"Class representation.\"\"\"\n        return \"<KeywordToken: %s>\" % self.short_id\n\n\ndef _build_cache(source_file, skip_cache=False):\n    \"\"\"Build the cached data.\n\n    Either by parsing the RDF taxonomy file or a vocabulary file.\n\n    :param source_file: source file of the taxonomy, RDF file\n    :param skip_cache: if True, build cache will not be\n        saved (pickled) - it is saved as <source_file.db>\n    \"\"\"\n    store = rdflib.ConjunctiveGraph()\n\n    if skip_cache:\n        log.info(\"You requested not to save the cache to disk.\")\n    else:\n        cache_path = _get_cache_path(source_file)\n        cache_dir = os.path.dirname(cache_path)\n        # Make sure we have a cache_dir readable and writable.\n        try:\n            os.makedirs(cache_dir)\n        except:\n            pass\n        if os.access(cache_dir, os.R_OK):\n            if not os.access(cache_dir, os.W_OK):\n                raise TaxonomyError(\"Cache directory exists but is not\"\n                                    \" writable. Check your permissions\"\n                                    \" for: %s\" % cache_dir)\n        else:\n            raise TaxonomyError(\"Cache directory does not exist\"\n                                \" (and could not be created): %s\" % cache_dir)\n\n    timer_start = time.clock()\n\n    namespace = None\n    single_keywords, composite_keywords = {}, {}\n\n    try:\n        log.info(\"Building RDFLib's conjunctive graph from: %s\" % source_file)\n        try:\n            store.parse(source_file)\n        except urllib2.URLError:\n            if source_file[0] == '/':\n                store.parse(\"file://\" + source_file)\n            else:\n                store.parse(\"file:///\" + source_file)\n\n    except rdflib.exceptions.Error as e:\n        log.error(\"Serious error reading RDF file\")\n        log.error(e)\n        log.error(traceback.format_exc())\n        raise rdflib.exceptions.Error(e)\n\n    except (xml.sax.SAXParseException, ImportError) as e:\n        # File is not a RDF file. We assume it is a controlled vocabulary.\n        log.error(e)\n        log.warning(\"The ontology file is probably not a valid RDF file. \\\n            Assuming it is a controlled vocabulary file.\")\n\n        filestream = open(source_file, \"r\")\n        for line in filestream:\n            keyword = line.strip()\n            kt = KeywordToken(keyword)\n            single_keywords[kt.short_id] = kt\n        if not len(single_keywords):\n            raise TaxonomyError('The ontology file is not well formated')\n\n    else:  # ok, no exception happened\n        log.info(\"Now building cache of keywords\")\n        # File is a RDF file.\n        namespace = rdflib.Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n\n        single_count = 0\n        composite_count = 0\n\n        subject_objects = store.subject_objects(namespace[\"prefLabel\"])\n        for subject, pref_label in subject_objects:\n            kt = KeywordToken(subject, store=store, namespace=namespace)\n            if kt.isComposite():\n                composite_count += 1\n                composite_keywords[kt.short_id] = kt\n            else:\n                single_keywords[kt.short_id] = kt\n                single_count += 1\n\n    cached_data = {}\n    cached_data[\"single\"] = single_keywords\n    cached_data[\"composite\"] = composite_keywords\n    cached_data[\"creation_time\"] = time.gmtime()\n    cached_data[\"version_info\"] = {'rdflib': rdflib.__version__,\n                                   'bibclassify': bconfig.VERSION}\n    log.debug(\"Building taxonomy... %d terms built in %.1f sec.\" %\n              (len(single_keywords) + len(composite_keywords),\n               time.clock() - timer_start))\n\n    log.info(\"Total count of single keywords: %d \"\n             % len(single_keywords))\n    log.info(\"Total count of composite keywords: %d \"\n             % len(composite_keywords))\n\n    if not skip_cache:\n        cache_path = _get_cache_path(source_file)\n        cache_dir = os.path.dirname(cache_path)\n        log.debug(\"Writing the cache into: %s\" % cache_path)\n        # test again, it could have changed\n        if os.access(cache_dir, os.R_OK):\n            if os.access(cache_dir, os.W_OK):\n                # Serialize.\n                filestream = None\n                try:\n                    filestream = open(cache_path, \"wb\")\n                except IOError as msg:\n                    # Impossible to write the cache.\n                    log.error(\"Impossible to write cache to '%s'.\"\n                              % cache_path)\n                    log.error(msg)\n                else:\n                    log.debug(\"Writing cache to file %s\" % cache_path)\n                    cPickle.dump(cached_data, filestream, 1)\n                if filestream:\n                    filestream.close()\n\n            else:\n                raise TaxonomyError(\"Cache directory exists but is not \"\n                                    \"writable. Check your permissions \"\n                                    \"for: %s\" % cache_dir)\n        else:\n            raise TaxonomyError(\"Cache directory does not exist\"\n                                \" (and could not be created): %s\" % cache_dir)\n\n    # now when the whole taxonomy was parsed,\n    # find sub-components of the composite kws\n    # it is important to keep this call after the taxonomy was saved,\n    # because we don't  want to pickle regexes multiple times\n    # (as they are must be re-compiled at load time)\n    for kt in composite_keywords.values():\n        kt.refreshCompositeOf(single_keywords, composite_keywords,\n                              store=store, namespace=namespace)\n\n    # house-cleaning\n    if store:\n        store.close()\n\n    return (single_keywords, composite_keywords)\n\n\ndef _capitalize_first_letter(word):\n    \"\"\"Return a regex pattern with the first letter.\n\n    Accepts both lowercase and uppercase.\n    \"\"\"\n    if word[0].isalpha():\n        # These two cases are necessary in order to get a regex pattern\n        # starting with '[xX]' and not '[Xx]'. This allows to check for\n        # colliding regex afterwards.\n        if word[0].isupper():\n            return \"[\" + word[0].swapcase() + word[0] + \"]\" + word[1:]\n        else:\n            return \"[\" + word[0] + word[0].swapcase() + \"]\" + word[1:]\n    return word\n\n\ndef _convert_punctuation(punctuation, conversion_table):\n    \"\"\"Return a regular expression for a punctuation string.\"\"\"\n    if punctuation in conversion_table:\n        return conversion_table[punctuation]\n    return re.escape(punctuation)\n\n\ndef _convert_word(word):\n    \"\"\"Return the plural form of the word if it exists.\n\n    Otherwise return the word itself.\n    \"\"\"\n    out = None\n\n    # Acronyms.\n    if word.isupper():\n        out = word + \"s?\"\n    # Proper nouns or word with digits.\n    elif word.istitle():\n        out = word + \"('?s)?\"\n    elif _contains_digit.search(word):\n        out = word\n\n    if out is not None:\n        return out\n\n    # Words with non or anti prefixes.\n    if _starts_with_non.search(word):\n        word = \"non-?\" + _capitalize_first_letter(_convert_word(word[3:]))\n    elif _starts_with_anti.search(word):\n        word = \"anti-?\" + _capitalize_first_letter(_convert_word(word[4:]))\n\n    if out is not None:\n        return _capitalize_first_letter(out)\n\n    # A few invariable words.\n    if word in bconfig.CFG_BIBCLASSIFY_INVARIABLE_WORDS:\n        return _capitalize_first_letter(word)\n\n    # Some exceptions that would not produce good results with the set of\n    # general_regular_expressions.\n    regexes = bconfig.CFG_BIBCLASSIFY_EXCEPTIONS\n    if word in regexes:\n        return _capitalize_first_letter(regexes[word])\n\n    regexes = bconfig.CFG_BIBCLASSIFY_UNCHANGE_REGULAR_EXPRESSIONS\n    for regex in regexes:\n        if regex.search(word) is not None:\n            return _capitalize_first_letter(word)\n\n    regexes = bconfig.CFG_BIBCLASSIFY_GENERAL_REGULAR_EXPRESSIONS\n    for regex, replacement in regexes:\n        stemmed = regex.sub(replacement, word)\n        if stemmed != word:\n            return _capitalize_first_letter(stemmed)\n\n    return _capitalize_first_letter(word + \"s?\")\n\n\ndef _get_cache(cache_file, source_file=None):\n    \"\"\"Get cached taxonomy using the cPickle module.\n\n    No check is done at that stage.\n\n    :param cache_file: full path to the file holding pickled data\n    :param source_file: if we discover the cache is obsolete, we\n        will build a new cache, therefore we need the source path\n        of the cache\n    :return: (single_keywords, composite_keywords).\n    \"\"\"\n    timer_start = time.clock()\n\n    filestream = open(cache_file, \"rb\")\n    try:\n        cached_data = cPickle.load(filestream)\n        version_info = cached_data['version_info']\n        if version_info['rdflib'] != rdflib.__version__\\\n                or version_info['bibclassify'] != bconfig.VERSION:\n            raise KeyError\n    except (cPickle.UnpicklingError, ImportError,\n            AttributeError, DeprecationWarning, EOFError):\n        log.warning(\"The existing cache in %s is not readable. \"\n                    \"Removing and rebuilding it.\" % cache_file)\n        filestream.close()\n        os.remove(cache_file)\n        return _build_cache(source_file)\n    except KeyError:\n        log.warning(\"The existing cache %s is not up-to-date. \"\n                    \"Removing and rebuilding it.\" % cache_file)\n        filestream.close()\n        os.remove(cache_file)\n        if source_file and os.path.exists(source_file):\n            return _build_cache(source_file)\n        else:\n            log.error(\"The cache contains obsolete data (and it was deleted), \"\n                      \"however I can't build a new cache, the source does not \"\n                      \"exist or is inaccessible! - %s\" % source_file)\n    filestream.close()\n\n    single_keywords = cached_data[\"single\"]\n    composite_keywords = cached_data[\"composite\"]\n\n    # the cache contains only keys of the composite keywords, not the objects\n    # so now let's resolve them into objects\n    for kw in composite_keywords.values():\n        kw.refreshCompositeOf(single_keywords, composite_keywords)\n\n    log.debug(\"Retrieved taxonomy from cache %s created on %s\" %\n              (cache_file, time.asctime(cached_data[\"creation_time\"])))\n\n    log.debug(\"%d terms read in %.1f sec.\" %\n              (len(single_keywords) + len(composite_keywords),\n               time.clock() - timer_start))\n\n    return (single_keywords, composite_keywords)\n\n\ndef _get_cache_path(source_file):\n    \"\"\"Return the path where the cache should be written/located.\n\n    :param onto_name: name of the ontology or the full path\n    :return: string, abs path to the cache file in the tmpdir/bibclassify\n    \"\"\"\n    local_name = os.path.basename(source_file)\n    cache_name = local_name + \".db\"\n    cache_dir = os.path.join(config.CFG_CACHEDIR, \"bibclassify\")\n\n    if not os.path.isdir(cache_dir):\n        os.makedirs(cache_dir)\n\n    return os.path.abspath(os.path.join(cache_dir, cache_name))\n\n\ndef _get_last_modification_date(url):\n    \"\"\"Get the last modification date of the ontology.\"\"\"\n    request = urllib2.Request(url)\n    request.get_method = lambda: \"HEAD\"\n    http_file = urlopen(request)\n    date_string = http_file.headers[\"last-modified\"]\n    parsed = time.strptime(date_string, \"%a, %d %b %Y %H:%M:%S %Z\")\n    return datetime(*(parsed)[0:6])\n\n\ndef _download_ontology(url, local_file):\n    \"\"\"Download the ontology and stores it in CFG_CACHEDIR.\"\"\"\n    log.debug(\"Copying remote ontology '%s' to file '%s'.\" % (url,\n                                                              local_file))\n    try:\n        url_desc = urlopen(url)\n        file_desc = open(local_file, 'w')\n        file_desc.write(url_desc.read())\n        file_desc.close()\n    except IOError as e:\n        print(e)\n        return False\n    except:\n        log.warning(\"Unable to download the ontology. '%s'\" %\n                    sys.exc_info()[0])\n        return False\n    else:\n        log.debug(\"Done copying.\")\n        return True\n\n\ndef _get_searchable_regex(basic=None, hidden=None):\n    \"\"\"Return the searchable regular expressions for the single keyword.\"\"\"\n    # Hidden labels are used to store regular expressions.\n    basic = basic or []\n    hidden = hidden or []\n\n    hidden_regex_dict = {}\n    for hidden_label in hidden:\n        if _is_regex(hidden_label):\n            hidden_regex_dict[hidden_label] = \\\n                re.compile(\n                    bconfig.CFG_BIBCLASSIFY_WORD_WRAP % hidden_label[1:-1]\n                )\n        else:\n            pattern = _get_regex_pattern(hidden_label)\n            hidden_regex_dict[hidden_label] = re.compile(\n                bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern\n            )\n\n    # We check if the basic label (preferred or alternative) is matched\n    # by a hidden label regex. If yes, discard it.\n    regex_dict = {}\n    # Create regex for plural forms and add them to the hidden labels.\n    for label in basic:\n        pattern = _get_regex_pattern(label)\n        regex_dict[label] = re.compile(\n            bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern\n        )\n\n    # Merge both dictionaries.\n    regex_dict.update(hidden_regex_dict)\n\n    return regex_dict.values()\n\n\ndef _get_regex_pattern(label):\n    \"\"\"Return a regular expression of the label.\n\n    This takes care of plural and different kinds of separators.\n    \"\"\"\n    parts = _split_by_punctuation.split(label)\n\n    for index, part in enumerate(parts):\n        if index % 2 == 0:\n            # Word\n            if not parts[index].isdigit() and len(parts[index]) > 1:\n                parts[index] = _convert_word(parts[index])\n        else:\n            # Punctuation\n            if not parts[index + 1]:\n                # The separator is not followed by another word. Treat\n                # it as a symbol.\n                parts[index] = _convert_punctuation(\n                    parts[index],\n                    bconfig.CFG_BIBCLASSIFY_SYMBOLS\n                )\n            else:\n                parts[index] = _convert_punctuation(\n                    parts[index],\n                    bconfig.CFG_BIBCLASSIFY_SEPARATORS\n                )\n\n    return \"\".join(parts)\n\n\ndef _is_regex(string):\n    \"\"\"Check if a concept is a regular expression.\"\"\"\n    return string[0] == \"/\" and string[-1] == \"/\"\n\n\ndef check_taxonomy(taxonomy):\n    \"\"\"Check the consistency of the taxonomy.\n\n    Outputs a list of errors and warnings.\n    \"\"\"\n    log.info(\"Building graph with Python RDFLib version %s\" %\n             rdflib.__version__)\n\n    store = rdflib.ConjunctiveGraph()\n\n    try:\n        store.parse(taxonomy)\n    except:\n        log.error(\"The taxonomy is not a valid RDF file. Are you \"\n                  \"trying to check a controlled vocabulary?\")\n        raise TaxonomyError('Error in RDF file')\n\n    log.info(\"Graph was successfully built.\")\n\n    prefLabel = \"prefLabel\"\n    hiddenLabel = \"hiddenLabel\"\n    altLabel = \"altLabel\"\n    composite = \"composite\"\n    compositeOf = \"compositeOf\"\n    note = \"note\"\n\n    both_skw_and_ckw = []\n\n    # Build a dictionary we will reason on later.\n    uniq_subjects = {}\n    for subject in store.subjects():\n        uniq_subjects[subject] = None\n\n    subjects = {}\n    for subject in uniq_subjects:\n        strsubject = str(subject).split(\"#Composite.\")[-1]\n        strsubject = strsubject.split(\"#\")[-1]\n        if (strsubject == \"http://cern.ch/thesauri/HEPontology.rdf\" or\n           strsubject == \"compositeOf\"):\n            continue\n        components = {}\n        for predicate, value in store.predicate_objects(subject):\n            strpredicate = str(predicate).split(\"#\")[-1]\n            strobject = str(value).split(\"#Composite.\")[-1]\n            strobject = strobject.split(\"#\")[-1]\n            components.setdefault(strpredicate, []).append(strobject)\n        if strsubject in subjects:\n            both_skw_and_ckw.append(strsubject)\n        else:\n            subjects[strsubject] = components\n\n    log.info(\"Taxonomy contains %s concepts.\" % len(subjects))\n\n    no_prefLabel = []\n    multiple_prefLabels = []\n    bad_notes = []\n    # Subjects with no composite or compositeOf predicate\n    lonely = []\n    both_composites = []\n    bad_hidden_labels = {}\n    bad_alt_labels = {}\n    # Problems with composite keywords\n    composite_problem1 = []\n    composite_problem2 = []\n    composite_problem3 = []\n    composite_problem4 = {}\n    composite_problem5 = []\n    composite_problem6 = []\n\n    stemming_collisions = []\n    interconcept_collisions = {}\n\n    for subject, predicates in iteritems(subjects):\n        # No prefLabel or multiple prefLabels\n        try:\n            if len(predicates[prefLabel]) > 1:\n                multiple_prefLabels.append(subject)\n        except KeyError:\n            no_prefLabel.append(subject)\n\n        # Lonely and both composites.\n        if composite not in predicates and compositeOf not in predicates:\n            lonely.append(subject)\n        elif composite in predicates and compositeOf in predicates:\n            both_composites.append(subject)\n\n        # Multiple or bad notes\n        if note in predicates:\n            bad_notes += [(subject, n) for n in predicates[note]\n                          if n not in ('nostandalone', 'core')]\n\n        # Bad hidden labels\n        if hiddenLabel in predicates:\n            for lbl in predicates[hiddenLabel]:\n                if lbl.startswith(\"/\") ^ lbl.endswith(\"/\"):\n                    bad_hidden_labels.setdefault(subject, []).append(lbl)\n\n        # Bad alt labels\n        if altLabel in predicates:\n            for lbl in predicates[altLabel]:\n                if len(re.findall(\"/\", lbl)) >= 2 or \":\" in lbl:\n                    bad_alt_labels.setdefault(subject, []).append(lbl)\n\n        # Check composite\n        if composite in predicates:\n            for ckw in predicates[composite]:\n                if ckw in subjects:\n                    if compositeOf in subjects[ckw]:\n                        if subject not in subjects[ckw][compositeOf]:\n                            composite_problem3.append((subject, ckw))\n                    else:\n                        if ckw not in both_skw_and_ckw:\n                            composite_problem2.append((subject, ckw))\n                else:\n                    composite_problem1.append((subject, ckw))\n\n        # Check compositeOf\n        if compositeOf in predicates:\n            for skw in predicates[compositeOf]:\n                if skw in subjects:\n                    if composite in subjects[skw]:\n                        if subject not in subjects[skw][composite]:\n                            composite_problem6.append((subject, skw))\n                    else:\n                        if skw not in both_skw_and_ckw:\n                            composite_problem5.append((subject, skw))\n                else:\n                    composite_problem4.setdefault(skw, []).append(subject)\n\n        # Check for stemmed labels\n        if compositeOf in predicates:\n            labels = (altLabel, hiddenLabel)\n        else:\n            labels = (prefLabel, altLabel, hiddenLabel)\n\n        patterns = {}\n        for label in [lbl for lbl in labels if lbl in predicates]:\n            for expression in [expr for expr in predicates[label]\n                               if not _is_regex(expr)]:\n                pattern = _get_regex_pattern(expression)\n                interconcept_collisions.setdefault(pattern, []).\\\n                    append((subject, label))\n                if pattern in patterns:\n                    stemming_collisions.append(\n                        (subject,\n                         patterns[pattern],\n                         (label, expression)\n                         )\n                    )\n                else:\n                    patterns[pattern] = (label, expression)\n\n    print(\"\\n==== ERRORS ====\")\n\n    if no_prefLabel:\n        print(\"\\nConcepts with no prefLabel: %d\" % len(no_prefLabel))\n        print(\"\\n\".join([\"   %s\" % subj for subj in no_prefLabel]))\n    if multiple_prefLabels:\n        print((\"\\nConcepts with multiple prefLabels: %d\" %\n               len(multiple_prefLabels)))\n        print(\"\\n\".join([\"   %s\" % subj for subj in multiple_prefLabels]))\n    if both_composites:\n        print((\"\\nConcepts with both composite properties: %d\" %\n               len(both_composites)))\n        print(\"\\n\".join([\"   %s\" % subj for subj in both_composites]))\n    if bad_hidden_labels:\n        print(\"\\nConcepts with bad hidden labels: %d\" % len(bad_hidden_labels))\n        for kw, lbls in iteritems(bad_hidden_labels):\n            print(\"   %s:\" % kw)\n            print(\"\\n\".join([\"      '%s'\" % lbl for lbl in lbls]))\n    if bad_alt_labels:\n        print(\"\\nConcepts with bad alt labels: %d\" % len(bad_alt_labels))\n        for kw, lbls in iteritems(bad_alt_labels):\n            print(\"   %s:\" % kw)\n            print(\"\\n\".join([\"      '%s'\" % lbl for lbl in lbls]))\n    if both_skw_and_ckw:\n        print((\"\\nKeywords that are both skw and ckw: %d\" %\n               len(both_skw_and_ckw)))\n        print(\"\\n\".join([\"   %s\" % subj for subj in both_skw_and_ckw]))\n\n    print()\n\n    if composite_problem1:\n        print(\"\\n\".join([\"SKW '%s' references an unexisting CKW '%s'.\" %\n                         (skw, ckw) for skw, ckw in composite_problem1]))\n    if composite_problem2:\n        print(\"\\n\".join([\"SKW '%s' references a SKW '%s'.\" %\n                         (skw, ckw) for skw, ckw in composite_problem2]))\n    if composite_problem3:\n        print(\"\\n\".join([\"SKW '%s' is not composite of CKW '%s'.\" %\n                         (skw, ckw) for skw, ckw in composite_problem3]))\n    if composite_problem4:\n        for skw, ckws in iteritems(composite_problem4):\n            print(\"SKW '%s' does not exist but is \" \"referenced by:\" % skw)\n            print(\"\\n\".join([\"    %s\" % ckw for ckw in ckws]))\n    if composite_problem5:\n        print(\"\\n\".join([\"CKW '%s' references a CKW '%s'.\" % kw\n                         for kw in composite_problem5]))\n    if composite_problem6:\n        print(\"\\n\".join([\"CKW '%s' is not composed by SKW '%s'.\" % kw\n                         for kw in composite_problem6]))\n\n    print(\"\\n==== WARNINGS ====\")\n\n    if bad_notes:\n        print((\"\\nConcepts with bad notes: %d\" % len(bad_notes)))\n        print(\"\\n\".join([\"   '%s': '%s'\" % _note for _note in bad_notes]))\n    if stemming_collisions:\n        print(\"\\nFollowing keywords have unnecessary labels that have \"\n              \"already been generated by BibClassify.\")\n        for subj in stemming_collisions:\n            print(\"   %s:\\n     %s\\n     and %s\" % subj)\n\n    print(\"\\nFinished.\")\n    sys.exit(0)\n\n\ndef test_cache(taxonomy_name='HEP', rebuild_cache=False, no_cache=False):\n    \"\"\"Test the cache lookup.\"\"\"\n    cache = get_cache(taxonomy_name)\n    if not cache:\n        set_cache(taxonomy_name, get_regular_expressions(taxonomy_name,\n                                                         rebuild=rebuild_cache,\n                                                         no_cache=no_cache))\n        cache = get_cache(taxonomy_name)\n    return (thread.get_ident(), cache)\n\n\nlog.info('Loaded ontology reader')\n\nif __name__ == '__main__':\n    test_cache()\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/chokribr/invenio/blob/d64fffea5a05775ba2db65cba5408c2e9635e354",
        "file_path": "/invenio/legacy/bibclassify/text_extractor.py",
        "source": "# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014 CERN.\n#\n# Invenio is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\n\"\"\"\nBibClassify text extractor.\n\nThis module provides method to extract the fulltext from local or remote\ndocuments. Currently 2 formats of documents are supported: PDF and text\ndocuments.\n\n2 methods provide the functionality of the module: text_lines_from_local_file\nand text_lines_from_url.\n\nThis module also provides the utility 'is_pdf' that uses GNU file in order to\ndetermine if a local file is a PDF file.\n\nThis module is STANDALONE safe\n\"\"\"\n\nimport os\nimport re\nimport tempfile\nimport urllib2\nfrom invenio.legacy.bibclassify import config as bconfig\n\nif bconfig.STANDALONE:\n    from urllib2 import urlopen\nelse:\n    from invenio.utils.url import make_invenio_opener\n\n    urlopen = make_invenio_opener('BibClassify').open\n\nlog = bconfig.get_logger(\"bibclassify.text_extractor\")\n\n_ONE_WORD = re.compile(\"[A-Za-z]{2,}\")\n\n\ndef is_pdf(document):\n    \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\"\n    if not executable_exists('pdftotext'):\n        log.warning(\"GNU file was not found on the system. \"\n                    \"Switching to a weak file extension test.\")\n        if document.lower().endswith(\".pdf\"):\n            return True\n        return False\n        # Tested with file version >= 4.10. First test is secure and works\n    # with file version 4.25. Second condition is tested for file\n    # version 4.10.\n    file_output = os.popen('file ' + re.escape(document)).read()\n    try:\n        filetype = file_output.split(\":\")[1]\n    except IndexError:\n        log.error(\"Your version of the 'file' utility seems to \"\n                  \"be unsupported. Please report this to cds.support@cern.ch.\")\n        raise Exception('Incompatible pdftotext')\n\n    pdf = filetype.find(\"PDF\") > -1\n    # This is how it should be done however this is incompatible with\n    # file version 4.10.\n    #os.popen('file -bi ' + document).read().find(\"application/pdf\")\n    return pdf\n\n\ndef text_lines_from_local_file(document, remote=False):\n    \"\"\"Returns the fulltext of the local file.\n    @var document: fullpath to the file that should be read\n    @var remote: boolean, if True does not count lines (gosh!)\n    @return: list of lines if st was read or an empty list\"\"\"\n\n    try:\n        if is_pdf(document):\n            if not executable_exists(\"pdftotext\"):\n                log.error(\"pdftotext is not available on the system.\")\n            cmd = \"pdftotext -q -enc UTF-8 %s -\" % re.escape(document)\n            filestream = os.popen(cmd)\n        else:\n            filestream = open(document, \"r\")\n    except IOError as ex1:\n        log.error(\"Unable to read from file %s. (%s)\" % (document, ex1.strerror))\n        return []\n\n    # FIXME - we assume it is utf-8 encoded / that is not good\n    lines = [line.decode(\"utf-8\", 'replace') for line in filestream]\n    filestream.close()\n\n    if not _is_english_text('\\n'.join(lines)):\n        log.warning(\"It seems the file '%s' is unvalid and doesn't \"\n                    \"contain text. Please communicate this file to the Invenio \"\n                    \"team.\" % document)\n\n    line_nb = len(lines)\n    word_nb = 0\n    for line in lines:\n        word_nb += len(re.findall(\"\\S+\", line))\n\n    # Discard lines that do not contain at least one word.\n    lines = [line for line in lines if _ONE_WORD.search(line) is not None]\n\n    if not remote:\n        log.info(\"Local file has %d lines and %d words.\" % (line_nb, word_nb))\n\n    return lines\n\n\ndef _is_english_text(text):\n    \"\"\"\n    Checks if a text is correct english.\n    Computes the number of words in the text and compares it to the\n    expected number of words (based on an average size of words of 5.1\n    letters).\n\n    @param text_lines: the text to analyze\n    @type text_lines:  string\n    @return:           True if the text is English, False otherwise\n    @rtype:            Boolean\n    \"\"\"\n    # Consider one word and one space.\n    avg_word_length = 2.55 + 1\n    expected_word_number = float(len(text)) / avg_word_length\n\n    words = [word\n             for word in re.split('\\W', text)\n             if word.isalpha()]\n\n    word_number = len(words)\n\n    return word_number > expected_word_number\n\n\ndef text_lines_from_url(url, user_agent=\"\"):\n    \"\"\"Returns the fulltext of the file found at the URL.\"\"\"\n    request = urllib2.Request(url)\n    if user_agent:\n        request.add_header(\"User-Agent\", user_agent)\n    try:\n        distant_stream = urlopen(request)\n        # Write the URL content to a temporary file.\n        local_file = tempfile.mkstemp(prefix=\"bibclassify.\")[1]\n        local_stream = open(local_file, \"w\")\n        local_stream.write(distant_stream.read())\n        local_stream.close()\n    except:\n        log.error(\"Unable to read from URL %s.\" % url)\n        return None\n    else:\n        # Read lines from the temporary file.\n        lines = text_lines_from_local_file(local_file, remote=True)\n        os.remove(local_file)\n\n        line_nb = len(lines)\n        word_nb = 0\n        for line in lines:\n            word_nb += len(re.findall(\"\\S+\", line))\n\n        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))\n\n        return lines\n\n\ndef executable_exists(executable):\n    \"\"\"Tests if an executable is available on the system.\"\"\"\n    for directory in os.getenv(\"PATH\").split(\":\"):\n        if os.path.exists(os.path.join(directory, executable)):\n            return True\n    return False\n\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/inveniosoftware/invenio/blob/d64fffea5a05775ba2db65cba5408c2e9635e354",
        "file_path": "/invenio/legacy/bibclassify/engine.py",
        "source": "# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.\n#\n# Invenio is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\"\"\"\nBibClassify engine.\n\nThis module is the main module of BibClassify. its two main methods are\noutput_keywords_for_sources and get_keywords_from_text. The first one output\nkeywords for a list of sources (local files or URLs, PDF or text) while the\nsecond one outputs the keywords for text lines (which are obtained using the\nmodule bibclassify_text_normalizer).\n\nThis module also takes care of the different outputs (text, MARCXML or HTML).\nBut unfortunately there is a confusion between running in a standalone mode\nand producing output suitable for printing, and running in a web-based\nmode where the webtemplate is used. For the moment the pieces of the representation\ncode are left in this module.\n\"\"\"\n\nfrom __future__ import print_function\n\nimport os\nfrom six import iteritems\nimport config as bconfig\n\nfrom invenio.legacy.bibclassify import ontology_reader as reader\nimport text_extractor as extractor\nimport text_normalizer as normalizer\nimport keyword_analyzer as keyworder\nimport acronym_analyzer as acronymer\n\nfrom invenio.utils.url import make_user_agent_string\nfrom invenio.utils.text import encode_for_xml\n\nlog = bconfig.get_logger(\"bibclassify.engine\")\n\n# ---------------------------------------------------------------------\n#                          API\n# ---------------------------------------------------------------------\n\n\ndef output_keywords_for_sources(input_sources, taxonomy_name, output_mode=\"text\",\n                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,\n                                match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,\n                                api=False, **kwargs):\n    \"\"\"Output the keywords for each source in sources.\"\"\"\n\n    # Inner function which does the job and it would be too much work to\n    # refactor the call (and it must be outside the loop, before it did\n    # not process multiple files)\n    def process_lines():\n        if output_mode == \"text\":\n            print(\"Input file: %s\" % source)\n\n        output = get_keywords_from_text(\n            text_lines,\n            taxonomy_name,\n            output_mode=output_mode,\n            output_limit=output_limit,\n            spires=spires,\n            match_mode=match_mode,\n            no_cache=no_cache,\n            with_author_keywords=with_author_keywords,\n            rebuild_cache=rebuild_cache,\n            only_core_tags=only_core_tags,\n            extract_acronyms=extract_acronyms\n        )\n        if api:\n            return output\n        else:\n            if isinstance(output, dict):\n                for i in output:\n                    print(output[i])\n\n    # Get the fulltext for each source.\n    for entry in input_sources:\n        log.info(\"Trying to read input file %s.\" % entry)\n        text_lines = None\n        source = \"\"\n        if os.path.isdir(entry):\n            for filename in os.listdir(entry):\n                if filename.startswith('.'):\n                    continue\n                filename = os.path.join(entry, filename)\n                if os.path.isfile(filename):\n                    text_lines = extractor.text_lines_from_local_file(filename)\n                    if text_lines:\n                        source = filename\n                        process_lines()\n        elif os.path.isfile(entry):\n            text_lines = extractor.text_lines_from_local_file(entry)\n            if text_lines:\n                source = os.path.basename(entry)\n                process_lines()\n        else:\n            # Treat as a URL.\n            text_lines = extractor.text_lines_from_url(entry,\n                                                       user_agent=make_user_agent_string(\"BibClassify\"))\n            if text_lines:\n                source = entry.split(\"/\")[-1]\n                process_lines()\n\n\ndef get_keywords_from_local_file(local_file, taxonomy_name, output_mode=\"text\",\n                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,\n                                 match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,\n                                 **kwargs):\n    \"\"\"Outputs keywords reading a local file. Arguments and output are the same\n    as for :see: get_keywords_from_text() \"\"\"\n\n    log.info(\"Analyzing keywords for local file %s.\" % local_file)\n    text_lines = extractor.text_lines_from_local_file(local_file)\n\n    return get_keywords_from_text(text_lines,\n                                  taxonomy_name,\n                                  output_mode=output_mode,\n                                  output_limit=output_limit,\n                                  spires=spires,\n                                  match_mode=match_mode,\n                                  no_cache=no_cache,\n                                  with_author_keywords=with_author_keywords,\n                                  rebuild_cache=rebuild_cache,\n                                  only_core_tags=only_core_tags,\n                                  extract_acronyms=extract_acronyms)\n\n\ndef get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\",\n                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,\n                           spires=False, match_mode=\"full\", no_cache=False,\n                           with_author_keywords=False, rebuild_cache=False,\n                           only_core_tags=False, extract_acronyms=False,\n                           **kwargs):\n    \"\"\"Extract keywords from the list of strings\n\n    :param text_lines: list of strings (will be normalized before being\n        joined into one string)\n    :param taxonomy_name: string, name of the taxonomy_name\n    :param output_mode: string - text|html|marcxml|raw\n    :param output_limit: int\n    :param spires: boolean, if True marcxml output reflect spires codes.\n    :param match_mode: str - partial|full; in partial mode only\n        beginning of the fulltext is searched.\n    :param no_cache: boolean, means loaded definitions will not be saved.\n    :param with_author_keywords: boolean, extract keywords from the pdfs.\n    :param rebuild_cache: boolean\n    :param only_core_tags: boolean\n    :return: if output_mode=raw, it will return\n        (single_keywords, composite_keywords, author_keywords, acronyms)\n        for other output modes it returns formatted string\n    \"\"\"\n\n    cache = reader.get_cache(taxonomy_name)\n    if not cache:\n        reader.set_cache(taxonomy_name,\n                         reader.get_regular_expressions(taxonomy_name,\n                                                        rebuild=rebuild_cache,\n                                                        no_cache=no_cache))\n        cache = reader.get_cache(taxonomy_name)\n    _skw = cache[0]\n    _ckw = cache[1]\n    text_lines = normalizer.cut_references(text_lines)\n    fulltext = normalizer.normalize_fulltext(\"\\n\".join(text_lines))\n\n    if match_mode == \"partial\":\n        fulltext = _get_partial_text(fulltext)\n    author_keywords = None\n    if with_author_keywords:\n        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)\n    acronyms = {}\n    if extract_acronyms:\n        acronyms = extract_abbreviations(fulltext)\n\n    single_keywords = extract_single_keywords(_skw, fulltext)\n    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)\n\n    if only_core_tags:\n        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))\n        composite_keywords = _filter_core_keywors(composite_keywords)\n    else:\n        # Filter out the \"nonstandalone\" keywords\n        single_keywords = clean_before_output(single_keywords)\n    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,\n                               author_keywords, acronyms, output_mode, output_limit,\n                               spires, only_core_tags)\n\n\ndef extract_single_keywords(skw_db, fulltext):\n    \"\"\"Find single keywords in the fulltext\n    :var skw_db: list of KeywordToken objects\n    :var fulltext: string, which will be searched\n    :return : dictionary of matches in a format {\n            <keyword object>, [[position, position...], ],\n            ..\n            }\n            or empty {}\n    \"\"\"\n    return keyworder.get_single_keywords(skw_db, fulltext) or {}\n\n\ndef extract_composite_keywords(ckw_db, fulltext, skw_spans):\n    \"\"\"Returns a list of composite keywords bound with the number of\n    occurrences found in the text string.\n    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)\n    :var fulltext: string to search in\n    :skw_spans: dictionary of already identified single keywords\n    :return : dictionary of matches in a format {\n            <keyword object>, [[position, position...], [info_about_matches] ],\n            ..\n            }\n            or empty {}\n    \"\"\"\n    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}\n\n\ndef extract_abbreviations(fulltext):\n    \"\"\"Extract acronyms from the fulltext\n    :var fulltext: utf-8 string\n    :return: dictionary of matches in a formt {\n          <keyword object>, [matched skw or ckw object, ....]\n          }\n          or empty {}\n    \"\"\"\n    acronyms = {}\n    K = reader.KeywordToken\n    for k, v in acronymer.get_acronyms(fulltext).items():\n        acronyms[K(k, type='acronym')] = v\n    return acronyms\n\n\ndef extract_author_keywords(skw_db, ckw_db, fulltext):\n    \"\"\"Finds out human defined keyowrds in a text string. Searches for\n    the string \"Keywords:\" and its declinations and matches the\n    following words.\n\n    :var skw_db: list single kw object\n    :var ckw_db: list of composite kw objects\n    :var fulltext: utf-8 string\n    :return: dictionary of matches in a formt {\n          <keyword object>, [matched skw or ckw object, ....]\n          }\n          or empty {}\n    \"\"\"\n    akw = {}\n    K = reader.KeywordToken\n    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():\n        akw[K(k, type='author-kw')] = v\n    return akw\n\n\n# ---------------------------------------------------------------------\n#                          presentation functions\n# ---------------------------------------------------------------------\n\n\ndef get_keywords_output(single_keywords, composite_keywords, taxonomy_name,\n                        author_keywords=None, acronyms=None, style=\"text\", output_limit=0,\n                        spires=False, only_core_tags=False):\n    \"\"\"Returns a formatted string representing the keywords according\n    to the chosen style. This is the main routing call, this function will\n    also strip unwanted keywords before output and limits the number\n    of returned keywords\n    :var single_keywords: list of single keywords\n    :var composite_keywords: list of composite keywords\n    :var taxonomy_name: string, taxonomy name\n    :keyword author_keywords: dictionary of author keywords extracted from fulltext\n    :keyword acronyms: dictionary of extracted acronyms\n    :keyword style: text|html|marc\n    :keyword output_limit: int, number of maximum keywords printed (it applies\n            to single and composite keywords separately)\n    :keyword spires: boolen meaning spires output style\n    :keyword only_core_tags: boolean\n    \"\"\"\n    categories = {}\n    # sort the keywords, but don't limit them (that will be done later)\n    single_keywords_p = _sort_kw_matches(single_keywords)\n\n    composite_keywords_p = _sort_kw_matches(composite_keywords)\n\n    for w in single_keywords_p:\n        categories[w[0].concept] = w[0].type\n    for w in single_keywords_p:\n        categories[w[0].concept] = w[0].type\n\n    complete_output = _output_complete(single_keywords_p, composite_keywords_p,\n                                       author_keywords, acronyms, spires,\n                                       only_core_tags, limit=output_limit)\n    functions = {\"text\": _output_text, \"marcxml\": _output_marc, \"html\":\n                 _output_html, \"dict\": _output_dict}\n    my_styles = {}\n\n    for s in style:\n        if s != \"raw\":\n            my_styles[s] = functions[s](complete_output, categories)\n        else:\n            if output_limit > 0:\n                my_styles[\"raw\"] = (_kw(_sort_kw_matches(single_keywords, output_limit)),\n                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),\n                                    author_keywords,  # this we don't limit (?)\n                                    _kw(_sort_kw_matches(acronyms, output_limit)))\n            else:\n                my_styles[\"raw\"] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)\n\n    return my_styles\n\n\ndef build_marc(recid, single_keywords, composite_keywords,\n               spires=False, author_keywords=None, acronyms=None):\n    \"\"\"Create xml record.\n\n    :var recid: ingeter\n    :var single_keywords: dictionary of kws\n    :var composite_keywords: dictionary of kws\n    :keyword spires: please don't use, left for historical\n        reasons\n    :keyword author_keywords: dictionary of extracted keywords\n    :keyword acronyms: dictionary of extracted acronyms\n    :return: str, marxml\n    \"\"\"\n    output = ['<collection><record>\\n'\n              '<controlfield tag=\"001\">%s</controlfield>' % recid]\n\n    # no need to sort\n    single_keywords = single_keywords.items()\n    composite_keywords = composite_keywords.items()\n\n    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))\n\n    output.append('</record></collection>')\n\n    return '\\n'.join(output)\n\n\ndef _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,\n                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,\n                 provenience='BibClassify'):\n    \"\"\"Output the keywords in the MARCXML format.\n\n    :var skw_matches: list of single keywords\n    :var ckw_matches: list of composite keywords\n    :var author_keywords: dictionary of extracted author keywords\n    :var acronyms: dictionary of acronyms\n    :var spires: boolean, True=generate spires output - BUT NOTE: it is\n            here only not to break compatibility, in fact spires output\n            should never be used for xml because if we read marc back\n            into the KeywordToken objects, we would not find them\n    :keyword provenience: string that identifies source (authority) that\n        assigned the contents of the field\n    :return: string, formatted MARC\"\"\"\n\n    kw_template = ('<datafield tag=\"%s\" ind1=\"%s\" ind2=\"%s\">\\n'\n                   '    <subfield code=\"2\">%s</subfield>\\n'\n                   '    <subfield code=\"a\">%s</subfield>\\n'\n                   '    <subfield code=\"n\">%s</subfield>\\n'\n                   '    <subfield code=\"9\">%s</subfield>\\n'\n                   '</datafield>\\n')\n\n    output = []\n\n    tag, ind1, ind2 = _parse_marc_code(kw_field)\n    for keywords in (output_complete[\"Single keywords\"], output_complete[\"Core keywords\"]):\n        for kw in keywords:\n            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),\n                                         encode_for_xml(kw), keywords[kw],\n                                         encode_for_xml(categories[kw])))\n\n    for field, keywords in ((auth_field, output_complete[\"Author keywords\"]),\n                            (acro_field, output_complete[\"Acronyms\"])):\n        if keywords and len(keywords) and field:  # field='' we shall not save the keywords\n            tag, ind1, ind2 = _parse_marc_code(field)\n            for kw, info in keywords.items():\n                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),\n                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))\n\n    return \"\".join(output)\n\n\ndef _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,\n                     acronyms=None, spires=False, only_core_tags=False,\n                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):\n\n    if limit:\n        resized_skw = skw_matches[0:limit]\n        resized_ckw = ckw_matches[0:limit]\n    else:\n        resized_skw = skw_matches\n        resized_ckw = ckw_matches\n\n    results = {\"Core keywords\": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}\n\n    if not only_core_tags:\n        results[\"Author keywords\"] = _get_author_keywords(author_keywords, spires=spires)\n        results[\"Composite keywords\"] = _get_compositekws(resized_ckw, spires=spires)\n        results[\"Single keywords\"] = _get_singlekws(resized_skw, spires=spires)\n        results[\"Field codes\"] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)\n        results[\"Acronyms\"] = _get_acronyms(acronyms)\n\n    return results\n\n\ndef _output_dict(complete_output, categories):\n    return {\n        \"complete_output\": complete_output,\n        \"categories\": categories\n    }\n\n\ndef _output_text(complete_output, categories):\n    \"\"\"Output the results obtained in text format.\n\n\n    :return: str, html formatted output\n    \"\"\"\n    output = \"\"\n\n    for result in complete_output:\n        list_result = complete_output[result]\n        if list_result:\n            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],\n                                        reverse=True)\n            output += \"\\n\\n{0}:\\n\".format(result)\n            for element in list_result_sorted:\n                output += \"\\n{0} {1}\".format(list_result[element], element)\n\n    output += \"\\n--\\n{0}\".format(_signature())\n\n    return output\n\n\ndef _output_html(complete_output, categories):\n    \"\"\"Output the same as txt output does, but HTML formatted.\n\n    :var skw_matches: sorted list of single keywords\n    :var ckw_matches: sorted list of composite keywords\n    :var author_keywords: dictionary of extracted author keywords\n    :var acronyms: dictionary of acronyms\n    :var spires: boolean\n    :var only_core_tags: boolean\n    :keyword limit: int, number of printed keywords\n    :return: str, html formatted output\n    \"\"\"\n    return \"\"\"<html>\n    <head>\n      <title>Automatically generated keywords by bibclassify</title>\n    </head>\n    <body>\n    {0}\n    </body>\n    </html>\"\"\".format(\n        _output_text(complete_output).replace('\\n', '<br>')\n    ).replace('\\n', '')\n\n\ndef _get_singlekws(skw_matches, spires=False):\n    \"\"\"\n    :var skw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: list of formatted keywords\n    \"\"\"\n    output = {}\n    for single_keyword, info in skw_matches:\n        output[single_keyword.output(spires)] = len(info[0])\n    return output\n\n\ndef _get_compositekws(ckw_matches, spires=False):\n    \"\"\"\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: list of formatted keywords\n    \"\"\"\n    output = {}\n    for composite_keyword, info in ckw_matches:\n        output[composite_keyword.output(spires)] = {\"numbers\": len(info[0]),\n                                                    \"details\": info[1]}\n    return output\n\n\ndef _get_acronyms(acronyms):\n    \"\"\"Return a formatted list of acronyms.\"\"\"\n    acronyms_str = {}\n    if acronyms:\n        for acronym, expansions in iteritems(acronyms):\n            expansions_str = \", \".join([\"%s (%d)\" % expansion\n                                        for expansion in expansions])\n            acronyms_str[acronym] = expansions_str\n\n    return acronyms\n\n\ndef _get_author_keywords(author_keywords, spires=False):\n    \"\"\"Format the output for the author keywords.\n\n    :return: list of formatted author keywors\n    \"\"\"\n    out = {}\n    if author_keywords:\n        for keyword, matches in author_keywords.items():\n            skw_matches = matches[0]  # dictionary of single keywords\n            ckw_matches = matches[1]  # dict of composite keywords\n            matches_str = []\n            for ckw, spans in ckw_matches.items():\n                matches_str.append(ckw.output(spires))\n            for skw, spans in skw_matches.items():\n                matches_str.append(skw.output(spires))\n            if matches_str:\n                out[keyword] = matches_str\n            else:\n                out[keyword] = 0\n\n    return out\n\n\ndef _get_fieldcodes(skw_matches, ckw_matches, spires=False):\n    \"\"\"Return the output for the field codes.\n\n    :var skw_matches: dict of {keyword: [info,...]}\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: string\"\"\"\n    fieldcodes = {}\n    output = {}\n\n    for skw, _ in skw_matches:\n        for fieldcode in skw.fieldcodes:\n            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))\n    for ckw, _ in ckw_matches:\n\n        if len(ckw.fieldcodes):\n            for fieldcode in ckw.fieldcodes:\n                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))\n        else:  # inherit field-codes from the composites\n            for kw in ckw.getComponents():\n                for fieldcode in kw.fieldcodes:\n                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))\n                    fieldcodes.setdefault('*', set()).add(kw.output(spires))\n\n    for fieldcode, keywords in fieldcodes.items():\n        output[fieldcode] = ', '.join(keywords)\n\n    return output\n\n\ndef _get_core_keywords(skw_matches, ckw_matches, spires=False):\n    \"\"\"Return the output for the field codes.\n\n    :var skw_matches: dict of {keyword: [info,...]}\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: set of formatted core keywords\n    \"\"\"\n    output = {}\n    category = {}\n\n    def _get_value_kw(kw):\n        \"\"\"Help to sort the Core keywords.\"\"\"\n        i = 0\n        while kw[i].isdigit():\n            i += 1\n        if i > 0:\n            return int(kw[:i])\n        else:\n            return 0\n\n    for skw, info in skw_matches:\n        if skw.core:\n            output[skw.output(spires)] = len(info[0])\n            category[skw.output(spires)] = skw.type\n    for ckw, info in ckw_matches:\n        if ckw.core:\n            output[ckw.output(spires)] = len(info[0])\n        else:\n            #test if one of the components is  not core\n            i = 0\n            for c in ckw.getComponents():\n                if c.core:\n                    output[c.output(spires)] = info[1][i]\n                i += 1\n    return output\n\n\ndef _filter_core_keywors(keywords):\n    matches = {}\n    for kw, info in keywords.items():\n        if kw.core:\n            matches[kw] = info\n    return matches\n\n\ndef _signature():\n    \"\"\"Print out the bibclassify signature.\n\n    #todo: add information about taxonomy, rdflib\"\"\"\n\n    return 'bibclassify v%s' % (bconfig.VERSION,)\n\n\ndef clean_before_output(kw_matches):\n    \"\"\"Return a clean copy of the keywords data structure.\n\n    Stripped off the standalone and other unwanted elements\"\"\"\n    filtered_kw_matches = {}\n\n    for kw_match, info in iteritems(kw_matches):\n        if not kw_match.nostandalone:\n            filtered_kw_matches[kw_match] = info\n\n    return filtered_kw_matches\n\n# ---------------------------------------------------------------------\n#                          helper functions\n# ---------------------------------------------------------------------\n\n\ndef _skw_matches_comparator(kw0, kw1):\n    \"\"\"\n    Compare 2 single keywords objects.\n\n    First by the number of their spans (ie. how many times they were found),\n    if it is equal it compares them by lenghts of their labels.\n    \"\"\"\n    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))\n    if list_comparison:\n        return list_comparison\n\n    if kw0[0].isComposite() and kw1[0].isComposite():\n        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])\n        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])\n        component_comparison = cmp(component_avg1, component_avg0)\n        if component_comparison:\n            return component_comparison\n\n    return cmp(len(str(kw1[0])), len(str(kw0[0])))\n\n\ndef _kw(keywords):\n    \"\"\"Turn list of keywords into dictionary.\"\"\"\n    r = {}\n    for k, v in keywords:\n        r[k] = v\n    return r\n\n\ndef _sort_kw_matches(skw_matches, limit=0):\n    \"\"\"Return a resized version of keywords to the given length.\"\"\"\n    sorted_keywords = list(skw_matches.items())\n    sorted_keywords.sort(_skw_matches_comparator)\n    return limit and sorted_keywords[:limit] or sorted_keywords\n\n\ndef _get_partial_text(fulltext):\n    \"\"\"\n    Return a short version of the fulltext used with the partial matching mode.\n\n    The version is composed of 20% in the beginning and 20% in the middle of the\n    text.\"\"\"\n    length = len(fulltext)\n\n    get_index = lambda x: int(float(x) / 100 * length)\n\n    partial_text = [fulltext[get_index(start):get_index(end)]\n                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]\n\n    return \"\\n\".join(partial_text)\n\n\ndef save_keywords(filename, xml):\n    tmp_dir = os.path.dirname(filename)\n    if not os.path.isdir(tmp_dir):\n        os.mkdir(tmp_dir)\n\n    file_desc = open(filename, \"w\")\n    file_desc.write(xml)\n    file_desc.close()\n\n\ndef get_tmp_file(recid):\n    tmp_directory = \"%s/bibclassify\" % bconfig.CFG_TMPDIR\n    if not os.path.isdir(tmp_directory):\n        os.mkdir(tmp_directory)\n    filename = \"bibclassify_%s.xml\" % recid\n    abs_path = os.path.join(tmp_directory, filename)\n    return abs_path\n\n\ndef _parse_marc_code(field):\n    \"\"\"Parse marc field and return default indicators if not filled in.\"\"\"\n    field = str(field)\n    if len(field) < 4:\n        raise Exception('Wrong field code: %s' % field)\n    else:\n        field += '__'\n    tag = field[0:3]\n    ind1 = field[3].replace('_', '')\n    ind2 = field[4].replace('_', '')\n    return tag, ind1, ind2\n\n\nif __name__ == \"__main__\":\n    log.error(\"Please use bibclassify_cli from now on.\")\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/inveniosoftware/invenio/blob/d64fffea5a05775ba2db65cba5408c2e9635e354",
        "file_path": "/invenio/legacy/bibclassify/ontology_reader.py",
        "source": "# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015 CERN.\n#\n# Invenio is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\n\"\"\"BibClassify ontology reader.\n\nThe ontology reader reads currently either a RDF/SKOS taxonomy or a\nsimple controlled vocabulary file (1 word per line). The first role of\nthis module is to manage the cached version of the ontology file. The\nsecond role is to hold all methods responsible for the creation of\nregular expressions. These methods are grammatically related as we take\ncare of different forms of the same words.  The grammatical rules can be\nconfigured via the configuration file.\n\nThe main method from this module is get_regular_expressions.\n\"\"\"\n\nfrom __future__ import print_function\n\nfrom datetime import datetime, timedelta\nfrom six import iteritems\nfrom six.moves import cPickle\n\nimport os\nimport re\nimport sys\nimport tempfile\nimport time\nimport urllib2\nimport traceback\nimport xml.sax\nimport thread\nimport rdflib\n\nfrom invenio.legacy.bibclassify import config as bconfig\nfrom invenio.modules.classifier.errors import TaxonomyError\n\nlog = bconfig.get_logger(\"bibclassify.ontology_reader\")\nfrom invenio import config\n\nfrom invenio.modules.classifier.registry import taxonomies\n\n# only if not running in a stanalone mode\nif bconfig.STANDALONE:\n    dbquery = None\n    from urllib2 import urlopen\nelse:\n    from invenio.legacy import dbquery\n    from invenio.utils.url import make_invenio_opener\n\n    urlopen = make_invenio_opener('BibClassify').open\n\n_contains_digit = re.compile(\"\\d\")\n_starts_with_non = re.compile(\"(?i)^non[a-z]\")\n_starts_with_anti = re.compile(\"(?i)^anti[a-z]\")\n_split_by_punctuation = re.compile(\"(\\W+)\")\n\n_CACHE = {}\n\n\ndef get_cache(taxonomy_id):\n    \"\"\"Return thread-safe cache for the given taxonomy id.\n\n    :param taxonomy_id: identifier of the taxonomy\n    :type taxonomy_id: str\n\n    :return: dictionary object (empty if no taxonomy_id\n        is found), you must not change anything inside it.\n        Create a new dictionary and use set_cache if you want\n        to update the cache!\n    \"\"\"\n    # Because of a standalone mode, we don't use the\n    # invenio.data_cacher.DataCacher, but it has no effect\n    # on proper functionality.\n\n    if taxonomy_id in _CACHE:\n        ctime, taxonomy = _CACHE[taxonomy_id]\n\n        # check it is fresh version\n        onto_name, onto_path, onto_url = _get_ontology(taxonomy_id)\n        cache_path = _get_cache_path(onto_name)\n\n        # if source exists and is newer than the cache hold in memory\n        if os.path.isfile(onto_path) and os.path.getmtime(onto_path) > ctime:\n            log.info('Forcing taxonomy rebuild as cached'\n                     ' version is newer/updated.')\n            return {}  # force cache rebuild\n\n        # if cache exists and is newer than the cache hold in memory\n        if os.path.isfile(cache_path) and os.path.getmtime(cache_path) > ctime:\n            log.info('Forcing taxonomy rebuild as source'\n                     ' file is newer/updated.')\n            return {}\n        log.info('Taxonomy retrieved from cache')\n        return taxonomy\n    return {}\n\n\ndef set_cache(taxonomy_id, contents):\n    \"\"\"Update cache in a thread-safe manner.\"\"\"\n    lock = thread.allocate_lock()\n    lock.acquire()\n    try:\n        _CACHE[taxonomy_id] = (time.time(), contents)\n    finally:\n        lock.release()\n\n\ndef get_regular_expressions(taxonomy_name, rebuild=False, no_cache=False):\n    \"\"\"Return a list of patterns compiled from the RDF/SKOS ontology.\n\n    Uses cache if it exists and if the taxonomy hasn't changed.\n    \"\"\"\n    # Translate the ontology name into a local path. Check if the name\n    # relates to an existing ontology.\n    onto_name, onto_path, onto_url = _get_ontology(taxonomy_name)\n    if not onto_path:\n        raise TaxonomyError(\"Unable to locate the taxonomy: '%s'.\"\n                            % taxonomy_name)\n\n    cache_path = _get_cache_path(onto_name)\n    log.debug('Taxonomy discovered, now we load it '\n              '(from cache: %s, onto_path: %s, cache_path: %s)'\n              % (not no_cache, onto_path, cache_path))\n\n    if os.access(cache_path, os.R_OK):\n        if os.access(onto_path, os.R_OK):\n            if rebuild or no_cache:\n                log.debug(\"Cache generation was manually forced.\")\n                return _build_cache(onto_path, skip_cache=no_cache)\n        else:\n            # ontology file not found. Use the cache instead.\n            log.warning(\"The ontology couldn't be located. However \"\n                        \"a cached version of it is available. Using it as a \"\n                        \"reference.\")\n            return _get_cache(cache_path, source_file=onto_path)\n\n        if (os.path.getmtime(cache_path) >\n                os.path.getmtime(onto_path)):\n            # Cache is more recent than the ontology: use cache.\n            log.debug(\"Normal situation, cache is older than ontology,\"\n                      \" so we load it from cache\")\n            return _get_cache(cache_path, source_file=onto_path)\n        else:\n            # Ontology is more recent than the cache: rebuild cache.\n            log.warning(\"Cache '%s' is older than '%s'. \"\n                        \"We will rebuild the cache\" %\n                        (cache_path, onto_path))\n            return _build_cache(onto_path, skip_cache=no_cache)\n\n    elif os.access(onto_path, os.R_OK):\n        if not no_cache and\\\n                os.path.exists(cache_path) and\\\n                not os.access(cache_path, os.W_OK):\n            raise TaxonomyError('We cannot read/write into: %s. '\n                                'Aborting!' % cache_path)\n        elif not no_cache and os.path.exists(cache_path):\n            log.warning('Cache %s exists, but is not readable!' % cache_path)\n        log.info(\"Cache not available. Building it now: %s\" % onto_path)\n        return _build_cache(onto_path, skip_cache=no_cache)\n\n    else:\n        raise TaxonomyError(\"We miss both source and cache\"\n                            \" of the taxonomy: %s\" % taxonomy_name)\n\n\ndef _get_remote_ontology(onto_url, time_difference=None):\n    \"\"\"Check if the online ontology is more recent than the local ontology.\n\n    If yes, try to download and store it in Invenio's cache directory.\n\n    Return a boolean describing the success of the operation.\n\n    :return: path to the downloaded ontology.\n    \"\"\"\n    if onto_url is None:\n        return False\n\n    dl_dir = ((config.CFG_CACHEDIR or tempfile.gettempdir()) + os.sep +\n              \"bibclassify\" + os.sep)\n    if not os.path.exists(dl_dir):\n        os.mkdir(dl_dir)\n\n    local_file = dl_dir + os.path.basename(onto_url)\n    remote_modif_time = _get_last_modification_date(onto_url)\n    try:\n        local_modif_seconds = os.path.getmtime(local_file)\n    except OSError:\n        # The local file does not exist. Download the ontology.\n        download = True\n        log.info(\"The local ontology could not be found.\")\n    else:\n        local_modif_time = datetime(*time.gmtime(local_modif_seconds)[0:6])\n        # Let's set a time delta of 1 hour and 10 minutes.\n        time_difference = time_difference or timedelta(hours=1, minutes=10)\n        download = remote_modif_time > local_modif_time + time_difference\n        if download:\n            log.info(\"The remote ontology '%s' is more recent \"\n                     \"than the local ontology.\" % onto_url)\n\n    if download:\n        if not _download_ontology(onto_url, local_file):\n            log.warning(\"Error downloading the ontology from: %s\" % onto_url)\n\n    return local_file\n\n\ndef _get_ontology(ontology):\n    \"\"\"Return the (name, path, url) to the short ontology name.\n\n    :param ontology: name of the ontology or path to the file or url.\n    \"\"\"\n    onto_name = onto_path = onto_url = None\n\n    # first assume we got the path to the file\n    if os.path.exists(ontology):\n        onto_name = os.path.split(os.path.abspath(ontology))[1]\n        onto_path = os.path.abspath(ontology)\n        onto_url = \"\"\n    else:\n        # if not, try to find it in a known locations\n        discovered_file = _discover_ontology(ontology)\n        if discovered_file:\n            onto_name = os.path.split(discovered_file)[1]\n            onto_path = discovered_file\n            # i know, this sucks\n            x = ontology.lower()\n            if \"http:\" in x or \"https:\" in x or \"ftp:\" in x or \"file:\" in x:\n                onto_url = ontology\n            else:\n                onto_url = \"\"\n        else:\n            # not found, look into a database\n            # (it is last because when bibclassify\n            # runs in a standalone mode,\n            # it has no database - [rca, old-heritage]\n            if not bconfig.STANDALONE:\n                result = dbquery.run_sql(\"SELECT name, location from clsMETHOD WHERE name LIKE %s\",\n                                         ('%' + ontology + '%',))\n                for onto_short_name, url in result:\n                    onto_name = onto_short_name\n                    onto_path = _get_remote_ontology(url)\n                    onto_url = url\n\n    return (onto_name, onto_path, onto_url)\n\n\ndef _discover_ontology(ontology_name):\n    \"\"\"Look for the file in a known places.\n\n    Inside invenio/etc/bibclassify and a few other places\n    like current directory.\n\n    :param ontology: name or path name or url\n    :type ontology: str\n\n    :return: absolute path of a file if found, or None\n    \"\"\"\n    last_part = os.path.split(os.path.abspath(ontology_name))[1]\n    if last_part in taxonomies:\n        return taxonomies.get(last_part)\n    elif last_part + \".rdf\" in taxonomies:\n        return taxonomies.get(last_part + \".rdf\")\n    else:\n        log.debug(\"No taxonomy with pattern '%s' found\" % ontology_name)\n\n    # LEGACY\n    possible_patterns = [last_part, last_part.lower()]\n    if not last_part.endswith('.rdf'):\n        possible_patterns.append(last_part + '.rdf')\n    places = [config.CFG_CACHEDIR,\n              config.CFG_ETCDIR,\n              os.path.join(config.CFG_CACHEDIR, \"bibclassify\"),\n              os.path.join(config.CFG_ETCDIR, \"bibclassify\"),\n              os.path.abspath('.'),\n              os.path.abspath(os.path.join(os.path.dirname(__file__),\n                                           \"../../../etc/bibclassify\")),\n              os.path.join(os.path.dirname(__file__), \"bibclassify\"),\n              config.CFG_WEBDIR]\n\n    log.debug(\"Searching for taxonomy using string: %s\" % last_part)\n    log.debug(\"Possible patterns: %s\" % possible_patterns)\n    for path in places:\n\n        try:\n            if os.path.isdir(path):\n                log.debug(\"Listing: %s\" % path)\n                for filename in os.listdir(path):\n                    #log.debug('Testing: %s' % filename)\n                    for pattern in possible_patterns:\n                        filename_lc = filename.lower()\n                        if pattern == filename_lc and\\\n                                os.path.exists(os.path.join(path, filename)):\n                            filepath = os.path.abspath(os.path.join(path,\n                                                                    filename))\n                            if (os.access(filepath, os.R_OK)):\n                                log.debug(\"Found taxonomy at: %s\" % filepath)\n                                return filepath\n                            else:\n                                log.warning('Found taxonony at: %s, but it is'\n                                            ' not readable. '\n                                            'Continue searching...'\n                                            % filepath)\n        except OSError, os_error_msg:\n            log.warning('OS Error when listing path '\n                        '\"%s\": %s' % (str(path), str(os_error_msg)))\n    log.debug(\"No taxonomy with pattern '%s' found\" % ontology_name)\n\n\nclass KeywordToken:\n\n    \"\"\"KeywordToken is a class used for the extracted keywords.\n\n    It can be initialized with values from RDF store or from\n    simple strings. Specialty of this class is that objects are\n    hashable by subject - so in the dictionary two objects with the\n    same subject appears as one -- :see: self.__hash__ and self.__cmp__.\n    \"\"\"\n\n    def __init__(self, subject, store=None, namespace=None, type='HEP'):\n        \"\"\"Initialize KeywordToken with a subject.\n\n        :param subject: string or RDF object\n        :param store: RDF graph object\n                      (will be used to get info about the subject)\n        :param namespace: RDF namespace object, used together with store\n        :param type: type of this keyword.\n        \"\"\"\n        self.id = subject\n        self.type = type\n        self.short_id = subject\n        self.concept = \"\"\n        self.regex = []\n        self.nostandalone = False\n        self.spires = False\n        self.fieldcodes = []\n        self.compositeof = []\n        self.core = False\n        # True means composite keyword\n        self._composite = '#Composite' in subject\n        self.__hash = None\n\n        # the tokens are coming possibly from a normal text file\n        if store is None:\n            subject = subject.strip()\n            self.concept = subject\n            self.regex = _get_searchable_regex(basic=[subject])\n            self.nostandalone = False\n            self.fieldcodes = []\n            self.core = False\n            if subject.find(' ') > -1:\n                self._composite = True\n\n        # definitions from rdf\n        else:\n            self.short_id = self.short_id.split('#')[-1]\n\n            # find alternate names for this label\n            basic_labels = []\n\n            # turn those patterns into regexes only for simple keywords\n            if self._composite is False:\n                try:\n                    for label in store.objects(subject,\n                                               namespace[\"prefLabel\"]):\n                        # XXX shall i make it unicode?\n                        basic_labels.append(str(label))\n                except TypeError:\n                    pass\n                self.concept = basic_labels[0]\n            else:\n                try:\n                    self.concept = str(store.value(subject,\n                                                   namespace[\"prefLabel\"],\n                                                   any=True))\n                except KeyError:\n                    log.warning(\"Keyword with subject %s has no prefLabel.\"\n                                \" We use raw name\" %\n                                self.short_id)\n                    self.concept = self.short_id\n\n            # this is common both to composite and simple keywords\n            try:\n                for label in store.objects(subject, namespace[\"altLabel\"]):\n                    basic_labels.append(str(label))\n            except TypeError:\n                pass\n\n            # hidden labels are special (possibly regex) codes\n            hidden_labels = []\n            try:\n                for label in store.objects(subject, namespace[\"hiddenLabel\"]):\n                    hidden_labels.append(unicode(label))\n            except TypeError:\n                pass\n\n            # compile regular expression that will identify this token\n            self.regex = _get_searchable_regex(basic_labels, hidden_labels)\n\n            try:\n                for note in map(lambda s: str(s).lower().strip(),\n                                store.objects(subject, namespace[\"note\"])):\n                    if note == 'core':\n                        self.core = True\n                    elif note in (\"nostandalone\", \"nonstandalone\"):\n                        self.nostandalone = True\n                    elif 'fc:' in note:\n                        self.fieldcodes.append(note[3:].strip())\n            except TypeError:\n                pass\n\n            # spiresLabel does not have multiple values\n            spires_label = store.value(subject, namespace[\"spiresLabel\"])\n            if spires_label:\n                self.spires = str(spires_label)\n\n        # important for comparisons\n        self.__hash = hash(self.short_id)\n\n        # extract composite parts ids\n        if store is not None and self.isComposite():\n            small_subject = self.id.split(\"#Composite.\")[-1]\n            component_positions = []\n            for label in store.objects(self.id, namespace[\"compositeOf\"]):\n                strlabel = str(label).split(\"#\")[-1]\n                component_name = label.split(\"#\")[-1]\n                component_positions.append((small_subject.find(component_name),\n                                            strlabel))\n            component_positions.sort()\n            if not component_positions:\n                log.error(\"Keyword is marked as composite, \"\n                          \"but no composite components refs found: %s\"\n                          % self.short_id)\n            else:\n                self.compositeof = map(lambda x: x[1], component_positions)\n\n    def refreshCompositeOf(self, single_keywords, composite_keywords,\n                           store=None, namespace=None):\n        \"\"\"Re-check sub-parts of this keyword.\n\n        This should be called after the whole RDF was processed, because\n        it is using a cache of single keywords and if that\n        one is incomplete, you will not identify all parts.\n        \"\"\"\n        def _get_ckw_components(new_vals, label):\n            if label in single_keywords:\n                new_vals.append(single_keywords[label])\n            elif ('Composite.%s' % label) in composite_keywords:\n                for l in composite_keywords['Composite.%s' % label].compositeof:\n                    _get_ckw_components(new_vals, l)\n            elif label in composite_keywords:\n                for l in composite_keywords[label].compositeof:\n                    _get_ckw_components(new_vals, l)\n            else:\n                # One single or composite keyword is missing from the taxonomy.\n                # This is due to an error in the taxonomy description.\n                message = \"The composite term \\\"%s\\\"\"\\\n                          \" should be made of single keywords,\"\\\n                          \" but at least one is missing.\" % self.id\n                if store is not None:\n                    message += \"Needed components: %s\"\\\n                               % list(store.objects(self.id,\n                                      namespace[\"compositeOf\"]))\n                message += \" Missing is: %s\" % label\n                raise TaxonomyError(message)\n\n        if self.compositeof:\n            new_vals = []\n            try:\n                for label in self.compositeof:\n                    _get_ckw_components(new_vals, label)\n                self.compositeof = new_vals\n            except TaxonomyError:\n                # the composites will be empty\n                # (better than to have confusing, partial matches)\n                self.compositeof = []\n                log.error(\n                    'We reset this composite keyword, so that it does not match anything. Please fix the taxonomy.')\n\n    def isComposite(self):\n        \"\"\"Return value of _composite.\"\"\"\n        return self._composite\n\n    def getComponents(self):\n        \"\"\"Return value of compositeof.\"\"\"\n        return self.compositeof\n\n    def getType(self):\n        \"\"\"Return value of type.\"\"\"\n        return self.type\n\n    def setType(self, value):\n        \"\"\"Set value of value.\"\"\"\n        self.type = value\n\n    def __hash__(self):\n        \"\"\"Return _hash.\n\n        This might change in the future but for the moment we want to\n        think that if the concept is the same, then it is the same\n        keyword - this sucks, but it is sort of how it is necessary\n        to use now.\n        \"\"\"\n        return self.__hash\n\n    def __cmp__(self, other):\n        \"\"\"Compare objects using _hash.\"\"\"\n        if self.__hash < other.__hash__():\n            return -1\n        elif self.__hash == other.__hash__():\n            return 0\n        else:\n            return 1\n\n    def __str__(self, spires=False):\n        \"\"\"Return the best output for the keyword.\"\"\"\n        if spires:\n            if self.spires:\n                return self.spires\n            elif self._composite:\n                return self.concept.replace(':', ',')\n            # default action\n        return self.concept\n\n    def output(self, spires=False):\n        \"\"\"Return string representation with spires value.\"\"\"\n        return self.__str__(spires=spires)\n\n    def __repr__(self):\n        \"\"\"Class representation.\"\"\"\n        return \"<KeywordToken: %s>\" % self.short_id\n\n\ndef _build_cache(source_file, skip_cache=False):\n    \"\"\"Build the cached data.\n\n    Either by parsing the RDF taxonomy file or a vocabulary file.\n\n    :param source_file: source file of the taxonomy, RDF file\n    :param skip_cache: if True, build cache will not be\n        saved (pickled) - it is saved as <source_file.db>\n    \"\"\"\n    store = rdflib.ConjunctiveGraph()\n\n    if skip_cache:\n        log.info(\"You requested not to save the cache to disk.\")\n    else:\n        cache_path = _get_cache_path(source_file)\n        cache_dir = os.path.dirname(cache_path)\n        # Make sure we have a cache_dir readable and writable.\n        try:\n            os.makedirs(cache_dir)\n        except:\n            pass\n        if os.access(cache_dir, os.R_OK):\n            if not os.access(cache_dir, os.W_OK):\n                raise TaxonomyError(\"Cache directory exists but is not\"\n                                    \" writable. Check your permissions\"\n                                    \" for: %s\" % cache_dir)\n        else:\n            raise TaxonomyError(\"Cache directory does not exist\"\n                                \" (and could not be created): %s\" % cache_dir)\n\n    timer_start = time.clock()\n\n    namespace = None\n    single_keywords, composite_keywords = {}, {}\n\n    try:\n        log.info(\"Building RDFLib's conjunctive graph from: %s\" % source_file)\n        try:\n            store.parse(source_file)\n        except urllib2.URLError:\n            if source_file[0] == '/':\n                store.parse(\"file://\" + source_file)\n            else:\n                store.parse(\"file:///\" + source_file)\n\n    except rdflib.exceptions.Error as e:\n        log.error(\"Serious error reading RDF file\")\n        log.error(e)\n        log.error(traceback.format_exc())\n        raise rdflib.exceptions.Error(e)\n\n    except (xml.sax.SAXParseException, ImportError) as e:\n        # File is not a RDF file. We assume it is a controlled vocabulary.\n        log.error(e)\n        log.warning(\"The ontology file is probably not a valid RDF file. \\\n            Assuming it is a controlled vocabulary file.\")\n\n        filestream = open(source_file, \"r\")\n        for line in filestream:\n            keyword = line.strip()\n            kt = KeywordToken(keyword)\n            single_keywords[kt.short_id] = kt\n        if not len(single_keywords):\n            raise TaxonomyError('The ontology file is not well formated')\n\n    else:  # ok, no exception happened\n        log.info(\"Now building cache of keywords\")\n        # File is a RDF file.\n        namespace = rdflib.Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n\n        single_count = 0\n        composite_count = 0\n\n        subject_objects = store.subject_objects(namespace[\"prefLabel\"])\n        for subject, pref_label in subject_objects:\n            kt = KeywordToken(subject, store=store, namespace=namespace)\n            if kt.isComposite():\n                composite_count += 1\n                composite_keywords[kt.short_id] = kt\n            else:\n                single_keywords[kt.short_id] = kt\n                single_count += 1\n\n    cached_data = {}\n    cached_data[\"single\"] = single_keywords\n    cached_data[\"composite\"] = composite_keywords\n    cached_data[\"creation_time\"] = time.gmtime()\n    cached_data[\"version_info\"] = {'rdflib': rdflib.__version__,\n                                   'bibclassify': bconfig.VERSION}\n    log.debug(\"Building taxonomy... %d terms built in %.1f sec.\" %\n              (len(single_keywords) + len(composite_keywords),\n               time.clock() - timer_start))\n\n    log.info(\"Total count of single keywords: %d \"\n             % len(single_keywords))\n    log.info(\"Total count of composite keywords: %d \"\n             % len(composite_keywords))\n\n    if not skip_cache:\n        cache_path = _get_cache_path(source_file)\n        cache_dir = os.path.dirname(cache_path)\n        log.debug(\"Writing the cache into: %s\" % cache_path)\n        # test again, it could have changed\n        if os.access(cache_dir, os.R_OK):\n            if os.access(cache_dir, os.W_OK):\n                # Serialize.\n                filestream = None\n                try:\n                    filestream = open(cache_path, \"wb\")\n                except IOError as msg:\n                    # Impossible to write the cache.\n                    log.error(\"Impossible to write cache to '%s'.\"\n                              % cache_path)\n                    log.error(msg)\n                else:\n                    log.debug(\"Writing cache to file %s\" % cache_path)\n                    cPickle.dump(cached_data, filestream, 1)\n                if filestream:\n                    filestream.close()\n\n            else:\n                raise TaxonomyError(\"Cache directory exists but is not \"\n                                    \"writable. Check your permissions \"\n                                    \"for: %s\" % cache_dir)\n        else:\n            raise TaxonomyError(\"Cache directory does not exist\"\n                                \" (and could not be created): %s\" % cache_dir)\n\n    # now when the whole taxonomy was parsed,\n    # find sub-components of the composite kws\n    # it is important to keep this call after the taxonomy was saved,\n    # because we don't  want to pickle regexes multiple times\n    # (as they are must be re-compiled at load time)\n    for kt in composite_keywords.values():\n        kt.refreshCompositeOf(single_keywords, composite_keywords,\n                              store=store, namespace=namespace)\n\n    # house-cleaning\n    if store:\n        store.close()\n\n    return (single_keywords, composite_keywords)\n\n\ndef _capitalize_first_letter(word):\n    \"\"\"Return a regex pattern with the first letter.\n\n    Accepts both lowercase and uppercase.\n    \"\"\"\n    if word[0].isalpha():\n        # These two cases are necessary in order to get a regex pattern\n        # starting with '[xX]' and not '[Xx]'. This allows to check for\n        # colliding regex afterwards.\n        if word[0].isupper():\n            return \"[\" + word[0].swapcase() + word[0] + \"]\" + word[1:]\n        else:\n            return \"[\" + word[0] + word[0].swapcase() + \"]\" + word[1:]\n    return word\n\n\ndef _convert_punctuation(punctuation, conversion_table):\n    \"\"\"Return a regular expression for a punctuation string.\"\"\"\n    if punctuation in conversion_table:\n        return conversion_table[punctuation]\n    return re.escape(punctuation)\n\n\ndef _convert_word(word):\n    \"\"\"Return the plural form of the word if it exists.\n\n    Otherwise return the word itself.\n    \"\"\"\n    out = None\n\n    # Acronyms.\n    if word.isupper():\n        out = word + \"s?\"\n    # Proper nouns or word with digits.\n    elif word.istitle():\n        out = word + \"('?s)?\"\n    elif _contains_digit.search(word):\n        out = word\n\n    if out is not None:\n        return out\n\n    # Words with non or anti prefixes.\n    if _starts_with_non.search(word):\n        word = \"non-?\" + _capitalize_first_letter(_convert_word(word[3:]))\n    elif _starts_with_anti.search(word):\n        word = \"anti-?\" + _capitalize_first_letter(_convert_word(word[4:]))\n\n    if out is not None:\n        return _capitalize_first_letter(out)\n\n    # A few invariable words.\n    if word in bconfig.CFG_BIBCLASSIFY_INVARIABLE_WORDS:\n        return _capitalize_first_letter(word)\n\n    # Some exceptions that would not produce good results with the set of\n    # general_regular_expressions.\n    regexes = bconfig.CFG_BIBCLASSIFY_EXCEPTIONS\n    if word in regexes:\n        return _capitalize_first_letter(regexes[word])\n\n    regexes = bconfig.CFG_BIBCLASSIFY_UNCHANGE_REGULAR_EXPRESSIONS\n    for regex in regexes:\n        if regex.search(word) is not None:\n            return _capitalize_first_letter(word)\n\n    regexes = bconfig.CFG_BIBCLASSIFY_GENERAL_REGULAR_EXPRESSIONS\n    for regex, replacement in regexes:\n        stemmed = regex.sub(replacement, word)\n        if stemmed != word:\n            return _capitalize_first_letter(stemmed)\n\n    return _capitalize_first_letter(word + \"s?\")\n\n\ndef _get_cache(cache_file, source_file=None):\n    \"\"\"Get cached taxonomy using the cPickle module.\n\n    No check is done at that stage.\n\n    :param cache_file: full path to the file holding pickled data\n    :param source_file: if we discover the cache is obsolete, we\n        will build a new cache, therefore we need the source path\n        of the cache\n    :return: (single_keywords, composite_keywords).\n    \"\"\"\n    timer_start = time.clock()\n\n    filestream = open(cache_file, \"rb\")\n    try:\n        cached_data = cPickle.load(filestream)\n        version_info = cached_data['version_info']\n        if version_info['rdflib'] != rdflib.__version__\\\n                or version_info['bibclassify'] != bconfig.VERSION:\n            raise KeyError\n    except (cPickle.UnpicklingError, ImportError,\n            AttributeError, DeprecationWarning, EOFError):\n        log.warning(\"The existing cache in %s is not readable. \"\n                    \"Removing and rebuilding it.\" % cache_file)\n        filestream.close()\n        os.remove(cache_file)\n        return _build_cache(source_file)\n    except KeyError:\n        log.warning(\"The existing cache %s is not up-to-date. \"\n                    \"Removing and rebuilding it.\" % cache_file)\n        filestream.close()\n        os.remove(cache_file)\n        if source_file and os.path.exists(source_file):\n            return _build_cache(source_file)\n        else:\n            log.error(\"The cache contains obsolete data (and it was deleted), \"\n                      \"however I can't build a new cache, the source does not \"\n                      \"exist or is inaccessible! - %s\" % source_file)\n    filestream.close()\n\n    single_keywords = cached_data[\"single\"]\n    composite_keywords = cached_data[\"composite\"]\n\n    # the cache contains only keys of the composite keywords, not the objects\n    # so now let's resolve them into objects\n    for kw in composite_keywords.values():\n        kw.refreshCompositeOf(single_keywords, composite_keywords)\n\n    log.debug(\"Retrieved taxonomy from cache %s created on %s\" %\n              (cache_file, time.asctime(cached_data[\"creation_time\"])))\n\n    log.debug(\"%d terms read in %.1f sec.\" %\n              (len(single_keywords) + len(composite_keywords),\n               time.clock() - timer_start))\n\n    return (single_keywords, composite_keywords)\n\n\ndef _get_cache_path(source_file):\n    \"\"\"Return the path where the cache should be written/located.\n\n    :param onto_name: name of the ontology or the full path\n    :return: string, abs path to the cache file in the tmpdir/bibclassify\n    \"\"\"\n    local_name = os.path.basename(source_file)\n    cache_name = local_name + \".db\"\n    cache_dir = os.path.join(config.CFG_CACHEDIR, \"bibclassify\")\n\n    if not os.path.isdir(cache_dir):\n        os.makedirs(cache_dir)\n\n    return os.path.abspath(os.path.join(cache_dir, cache_name))\n\n\ndef _get_last_modification_date(url):\n    \"\"\"Get the last modification date of the ontology.\"\"\"\n    request = urllib2.Request(url)\n    request.get_method = lambda: \"HEAD\"\n    http_file = urlopen(request)\n    date_string = http_file.headers[\"last-modified\"]\n    parsed = time.strptime(date_string, \"%a, %d %b %Y %H:%M:%S %Z\")\n    return datetime(*(parsed)[0:6])\n\n\ndef _download_ontology(url, local_file):\n    \"\"\"Download the ontology and stores it in CFG_CACHEDIR.\"\"\"\n    log.debug(\"Copying remote ontology '%s' to file '%s'.\" % (url,\n                                                              local_file))\n    try:\n        url_desc = urlopen(url)\n        file_desc = open(local_file, 'w')\n        file_desc.write(url_desc.read())\n        file_desc.close()\n    except IOError as e:\n        print(e)\n        return False\n    except:\n        log.warning(\"Unable to download the ontology. '%s'\" %\n                    sys.exc_info()[0])\n        return False\n    else:\n        log.debug(\"Done copying.\")\n        return True\n\n\ndef _get_searchable_regex(basic=None, hidden=None):\n    \"\"\"Return the searchable regular expressions for the single keyword.\"\"\"\n    # Hidden labels are used to store regular expressions.\n    basic = basic or []\n    hidden = hidden or []\n\n    hidden_regex_dict = {}\n    for hidden_label in hidden:\n        if _is_regex(hidden_label):\n            hidden_regex_dict[hidden_label] = \\\n                re.compile(\n                    bconfig.CFG_BIBCLASSIFY_WORD_WRAP % hidden_label[1:-1]\n                )\n        else:\n            pattern = _get_regex_pattern(hidden_label)\n            hidden_regex_dict[hidden_label] = re.compile(\n                bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern\n            )\n\n    # We check if the basic label (preferred or alternative) is matched\n    # by a hidden label regex. If yes, discard it.\n    regex_dict = {}\n    # Create regex for plural forms and add them to the hidden labels.\n    for label in basic:\n        pattern = _get_regex_pattern(label)\n        regex_dict[label] = re.compile(\n            bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern\n        )\n\n    # Merge both dictionaries.\n    regex_dict.update(hidden_regex_dict)\n\n    return regex_dict.values()\n\n\ndef _get_regex_pattern(label):\n    \"\"\"Return a regular expression of the label.\n\n    This takes care of plural and different kinds of separators.\n    \"\"\"\n    parts = _split_by_punctuation.split(label)\n\n    for index, part in enumerate(parts):\n        if index % 2 == 0:\n            # Word\n            if not parts[index].isdigit() and len(parts[index]) > 1:\n                parts[index] = _convert_word(parts[index])\n        else:\n            # Punctuation\n            if not parts[index + 1]:\n                # The separator is not followed by another word. Treat\n                # it as a symbol.\n                parts[index] = _convert_punctuation(\n                    parts[index],\n                    bconfig.CFG_BIBCLASSIFY_SYMBOLS\n                )\n            else:\n                parts[index] = _convert_punctuation(\n                    parts[index],\n                    bconfig.CFG_BIBCLASSIFY_SEPARATORS\n                )\n\n    return \"\".join(parts)\n\n\ndef _is_regex(string):\n    \"\"\"Check if a concept is a regular expression.\"\"\"\n    return string[0] == \"/\" and string[-1] == \"/\"\n\n\ndef check_taxonomy(taxonomy):\n    \"\"\"Check the consistency of the taxonomy.\n\n    Outputs a list of errors and warnings.\n    \"\"\"\n    log.info(\"Building graph with Python RDFLib version %s\" %\n             rdflib.__version__)\n\n    store = rdflib.ConjunctiveGraph()\n\n    try:\n        store.parse(taxonomy)\n    except:\n        log.error(\"The taxonomy is not a valid RDF file. Are you \"\n                  \"trying to check a controlled vocabulary?\")\n        raise TaxonomyError('Error in RDF file')\n\n    log.info(\"Graph was successfully built.\")\n\n    prefLabel = \"prefLabel\"\n    hiddenLabel = \"hiddenLabel\"\n    altLabel = \"altLabel\"\n    composite = \"composite\"\n    compositeOf = \"compositeOf\"\n    note = \"note\"\n\n    both_skw_and_ckw = []\n\n    # Build a dictionary we will reason on later.\n    uniq_subjects = {}\n    for subject in store.subjects():\n        uniq_subjects[subject] = None\n\n    subjects = {}\n    for subject in uniq_subjects:\n        strsubject = str(subject).split(\"#Composite.\")[-1]\n        strsubject = strsubject.split(\"#\")[-1]\n        if (strsubject == \"http://cern.ch/thesauri/HEPontology.rdf\" or\n           strsubject == \"compositeOf\"):\n            continue\n        components = {}\n        for predicate, value in store.predicate_objects(subject):\n            strpredicate = str(predicate).split(\"#\")[-1]\n            strobject = str(value).split(\"#Composite.\")[-1]\n            strobject = strobject.split(\"#\")[-1]\n            components.setdefault(strpredicate, []).append(strobject)\n        if strsubject in subjects:\n            both_skw_and_ckw.append(strsubject)\n        else:\n            subjects[strsubject] = components\n\n    log.info(\"Taxonomy contains %s concepts.\" % len(subjects))\n\n    no_prefLabel = []\n    multiple_prefLabels = []\n    bad_notes = []\n    # Subjects with no composite or compositeOf predicate\n    lonely = []\n    both_composites = []\n    bad_hidden_labels = {}\n    bad_alt_labels = {}\n    # Problems with composite keywords\n    composite_problem1 = []\n    composite_problem2 = []\n    composite_problem3 = []\n    composite_problem4 = {}\n    composite_problem5 = []\n    composite_problem6 = []\n\n    stemming_collisions = []\n    interconcept_collisions = {}\n\n    for subject, predicates in iteritems(subjects):\n        # No prefLabel or multiple prefLabels\n        try:\n            if len(predicates[prefLabel]) > 1:\n                multiple_prefLabels.append(subject)\n        except KeyError:\n            no_prefLabel.append(subject)\n\n        # Lonely and both composites.\n        if composite not in predicates and compositeOf not in predicates:\n            lonely.append(subject)\n        elif composite in predicates and compositeOf in predicates:\n            both_composites.append(subject)\n\n        # Multiple or bad notes\n        if note in predicates:\n            bad_notes += [(subject, n) for n in predicates[note]\n                          if n not in ('nostandalone', 'core')]\n\n        # Bad hidden labels\n        if hiddenLabel in predicates:\n            for lbl in predicates[hiddenLabel]:\n                if lbl.startswith(\"/\") ^ lbl.endswith(\"/\"):\n                    bad_hidden_labels.setdefault(subject, []).append(lbl)\n\n        # Bad alt labels\n        if altLabel in predicates:\n            for lbl in predicates[altLabel]:\n                if len(re.findall(\"/\", lbl)) >= 2 or \":\" in lbl:\n                    bad_alt_labels.setdefault(subject, []).append(lbl)\n\n        # Check composite\n        if composite in predicates:\n            for ckw in predicates[composite]:\n                if ckw in subjects:\n                    if compositeOf in subjects[ckw]:\n                        if subject not in subjects[ckw][compositeOf]:\n                            composite_problem3.append((subject, ckw))\n                    else:\n                        if ckw not in both_skw_and_ckw:\n                            composite_problem2.append((subject, ckw))\n                else:\n                    composite_problem1.append((subject, ckw))\n\n        # Check compositeOf\n        if compositeOf in predicates:\n            for skw in predicates[compositeOf]:\n                if skw in subjects:\n                    if composite in subjects[skw]:\n                        if subject not in subjects[skw][composite]:\n                            composite_problem6.append((subject, skw))\n                    else:\n                        if skw not in both_skw_and_ckw:\n                            composite_problem5.append((subject, skw))\n                else:\n                    composite_problem4.setdefault(skw, []).append(subject)\n\n        # Check for stemmed labels\n        if compositeOf in predicates:\n            labels = (altLabel, hiddenLabel)\n        else:\n            labels = (prefLabel, altLabel, hiddenLabel)\n\n        patterns = {}\n        for label in [lbl for lbl in labels if lbl in predicates]:\n            for expression in [expr for expr in predicates[label]\n                               if not _is_regex(expr)]:\n                pattern = _get_regex_pattern(expression)\n                interconcept_collisions.setdefault(pattern, []).\\\n                    append((subject, label))\n                if pattern in patterns:\n                    stemming_collisions.append(\n                        (subject,\n                         patterns[pattern],\n                         (label, expression)\n                         )\n                    )\n                else:\n                    patterns[pattern] = (label, expression)\n\n    print(\"\\n==== ERRORS ====\")\n\n    if no_prefLabel:\n        print(\"\\nConcepts with no prefLabel: %d\" % len(no_prefLabel))\n        print(\"\\n\".join([\"   %s\" % subj for subj in no_prefLabel]))\n    if multiple_prefLabels:\n        print((\"\\nConcepts with multiple prefLabels: %d\" %\n               len(multiple_prefLabels)))\n        print(\"\\n\".join([\"   %s\" % subj for subj in multiple_prefLabels]))\n    if both_composites:\n        print((\"\\nConcepts with both composite properties: %d\" %\n               len(both_composites)))\n        print(\"\\n\".join([\"   %s\" % subj for subj in both_composites]))\n    if bad_hidden_labels:\n        print(\"\\nConcepts with bad hidden labels: %d\" % len(bad_hidden_labels))\n        for kw, lbls in iteritems(bad_hidden_labels):\n            print(\"   %s:\" % kw)\n            print(\"\\n\".join([\"      '%s'\" % lbl for lbl in lbls]))\n    if bad_alt_labels:\n        print(\"\\nConcepts with bad alt labels: %d\" % len(bad_alt_labels))\n        for kw, lbls in iteritems(bad_alt_labels):\n            print(\"   %s:\" % kw)\n            print(\"\\n\".join([\"      '%s'\" % lbl for lbl in lbls]))\n    if both_skw_and_ckw:\n        print((\"\\nKeywords that are both skw and ckw: %d\" %\n               len(both_skw_and_ckw)))\n        print(\"\\n\".join([\"   %s\" % subj for subj in both_skw_and_ckw]))\n\n    print()\n\n    if composite_problem1:\n        print(\"\\n\".join([\"SKW '%s' references an unexisting CKW '%s'.\" %\n                         (skw, ckw) for skw, ckw in composite_problem1]))\n    if composite_problem2:\n        print(\"\\n\".join([\"SKW '%s' references a SKW '%s'.\" %\n                         (skw, ckw) for skw, ckw in composite_problem2]))\n    if composite_problem3:\n        print(\"\\n\".join([\"SKW '%s' is not composite of CKW '%s'.\" %\n                         (skw, ckw) for skw, ckw in composite_problem3]))\n    if composite_problem4:\n        for skw, ckws in iteritems(composite_problem4):\n            print(\"SKW '%s' does not exist but is \" \"referenced by:\" % skw)\n            print(\"\\n\".join([\"    %s\" % ckw for ckw in ckws]))\n    if composite_problem5:\n        print(\"\\n\".join([\"CKW '%s' references a CKW '%s'.\" % kw\n                         for kw in composite_problem5]))\n    if composite_problem6:\n        print(\"\\n\".join([\"CKW '%s' is not composed by SKW '%s'.\" % kw\n                         for kw in composite_problem6]))\n\n    print(\"\\n==== WARNINGS ====\")\n\n    if bad_notes:\n        print((\"\\nConcepts with bad notes: %d\" % len(bad_notes)))\n        print(\"\\n\".join([\"   '%s': '%s'\" % _note for _note in bad_notes]))\n    if stemming_collisions:\n        print(\"\\nFollowing keywords have unnecessary labels that have \"\n              \"already been generated by BibClassify.\")\n        for subj in stemming_collisions:\n            print(\"   %s:\\n     %s\\n     and %s\" % subj)\n\n    print(\"\\nFinished.\")\n    sys.exit(0)\n\n\ndef test_cache(taxonomy_name='HEP', rebuild_cache=False, no_cache=False):\n    \"\"\"Test the cache lookup.\"\"\"\n    cache = get_cache(taxonomy_name)\n    if not cache:\n        set_cache(taxonomy_name, get_regular_expressions(taxonomy_name,\n                                                         rebuild=rebuild_cache,\n                                                         no_cache=no_cache))\n        cache = get_cache(taxonomy_name)\n    return (thread.get_ident(), cache)\n\n\nlog.info('Loaded ontology reader')\n\nif __name__ == '__main__':\n    test_cache()\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/inveniosoftware/invenio/blob/d64fffea5a05775ba2db65cba5408c2e9635e354",
        "file_path": "/invenio/legacy/bibclassify/text_extractor.py",
        "source": "# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014 CERN.\n#\n# Invenio is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\n\"\"\"\nBibClassify text extractor.\n\nThis module provides method to extract the fulltext from local or remote\ndocuments. Currently 2 formats of documents are supported: PDF and text\ndocuments.\n\n2 methods provide the functionality of the module: text_lines_from_local_file\nand text_lines_from_url.\n\nThis module also provides the utility 'is_pdf' that uses GNU file in order to\ndetermine if a local file is a PDF file.\n\nThis module is STANDALONE safe\n\"\"\"\n\nimport os\nimport re\nimport tempfile\nimport urllib2\nfrom invenio.legacy.bibclassify import config as bconfig\n\nif bconfig.STANDALONE:\n    from urllib2 import urlopen\nelse:\n    from invenio.utils.url import make_invenio_opener\n\n    urlopen = make_invenio_opener('BibClassify').open\n\nlog = bconfig.get_logger(\"bibclassify.text_extractor\")\n\n_ONE_WORD = re.compile(\"[A-Za-z]{2,}\")\n\n\ndef is_pdf(document):\n    \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\"\n    if not executable_exists('pdftotext'):\n        log.warning(\"GNU file was not found on the system. \"\n                    \"Switching to a weak file extension test.\")\n        if document.lower().endswith(\".pdf\"):\n            return True\n        return False\n        # Tested with file version >= 4.10. First test is secure and works\n    # with file version 4.25. Second condition is tested for file\n    # version 4.10.\n    file_output = os.popen('file ' + re.escape(document)).read()\n    try:\n        filetype = file_output.split(\":\")[1]\n    except IndexError:\n        log.error(\"Your version of the 'file' utility seems to \"\n                  \"be unsupported. Please report this to cds.support@cern.ch.\")\n        raise Exception('Incompatible pdftotext')\n\n    pdf = filetype.find(\"PDF\") > -1\n    # This is how it should be done however this is incompatible with\n    # file version 4.10.\n    #os.popen('file -bi ' + document).read().find(\"application/pdf\")\n    return pdf\n\n\ndef text_lines_from_local_file(document, remote=False):\n    \"\"\"Returns the fulltext of the local file.\n    @var document: fullpath to the file that should be read\n    @var remote: boolean, if True does not count lines (gosh!)\n    @return: list of lines if st was read or an empty list\"\"\"\n\n    try:\n        if is_pdf(document):\n            if not executable_exists(\"pdftotext\"):\n                log.error(\"pdftotext is not available on the system.\")\n            cmd = \"pdftotext -q -enc UTF-8 %s -\" % re.escape(document)\n            filestream = os.popen(cmd)\n        else:\n            filestream = open(document, \"r\")\n    except IOError as ex1:\n        log.error(\"Unable to read from file %s. (%s)\" % (document, ex1.strerror))\n        return []\n\n    # FIXME - we assume it is utf-8 encoded / that is not good\n    lines = [line.decode(\"utf-8\", 'replace') for line in filestream]\n    filestream.close()\n\n    if not _is_english_text('\\n'.join(lines)):\n        log.warning(\"It seems the file '%s' is unvalid and doesn't \"\n                    \"contain text. Please communicate this file to the Invenio \"\n                    \"team.\" % document)\n\n    line_nb = len(lines)\n    word_nb = 0\n    for line in lines:\n        word_nb += len(re.findall(\"\\S+\", line))\n\n    # Discard lines that do not contain at least one word.\n    lines = [line for line in lines if _ONE_WORD.search(line) is not None]\n\n    if not remote:\n        log.info(\"Local file has %d lines and %d words.\" % (line_nb, word_nb))\n\n    return lines\n\n\ndef _is_english_text(text):\n    \"\"\"\n    Checks if a text is correct english.\n    Computes the number of words in the text and compares it to the\n    expected number of words (based on an average size of words of 5.1\n    letters).\n\n    @param text_lines: the text to analyze\n    @type text_lines:  string\n    @return:           True if the text is English, False otherwise\n    @rtype:            Boolean\n    \"\"\"\n    # Consider one word and one space.\n    avg_word_length = 2.55 + 1\n    expected_word_number = float(len(text)) / avg_word_length\n\n    words = [word\n             for word in re.split('\\W', text)\n             if word.isalpha()]\n\n    word_number = len(words)\n\n    return word_number > expected_word_number\n\n\ndef text_lines_from_url(url, user_agent=\"\"):\n    \"\"\"Returns the fulltext of the file found at the URL.\"\"\"\n    request = urllib2.Request(url)\n    if user_agent:\n        request.add_header(\"User-Agent\", user_agent)\n    try:\n        distant_stream = urlopen(request)\n        # Write the URL content to a temporary file.\n        local_file = tempfile.mkstemp(prefix=\"bibclassify.\")[1]\n        local_stream = open(local_file, \"w\")\n        local_stream.write(distant_stream.read())\n        local_stream.close()\n    except:\n        log.error(\"Unable to read from URL %s.\" % url)\n        return None\n    else:\n        # Read lines from the temporary file.\n        lines = text_lines_from_local_file(local_file, remote=True)\n        os.remove(local_file)\n\n        line_nb = len(lines)\n        word_nb = 0\n        for line in lines:\n            word_nb += len(re.findall(\"\\S+\", line))\n\n        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))\n\n        return lines\n\n\ndef executable_exists(executable):\n    \"\"\"Tests if an executable is available on the system.\"\"\"\n    for directory in os.getenv(\"PATH\").split(\":\"):\n        if os.path.exists(os.path.join(directory, executable)):\n            return True\n    return False\n\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/Scout24/monitoring-config-generator/blob/c7aa6c72e291802c190d2c2312d00e37c9c1f299",
        "file_path": "/src/main/python/monitoring_config_generator/yaml_tools/readers.py",
        "source": "import datetime\nimport os\nimport os.path\nimport urlparse\nimport socket\nfrom time import localtime, strftime, time\n\nfrom requests.exceptions import RequestException, ConnectionError, Timeout\nimport requests\nimport yaml\n\nfrom monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, HostUnreachableException\nfrom monitoring_config_generator.yaml_tools.merger import merge_yaml_files\n\ndef is_file(parsed_uri):\n    return parsed_uri.scheme in ['', 'file']\n\n\ndef is_host(parsed_uri):\n    return parsed_uri.scheme in ['http', 'https']\n\n\ndef read_config(uri):\n    uri_parsed = urlparse.urlparse(uri)\n    if is_file(uri_parsed):\n        return read_config_from_file(uri_parsed.path)\n    elif is_host(uri_parsed):\n        return read_config_from_host(uri)\n    else:\n        raise ValueError('Given url was not acceptable %s' % uri)\n\n\ndef read_config_from_file(path):\n    yaml_config = merge_yaml_files(path)\n    etag = None\n    mtime = os.path.getmtime(path)\n    return yaml_config, Header(etag=etag, mtime=mtime)\n\n\ndef read_config_from_host(url):\n    try:\n        response = requests.get(url)\n    except socket.error as e:\n        msg = \"Could not open socket for '%s', error: %s\" % (url, e)\n        raise HostUnreachableException(msg)\n    except ConnectionError as e:\n        msg = \"Could not establish connection for '%s', error: %s\" % (url, e)\n        raise HostUnreachableException(msg)\n    except Timeout as e:\n        msg = \"Connect timed out for '%s', error: %s\" % (url, e)\n        raise HostUnreachableException(msg)\n    except RequestException as e:\n        msg = \"Could not get monitoring yaml from '%s', error: %s\" % (url, e)\n        raise MonitoringConfigGeneratorException(msg)\n\n    def get_from_header(field):\n        return response.headers[field] if field in response.headers else None\n\n    if response.status_code == 200:\n        yaml_config = yaml.load(response.content)\n        etag = get_from_header('etag')\n        mtime = get_from_header('last-modified')\n        mtime = datetime.datetime.strptime(mtime, '%a, %d %b %Y %H:%M:%S %Z').strftime('%s') if mtime else int(time())\n    else:\n        msg = \"Request %s returned with status %s. I don't know how to handle that.\" % (url, response.status_code)\n        raise MonitoringConfigGeneratorException(msg)\n\n    return yaml_config, Header(etag=etag, mtime=mtime)\n\n\nclass Header(object):\n    MON_CONF_GEN_COMMENT = '# Created by MonitoringConfigGenerator'\n    ETAG_COMMENT = '# ETag: '\n    MTIME_COMMMENT = '# MTime: '\n\n    def __init__(self, etag=None, mtime=0):\n        self.etag = etag\n        self.mtime = int(mtime)\n\n    def __nonzero__(self):\n        return self.etag is None and self.mtime is 0\n\n    def __eq__(self, other):\n        return self.etag == other.etag and self.mtime == other.mtime\n\n    def __repr__(self):\n        return \"Header(%s, %d)\" % (self.etag, self.mtime)\n\n    def is_newer_than(self, other):\n        if self.etag != other.etag or self.etag is None:\n            return cmp(self.mtime, other.mtime) > 0\n        else:\n            return False\n\n    def serialize(self):\n        lines = []\n        time_string = strftime(\"%Y-%m-%d %H:%M:%S\", localtime())\n        lines.append(\"%s on %s\" % (Header.MON_CONF_GEN_COMMENT, time_string))\n        if self.etag:\n            lines.append(\"%s%s\" % (Header.ETAG_COMMENT, self.etag))\n        if self.mtime:\n            lines.append(\"%s%d\" % (Header.MTIME_COMMMENT, self.mtime))\n        return lines\n\n    @staticmethod\n    def parse(file_name):\n        etag, mtime = None, 0\n\n        def extract(comment, current_value):\n            value = None\n            if line.startswith(comment):\n                value = line.rstrip()[len(comment):]\n            return value or current_value\n\n        try:\n            with open(file_name, 'r') as config_file:\n                for line in config_file.xreadlines():\n                    etag = extract(Header.ETAG_COMMENT, etag)\n                    mtime = extract(Header.MTIME_COMMMENT, mtime)\n                    if etag and mtime:\n                        break\n        except IOError as e:\n            # it is totally fine to not have an etag, in that case there\n            # will just be no caching and the server will have to deliver the data again\n            pass\n        finally:\n            return Header(etag=etag, mtime=mtime)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/Scout24/monitoring-config-generator/blob/e4ab5e71a52ad3c09390939225957a8bec334279",
        "file_path": "/src/main/python/monitoring_config_generator/MonitoringConfigGenerator.py",
        "source": "\"\"\"monconfgenerator\n\nCreates an Icinga monitoring configuration. It does it by querying an URL from\nwhich it receives a specially formatted yaml file. This file is transformed into\na valid Icinga configuration file.\nIf no URL is given it reads it's default configuration from file system. The\nconfiguration file is: /etc/monitoring_config_generator/config.yaml'\n\nUsage:\n  monconfgenerator [--debug] [--targetdir=<directory>] [--skip-checks] [URL]\n  monconfgenerator -h\n\nOptions:\n  -h                Show this message.\n  --debug           Print additional information.\n  --targetdir=DIR   The generated Icinga monitoring configuration is written\n                    into this directory. If no target directory is given its\n                    value is read from /etc/monitoring_config_generator/config.yaml\n  --skip-checks     Do not run checks on the yaml file received from the URL.\n\n\"\"\"\nfrom datetime import datetime\nimport logging\nimport os\nimport sys\n\nfrom docopt import docopt\n\nfrom monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, \\\n    ConfigurationContainsUndefinedVariables, NoSuchHostname, HostUnreachableException\nfrom monitoring_config_generator import set_log_level_to_debug\nfrom monitoring_config_generator.yaml_tools.readers import Header, read_config\nfrom monitoring_config_generator.yaml_tools.config import YamlConfig\nfrom monitoring_config_generator.settings import CONFIG\n\n\nEXIT_CODE_CONFIG_WRITTEN = 0\nEXIT_CODE_ERROR = 1\nEXIT_CODE_NOT_WRITTEN = 2\n\nLOG = logging.getLogger(\"monconfgenerator\")\n\n\nclass MonitoringConfigGenerator(object):\n    def __init__(self, url, debug_enabled=False, target_dir=None, skip_checks=False):\n        self.skip_checks = skip_checks\n        self.target_dir = target_dir if target_dir else CONFIG['TARGET_DIR']\n        self.source = url\n\n        if debug_enabled:\n            set_log_level_to_debug()\n\n        if not self.target_dir or not os.path.isdir(self.target_dir):\n            raise MonitoringConfigGeneratorException(\"%s is not a directory\" % self.target_dir)\n\n        LOG.debug(\"Using %s as target dir\" % self.target_dir)\n        LOG.debug(\"Using URL: %s\" % self.source)\n        LOG.debug(\"MonitoringConfigGenerator start: reading from %s, writing to %s\" %\n                  (self.source, self.target_dir))\n\n    def _is_newer(self, header_source, hostname):\n        if not hostname:\n            raise NoSuchHostname('hostname not found')\n        output_path = self.output_path(self.create_filename(hostname))\n        old_header = Header.parse(output_path)\n        return header_source.is_newer_than(old_header)\n\n    def output_path(self, file_name):\n        return os.path.join(self.target_dir, file_name)\n\n    def write_output(self, file_name, yaml_icinga):\n        lines = yaml_icinga.icinga_lines\n        output_writer = OutputWriter(self.output_path(file_name))\n        output_writer.write_lines(lines)\n\n    @staticmethod\n    def create_filename(hostname):\n        name = '%s.cfg' % hostname\n        if name != os.path.basename(name):\n            msg = \"Directory traversal attempt detected for host name %r\"\n            raise Exception(msg % hostname)\n        return name\n\n    def generate(self):\n        file_name = None\n        raw_yaml_config, header_source = read_config(self.source)\n\n        if raw_yaml_config is None:\n            raise SystemExit(\"Raw yaml config from source '%s' is 'None'.\" % self.source)\n\n        yaml_config = YamlConfig(raw_yaml_config,\n                                 skip_checks=self.skip_checks)\n\n        if yaml_config.host and self._is_newer(header_source, yaml_config.host_name):\n            file_name = self.create_filename(yaml_config.host_name)\n            yaml_icinga = YamlToIcinga(yaml_config, header_source)\n            self.write_output(file_name, yaml_icinga)\n\n        if file_name:\n            LOG.info(\"Icinga config file '%s' created.\" % file_name)\n\n        return file_name\n\nclass YamlToIcinga(object):\n    def __init__(self, yaml_config, header):\n        self.icinga_lines = []\n        self.indent = CONFIG['INDENT']\n        self.icinga_lines.extend(header.serialize())\n        self.write_section('host', yaml_config.host)\n        for service in yaml_config.services:\n            self.write_section('service', service)\n\n    def write_line(self, line):\n        self.icinga_lines.append(line)\n\n    def write_section(self, section_name, section_data):\n        self.write_line(\"\")\n        self.write_line(\"define %s {\" % section_name)\n        sorted_keys = section_data.keys()\n        sorted_keys.sort()\n        for key in sorted_keys:\n            value = section_data[key]\n            self.icinga_lines.append((\"%s%-45s%s\" % (self.indent, key, self.value_to_icinga(value))))\n        self.write_line(\"}\")\n\n    @staticmethod\n    def value_to_icinga(value):\n        \"\"\"Convert a scalar or list to Icinga value format. Lists are concatenated by ,\n        and empty (None) values produce an empty string\"\"\"\n        if isinstance(value, list):\n            # explicitly set None values to empty string\n            return \",\".join([str(x) if (x is not None) else \"\" for x in value])\n        else:\n            return str(value)\n\n\nclass OutputWriter(object):\n    def __init__(self, output_file):\n        self.output_file = output_file\n\n    def write_lines(self, lines):\n        with open(self.output_file, 'w') as f:\n            for line in lines:\n                f.write(line + \"\\n\")\n        LOG.debug(\"Created %s\" % self.output_file)\n\n\ndef generate_config():\n    arg = docopt(__doc__, version='0.1.0')\n    start_time = datetime.now()\n    try:\n        file_name = MonitoringConfigGenerator(arg['URL'],\n                                              arg['--debug'],\n                                              arg['--targetdir'],\n                                              arg['--skip-checks']).generate()\n        exit_code = EXIT_CODE_CONFIG_WRITTEN if file_name else EXIT_CODE_NOT_WRITTEN\n    except HostUnreachableException:\n        LOG.warn(\"Target url {0} unreachable. Could not get yaml config!\".format(arg['URL']))\n        exit_code = EXIT_CODE_NOT_WRITTEN\n    except ConfigurationContainsUndefinedVariables:\n        LOG.error(\"Configuration contained undefined variables!\")\n        exit_code = EXIT_CODE_ERROR\n    except SystemExit as e:\n        exit_code = e.code\n    except BaseException as e:\n        LOG.error(e)\n        exit_code = EXIT_CODE_ERROR\n    finally:\n        stop_time = datetime.now()\n        LOG.info(\"finished in %s\" % (stop_time - start_time))\n    sys.exit(exit_code)\n\n\nif __name__ == '__main__':\n    generate_config()\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/OrkoHunter/pep8speaks/blob/4eb0f1682ed20cc206244d50edb801d8f127b2b3",
        "file_path": "/pep8speaks/helpers.py",
        "source": "# -*- coding: utf-8 -*-\n\nimport base64\nimport collections\nimport datetime\nimport hmac\nimport json\nimport os\nimport re\nimport subprocess\nimport time\n\nimport psycopg2\nimport requests\nimport unidiff\nimport yaml\nfrom flask import abort\n\n\ndef update_users(repository):\n    \"\"\"Update users of the integration in the database\"\"\"\n    if os.environ.get(\"OVER_HEROKU\", False) is not False:\n        # Check if repository exists in database\n        query = r\"INSERT INTO Users (repository, created_at) VALUES ('{}', now());\" \\\n                \"\".format(repository)\n\n        try:\n            cursor.execute(query)\n            conn.commit()\n        except psycopg2.IntegrityError:  # If already exists\n            conn.rollback()\n\n\ndef follow_user(user):\n    \"\"\"Follow the user of the service\"\"\"\n    headers = {\n        \"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"],\n        \"Content-Length\": \"0\",\n    }\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    url = \"https://api.github.com/user/following/{}\"\n    url = url.format(user)\n    r = requests.put(url, headers=headers, auth=auth)\n\n\ndef update_dict(base, head):\n    \"\"\"\n    Recursively merge or update dict-like objects.\n    >>> update({'k1': 1}, {'k1': {'k2': {'k3': 3}}})\n\n    Source : http://stackoverflow.com/a/32357112/4698026\n    \"\"\"\n    for key, value in head.items():\n        if isinstance(base, collections.Mapping):\n            if isinstance(value, collections.Mapping):\n                base[key] = update_dict(base.get(key, {}), value)\n            else:\n                base[key] = head[key]\n        else:\n            base = {key: head[key]}\n    return base\n\n\ndef match_webhook_secret(request):\n    \"\"\"Match the webhook secret sent from GitHub\"\"\"\n    if os.environ.get(\"OVER_HEROKU\", False) is not False:\n        header_signature = request.headers.get('X-Hub-Signature')\n        if header_signature is None:\n            abort(403)\n        sha_name, signature = header_signature.split('=')\n        if sha_name != 'sha1':\n            abort(501)\n        mac = hmac.new(os.environ[\"GITHUB_PAYLOAD_SECRET\"].encode(), msg=request.data,\n                       digestmod=\"sha1\")\n        if not hmac.compare_digest(str(mac.hexdigest()), str(signature)):\n            abort(403)\n    return True\n\n\ndef check_pythonic_pr(data):\n    \"\"\"\n    Return True if the PR contains at least one Python file\n    \"\"\"\n    files = list(get_files_involved_in_pr(data).keys())\n    pythonic = False\n    for file in files:\n        if file[-3:] == '.py':\n            pythonic = True\n            break\n\n    return pythonic\n\n\ndef get_config(data):\n    \"\"\"\n    Get .pep8speaks.yml config file from the repository and return\n    the config dictionary\n    \"\"\"\n\n    # Default configuration parameters\n    config = {\n        \"message\": {\n            \"opened\": {\n                \"header\": \"\",\n                \"footer\": \"\"\n            },\n            \"updated\": {\n                \"header\": \"\",\n                \"footer\": \"\"\n            }\n        },\n        \"scanner\": {\"diff_only\": False},\n        \"pycodestyle\": {\n            \"ignore\": [],\n            \"max-line-length\": 79,\n            \"count\": False,\n            \"first\": False,\n            \"show-pep8\": False,\n            \"filename\": [],\n            \"exclude\": [],\n            \"select\": [],\n            \"show-source\": False,\n            \"statistics\": False,\n            \"hang-closing\": False,\n        },\n        \"no_blank_comment\": True,\n        \"only_mention_files_with_errors\": True,\n    }\n\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    # Configuration file\n    url = \"https://raw.githubusercontent.com/{}/{}/.pep8speaks.yml\"\n\n    url = url.format(data[\"repository\"], data[\"after_commit_hash\"])\n    r = requests.get(url, headers=headers, auth=auth)\n    if r.status_code == 200:\n        try:\n            new_config = yaml.load(r.text)\n            # overloading the default configuration with the one specified\n            config = update_dict(config, new_config)\n        except yaml.YAMLError:  # Bad YAML file\n            pass\n\n    # Create pycodestyle command line arguments\n    arguments = []\n    confs = config[\"pycodestyle\"]\n    for key, value in confs.items():\n        if value:  # Non empty\n            if isinstance(value, int):\n                if isinstance(value, bool):\n                    arguments.append(\"--{}\".format(key))\n                else:\n                    arguments.append(\"--{}={}\".format(key, value))\n            elif isinstance(value, list):\n                arguments.append(\"--{}={}\".format(key, ','.join(value)))\n    config[\"pycodestyle_cmd_config\"] = ' {arguments}'.format(arguments=' '.join(arguments))\n\n    # pycodestyle is case-sensitive\n    config[\"pycodestyle\"][\"ignore\"] = [e.upper() for e in list(config[\"pycodestyle\"][\"ignore\"])]\n\n    return config\n\n\ndef get_files_involved_in_pr(data):\n    \"\"\"\n    Return a list of file names modified/added in the PR\n    \"\"\"\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    diff_headers = headers.copy()\n    diff_headers[\"Accept\"] = \"application/vnd.github.VERSION.diff\"\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    repository = data[\"repository\"]\n    after_commit_hash = data[\"after_commit_hash\"]\n    author = data[\"author\"]\n    diff_url = \"https://api.github.com/repos/{}/pulls/{}\"\n    diff_url = diff_url.format(repository, str(data[\"pr_number\"]))\n    r = requests.get(diff_url, headers=diff_headers, auth=auth)\n    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)\n\n    files = {}\n\n    for patchset in patch:\n        file = patchset.target_file[1:]\n        files[file] = []\n        for hunk in patchset:\n            for line in hunk.target_lines():\n                if line.is_added:\n                    files[file].append(line.target_line_no)\n\n    return files\n\n\ndef get_python_files_involved_in_pr(data):\n    files = get_files_involved_in_pr(data)\n    for file in list(files.keys()):\n        if file[-3:] != \".py\":\n            del files[file]\n\n    return files\n\n\ndef run_pycodestyle(data, config):\n    \"\"\"\n    Run pycodestyle script on the files and update the data\n    dictionary\n    \"\"\"\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    repository = data[\"repository\"]\n    after_commit_hash = data[\"after_commit_hash\"]\n    author = data[\"author\"]\n\n    # Run pycodestyle\n    ## All the python files with additions\n    # A dictionary with filename paired with list of new line numbers\n    py_files = get_python_files_involved_in_pr(data)\n\n    for file in py_files:\n        filename = file[1:]\n        url = \"https://raw.githubusercontent.com/{}/{}/{}\"\n        url = url.format(repository, after_commit_hash, file)\n        r = requests.get(url, headers=headers, auth=auth)\n        with open(\"file_to_check.py\", 'w+', encoding=r.encoding) as file_to_check:\n            file_to_check.write(r.text)\n\n        # Use the command line here\n        cmd = 'pycodestyle {config[pycodestyle_cmd_config]} file_to_check.py'.format(\n            config=config)\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n        stdout, _ = proc.communicate()\n        data[\"extra_results\"][filename] = stdout.decode(r.encoding).splitlines()\n\n        # Put only relevant errors in the data[\"results\"] dictionary\n        data[\"results\"][filename] = []\n        for error in list(data[\"extra_results\"][filename]):\n            if re.search(\"^file_to_check.py:\\d+:\\d+:\\s[WE]\\d+\\s.*\", error):\n                data[\"results\"][filename].append(error.replace(\"file_to_check.py\", filename))\n                data[\"extra_results\"][filename].remove(error)\n\n        ## Remove errors in case of diff_only = True\n        ## which are caused in the whole file\n        for error in list(data[\"results\"][filename]):\n            if config[\"scanner\"][\"diff_only\"]:\n                if not int(error.split(\":\")[1]) in py_files[file]:\n                    data[\"results\"][filename].remove(error)\n\n        ## Store the link to the file\n        url = \"https://github.com/{}/blob/{}{}\"\n        data[filename + \"_link\"] = url.format(repository, after_commit_hash, file)\n        os.remove(\"file_to_check.py\")\n\n\ndef prepare_comment(request, data, config):\n    \"\"\"Construct the string of comment i.e. its header, body and footer\"\"\"\n    author = data[\"author\"]\n    # Write the comment body\n    ## Header\n    comment_header = \"\"\n    if request.json[\"action\"] == \"opened\":\n        if config[\"message\"][\"opened\"][\"header\"] == \"\":\n            comment_header = \"Hello @\" + author + \"! Thanks for submitting the PR.\\n\\n\"\n        else:\n            comment_header = config[\"message\"][\"opened\"][\"header\"] + \"\\n\\n\"\n    elif request.json[\"action\"] in [\"synchronize\", \"reopened\"]:\n        if config[\"message\"][\"updated\"][\"header\"] == \"\":\n            comment_header = \"Hello @\" + author + \"! Thanks for updating the PR.\\n\\n\"\n        else:\n            comment_header = config[\"message\"][\"updated\"][\"header\"] + \"\\n\\n\"\n\n    ## Body\n    ERROR = False  # Set to True when any pep8 error exists\n    comment_body = []\n    for file, issues in data[\"results\"].items():\n        if len(issues) == 0:\n            if not config[\"only_mention_files_with_errors\"]:\n                comment_body.append(\n                    \" - There are no PEP8 issues in the\"\n                    \" file [`{0}`]({1}) !\".format(file, data[file + \"_link\"]))\n        else:\n            ERROR = True\n            comment_body.append(\n                \" - In the file [`{0}`]({1}), following \"\n                \"are the PEP8 issues :\\n\".format(file, data[file + \"_link\"]))\n            for issue in issues:\n                ## Replace filename with L\n                error_string = issue.replace(file + \":\", \"Line \")\n\n                ## Link error codes to search query\n                error_string_list = error_string.split(\" \")\n                code = error_string_list[2]\n                code_url = \"https://duckduckgo.com/?q=pep8%20{0}\".format(code)\n                error_string_list[2] = \"[{0}]({1})\".format(code, code_url)\n\n                ## Link line numbers in the file\n                line, col = error_string_list[1][:-1].split(\":\")\n                line_url = data[file + \"_link\"] + \"#L\" + line\n                error_string_list[1] = \"[{0}:{1}]({2}):\".format(line, col, line_url)\n                error_string = \" \".join(error_string_list)\n                error_string = error_string.replace(\"Line [\", \"[Line \")\n                comment_body.append(\"\\n> {0}\".format(error_string))\n\n        comment_body.append(\"\\n\\n\")\n        if len(data[\"extra_results\"][file]) > 0:\n            comment_body.append(\" - Complete extra results for this file :\\n\\n\")\n            comment_body.append(\"> \" + \"\".join(data[\"extra_results\"][file]))\n            comment_body.append(\"---\\n\\n\")\n\n    if config[\"only_mention_files_with_errors\"] and not ERROR:\n        comment_body.append(\"Cheers ! There are no PEP8 issues in this Pull Request. :beers: \")\n\n\n    comment_body = ''.join(comment_body)\n\n\n    ## Footer\n    comment_footer = []\n    if request.json[\"action\"] == \"opened\":\n        comment_footer.append(config[\"message\"][\"opened\"][\"footer\"])\n    elif request.json[\"action\"] in [\"synchronize\", \"reopened\"]:\n        comment_footer.append(config[\"message\"][\"updated\"][\"footer\"])\n\n    comment_footer = ''.join(comment_footer)\n\n    return comment_header, comment_body, comment_footer, ERROR\n\n\ndef comment_permission_check(data, comment):\n    \"\"\"Check for quite and resume status or duplicate comments\"\"\"\n    PERMITTED_TO_COMMENT = True\n    repository = data[\"repository\"]\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    # Check for duplicate comment\n    url = \"https://api.github.com/repos/{}/issues/{}/comments\"\n    url = url.format(repository, str(data[\"pr_number\"]))\n    comments = requests.get(url, headers=headers, auth=auth).json()\n\n    # Get the last comment by the bot\n    last_comment = \"\"\n    for old_comment in reversed(comments):\n        if old_comment[\"user\"][\"id\"] == 24736507:  # ID of @pep8speaks\n            last_comment = old_comment[\"body\"]\n            break\n\n    \"\"\"\n    # Disabling this because only a single comment is made per PR\n    text1 = ''.join(BeautifulSoup(markdown(comment)).findAll(text=True))\n    text2 = ''.join(BeautifulSoup(markdown(last_comment)).findAll(text=True))\n    if text1 == text2.replace(\"submitting\", \"updating\"):\n        PERMITTED_TO_COMMENT = False\n    \"\"\"\n\n    # Check if the bot is asked to keep quiet\n    for old_comment in reversed(comments):\n        if '@pep8speaks' in old_comment['body']:\n            if 'resume' in old_comment['body'].lower():\n                break\n            elif 'quiet' in old_comment['body'].lower():\n                PERMITTED_TO_COMMENT = False\n\n\n    return PERMITTED_TO_COMMENT\n\n\ndef create_or_update_comment(data, comment):\n    comment_mode = None\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    query = \"https://api.github.com/repos/{}/issues/{}/comments\"\n    query = query.format(data[\"repository\"], str(data[\"pr_number\"]))\n    comments = requests.get(query, headers=headers, auth=auth).json()\n\n    # Get the last comment id by the bot\n    last_comment_id = None\n    for old_comment in comments:\n        if old_comment[\"user\"][\"id\"] == 24736507:  # ID of @pep8speaks\n            last_comment_id = old_comment[\"id\"]\n            break\n\n    if last_comment_id is None:  # Create a new comment\n        response = requests.post(query, json={\"body\": comment}, headers=headers, auth=auth)\n        data[\"comment_response\"] = response.json()\n    else:  # Update the last comment\n        utc_time = datetime.datetime.utcnow()\n        time_now = utc_time.strftime(\"%B %d, %Y at %H:%M Hours UTC\")\n        comment += \"\\n\\n##### Comment last updated on {}\"\n        comment = comment.format(time_now)\n\n        query = \"https://api.github.com/repos/{}/issues/comments/{}\"\n        query = query.format(data[\"repository\"], str(last_comment_id))\n        response = requests.patch(query, json={\"body\": comment}, headers=headers, auth=auth)\n\n\ndef autopep8(data, config):\n    # Run pycodestyle\n\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(data[\"diff_url\"], headers=headers, auth=auth)\n    ## All the python files with additions\n    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)\n\n    # A dictionary with filename paired with list of new line numbers\n    py_files = {}\n\n    for patchset in patch:\n        if patchset.target_file[-3:] == '.py':\n            py_file = patchset.target_file[1:]\n            py_files[py_file] = []\n            for hunk in patchset:\n                for line in hunk.target_lines():\n                    if line.is_added:\n                        py_files[py_file].append(line.target_line_no)\n\n    # Ignore errors and warnings specified in the config file\n    to_ignore = \",\".join(config[\"pycodestyle\"][\"ignore\"])\n    arg_to_ignore = \"\"\n    if len(to_ignore) > 0:\n        arg_to_ignore = \"--ignore \" + to_ignore\n\n    for file in py_files:\n        filename = file[1:]\n        url = \"https://raw.githubusercontent.com/{}/{}/{}\"\n        url = url.format(data[\"repository\"], data[\"sha\"], file)\n        r = requests.get(url, headers=headers, auth=auth)\n        with open(\"file_to_fix.py\", 'w+', encoding=r.encoding) as file_to_fix:\n            file_to_fix.write(r.text)\n\n        cmd = 'autopep8 file_to_fix.py --diff {arg_to_ignore}'.format(\n            arg_to_ignore=arg_to_ignore)\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n        stdout, _ = proc.communicate()\n        data[\"diff\"][filename] = stdout.decode(r.encoding)\n\n        # Fix the errors\n        data[\"diff\"][filename] = data[\"diff\"][filename].replace(\"file_to_check.py\", filename)\n        data[\"diff\"][filename] = data[\"diff\"][filename].replace(\"\\\\\", \"\\\\\\\\\")\n\n        ## Store the link to the file\n        url = \"https://github.com/{}/blob/{}{}\"\n        data[filename + \"_link\"] = url.format(data[\"repository\"], data[\"sha\"], file)\n        os.remove(\"file_to_fix.py\")\n\n\ndef create_gist(data, config):\n    \"\"\"Create gists for diff files\"\"\"\n    REQUEST_JSON = {}\n    REQUEST_JSON[\"public\"] = True\n    REQUEST_JSON[\"files\"] = {}\n    REQUEST_JSON[\"description\"] = \"In response to @{0}'s comment : {1}\".format(\n        data[\"reviewer\"], data[\"review_url\"])\n\n    for file, diffs in data[\"diff\"].items():\n        if len(diffs) != 0:\n            REQUEST_JSON[\"files\"][file.split(\"/\")[-1] + \".diff\"] = {\n                \"content\": diffs\n            }\n\n    # Call github api to create the gist\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    url = \"https://api.github.com/gists\"\n    res = requests.post(url, json=REQUEST_JSON, headers=headers, auth=auth).json()\n    data[\"gist_response\"] = res\n    data[\"gist_url\"] = res[\"html_url\"]\n\n\ndef delete_if_forked(data):\n    FORKED = False\n    url = \"https://api.github.com/user/repos\"\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(url, headers=headers, auth=auth)\n    for repo in r.json():\n        if repo[\"description\"]:\n            if data[\"target_repo_fullname\"] in repo[\"description\"]:\n                FORKED = True\n                r = requests.delete(\"https://api.github.com/repos/\"\n                                \"{}\".format(repo[\"full_name\"]),\n                                headers=headers, auth=auth)\n    return FORKED\n\n\ndef fork_for_pr(data):\n    FORKED = False\n    url = \"https://api.github.com/repos/{}/forks\"\n    url = url.format(data[\"target_repo_fullname\"])\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.post(url, headers=headers, auth=auth)\n    if r.status_code == 202:\n        data[\"fork_fullname\"] = r.json()[\"full_name\"]\n        FORKED = True\n    else:\n        data[\"error\"] = \"Unable to fork\"\n    return FORKED\n\n\ndef update_fork_desc(data):\n    # Check if forked (takes time)\n    url = \"https://api.github.com/repos/{}\".format(data[\"fork_fullname\"])\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(url, headers=headers, auth=auth)\n    ATTEMPT = 0\n    while(r.status_code != 200):\n        time.sleep(5)\n        r = requests.get(url, headers=headers, auth=auth)\n        ATTEMPT += 1\n        if ATTEMPT > 10:\n            data[\"error\"] = \"Forking is taking more than usual time\"\n            break\n\n    full_name = data[\"target_repo_fullname\"]\n    author, name = full_name.split(\"/\")\n    request_json = {\n        \"name\": name,\n        \"description\": \"Forked from @{}'s {}\".format(author, full_name)\n    }\n    r = requests.patch(url, data=json.dumps(request_json), headers=headers, auth=auth)\n    if r.status_code != 200:\n        data[\"error\"] = \"Could not update description of the fork\"\n\n\ndef create_new_branch(data):\n    url = \"https://api.github.com/repos/{}/git/refs/heads\"\n    url = url.format(data[\"fork_fullname\"])\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    sha = None\n    r = requests.get(url, headers=headers, auth=auth)\n    for ref in r.json():\n        if ref[\"ref\"].split(\"/\")[-1] == data[\"target_repo_branch\"]:\n            sha = ref[\"object\"][\"sha\"]\n\n    url = \"https://api.github.com/repos/{}/git/refs\"\n    url = url.format(data[\"fork_fullname\"])\n    data[\"new_branch\"] = \"{}-pep8-patch\".format(data[\"target_repo_branch\"])\n    request_json = {\n        \"ref\": \"refs/heads/{}\".format(data[\"new_branch\"]),\n        \"sha\": sha,\n    }\n    r = requests.post(url, json=request_json, headers=headers, auth=auth)\n\n    if r.status_code != 200:\n        data[\"error\"] = \"Could not create new branch in the fork\"\n\n\ndef autopep8ify(data, config):\n    # Run pycodestyle\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(data[\"diff_url\"], headers=headers, auth=auth)\n\n    ## All the python files with additions\n    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)\n\n    # A dictionary with filename paired with list of new line numbers\n    py_files = {}\n\n    for patchset in patch:\n        if patchset.target_file[-3:] == '.py':\n            py_file = patchset.target_file[1:]\n            py_files[py_file] = []\n            for hunk in patchset:\n                for line in hunk.target_lines():\n                    if line.is_added:\n                        py_files[py_file].append(line.target_line_no)\n\n    # Ignore errors and warnings specified in the config file\n    to_ignore = \",\".join(config[\"pycodestyle\"][\"ignore\"])\n    arg_to_ignore = \"\"\n    if len(to_ignore) > 0:\n        arg_to_ignore = \"--ignore \" + to_ignore\n\n    for file in py_files:\n        filename = file[1:]\n        url = \"https://raw.githubusercontent.com/{}/{}/{}\"\n        url = url.format(data[\"repository\"], data[\"sha\"], file)\n        r = requests.get(url, headers=headers, auth=auth)\n        with open(\"file_to_fix.py\", 'w+', encoding=r.encoding) as file_to_fix:\n            file_to_fix.write(r.text)\n\n        cmd = 'autopep8 file_to_fix.py {arg_to_ignore}'.format(\n            arg_to_ignore=arg_to_ignore)\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n        stdout, _ = proc.communicate()\n        data[\"results\"][filename] = stdout.decode(r.encoding)\n\n        os.remove(\"file_to_fix.py\")\n\n\ndef commit(data):\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    fullname = data.get(\"fork_fullname\")\n\n    for file, new_file in data[\"results\"].items():\n        url = \"https://api.github.com/repos/{}/contents/{}\"\n        url = url.format(fullname, file)\n        params = {\"ref\": data[\"new_branch\"]}\n        r = requests.get(url, params=params, headers=headers, auth=auth)\n        sha_blob = r.json().get(\"sha\")\n        params[\"path\"] = file\n        content_code = base64.b64encode(new_file.encode()).decode(\"utf-8\")\n        request_json = {\n            \"path\": file,\n            \"message\": \"Fix pep8 errors in {}\".format(file),\n            \"content\": content_code,\n            \"sha\": sha_blob,\n            \"branch\": data.get(\"new_branch\"),\n        }\n        r = requests.put(url, json=request_json, headers=headers, auth=auth)\n\n\ndef create_pr(data):\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    url = \"https://api.github.com/repos/{}/pulls\"\n    url = url.format(data[\"target_repo_fullname\"])\n    request_json = {\n        \"title\": \"Fix pep8 errors\",\n        \"head\": \"pep8speaks:{}\".format(data[\"new_branch\"]),\n        \"base\": data[\"target_repo_branch\"],\n        \"body\": \"The changes are suggested by autopep8\",\n    }\n    r = requests.post(url, json=request_json, headers=headers, auth=auth)\n    if r.status_code == 201:\n        data[\"pr_url\"] = r.json()[\"html_url\"]\n    else:\n        data[\"error\"] = \"Pull request could not be created\"\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/ntc-chip-revived/ChippyRuxpin/blob/c1bd69699205ae7f8b3fadfd9a52823e413ec384",
        "file_path": "/chippyRuxpin.py",
        "source": "#!/usr/bin/python\r\n# Chippy Ruxpin by Next Thing Co\r\n# Powered by C.H.I.P., the world's first $9 computer!\r\n\r\n# apt-get install python-setuptools python-dev build-essential espeak alsa-utils\r\n# apt-get install python-alsaaudio python-numpy python-twitter python-bottle mplayer\r\n\r\n# IMPORTANT NOTE ABOUT TWITTER STUFF!\r\n# In order to retrieve tweets, you need to authorize this code to use your twitter account.\r\n# This involves obtaining some special tokens that are specific to you.\r\n# Please visit Twitter's website to obtain this information and put the values in the variables below.\r\n# For more information, visit this URL:\r\n# https://dev.twitter.com/oauth/overview/application-owner-access-tokens\r\n\r\nconsumerKey='INSERT YOUR CONSUMER KEY HERE FROM TWITTER'\r\nconsumerSecret='INSERT YOUR CONSUMER SECRET HERE FROM TWITTER'\r\naccessTokenKey='INSERT YOUR ACCESS TOKEN KEY HERE FROM TWITTER'\r\naccessTokenSecret='INSERT YOUR ACCESS TOKEN SECRET HERE FROM TWITTER'\r\n\r\nimport sys\r\nimport time\r\nimport subprocess\r\nimport os\r\nfrom random import randint\r\nfrom threading import Thread\r\nfrom chippyRuxpin_audioPlayer import AudioPlayer\r\nfrom chippyRuxpin_gpio import GPIO\r\nfrom chippyRuxpin_twitter import ChippyTwitter\r\nfrom chippyRuxpin_webFramework import WebFramework\r\n\r\nfullMsg = \"\"\r\n\r\nMOUTH_OPEN = 408 # GPIO pin assigned to open the mouth. XIO-P0\r\nMOUTH_CLOSE = 412 # GPIO pin assigned to close the mouth. XIO-P2\r\nEYES_OPEN = 410 # GPIO pin assigned to open the eyes. XIO-P4\r\nEYES_CLOSE = 414 # GPIO pin assigned to close the eyes. XIO-P6\r\n\r\nio = GPIO() #Establish connection to our GPIO pins.\r\nio.setup( MOUTH_OPEN )\r\nio.setup( EYES_OPEN )\r\nio.setup( MOUTH_CLOSE )\r\nio.setup( EYES_CLOSE )\r\n\r\naudio = None\r\nisRunning = True\r\n\r\ndef updateMouth():\r\n    lastMouthEvent = 0\r\n    lastMouthEventTime = 0\r\n\r\n    while( audio == None ):\r\n        time.sleep( 0.1 )\r\n        \r\n    while isRunning:\r\n        if( audio.mouthValue != lastMouthEvent ):\r\n            lastMouthEvent = audio.mouthValue\r\n            lastMouthEventTime = time.time()\r\n\r\n            if( audio.mouthValue == 1 ):\r\n                io.set( MOUTH_OPEN, 1 )\r\n                io.set( MOUTH_CLOSE, 0 )\r\n            else:\r\n                io.set( MOUTH_OPEN, 0 )\r\n                io.set( MOUTH_CLOSE, 1 )\r\n        else:\r\n            if( time.time() - lastMouthEventTime > 0.4 ):\r\n                io.set( MOUTH_OPEN, 0 )\r\n                io.set( MOUTH_CLOSE, 0 )\r\n\r\n# A routine for blinking the eyes in a semi-random fashion.\r\ndef updateEyes():\r\n    while isRunning:\r\n        io.set( EYES_CLOSE, 1 )\r\n        io.set( EYES_OPEN, 0 )\r\n        time.sleep(0.4)\r\n        io.set( EYES_CLOSE, 0 )\r\n        io.set( EYES_OPEN, 1 )\r\n        time.sleep(0.4)\r\n        io.set( EYES_CLOSE, 1 )\r\n        io.set( EYES_OPEN, 0 )\r\n        time.sleep(0.4)\r\n        io.set( EYES_CLOSE, 0 )\r\n        io.set( EYES_OPEN, 0 )\r\n        time.sleep( randint( 0,7) )\r\n   \r\ndef talk(myText):\r\n    if( myText.find( \"twitter\" ) >= 0 ):\r\n        myText += \"0\"\r\n        myText = myText[7:-1]\r\n        try:\r\n\t    myText = twitter.getTweet( myText )\r\n\texcept:\r\n\t    print( \"!!!ERROR: INVALID TWITTER CREDENTIALS. Please read README.md for instructions.\")\r\n            return\r\n    \r\n    os.system( \"espeak \\\",...\\\" 2>/dev/null\" ) # Sometimes the beginning of audio can get cut off. Insert silence.\r\n    time.sleep( 0.5 )\r\n    os.system( \"espeak -w speech.wav \\\"\" + myText + \"\\\" -s 130\" )\r\n    audio.play(\"speech.wav\")\r\n    return myText\r\n\r\nmouthThread = Thread(target=updateMouth)\r\nmouthThread.start()\r\neyesThread = Thread(target=updateEyes)\r\neyesThread.start()     \r\naudio = AudioPlayer()\r\n\r\nif( consumerKey.find( 'TWITTER' ) >= 0 ):\r\n    print( \"WARNING: INVALID TWITTER CREDENTIALS. Please read README.md for instructions.\" )    \r\nelse:\r\n    twitter = ChippyTwitter(consumerKey,consumerSecret,accessTokenKey,accessTokenSecret)\r\n\r\nweb = WebFramework(talk)\r\nisRunning = False\r\nio.cleanup()\r\nsys.exit(1)\r\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kkltcjk/YouCompleteMe/blob/abfc3ee36adab11c0c0b9d086a164a69006fec79",
        "file_path": "/python/ycm/client/base_request.py",
        "source": "#!/usr/bin/env python\n#\n# Copyright (C) 2013  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport vim\nimport requests\nimport urlparse\nfrom retries import retries\nfrom requests_futures.sessions import FuturesSession\nfrom ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\nfrom ycm import vimsupport\nfrom ycm.utils import ToUtf8Json\nfrom ycm.server.responses import ServerError, UnknownExtraConf\n\n_HEADERS = {'content-type': 'application/json'}\n_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n# Setting this to None seems to screw up the Requests/urllib3 libs.\n_DEFAULT_TIMEOUT_SEC = 30\n\nclass BaseRequest( object ):\n  def __init__( self ):\n    pass\n\n\n  def Start( self ):\n    pass\n\n\n  def Done( self ):\n    return True\n\n\n  def Response( self ):\n    return {}\n\n  # This method blocks\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',\n                                                            handler,\n                                                            'GET',\n                                                            timeout ) )\n\n\n  # This is the blocking version of the method. See below for async.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,\n                                                               handler,\n                                                               timeout ) )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |method| is either 'POST' or 'GET'.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def _TalkToHandlerAsync( data,\n                           handler,\n                           method,\n                           timeout = _DEFAULT_TIMEOUT_SEC ):\n    def SendRequest( data, handler, method, timeout ):\n      if method == 'POST':\n        return BaseRequest.session.post( _BuildUri( handler ),\n                                        data = ToUtf8Json( data ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n      if method == 'GET':\n        return BaseRequest.session.get( _BuildUri( handler ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n\n    @retries( 5, delay = 0.5, backoff = 1.5 )\n    def DelayedSendRequest( data, handler, method ):\n      if method == 'POST':\n        return requests.post( _BuildUri( handler ),\n                              data = ToUtf8Json( data ),\n                              headers = _HEADERS )\n      if method == 'GET':\n        return requests.get( _BuildUri( handler ),\n                             headers = _HEADERS )\n\n    if not _CheckServerIsHealthyWithCache():\n      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n\n    return SendRequest( data, handler, method, timeout )\n\n\n  session = FuturesSession( executor = _EXECUTOR )\n  server_location = 'http://localhost:6666'\n\n\ndef BuildRequestData( start_column = None,\n                      query = None,\n                      include_buffer_data = True ):\n  line, column = vimsupport.CurrentLineAndColumn()\n  filepath = vimsupport.GetCurrentBufferFilepath()\n  request_data = {\n    'filetypes': vimsupport.CurrentFiletypes(),\n    'line_num': line,\n    'column_num': column,\n    'start_column': start_column,\n    'line_value': vim.current.line,\n    'filepath': filepath\n  }\n\n  if include_buffer_data:\n    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()\n  if query:\n    request_data[ 'query' ] = query\n\n  return request_data\n\n\ndef JsonFromFuture( future ):\n  response = future.result()\n  if response.status_code == requests.codes.server_error:\n    _RaiseExceptionForData( response.json() )\n\n  # We let Requests handle the other status types, we only handle the 500\n  # error code.\n  response.raise_for_status()\n\n  if response.text:\n    return response.json()\n  return None\n\n\ndef _BuildUri( handler ):\n  return urlparse.urljoin( BaseRequest.server_location, handler )\n\n\nSERVER_HEALTHY = False\n\ndef _CheckServerIsHealthyWithCache():\n  global SERVER_HEALTHY\n\n  def _ServerIsHealthy():\n    response = requests.get( _BuildUri( 'healthy' ) )\n    response.raise_for_status()\n    return response.json()\n\n  if SERVER_HEALTHY:\n    return True\n\n  try:\n    SERVER_HEALTHY = _ServerIsHealthy()\n    return SERVER_HEALTHY\n  except:\n    return False\n\n\ndef _RaiseExceptionForData( data ):\n  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:\n    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )\n\n  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],\n                                        data[ 'message' ] ) )\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kkltcjk/YouCompleteMe/blob/abfc3ee36adab11c0c0b9d086a164a69006fec79",
        "file_path": "/python/ycm/youcompleteme.py",
        "source": "#!/usr/bin/env python\n#\n# Copyright (C) 2011, 2012  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\nimport vim\nimport tempfile\nimport json\nimport signal\nfrom subprocess import PIPE\nfrom ycm import vimsupport\nfrom ycm import utils\nfrom ycm.diagnostic_interface import DiagnosticInterface\nfrom ycm.completers.all.omni_completer import OmniCompleter\nfrom ycm.completers.general import syntax_parse\nfrom ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype\nfrom ycm.client.ycmd_keepalive import YcmdKeepalive\nfrom ycm.client.base_request import BaseRequest, BuildRequestData\nfrom ycm.client.command_request import SendCommandRequest\nfrom ycm.client.completion_request import CompletionRequest\nfrom ycm.client.omni_completion_request import OmniCompletionRequest\nfrom ycm.client.event_notification import ( SendEventNotificationAsync,\n                                            EventNotification )\nfrom ycm.server.responses import ServerError\n\ntry:\n  from UltiSnips import UltiSnips_Manager\n  USE_ULTISNIPS_DATA = True\nexcept ImportError:\n  USE_ULTISNIPS_DATA = False\n\n# We need this so that Requests doesn't end up using the local HTTP proxy when\n# talking to ycmd. Users should actually be setting this themselves when\n# configuring a proxy server on their machine, but most don't know they need to\n# or how to do it, so we do it for them.\n# Relevant issues:\n#  https://github.com/Valloric/YouCompleteMe/issues/641\n#  https://github.com/kennethreitz/requests/issues/879\nos.environ['no_proxy'] = '127.0.0.1,localhost'\n\n# Force the Python interpreter embedded in Vim (in which we are running) to\n# ignore the SIGINT signal. This helps reduce the fallout of a user pressing\n# Ctrl-C in Vim.\nsignal.signal( signal.SIGINT, signal.SIG_IGN )\n\nNUM_YCMD_STDERR_LINES_ON_CRASH = 30\nSERVER_CRASH_MESSAGE_STDERR_FILE = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n  'Stderr (last {0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )\nSERVER_CRASH_MESSAGE_SAME_STDERR = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '\n  ' check console output for logs!' )\nSERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours\n\n\nclass YouCompleteMe( object ):\n  def __init__( self, user_options ):\n    self._user_options = user_options\n    self._user_notified_about_crash = False\n    self._diag_interface = DiagnosticInterface( user_options )\n    self._omnicomp = OmniCompleter( user_options )\n    self._latest_completion_request = None\n    self._latest_file_parse_request = None\n    self._server_stdout = None\n    self._server_stderr = None\n    self._server_popen = None\n    self._filetypes_with_keywords_loaded = set()\n    self._temp_options_filename = None\n    self._ycmd_keepalive = YcmdKeepalive()\n    self._SetupServer()\n    self._ycmd_keepalive.Start()\n\n  def _SetupServer( self ):\n    server_port = utils.GetUnusedLocalhostPort()\n    with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n      self._temp_options_filename = options_file.name\n      json.dump( dict( self._user_options ), options_file )\n      options_file.flush()\n\n      args = [ utils.PathToPythonInterpreter(),\n               _PathToServerScript(),\n               '--port={0}'.format( server_port ),\n               '--options_file={0}'.format( options_file.name ),\n               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),\n               '--idle_suicide_seconds={0}'.format(\n                  SERVER_IDLE_SUICIDE_SECONDS )]\n\n      if not self._user_options[ 'server_use_vim_stdout' ]:\n        filename_format = os.path.join( utils.PathToTempDir(),\n                                        'server_{port}_{std}.log' )\n\n        self._server_stdout = filename_format.format( port = server_port,\n                                                      std = 'stdout' )\n        self._server_stderr = filename_format.format( port = server_port,\n                                                      std = 'stderr' )\n        args.append('--stdout={0}'.format( self._server_stdout ))\n        args.append('--stderr={0}'.format( self._server_stderr ))\n\n        if self._user_options[ 'server_keep_logfiles' ]:\n          args.append('--keep_logfiles')\n\n      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n      BaseRequest.server_location = 'http://localhost:' + str( server_port )\n\n    self._NotifyUserIfServerCrashed()\n\n  def _IsServerAlive( self ):\n    returncode = self._server_popen.poll()\n    # When the process hasn't finished yet, poll() returns None.\n    return returncode is None\n\n\n  def _NotifyUserIfServerCrashed( self ):\n    if self._user_notified_about_crash or self._IsServerAlive():\n      return\n    self._user_notified_about_crash = True\n    if self._server_stderr:\n      with open( self._server_stderr, 'r' ) as server_stderr_file:\n        error_output = ''.join( server_stderr_file.readlines()[\n            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )\n        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +\n                                        error_output )\n    else:\n        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )\n\n\n  def ServerPid( self ):\n    if not self._server_popen:\n      return -1\n    return self._server_popen.pid\n\n\n  def _ServerCleanup( self ):\n    if self._IsServerAlive():\n      self._server_popen.terminate()\n    utils.RemoveIfExists( self._temp_options_filename )\n\n\n  def RestartServer( self ):\n    vimsupport.PostVimMessage( 'Restarting ycmd server...' )\n    self._user_notified_about_crash = False\n    self._ServerCleanup()\n    self._SetupServer()\n\n\n  def CreateCompletionRequest( self, force_semantic = False ):\n    # We have to store a reference to the newly created CompletionRequest\n    # because VimScript can't store a reference to a Python object across\n    # function calls... Thus we need to keep this request somewhere.\n    if ( not self.NativeFiletypeCompletionAvailable() and\n         self.CurrentFiletypeCompletionEnabled() and\n         self._omnicomp.ShouldUseNow() ):\n      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )\n    else:\n      extra_data = {}\n      self._AddExtraConfDataIfNeeded( extra_data )\n      if force_semantic:\n        extra_data[ 'force_semantic' ] = True\n\n      self._latest_completion_request = ( CompletionRequest( extra_data )\n                                          if self._IsServerAlive() else\n                                          None )\n    return self._latest_completion_request\n\n\n  def SendCommandRequest( self, arguments, completer ):\n    if self._IsServerAlive():\n      return SendCommandRequest( arguments, completer )\n\n\n  def GetDefinedSubcommands( self ):\n    if self._IsServerAlive():\n      return BaseRequest.PostDataToHandler( BuildRequestData(),\n                                            'defined_subcommands' )\n    else:\n      return []\n\n\n  def GetCurrentCompletionRequest( self ):\n    return self._latest_completion_request\n\n\n  def GetOmniCompleter( self ):\n    return self._omnicomp\n\n\n  def NativeFiletypeCompletionAvailable( self ):\n    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in\n                  vimsupport.CurrentFiletypes() ] )\n\n\n  def NativeFiletypeCompletionUsable( self ):\n    return ( self.CurrentFiletypeCompletionEnabled() and\n             self.NativeFiletypeCompletionAvailable() )\n\n\n  def OnFileReadyToParse( self ):\n    self._omnicomp.OnFileReadyToParse( None )\n\n    if not self._IsServerAlive():\n      self._NotifyUserIfServerCrashed()\n\n    extra_data = {}\n    self._AddTagsFilesIfNeeded( extra_data )\n    self._AddSyntaxDataIfNeeded( extra_data )\n    self._AddExtraConfDataIfNeeded( extra_data )\n\n    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',\n                                                          extra_data )\n    self._latest_file_parse_request.Start()\n\n\n  def OnBufferUnload( self, deleted_buffer_file ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'BufferUnload',\n                                { 'unloaded_buffer': deleted_buffer_file } )\n\n\n  def OnBufferVisit( self ):\n    if not self._IsServerAlive():\n      return\n    extra_data = {}\n    _AddUltiSnipsDataIfNeeded( extra_data )\n    SendEventNotificationAsync( 'BufferVisit', extra_data )\n\n\n  def OnInsertLeave( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'InsertLeave' )\n\n\n  def OnCursorMoved( self ):\n    self._diag_interface.OnCursorMoved()\n\n\n  def OnVimLeave( self ):\n    self._ServerCleanup()\n\n\n  def OnCurrentIdentifierFinished( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'CurrentIdentifierFinished' )\n\n\n  def DiagnosticsForCurrentFileReady( self ):\n    return bool( self._latest_file_parse_request and\n                 self._latest_file_parse_request.Done() )\n\n\n  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):\n    if self.DiagnosticsForCurrentFileReady():\n      diagnostics = self._latest_file_parse_request.Response()\n      # We set the diagnostics request to None because we want to prevent\n      # Syntastic from repeatedly refreshing the buffer with the same diags.\n      # Setting this to None makes DiagnosticsForCurrentFileReady return False\n      # until the next request is created.\n      self._latest_file_parse_request = None\n      if qflist_format:\n        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )\n      else:\n        return diagnostics\n    return []\n\n\n  def UpdateDiagnosticInterface( self ):\n    if not self.DiagnosticsForCurrentFileReady():\n      return\n    self._diag_interface.UpdateWithNewDiagnostics(\n      self.GetDiagnosticsFromStoredRequest() )\n\n\n  def ShowDetailedDiagnostic( self ):\n    if not self._IsServerAlive():\n      return\n    try:\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'detailed_diagnostic' )\n      if 'message' in debug_info:\n        vimsupport.EchoText( debug_info[ 'message' ] )\n    except ServerError as e:\n      vimsupport.PostVimMessage( str( e ) )\n\n\n  def DebugInfo( self ):\n    if self._IsServerAlive():\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'debug_info' )\n    else:\n      debug_info = 'Server crashed, no debug info from server'\n    debug_info += '\\nServer running at: {0}'.format(\n        BaseRequest.server_location )\n    debug_info += '\\nServer process ID: {0}'.format( self._server_popen.pid )\n    if self._server_stderr or self._server_stdout:\n      debug_info += '\\nServer logfiles:\\n  {0}\\n  {1}'.format(\n        self._server_stdout,\n        self._server_stderr )\n\n    return debug_info\n\n\n  def CurrentFiletypeCompletionEnabled( self ):\n    filetypes = vimsupport.CurrentFiletypes()\n    filetype_to_disable = self._user_options[\n      'filetype_specific_completion_to_disable' ]\n    return not all([ x in filetype_to_disable for x in filetypes ])\n\n\n  def _AddSyntaxDataIfNeeded( self, extra_data ):\n    if not self._user_options[ 'seed_identifiers_with_syntax' ]:\n      return\n    filetype = vimsupport.CurrentFiletypes()[ 0 ]\n    if filetype in self._filetypes_with_keywords_loaded:\n      return\n\n    self._filetypes_with_keywords_loaded.add( filetype )\n    extra_data[ 'syntax_keywords' ] = list(\n       syntax_parse.SyntaxKeywordsForCurrentBuffer() )\n\n\n  def _AddTagsFilesIfNeeded( self, extra_data ):\n    def GetTagFiles():\n      tag_files = vim.eval( 'tagfiles()' )\n      current_working_directory = os.getcwd()\n      return [ os.path.join( current_working_directory, x ) for x in tag_files ]\n\n    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:\n      return\n    extra_data[ 'tag_files' ] = GetTagFiles()\n\n\n  def _AddExtraConfDataIfNeeded( self, extra_data ):\n    def BuildExtraConfData( extra_conf_vim_data ):\n      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )\n                   for expr in extra_conf_vim_data )\n\n    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]\n    if extra_conf_vim_data:\n      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(\n        extra_conf_vim_data )\n\n\ndef _PathToServerScript():\n  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )\n  return os.path.join( dir_of_current_script, 'server/ycmd.py' )\n\n\ndef _AddUltiSnipsDataIfNeeded( extra_data ):\n  if not USE_ULTISNIPS_DATA:\n    return\n\n  try:\n    rawsnips = UltiSnips_Manager._snips( '', 1 )\n  except:\n    return\n\n  # UltiSnips_Manager._snips() returns a class instance where:\n  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )\n  # class.description - description of the snippet\n  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,\n                                           'description': x.description\n                                         } for x in rawsnips ]\n\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/bujigr/YouCompleteMe/blob/abfc3ee36adab11c0c0b9d086a164a69006fec79",
        "file_path": "/python/ycm/client/base_request.py",
        "source": "#!/usr/bin/env python\n#\n# Copyright (C) 2013  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport vim\nimport requests\nimport urlparse\nfrom retries import retries\nfrom requests_futures.sessions import FuturesSession\nfrom ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\nfrom ycm import vimsupport\nfrom ycm.utils import ToUtf8Json\nfrom ycm.server.responses import ServerError, UnknownExtraConf\n\n_HEADERS = {'content-type': 'application/json'}\n_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n# Setting this to None seems to screw up the Requests/urllib3 libs.\n_DEFAULT_TIMEOUT_SEC = 30\n\nclass BaseRequest( object ):\n  def __init__( self ):\n    pass\n\n\n  def Start( self ):\n    pass\n\n\n  def Done( self ):\n    return True\n\n\n  def Response( self ):\n    return {}\n\n  # This method blocks\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',\n                                                            handler,\n                                                            'GET',\n                                                            timeout ) )\n\n\n  # This is the blocking version of the method. See below for async.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,\n                                                               handler,\n                                                               timeout ) )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |method| is either 'POST' or 'GET'.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def _TalkToHandlerAsync( data,\n                           handler,\n                           method,\n                           timeout = _DEFAULT_TIMEOUT_SEC ):\n    def SendRequest( data, handler, method, timeout ):\n      if method == 'POST':\n        return BaseRequest.session.post( _BuildUri( handler ),\n                                        data = ToUtf8Json( data ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n      if method == 'GET':\n        return BaseRequest.session.get( _BuildUri( handler ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n\n    @retries( 5, delay = 0.5, backoff = 1.5 )\n    def DelayedSendRequest( data, handler, method ):\n      if method == 'POST':\n        return requests.post( _BuildUri( handler ),\n                              data = ToUtf8Json( data ),\n                              headers = _HEADERS )\n      if method == 'GET':\n        return requests.get( _BuildUri( handler ),\n                             headers = _HEADERS )\n\n    if not _CheckServerIsHealthyWithCache():\n      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n\n    return SendRequest( data, handler, method, timeout )\n\n\n  session = FuturesSession( executor = _EXECUTOR )\n  server_location = 'http://localhost:6666'\n\n\ndef BuildRequestData( start_column = None,\n                      query = None,\n                      include_buffer_data = True ):\n  line, column = vimsupport.CurrentLineAndColumn()\n  filepath = vimsupport.GetCurrentBufferFilepath()\n  request_data = {\n    'filetypes': vimsupport.CurrentFiletypes(),\n    'line_num': line,\n    'column_num': column,\n    'start_column': start_column,\n    'line_value': vim.current.line,\n    'filepath': filepath\n  }\n\n  if include_buffer_data:\n    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()\n  if query:\n    request_data[ 'query' ] = query\n\n  return request_data\n\n\ndef JsonFromFuture( future ):\n  response = future.result()\n  if response.status_code == requests.codes.server_error:\n    _RaiseExceptionForData( response.json() )\n\n  # We let Requests handle the other status types, we only handle the 500\n  # error code.\n  response.raise_for_status()\n\n  if response.text:\n    return response.json()\n  return None\n\n\ndef _BuildUri( handler ):\n  return urlparse.urljoin( BaseRequest.server_location, handler )\n\n\nSERVER_HEALTHY = False\n\ndef _CheckServerIsHealthyWithCache():\n  global SERVER_HEALTHY\n\n  def _ServerIsHealthy():\n    response = requests.get( _BuildUri( 'healthy' ) )\n    response.raise_for_status()\n    return response.json()\n\n  if SERVER_HEALTHY:\n    return True\n\n  try:\n    SERVER_HEALTHY = _ServerIsHealthy()\n    return SERVER_HEALTHY\n  except:\n    return False\n\n\ndef _RaiseExceptionForData( data ):\n  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:\n    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )\n\n  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],\n                                        data[ 'message' ] ) )\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/bujigr/YouCompleteMe/blob/abfc3ee36adab11c0c0b9d086a164a69006fec79",
        "file_path": "/python/ycm/youcompleteme.py",
        "source": "#!/usr/bin/env python\n#\n# Copyright (C) 2011, 2012  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\nimport vim\nimport tempfile\nimport json\nimport signal\nfrom subprocess import PIPE\nfrom ycm import vimsupport\nfrom ycm import utils\nfrom ycm.diagnostic_interface import DiagnosticInterface\nfrom ycm.completers.all.omni_completer import OmniCompleter\nfrom ycm.completers.general import syntax_parse\nfrom ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype\nfrom ycm.client.ycmd_keepalive import YcmdKeepalive\nfrom ycm.client.base_request import BaseRequest, BuildRequestData\nfrom ycm.client.command_request import SendCommandRequest\nfrom ycm.client.completion_request import CompletionRequest\nfrom ycm.client.omni_completion_request import OmniCompletionRequest\nfrom ycm.client.event_notification import ( SendEventNotificationAsync,\n                                            EventNotification )\nfrom ycm.server.responses import ServerError\n\ntry:\n  from UltiSnips import UltiSnips_Manager\n  USE_ULTISNIPS_DATA = True\nexcept ImportError:\n  USE_ULTISNIPS_DATA = False\n\n# We need this so that Requests doesn't end up using the local HTTP proxy when\n# talking to ycmd. Users should actually be setting this themselves when\n# configuring a proxy server on their machine, but most don't know they need to\n# or how to do it, so we do it for them.\n# Relevant issues:\n#  https://github.com/Valloric/YouCompleteMe/issues/641\n#  https://github.com/kennethreitz/requests/issues/879\nos.environ['no_proxy'] = '127.0.0.1,localhost'\n\n# Force the Python interpreter embedded in Vim (in which we are running) to\n# ignore the SIGINT signal. This helps reduce the fallout of a user pressing\n# Ctrl-C in Vim.\nsignal.signal( signal.SIGINT, signal.SIG_IGN )\n\nNUM_YCMD_STDERR_LINES_ON_CRASH = 30\nSERVER_CRASH_MESSAGE_STDERR_FILE = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n  'Stderr (last {0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )\nSERVER_CRASH_MESSAGE_SAME_STDERR = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '\n  ' check console output for logs!' )\nSERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours\n\n\nclass YouCompleteMe( object ):\n  def __init__( self, user_options ):\n    self._user_options = user_options\n    self._user_notified_about_crash = False\n    self._diag_interface = DiagnosticInterface( user_options )\n    self._omnicomp = OmniCompleter( user_options )\n    self._latest_completion_request = None\n    self._latest_file_parse_request = None\n    self._server_stdout = None\n    self._server_stderr = None\n    self._server_popen = None\n    self._filetypes_with_keywords_loaded = set()\n    self._temp_options_filename = None\n    self._ycmd_keepalive = YcmdKeepalive()\n    self._SetupServer()\n    self._ycmd_keepalive.Start()\n\n  def _SetupServer( self ):\n    server_port = utils.GetUnusedLocalhostPort()\n    with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n      self._temp_options_filename = options_file.name\n      json.dump( dict( self._user_options ), options_file )\n      options_file.flush()\n\n      args = [ utils.PathToPythonInterpreter(),\n               _PathToServerScript(),\n               '--port={0}'.format( server_port ),\n               '--options_file={0}'.format( options_file.name ),\n               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),\n               '--idle_suicide_seconds={0}'.format(\n                  SERVER_IDLE_SUICIDE_SECONDS )]\n\n      if not self._user_options[ 'server_use_vim_stdout' ]:\n        filename_format = os.path.join( utils.PathToTempDir(),\n                                        'server_{port}_{std}.log' )\n\n        self._server_stdout = filename_format.format( port = server_port,\n                                                      std = 'stdout' )\n        self._server_stderr = filename_format.format( port = server_port,\n                                                      std = 'stderr' )\n        args.append('--stdout={0}'.format( self._server_stdout ))\n        args.append('--stderr={0}'.format( self._server_stderr ))\n\n        if self._user_options[ 'server_keep_logfiles' ]:\n          args.append('--keep_logfiles')\n\n      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n      BaseRequest.server_location = 'http://localhost:' + str( server_port )\n\n    self._NotifyUserIfServerCrashed()\n\n  def _IsServerAlive( self ):\n    returncode = self._server_popen.poll()\n    # When the process hasn't finished yet, poll() returns None.\n    return returncode is None\n\n\n  def _NotifyUserIfServerCrashed( self ):\n    if self._user_notified_about_crash or self._IsServerAlive():\n      return\n    self._user_notified_about_crash = True\n    if self._server_stderr:\n      with open( self._server_stderr, 'r' ) as server_stderr_file:\n        error_output = ''.join( server_stderr_file.readlines()[\n            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )\n        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +\n                                        error_output )\n    else:\n        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )\n\n\n  def ServerPid( self ):\n    if not self._server_popen:\n      return -1\n    return self._server_popen.pid\n\n\n  def _ServerCleanup( self ):\n    if self._IsServerAlive():\n      self._server_popen.terminate()\n    utils.RemoveIfExists( self._temp_options_filename )\n\n\n  def RestartServer( self ):\n    vimsupport.PostVimMessage( 'Restarting ycmd server...' )\n    self._user_notified_about_crash = False\n    self._ServerCleanup()\n    self._SetupServer()\n\n\n  def CreateCompletionRequest( self, force_semantic = False ):\n    # We have to store a reference to the newly created CompletionRequest\n    # because VimScript can't store a reference to a Python object across\n    # function calls... Thus we need to keep this request somewhere.\n    if ( not self.NativeFiletypeCompletionAvailable() and\n         self.CurrentFiletypeCompletionEnabled() and\n         self._omnicomp.ShouldUseNow() ):\n      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )\n    else:\n      extra_data = {}\n      self._AddExtraConfDataIfNeeded( extra_data )\n      if force_semantic:\n        extra_data[ 'force_semantic' ] = True\n\n      self._latest_completion_request = ( CompletionRequest( extra_data )\n                                          if self._IsServerAlive() else\n                                          None )\n    return self._latest_completion_request\n\n\n  def SendCommandRequest( self, arguments, completer ):\n    if self._IsServerAlive():\n      return SendCommandRequest( arguments, completer )\n\n\n  def GetDefinedSubcommands( self ):\n    if self._IsServerAlive():\n      return BaseRequest.PostDataToHandler( BuildRequestData(),\n                                            'defined_subcommands' )\n    else:\n      return []\n\n\n  def GetCurrentCompletionRequest( self ):\n    return self._latest_completion_request\n\n\n  def GetOmniCompleter( self ):\n    return self._omnicomp\n\n\n  def NativeFiletypeCompletionAvailable( self ):\n    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in\n                  vimsupport.CurrentFiletypes() ] )\n\n\n  def NativeFiletypeCompletionUsable( self ):\n    return ( self.CurrentFiletypeCompletionEnabled() and\n             self.NativeFiletypeCompletionAvailable() )\n\n\n  def OnFileReadyToParse( self ):\n    self._omnicomp.OnFileReadyToParse( None )\n\n    if not self._IsServerAlive():\n      self._NotifyUserIfServerCrashed()\n\n    extra_data = {}\n    self._AddTagsFilesIfNeeded( extra_data )\n    self._AddSyntaxDataIfNeeded( extra_data )\n    self._AddExtraConfDataIfNeeded( extra_data )\n\n    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',\n                                                          extra_data )\n    self._latest_file_parse_request.Start()\n\n\n  def OnBufferUnload( self, deleted_buffer_file ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'BufferUnload',\n                                { 'unloaded_buffer': deleted_buffer_file } )\n\n\n  def OnBufferVisit( self ):\n    if not self._IsServerAlive():\n      return\n    extra_data = {}\n    _AddUltiSnipsDataIfNeeded( extra_data )\n    SendEventNotificationAsync( 'BufferVisit', extra_data )\n\n\n  def OnInsertLeave( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'InsertLeave' )\n\n\n  def OnCursorMoved( self ):\n    self._diag_interface.OnCursorMoved()\n\n\n  def OnVimLeave( self ):\n    self._ServerCleanup()\n\n\n  def OnCurrentIdentifierFinished( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'CurrentIdentifierFinished' )\n\n\n  def DiagnosticsForCurrentFileReady( self ):\n    return bool( self._latest_file_parse_request and\n                 self._latest_file_parse_request.Done() )\n\n\n  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):\n    if self.DiagnosticsForCurrentFileReady():\n      diagnostics = self._latest_file_parse_request.Response()\n      # We set the diagnostics request to None because we want to prevent\n      # Syntastic from repeatedly refreshing the buffer with the same diags.\n      # Setting this to None makes DiagnosticsForCurrentFileReady return False\n      # until the next request is created.\n      self._latest_file_parse_request = None\n      if qflist_format:\n        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )\n      else:\n        return diagnostics\n    return []\n\n\n  def UpdateDiagnosticInterface( self ):\n    if not self.DiagnosticsForCurrentFileReady():\n      return\n    self._diag_interface.UpdateWithNewDiagnostics(\n      self.GetDiagnosticsFromStoredRequest() )\n\n\n  def ShowDetailedDiagnostic( self ):\n    if not self._IsServerAlive():\n      return\n    try:\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'detailed_diagnostic' )\n      if 'message' in debug_info:\n        vimsupport.EchoText( debug_info[ 'message' ] )\n    except ServerError as e:\n      vimsupport.PostVimMessage( str( e ) )\n\n\n  def DebugInfo( self ):\n    if self._IsServerAlive():\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'debug_info' )\n    else:\n      debug_info = 'Server crashed, no debug info from server'\n    debug_info += '\\nServer running at: {0}'.format(\n        BaseRequest.server_location )\n    debug_info += '\\nServer process ID: {0}'.format( self._server_popen.pid )\n    if self._server_stderr or self._server_stdout:\n      debug_info += '\\nServer logfiles:\\n  {0}\\n  {1}'.format(\n        self._server_stdout,\n        self._server_stderr )\n\n    return debug_info\n\n\n  def CurrentFiletypeCompletionEnabled( self ):\n    filetypes = vimsupport.CurrentFiletypes()\n    filetype_to_disable = self._user_options[\n      'filetype_specific_completion_to_disable' ]\n    return not all([ x in filetype_to_disable for x in filetypes ])\n\n\n  def _AddSyntaxDataIfNeeded( self, extra_data ):\n    if not self._user_options[ 'seed_identifiers_with_syntax' ]:\n      return\n    filetype = vimsupport.CurrentFiletypes()[ 0 ]\n    if filetype in self._filetypes_with_keywords_loaded:\n      return\n\n    self._filetypes_with_keywords_loaded.add( filetype )\n    extra_data[ 'syntax_keywords' ] = list(\n       syntax_parse.SyntaxKeywordsForCurrentBuffer() )\n\n\n  def _AddTagsFilesIfNeeded( self, extra_data ):\n    def GetTagFiles():\n      tag_files = vim.eval( 'tagfiles()' )\n      current_working_directory = os.getcwd()\n      return [ os.path.join( current_working_directory, x ) for x in tag_files ]\n\n    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:\n      return\n    extra_data[ 'tag_files' ] = GetTagFiles()\n\n\n  def _AddExtraConfDataIfNeeded( self, extra_data ):\n    def BuildExtraConfData( extra_conf_vim_data ):\n      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )\n                   for expr in extra_conf_vim_data )\n\n    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]\n    if extra_conf_vim_data:\n      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(\n        extra_conf_vim_data )\n\n\ndef _PathToServerScript():\n  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )\n  return os.path.join( dir_of_current_script, 'server/ycmd.py' )\n\n\ndef _AddUltiSnipsDataIfNeeded( extra_data ):\n  if not USE_ULTISNIPS_DATA:\n    return\n\n  try:\n    rawsnips = UltiSnips_Manager._snips( '', 1 )\n  except:\n    return\n\n  # UltiSnips_Manager._snips() returns a class instance where:\n  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )\n  # class.description - description of the snippet\n  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,\n                                           'description': x.description\n                                         } for x in rawsnips ]\n\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/viperauter/ubuntu-fix/blob/abfc3ee36adab11c0c0b9d086a164a69006fec79",
        "file_path": "/python/ycm/client/base_request.py",
        "source": "#!/usr/bin/env python\n#\n# Copyright (C) 2013  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport vim\nimport requests\nimport urlparse\nfrom retries import retries\nfrom requests_futures.sessions import FuturesSession\nfrom ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\nfrom ycm import vimsupport\nfrom ycm.utils import ToUtf8Json\nfrom ycm.server.responses import ServerError, UnknownExtraConf\n\n_HEADERS = {'content-type': 'application/json'}\n_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n# Setting this to None seems to screw up the Requests/urllib3 libs.\n_DEFAULT_TIMEOUT_SEC = 30\n\nclass BaseRequest( object ):\n  def __init__( self ):\n    pass\n\n\n  def Start( self ):\n    pass\n\n\n  def Done( self ):\n    return True\n\n\n  def Response( self ):\n    return {}\n\n  # This method blocks\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',\n                                                            handler,\n                                                            'GET',\n                                                            timeout ) )\n\n\n  # This is the blocking version of the method. See below for async.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,\n                                                               handler,\n                                                               timeout ) )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |method| is either 'POST' or 'GET'.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def _TalkToHandlerAsync( data,\n                           handler,\n                           method,\n                           timeout = _DEFAULT_TIMEOUT_SEC ):\n    def SendRequest( data, handler, method, timeout ):\n      if method == 'POST':\n        return BaseRequest.session.post( _BuildUri( handler ),\n                                        data = ToUtf8Json( data ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n      if method == 'GET':\n        return BaseRequest.session.get( _BuildUri( handler ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n\n    @retries( 5, delay = 0.5, backoff = 1.5 )\n    def DelayedSendRequest( data, handler, method ):\n      if method == 'POST':\n        return requests.post( _BuildUri( handler ),\n                              data = ToUtf8Json( data ),\n                              headers = _HEADERS )\n      if method == 'GET':\n        return requests.get( _BuildUri( handler ),\n                             headers = _HEADERS )\n\n    if not _CheckServerIsHealthyWithCache():\n      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n\n    return SendRequest( data, handler, method, timeout )\n\n\n  session = FuturesSession( executor = _EXECUTOR )\n  server_location = 'http://localhost:6666'\n\n\ndef BuildRequestData( start_column = None,\n                      query = None,\n                      include_buffer_data = True ):\n  line, column = vimsupport.CurrentLineAndColumn()\n  filepath = vimsupport.GetCurrentBufferFilepath()\n  request_data = {\n    'filetypes': vimsupport.CurrentFiletypes(),\n    'line_num': line,\n    'column_num': column,\n    'start_column': start_column,\n    'line_value': vim.current.line,\n    'filepath': filepath\n  }\n\n  if include_buffer_data:\n    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()\n  if query:\n    request_data[ 'query' ] = query\n\n  return request_data\n\n\ndef JsonFromFuture( future ):\n  response = future.result()\n  if response.status_code == requests.codes.server_error:\n    _RaiseExceptionForData( response.json() )\n\n  # We let Requests handle the other status types, we only handle the 500\n  # error code.\n  response.raise_for_status()\n\n  if response.text:\n    return response.json()\n  return None\n\n\ndef _BuildUri( handler ):\n  return urlparse.urljoin( BaseRequest.server_location, handler )\n\n\nSERVER_HEALTHY = False\n\ndef _CheckServerIsHealthyWithCache():\n  global SERVER_HEALTHY\n\n  def _ServerIsHealthy():\n    response = requests.get( _BuildUri( 'healthy' ) )\n    response.raise_for_status()\n    return response.json()\n\n  if SERVER_HEALTHY:\n    return True\n\n  try:\n    SERVER_HEALTHY = _ServerIsHealthy()\n    return SERVER_HEALTHY\n  except:\n    return False\n\n\ndef _RaiseExceptionForData( data ):\n  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:\n    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )\n\n  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],\n                                        data[ 'message' ] ) )\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/viperauter/ubuntu-fix/blob/abfc3ee36adab11c0c0b9d086a164a69006fec79",
        "file_path": "/python/ycm/youcompleteme.py",
        "source": "#!/usr/bin/env python\n#\n# Copyright (C) 2011, 2012  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\nimport vim\nimport tempfile\nimport json\nimport signal\nfrom subprocess import PIPE\nfrom ycm import vimsupport\nfrom ycm import utils\nfrom ycm.diagnostic_interface import DiagnosticInterface\nfrom ycm.completers.all.omni_completer import OmniCompleter\nfrom ycm.completers.general import syntax_parse\nfrom ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype\nfrom ycm.client.ycmd_keepalive import YcmdKeepalive\nfrom ycm.client.base_request import BaseRequest, BuildRequestData\nfrom ycm.client.command_request import SendCommandRequest\nfrom ycm.client.completion_request import CompletionRequest\nfrom ycm.client.omni_completion_request import OmniCompletionRequest\nfrom ycm.client.event_notification import ( SendEventNotificationAsync,\n                                            EventNotification )\nfrom ycm.server.responses import ServerError\n\ntry:\n  from UltiSnips import UltiSnips_Manager\n  USE_ULTISNIPS_DATA = True\nexcept ImportError:\n  USE_ULTISNIPS_DATA = False\n\n# We need this so that Requests doesn't end up using the local HTTP proxy when\n# talking to ycmd. Users should actually be setting this themselves when\n# configuring a proxy server on their machine, but most don't know they need to\n# or how to do it, so we do it for them.\n# Relevant issues:\n#  https://github.com/Valloric/YouCompleteMe/issues/641\n#  https://github.com/kennethreitz/requests/issues/879\nos.environ['no_proxy'] = '127.0.0.1,localhost'\n\n# Force the Python interpreter embedded in Vim (in which we are running) to\n# ignore the SIGINT signal. This helps reduce the fallout of a user pressing\n# Ctrl-C in Vim.\nsignal.signal( signal.SIGINT, signal.SIG_IGN )\n\nNUM_YCMD_STDERR_LINES_ON_CRASH = 30\nSERVER_CRASH_MESSAGE_STDERR_FILE = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n  'Stderr (last {0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )\nSERVER_CRASH_MESSAGE_SAME_STDERR = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '\n  ' check console output for logs!' )\nSERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours\n\n\nclass YouCompleteMe( object ):\n  def __init__( self, user_options ):\n    self._user_options = user_options\n    self._user_notified_about_crash = False\n    self._diag_interface = DiagnosticInterface( user_options )\n    self._omnicomp = OmniCompleter( user_options )\n    self._latest_completion_request = None\n    self._latest_file_parse_request = None\n    self._server_stdout = None\n    self._server_stderr = None\n    self._server_popen = None\n    self._filetypes_with_keywords_loaded = set()\n    self._temp_options_filename = None\n    self._ycmd_keepalive = YcmdKeepalive()\n    self._SetupServer()\n    self._ycmd_keepalive.Start()\n\n  def _SetupServer( self ):\n    server_port = utils.GetUnusedLocalhostPort()\n    with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n      self._temp_options_filename = options_file.name\n      json.dump( dict( self._user_options ), options_file )\n      options_file.flush()\n\n      args = [ utils.PathToPythonInterpreter(),\n               _PathToServerScript(),\n               '--port={0}'.format( server_port ),\n               '--options_file={0}'.format( options_file.name ),\n               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),\n               '--idle_suicide_seconds={0}'.format(\n                  SERVER_IDLE_SUICIDE_SECONDS )]\n\n      if not self._user_options[ 'server_use_vim_stdout' ]:\n        filename_format = os.path.join( utils.PathToTempDir(),\n                                        'server_{port}_{std}.log' )\n\n        self._server_stdout = filename_format.format( port = server_port,\n                                                      std = 'stdout' )\n        self._server_stderr = filename_format.format( port = server_port,\n                                                      std = 'stderr' )\n        args.append('--stdout={0}'.format( self._server_stdout ))\n        args.append('--stderr={0}'.format( self._server_stderr ))\n\n        if self._user_options[ 'server_keep_logfiles' ]:\n          args.append('--keep_logfiles')\n\n      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n      BaseRequest.server_location = 'http://localhost:' + str( server_port )\n\n    self._NotifyUserIfServerCrashed()\n\n  def _IsServerAlive( self ):\n    returncode = self._server_popen.poll()\n    # When the process hasn't finished yet, poll() returns None.\n    return returncode is None\n\n\n  def _NotifyUserIfServerCrashed( self ):\n    if self._user_notified_about_crash or self._IsServerAlive():\n      return\n    self._user_notified_about_crash = True\n    if self._server_stderr:\n      with open( self._server_stderr, 'r' ) as server_stderr_file:\n        error_output = ''.join( server_stderr_file.readlines()[\n            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )\n        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +\n                                        error_output )\n    else:\n        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )\n\n\n  def ServerPid( self ):\n    if not self._server_popen:\n      return -1\n    return self._server_popen.pid\n\n\n  def _ServerCleanup( self ):\n    if self._IsServerAlive():\n      self._server_popen.terminate()\n    utils.RemoveIfExists( self._temp_options_filename )\n\n\n  def RestartServer( self ):\n    vimsupport.PostVimMessage( 'Restarting ycmd server...' )\n    self._user_notified_about_crash = False\n    self._ServerCleanup()\n    self._SetupServer()\n\n\n  def CreateCompletionRequest( self, force_semantic = False ):\n    # We have to store a reference to the newly created CompletionRequest\n    # because VimScript can't store a reference to a Python object across\n    # function calls... Thus we need to keep this request somewhere.\n    if ( not self.NativeFiletypeCompletionAvailable() and\n         self.CurrentFiletypeCompletionEnabled() and\n         self._omnicomp.ShouldUseNow() ):\n      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )\n    else:\n      extra_data = {}\n      self._AddExtraConfDataIfNeeded( extra_data )\n      if force_semantic:\n        extra_data[ 'force_semantic' ] = True\n\n      self._latest_completion_request = ( CompletionRequest( extra_data )\n                                          if self._IsServerAlive() else\n                                          None )\n    return self._latest_completion_request\n\n\n  def SendCommandRequest( self, arguments, completer ):\n    if self._IsServerAlive():\n      return SendCommandRequest( arguments, completer )\n\n\n  def GetDefinedSubcommands( self ):\n    if self._IsServerAlive():\n      return BaseRequest.PostDataToHandler( BuildRequestData(),\n                                            'defined_subcommands' )\n    else:\n      return []\n\n\n  def GetCurrentCompletionRequest( self ):\n    return self._latest_completion_request\n\n\n  def GetOmniCompleter( self ):\n    return self._omnicomp\n\n\n  def NativeFiletypeCompletionAvailable( self ):\n    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in\n                  vimsupport.CurrentFiletypes() ] )\n\n\n  def NativeFiletypeCompletionUsable( self ):\n    return ( self.CurrentFiletypeCompletionEnabled() and\n             self.NativeFiletypeCompletionAvailable() )\n\n\n  def OnFileReadyToParse( self ):\n    self._omnicomp.OnFileReadyToParse( None )\n\n    if not self._IsServerAlive():\n      self._NotifyUserIfServerCrashed()\n\n    extra_data = {}\n    self._AddTagsFilesIfNeeded( extra_data )\n    self._AddSyntaxDataIfNeeded( extra_data )\n    self._AddExtraConfDataIfNeeded( extra_data )\n\n    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',\n                                                          extra_data )\n    self._latest_file_parse_request.Start()\n\n\n  def OnBufferUnload( self, deleted_buffer_file ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'BufferUnload',\n                                { 'unloaded_buffer': deleted_buffer_file } )\n\n\n  def OnBufferVisit( self ):\n    if not self._IsServerAlive():\n      return\n    extra_data = {}\n    _AddUltiSnipsDataIfNeeded( extra_data )\n    SendEventNotificationAsync( 'BufferVisit', extra_data )\n\n\n  def OnInsertLeave( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'InsertLeave' )\n\n\n  def OnCursorMoved( self ):\n    self._diag_interface.OnCursorMoved()\n\n\n  def OnVimLeave( self ):\n    self._ServerCleanup()\n\n\n  def OnCurrentIdentifierFinished( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'CurrentIdentifierFinished' )\n\n\n  def DiagnosticsForCurrentFileReady( self ):\n    return bool( self._latest_file_parse_request and\n                 self._latest_file_parse_request.Done() )\n\n\n  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):\n    if self.DiagnosticsForCurrentFileReady():\n      diagnostics = self._latest_file_parse_request.Response()\n      # We set the diagnostics request to None because we want to prevent\n      # Syntastic from repeatedly refreshing the buffer with the same diags.\n      # Setting this to None makes DiagnosticsForCurrentFileReady return False\n      # until the next request is created.\n      self._latest_file_parse_request = None\n      if qflist_format:\n        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )\n      else:\n        return diagnostics\n    return []\n\n\n  def UpdateDiagnosticInterface( self ):\n    if not self.DiagnosticsForCurrentFileReady():\n      return\n    self._diag_interface.UpdateWithNewDiagnostics(\n      self.GetDiagnosticsFromStoredRequest() )\n\n\n  def ShowDetailedDiagnostic( self ):\n    if not self._IsServerAlive():\n      return\n    try:\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'detailed_diagnostic' )\n      if 'message' in debug_info:\n        vimsupport.EchoText( debug_info[ 'message' ] )\n    except ServerError as e:\n      vimsupport.PostVimMessage( str( e ) )\n\n\n  def DebugInfo( self ):\n    if self._IsServerAlive():\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'debug_info' )\n    else:\n      debug_info = 'Server crashed, no debug info from server'\n    debug_info += '\\nServer running at: {0}'.format(\n        BaseRequest.server_location )\n    debug_info += '\\nServer process ID: {0}'.format( self._server_popen.pid )\n    if self._server_stderr or self._server_stdout:\n      debug_info += '\\nServer logfiles:\\n  {0}\\n  {1}'.format(\n        self._server_stdout,\n        self._server_stderr )\n\n    return debug_info\n\n\n  def CurrentFiletypeCompletionEnabled( self ):\n    filetypes = vimsupport.CurrentFiletypes()\n    filetype_to_disable = self._user_options[\n      'filetype_specific_completion_to_disable' ]\n    return not all([ x in filetype_to_disable for x in filetypes ])\n\n\n  def _AddSyntaxDataIfNeeded( self, extra_data ):\n    if not self._user_options[ 'seed_identifiers_with_syntax' ]:\n      return\n    filetype = vimsupport.CurrentFiletypes()[ 0 ]\n    if filetype in self._filetypes_with_keywords_loaded:\n      return\n\n    self._filetypes_with_keywords_loaded.add( filetype )\n    extra_data[ 'syntax_keywords' ] = list(\n       syntax_parse.SyntaxKeywordsForCurrentBuffer() )\n\n\n  def _AddTagsFilesIfNeeded( self, extra_data ):\n    def GetTagFiles():\n      tag_files = vim.eval( 'tagfiles()' )\n      current_working_directory = os.getcwd()\n      return [ os.path.join( current_working_directory, x ) for x in tag_files ]\n\n    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:\n      return\n    extra_data[ 'tag_files' ] = GetTagFiles()\n\n\n  def _AddExtraConfDataIfNeeded( self, extra_data ):\n    def BuildExtraConfData( extra_conf_vim_data ):\n      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )\n                   for expr in extra_conf_vim_data )\n\n    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]\n    if extra_conf_vim_data:\n      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(\n        extra_conf_vim_data )\n\n\ndef _PathToServerScript():\n  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )\n  return os.path.join( dir_of_current_script, 'server/ycmd.py' )\n\n\ndef _AddUltiSnipsDataIfNeeded( extra_data ):\n  if not USE_ULTISNIPS_DATA:\n    return\n\n  try:\n    rawsnips = UltiSnips_Manager._snips( '', 1 )\n  except:\n    return\n\n  # UltiSnips_Manager._snips() returns a class instance where:\n  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )\n  # class.description - description of the snippet\n  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,\n                                           'description': x.description\n                                         } for x in rawsnips ]\n\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/tibi77/vim/blob/abfc3ee36adab11c0c0b9d086a164a69006fec79",
        "file_path": "/python/ycm/client/base_request.py",
        "source": "#!/usr/bin/env python\n#\n# Copyright (C) 2013  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport vim\nimport requests\nimport urlparse\nfrom retries import retries\nfrom requests_futures.sessions import FuturesSession\nfrom ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\nfrom ycm import vimsupport\nfrom ycm.utils import ToUtf8Json\nfrom ycm.server.responses import ServerError, UnknownExtraConf\n\n_HEADERS = {'content-type': 'application/json'}\n_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n# Setting this to None seems to screw up the Requests/urllib3 libs.\n_DEFAULT_TIMEOUT_SEC = 30\n\nclass BaseRequest( object ):\n  def __init__( self ):\n    pass\n\n\n  def Start( self ):\n    pass\n\n\n  def Done( self ):\n    return True\n\n\n  def Response( self ):\n    return {}\n\n  # This method blocks\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',\n                                                            handler,\n                                                            'GET',\n                                                            timeout ) )\n\n\n  # This is the blocking version of the method. See below for async.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,\n                                                               handler,\n                                                               timeout ) )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |method| is either 'POST' or 'GET'.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def _TalkToHandlerAsync( data,\n                           handler,\n                           method,\n                           timeout = _DEFAULT_TIMEOUT_SEC ):\n    def SendRequest( data, handler, method, timeout ):\n      if method == 'POST':\n        return BaseRequest.session.post( _BuildUri( handler ),\n                                        data = ToUtf8Json( data ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n      if method == 'GET':\n        return BaseRequest.session.get( _BuildUri( handler ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n\n    @retries( 5, delay = 0.5, backoff = 1.5 )\n    def DelayedSendRequest( data, handler, method ):\n      if method == 'POST':\n        return requests.post( _BuildUri( handler ),\n                              data = ToUtf8Json( data ),\n                              headers = _HEADERS )\n      if method == 'GET':\n        return requests.get( _BuildUri( handler ),\n                             headers = _HEADERS )\n\n    if not _CheckServerIsHealthyWithCache():\n      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n\n    return SendRequest( data, handler, method, timeout )\n\n\n  session = FuturesSession( executor = _EXECUTOR )\n  server_location = 'http://localhost:6666'\n\n\ndef BuildRequestData( start_column = None,\n                      query = None,\n                      include_buffer_data = True ):\n  line, column = vimsupport.CurrentLineAndColumn()\n  filepath = vimsupport.GetCurrentBufferFilepath()\n  request_data = {\n    'filetypes': vimsupport.CurrentFiletypes(),\n    'line_num': line,\n    'column_num': column,\n    'start_column': start_column,\n    'line_value': vim.current.line,\n    'filepath': filepath\n  }\n\n  if include_buffer_data:\n    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()\n  if query:\n    request_data[ 'query' ] = query\n\n  return request_data\n\n\ndef JsonFromFuture( future ):\n  response = future.result()\n  if response.status_code == requests.codes.server_error:\n    _RaiseExceptionForData( response.json() )\n\n  # We let Requests handle the other status types, we only handle the 500\n  # error code.\n  response.raise_for_status()\n\n  if response.text:\n    return response.json()\n  return None\n\n\ndef _BuildUri( handler ):\n  return urlparse.urljoin( BaseRequest.server_location, handler )\n\n\nSERVER_HEALTHY = False\n\ndef _CheckServerIsHealthyWithCache():\n  global SERVER_HEALTHY\n\n  def _ServerIsHealthy():\n    response = requests.get( _BuildUri( 'healthy' ) )\n    response.raise_for_status()\n    return response.json()\n\n  if SERVER_HEALTHY:\n    return True\n\n  try:\n    SERVER_HEALTHY = _ServerIsHealthy()\n    return SERVER_HEALTHY\n  except:\n    return False\n\n\ndef _RaiseExceptionForData( data ):\n  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:\n    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )\n\n  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],\n                                        data[ 'message' ] ) )\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/tibi77/vim/blob/abfc3ee36adab11c0c0b9d086a164a69006fec79",
        "file_path": "/python/ycm/youcompleteme.py",
        "source": "#!/usr/bin/env python\n#\n# Copyright (C) 2011, 2012  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\nimport vim\nimport tempfile\nimport json\nimport signal\nfrom subprocess import PIPE\nfrom ycm import vimsupport\nfrom ycm import utils\nfrom ycm.diagnostic_interface import DiagnosticInterface\nfrom ycm.completers.all.omni_completer import OmniCompleter\nfrom ycm.completers.general import syntax_parse\nfrom ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype\nfrom ycm.client.ycmd_keepalive import YcmdKeepalive\nfrom ycm.client.base_request import BaseRequest, BuildRequestData\nfrom ycm.client.command_request import SendCommandRequest\nfrom ycm.client.completion_request import CompletionRequest\nfrom ycm.client.omni_completion_request import OmniCompletionRequest\nfrom ycm.client.event_notification import ( SendEventNotificationAsync,\n                                            EventNotification )\nfrom ycm.server.responses import ServerError\n\ntry:\n  from UltiSnips import UltiSnips_Manager\n  USE_ULTISNIPS_DATA = True\nexcept ImportError:\n  USE_ULTISNIPS_DATA = False\n\n# We need this so that Requests doesn't end up using the local HTTP proxy when\n# talking to ycmd. Users should actually be setting this themselves when\n# configuring a proxy server on their machine, but most don't know they need to\n# or how to do it, so we do it for them.\n# Relevant issues:\n#  https://github.com/Valloric/YouCompleteMe/issues/641\n#  https://github.com/kennethreitz/requests/issues/879\nos.environ['no_proxy'] = '127.0.0.1,localhost'\n\n# Force the Python interpreter embedded in Vim (in which we are running) to\n# ignore the SIGINT signal. This helps reduce the fallout of a user pressing\n# Ctrl-C in Vim.\nsignal.signal( signal.SIGINT, signal.SIG_IGN )\n\nNUM_YCMD_STDERR_LINES_ON_CRASH = 30\nSERVER_CRASH_MESSAGE_STDERR_FILE = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n  'Stderr (last {0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )\nSERVER_CRASH_MESSAGE_SAME_STDERR = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '\n  ' check console output for logs!' )\nSERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours\n\n\nclass YouCompleteMe( object ):\n  def __init__( self, user_options ):\n    self._user_options = user_options\n    self._user_notified_about_crash = False\n    self._diag_interface = DiagnosticInterface( user_options )\n    self._omnicomp = OmniCompleter( user_options )\n    self._latest_completion_request = None\n    self._latest_file_parse_request = None\n    self._server_stdout = None\n    self._server_stderr = None\n    self._server_popen = None\n    self._filetypes_with_keywords_loaded = set()\n    self._temp_options_filename = None\n    self._ycmd_keepalive = YcmdKeepalive()\n    self._SetupServer()\n    self._ycmd_keepalive.Start()\n\n  def _SetupServer( self ):\n    server_port = utils.GetUnusedLocalhostPort()\n    with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n      self._temp_options_filename = options_file.name\n      json.dump( dict( self._user_options ), options_file )\n      options_file.flush()\n\n      args = [ utils.PathToPythonInterpreter(),\n               _PathToServerScript(),\n               '--port={0}'.format( server_port ),\n               '--options_file={0}'.format( options_file.name ),\n               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),\n               '--idle_suicide_seconds={0}'.format(\n                  SERVER_IDLE_SUICIDE_SECONDS )]\n\n      if not self._user_options[ 'server_use_vim_stdout' ]:\n        filename_format = os.path.join( utils.PathToTempDir(),\n                                        'server_{port}_{std}.log' )\n\n        self._server_stdout = filename_format.format( port = server_port,\n                                                      std = 'stdout' )\n        self._server_stderr = filename_format.format( port = server_port,\n                                                      std = 'stderr' )\n        args.append('--stdout={0}'.format( self._server_stdout ))\n        args.append('--stderr={0}'.format( self._server_stderr ))\n\n        if self._user_options[ 'server_keep_logfiles' ]:\n          args.append('--keep_logfiles')\n\n      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n      BaseRequest.server_location = 'http://localhost:' + str( server_port )\n\n    self._NotifyUserIfServerCrashed()\n\n  def _IsServerAlive( self ):\n    returncode = self._server_popen.poll()\n    # When the process hasn't finished yet, poll() returns None.\n    return returncode is None\n\n\n  def _NotifyUserIfServerCrashed( self ):\n    if self._user_notified_about_crash or self._IsServerAlive():\n      return\n    self._user_notified_about_crash = True\n    if self._server_stderr:\n      with open( self._server_stderr, 'r' ) as server_stderr_file:\n        error_output = ''.join( server_stderr_file.readlines()[\n            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )\n        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +\n                                        error_output )\n    else:\n        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )\n\n\n  def ServerPid( self ):\n    if not self._server_popen:\n      return -1\n    return self._server_popen.pid\n\n\n  def _ServerCleanup( self ):\n    if self._IsServerAlive():\n      self._server_popen.terminate()\n    utils.RemoveIfExists( self._temp_options_filename )\n\n\n  def RestartServer( self ):\n    vimsupport.PostVimMessage( 'Restarting ycmd server...' )\n    self._user_notified_about_crash = False\n    self._ServerCleanup()\n    self._SetupServer()\n\n\n  def CreateCompletionRequest( self, force_semantic = False ):\n    # We have to store a reference to the newly created CompletionRequest\n    # because VimScript can't store a reference to a Python object across\n    # function calls... Thus we need to keep this request somewhere.\n    if ( not self.NativeFiletypeCompletionAvailable() and\n         self.CurrentFiletypeCompletionEnabled() and\n         self._omnicomp.ShouldUseNow() ):\n      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )\n    else:\n      extra_data = {}\n      self._AddExtraConfDataIfNeeded( extra_data )\n      if force_semantic:\n        extra_data[ 'force_semantic' ] = True\n\n      self._latest_completion_request = ( CompletionRequest( extra_data )\n                                          if self._IsServerAlive() else\n                                          None )\n    return self._latest_completion_request\n\n\n  def SendCommandRequest( self, arguments, completer ):\n    if self._IsServerAlive():\n      return SendCommandRequest( arguments, completer )\n\n\n  def GetDefinedSubcommands( self ):\n    if self._IsServerAlive():\n      return BaseRequest.PostDataToHandler( BuildRequestData(),\n                                            'defined_subcommands' )\n    else:\n      return []\n\n\n  def GetCurrentCompletionRequest( self ):\n    return self._latest_completion_request\n\n\n  def GetOmniCompleter( self ):\n    return self._omnicomp\n\n\n  def NativeFiletypeCompletionAvailable( self ):\n    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in\n                  vimsupport.CurrentFiletypes() ] )\n\n\n  def NativeFiletypeCompletionUsable( self ):\n    return ( self.CurrentFiletypeCompletionEnabled() and\n             self.NativeFiletypeCompletionAvailable() )\n\n\n  def OnFileReadyToParse( self ):\n    self._omnicomp.OnFileReadyToParse( None )\n\n    if not self._IsServerAlive():\n      self._NotifyUserIfServerCrashed()\n\n    extra_data = {}\n    self._AddTagsFilesIfNeeded( extra_data )\n    self._AddSyntaxDataIfNeeded( extra_data )\n    self._AddExtraConfDataIfNeeded( extra_data )\n\n    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',\n                                                          extra_data )\n    self._latest_file_parse_request.Start()\n\n\n  def OnBufferUnload( self, deleted_buffer_file ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'BufferUnload',\n                                { 'unloaded_buffer': deleted_buffer_file } )\n\n\n  def OnBufferVisit( self ):\n    if not self._IsServerAlive():\n      return\n    extra_data = {}\n    _AddUltiSnipsDataIfNeeded( extra_data )\n    SendEventNotificationAsync( 'BufferVisit', extra_data )\n\n\n  def OnInsertLeave( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'InsertLeave' )\n\n\n  def OnCursorMoved( self ):\n    self._diag_interface.OnCursorMoved()\n\n\n  def OnVimLeave( self ):\n    self._ServerCleanup()\n\n\n  def OnCurrentIdentifierFinished( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'CurrentIdentifierFinished' )\n\n\n  def DiagnosticsForCurrentFileReady( self ):\n    return bool( self._latest_file_parse_request and\n                 self._latest_file_parse_request.Done() )\n\n\n  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):\n    if self.DiagnosticsForCurrentFileReady():\n      diagnostics = self._latest_file_parse_request.Response()\n      # We set the diagnostics request to None because we want to prevent\n      # Syntastic from repeatedly refreshing the buffer with the same diags.\n      # Setting this to None makes DiagnosticsForCurrentFileReady return False\n      # until the next request is created.\n      self._latest_file_parse_request = None\n      if qflist_format:\n        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )\n      else:\n        return diagnostics\n    return []\n\n\n  def UpdateDiagnosticInterface( self ):\n    if not self.DiagnosticsForCurrentFileReady():\n      return\n    self._diag_interface.UpdateWithNewDiagnostics(\n      self.GetDiagnosticsFromStoredRequest() )\n\n\n  def ShowDetailedDiagnostic( self ):\n    if not self._IsServerAlive():\n      return\n    try:\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'detailed_diagnostic' )\n      if 'message' in debug_info:\n        vimsupport.EchoText( debug_info[ 'message' ] )\n    except ServerError as e:\n      vimsupport.PostVimMessage( str( e ) )\n\n\n  def DebugInfo( self ):\n    if self._IsServerAlive():\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'debug_info' )\n    else:\n      debug_info = 'Server crashed, no debug info from server'\n    debug_info += '\\nServer running at: {0}'.format(\n        BaseRequest.server_location )\n    debug_info += '\\nServer process ID: {0}'.format( self._server_popen.pid )\n    if self._server_stderr or self._server_stdout:\n      debug_info += '\\nServer logfiles:\\n  {0}\\n  {1}'.format(\n        self._server_stdout,\n        self._server_stderr )\n\n    return debug_info\n\n\n  def CurrentFiletypeCompletionEnabled( self ):\n    filetypes = vimsupport.CurrentFiletypes()\n    filetype_to_disable = self._user_options[\n      'filetype_specific_completion_to_disable' ]\n    return not all([ x in filetype_to_disable for x in filetypes ])\n\n\n  def _AddSyntaxDataIfNeeded( self, extra_data ):\n    if not self._user_options[ 'seed_identifiers_with_syntax' ]:\n      return\n    filetype = vimsupport.CurrentFiletypes()[ 0 ]\n    if filetype in self._filetypes_with_keywords_loaded:\n      return\n\n    self._filetypes_with_keywords_loaded.add( filetype )\n    extra_data[ 'syntax_keywords' ] = list(\n       syntax_parse.SyntaxKeywordsForCurrentBuffer() )\n\n\n  def _AddTagsFilesIfNeeded( self, extra_data ):\n    def GetTagFiles():\n      tag_files = vim.eval( 'tagfiles()' )\n      current_working_directory = os.getcwd()\n      return [ os.path.join( current_working_directory, x ) for x in tag_files ]\n\n    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:\n      return\n    extra_data[ 'tag_files' ] = GetTagFiles()\n\n\n  def _AddExtraConfDataIfNeeded( self, extra_data ):\n    def BuildExtraConfData( extra_conf_vim_data ):\n      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )\n                   for expr in extra_conf_vim_data )\n\n    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]\n    if extra_conf_vim_data:\n      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(\n        extra_conf_vim_data )\n\n\ndef _PathToServerScript():\n  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )\n  return os.path.join( dir_of_current_script, 'server/ycmd.py' )\n\n\ndef _AddUltiSnipsDataIfNeeded( extra_data ):\n  if not USE_ULTISNIPS_DATA:\n    return\n\n  try:\n    rawsnips = UltiSnips_Manager._snips( '', 1 )\n  except:\n    return\n\n  # UltiSnips_Manager._snips() returns a class instance where:\n  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )\n  # class.description - description of the snippet\n  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,\n                                           'description': x.description\n                                         } for x in rawsnips ]\n\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/grinchios/vimplugins/blob/abfc3ee36adab11c0c0b9d086a164a69006fec79",
        "file_path": "/python/ycm/client/base_request.py",
        "source": "#!/usr/bin/env python\n#\n# Copyright (C) 2013  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport vim\nimport requests\nimport urlparse\nfrom retries import retries\nfrom requests_futures.sessions import FuturesSession\nfrom ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\nfrom ycm import vimsupport\nfrom ycm.utils import ToUtf8Json\nfrom ycm.server.responses import ServerError, UnknownExtraConf\n\n_HEADERS = {'content-type': 'application/json'}\n_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n# Setting this to None seems to screw up the Requests/urllib3 libs.\n_DEFAULT_TIMEOUT_SEC = 30\n\nclass BaseRequest( object ):\n  def __init__( self ):\n    pass\n\n\n  def Start( self ):\n    pass\n\n\n  def Done( self ):\n    return True\n\n\n  def Response( self ):\n    return {}\n\n  # This method blocks\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',\n                                                            handler,\n                                                            'GET',\n                                                            timeout ) )\n\n\n  # This is the blocking version of the method. See below for async.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,\n                                                               handler,\n                                                               timeout ) )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |method| is either 'POST' or 'GET'.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def _TalkToHandlerAsync( data,\n                           handler,\n                           method,\n                           timeout = _DEFAULT_TIMEOUT_SEC ):\n    def SendRequest( data, handler, method, timeout ):\n      if method == 'POST':\n        return BaseRequest.session.post( _BuildUri( handler ),\n                                        data = ToUtf8Json( data ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n      if method == 'GET':\n        return BaseRequest.session.get( _BuildUri( handler ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n\n    @retries( 5, delay = 0.5, backoff = 1.5 )\n    def DelayedSendRequest( data, handler, method ):\n      if method == 'POST':\n        return requests.post( _BuildUri( handler ),\n                              data = ToUtf8Json( data ),\n                              headers = _HEADERS )\n      if method == 'GET':\n        return requests.get( _BuildUri( handler ),\n                             headers = _HEADERS )\n\n    if not _CheckServerIsHealthyWithCache():\n      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n\n    return SendRequest( data, handler, method, timeout )\n\n\n  session = FuturesSession( executor = _EXECUTOR )\n  server_location = 'http://localhost:6666'\n\n\ndef BuildRequestData( start_column = None,\n                      query = None,\n                      include_buffer_data = True ):\n  line, column = vimsupport.CurrentLineAndColumn()\n  filepath = vimsupport.GetCurrentBufferFilepath()\n  request_data = {\n    'filetypes': vimsupport.CurrentFiletypes(),\n    'line_num': line,\n    'column_num': column,\n    'start_column': start_column,\n    'line_value': vim.current.line,\n    'filepath': filepath\n  }\n\n  if include_buffer_data:\n    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()\n  if query:\n    request_data[ 'query' ] = query\n\n  return request_data\n\n\ndef JsonFromFuture( future ):\n  response = future.result()\n  if response.status_code == requests.codes.server_error:\n    _RaiseExceptionForData( response.json() )\n\n  # We let Requests handle the other status types, we only handle the 500\n  # error code.\n  response.raise_for_status()\n\n  if response.text:\n    return response.json()\n  return None\n\n\ndef _BuildUri( handler ):\n  return urlparse.urljoin( BaseRequest.server_location, handler )\n\n\nSERVER_HEALTHY = False\n\ndef _CheckServerIsHealthyWithCache():\n  global SERVER_HEALTHY\n\n  def _ServerIsHealthy():\n    response = requests.get( _BuildUri( 'healthy' ) )\n    response.raise_for_status()\n    return response.json()\n\n  if SERVER_HEALTHY:\n    return True\n\n  try:\n    SERVER_HEALTHY = _ServerIsHealthy()\n    return SERVER_HEALTHY\n  except:\n    return False\n\n\ndef _RaiseExceptionForData( data ):\n  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:\n    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )\n\n  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],\n                                        data[ 'message' ] ) )\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/grinchios/vimplugins/blob/abfc3ee36adab11c0c0b9d086a164a69006fec79",
        "file_path": "/python/ycm/youcompleteme.py",
        "source": "#!/usr/bin/env python\n#\n# Copyright (C) 2011, 2012  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\nimport vim\nimport tempfile\nimport json\nimport signal\nfrom subprocess import PIPE\nfrom ycm import vimsupport\nfrom ycm import utils\nfrom ycm.diagnostic_interface import DiagnosticInterface\nfrom ycm.completers.all.omni_completer import OmniCompleter\nfrom ycm.completers.general import syntax_parse\nfrom ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype\nfrom ycm.client.ycmd_keepalive import YcmdKeepalive\nfrom ycm.client.base_request import BaseRequest, BuildRequestData\nfrom ycm.client.command_request import SendCommandRequest\nfrom ycm.client.completion_request import CompletionRequest\nfrom ycm.client.omni_completion_request import OmniCompletionRequest\nfrom ycm.client.event_notification import ( SendEventNotificationAsync,\n                                            EventNotification )\nfrom ycm.server.responses import ServerError\n\ntry:\n  from UltiSnips import UltiSnips_Manager\n  USE_ULTISNIPS_DATA = True\nexcept ImportError:\n  USE_ULTISNIPS_DATA = False\n\n# We need this so that Requests doesn't end up using the local HTTP proxy when\n# talking to ycmd. Users should actually be setting this themselves when\n# configuring a proxy server on their machine, but most don't know they need to\n# or how to do it, so we do it for them.\n# Relevant issues:\n#  https://github.com/Valloric/YouCompleteMe/issues/641\n#  https://github.com/kennethreitz/requests/issues/879\nos.environ['no_proxy'] = '127.0.0.1,localhost'\n\n# Force the Python interpreter embedded in Vim (in which we are running) to\n# ignore the SIGINT signal. This helps reduce the fallout of a user pressing\n# Ctrl-C in Vim.\nsignal.signal( signal.SIGINT, signal.SIG_IGN )\n\nNUM_YCMD_STDERR_LINES_ON_CRASH = 30\nSERVER_CRASH_MESSAGE_STDERR_FILE = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n  'Stderr (last {0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )\nSERVER_CRASH_MESSAGE_SAME_STDERR = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '\n  ' check console output for logs!' )\nSERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours\n\n\nclass YouCompleteMe( object ):\n  def __init__( self, user_options ):\n    self._user_options = user_options\n    self._user_notified_about_crash = False\n    self._diag_interface = DiagnosticInterface( user_options )\n    self._omnicomp = OmniCompleter( user_options )\n    self._latest_completion_request = None\n    self._latest_file_parse_request = None\n    self._server_stdout = None\n    self._server_stderr = None\n    self._server_popen = None\n    self._filetypes_with_keywords_loaded = set()\n    self._temp_options_filename = None\n    self._ycmd_keepalive = YcmdKeepalive()\n    self._SetupServer()\n    self._ycmd_keepalive.Start()\n\n  def _SetupServer( self ):\n    server_port = utils.GetUnusedLocalhostPort()\n    with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n      self._temp_options_filename = options_file.name\n      json.dump( dict( self._user_options ), options_file )\n      options_file.flush()\n\n      args = [ utils.PathToPythonInterpreter(),\n               _PathToServerScript(),\n               '--port={0}'.format( server_port ),\n               '--options_file={0}'.format( options_file.name ),\n               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),\n               '--idle_suicide_seconds={0}'.format(\n                  SERVER_IDLE_SUICIDE_SECONDS )]\n\n      if not self._user_options[ 'server_use_vim_stdout' ]:\n        filename_format = os.path.join( utils.PathToTempDir(),\n                                        'server_{port}_{std}.log' )\n\n        self._server_stdout = filename_format.format( port = server_port,\n                                                      std = 'stdout' )\n        self._server_stderr = filename_format.format( port = server_port,\n                                                      std = 'stderr' )\n        args.append('--stdout={0}'.format( self._server_stdout ))\n        args.append('--stderr={0}'.format( self._server_stderr ))\n\n        if self._user_options[ 'server_keep_logfiles' ]:\n          args.append('--keep_logfiles')\n\n      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n      BaseRequest.server_location = 'http://localhost:' + str( server_port )\n\n    self._NotifyUserIfServerCrashed()\n\n  def _IsServerAlive( self ):\n    returncode = self._server_popen.poll()\n    # When the process hasn't finished yet, poll() returns None.\n    return returncode is None\n\n\n  def _NotifyUserIfServerCrashed( self ):\n    if self._user_notified_about_crash or self._IsServerAlive():\n      return\n    self._user_notified_about_crash = True\n    if self._server_stderr:\n      with open( self._server_stderr, 'r' ) as server_stderr_file:\n        error_output = ''.join( server_stderr_file.readlines()[\n            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )\n        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +\n                                        error_output )\n    else:\n        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )\n\n\n  def ServerPid( self ):\n    if not self._server_popen:\n      return -1\n    return self._server_popen.pid\n\n\n  def _ServerCleanup( self ):\n    if self._IsServerAlive():\n      self._server_popen.terminate()\n    utils.RemoveIfExists( self._temp_options_filename )\n\n\n  def RestartServer( self ):\n    vimsupport.PostVimMessage( 'Restarting ycmd server...' )\n    self._user_notified_about_crash = False\n    self._ServerCleanup()\n    self._SetupServer()\n\n\n  def CreateCompletionRequest( self, force_semantic = False ):\n    # We have to store a reference to the newly created CompletionRequest\n    # because VimScript can't store a reference to a Python object across\n    # function calls... Thus we need to keep this request somewhere.\n    if ( not self.NativeFiletypeCompletionAvailable() and\n         self.CurrentFiletypeCompletionEnabled() and\n         self._omnicomp.ShouldUseNow() ):\n      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )\n    else:\n      extra_data = {}\n      self._AddExtraConfDataIfNeeded( extra_data )\n      if force_semantic:\n        extra_data[ 'force_semantic' ] = True\n\n      self._latest_completion_request = ( CompletionRequest( extra_data )\n                                          if self._IsServerAlive() else\n                                          None )\n    return self._latest_completion_request\n\n\n  def SendCommandRequest( self, arguments, completer ):\n    if self._IsServerAlive():\n      return SendCommandRequest( arguments, completer )\n\n\n  def GetDefinedSubcommands( self ):\n    if self._IsServerAlive():\n      return BaseRequest.PostDataToHandler( BuildRequestData(),\n                                            'defined_subcommands' )\n    else:\n      return []\n\n\n  def GetCurrentCompletionRequest( self ):\n    return self._latest_completion_request\n\n\n  def GetOmniCompleter( self ):\n    return self._omnicomp\n\n\n  def NativeFiletypeCompletionAvailable( self ):\n    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in\n                  vimsupport.CurrentFiletypes() ] )\n\n\n  def NativeFiletypeCompletionUsable( self ):\n    return ( self.CurrentFiletypeCompletionEnabled() and\n             self.NativeFiletypeCompletionAvailable() )\n\n\n  def OnFileReadyToParse( self ):\n    self._omnicomp.OnFileReadyToParse( None )\n\n    if not self._IsServerAlive():\n      self._NotifyUserIfServerCrashed()\n\n    extra_data = {}\n    self._AddTagsFilesIfNeeded( extra_data )\n    self._AddSyntaxDataIfNeeded( extra_data )\n    self._AddExtraConfDataIfNeeded( extra_data )\n\n    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',\n                                                          extra_data )\n    self._latest_file_parse_request.Start()\n\n\n  def OnBufferUnload( self, deleted_buffer_file ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'BufferUnload',\n                                { 'unloaded_buffer': deleted_buffer_file } )\n\n\n  def OnBufferVisit( self ):\n    if not self._IsServerAlive():\n      return\n    extra_data = {}\n    _AddUltiSnipsDataIfNeeded( extra_data )\n    SendEventNotificationAsync( 'BufferVisit', extra_data )\n\n\n  def OnInsertLeave( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'InsertLeave' )\n\n\n  def OnCursorMoved( self ):\n    self._diag_interface.OnCursorMoved()\n\n\n  def OnVimLeave( self ):\n    self._ServerCleanup()\n\n\n  def OnCurrentIdentifierFinished( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'CurrentIdentifierFinished' )\n\n\n  def DiagnosticsForCurrentFileReady( self ):\n    return bool( self._latest_file_parse_request and\n                 self._latest_file_parse_request.Done() )\n\n\n  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):\n    if self.DiagnosticsForCurrentFileReady():\n      diagnostics = self._latest_file_parse_request.Response()\n      # We set the diagnostics request to None because we want to prevent\n      # Syntastic from repeatedly refreshing the buffer with the same diags.\n      # Setting this to None makes DiagnosticsForCurrentFileReady return False\n      # until the next request is created.\n      self._latest_file_parse_request = None\n      if qflist_format:\n        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )\n      else:\n        return diagnostics\n    return []\n\n\n  def UpdateDiagnosticInterface( self ):\n    if not self.DiagnosticsForCurrentFileReady():\n      return\n    self._diag_interface.UpdateWithNewDiagnostics(\n      self.GetDiagnosticsFromStoredRequest() )\n\n\n  def ShowDetailedDiagnostic( self ):\n    if not self._IsServerAlive():\n      return\n    try:\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'detailed_diagnostic' )\n      if 'message' in debug_info:\n        vimsupport.EchoText( debug_info[ 'message' ] )\n    except ServerError as e:\n      vimsupport.PostVimMessage( str( e ) )\n\n\n  def DebugInfo( self ):\n    if self._IsServerAlive():\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'debug_info' )\n    else:\n      debug_info = 'Server crashed, no debug info from server'\n    debug_info += '\\nServer running at: {0}'.format(\n        BaseRequest.server_location )\n    debug_info += '\\nServer process ID: {0}'.format( self._server_popen.pid )\n    if self._server_stderr or self._server_stdout:\n      debug_info += '\\nServer logfiles:\\n  {0}\\n  {1}'.format(\n        self._server_stdout,\n        self._server_stderr )\n\n    return debug_info\n\n\n  def CurrentFiletypeCompletionEnabled( self ):\n    filetypes = vimsupport.CurrentFiletypes()\n    filetype_to_disable = self._user_options[\n      'filetype_specific_completion_to_disable' ]\n    return not all([ x in filetype_to_disable for x in filetypes ])\n\n\n  def _AddSyntaxDataIfNeeded( self, extra_data ):\n    if not self._user_options[ 'seed_identifiers_with_syntax' ]:\n      return\n    filetype = vimsupport.CurrentFiletypes()[ 0 ]\n    if filetype in self._filetypes_with_keywords_loaded:\n      return\n\n    self._filetypes_with_keywords_loaded.add( filetype )\n    extra_data[ 'syntax_keywords' ] = list(\n       syntax_parse.SyntaxKeywordsForCurrentBuffer() )\n\n\n  def _AddTagsFilesIfNeeded( self, extra_data ):\n    def GetTagFiles():\n      tag_files = vim.eval( 'tagfiles()' )\n      current_working_directory = os.getcwd()\n      return [ os.path.join( current_working_directory, x ) for x in tag_files ]\n\n    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:\n      return\n    extra_data[ 'tag_files' ] = GetTagFiles()\n\n\n  def _AddExtraConfDataIfNeeded( self, extra_data ):\n    def BuildExtraConfData( extra_conf_vim_data ):\n      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )\n                   for expr in extra_conf_vim_data )\n\n    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]\n    if extra_conf_vim_data:\n      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(\n        extra_conf_vim_data )\n\n\ndef _PathToServerScript():\n  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )\n  return os.path.join( dir_of_current_script, 'server/ycmd.py' )\n\n\ndef _AddUltiSnipsDataIfNeeded( extra_data ):\n  if not USE_ULTISNIPS_DATA:\n    return\n\n  try:\n    rawsnips = UltiSnips_Manager._snips( '', 1 )\n  except:\n    return\n\n  # UltiSnips_Manager._snips() returns a class instance where:\n  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )\n  # class.description - description of the snippet\n  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,\n                                           'description': x.description\n                                         } for x in rawsnips ]\n\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/pipermerriam/flex/blob/f7452a590cfc08c06e0a7209669593b160a9a8fb",
        "file_path": "/flex/core.py",
        "source": "from __future__ import unicode_literals\n\nfrom six.moves import urllib_parse as urlparse\nimport os\nimport collections\nimport requests\n\nimport six\nimport json\nimport yaml\n\nfrom flex.context_managers import ErrorDict\nfrom flex.exceptions import ValidationError\nfrom flex.loading.definitions import (\n    definitions_validator,\n)\nfrom flex.loading.schema import (\n    swagger_schema_validator,\n)\nfrom flex.loading.schema.paths.path_item.operation.responses.single.schema import (\n    schema_validator,\n)\nfrom flex.http import (\n    normalize_request,\n    normalize_response,\n)\nfrom flex.validation.common import validate_object\nfrom flex.validation.request import validate_request\nfrom flex.validation.response import validate_response\n\n\ndef load_source(source):\n    \"\"\"\n    Common entry point for loading some form of raw swagger schema.\n\n    Supports:\n        - python object (dictionary-like)\n        - path to yaml file\n        - path to json file\n        - file object (json or yaml).\n        - json string.\n        - yaml string.\n    \"\"\"\n    if isinstance(source, collections.Mapping):\n        return source\n    elif hasattr(source, 'read') and callable(source.read):\n        raw_source = source.read()\n    elif os.path.exists(os.path.expanduser(str(source))):\n        with open(os.path.expanduser(str(source)), 'r') as source_file:\n            raw_source = source_file.read()\n    elif isinstance(source, six.string_types):\n        parts = urlparse.urlparse(source)\n        if parts.scheme and parts.netloc:\n            response = requests.get(source)\n            if isinstance(response.content, six.binary_type):\n                raw_source = six.text_type(response.content, encoding='utf-8')\n            else:\n                raw_source = response.content\n        else:\n            raw_source = source\n\n    try:\n        try:\n            return json.loads(raw_source)\n        except ValueError:\n            pass\n\n        try:\n            return yaml.load(raw_source)\n        except (yaml.scanner.ScannerError, yaml.parser.ParserError):\n            pass\n    except NameError:\n        pass\n\n    raise ValueError(\n        \"Unable to parse `{0}`.  Tried yaml and json.\".format(source),\n    )\n\n\ndef parse(raw_schema):\n    context = {\n        'deferred_references': set(),\n    }\n    swagger_definitions = definitions_validator(raw_schema, context=context)\n\n    swagger_schema = swagger_schema_validator(\n        raw_schema,\n        context=swagger_definitions,\n    )\n    return swagger_schema\n\n\ndef load(target):\n    \"\"\"\n    Given one of the supported target formats, load a swagger schema into it's\n    python representation.\n    \"\"\"\n    raw_schema = load_source(target)\n    return parse(raw_schema)\n\n\ndef validate(raw_schema, target=None, **kwargs):\n    \"\"\"\n    Given the python representation of a JSONschema as defined in the swagger\n    spec, validate that the schema complies to spec.  If `target` is provided,\n    that target will be validated against the provided schema.\n    \"\"\"\n    schema = schema_validator(raw_schema, **kwargs)\n    if target is not None:\n        validate_object(target, schema=schema, **kwargs)\n\n\ndef validate_api_request(schema, raw_request):\n    request = normalize_request(raw_request)\n\n    with ErrorDict():\n        validate_request(request=request, schema=schema)\n\n\ndef validate_api_response(schema, raw_response, request_method='get', raw_request=None):\n    \"\"\"\n    Validate the response of an api call against a swagger schema.\n    \"\"\"\n    request = None\n    if raw_request is not None:\n        request = normalize_request(raw_request)\n\n    response = None\n    if raw_response is not None:\n        response = normalize_response(raw_response, request=request)\n\n    if response is not None:\n        validate_response(\n            response=response,\n            request_method=request_method,\n            schema=schema\n        )\n\n\ndef validate_api_call(schema, raw_request, raw_response):\n    \"\"\"\n    Validate the request/response cycle of an api call against a swagger\n    schema.  Request/Response objects from the `requests` and `urllib` library\n    are supported.\n    \"\"\"\n    request = normalize_request(raw_request)\n\n    with ErrorDict() as errors:\n        try:\n            validate_request(\n                request=request,\n                schema=schema,\n            )\n        except ValidationError as err:\n            errors['request'].add_error(err.messages or getattr(err, 'detail'))\n            return\n\n        response = normalize_response(raw_response, raw_request)\n\n        try:\n            validate_response(\n                response=response,\n                request_method=request.method,\n                schema=schema\n            )\n        except ValidationError as err:\n            errors['response'].add_error(err.messages or getattr(err, 'detail'))\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/pipermerriam/flex/blob/f7452a590cfc08c06e0a7209669593b160a9a8fb",
        "file_path": "/tests/core/test_load_source.py",
        "source": "from __future__ import unicode_literals\n\nimport tempfile\nimport collections\n\nimport six\n\nimport json\nimport yaml\n\nfrom flex.core import load_source\n\n\ndef test_native_mapping_is_passthrough():\n    source = {'foo': 'bar'}\n    result = load_source(source)\n\n    assert result == source\n\n\ndef test_json_string():\n    native = {'foo': 'bar'}\n    source = json.dumps(native)\n    result = load_source(source)\n\n    assert result == native\n\n\ndef test_yaml_string():\n    native = {'foo': 'bar'}\n    source = yaml.dump(native)\n    result = load_source(source)\n\n    assert result == native\n\n\ndef test_json_file_object():\n    native = {'foo': 'bar'}\n    source = json.dumps(native)\n\n    tmp_file = tempfile.NamedTemporaryFile(mode='w')\n    tmp_file.write(source)\n    tmp_file.file.seek(0)\n\n    with open(tmp_file.name) as json_file:\n        result = load_source(json_file)\n\n    assert result == native\n\n\ndef test_json_file_path():\n    native = {'foo': 'bar'}\n    source = json.dumps(native)\n\n    tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json')\n    tmp_file.write(source)\n    tmp_file.flush()\n\n    result = load_source(tmp_file.name)\n\n    assert result == native\n\n\ndef test_yaml_file_object():\n    native = {'foo': 'bar'}\n    source = yaml.dump(native)\n\n    tmp_file = tempfile.NamedTemporaryFile(mode='w')\n    tmp_file.write(source)\n    tmp_file.flush()\n\n    with open(tmp_file.name) as yaml_file:\n        result = load_source(yaml_file)\n\n    assert result == native\n\n\ndef test_yaml_file_path():\n    native = {'foo': 'bar'}\n    source = yaml.dump(native)\n\n    tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml')\n    tmp_file.write(source)\n    tmp_file.flush()\n\n    result = load_source(tmp_file.name)\n\n    assert result == native\n\n\ndef test_url(httpbin):\n    native = {\n        'origin': '127.0.0.1',\n        #'headers': {\n        #    'Content-Length': '',\n        #    'Accept-Encoding': 'gzip, deflate',\n        #    'Host': '127.0.0.1:54634',\n        #    'Accept': '*/*',\n        #    'User-Agent': 'python-requests/2.4.3 CPython/2.7.8 Darwin/14.0.0',\n        #    'Connection': 'keep-alive',\n        #},\n        'args': {},\n        #'url': 'http://127.0.0.1:54634/get',\n    }\n    source = httpbin.url + '/get'\n    result = load_source(source)\n    assert isinstance(result, collections.Mapping)\n    result.pop('headers')\n    result.pop('url')\n    assert result == native\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/microsoft/ptvsd/blob/2e78687bea247799b7ffccd00c41523bc9f38eda",
        "file_path": "/ptvsd/__main__.py",
        "source": "# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See LICENSE in the project root\n# for license information.\n\nimport argparse\nimport os.path\nimport sys\n\nfrom ptvsd._local import debug_main, run_main\nfrom ptvsd.socket import Address\nfrom ptvsd.version import __version__, __author__  # noqa\n\n\n##################################\n# the script\n\n\"\"\"\nFor the PyDevd CLI handling see:\n\n  https://github.com/fabioz/PyDev.Debugger/blob/master/_pydevd_bundle/pydevd_command_line_handling.py\n  https://github.com/fabioz/PyDev.Debugger/blob/master/pydevd.py#L1450  (main func)\n\"\"\"  # noqa\n\nPYDEVD_OPTS = {\n    '--file',\n    '--client',\n    #'--port',\n    '--vm_type',\n}\n\nPYDEVD_FLAGS = {\n    '--DEBUG',\n    '--DEBUG_RECORD_SOCKET_READS',\n    '--cmd-line',\n    '--module',\n    '--multiproc',\n    '--multiprocess',\n    '--print-in-debugger-startup',\n    '--save-signatures',\n    '--save-threading',\n    '--save-asyncio',\n    '--server',\n    '--qt-support=auto',\n}\n\nUSAGE = \"\"\"\n  {0} [-h] [-V] [--nodebug] [--host HOST | --server-host HOST] --port PORT -m MODULE [arg ...]\n  {0} [-h] [-V] [--nodebug] [--host HOST | --server-host HOST] --port PORT FILENAME [arg ...]\n\"\"\"  # noqa\n\n\ndef parse_args(argv=None):\n    \"\"\"Return the parsed args to use in main().\"\"\"\n    if argv is None:\n        argv = sys.argv\n        prog = argv[0]\n        if prog == __file__:\n            prog = '{} -m ptvsd'.format(os.path.basename(sys.executable))\n    else:\n        prog = argv[0]\n    argv = argv[1:]\n\n    supported, pydevd, script = _group_args(argv)\n    args = _parse_args(prog, supported)\n    # '--' is used in _run_args to extract pydevd specific args\n    extra = pydevd + ['--']\n    if script:\n        extra += script\n    return args, extra\n\n\ndef _group_args(argv):\n    supported = []\n    pydevd = []\n    script = []\n\n    try:\n        pos = argv.index('--')\n    except ValueError:\n        script = []\n    else:\n        script = argv[pos + 1:]\n        argv = argv[:pos]\n\n    for arg in argv:\n        if arg == '-h' or arg == '--help':\n            return argv, [], script\n\n    gottarget = False\n    skip = 0\n    for i in range(len(argv)):\n        if skip:\n            skip -= 1\n            continue\n\n        arg = argv[i]\n        try:\n            nextarg = argv[i + 1]\n        except IndexError:\n            nextarg = None\n\n        # TODO: Deprecate the PyDevd arg support.\n        # PyDevd support\n        if gottarget:\n            script = argv[i:] + script\n            break\n        if arg == '--client':\n            arg = '--host'\n        elif arg == '--file':\n            if nextarg is None:  # The filename is missing...\n                pydevd.append(arg)\n                continue  # This will get handled later.\n            if nextarg.endswith(':') and '--module' in pydevd:\n                pydevd.remove('--module')\n                arg = '-m'\n                argv[i + 1] = nextarg = nextarg[:-1]\n            else:\n                arg = nextarg\n                skip += 1\n\n        if arg in PYDEVD_OPTS:\n            pydevd.append(arg)\n            if nextarg is not None:\n                pydevd.append(nextarg)\n            skip += 1\n        elif arg in PYDEVD_FLAGS:\n            pydevd.append(arg)\n        elif arg == '--nodebug':\n            supported.append(arg)\n\n        # ptvsd support\n        elif arg in ('--host', '--server-host', '--port', '-m'):\n            if arg == '-m':\n                gottarget = True\n            supported.append(arg)\n            if nextarg is not None:\n                supported.append(nextarg)\n            skip += 1\n        elif arg in ('--single-session',):\n            supported.append(arg)\n        elif not arg.startswith('-'):\n            supported.append(arg)\n            gottarget = True\n\n        # unsupported arg\n        else:\n            supported.append(arg)\n            break\n\n    return supported, pydevd, script\n\n\ndef _parse_args(prog, argv):\n    parser = argparse.ArgumentParser(\n        prog=prog,\n        usage=USAGE.format(prog),\n    )\n    parser.add_argument('--nodebug', action='store_true')\n    host = parser.add_mutually_exclusive_group()\n    host.add_argument('--host')\n    host.add_argument('--server-host')\n    parser.add_argument('--port', type=int, required=True)\n\n    target = parser.add_mutually_exclusive_group(required=True)\n    target.add_argument('-m', dest='module')\n    target.add_argument('filename', nargs='?')\n\n    parser.add_argument('--single-session', action='store_true')\n    parser.add_argument('-V', '--version', action='version')\n    parser.version = __version__\n\n    args = parser.parse_args(argv)\n    ns = vars(args)\n\n    serverhost = ns.pop('server_host', None)\n    clienthost = ns.pop('host', None)\n    if serverhost:\n        args.address = Address.as_server(serverhost, ns.pop('port'))\n    elif not clienthost:\n        if args.nodebug:\n            args.address = Address.as_client(clienthost, ns.pop('port'))\n        else:\n            args.address = Address.as_server(clienthost, ns.pop('port'))\n    else:\n        args.address = Address.as_client(clienthost, ns.pop('port'))\n\n    module = ns.pop('module')\n    filename = ns.pop('filename')\n    if module is None:\n        args.name = filename\n        args.kind = 'script'\n    else:\n        args.name = module\n        args.kind = 'module'\n    #if argv[-1] != args.name or (module and argv[-1] != '-m'):\n    #    parser.error('script/module must be last arg')\n\n    return args\n\n\ndef main(addr, name, kind, extra=(), nodebug=False, **kwargs):\n    if nodebug:\n        run_main(addr, name, kind, *extra, **kwargs)\n    else:\n        debug_main(addr, name, kind, *extra, **kwargs)\n\n\nif __name__ == '__main__':\n    args, extra = parse_args()\n    main(args.address, args.name, args.kind, extra, nodebug=args.nodebug,\n         singlesession=args.single_session)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/microsoft/ptvsd/blob/2e78687bea247799b7ffccd00c41523bc9f38eda",
        "file_path": "/tests/helpers/debugclient.py",
        "source": "from __future__ import absolute_import\n\nimport os\nimport traceback\nimport warnings\n\nfrom ptvsd.socket import Address\nfrom ptvsd._util import new_hidden_thread, Closeable, ClosedError\nfrom .debugadapter import DebugAdapter, wait_for_socket_server\nfrom .debugsession import DebugSession\n\n# TODO: Add a helper function to start a remote debugger for testing\n# remote debugging?\n\n\nclass _LifecycleClient(Closeable):\n\n    SESSION = DebugSession\n\n    def __init__(\n            self,\n            addr=None,\n            port=8888,\n            breakpoints=None,\n            connecttimeout=1.0,\n    ):\n        super(_LifecycleClient, self).__init__()\n        self._addr = Address.from_raw(addr, defaultport=port)\n        self._connecttimeout = connecttimeout\n        self._adapter = None\n        self._session = None\n\n        self._breakpoints = breakpoints\n\n    @property\n    def adapter(self):\n        return self._adapter\n\n    @property\n    def session(self):\n        return self._session\n\n    def start_debugging(self, launchcfg):\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if self._adapter is not None:\n            raise RuntimeError('debugger already running')\n        assert self._session is None\n\n        raise NotImplementedError\n\n    def stop_debugging(self):\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if self._adapter is None:\n            raise RuntimeError('debugger not running')\n\n        if self._session is not None:\n            self._detach()\n\n        try:\n            self._adapter.close()\n        except ClosedError:\n            pass\n        self._adapter = None\n\n    def attach_pid(self, pid, **kwargs):\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if self._adapter is None:\n            raise RuntimeError('debugger not running')\n        if self._session is not None:\n            raise RuntimeError('already attached')\n\n        raise NotImplementedError\n\n    def attach_socket(self, addr=None, adapter=None, **kwargs):\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if adapter is None:\n            adapter = self._adapter\n        elif self._adapter is not None:\n            raise RuntimeError('already using managed adapter')\n        if adapter is None:\n            raise RuntimeError('debugger not running')\n        if self._session is not None:\n            raise RuntimeError('already attached')\n\n        if addr is None:\n            addr = adapter.address\n        self._attach(addr, **kwargs)\n        return self._session\n\n    def detach(self, adapter=None):\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if self._session is None:\n            raise RuntimeError('not attached')\n        if adapter is None:\n            adapter = self._adapter\n        assert adapter is not None\n        if not self._session.is_client:\n            raise RuntimeError('detach not supported')\n\n        self._detach()\n\n    # internal methods\n\n    def _close(self):\n        if self._session is not None:\n            try:\n                self._session.close()\n            except ClosedError:\n                pass\n        if self._adapter is not None:\n            try:\n                self._adapter.close()\n            except ClosedError:\n                pass\n\n    def _launch(self,\n                argv,\n                script=None,\n                wait_for_connect=None,\n                detachable=True,\n                env=None,\n                cwd=None,\n                **kwargs):\n        if script is not None:\n            def start(*args, **kwargs):\n                return DebugAdapter.start_wrapper_script(\n                    script, *args, **kwargs)\n        else:\n            start = DebugAdapter.start\n        new_addr = Address.as_server if detachable else Address.as_client\n        addr = new_addr(None, self._addr.port)\n        self._adapter = start(argv, addr=addr, env=env, cwd=cwd)\n\n        if wait_for_connect:\n            wait_for_connect()\n        else:\n            wait_for_socket_server(addr)\n            self._attach(addr, **kwargs)\n\n    def _attach(self, addr, **kwargs):\n        if addr is None:\n            addr = self._addr\n        assert addr.host == 'localhost'\n        self._session = self.SESSION.create_client(addr, **kwargs)\n\n    def _detach(self):\n        session = self._session\n        if session is None:\n            return\n        self._session = None\n        try:\n            session.close()\n        except ClosedError:\n            pass\n\n\nclass DebugClient(_LifecycleClient):\n    \"\"\"A high-level abstraction of a debug client (i.e. editor).\"\"\"\n\n    # TODO: Manage breakpoints, etc.\n    # TODO: Add debugger methods here (e.g. \"pause\").\n\n\nclass EasyDebugClient(DebugClient):\n    def start_detached(self, argv):\n        \"\"\"Start an adapter in a background process.\"\"\"\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if self._adapter is not None:\n            raise RuntimeError('debugger already running')\n        assert self._session is None\n\n        # TODO: Launch, handshake and detach?\n        self._adapter = DebugAdapter.start(argv, port=self._port)\n        return self._adapter\n\n    def host_local_debugger(self,\n                            argv,\n                            script=None,\n                            env=None,\n                            cwd=None,\n                            **kwargs):  # noqa\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if self._adapter is not None:\n            raise RuntimeError('debugger already running')\n        assert self._session is None\n        addr = ('localhost', self._addr.port)\n\n        self._run_server_ex = None\n\n        def run():\n            try:\n                self._session = self.SESSION.create_server(addr, **kwargs)\n            except Exception as ex:\n                self._run_server_ex = traceback.format_exc()\n\n        t = new_hidden_thread(\n            target=run,\n            name='test.client',\n        )\n        t.start()\n\n        def wait():\n            t.join(timeout=self._connecttimeout)\n            if t.is_alive():\n                warnings.warn('timed out waiting for connection')\n            if self._session is None:\n                message = 'unable to connect after {} secs'.format(  # noqa\n                    self._connecttimeout)\n                if self._run_server_ex is None:\n                    raise Exception(message)\n                else:\n                    message = message + os.linesep + self._run_server_ex # noqa\n                    raise Exception(message)\n\n            # The adapter will close when the connection does.\n\n        self._launch(\n            argv,\n            script=script,\n            wait_for_connect=wait,\n            detachable=False,\n            env=env,\n            cwd=cwd)\n\n        return self._adapter, self._session\n\n    def launch_script(self, filename, *argv, **kwargs):\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if self._adapter is not None:\n            raise RuntimeError('debugger already running')\n        assert self._session is None\n\n        argv = [\n            filename,\n        ] + list(argv)\n        if kwargs.pop('nodebug', False):\n            argv.insert(0, '--nodebug')\n        self._launch(argv, **kwargs)\n        return self._adapter, self._session\n\n    def launch_module(self, module, *argv, **kwargs):\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if self._adapter is not None:\n            raise RuntimeError('debugger already running')\n        assert self._session is None\n\n        argv = [\n            '-m',\n            module,\n        ] + list(argv)\n        if kwargs.pop('nodebug', False):\n            argv.insert(0, '--nodebug')\n        self._launch(argv, **kwargs)\n        return self._adapter, self._session\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/microsoft/ptvsd/blob/2e78687bea247799b7ffccd00c41523bc9f38eda",
        "file_path": "/tests/helpers/debugsession.py",
        "source": "from __future__ import absolute_import, print_function\n\nimport contextlib\nimport json\nimport socket\nimport sys\nimport time\nimport threading\nimport warnings\n\nfrom ptvsd._util import new_hidden_thread, Closeable, ClosedError\nfrom .message import (\n    raw_read_all as read_messages,\n    raw_write_one as write_message\n)\nfrom .socket import (\n    Connection, create_server, create_client, close,\n    recv_as_read, send_as_write,\n    timeout as socket_timeout)\nfrom .threading import get_locked_and_waiter\nfrom .vsc import parse_message\n\n\nclass DebugSessionConnection(Closeable):\n\n    VERBOSE = False\n    #VERBOSE = True\n\n    TIMEOUT = 5.0\n\n    @classmethod\n    def create_client(cls, addr, **kwargs):\n        def connect(addr, timeout):\n            sock = create_client()\n            for _ in range(int(timeout * 10)):\n                try:\n                    sock.connect(addr)\n                except (OSError, socket.error):\n                    if cls.VERBOSE:\n                        print('+', end='')\n                        sys.stdout.flush()\n                    time.sleep(0.1)\n                else:\n                    break\n            else:\n                raise RuntimeError('could not connect')\n            return sock\n        return cls._create(connect, addr, **kwargs)\n\n    @classmethod\n    def create_server(cls, addr, **kwargs):\n        def connect(addr, timeout):\n            server = create_server(addr)\n            with socket_timeout(server, timeout):\n                client, _ = server.accept()\n            return Connection(client, server)\n        return cls._create(connect, addr, **kwargs)\n\n    @classmethod\n    def _create(cls, connect, addr, timeout=None):\n        if timeout is None:\n            timeout = cls.TIMEOUT\n        sock = connect(addr, timeout)\n        if cls.VERBOSE:\n            print('connected')\n        self = cls(sock, ownsock=True)\n        self._addr = addr\n        return self\n\n    def __init__(self, sock, ownsock=False):\n        super(DebugSessionConnection, self).__init__()\n        self._sock = sock\n        self._ownsock = ownsock\n\n    @property\n    def is_client(self):\n        try:\n            return self._sock.server is None\n        except AttributeError:\n            return True\n\n    def iter_messages(self):\n        if self.closed:\n            raise RuntimeError('connection closed')\n\n        def stop():\n            return self.closed\n        read = recv_as_read(self._sock)\n        for msg, _, _ in read_messages(read, stop=stop):\n            if self.VERBOSE:\n                print(repr(msg))\n            yield parse_message(msg)\n\n    def send(self, req):\n        if self.closed:\n            raise RuntimeError('connection closed')\n\n        def stop():\n            return self.closed\n        write = send_as_write(self._sock)\n        body = json.dumps(req)\n        write_message(write, body, stop=stop)\n\n    # internal methods\n\n    def _close(self):\n        if self._ownsock:\n            close(self._sock)\n\n\nclass DebugSession(Closeable):\n\n    VERBOSE = False\n    #VERBOSE = True\n\n    HOST = 'localhost'\n    PORT = 8888\n\n    TIMEOUT = None\n\n    @classmethod\n    def create_client(cls, addr=None, **kwargs):\n        if addr is None:\n            addr = (cls.HOST, cls.PORT)\n        conn = DebugSessionConnection.create_client(\n            addr,\n            timeout=kwargs.get('timeout'),\n        )\n        return cls(conn, owned=True, **kwargs)\n\n    @classmethod\n    def create_server(cls, addr=None, **kwargs):\n        if addr is None:\n            addr = (cls.HOST, cls.PORT)\n        conn = DebugSessionConnection.create_server(addr, **kwargs)\n        return cls(conn, owned=True, **kwargs)\n\n    def __init__(self, conn, seq=1000, handlers=(), timeout=None, owned=False):\n        super(DebugSession, self).__init__()\n        self._conn = conn\n        self._seq = seq\n        self._timeout = timeout\n        self._owned = owned\n\n        self._handlers = []\n        for handler in handlers:\n            if callable(handler):\n                self._add_handler(handler)\n            else:\n                self._add_handler(*handler)\n        self._received = []\n        self._listenerthread = new_hidden_thread(\n            target=self._listen,\n            name='test.session',\n        )\n        self._listenerthread.start()\n\n    @property\n    def is_client(self):\n        return self._conn.is_client\n\n    @property\n    def received(self):\n        return list(self._received)\n\n    def _create_request(self, command, **args):\n        seq = self._seq\n        self._seq += 1\n        return {\n            'type': 'request',\n            'seq': seq,\n            'command': command,\n            'arguments': args,\n        }\n\n    def send_request(self, command, **args):\n        if self.closed:\n            raise RuntimeError('session closed')\n\n        wait = args.pop('wait', False)\n        req = self._create_request(command, **args)\n        if self.VERBOSE:\n            msg = parse_message(req)\n            print(' <-', msg)\n\n        if wait:\n            with self.wait_for_response(req) as resp:\n                self._conn.send(req)\n            resp_awaiter = AwaitableResponse(req, lambda: resp[\"msg\"])\n        else:\n            resp_awaiter = self._get_awaiter_for_request(req, **args)\n            self._conn.send(req)\n        return resp_awaiter\n\n    def add_handler(self, handler, **kwargs):\n        if self.closed:\n            raise RuntimeError('session closed')\n\n        self._add_handler(handler, **kwargs)\n\n    @contextlib.contextmanager\n    def wait_for_event(self, event, **kwargs):\n        if self.closed:\n            raise RuntimeError('session closed')\n        result = {'msg': None}\n\n        def match(msg):\n            result['msg'] = msg\n            return msg.type == 'event' and msg.event == event\n        handlername = 'event {!r}'.format(event)\n        with self._wait_for_message(match, handlername, **kwargs):\n            yield result\n\n    def get_awaiter_for_event(self, event, condition=lambda msg: True, **kwargs): # noqa\n        if self.closed:\n            raise RuntimeError('session closed')\n        result = {'msg': None}\n\n        def match(msg):\n            result['msg'] = msg\n            return msg.type == 'event' and msg.event == event and condition(msg) # noqa\n        handlername = 'event {!r}'.format(event)\n        evt = self._get_message_handle(match, handlername)\n\n        return AwaitableEvent(event, lambda: result[\"msg\"], evt)\n\n    def _get_awaiter_for_request(self, req, **kwargs):\n        if self.closed:\n            raise RuntimeError('session closed')\n\n        try:\n            command, seq = req.command, req.seq\n        except AttributeError:\n            command, seq = req['command'], req['seq']\n        result = {'msg': None}\n\n        def match(msg):\n            if msg.type != 'response':\n                return False\n            result['msg'] = msg\n            return msg.request_seq == seq\n        handlername = 'response (cmd:{} seq:{})'.format(command, seq)\n        evt = self._get_message_handle(match, handlername)\n\n        return AwaitableResponse(req, lambda: result[\"msg\"], evt)\n\n    @contextlib.contextmanager\n    def wait_for_response(self, req, **kwargs):\n        if self.closed:\n            raise RuntimeError('session closed')\n\n        try:\n            command, seq = req.command, req.seq\n        except AttributeError:\n            command, seq = req['command'], req['seq']\n        result = {'msg': None}\n\n        def match(msg):\n            if msg.type != 'response':\n                return False\n            result['msg'] = msg\n            return msg.request_seq == seq\n        handlername = 'response (cmd:{} seq:{})'.format(command, seq)\n        with self._wait_for_message(match, handlername, **kwargs):\n            yield result\n\n    # internal methods\n\n    def _close(self):\n        if self._owned:\n            try:\n                self._conn.close()\n            except ClosedError:\n                pass\n        if self._listenerthread != threading.current_thread():\n            self._listenerthread.join(timeout=1.0)\n            if self._listenerthread.is_alive():\n                warnings.warn('session listener still running')\n        self._check_handlers()\n\n    def _listen(self):\n        try:\n            for msg in self._conn.iter_messages():\n                if self.VERBOSE:\n                    print(' ->', msg)\n                self._receive_message(msg)\n        except EOFError:\n            try:\n                self.close()\n            except ClosedError:\n                pass\n\n    def _receive_message(self, msg):\n        for i, handler in enumerate(list(self._handlers)):\n            handle_message, _, _ = handler\n            handled = handle_message(msg)\n            try:\n                msg, handled = handled\n            except TypeError:\n                pass\n            if handled:\n                self._handlers.remove(handler)\n                break\n        self._received.append(msg)\n\n    def _add_handler(self, handle_msg, handlername=None, required=True):\n        self._handlers.append(\n            (handle_msg, handlername, required))\n\n    def _check_handlers(self):\n        unhandled = []\n        for handle_msg, name, required in self._handlers:\n            if not required:\n                continue\n            unhandled.append(name or repr(handle_msg))\n        if unhandled:\n            raise RuntimeError('unhandled: {}'.format(unhandled))\n\n    @contextlib.contextmanager\n    def _wait_for_message(self, match, handlername, timeout=None):\n        if timeout is None:\n            timeout = self.TIMEOUT\n        lock, wait = get_locked_and_waiter()\n\n        def handler(msg):\n            if not match(msg):\n                return msg, False\n            lock.release()\n            return msg, True\n        self._add_handler(handler, handlername)\n        try:\n            yield\n        finally:\n            wait(timeout or self._timeout, handlername, fail=True)\n\n    def _get_message_handle(self, match, handlername):\n        event = threading.Event()\n\n        def handler(msg):\n            if not match(msg):\n                return msg, False\n            event.set()\n            return msg, True\n        self._add_handler(handler, handlername, False)\n        return event\n\n\nclass Awaitable(object):\n\n    @classmethod\n    def wait_all(cls, *awaitables):\n        timeout = 3.0\n        messages = []\n        for _ in range(int(timeout * 10)):\n            time.sleep(0.1)\n            messages = []\n            not_ready = (a for a in awaitables if a._event is not None and not a._event.is_set()) # noqa\n            for awaitable in not_ready:\n                if isinstance(awaitable, AwaitableEvent):\n                    messages.append('Event {}'.format(awaitable.name))\n                else:\n                    messages.append('Response {}'.format(awaitable.name))\n            if len(messages) == 0:\n                return\n        else:\n            raise TimeoutError('Timeout waiting for {}'.format(','.join(messages))) # noqa\n\n    def __init__(self, name, event=None):\n        self._event = event\n        self.name = name\n\n    def wait(self, timeout=1.0):\n        if self._event is None:\n            return\n        if not self._event.wait(timeout):\n            message = 'Timeout waiting for '\n            if isinstance(self, AwaitableEvent):\n                message += 'Event {}'.format(self.name)\n            else:\n                message += 'Response {}'.format(self.name)\n            raise TimeoutError(message)\n\n\nclass AwaitableResponse(Awaitable):\n\n    def __init__(self, req, result_getter, event=None):\n        super(AwaitableResponse, self).__init__(req[\"command\"], event)\n        self.req = req\n        self._result_getter = result_getter\n\n    @property\n    def resp(self):\n        return self._result_getter()\n\n\nclass AwaitableEvent(Awaitable):\n\n    def __init__(self, name, result_getter, event=None):\n        super(AwaitableEvent, self).__init__(name, event)\n        self._result_getter = result_getter\n\n    @property\n    def event(self):\n        return self._result_getter()\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/johanbluecreek/reddytt/blob/3b202f838e647c903ab1e1ee61f84fa9372d9cbf",
        "file_path": "/reddytt.py",
        "source": "#!/usr/bin/env python3\n\n################\n# Imports\n################\n\nimport os\nimport pickle\nfrom bs4 import BeautifulSoup\nimport urllib3\nimport certifi\nimport re\nimport sys\nimport argparse as ap\n#from argparse import ArgumentParser, REMINDER\n\n################\n# Functions\n################\n\n# Function to flatten a list\nflatten = lambda l: [item for sublist in l for item in sublist]\n# cheers to https://stackoverflow.com/a/952952\n\n# Get and parse out links\ndef getytlinks(link):\n    pm = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())\n    html_page = pm.request('GET', link)\n    soup = BeautifulSoup(html_page.data, \"lxml\")\n    links = [a.get('href') for a in soup('a') if a.get('href')]\n\n    # Pick out youtube links\n    new_links = [x for x in links if re.match(\"^https://youtu\\.be\", x)]\n    newer_links = [x for x in links if re.match(\"^https://www\\.youtube\\.com/watch\", x)]\n    # the youtube.com links are not always well formatted for mpv, so we reformat them:\n    for lk in newer_links:\n        videolabel = re.search('v=([^&?]*)', lk)[1]\n        if videolabel is None:\n            print('Reddytt: skipping URL without video label:', lk)\n            continue\n        new_links.append('https://www.youtube.com/watch?v=' + videolabel)\n    # in principal, add anything here you want. I guess all of these should work: https://rg3.github.io/youtube-dl/supportedsites.html\n    return new_links, links\n\n################\n# Main\n################\n\nif __name__ == '__main__':\n\n    parser = ap.ArgumentParser(usage='%(prog)s [options] <subreddit> [-- [mpv-arguments]]', description='Play the youtube links from your favourite subreddit.')\n\n    parser.add_argument('--depth', metavar='d', type=int, default=0, help='How many pages into the subreddit you want to go.')\n    parser.add_argument('subreddit', type=str, help='The subreddit you want to play.')\n    parser.add_argument('mpv', nargs=ap.REMAINDER, help='Arguments to pass to `mpv`.')\n\n    args = parser.parse_args()\n\n    subreddit = args.subreddit\n    depth = args.depth\n    mpv = \" \".join(args.mpv)\n\n    subreddit_link = \"https://reddit.com/r/\" + subreddit\n\n    # Setup working directory\n    work_dir = os.environ['HOME'] + \"/.reddytt\"\n    sr_dir = work_dir + \"/%s\" % subreddit\n    seen_file = sr_dir + \"/seen\"\n    seen_links = []\n    unseen_file = sr_dir + \"/unseen\"\n    unseen_links = []\n    print(\"Reddytt: Checking for reddytt working directory (%s).\" % work_dir)\n\n    if not os.path.isdir(work_dir):\n        print(\"Reddytt: Working directory not found. Creating %s, and files.\" % work_dir)\n        os.mkdir(work_dir)\n        os.mkdir(sr_dir)\n        os.system(\"touch %s\" % seen_file)\n        with open(seen_file, 'wb') as f:\n            pickle.dump(seen_links, f)\n        os.system(\"touch %s\" % unseen_file)\n        with open(unseen_file, 'wb') as f:\n            pickle.dump(unseen_links, f)\n    elif not os.path.isdir(sr_dir):\n        print(\"Reddytt: Working directory found, but no subreddit directory. Creating %s, and files.\" % sr_dir)\n        os.mkdir(sr_dir)\n        os.system(\"touch %s\" % seen_file)\n        with open(seen_file, 'wb') as f:\n            pickle.dump(seen_links, f)\n        os.system(\"touch %s\" % unseen_file)\n        with open(unseen_file, 'wb') as f:\n            pickle.dump(unseen_links, f)\n    else:\n        print(\"Reddytt: Working directory found. Loading variables.\")\n        with open(seen_file, 'rb') as f:\n            seen_links = pickle.load(f)\n        with open(unseen_file, 'rb') as f:\n            unseen_links = pickle.load(f)\n\n    new_links, links = getytlinks(subreddit_link)\n\n    # Go deeper\n    if depth > 0:\n        for d in range(depth):\n            link = \"\"\n            for l in links:\n                if re.search(\"after=\", l):\n                    link = l\n            if link == \"\":\n                print(\"Reddytt: Could not identify 'after'-variable to progress deeper.\")\n            else:\n                newer_links, links = getytlinks(link)\n                new_links += newer_links\n                new_links = list(set(new_links))\n\n    # we also want to watch the stored ones\n    new_links += unseen_links\n    new_links = list(set(new_links))\n\n    # Start watching\n    save_links = new_links\n    for link in new_links:\n        if link in seen_links:\n            print(\"Reddytt: Link seen. Skipping.\")\n        else:\n            x = os.system(\"mpv %(args)s %(link)s\" % {\"link\": link, \"args\": mpv})\n            print(\"Reddytt: That was: %s\" % link)\n            if x == 0:\n                # The video finished or you hit 'q' (or whatever your binding is), this is a good exit.\n                # Store the video in seen_links.\n                seen_links.append(link)\n                save_links.remove(link)\n            elif x == 1024:\n                # You made a hard exit, and want to stop. (Ctrl+C)\n                # Store the links and exit the program.\n                print(\"Reddytt: Forced exit detected. Saving and exiting.\")\n                with open(seen_file, 'wb') as f:\n                    pickle.dump(seen_links, f)\n                with open(unseen_file, 'wb') as f:\n                    pickle.dump(save_links, f)\n                # Exit program.\n                sys.exit()\n            else:\n                # Something else happened. Bad link perhaps.\n                # Store in seen_links to avoid in the future.\n\n                seen_links.append(link)\n                save_links.remove(link)\n\n    # The playlist is finished. Save everything.\n    with open(seen_file, 'wb') as f:\n        pickle.dump(seen_links, f)\n    with open(unseen_file, 'wb') as f:\n        pickle.dump(save_links, f)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/EKami/deep_learning_A-Z/blob/2576712a441746db80bbad97b899f27618f9cf2f",
        "file_path": "/Get_the_machine_learning_basics/Classification_Template/classification_template.py",
        "source": "# Classification template\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Social_Network_Ads.csv')\nX = dataset.iloc[:, [2, 3]].values\ny = dataset.iloc[:, 4].values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Fitting classifier to the Training set\n# Create your classifier here\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Visualising the Training set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),\n                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha=0.75, cmap=ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Classifier (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n# Visualising the Test set results\nfrom matplotlib.colors import ListedColormap\n\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),\n                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha=0.75, cmap=ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c=ListedColormap(('red', 'green'))(i), label=j)\nplt.title('Classifier (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/EKami/deep_learning_A-Z/blob/2576712a441746db80bbad97b899f27618f9cf2f",
        "file_path": "/Get_the_machine_learning_basics/Data_Preprocessing_Template/data_preprocessing_template.py",
        "source": "# Data Preprocessing\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Data.csv')\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/EKami/deep_learning_A-Z/blob/2576712a441746db80bbad97b899f27618f9cf2f",
        "file_path": "/Volume_1-Supervised_Deep_Learning/Part_1-Artificial_Neural_Networks-ANN/Section_4-Building_an_ANN/ann.py",
        "source": "# Artificial Neural Network\n\n# Installing Theano\n# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n\n# Installing Tensorflow\n# pip install tensorflow\n\n# Installing Keras\n# pip install --upgrade keras\n\n# Part 1 - Data Preprocessing\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Churn_Modelling.csv')\nX = dataset.iloc[:, 3:13].values\ny = dataset.iloc[:, 13].values\n\n# Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_1 = LabelEncoder()\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\nlabelencoder_X_2 = LabelEncoder()\nX[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\nonehotencoder = OneHotEncoder(categorical_features = [1])\nX = onehotencoder.fit_transform(X).toarray()\nX = X[:, 1:]\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Part 2 - Now let's make the ANN!\n\n# Importing the Keras libraries and packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size = 10, epochs = 100)\n\n# Part 3 - Making predictions and evaluating the model\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/Spredzy/agent2/blob/f8414ae94ae99856083be6309ad84a7bfa37b00b",
        "file_path": "/dciagent/plugins/ansibleplugin.py",
        "source": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2016 Red Hat, Inc\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nfrom ansible import inventory\nfrom ansible import vars\nfrom ansible.executor import playbook_executor\nfrom ansible.parsing import dataloader\n\nfrom ansible.utils.display import Display\n\nfrom dciclient.v1 import helper as dci_helper\nfrom dciagent.plugins import plugin\n\n\nimport jinja2\nimport os\nimport subprocess\n\ndisplay = Display()\n\nclass Options(object):\n    def __init__(self, verbosity=None, inventory=None, listhosts=None, subset=None, module_paths=None, extra_vars=None,\n                 forks=None, ask_vault_pass=None, vault_password_files=None, new_vault_password_file=None,\n                 output_file=None, tags=None, skip_tags=None, one_line=None, tree=None, ask_sudo_pass=None, ask_su_pass=None,\n                 sudo=None, sudo_user=None, become=None, become_method=None, become_user=None, become_ask_pass=None,\n                 ask_pass=None, private_key_file=None, remote_user=None, connection=None, timeout=None, ssh_common_args=None,\n                 sftp_extra_args=None, scp_extra_args=None, ssh_extra_args=None, poll_interval=None, seconds=None, check=None,\n                 syntax=None, diff=None, force_handlers=None, flush_cache=None, listtasks=None, listtags=None, module_path=None):\n        self.verbosity = verbosity\n        self.inventory = inventory\n        self.listhosts = listhosts\n        self.subset = subset\n        self.module_paths = module_paths\n        self.extra_vars = extra_vars\n        self.forks = forks\n        self.ask_vault_pass = ask_vault_pass\n        self.vault_password_files = vault_password_files\n        self.new_vault_password_file = new_vault_password_file\n        self.output_file = output_file\n        self.tags = tags\n        self.skip_tags = skip_tags\n        self.one_line = one_line\n        self.tree = tree\n        self.ask_sudo_pass = ask_sudo_pass\n        self.ask_su_pass = ask_su_pass\n        self.sudo = sudo\n        self.sudo_user = sudo_user\n        self.become = become\n        self.become_method = become_method\n        self.become_user = become_user\n        self.become_ask_pass = become_ask_pass\n        self.ask_pass = ask_pass\n        self.private_key_file = private_key_file\n        self.remote_user = remote_user\n        self.connection = connection\n        self.timeout = timeout\n        self.ssh_common_args = ssh_common_args\n        self.sftp_extra_args = sftp_extra_args\n        self.scp_extra_args = scp_extra_args\n        self.ssh_extra_args = ssh_extra_args\n        self.poll_interval = poll_interval\n        self.seconds = seconds\n        self.check = check\n        self.syntax = syntax\n        self.diff = diff\n        self.force_handlers = force_handlers\n        self.flush_cache = flush_cache\n        self.listtasks = listtasks\n        self.listtags = listtags\n        self.module_path = module_path\n\n\nclass Runner(object):\n\n    def __init__(self, playbook, options=None, verbosity=0):\n\n        if options is None:\n            self.options = Options()\n            self.options.verbosity = verbosity\n\n        self.loader = dataloader.DataLoader()\n        self.variable_manager = vars.VariableManager()\n\n        self.inventory = inventory.Inventory(\n            loader=self.loader,\n            variable_manager=self.variable_manager,\n            host_list='/etc/ansible/hosts'\n        )\n        self.variable_manager.set_inventory(self.inventory)\n\n        # Playbook to run, from the current working directory.\n        pb_dir = os.path.abspath('.')\n        playbook_path = \"%s/%s\" % (pb_dir, playbook)\n        display.verbosity = self.options.verbosity\n\n        self.pbex = playbook_executor.PlaybookExecutor(\n            #playbooks=[playbook_path],\n            playbooks=[playbook],\n            inventory=self.inventory,\n            variable_manager=self.variable_manager,\n            loader=self.loader,\n            options=self.options,\n            passwords={})\n\n    def run(self, job_id):\n        \"\"\"Run the playbook and returns the playbook's stats.\"\"\"\n        self.variable_manager.extra_vars = {'job_id': job_id}\n        self.pbex.run()\n        return self.pbex._tqm._stats\n\n\nclass AnsiblePlugin(plugin.Plugin):\n\n    def __init__(self, conf):\n        super(AnsiblePlugin, self).__init__(conf)\n\n\n    def generate_ansible_playbook_from_template(self, template_file, data):\n\n        templateLoader = jinja2.FileSystemLoader( searchpath=\"/\" )\n        templateEnv = jinja2.Environment( loader=templateLoader )\n        template = templateEnv.get_template( template_file )\n        outputText = template.render( data )\n\n        return outputText\n\n\n\n\n\n    def run(self, state, data=None, context=None):\n        \"\"\"Run ansible-playbook on the specified playbook. \"\"\"\n\n        playbook = None\n        log_file = None\n        template = None\n\n        if state in self.conf:\n            if 'playbook' in self.conf[state]:\n                playbook = self.conf[state]['playbook']\n            if 'log_file' in self.conf[state]:\n                log_file = self.conf[state]['log_file']\n            if 'template' in self.conf[state]:\n                template = self.conf[state]['template']\n\n        if playbook is None:\n            playbook = self.conf['playbook']\n        if template is None and template in self.conf:\n            template = self.conf['template']\n\n        if log_file is None:\n            if 'log_file' in self.conf:\n                log_file = self.conf['log_file']\n            else:\n                log_file = open(os.devnull, 'w')\n\n        if template:\n            open(playbook, 'w').write(\n                self.generate_ansible_playbook_from_template(template, data)\n            )\n            \n        runner = Runner(playbook=playbook, verbosity=0)\n        stats = runner.run(job_id=context.last_job_id)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/Azure/WALinuxAgent/blob/579416da57410c65042ab916183e78686d69913c",
        "file_path": "/azurelinuxagent/common/osutil/bigip.py",
        "source": "# Copyright 2016 F5 Networks Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Requires Python 2.6+ and Openssl 1.0+\n#\n\nimport array\nimport fcntl\nimport os\nimport platform\nimport re\nimport socket\nimport struct\nimport time\n\ntry:\n    # WAAgent > 2.1.3\n    import azurelinuxagent.common.logger as logger\n    import azurelinuxagent.common.utils.shellutil as shellutil\n\n    from azurelinuxagent.common.exception import OSUtilError\n    from azurelinuxagent.common.osutil.default import DefaultOSUtil\nexcept ImportError:\n    # WAAgent <= 2.1.3\n    import azurelinuxagent.logger as logger\n    import azurelinuxagent.utils.shellutil as shellutil\n\n    from azurelinuxagent.exception import OSUtilError\n    from azurelinuxagent.distro.default.osutil import DefaultOSUtil\n\n\nclass BigIpOSUtil(DefaultOSUtil):\n    def __init__(self):\n        super(BigIpOSUtil, self).__init__()\n\n    def _wait_until_mcpd_is_initialized(self):\n        \"\"\"Wait for mcpd to become available\n\n        All configuration happens in mcpd so we need to wait that this is\n        available before we go provisioning the system. I call this method\n        at the first opportunity I have (during the DVD mounting call).\n        This ensures that the rest of the provisioning does not need to wait\n        for mcpd to be available unless it absolutely wants to.\n\n        :return bool: Returns True upon success\n        :raises OSUtilError: Raises exception if mcpd does not come up within\n                             roughly 50 minutes (100 * 30 seconds)\n        \"\"\"\n        for retries in range(1, 100):\n            # Retry until mcpd completes startup:\n            logger.info(\"Checking to see if mcpd is up\")\n            rc = shellutil.run(\"/usr/bin/tmsh -a show sys mcp-state field-fmt 2>/dev/null | grep phase | grep running\", chk_err=False)\n            if rc == 0:\n                logger.info(\"mcpd is up!\")\n                break\n            time.sleep(30)\n\n        if rc is 0:\n            return True\n\n        raise OSUtilError(\n            \"mcpd hasn't completed initialization! Cannot proceed!\"\n        )\n\n    def _save_sys_config(self):\n        cmd = \"/usr/bin/tmsh save sys config\"\n        rc = shellutil.run(cmd)\n        if rc != 0:\n            logger.error(\"WARNING: Cannot save sys config on 1st boot.\")\n        return rc\n\n    def restart_ssh_service(self):\n        return shellutil.run(\"/usr/bin/bigstart restart sshd\", chk_err=False)\n\n    def stop_agent_service(self):\n        return shellutil.run(\"/sbin/service waagent stop\", chk_err=False)\n\n    def start_agent_service(self):\n        return shellutil.run(\"/sbin/service waagent start\", chk_err=False)\n\n    def register_agent_service(self):\n        return shellutil.run(\"/sbin/chkconfig --add waagent\", chk_err=False)\n\n    def unregister_agent_service(self):\n        return shellutil.run(\"/sbin/chkconfig --del waagent\", chk_err=False)\n\n    def get_dhcp_pid(self):\n        ret = shellutil.run_get_output(\"/sbin/pidof dhclient\")\n        return ret[1] if ret[0] == 0 else None\n\n    def set_hostname(self, hostname):\n        \"\"\"Set the static hostname of the device\n\n        Normally, tmsh is used to set the hostname for the system. For our\n        purposes at this time though, I would hesitate to trust this function.\n\n        Azure(Stack) uses the name that you provide in the Web UI or ARM (for\n        example) as the value of the hostname argument to this method. The\n        problem is that there is nowhere in the UI that specifies the\n        restrictions and checks that tmsh has for the hostname.\n\n        For example, if you set the name \"bigip1\" in the Web UI, Azure(Stack)\n        considers that a perfectly valid name. When WAAgent gets around to\n        running though, tmsh will reject that value because it is not a fully\n        qualified domain name. The proper value should have been bigip.xxx.yyy\n\n        WAAgent will not fail if this command fails, but the hostname will not\n        be what the user set either. Currently we do not set the hostname when\n        WAAgent starts up, so I am passing on setting it here too.\n\n        :param hostname: The hostname to set on the device\n        \"\"\"\n        return None\n\n    def set_dhcp_hostname(self, hostname):\n        \"\"\"Sets the DHCP hostname\n\n        See `set_hostname` for an explanation of why I pass here\n\n        :param hostname: The hostname to set on the device\n        \"\"\"\n        return None\n\n    def useradd(self, username, expiration=None):\n        \"\"\"Create user account using tmsh\n\n        Our policy is to create two accounts when booting a BIG-IP instance.\n        The first account is the one that the user specified when they did\n        the instance creation. The second one is the admin account that is,\n        or should be, built in to the system.\n\n        :param username: The username that you want to add to the system\n        :param expiration: The expiration date to use. We do not use this\n                           value.\n        \"\"\"\n        if self.get_userentry(username):\n            logger.info(\"User {0} already exists, skip useradd\", username)\n            return None\n\n        cmd = \"/usr/bin/tmsh create auth user %s partition-access add { all-partitions { role admin } } shell bash\" % (username)\n        retcode, out = shellutil.run_get_output(cmd, log_cmd=True, chk_err=True)\n        if retcode != 0:\n            raise OSUtilError(\n                \"Failed to create user account:{0}, retcode:{1}, output:{2}\".format(username, retcode, out)\n            )\n        self._save_sys_config()\n        return retcode\n\n    def chpasswd(self, username, password, crypt_id=6, salt_len=10):\n        \"\"\"Change a user's password with tmsh\n\n        Since we are creating the user specified account and additionally\n        changing the password of the built-in 'admin' account, both must\n        be modified in this method.\n\n        Note that the default method also checks for a \"system level\" of the\n        user; based on the value of UID_MIN in /etc/login.defs. In our env,\n        all user accounts have the UID 0. So we can't rely on this value.\n\n        :param username: The username whose password to change\n        :param password: The unencrypted password to set for the user\n        :param crypt_id: If encrypting the password, the crypt_id that was used\n        :param salt_len: If encrypting the password, the length of the salt\n                         value used to do it.\n        \"\"\"\n\n        # Start by setting the password of the user provided account\n        cmd = \"/usr/bin/tmsh modify auth user {0} password '{1}'\".format(username, password)\n        ret, output = shellutil.run_get_output(cmd, log_cmd=False, chk_err=True)\n        if ret != 0:\n            raise OSUtilError(\n                \"Failed to set password for {0}: {1}\".format(username, output)\n            )\n\n        # Next, set the password of the built-in 'admin' account to be have\n        # the same password as the user provided account\n        userentry = self.get_userentry('admin')\n        if userentry is None:\n            raise OSUtilError(\"The 'admin' user account was not found!\")\n\n        cmd = \"/usr/bin/tmsh modify auth user 'admin' password '{0}'\".format(password)\n        ret, output = shellutil.run_get_output(cmd, log_cmd=False, chk_err=True)\n        if ret != 0:\n            raise OSUtilError(\n                \"Failed to set password for 'admin': {0}\".format(output)\n            )\n        self._save_sys_config()\n        return ret\n\n    def del_account(self, username):\n        \"\"\"Deletes a user account.\n\n        Note that the default method also checks for a \"system level\" of the\n        user; based on the value of UID_MIN in /etc/login.defs. In our env,\n        all user accounts have the UID 0. So we can't rely on this value.\n\n        We also don't use sudo, so we remove that method call as well.\n\n        :param username:\n        :return:\n        \"\"\"\n        shellutil.run(\"> /var/run/utmp\")\n        shellutil.run(\"/usr/bin/tmsh delete auth user \" + username)\n\n    def get_dvd_device(self, dev_dir='/dev'):\n        \"\"\"Find BIG-IP's CD/DVD device\n\n        This device is almost certainly /dev/cdrom so I added the ? to this pattern.\n        Note that this method will return upon the first device found, but in my\n        tests with 12.1.1 it will also find /dev/sr0 on occasion. This is NOT the\n        correct CD/DVD device though.\n\n        :todo: Consider just always returning \"/dev/cdrom\" here if that device device\n               exists on all platforms that are supported on Azure(Stack)\n        :param dev_dir: The root directory from which to look for devices\n        \"\"\"\n        patten = r'(sr[0-9]|hd[c-z]|cdrom[0-9]?)'\n        for dvd in [re.match(patten, dev) for dev in os.listdir(dev_dir)]:\n            if dvd is not None:\n                return \"/dev/{0}\".format(dvd.group(0))\n        raise OSUtilError(\"Failed to get dvd device\")\n\n    def mount_dvd(self, **kwargs):\n        \"\"\"Mount the DVD containing the provisioningiso.iso file\n\n        This is the _first_ hook that WAAgent provides for us, so this is the\n        point where we should wait for mcpd to load. I am just overloading\n        this method to add the mcpd wait. Then I proceed with the stock code.\n\n        :param max_retry: Maximum number of retries waagent will make when\n                          mounting the provisioningiso.iso DVD\n        :param chk_err: Whether to check for errors or not in the mounting\n                        commands\n        \"\"\"\n        self._wait_until_mcpd_is_initialized()\n        return super(BigIpOSUtil, self).mount_dvd(**kwargs)\n\n    def eject_dvd(self, chk_err=True):\n        \"\"\"Runs the eject command to eject the provisioning DVD\n\n        BIG-IP does not include an eject command. It is sufficient to just\n        umount the DVD disk. But I will log that we do not support this for\n        future reference.\n\n        :param chk_err: Whether or not to check for errors raised by the eject\n                        command\n        \"\"\"\n        logger.warn(\"Eject is not supported on this platform\")\n\n    def get_first_if(self):\n        \"\"\"Return the interface name, and ip addr of the management interface.\n\n        We need to add a struct_size check here because, curiously, our 64bit\n        platform is identified by python in Azure(Stack) as 32 bit and without\n        adjusting the struct_size, we can't get the information we need.\n\n        I believe this may be caused by only python i686 being shipped with\n        BIG-IP instead of python x86_64??\n        \"\"\"\n        iface = ''\n        expected = 16  # how many devices should I expect...\n\n        python_arc = platform.architecture()[0]\n        if python_arc == '64bit':\n            struct_size = 40  # for 64bit the size is 40 bytes\n        else:\n            struct_size = 32  # for 32bit the size is 32 bytes\n        sock = socket.socket(socket.AF_INET,\n                             socket.SOCK_DGRAM,\n                             socket.IPPROTO_UDP)\n        buff = array.array('B', b'\\0' * (expected * struct_size))\n        param = struct.pack('iL',\n                            expected*struct_size,\n                            buff.buffer_info()[0])\n        ret = fcntl.ioctl(sock.fileno(), 0x8912, param)\n        retsize = (struct.unpack('iL', ret)[0])\n        if retsize == (expected * struct_size):\n            logger.warn(('SIOCGIFCONF returned more than {0} up '\n                         'network interfaces.'), expected)\n        sock = buff.tostring()\n        for i in range(0, struct_size * expected, struct_size):\n            iface = self._format_single_interface_name(sock, i)\n\n            # Azure public was returning \"lo:1\" when deploying WAF\n            if b'lo' in iface:\n                continue\n            else:\n                break\n        return iface.decode('latin-1'), socket.inet_ntoa(sock[i+20:i+24])\n\n    def _format_single_interface_name(self, sock, offset):\n        return sock[offset:offset+16].split(b'\\0', 1)[0]\n\n    def route_add(self, net, mask, gateway):\n        \"\"\"Add specified route using tmsh.\n\n        :param net:\n        :param mask:\n        :param gateway:\n        :return:\n        \"\"\"\n        cmd = (\"/usr/bin/tmsh create net route \"\n               \"{0}/{1} gw {2}\").format(net, mask, gateway)\n        return shellutil.run(cmd, chk_err=False)\n\n    def device_for_ide_port(self, port_id):\n        \"\"\"Return device name attached to ide port 'n'.\n\n        Include a wait in here because BIG-IP may not have yet initialized\n        this list of devices.\n\n        :param port_id:\n        :return:\n        \"\"\"\n        for retries in range(1, 100):\n            # Retry until devices are ready\n            if os.path.exists(\"/sys/bus/vmbus/devices/\"):\n                break\n            else:\n                time.sleep(10)\n        return super(BigIpOSUtil, self).device_for_ide_port(port_id)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/Azure/WALinuxAgent/blob/579416da57410c65042ab916183e78686d69913c",
        "file_path": "/azurelinuxagent/common/osutil/default.py",
        "source": "#\n# Copyright 2018 Microsoft Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Requires Python 2.6+ and Openssl 1.0+\n#\n\nimport array\nimport base64\nimport datetime\nimport errno\nimport fcntl\nimport glob\nimport multiprocessing\nimport os\nimport platform\nimport pwd\nimport re\nimport shutil\nimport socket\nimport struct\nimport sys\nimport time\n\nimport azurelinuxagent.common.logger as logger\nimport azurelinuxagent.common.conf as conf\nimport azurelinuxagent.common.utils.fileutil as fileutil\nimport azurelinuxagent.common.utils.shellutil as shellutil\nimport azurelinuxagent.common.utils.textutil as textutil\n\nfrom azurelinuxagent.common.exception import OSUtilError\nfrom azurelinuxagent.common.future import ustr\nfrom azurelinuxagent.common.utils.cryptutil import CryptUtil\nfrom azurelinuxagent.common.utils.flexible_version import FlexibleVersion\n\n__RULES_FILES__ = [ \"/lib/udev/rules.d/75-persistent-net-generator.rules\",\n                    \"/etc/udev/rules.d/70-persistent-net.rules\" ]\n\n\"\"\"\nDefine distro specific behavior. OSUtil class defines default behavior\nfor all distros. Each concrete distro classes could overwrite default behavior\nif needed.\n\"\"\"\n\nIPTABLES_VERSION_PATTERN = re.compile(\"^[^\\d\\.]*([\\d\\.]+).*$\")\nIPTABLES_VERSION = \"iptables --version\"\nIPTABLES_LOCKING_VERSION = FlexibleVersion('1.4.21')\n\nFIREWALL_ACCEPT = \"iptables {0} -t security -{1} OUTPUT -d {2} -p tcp -m owner --uid-owner {3} -j ACCEPT\"\n# Note:\n# -- Initially \"flight\" the change to ACCEPT packets and develop a metric baseline\n#    A subsequent release will convert the ACCEPT to DROP\n# FIREWALL_DROP = \"iptables {0} -t security -{1} OUTPUT -d {2} -p tcp -m conntrack --ctstate INVALID,NEW -j ACCEPT\"\nFIREWALL_DROP = \"iptables {0} -t security -{1} OUTPUT -d {2} -p tcp -m conntrack --ctstate INVALID,NEW -j DROP\"\nFIREWALL_LIST = \"iptables {0} -t security -L -nxv\"\nFIREWALL_PACKETS = \"iptables {0} -t security -L OUTPUT --zero OUTPUT -nxv\"\nFIREWALL_FLUSH = \"iptables {0} -t security --flush\"\n\n# Precisely delete the rules created by the agent.\n# this rule was used <= 2.2.25.  This rule helped to validate our change, and determine impact.\nFIREWALL_DELETE_CONNTRACK_ACCEPT = \"iptables {0} -t security -D OUTPUT -d {1} -p tcp -m conntrack --ctstate INVALID,NEW -j ACCEPT\"\nFIREWALL_DELETE_OWNER_ACCEPT = \"iptables {0} -t security -D OUTPUT -d {1} -p tcp -m owner --uid-owner {2} -j ACCEPT\"\nFIREWALL_DELETE_CONNTRACK_DROP = \"iptables {0} -t security -D OUTPUT -d {1} -p tcp -m conntrack --ctstate INVALID,NEW -j DROP\"\n\nPACKET_PATTERN = \"^\\s*(\\d+)\\s+(\\d+)\\s+DROP\\s+.*{0}[^\\d]*$\"\nALL_CPUS_REGEX = re.compile('^cpu .*')\n\n\n_enable_firewall = True\n\nDMIDECODE_CMD = 'dmidecode --string system-uuid'\nPRODUCT_ID_FILE = '/sys/class/dmi/id/product_uuid'\nUUID_PATTERN = re.compile(\n    r'^\\s*[A-F0-9]{8}(?:\\-[A-F0-9]{4}){3}\\-[A-F0-9]{12}\\s*$',\n    re.IGNORECASE)\n\nIOCTL_SIOCGIFCONF = 0x8912\nIOCTL_SIOCGIFFLAGS = 0x8913\nIOCTL_SIOCGIFHWADDR = 0x8927\nIFNAMSIZ = 16\n\n\nclass DefaultOSUtil(object):\n    def __init__(self):\n        self.agent_conf_file_path = '/etc/waagent.conf'\n        self.selinux = None\n        self.disable_route_warning = False\n\n    def get_firewall_dropped_packets(self, dst_ip=None):\n        # If a previous attempt failed, do not retry\n        global _enable_firewall\n        if not _enable_firewall:\n            return 0\n\n        try:\n            wait = self.get_firewall_will_wait()\n\n            rc, output = shellutil.run_get_output(FIREWALL_PACKETS.format(wait), log_cmd=False)\n            if rc == 3:\n                # Transient error  that we ignore.  This code fires every loop\n                # of the daemon (60m), so we will get the value eventually.\n                return 0\n\n            if rc != 0:\n                return -1\n\n            pattern = re.compile(PACKET_PATTERN.format(dst_ip))\n            for line in output.split('\\n'):\n                m = pattern.match(line)\n                if m is not None:\n                    return int(m.group(1))\n            \n            return 0\n\n        except Exception as e:\n            _enable_firewall = False\n            logger.warn(\"Unable to retrieve firewall packets dropped\"\n                        \"{0}\".format(ustr(e)))\n            return -1\n\n    def get_firewall_will_wait(self):\n        # Determine if iptables will serialize access\n        rc, output = shellutil.run_get_output(IPTABLES_VERSION)\n        if rc != 0:\n            msg = \"Unable to determine version of iptables\"\n            logger.warn(msg)\n            raise Exception(msg)\n\n        m = IPTABLES_VERSION_PATTERN.match(output)\n        if m is None:\n            msg = \"iptables did not return version information\"\n            logger.warn(msg)\n            raise Exception(msg)\n\n        wait = \"-w\" \\\n                if FlexibleVersion(m.group(1)) >= IPTABLES_LOCKING_VERSION \\\n                else \"\"\n        return wait\n\n    def _delete_rule(self, rule):\n        \"\"\"\n        Continually execute the delete operation until the return\n        code is non-zero or the limit has been reached.\n        \"\"\"\n        for i in range(1, 100):\n            rc = shellutil.run(rule, chk_err=False)\n            if rc == 1:\n                return\n            elif rc == 2:\n                raise Exception(\"invalid firewall deletion rule '{0}'\".format(rule))\n\n    def remove_firewall(self, dst_ip=None, uid=None):\n        # If a previous attempt failed, do not retry\n        global _enable_firewall\n        if not _enable_firewall:\n            return False\n\n        try:\n            if dst_ip is None or uid is None:\n                msg = \"Missing arguments to enable_firewall\"\n                logger.warn(msg)\n                raise Exception(msg)\n\n            wait = self.get_firewall_will_wait()\n\n            # This rule was <= 2.2.25 only, and may still exist on some VMs.  Until 2.2.25\n            # has aged out, keep this cleanup in place.\n            self._delete_rule(FIREWALL_DELETE_CONNTRACK_ACCEPT.format(wait, dst_ip))\n\n            self._delete_rule(FIREWALL_DELETE_OWNER_ACCEPT.format(wait, dst_ip, uid))\n            self._delete_rule(FIREWALL_DELETE_CONNTRACK_DROP.format(wait, dst_ip))\n\n            return True\n\n        except Exception as e:\n            _enable_firewall = False\n            logger.info(\"Unable to remove firewall -- \"\n                        \"no further attempts will be made: \"\n                        \"{0}\".format(ustr(e)))\n            return False\n\n    def enable_firewall(self, dst_ip=None, uid=None):\n        # If a previous attempt failed, do not retry\n        global _enable_firewall\n        if not _enable_firewall:\n            return False\n\n        try:\n            if dst_ip is None or uid is None:\n                msg = \"Missing arguments to enable_firewall\"\n                logger.warn(msg)\n                raise Exception(msg)\n\n            wait = self.get_firewall_will_wait()\n\n            # If the DROP rule exists, make no changes\n            drop_rule = FIREWALL_DROP.format(wait, \"C\", dst_ip)\n            rc = shellutil.run(drop_rule, chk_err=False)\n            if rc == 0:\n                logger.verbose(\"Firewall appears established\")\n                return True\n            elif rc == 2:\n                self.remove_firewall(dst_ip, uid)\n                msg = \"please upgrade iptables to a version that supports the -C option\"\n                logger.warn(msg)\n                raise Exception(msg)\n\n            # Otherwise, append both rules\n            accept_rule = FIREWALL_ACCEPT.format(wait, \"A\", dst_ip, uid)\n            drop_rule = FIREWALL_DROP.format(wait, \"A\", dst_ip)\n\n            if shellutil.run(accept_rule) != 0:\n                msg = \"Unable to add ACCEPT firewall rule '{0}'\".format(\n                    accept_rule)\n                logger.warn(msg)\n                raise Exception(msg)\n\n            if shellutil.run(drop_rule) != 0:\n                msg = \"Unable to add DROP firewall rule '{0}'\".format(\n                    drop_rule)\n                logger.warn(msg)\n                raise Exception(msg)\n\n            logger.info(\"Successfully added Azure fabric firewall rules\")\n\n            rc, output = shellutil.run_get_output(FIREWALL_LIST.format(wait))\n            if rc == 0:\n                logger.info(\"Firewall rules:\\n{0}\".format(output))\n            else:\n                logger.warn(\"Listing firewall rules failed: {0}\".format(output))\n\n            return True\n\n        except Exception as e:\n            _enable_firewall = False\n            logger.info(\"Unable to establish firewall -- \"\n                        \"no further attempts will be made: \"\n                        \"{0}\".format(ustr(e)))\n            return False\n\n    def _correct_instance_id(self, id):\n        '''\n        Azure stores the instance ID with an incorrect byte ordering for the\n        first parts. For example, the ID returned by the metadata service:\n\n            D0DF4C54-4ECB-4A4B-9954-5BDF3ED5C3B8\n\n        will be found as:\n\n            544CDFD0-CB4E-4B4A-9954-5BDF3ED5C3B8\n\n        This code corrects the byte order such that it is consistent with\n        that returned by the metadata service.\n        '''\n\n        if not UUID_PATTERN.match(id):\n            return id\n\n        parts = id.split('-')\n        return '-'.join([\n                textutil.swap_hexstring(parts[0], width=2),\n                textutil.swap_hexstring(parts[1], width=2),\n                textutil.swap_hexstring(parts[2], width=2),\n                parts[3],\n                parts[4]\n            ])\n\n    def is_current_instance_id(self, id_that):\n        '''\n        Compare two instance IDs for equality, but allow that some IDs\n        may have been persisted using the incorrect byte ordering.\n        '''\n        id_this = self.get_instance_id()\n        return id_that == id_this or \\\n            id_that == self._correct_instance_id(id_this)\n\n    def is_cgroups_supported(self):\n        return False\n\n    def mount_cgroups(self):\n        pass\n\n    def get_agent_conf_file_path(self):\n        return self.agent_conf_file_path\n\n    def get_instance_id(self):\n        '''\n        Azure records a UUID as the instance ID\n        First check /sys/class/dmi/id/product_uuid.\n        If that is missing, then extracts from dmidecode\n        If nothing works (for old VMs), return the empty string\n        '''\n        if os.path.isfile(PRODUCT_ID_FILE):\n            s = fileutil.read_file(PRODUCT_ID_FILE).strip()\n\n        else:\n            rc, s = shellutil.run_get_output(DMIDECODE_CMD)\n            if rc != 0 or UUID_PATTERN.match(s) is None:\n                return \"\"\n\n        return self._correct_instance_id(s.strip())\n\n    def get_userentry(self, username):\n        try:\n            return pwd.getpwnam(username)\n        except KeyError:\n            return None\n\n    def is_sys_user(self, username):\n        \"\"\"\n        Check whether use is a system user. \n        If reset sys user is allowed in conf, return False\n        Otherwise, check whether UID is less than UID_MIN\n        \"\"\"\n        if conf.get_allow_reset_sys_user():\n            return False\n\n        userentry = self.get_userentry(username)\n        uidmin = None\n        try:\n            uidmin_def = fileutil.get_line_startingwith(\"UID_MIN\",\n                                                        \"/etc/login.defs\")\n            if uidmin_def is not None:\n                uidmin = int(uidmin_def.split()[1])\n        except IOError as e:\n            pass\n        if uidmin == None:\n            uidmin = 100\n        if userentry != None and userentry[2] < uidmin:\n            return True\n        else:\n            return False\n\n    def useradd(self, username, expiration=None):\n        \"\"\"\n        Create user account with 'username'\n        \"\"\"\n        userentry = self.get_userentry(username)\n        if userentry is not None:\n            logger.info(\"User {0} already exists, skip useradd\", username)\n            return\n\n        if expiration is not None:\n            cmd = \"useradd -m {0} -e {1}\".format(username, expiration)\n        else:\n            cmd = \"useradd -m {0}\".format(username)\n        retcode, out = shellutil.run_get_output(cmd)\n        if retcode != 0:\n            raise OSUtilError((\"Failed to create user account:{0}, \"\n                               \"retcode:{1}, \"\n                               \"output:{2}\").format(username, retcode, out))\n\n    def chpasswd(self, username, password, crypt_id=6, salt_len=10):\n        if self.is_sys_user(username):\n            raise OSUtilError((\"User {0} is a system user, \"\n                               \"will not set password.\").format(username))\n        passwd_hash = textutil.gen_password_hash(password, crypt_id, salt_len)\n        cmd = \"usermod -p '{0}' {1}\".format(passwd_hash, username)\n        ret, output = shellutil.run_get_output(cmd, log_cmd=False)\n        if ret != 0:\n            raise OSUtilError((\"Failed to set password for {0}: {1}\"\n                               \"\").format(username, output))\n\n    def conf_sudoer(self, username, nopasswd=False, remove=False):\n        sudoers_dir = conf.get_sudoers_dir()\n        sudoers_wagent = os.path.join(sudoers_dir, 'waagent')\n\n        if not remove:\n            # for older distros create sudoers.d\n            if not os.path.isdir(sudoers_dir):\n                sudoers_file = os.path.join(sudoers_dir, '../sudoers')\n                # create the sudoers.d directory\n                os.mkdir(sudoers_dir)\n                # add the include of sudoers.d to the /etc/sudoers\n                sudoers = '\\n#includedir ' + sudoers_dir + '\\n'\n                fileutil.append_file(sudoers_file, sudoers)\n            sudoer = None\n            if nopasswd:\n                sudoer = \"{0} ALL=(ALL) NOPASSWD: ALL\".format(username)\n            else:\n                sudoer = \"{0} ALL=(ALL) ALL\".format(username)\n            if not os.path.isfile(sudoers_wagent) or \\\n                    fileutil.findstr_in_file(sudoers_wagent, sudoer) is False:\n                fileutil.append_file(sudoers_wagent, \"{0}\\n\".format(sudoer))\n            fileutil.chmod(sudoers_wagent, 0o440)\n        else:\n            # remove user from sudoers\n            if os.path.isfile(sudoers_wagent):\n                try:\n                    content = fileutil.read_file(sudoers_wagent)\n                    sudoers = content.split(\"\\n\")\n                    sudoers = [x for x in sudoers if username not in x]\n                    fileutil.write_file(sudoers_wagent, \"\\n\".join(sudoers))\n                except IOError as e:\n                    raise OSUtilError(\"Failed to remove sudoer: {0}\".format(e))\n\n    def del_root_password(self):\n        try:\n            passwd_file_path = conf.get_passwd_file_path()\n            passwd_content = fileutil.read_file(passwd_file_path)\n            passwd = passwd_content.split('\\n')\n            new_passwd = [x for x in passwd if not x.startswith(\"root:\")]\n            new_passwd.insert(0, \"root:*LOCK*:14600::::::\")\n            fileutil.write_file(passwd_file_path, \"\\n\".join(new_passwd))\n        except IOError as e:\n            raise OSUtilError(\"Failed to delete root password:{0}\".format(e))\n\n    def _norm_path(self, filepath):\n        home = conf.get_home_dir()\n        # Expand HOME variable if present in path\n        path = os.path.normpath(filepath.replace(\"$HOME\", home))\n        return path\n\n    def deploy_ssh_keypair(self, username, keypair):\n        \"\"\"\n        Deploy id_rsa and id_rsa.pub\n        \"\"\"\n        path, thumbprint = keypair\n        path = self._norm_path(path)\n        dir_path = os.path.dirname(path)\n        fileutil.mkdir(dir_path, mode=0o700, owner=username)\n        lib_dir = conf.get_lib_dir()\n        prv_path = os.path.join(lib_dir, thumbprint + '.prv')\n        if not os.path.isfile(prv_path):\n            raise OSUtilError(\"Can't find {0}.prv\".format(thumbprint))\n        shutil.copyfile(prv_path, path)\n        pub_path = path + '.pub'\n        crytputil = CryptUtil(conf.get_openssl_cmd())\n        pub = crytputil.get_pubkey_from_prv(prv_path)\n        fileutil.write_file(pub_path, pub)\n        self.set_selinux_context(pub_path, 'unconfined_u:object_r:ssh_home_t:s0')\n        self.set_selinux_context(path, 'unconfined_u:object_r:ssh_home_t:s0')\n        os.chmod(path, 0o644)\n        os.chmod(pub_path, 0o600)\n\n    def openssl_to_openssh(self, input_file, output_file):\n        cryptutil = CryptUtil(conf.get_openssl_cmd())\n        cryptutil.crt_to_ssh(input_file, output_file)\n\n    def deploy_ssh_pubkey(self, username, pubkey):\n        \"\"\"\n        Deploy authorized_key\n        \"\"\"\n        path, thumbprint, value = pubkey\n        if path is None:\n            raise OSUtilError(\"Public key path is None\")\n\n        crytputil = CryptUtil(conf.get_openssl_cmd())\n\n        path = self._norm_path(path)\n        dir_path = os.path.dirname(path)\n        fileutil.mkdir(dir_path, mode=0o700, owner=username)\n        if value is not None:\n            if not value.startswith(\"ssh-\"):\n                raise OSUtilError(\"Bad public key: {0}\".format(value))\n            fileutil.write_file(path, value)\n        elif thumbprint is not None:\n            lib_dir = conf.get_lib_dir()\n            crt_path = os.path.join(lib_dir, thumbprint + '.crt')\n            if not os.path.isfile(crt_path):\n                raise OSUtilError(\"Can't find {0}.crt\".format(thumbprint))\n            pub_path = os.path.join(lib_dir, thumbprint + '.pub')\n            pub = crytputil.get_pubkey_from_crt(crt_path)\n            fileutil.write_file(pub_path, pub)\n            self.set_selinux_context(pub_path,\n                                     'unconfined_u:object_r:ssh_home_t:s0')\n            self.openssl_to_openssh(pub_path, path)\n            fileutil.chmod(pub_path, 0o600)\n        else:\n            raise OSUtilError(\"SSH public key Fingerprint and Value are None\")\n\n        self.set_selinux_context(path, 'unconfined_u:object_r:ssh_home_t:s0')\n        fileutil.chowner(path, username)\n        fileutil.chmod(path, 0o644)\n\n    def is_selinux_system(self):\n        \"\"\"\n        Checks and sets self.selinux = True if SELinux is available on system.\n        \"\"\"\n        if self.selinux == None:\n            if shellutil.run(\"which getenforce\", chk_err=False) == 0:\n                self.selinux = True\n            else:\n                self.selinux = False\n        return self.selinux\n\n    def is_selinux_enforcing(self):\n        \"\"\"\n        Calls shell command 'getenforce' and returns True if 'Enforcing'.\n        \"\"\"\n        if self.is_selinux_system():\n            output = shellutil.run_get_output(\"getenforce\")[1]\n            return output.startswith(\"Enforcing\")\n        else:\n            return False\n\n    def set_selinux_context(self, path, con):\n        \"\"\"\n        Calls shell 'chcon' with 'path' and 'con' context.\n        Returns exit result.\n        \"\"\"\n        if self.is_selinux_system():\n            if not os.path.exists(path):\n                logger.error(\"Path does not exist: {0}\".format(path))\n                return 1\n            return shellutil.run('chcon ' + con + ' ' + path)\n\n    def conf_sshd(self, disable_password):\n        option = \"no\" if disable_password else \"yes\"\n        conf_file_path = conf.get_sshd_conf_file_path()\n        conf_file = fileutil.read_file(conf_file_path).split(\"\\n\")\n        textutil.set_ssh_config(conf_file, \"PasswordAuthentication\", option)\n        textutil.set_ssh_config(conf_file, \"ChallengeResponseAuthentication\", option)\n        textutil.set_ssh_config(conf_file, \"ClientAliveInterval\", str(conf.get_ssh_client_alive_interval()))\n        fileutil.write_file(conf_file_path, \"\\n\".join(conf_file))\n        logger.info(\"{0} SSH password-based authentication methods.\"\n                    .format(\"Disabled\" if disable_password else \"Enabled\"))\n        logger.info(\"Configured SSH client probing to keep connections alive.\")\n\n    def get_dvd_device(self, dev_dir='/dev'):\n        pattern = r'(sr[0-9]|hd[c-z]|cdrom[0-9]|cd[0-9])'\n        device_list = os.listdir(dev_dir)\n        for dvd in [re.match(pattern, dev) for dev in device_list]:\n            if dvd is not None:\n                return \"/dev/{0}\".format(dvd.group(0))\n        inner_detail = \"The following devices were found, but none matched \" \\\n                       \"the pattern [{0}]: {1}\\n\".format(pattern, device_list)\n        raise OSUtilError(msg=\"Failed to get dvd device from {0}\".format(dev_dir),\n                          inner=inner_detail)\n\n    def mount_dvd(self,\n                  max_retry=6,\n                  chk_err=True,\n                  dvd_device=None,\n                  mount_point=None,\n                  sleep_time=5):\n        if dvd_device is None:\n            dvd_device = self.get_dvd_device()\n        if mount_point is None:\n            mount_point = conf.get_dvd_mount_point()\n        mount_list = shellutil.run_get_output(\"mount\")[1]\n        existing = self.get_mount_point(mount_list, dvd_device)\n\n        if existing is not None:\n            # already mounted\n            logger.info(\"{0} is already mounted at {1}\", dvd_device, existing)\n            return\n\n        if not os.path.isdir(mount_point):\n            os.makedirs(mount_point)\n\n        err = ''\n        for retry in range(1, max_retry):\n            return_code, err = self.mount(dvd_device,\n                                          mount_point,\n                                          option=\"-o ro -t udf,iso9660\",\n                                          chk_err=False)\n            if return_code == 0:\n                logger.info(\"Successfully mounted dvd\")\n                return\n            else:\n                logger.warn(\n                    \"Mounting dvd failed [retry {0}/{1}, sleeping {2} sec]\",\n                    retry,\n                    max_retry - 1,\n                    sleep_time)\n                if retry < max_retry:\n                    time.sleep(sleep_time)\n        if chk_err:\n            raise OSUtilError(\"Failed to mount dvd device\", inner=err)\n\n    def umount_dvd(self, chk_err=True, mount_point=None):\n        if mount_point is None:\n            mount_point = conf.get_dvd_mount_point()\n        return_code = self.umount(mount_point, chk_err=chk_err)\n        if chk_err and return_code != 0:\n            raise OSUtilError(\"Failed to unmount dvd device at {0}\",\n                              mount_point)\n\n    def eject_dvd(self, chk_err=True):\n        dvd = self.get_dvd_device()\n        retcode = shellutil.run(\"eject {0}\".format(dvd))\n        if chk_err and retcode != 0:\n            raise OSUtilError(\"Failed to eject dvd: ret={0}\".format(retcode))\n\n    def try_load_atapiix_mod(self):\n        try:\n            self.load_atapiix_mod()\n        except Exception as e:\n            logger.warn(\"Could not load ATAPI driver: {0}\".format(e))\n\n    def load_atapiix_mod(self):\n        if self.is_atapiix_mod_loaded():\n            return\n        ret, kern_version = shellutil.run_get_output(\"uname -r\")\n        if ret != 0:\n            raise Exception(\"Failed to call uname -r\")\n        mod_path = os.path.join('/lib/modules',\n                                kern_version.strip('\\n'),\n                                'kernel/drivers/ata/ata_piix.ko')\n        if not os.path.isfile(mod_path):\n            raise Exception(\"Can't find module file:{0}\".format(mod_path))\n\n        ret, output = shellutil.run_get_output(\"insmod \" + mod_path)\n        if ret != 0:\n            raise Exception(\"Error calling insmod for ATAPI CD-ROM driver\")\n        if not self.is_atapiix_mod_loaded(max_retry=3):\n            raise Exception(\"Failed to load ATAPI CD-ROM driver\")\n\n    def is_atapiix_mod_loaded(self, max_retry=1):\n        for retry in range(0, max_retry):\n            ret = shellutil.run(\"lsmod | grep ata_piix\", chk_err=False)\n            if ret == 0:\n                logger.info(\"Module driver for ATAPI CD-ROM is already present.\")\n                return True\n            if retry < max_retry - 1:\n                time.sleep(1)\n        return False\n\n    def mount(self, device, mount_point, option=\"\", chk_err=True):\n        cmd = \"mount {0} {1} {2}\".format(option, device, mount_point)\n        retcode, err = shellutil.run_get_output(cmd, chk_err)\n        if retcode != 0:\n            detail = \"[{0}] returned {1}: {2}\".format(cmd, retcode, err)\n            err = detail\n        return retcode, err\n\n    def umount(self, mount_point, chk_err=True):\n        return shellutil.run(\"umount {0}\".format(mount_point), chk_err=chk_err)\n\n    def allow_dhcp_broadcast(self):\n        # Open DHCP port if iptables is enabled.\n        # We supress error logging on error.\n        shellutil.run(\"iptables -D INPUT -p udp --dport 68 -j ACCEPT\",\n                      chk_err=False)\n        shellutil.run(\"iptables -I INPUT -p udp --dport 68 -j ACCEPT\",\n                      chk_err=False)\n\n\n    def remove_rules_files(self, rules_files=__RULES_FILES__):\n        lib_dir = conf.get_lib_dir()\n        for src in rules_files:\n            file_name = fileutil.base_name(src)\n            dest = os.path.join(lib_dir, file_name)\n            if os.path.isfile(dest):\n                os.remove(dest)\n            if os.path.isfile(src):\n                logger.warn(\"Move rules file {0} to {1}\", file_name, dest)\n                shutil.move(src, dest)\n\n    def restore_rules_files(self, rules_files=__RULES_FILES__):\n        lib_dir = conf.get_lib_dir()\n        for dest in rules_files:\n            filename = fileutil.base_name(dest)\n            src = os.path.join(lib_dir, filename)\n            if os.path.isfile(dest):\n                continue\n            if os.path.isfile(src):\n                logger.warn(\"Move rules file {0} to {1}\", filename, dest)\n                shutil.move(src, dest)\n\n    def get_mac_addr(self):\n        \"\"\"\n        Convenience function, returns mac addr bound to\n        first non-loopback interface.\n        \"\"\"\n        ifname = self.get_if_name()\n        addr = self.get_if_mac(ifname)\n        return textutil.hexstr_to_bytearray(addr)\n\n    def get_if_mac(self, ifname):\n        \"\"\"\n        Return the mac-address bound to the socket.\n        \"\"\"\n        sock = socket.socket(socket.AF_INET,\n                             socket.SOCK_DGRAM,\n                             socket.IPPROTO_UDP)\n        param = struct.pack('256s', (ifname[:15]+('\\0'*241)).encode('latin-1'))\n        info = fcntl.ioctl(sock.fileno(), IOCTL_SIOCGIFHWADDR, param)\n        sock.close()\n        return ''.join(['%02X' % textutil.str_to_ord(char) for char in info[18:24]])\n\n    @staticmethod\n    def _get_struct_ifconf_size():\n        \"\"\"\n        Return the sizeof struct ifinfo. On 64-bit platforms the size is 40 bytes;\n        on 32-bit platforms the size is 32 bytes.\n        \"\"\"\n        python_arc = platform.architecture()[0]\n        struct_size = 32 if python_arc == '32bit' else 40\n        return struct_size\n\n    def _get_all_interfaces(self):\n        \"\"\"\n        Return a dictionary mapping from interface name to IPv4 address.\n        Interfaces without a name are ignored.\n        \"\"\"\n        expected=16 # how many devices should I expect...\n        struct_size = DefaultOSUtil._get_struct_ifconf_size()\n        array_size = expected * struct_size\n\n        buff = array.array('B', b'\\0' * array_size)\n        param = struct.pack('iL', array_size, buff.buffer_info()[0])\n\n        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)\n        ret = fcntl.ioctl(sock.fileno(), IOCTL_SIOCGIFCONF, param)\n        retsize = (struct.unpack('iL', ret)[0])\n        sock.close()\n\n        if retsize == array_size:\n            logger.warn(('SIOCGIFCONF returned more than {0} up '\n                         'network interfaces.'), expected)\n\n        ifconf_buff = buff.tostring()\n\n        ifaces = {}\n        for i in range(0, array_size, struct_size):\n            iface = ifconf_buff[i:i+IFNAMSIZ].split(b'\\0', 1)[0]\n            if len(iface) > 0:\n                iface_name = iface.decode('latin-1')\n                if iface_name not in ifaces:\n                    ifaces[iface_name] = socket.inet_ntoa(ifconf_buff[i+20:i+24])\n        return ifaces\n\n\n    def get_first_if(self):\n        \"\"\"\n        Return the interface name, and IPv4 addr of the \"primary\" interface or,\n        failing that, any active non-loopback interface.\n        \"\"\"\n        primary = self.get_primary_interface()\n        ifaces = self._get_all_interfaces()\n\n        if primary in ifaces:\n            return primary, ifaces[primary]\n\n        for iface_name in ifaces.keys():\n            if not self.is_loopback(iface_name):\n                logger.info(\"Choosing non-primary [{0}]\".format(iface_name))\n                return iface_name, ifaces[iface_name]\n\n        return '', ''\n\n\n    def get_primary_interface(self):\n        \"\"\"\n        Get the name of the primary interface, which is the one with the\n        default route attached to it; if there are multiple default routes,\n        the primary has the lowest Metric.\n        :return: the interface which has the default route\n        \"\"\"\n        # from linux/route.h\n        RTF_GATEWAY = 0x02\n        DEFAULT_DEST = \"00000000\"\n\n        hdr_iface = \"Iface\"\n        hdr_dest = \"Destination\"\n        hdr_flags = \"Flags\"\n        hdr_metric = \"Metric\"\n\n        idx_iface = -1\n        idx_dest = -1\n        idx_flags = -1\n        idx_metric = -1\n        primary = None\n        primary_metric = None\n\n        if not self.disable_route_warning:\n            logger.info(\"Examine /proc/net/route for primary interface\")\n        with open('/proc/net/route') as routing_table:\n            idx = 0\n            for header in filter(lambda h: len(h) > 0, routing_table.readline().strip(\" \\n\").split(\"\\t\")):\n                if header == hdr_iface:\n                    idx_iface = idx\n                elif header == hdr_dest:\n                    idx_dest = idx\n                elif header == hdr_flags:\n                    idx_flags = idx\n                elif header == hdr_metric:\n                    idx_metric = idx\n                idx = idx + 1\n            for entry in routing_table.readlines():\n                route = entry.strip(\" \\n\").split(\"\\t\")\n                if route[idx_dest] == DEFAULT_DEST and int(route[idx_flags]) & RTF_GATEWAY == RTF_GATEWAY:\n                    metric = int(route[idx_metric])\n                    iface = route[idx_iface]\n                    if primary is None or metric < primary_metric:\n                        primary = iface\n                        primary_metric = metric\n\n        if primary is None:\n            primary = ''\n            if not self.disable_route_warning:\n                with open('/proc/net/route') as routing_table_fh:\n                    routing_table_text = routing_table_fh.read()\n                    logger.warn('Could not determine primary interface, '\n                                'please ensure /proc/net/route is correct')\n                    logger.warn('Contents of /proc/net/route:\\n{0}'.format(routing_table_text))\n                    logger.warn('Primary interface examination will retry silently')\n                    self.disable_route_warning = True\n        else:\n            logger.info('Primary interface is [{0}]'.format(primary))\n            self.disable_route_warning = False\n        return primary\n\n    def is_primary_interface(self, ifname):\n        \"\"\"\n        Indicate whether the specified interface is the primary.\n        :param ifname: the name of the interface - eth0, lo, etc.\n        :return: True if this interface binds the default route\n        \"\"\"\n        return self.get_primary_interface() == ifname\n\n    def is_loopback(self, ifname):\n        \"\"\"\n        Determine if a named interface is loopback.\n        \"\"\"\n        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)\n        ifname_buff = ifname + ('\\0'*256)\n        result = fcntl.ioctl(s.fileno(), IOCTL_SIOCGIFFLAGS, ifname_buff)\n        flags, = struct.unpack('H', result[16:18])\n        isloopback = flags & 8 == 8\n        if not self.disable_route_warning:\n            logger.info('interface [{0}] has flags [{1}], '\n                        'is loopback [{2}]'.format(ifname, flags, isloopback))\n        s.close()\n        return isloopback\n\n    def get_dhcp_lease_endpoint(self):\n        \"\"\"\n        OS specific, this should return the decoded endpoint of\n        the wireserver from option 245 in the dhcp leases file\n        if it exists on disk.\n        :return: The endpoint if available, or None\n        \"\"\"\n        return None\n\n    @staticmethod\n    def get_endpoint_from_leases_path(pathglob):\n        \"\"\"\n        Try to discover and decode the wireserver endpoint in the\n        specified dhcp leases path.\n        :param pathglob: The path containing dhcp lease files\n        :return: The endpoint if available, otherwise None\n        \"\"\"\n        endpoint = None\n\n        HEADER_LEASE = \"lease\"\n        HEADER_OPTION = \"option unknown-245\"\n        HEADER_DNS = \"option domain-name-servers\"\n        HEADER_EXPIRE = \"expire\"\n        FOOTER_LEASE = \"}\"\n        FORMAT_DATETIME = \"%Y/%m/%d %H:%M:%S\"\n\n        logger.info(\"looking for leases in path [{0}]\".format(pathglob))\n        for lease_file in glob.glob(pathglob):\n            leases = open(lease_file).read()\n            if HEADER_OPTION in leases:\n                cached_endpoint = None\n                has_option_245 = False\n                expired = True  # assume expired\n                for line in leases.splitlines():\n                    if line.startswith(HEADER_LEASE):\n                        cached_endpoint = None\n                        has_option_245 = False\n                        expired = True\n                    elif HEADER_DNS in line:\n                        cached_endpoint = line.replace(HEADER_DNS, '').strip(\" ;\")\n                    elif HEADER_OPTION in line:\n                        has_option_245 = True\n                    elif HEADER_EXPIRE in line:\n                        if \"never\" in line:\n                            expired = False\n                        else:\n                            try:\n                                expire_string = line.split(\" \", 4)[-1].strip(\";\")\n                                expire_date = datetime.datetime.strptime(expire_string, FORMAT_DATETIME)\n                                if expire_date > datetime.datetime.utcnow():\n                                    expired = False\n                            except:\n                                logger.error(\"could not parse expiry token '{0}'\".format(line))\n                    elif FOOTER_LEASE in line:\n                        logger.info(\"dhcp entry:{0}, 245:{1}, expired:{2}\".format(\n                            cached_endpoint, has_option_245, expired))\n                        if not expired and cached_endpoint is not None and has_option_245:\n                            endpoint = cached_endpoint\n                            logger.info(\"found endpoint [{0}]\".format(endpoint))\n                            # we want to return the last valid entry, so\n                            # keep searching\n        if endpoint is not None:\n            logger.info(\"cached endpoint found [{0}]\".format(endpoint))\n        else:\n            logger.info(\"cached endpoint not found\")\n        return endpoint\n\n    def is_missing_default_route(self):\n        routes = shellutil.run_get_output(\"route -n\")[1]\n        for route in routes.split(\"\\n\"):\n            if route.startswith(\"0.0.0.0 \") or route.startswith(\"default \"):\n               return False\n        return True\n\n    def get_if_name(self):\n        if_name = ''\n        if_found = False\n        while not if_found:\n            if_name = self.get_first_if()[0]\n            if_found = len(if_name) >= 2\n            if not if_found:\n                time.sleep(2)\n        return if_name\n\n    def get_ip4_addr(self):\n        return self.get_first_if()[1]\n\n    def set_route_for_dhcp_broadcast(self, ifname):\n        return shellutil.run(\"route add 255.255.255.255 dev {0}\".format(ifname),\n                             chk_err=False)\n\n    def remove_route_for_dhcp_broadcast(self, ifname):\n        shellutil.run(\"route del 255.255.255.255 dev {0}\".format(ifname),\n                      chk_err=False)\n\n    def is_dhcp_enabled(self):\n        return False\n\n    def stop_dhcp_service(self):\n        pass\n\n    def start_dhcp_service(self):\n        pass\n\n    def start_network(self):\n        pass\n\n    def start_agent_service(self):\n        pass\n\n    def stop_agent_service(self):\n        pass\n\n    def register_agent_service(self):\n        pass\n\n    def unregister_agent_service(self):\n        pass\n\n    def restart_ssh_service(self):\n        pass\n\n    def route_add(self, net, mask, gateway):\n        \"\"\"\n        Add specified route using /sbin/route add -net.\n        \"\"\"\n        cmd = (\"/sbin/route add -net \"\n               \"{0} netmask {1} gw {2}\").format(net, mask, gateway)\n        return shellutil.run(cmd, chk_err=False)\n\n    def get_dhcp_pid(self):\n        ret = shellutil.run_get_output(\"pidof dhclient\", chk_err=False)\n        return ret[1] if ret[0] == 0 else None\n\n    def set_hostname(self, hostname):\n        fileutil.write_file('/etc/hostname', hostname)\n        shellutil.run(\"hostname {0}\".format(hostname), chk_err=False)\n\n    def set_dhcp_hostname(self, hostname):\n        autosend = r'^[^#]*?send\\s*host-name.*?(<hostname>|gethostname[(,)])'\n        dhclient_files = ['/etc/dhcp/dhclient.conf', '/etc/dhcp3/dhclient.conf', '/etc/dhclient.conf']\n        for conf_file in dhclient_files:\n            if not os.path.isfile(conf_file):\n                continue\n            if fileutil.findre_in_file(conf_file, autosend):\n                #Return if auto send host-name is configured\n                return\n            fileutil.update_conf_file(conf_file,\n                                      'send host-name',\n                                      'send host-name \"{0}\";'.format(hostname))\n\n    def restart_if(self, ifname, retries=3, wait=5):\n        retry_limit=retries+1\n        for attempt in range(1, retry_limit):\n            return_code=shellutil.run(\"ifdown {0} && ifup {0}\".format(ifname))\n            if return_code == 0:\n                return\n            logger.warn(\"failed to restart {0}: return code {1}\".format(ifname, return_code))\n            if attempt < retry_limit:\n                logger.info(\"retrying in {0} seconds\".format(wait))\n                time.sleep(wait)\n            else:\n                logger.warn(\"exceeded restart retries\")\n\n    def publish_hostname(self, hostname):\n        self.set_dhcp_hostname(hostname)\n        self.set_hostname_record(hostname)\n        ifname = self.get_if_name()\n        self.restart_if(ifname)\n\n    def set_scsi_disks_timeout(self, timeout):\n        for dev in os.listdir(\"/sys/block\"):\n            if dev.startswith('sd'):\n                self.set_block_device_timeout(dev, timeout)\n\n    def set_block_device_timeout(self, dev, timeout):\n        if dev is not None and timeout is not None:\n            file_path = \"/sys/block/{0}/device/timeout\".format(dev)\n            content = fileutil.read_file(file_path)\n            original = content.splitlines()[0].rstrip()\n            if original != timeout:\n                fileutil.write_file(file_path, timeout)\n                logger.info(\"Set block dev timeout: {0} with timeout: {1}\",\n                            dev, timeout)\n\n    def get_mount_point(self, mountlist, device):\n        \"\"\"\n        Example of mountlist:\n            /dev/sda1 on / type ext4 (rw)\n            proc on /proc type proc (rw)\n            sysfs on /sys type sysfs (rw)\n            devpts on /dev/pts type devpts (rw,gid=5,mode=620)\n            tmpfs on /dev/shm type tmpfs\n            (rw,rootcontext=\"system_u:object_r:tmpfs_t:s0\")\n            none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)\n            /dev/sdb1 on /mnt/resource type ext4 (rw)\n        \"\"\"\n        if (mountlist and device):\n            for entry in mountlist.split('\\n'):\n                if(re.search(device, entry)):\n                    tokens = entry.split()\n                    #Return the 3rd column of this line\n                    return tokens[2] if len(tokens) > 2 else None\n        return None\n\n    def device_for_ide_port(self, port_id):\n        \"\"\"\n        Return device name attached to ide port 'n'.\n        \"\"\"\n        if port_id > 3:\n            return None\n        g0 = \"00000000\"\n        if port_id > 1:\n            g0 = \"00000001\"\n            port_id = port_id - 2\n        device = None\n        path = \"/sys/bus/vmbus/devices/\"\n        if os.path.exists(path):\n            try:\n                for vmbus in os.listdir(path):\n                    deviceid = fileutil.read_file(os.path.join(path, vmbus, \"device_id\"))\n                    guid = deviceid.lstrip('{').split('-')\n                    if guid[0] == g0 and guid[1] == \"000\" + ustr(port_id):\n                        for root, dirs, files in os.walk(path + vmbus):\n                            if root.endswith(\"/block\"):\n                                device = dirs[0]\n                                break\n                            else:\n                                # older distros\n                                for d in dirs:\n                                    if ':' in d and \"block\" == d.split(':')[0]:\n                                        device = d.split(':')[1]\n                                        break\n                        break\n            except OSError as oe:\n                logger.warn('Could not obtain device for IDE port {0}: {1}', port_id, ustr(oe))\n        return device\n\n    def set_hostname_record(self, hostname):\n        fileutil.write_file(conf.get_published_hostname(), contents=hostname)\n\n    def get_hostname_record(self):\n        hostname_record = conf.get_published_hostname()\n        if not os.path.exists(hostname_record):\n            # this file is created at provisioning time with agents >= 2.2.3\n            hostname = socket.gethostname()\n            logger.info('Hostname record does not exist, '\n                        'creating [{0}] with hostname [{1}]',\n                        hostname_record,\n                        hostname)\n            self.set_hostname_record(hostname)\n        record = fileutil.read_file(hostname_record)\n        return record\n\n    def del_account(self, username):\n        if self.is_sys_user(username):\n            logger.error(\"{0} is a system user. Will not delete it.\", username)\n        shellutil.run(\"> /var/run/utmp\")\n        shellutil.run(\"userdel -f -r \" + username)\n        self.conf_sudoer(username, remove=True)\n\n    def decode_customdata(self, data):\n        return base64.b64decode(data).decode('utf-8')\n\n    def get_total_mem(self):\n        # Get total memory in bytes and divide by 1024**2 to get the value in MB.\n        return os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES') / (1024**2)\n\n    def get_processor_cores(self):\n        return multiprocessing.cpu_count()\n\n    def check_pid_alive(self, pid):\n        try:\n            pid = int(pid)\n            os.kill(pid, 0)\n        except (ValueError, TypeError):\n            return False\n        except OSError as e:\n            if e.errno == errno.EPERM:\n                return True\n            return False\n        return True\n\n    @property\n    def is_64bit(self):\n        return sys.maxsize > 2**32\n\n    @staticmethod\n    def _get_proc_stat():\n        \"\"\"\n        Get the contents of /proc/stat.\n        # cpu  813599 3940 909253 154538746 874851 0 6589 0 0 0\n        # cpu0 401094 1516 453006 77276738 452939 0 3312 0 0 0\n        # cpu1 412505 2423 456246 77262007 421912 0 3276 0 0 0\n\n        :return: A single string with the contents of /proc/stat\n        :rtype: str\n        \"\"\"\n        results = None\n        try:\n            results = fileutil.read_file('/proc/stat')\n        except (OSError, IOError) as ex:\n            logger.warn(\"Couldn't read /proc/stat: {0}\".format(ex.strerror))\n\n        return results\n\n    @staticmethod\n    def get_total_cpu_ticks_since_boot():\n        \"\"\"\n        Compute the number of USER_HZ units of time that have elapsed in all categories, across all cores, since boot.\n\n        :return: int\n        \"\"\"\n        system_cpu = 0\n        proc_stat = DefaultOSUtil._get_proc_stat()\n        if proc_stat is not None:\n            for line in proc_stat.splitlines():\n                if ALL_CPUS_REGEX.match(line):\n                    system_cpu = sum(int(i) for i in line.split()[1:7])\n                    break\n        return system_cpu\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/Azure/WALinuxAgent/blob/579416da57410c65042ab916183e78686d69913c",
        "file_path": "/azurelinuxagent/common/osutil/freebsd.py",
        "source": "# Microsoft Azure Linux Agent\n#\n# Copyright 2018 Microsoft Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Requires Python 2.6+ and Openssl 1.0+\n\nimport azurelinuxagent.common.utils.fileutil as fileutil\nimport azurelinuxagent.common.utils.shellutil as shellutil\nimport azurelinuxagent.common.utils.textutil as textutil\nimport azurelinuxagent.common.logger as logger\nfrom azurelinuxagent.common.exception import OSUtilError\nfrom azurelinuxagent.common.osutil.default import DefaultOSUtil\nfrom azurelinuxagent.common.future import ustr\n\nclass FreeBSDOSUtil(DefaultOSUtil):\n    def __init__(self):\n        super(FreeBSDOSUtil, self).__init__()\n        self._scsi_disks_timeout_set = False\n\n    def set_hostname(self, hostname):\n        rc_file_path = '/etc/rc.conf'\n        conf_file = fileutil.read_file(rc_file_path).split(\"\\n\")\n        textutil.set_ini_config(conf_file, \"hostname\", hostname)\n        fileutil.write_file(rc_file_path, \"\\n\".join(conf_file))\n        shellutil.run(\"hostname {0}\".format(hostname), chk_err=False)\n\n    def restart_ssh_service(self):\n        return shellutil.run('service sshd restart', chk_err=False)\n\n    def useradd(self, username, expiration=None):\n        \"\"\"\n        Create user account with 'username'\n        \"\"\"\n        userentry = self.get_userentry(username)\n        if userentry is not None:\n            logger.warn(\"User {0} already exists, skip useradd\", username)\n            return\n\n        if expiration is not None:\n            cmd = \"pw useradd {0} -e {1} -m\".format(username, expiration)\n        else:\n            cmd = \"pw useradd {0} -m\".format(username)\n        retcode, out = shellutil.run_get_output(cmd)\n        if retcode != 0:\n            raise OSUtilError((\"Failed to create user account:{0}, \"\n                               \"retcode:{1}, \"\n                               \"output:{2}\").format(username, retcode, out))\n\n    def del_account(self, username):\n        if self.is_sys_user(username):\n            logger.error(\"{0} is a system user. Will not delete it.\", username)\n        shellutil.run('> /var/run/utx.active')\n        shellutil.run('rmuser -y ' + username)\n        self.conf_sudoer(username, remove=True)\n\n    def chpasswd(self, username, password, crypt_id=6, salt_len=10):\n        if self.is_sys_user(username):\n            raise OSUtilError((\"User {0} is a system user, \"\n                               \"will not set password.\").format(username))\n        passwd_hash = textutil.gen_password_hash(password, crypt_id, salt_len)\n        cmd = \"echo '{0}'|pw usermod {1} -H 0 \".format(passwd_hash, username)\n        ret, output = shellutil.run_get_output(cmd, log_cmd=False)\n        if ret != 0:\n            raise OSUtilError((\"Failed to set password for {0}: {1}\"\n                               \"\").format(username, output))\n\n    def del_root_password(self):\n        err = shellutil.run('pw usermod root -h -')\n        if err:\n            raise OSUtilError(\"Failed to delete root password: Failed to update password database.\")\n\n    def get_if_mac(self, ifname):\n        data = self._get_net_info()\n        if data[0] == ifname:\n            return data[2].replace(':', '').upper()\n        return None\n\n    def get_first_if(self):\n        return self._get_net_info()[:2]\n\n    def route_add(self, net, mask, gateway):\n        cmd = 'route add {0} {1} {2}'.format(net, gateway, mask)\n        return shellutil.run(cmd, chk_err=False)\n\n    def is_missing_default_route(self):\n        \"\"\"\n        For FreeBSD, the default broadcast goes to current default gw, not a all-ones broadcast address, need to\n        specify the route manually to get it work in a VNET environment.\n        SEE ALSO: man ip(4) IP_ONESBCAST,\n        \"\"\"\n        return True\n\n    def is_dhcp_enabled(self):\n        return True\n\n    def start_dhcp_service(self):\n        shellutil.run(\"/etc/rc.d/dhclient start {0}\".format(self.get_if_name()), chk_err=False)\n\n    def allow_dhcp_broadcast(self):\n        pass\n\n    def set_route_for_dhcp_broadcast(self, ifname):\n        return shellutil.run(\"route add 255.255.255.255 -iface {0}\".format(ifname), chk_err=False)\n\n    def remove_route_for_dhcp_broadcast(self, ifname):\n        shellutil.run(\"route delete 255.255.255.255 -iface {0}\".format(ifname), chk_err=False)\n\n    def get_dhcp_pid(self):\n        ret = shellutil.run_get_output(\"pgrep -n dhclient\", chk_err=False)\n        return ret[1] if ret[0] == 0 else None\n\n    def eject_dvd(self, chk_err=True):\n        dvd = self.get_dvd_device()\n        retcode = shellutil.run(\"cdcontrol -f {0} eject\".format(dvd))\n        if chk_err and retcode != 0:\n            raise OSUtilError(\"Failed to eject dvd: ret={0}\".format(retcode))\n\n    def restart_if(self, ifname):\n        # Restart dhclient only to publish hostname\n        shellutil.run(\"/etc/rc.d/dhclient restart {0}\".format(ifname), chk_err=False)\n\n    def get_total_mem(self):\n        cmd = \"sysctl hw.physmem |awk '{print $2}'\"\n        ret, output = shellutil.run_get_output(cmd)\n        if ret:\n            raise OSUtilError(\"Failed to get total memory: {0}\".format(output))\n        try:\n            return int(output)/1024/1024\n        except ValueError:\n            raise OSUtilError(\"Failed to get total memory: {0}\".format(output))\n\n    def get_processor_cores(self):\n        ret, output = shellutil.run_get_output(\"sysctl hw.ncpu |awk '{print $2}'\")\n        if ret:\n            raise OSUtilError(\"Failed to get processor cores.\")\n\n        try:\n            return int(output)\n        except ValueError:\n            raise OSUtilError(\"Failed to get total memory: {0}\".format(output))\n\n    def set_scsi_disks_timeout(self, timeout):\n        if self._scsi_disks_timeout_set:\n            return\n\n        ret, output = shellutil.run_get_output('sysctl kern.cam.da.default_timeout={0}'.format(timeout))\n        if ret:\n            raise OSUtilError(\"Failed set SCSI disks timeout: {0}\".format(output))\n        self._scsi_disks_timeout_set = True\n\n    def check_pid_alive(self, pid):\n        return shellutil.run('ps -p {0}'.format(pid), chk_err=False) == 0\n\n    @staticmethod\n    def _get_net_info():\n        \"\"\"\n        There is no SIOCGIFCONF\n        on freeBSD - just parse ifconfig.\n        Returns strings: iface, inet4_addr, and mac\n        or 'None,None,None' if unable to parse.\n        We will sleep and retry as the network must be up.\n        \"\"\"\n        iface = ''\n        inet = ''\n        mac = ''\n\n        err, output = shellutil.run_get_output('ifconfig -l ether', chk_err=False)\n        if err:\n            raise OSUtilError(\"Can't find ether interface:{0}\".format(output))\n        ifaces = output.split()\n        if not ifaces:\n            raise OSUtilError(\"Can't find ether interface.\")\n        iface = ifaces[0]\n\n        err, output = shellutil.run_get_output('ifconfig ' + iface, chk_err=False)\n        if err:\n            raise OSUtilError(\"Can't get info for interface:{0}\".format(iface))\n\n        for line in output.split('\\n'):\n            if line.find('inet ') != -1:\n                inet = line.split()[1]\n            elif line.find('ether ') != -1:\n                mac = line.split()[1]\n        logger.verbose(\"Interface info: ({0},{1},{2})\", iface, inet, mac)\n\n        return iface, inet, mac\n\n    def device_for_ide_port(self, port_id):\n        \"\"\"\n        Return device name attached to ide port 'n'.\n        \"\"\"\n        if port_id > 3:\n            return None\n        g0 = \"00000000\"\n        if port_id > 1:\n            g0 = \"00000001\"\n            port_id = port_id - 2\n        err, output = shellutil.run_get_output('sysctl dev.storvsc | grep pnpinfo | grep deviceid=')\n        if err:\n            return None\n        g1 = \"000\" + ustr(port_id)\n        g0g1 = \"{0}-{1}\".format(g0, g1)\n        \"\"\"\n        search 'X' from 'dev.storvsc.X.%pnpinfo: classid=32412632-86cb-44a2-9b5c-50d1417354f5 deviceid=00000000-0001-8899-0000-000000000000'\n        \"\"\"\n        cmd_search_ide = \"sysctl dev.storvsc | grep pnpinfo | grep deviceid={0}\".format(g0g1)\n        err, output = shellutil.run_get_output(cmd_search_ide)\n        if err:\n            return None\n        cmd_extract_id = cmd_search_ide + \"|awk -F . '{print $3}'\"\n        err, output = shellutil.run_get_output(cmd_extract_id)\n        \"\"\"\n        try to search 'blkvscX' and 'storvscX' to find device name\n        \"\"\"\n        output = output.rstrip()\n        cmd_search_blkvsc = \"camcontrol devlist -b | grep blkvsc{0} | awk '{{print $1}}'\".format(output)\n        err, output = shellutil.run_get_output(cmd_search_blkvsc)\n        if err == 0:\n            output = output.rstrip()\n            cmd_search_dev=\"camcontrol devlist | grep {0} | awk -F \\( '{{print $2}}'|sed -e 's/.*(//'| sed -e 's/).*//'\".format(output)\n            err, output = shellutil.run_get_output(cmd_search_dev)\n            if err == 0:\n                for possible in output.rstrip().split(','):\n                    if not possible.startswith('pass'):\n                        return possible\n\n        cmd_search_storvsc = \"camcontrol devlist -b | grep storvsc{0} | awk '{{print $1}}'\".format(output)\n        err, output = shellutil.run_get_output(cmd_search_storvsc)\n        if err == 0:\n            output = output.rstrip()\n            cmd_search_dev=\"camcontrol devlist | grep {0} | awk -F \\( '{{print $2}}'|sed -e 's/.*(//'| sed -e 's/).*//'\".format(output)\n            err, output = shellutil.run_get_output(cmd_search_dev)\n            if err == 0:\n                for possible in output.rstrip().split(','):\n                    if not possible.startswith('pass'):\n                        return possible\n        return None\n\n    @staticmethod\n    def get_total_cpu_ticks_since_boot():\n        return 0\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/Azure/WALinuxAgent/blob/579416da57410c65042ab916183e78686d69913c",
        "file_path": "/azurelinuxagent/common/protocol/wire.py",
        "source": "# Microsoft Azure Linux Agent\n#\n# Copyright 2018 Microsoft Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Requires Python 2.6+ and Openssl 1.0+\n\nfrom datetime import datetime\n\nimport json\nimport os\nimport random\nimport re\nimport time\nimport xml.sax.saxutils as saxutils\n\nimport azurelinuxagent.common.conf as conf\nimport azurelinuxagent.common.utils.fileutil as fileutil\nimport azurelinuxagent.common.utils.textutil as textutil\n\nfrom azurelinuxagent.common.exception import ProtocolNotFoundError, \\\n                                            ResourceGoneError\nfrom azurelinuxagent.common.future import httpclient, bytebuffer\nfrom azurelinuxagent.common.protocol.hostplugin import HostPluginProtocol, URI_FORMAT_GET_EXTENSION_ARTIFACT, \\\n    HOST_PLUGIN_PORT\nfrom azurelinuxagent.common.protocol.restapi import *\nfrom azurelinuxagent.common.utils.archive import StateFlusher\nfrom azurelinuxagent.common.utils.cryptutil import CryptUtil\nfrom azurelinuxagent.common.utils.textutil import parse_doc, findall, find, \\\n    findtext, getattrib, gettext, remove_bom, get_bytes_from_pem, parse_json\nfrom azurelinuxagent.common.version import AGENT_NAME\n\nVERSION_INFO_URI = \"http://{0}/?comp=versions\"\nGOAL_STATE_URI = \"http://{0}/machine/?comp=goalstate\"\nHEALTH_REPORT_URI = \"http://{0}/machine?comp=health\"\nROLE_PROP_URI = \"http://{0}/machine?comp=roleProperties\"\nTELEMETRY_URI = \"http://{0}/machine?comp=telemetrydata\"\n\nWIRE_SERVER_ADDR_FILE_NAME = \"WireServer\"\nINCARNATION_FILE_NAME = \"Incarnation\"\nGOAL_STATE_FILE_NAME = \"GoalState.{0}.xml\"\nHOSTING_ENV_FILE_NAME = \"HostingEnvironmentConfig.xml\"\nSHARED_CONF_FILE_NAME = \"SharedConfig.xml\"\nCERTS_FILE_NAME = \"Certificates.xml\"\nP7M_FILE_NAME = \"Certificates.p7m\"\nPEM_FILE_NAME = \"Certificates.pem\"\nEXT_CONF_FILE_NAME = \"ExtensionsConfig.{0}.xml\"\nMANIFEST_FILE_NAME = \"{0}.{1}.manifest.xml\"\nAGENTS_MANIFEST_FILE_NAME = \"{0}.{1}.agentsManifest\"\nTRANSPORT_CERT_FILE_NAME = \"TransportCert.pem\"\nTRANSPORT_PRV_FILE_NAME = \"TransportPrivate.pem\"\n\nPROTOCOL_VERSION = \"2012-11-30\"\nENDPOINT_FINE_NAME = \"WireServer\"\n\nSHORT_WAITING_INTERVAL = 1  # 1 second\n\n\nclass UploadError(HttpError):\n    pass\n\n\nclass WireProtocol(Protocol):\n    \"\"\"Slim layer to adapt wire protocol data to metadata protocol interface\"\"\"\n\n    # TODO: Clean-up goal state processing\n    #  At present, some methods magically update GoalState (e.g.,\n    #  get_vmagent_manifests), others (e.g., get_vmagent_pkgs)\n    #  assume its presence. A better approach would make an explicit update\n    #  call that returns the incarnation number and\n    #  establishes that number the \"context\" for all other calls (either by\n    #  updating the internal state of the protocol or\n    #  by having callers pass the incarnation number to the method).\n\n    def __init__(self, endpoint):\n        if endpoint is None:\n            raise ProtocolError(\"WireProtocol endpoint is None\")\n        self.endpoint = endpoint\n        self.client = WireClient(self.endpoint)\n\n    def detect(self):\n        self.client.check_wire_protocol_version()\n\n        trans_prv_file = os.path.join(conf.get_lib_dir(),\n                                      TRANSPORT_PRV_FILE_NAME)\n        trans_cert_file = os.path.join(conf.get_lib_dir(),\n                                       TRANSPORT_CERT_FILE_NAME)\n        cryptutil = CryptUtil(conf.get_openssl_cmd())\n        cryptutil.gen_transport_cert(trans_prv_file, trans_cert_file)\n\n        self.update_goal_state(forced=True)\n\n    def update_goal_state(self, forced=False, max_retry=3):\n        self.client.update_goal_state(forced=forced, max_retry=max_retry)\n\n    def get_vminfo(self):\n        goal_state = self.client.get_goal_state()\n        hosting_env = self.client.get_hosting_env()\n\n        vminfo = VMInfo()\n        vminfo.subscriptionId = None\n        vminfo.vmName = hosting_env.vm_name\n        vminfo.tenantName = hosting_env.deployment_name\n        vminfo.roleName = hosting_env.role_name\n        vminfo.roleInstanceName = goal_state.role_instance_id\n        vminfo.containerId = goal_state.container_id\n        return vminfo\n\n    def get_certs(self):\n        certificates = self.client.get_certs()\n        return certificates.cert_list\n\n    def get_incarnation(self):\n        path = os.path.join(conf.get_lib_dir(), INCARNATION_FILE_NAME)\n        if os.path.exists(path):\n            return fileutil.read_file(path)\n        else:\n            return 0\n\n    def get_vmagent_manifests(self):\n        # Update goal state to get latest extensions config\n        self.update_goal_state()\n        goal_state = self.client.get_goal_state()\n        ext_conf = self.client.get_ext_conf()\n        return ext_conf.vmagent_manifests, goal_state.incarnation\n\n    def get_vmagent_pkgs(self, vmagent_manifest):\n        goal_state = self.client.get_goal_state()\n        ga_manifest = self.client.get_gafamily_manifest(vmagent_manifest, goal_state)\n        valid_pkg_list = self.client.filter_package_list(vmagent_manifest.family, ga_manifest, goal_state)\n        return valid_pkg_list\n\n    def get_ext_handlers(self):\n        logger.verbose(\"Get extension handler config\")\n        # Update goal state to get latest extensions config\n        self.update_goal_state()\n        goal_state = self.client.get_goal_state()\n        ext_conf = self.client.get_ext_conf()\n        # In wire protocol, incarnation is equivalent to ETag\n        return ext_conf.ext_handlers, goal_state.incarnation\n\n    def get_ext_handler_pkgs(self, ext_handler):\n        logger.verbose(\"Get extension handler package\")\n        goal_state = self.client.get_goal_state()\n        man = self.client.get_ext_manifest(ext_handler, goal_state)\n        return man.pkg_list\n\n    def get_artifacts_profile(self):\n        logger.verbose(\"Get In-VM Artifacts Profile\")\n        return self.client.get_artifacts_profile()\n\n    def download_ext_handler_pkg(self, uri, headers=None, use_proxy=True):\n        package = self.client.fetch(uri, headers=headers, use_proxy=use_proxy, decode=False)\n\n        if package is None:\n            logger.verbose(\"Download did not succeed, falling back to host plugin\")\n            host = self.client.get_host_plugin()\n            uri, headers = host.get_artifact_request(uri, host.manifest_uri)\n            package = self.client.fetch(uri, headers=headers, use_proxy=False, decode=False)\n\n        return package\n\n    def report_provision_status(self, provision_status):\n        validate_param(\"provision_status\", provision_status, ProvisionStatus)\n\n        if provision_status.status is not None:\n            self.client.report_health(provision_status.status,\n                                      provision_status.subStatus,\n                                      provision_status.description)\n        if provision_status.properties.certificateThumbprint is not None:\n            thumbprint = provision_status.properties.certificateThumbprint\n            self.client.report_role_prop(thumbprint)\n\n    def report_vm_status(self, vm_status):\n        validate_param(\"vm_status\", vm_status, VMStatus)\n        self.client.status_blob.set_vm_status(vm_status)\n        self.client.upload_status_blob()\n\n    def report_ext_status(self, ext_handler_name, ext_name, ext_status):\n        validate_param(\"ext_status\", ext_status, ExtensionStatus)\n        self.client.status_blob.set_ext_status(ext_handler_name, ext_status)\n\n    def report_event(self, events):\n        validate_param(\"events\", events, TelemetryEventList)\n        self.client.report_event(events)\n\n\ndef _build_role_properties(container_id, role_instance_id, thumbprint):\n    xml = (u\"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?>\"\n           u\"<RoleProperties>\"\n           u\"<Container>\"\n           u\"<ContainerId>{0}</ContainerId>\"\n           u\"<RoleInstances>\"\n           u\"<RoleInstance>\"\n           u\"<Id>{1}</Id>\"\n           u\"<Properties>\"\n           u\"<Property name=\\\"CertificateThumbprint\\\" value=\\\"{2}\\\" />\"\n           u\"</Properties>\"\n           u\"</RoleInstance>\"\n           u\"</RoleInstances>\"\n           u\"</Container>\"\n           u\"</RoleProperties>\"\n           u\"\").format(container_id, role_instance_id, thumbprint)\n    return xml\n\n\ndef _build_health_report(incarnation, container_id, role_instance_id,\n                         status, substatus, description):\n    # Escape '&', '<' and '>'\n    description = saxutils.escape(ustr(description))\n    detail = u''\n    if substatus is not None:\n        substatus = saxutils.escape(ustr(substatus))\n        detail = (u\"<Details>\"\n                  u\"<SubStatus>{0}</SubStatus>\"\n                  u\"<Description>{1}</Description>\"\n                  u\"</Details>\").format(substatus, description)\n    xml = (u\"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?>\"\n           u\"<Health \"\n           u\"xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\"\"\n           u\" xmlns:xsd=\\\"http://www.w3.org/2001/XMLSchema\\\">\"\n           u\"<GoalStateIncarnation>{0}</GoalStateIncarnation>\"\n           u\"<Container>\"\n           u\"<ContainerId>{1}</ContainerId>\"\n           u\"<RoleInstanceList>\"\n           u\"<Role>\"\n           u\"<InstanceId>{2}</InstanceId>\"\n           u\"<Health>\"\n           u\"<State>{3}</State>\"\n           u\"{4}\"\n           u\"</Health>\"\n           u\"</Role>\"\n           u\"</RoleInstanceList>\"\n           u\"</Container>\"\n           u\"</Health>\"\n           u\"\").format(incarnation,\n                       container_id,\n                       role_instance_id,\n                       status,\n                       detail)\n    return xml\n\n\ndef ga_status_to_guest_info(ga_status):\n    \"\"\"\n    Convert VMStatus object to status blob format\n    \"\"\"\n    v1_ga_guest_info = {\n        \"computerName\" : ga_status.hostname,\n        \"osName\" : ga_status.osname,\n        \"osVersion\" : ga_status.osversion,\n        \"version\" : ga_status.version,\n    }\n    return v1_ga_guest_info\n\n\ndef ga_status_to_v1(ga_status):\n    formatted_msg = {\n        'lang': 'en-US',\n        'message': ga_status.message\n    }\n    v1_ga_status = {\n        \"version\" : ga_status.version,\n        \"status\" : ga_status.status,\n        \"formattedMessage\" : formatted_msg\n    }\n    return v1_ga_status\n\n\ndef ext_substatus_to_v1(sub_status_list):\n    status_list = []\n    for substatus in sub_status_list:\n        status = {\n            \"name\": substatus.name,\n            \"status\": substatus.status,\n            \"code\": substatus.code,\n            \"formattedMessage\": {\n                \"lang\": \"en-US\",\n                \"message\": substatus.message\n            }\n        }\n        status_list.append(status)\n    return status_list\n\n\ndef ext_status_to_v1(ext_name, ext_status):\n    if ext_status is None:\n        return None\n    timestamp = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n    v1_sub_status = ext_substatus_to_v1(ext_status.substatusList)\n    v1_ext_status = {\n        \"status\": {\n            \"name\": ext_name,\n            \"configurationAppliedTime\": ext_status.configurationAppliedTime,\n            \"operation\": ext_status.operation,\n            \"status\": ext_status.status,\n            \"code\": ext_status.code,\n            \"formattedMessage\": {\n                \"lang\": \"en-US\",\n                \"message\": ext_status.message\n            }\n        },\n        \"version\": 1.0,\n        \"timestampUTC\": timestamp\n    }\n    if len(v1_sub_status) != 0:\n        v1_ext_status['status']['substatus'] = v1_sub_status\n    return v1_ext_status\n\n\ndef ext_handler_status_to_v1(handler_status, ext_statuses, timestamp):\n    v1_handler_status = {\n        'handlerVersion': handler_status.version,\n        'handlerName': handler_status.name,\n        'status': handler_status.status,\n        'code': handler_status.code\n    }\n    if handler_status.message is not None:\n        v1_handler_status[\"formattedMessage\"] = {\n            \"lang\": \"en-US\",\n            \"message\": handler_status.message\n        }\n\n    if handler_status.upgradeGuid is not None:\n        v1_handler_status[\"upgradeGuid\"] = handler_status.upgradeGuid\n\n    if len(handler_status.extensions) > 0:\n        # Currently, no more than one extension per handler\n        ext_name = handler_status.extensions[0]\n        ext_status = ext_statuses.get(ext_name)\n        v1_ext_status = ext_status_to_v1(ext_name, ext_status)\n        if ext_status is not None and v1_ext_status is not None:\n            v1_handler_status[\"runtimeSettingsStatus\"] = {\n                'settingsStatus': v1_ext_status,\n                'sequenceNumber': ext_status.sequenceNumber\n            }\n    return v1_handler_status\n\n\ndef vm_status_to_v1(vm_status, ext_statuses):\n    timestamp = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n\n    v1_ga_guest_info = ga_status_to_guest_info(vm_status.vmAgent)\n    v1_ga_status = ga_status_to_v1(vm_status.vmAgent)\n    v1_handler_status_list = []\n    for handler_status in vm_status.vmAgent.extensionHandlers:\n        v1_handler_status = ext_handler_status_to_v1(handler_status,\n                                                     ext_statuses, timestamp)\n        if v1_handler_status is not None:\n            v1_handler_status_list.append(v1_handler_status)\n\n    v1_agg_status = {\n        'guestAgentStatus': v1_ga_status,\n        'handlerAggregateStatus': v1_handler_status_list\n    }\n    v1_vm_status = {\n        'version': '1.1',\n        'timestampUTC': timestamp,\n        'aggregateStatus': v1_agg_status,\n        'guestOSInfo' : v1_ga_guest_info\n    }\n    return v1_vm_status\n\n\nclass StatusBlob(object):\n    def __init__(self, client):\n        self.vm_status = None\n        self.ext_statuses = {}\n        self.client = client\n        self.type = None\n        self.data = None\n\n    def set_vm_status(self, vm_status):\n        validate_param(\"vmAgent\", vm_status, VMStatus)\n        self.vm_status = vm_status\n\n    def set_ext_status(self, ext_handler_name, ext_status):\n        validate_param(\"extensionStatus\", ext_status, ExtensionStatus)\n        self.ext_statuses[ext_handler_name] = ext_status\n\n    def to_json(self):\n        report = vm_status_to_v1(self.vm_status, self.ext_statuses)\n        return json.dumps(report)\n\n    __storage_version__ = \"2014-02-14\"\n\n    def prepare(self, blob_type):\n        logger.verbose(\"Prepare status blob\")\n        self.data = self.to_json()\n        self.type = blob_type\n\n    def upload(self, url):\n        try:\n            if not self.type in [\"BlockBlob\", \"PageBlob\"]:\n                raise ProtocolError(\"Illegal blob type: {0}\".format(self.type))\n\n            if self.type == \"BlockBlob\":\n                self.put_block_blob(url, self.data)\n            else:\n                self.put_page_blob(url, self.data)\n            return True\n\n        except Exception as e:\n            logger.verbose(\"Initial status upload failed: {0}\", e)\n\n        return False\n\n    def get_block_blob_headers(self, blob_size):\n        return {\n            \"Content-Length\": ustr(blob_size),\n            \"x-ms-blob-type\": \"BlockBlob\",\n            \"x-ms-date\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n            \"x-ms-version\": self.__class__.__storage_version__\n        }\n\n    def put_block_blob(self, url, data):\n        logger.verbose(\"Put block blob\")\n        headers = self.get_block_blob_headers(len(data))\n        resp = self.client.call_storage_service(restutil.http_put, url, data, headers)\n        if resp.status != httpclient.CREATED:\n            raise UploadError(\n                \"Failed to upload block blob: {0}\".format(resp.status))\n\n    def get_page_blob_create_headers(self, blob_size):\n        return {\n            \"Content-Length\": \"0\",\n            \"x-ms-blob-content-length\": ustr(blob_size),\n            \"x-ms-blob-type\": \"PageBlob\",\n            \"x-ms-date\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n            \"x-ms-version\": self.__class__.__storage_version__\n        }\n\n    def get_page_blob_page_headers(self, start, end):\n        return {\n            \"Content-Length\": ustr(end - start),\n            \"x-ms-date\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n            \"x-ms-range\": \"bytes={0}-{1}\".format(start, end - 1),\n            \"x-ms-page-write\": \"update\",\n            \"x-ms-version\": self.__class__.__storage_version__\n        }\n\n    def put_page_blob(self, url, data):\n        logger.verbose(\"Put page blob\")\n\n        # Convert string into bytes and align to 512 bytes\n        data = bytearray(data, encoding='utf-8')\n        page_blob_size = int((len(data) + 511) / 512) * 512\n\n        headers = self.get_page_blob_create_headers(page_blob_size)\n        resp = self.client.call_storage_service(restutil.http_put, url, \"\", headers)\n        if resp.status != httpclient.CREATED:\n            raise UploadError(\n                \"Failed to clean up page blob: {0}\".format(resp.status))\n\n        if url.count(\"?\") <= 0:\n            url = \"{0}?comp=page\".format(url)\n        else:\n            url = \"{0}&comp=page\".format(url)\n\n        logger.verbose(\"Upload page blob\")\n        page_max = 4 * 1024 * 1024  # Max page size: 4MB\n        start = 0\n        end = 0\n        while end < len(data):\n            end = min(len(data), start + page_max)\n            content_size = end - start\n            # Align to 512 bytes\n            page_end = int((end + 511) / 512) * 512\n            buf_size = page_end - start\n            buf = bytearray(buf_size)\n            buf[0: content_size] = data[start: end]\n            headers = self.get_page_blob_page_headers(start, page_end)\n            resp = self.client.call_storage_service(\n                restutil.http_put,\n                url,\n                bytebuffer(buf),\n                headers)\n            if resp is None or resp.status != httpclient.CREATED:\n                raise UploadError(\n                    \"Failed to upload page blob: {0}\".format(resp.status))\n            start = end\n\n\ndef event_param_to_v1(param):\n    param_format = '<Param Name=\"{0}\" Value={1} T=\"{2}\" />'\n    param_type = type(param.value)\n    attr_type = \"\"\n    if param_type is int:\n        attr_type = 'mt:uint64'\n    elif param_type is str:\n        attr_type = 'mt:wstr'\n    elif ustr(param_type).count(\"'unicode'\") > 0:\n        attr_type = 'mt:wstr'\n    elif param_type is bool:\n        attr_type = 'mt:bool'\n    elif param_type is float:\n        attr_type = 'mt:float64'\n    return param_format.format(param.name,\n                               saxutils.quoteattr(ustr(param.value)),\n                               attr_type)\n\n\ndef event_to_v1(event):\n    params = \"\"\n    for param in event.parameters:\n        params += event_param_to_v1(param)\n    event_str = ('<Event id=\"{0}\">'\n                 '<![CDATA[{1}]]>'\n                 '</Event>').format(event.eventId, params)\n    return event_str\n\n\nclass WireClient(object):\n    def __init__(self, endpoint):\n        logger.info(\"Wire server endpoint:{0}\", endpoint)\n        self.endpoint = endpoint\n        self.goal_state = None\n        self.updated = None\n        self.hosting_env = None\n        self.shared_conf = None\n        self.certs = None\n        self.ext_conf = None\n        self.host_plugin = None\n        self.status_blob = StatusBlob(self)\n        self.goal_state_flusher = StateFlusher(conf.get_lib_dir())\n\n    def call_wireserver(self, http_req, *args, **kwargs):\n        try:\n            # Never use the HTTP proxy for wireserver\n            kwargs['use_proxy'] = False\n            resp = http_req(*args, **kwargs)\n\n            if restutil.request_failed(resp):\n                msg = \"[Wireserver Failed] URI {0} \".format(args[0])\n                if resp is not None:\n                    msg += \" [HTTP Failed] Status Code {0}\".format(resp.status)\n                raise ProtocolError(msg)\n\n        # If the GoalState is stale, pass along the exception to the caller\n        except ResourceGoneError:\n            raise\n\n        except Exception as e:\n            raise ProtocolError(\"[Wireserver Exception] {0}\".format(\n                ustr(e)))\n\n        return resp\n\n    def decode_config(self, data):\n        if data is None:\n            return None\n        data = remove_bom(data)\n        xml_text = ustr(data, encoding='utf-8')\n        return xml_text\n\n    def fetch_config(self, uri, headers):\n        resp = self.call_wireserver(restutil.http_get,\n                                    uri,\n                                    headers=headers)\n        return self.decode_config(resp.read())\n\n    def fetch_cache(self, local_file):\n        if not os.path.isfile(local_file):\n            raise ProtocolError(\"{0} is missing.\".format(local_file))\n        try:\n            return fileutil.read_file(local_file)\n        except IOError as e:\n            raise ProtocolError(\"Failed to read cache: {0}\".format(e))\n\n    def save_cache(self, local_file, data):\n        try:\n            fileutil.write_file(local_file, data)\n        except IOError as e:\n            fileutil.clean_ioerror(e,\n                paths=[local_file])\n            raise ProtocolError(\"Failed to write cache: {0}\".format(e))\n\n    @staticmethod\n    def call_storage_service(http_req, *args, **kwargs):\n        # Default to use the configured HTTP proxy\n        if not 'use_proxy' in kwargs or kwargs['use_proxy'] is None:\n            kwargs['use_proxy'] = True\n\n        return http_req(*args, **kwargs)\n\n    def fetch_manifest(self, version_uris):\n        logger.verbose(\"Fetch manifest\")\n        version_uris_shuffled = version_uris\n        random.shuffle(version_uris_shuffled)\n\n        for version in version_uris_shuffled:\n            # GA expects a location and failoverLocation in ExtensionsConfig, but\n            # this is not always the case. See #1147.\n            if version.uri is None:\n                logger.verbose('The specified manifest URL is empty, ignored.')\n                continue\n\n            response = None\n            if not HostPluginProtocol.is_default_channel():\n                response = self.fetch(version.uri)\n\n            if not response:\n                if HostPluginProtocol.is_default_channel():\n                    logger.verbose(\"Using host plugin as default channel\")\n                else:\n                    logger.verbose(\"Failed to download manifest, \"\n                                   \"switching to host plugin\")\n\n                try:\n                    host = self.get_host_plugin()\n                    uri, headers = host.get_artifact_request(version.uri)\n                    response = self.fetch(uri, headers, use_proxy=False)\n\n                # If the HostPlugin rejects the request,\n                # let the error continue, but set to use the HostPlugin\n                except ResourceGoneError:\n                    HostPluginProtocol.set_default_channel(True)\n                    raise\n\n                host.manifest_uri = version.uri\n                logger.verbose(\"Manifest downloaded successfully from host plugin\")\n                if not HostPluginProtocol.is_default_channel():\n                    logger.info(\"Setting host plugin as default channel\")\n                    HostPluginProtocol.set_default_channel(True)\n\n            if response:\n                return response\n\n        raise ProtocolError(\"Failed to fetch manifest from all sources\")\n\n    def fetch(self, uri, headers=None, use_proxy=None, decode=True):\n        content = None\n        logger.verbose(\"Fetch [{0}] with headers [{1}]\", uri, headers)\n        try:\n            resp = self.call_storage_service(\n                        restutil.http_get,\n                        uri,\n                        headers=headers,\n                        use_proxy=use_proxy)\n\n            if restutil.request_failed(resp):\n                error_response = restutil.read_response_error(resp)\n                msg = \"Fetch failed from [{0}]: {1}\".format(uri, error_response)\n                logger.warn(msg)\n                if self.host_plugin is not None:\n                    self.host_plugin.report_fetch_health(uri,\n                                                         is_healthy=not restutil.request_failed_at_hostplugin(resp),\n                                                         source='WireClient',\n                                                         response=error_response)\n                raise ProtocolError(msg)\n            else:\n                response_content = resp.read()\n                content = self.decode_config(response_content) if decode else response_content\n                if self.host_plugin is not None:\n                    self.host_plugin.report_fetch_health(uri, source='WireClient')\n\n        except (HttpError, ProtocolError, IOError) as e:\n            logger.verbose(\"Fetch failed from [{0}]: {1}\", uri, e)\n            if isinstance(e, ResourceGoneError):\n                raise\n\n        return content\n\n    def update_hosting_env(self, goal_state):\n        if goal_state.hosting_env_uri is None:\n            raise ProtocolError(\"HostingEnvironmentConfig uri is empty\")\n        local_file = os.path.join(conf.get_lib_dir(), HOSTING_ENV_FILE_NAME)\n        xml_text = self.fetch_config(goal_state.hosting_env_uri,\n                                     self.get_header())\n        self.save_cache(local_file, xml_text)\n        self.hosting_env = HostingEnv(xml_text)\n\n    def update_shared_conf(self, goal_state):\n        if goal_state.shared_conf_uri is None:\n            raise ProtocolError(\"SharedConfig uri is empty\")\n        local_file = os.path.join(conf.get_lib_dir(), SHARED_CONF_FILE_NAME)\n        xml_text = self.fetch_config(goal_state.shared_conf_uri,\n                                     self.get_header())\n        self.save_cache(local_file, xml_text)\n        self.shared_conf = SharedConfig(xml_text)\n\n    def update_certs(self, goal_state):\n        if goal_state.certs_uri is None:\n            return\n        local_file = os.path.join(conf.get_lib_dir(), CERTS_FILE_NAME)\n        xml_text = self.fetch_config(goal_state.certs_uri,\n                                     self.get_header_for_cert())\n        self.save_cache(local_file, xml_text)\n        self.certs = Certificates(self, xml_text)\n\n    def update_ext_conf(self, goal_state):\n        if goal_state.ext_uri is None:\n            logger.info(\"ExtensionsConfig.xml uri is empty\")\n            self.ext_conf = ExtensionsConfig(None)\n            return\n        incarnation = goal_state.incarnation\n        local_file = os.path.join(conf.get_lib_dir(),\n                                  EXT_CONF_FILE_NAME.format(incarnation))\n        xml_text = self.fetch_config(goal_state.ext_uri, self.get_header())\n        self.save_cache(local_file, xml_text)\n        self.ext_conf = ExtensionsConfig(xml_text)\n\n    def update_goal_state(self, forced=False, max_retry=3):\n        incarnation_file = os.path.join(conf.get_lib_dir(),\n                                        INCARNATION_FILE_NAME)\n        uri = GOAL_STATE_URI.format(self.endpoint)\n\n        goal_state = None\n        for retry in range(0, max_retry):\n            try:\n                if goal_state is None:\n                    xml_text = self.fetch_config(uri, self.get_header())\n                    goal_state = GoalState(xml_text)\n\n                    if not forced:\n                        last_incarnation = None\n                        if os.path.isfile(incarnation_file):\n                            last_incarnation = fileutil.read_file(\n                                                    incarnation_file)\n                        new_incarnation = goal_state.incarnation\n                        if last_incarnation is not None and \\\n                                        last_incarnation == new_incarnation:\n                            # Goalstate is not updated.\n                            return\n\n                self.goal_state_flusher.flush(datetime.utcnow())\n\n                self.goal_state = goal_state\n                file_name = GOAL_STATE_FILE_NAME.format(goal_state.incarnation)\n                goal_state_file = os.path.join(conf.get_lib_dir(), file_name)\n                self.save_cache(goal_state_file, xml_text)\n                self.update_hosting_env(goal_state)\n                self.update_shared_conf(goal_state)\n                self.update_certs(goal_state)\n                self.update_ext_conf(goal_state)\n                self.save_cache(incarnation_file, goal_state.incarnation)\n\n                if self.host_plugin is not None:\n                    self.host_plugin.container_id = goal_state.container_id\n                    self.host_plugin.role_config_name = goal_state.role_config_name\n\n                return\n\n            except ResourceGoneError:\n                logger.info(\"GoalState is stale -- re-fetching\")\n                goal_state = None\n\n            except Exception as e:\n                log_method = logger.info \\\n                                if type(e) is ProtocolError \\\n                                else logger.warn\n                log_method(\n                    \"Exception processing GoalState-related files: {0}\".format(\n                        ustr(e)))\n\n                if retry < max_retry-1:\n                    continue\n                raise\n\n        raise ProtocolError(\"Exceeded max retry updating goal state\")\n\n    def get_goal_state(self):\n        if self.goal_state is None:\n            incarnation_file = os.path.join(conf.get_lib_dir(),\n                                            INCARNATION_FILE_NAME)\n            incarnation = self.fetch_cache(incarnation_file)\n\n            file_name = GOAL_STATE_FILE_NAME.format(incarnation)\n            goal_state_file = os.path.join(conf.get_lib_dir(), file_name)\n            xml_text = self.fetch_cache(goal_state_file)\n            self.goal_state = GoalState(xml_text)\n        return self.goal_state\n\n    def get_hosting_env(self):\n        if self.hosting_env is None:\n            local_file = os.path.join(conf.get_lib_dir(),\n                                      HOSTING_ENV_FILE_NAME)\n            xml_text = self.fetch_cache(local_file)\n            self.hosting_env = HostingEnv(xml_text)\n        return self.hosting_env\n\n    def get_shared_conf(self):\n        if self.shared_conf is None:\n            local_file = os.path.join(conf.get_lib_dir(),\n                                      SHARED_CONF_FILE_NAME)\n            xml_text = self.fetch_cache(local_file)\n            self.shared_conf = SharedConfig(xml_text)\n        return self.shared_conf\n\n    def get_certs(self):\n        if self.certs is None:\n            local_file = os.path.join(conf.get_lib_dir(), CERTS_FILE_NAME)\n            xml_text = self.fetch_cache(local_file)\n            self.certs = Certificates(self, xml_text)\n        if self.certs is None:\n            return None\n        return self.certs\n\n    def get_ext_conf(self):\n        if self.ext_conf is None:\n            goal_state = self.get_goal_state()\n            if goal_state.ext_uri is None:\n                self.ext_conf = ExtensionsConfig(None)\n            else:\n                local_file = EXT_CONF_FILE_NAME.format(goal_state.incarnation)\n                local_file = os.path.join(conf.get_lib_dir(), local_file)\n                xml_text = self.fetch_cache(local_file)\n                self.ext_conf = ExtensionsConfig(xml_text)\n        return self.ext_conf\n\n    def get_ext_manifest(self, ext_handler, goal_state):\n        for update_goal_state in [False, True]:\n            try:\n                if update_goal_state:\n                    self.update_goal_state(forced=True)\n                    goal_state = self.get_goal_state()\n\n                local_file = MANIFEST_FILE_NAME.format(\n                                ext_handler.name,\n                                goal_state.incarnation)\n                local_file = os.path.join(conf.get_lib_dir(), local_file)\n                xml_text = self.fetch_manifest(ext_handler.versionUris)\n                self.save_cache(local_file, xml_text)\n                return ExtensionManifest(xml_text)\n\n            except ResourceGoneError:\n                continue\n\n        raise ProtocolError(\"Failed to retrieve extension manifest\")\n\n    def filter_package_list(self, family, ga_manifest, goal_state):\n        complete_list = ga_manifest.pkg_list\n        agent_manifest = os.path.join(conf.get_lib_dir(),\n                                      AGENTS_MANIFEST_FILE_NAME.format(\n                                          family,\n                                          goal_state.incarnation))\n\n        if not os.path.exists(agent_manifest):\n            # clear memory cache\n            ga_manifest.allowed_versions = None\n\n            # create disk cache\n            with open(agent_manifest, mode='w') as manifest_fh:\n                for version in complete_list.versions:\n                    manifest_fh.write('{0}\\n'.format(version.version))\n            fileutil.chmod(agent_manifest, 0o644)\n\n            return complete_list\n\n        else:\n            # use allowed versions from cache, otherwise from disk\n            if ga_manifest.allowed_versions is None:\n                with open(agent_manifest, mode='r') as manifest_fh:\n                    ga_manifest.allowed_versions = [v.strip('\\n') for v\n                                                    in manifest_fh.readlines()]\n\n            # use the updated manifest urls for allowed versions\n            allowed_list = ExtHandlerPackageList()\n            allowed_list.versions = [version for version\n                                     in complete_list.versions\n                                     if version.version\n                                     in ga_manifest.allowed_versions]\n\n            return allowed_list\n\n    def get_gafamily_manifest(self, vmagent_manifest, goal_state):\n        for update_goal_state in [False, True]:\n            try:\n                if update_goal_state:\n                    self.update_goal_state(forced=True)\n                    goal_state = self.get_goal_state()\n\n                self._remove_stale_agent_manifest(\n                    vmagent_manifest.family,\n                    goal_state.incarnation)\n\n                local_file = MANIFEST_FILE_NAME.format(\n                                vmagent_manifest.family,\n                                goal_state.incarnation)\n                local_file = os.path.join(conf.get_lib_dir(), local_file)\n                xml_text = self.fetch_manifest(\n                            vmagent_manifest.versionsManifestUris)\n                fileutil.write_file(local_file, xml_text)\n                return ExtensionManifest(xml_text)\n\n            except ResourceGoneError:\n                continue\n\n        raise ProtocolError(\"Failed to retrieve GAFamily manifest\")\n\n    def _remove_stale_agent_manifest(self, family, incarnation):\n        \"\"\"\n        The incarnation number can reset at any time, which means there\n        could be a stale agentsManifest on disk.  Stale files are cleaned\n        on demand as new goal states arrive from WireServer. If the stale\n        file is not removed agent upgrade may be delayed.\n\n        :param family: GA family, e.g. Prod or Test\n        :param incarnation: incarnation of the current goal state\n        \"\"\"\n        fn = AGENTS_MANIFEST_FILE_NAME.format(\n            family,\n            incarnation)\n\n        agent_manifest = os.path.join(conf.get_lib_dir(), fn)\n\n        if os.path.exists(agent_manifest):\n            os.unlink(agent_manifest)\n\n    def check_wire_protocol_version(self):\n        uri = VERSION_INFO_URI.format(self.endpoint)\n        version_info_xml = self.fetch_config(uri, None)\n        version_info = VersionInfo(version_info_xml)\n\n        preferred = version_info.get_preferred()\n        if PROTOCOL_VERSION == preferred:\n            logger.info(\"Wire protocol version:{0}\", PROTOCOL_VERSION)\n        elif PROTOCOL_VERSION in version_info.get_supported():\n            logger.info(\"Wire protocol version:{0}\", PROTOCOL_VERSION)\n            logger.info(\"Server preferred version:{0}\", preferred)\n        else:\n            error = (\"Agent supported wire protocol version: {0} was not \"\n                     \"advised by Fabric.\").format(PROTOCOL_VERSION)\n            raise ProtocolNotFoundError(error)\n\n    def upload_status_blob(self):\n        try:\n            self.update_goal_state()\n            ext_conf = self.get_ext_conf()\n\n            blob_uri = ext_conf.status_upload_blob\n            blob_type = ext_conf.status_upload_blob_type\n\n            if blob_uri is None:\n                raise ProtocolError(\"No blob uri found\")\n\n            if blob_type not in [\"BlockBlob\", \"PageBlob\"]:\n                blob_type = \"BlockBlob\"\n                logger.verbose(\"Status Blob type is unspecified, assuming BlockBlob\")\n\n            try:\n                self.status_blob.prepare(blob_type)\n            except Exception as e:\n                raise ProtocolError(\"Exception creating status blob: {0}\", ustr(e))\n\n            # Swap the order of use for the HostPlugin vs. the \"direct\" route.\n            # Prefer the use of HostPlugin. If HostPlugin fails fall back to the\n            # direct route.\n            #\n            # The code previously preferred the \"direct\" route always, and only fell back\n            # to the HostPlugin *if* there was an error.  We would like to move to\n            # the HostPlugin for all traffic, but this is a big change.  We would like\n            # to see how this behaves at scale, and have a fallback should things go\n            # wrong.  This is why we try HostPlugin then direct.\n            try:\n                host = self.get_host_plugin()\n                host.put_vm_status(self.status_blob,\n                                   ext_conf.status_upload_blob,\n                                   ext_conf.status_upload_blob_type)\n                return\n            except ResourceGoneError:\n                # do not attempt direct, force goal state update and wait to try again\n                self.update_goal_state(forced=True)\n                return\n            except Exception as e:\n                # for all other errors, fall back to direct\n                pass\n\n            self.report_status_event(\"direct\")\n            if self.status_blob.upload(blob_uri):\n                return\n\n        except Exception as e:\n            self.report_status_event(\"Exception uploading status blob: {0}\", ustr(e))\n\n        raise ProtocolError(\"Failed to upload status blob via either channel\")\n\n    def report_role_prop(self, thumbprint):\n        goal_state = self.get_goal_state()\n        role_prop = _build_role_properties(goal_state.container_id,\n                                           goal_state.role_instance_id,\n                                           thumbprint)\n        role_prop = role_prop.encode(\"utf-8\")\n        role_prop_uri = ROLE_PROP_URI.format(self.endpoint)\n        headers = self.get_header_for_xml_content()\n        try:\n            resp = self.call_wireserver(restutil.http_post,\n                                        role_prop_uri,\n                                        role_prop,\n                                        headers=headers)\n        except HttpError as e:\n            raise ProtocolError((u\"Failed to send role properties: \"\n                                 u\"{0}\").format(e))\n        if resp.status != httpclient.ACCEPTED:\n            raise ProtocolError((u\"Failed to send role properties: \"\n                                 u\",{0}: {1}\").format(resp.status,\n                                                      resp.read()))\n\n    def report_health(self, status, substatus, description):\n        goal_state = self.get_goal_state()\n        health_report = _build_health_report(goal_state.incarnation,\n                                             goal_state.container_id,\n                                             goal_state.role_instance_id,\n                                             status,\n                                             substatus,\n                                             description)\n        health_report = health_report.encode(\"utf-8\")\n        health_report_uri = HEALTH_REPORT_URI.format(self.endpoint)\n        headers = self.get_header_for_xml_content()\n        try:\n            # 30 retries with 10s sleep gives ~5min for wireserver updates;\n            # this is retried 3 times with 15s sleep before throwing a\n            # ProtocolError, for a total of ~15min.\n            resp = self.call_wireserver(restutil.http_post,\n                                        health_report_uri,\n                                        health_report,\n                                        headers=headers,\n                                        max_retry=30,\n                                        retry_delay=15)\n        except HttpError as e:\n            raise ProtocolError((u\"Failed to send provision status: \"\n                                 u\"{0}\").format(e))\n        if restutil.request_failed(resp):\n            raise ProtocolError((u\"Failed to send provision status: \"\n                                 u\",{0}: {1}\").format(resp.status,\n                                                      resp.read()))\n\n    def send_event(self, provider_id, event_str):\n        uri = TELEMETRY_URI.format(self.endpoint)\n        data_format = ('<?xml version=\"1.0\"?>'\n                       '<TelemetryData version=\"1.0\">'\n                       '<Provider id=\"{0}\">{1}'\n                       '</Provider>'\n                       '</TelemetryData>')\n        data = data_format.format(provider_id, event_str)\n        try:\n            header = self.get_header_for_xml_content()\n            resp = self.call_wireserver(restutil.http_post, uri, data, header)\n        except HttpError as e:\n            raise ProtocolError(\"Failed to send events:{0}\".format(e))\n\n        if restutil.request_failed(resp):\n            logger.verbose(resp.read())\n            raise ProtocolError(\n                \"Failed to send events:{0}\".format(resp.status))\n\n    def report_event(self, event_list):\n        buf = {}\n        # Group events by providerId\n        for event in event_list.events:\n            if event.providerId not in buf:\n                buf[event.providerId] = \"\"\n            event_str = event_to_v1(event)\n            if len(event_str) >= 63 * 1024:\n                logger.warn(\"Single event too large: {0}\", event_str[300:])\n                continue\n            if len(buf[event.providerId] + event_str) >= 63 * 1024:\n                self.send_event(event.providerId, buf[event.providerId])\n                buf[event.providerId] = \"\"\n            buf[event.providerId] = buf[event.providerId] + event_str\n\n        # Send out all events left in buffer.\n        for provider_id in list(buf.keys()):\n            if len(buf[provider_id]) > 0:\n                self.send_event(provider_id, buf[provider_id])\n\n    def report_status_event(self, message, *args):\n        from azurelinuxagent.common.event import report_event, \\\n                WALAEventOperation\n\n        message = message.format(*args)\n        logger.warn(message)\n        report_event(op=WALAEventOperation.ReportStatus,\n                    is_success=False,\n                    message=message)\n\n    def get_header(self):\n        return {\n            \"x-ms-agent-name\": \"WALinuxAgent\",\n            \"x-ms-version\": PROTOCOL_VERSION\n        }\n\n    def get_header_for_xml_content(self):\n        return {\n            \"x-ms-agent-name\": \"WALinuxAgent\",\n            \"x-ms-version\": PROTOCOL_VERSION,\n            \"Content-Type\": \"text/xml;charset=utf-8\"\n        }\n\n    def get_header_for_cert(self):\n        trans_cert_file = os.path.join(conf.get_lib_dir(),\n                                       TRANSPORT_CERT_FILE_NAME)\n        content = self.fetch_cache(trans_cert_file)\n        cert = get_bytes_from_pem(content)\n        return {\n            \"x-ms-agent-name\": \"WALinuxAgent\",\n            \"x-ms-version\": PROTOCOL_VERSION,\n            \"x-ms-cipher-name\": \"DES_EDE3_CBC\",\n            \"x-ms-guest-agent-public-x509-cert\": cert\n        }\n\n    def get_host_plugin(self):\n        if self.host_plugin is None:\n            goal_state = self.get_goal_state()\n            self.host_plugin = HostPluginProtocol(self.endpoint,\n                                                  goal_state.container_id,\n                                                  goal_state.role_config_name)\n        return self.host_plugin\n\n    def has_artifacts_profile_blob(self):\n        return self.ext_conf and not \\\n               textutil.is_str_none_or_whitespace(self.ext_conf.artifacts_profile_blob)\n\n    def get_artifacts_profile(self):\n        artifacts_profile = None\n        for update_goal_state in [False, True]:\n            try:\n                if update_goal_state:\n                    self.update_goal_state(forced=True)\n\n                if self.has_artifacts_profile_blob():\n                    blob = self.ext_conf.artifacts_profile_blob\n\n                    profile = None\n                    if not HostPluginProtocol.is_default_channel():\n                        logger.verbose(\"Retrieving the artifacts profile\")\n                        profile = self.fetch(blob)\n\n                    if profile is None:\n                        if HostPluginProtocol.is_default_channel():\n                            logger.verbose(\"Using host plugin as default channel\")\n                        else:\n                            logger.verbose(\"Failed to download artifacts profile, \"\n                                           \"switching to host plugin\")\n\n                        host = self.get_host_plugin()\n                        uri, headers = host.get_artifact_request(blob)\n                        profile = self.fetch(uri, headers, use_proxy=False)\n\n                    if not textutil.is_str_none_or_whitespace(profile):\n                        logger.verbose(\"Artifacts profile downloaded\")\n                        artifacts_profile = InVMArtifactsProfile(profile)\n\n                return artifacts_profile\n\n            except ResourceGoneError:\n                HostPluginProtocol.set_default_channel(True)\n                continue\n\n            except Exception as e:\n                logger.warn(\n                    \"Exception retrieving artifacts profile: {0}\".format(\n                        ustr(e)))\n\n        return None\n\n\nclass VersionInfo(object):\n    def __init__(self, xml_text):\n        \"\"\"\n        Query endpoint server for wire protocol version.\n        Fail if our desired protocol version is not seen.\n        \"\"\"\n        logger.verbose(\"Load Version.xml\")\n        self.parse(xml_text)\n\n    def parse(self, xml_text):\n        xml_doc = parse_doc(xml_text)\n        preferred = find(xml_doc, \"Preferred\")\n        self.preferred = findtext(preferred, \"Version\")\n        logger.info(\"Fabric preferred wire protocol version:{0}\",\n                    self.preferred)\n\n        self.supported = []\n        supported = find(xml_doc, \"Supported\")\n        supported_version = findall(supported, \"Version\")\n        for node in supported_version:\n            version = gettext(node)\n            logger.verbose(\"Fabric supported wire protocol version:{0}\",\n                           version)\n            self.supported.append(version)\n\n    def get_preferred(self):\n        return self.preferred\n\n    def get_supported(self):\n        return self.supported\n\n\nclass GoalState(object):\n    def __init__(self, xml_text):\n        if xml_text is None:\n            raise ValueError(\"GoalState.xml is None\")\n        logger.verbose(\"Load GoalState.xml\")\n        self.incarnation = None\n        self.expected_state = None\n        self.hosting_env_uri = None\n        self.shared_conf_uri = None\n        self.certs_uri = None\n        self.ext_uri = None\n        self.role_instance_id = None\n        self.role_config_name = None\n        self.container_id = None\n        self.load_balancer_probe_port = None\n        self.xml_text = None\n        self.parse(xml_text)\n\n    def parse(self, xml_text):\n        \"\"\"\n        Request configuration data from endpoint server.\n        \"\"\"\n        self.xml_text = xml_text\n        xml_doc = parse_doc(xml_text)\n        self.incarnation = findtext(xml_doc, \"Incarnation\")\n        self.expected_state = findtext(xml_doc, \"ExpectedState\")\n        self.hosting_env_uri = findtext(xml_doc, \"HostingEnvironmentConfig\")\n        self.shared_conf_uri = findtext(xml_doc, \"SharedConfig\")\n        self.certs_uri = findtext(xml_doc, \"Certificates\")\n        self.ext_uri = findtext(xml_doc, \"ExtensionsConfig\")\n        role_instance = find(xml_doc, \"RoleInstance\")\n        self.role_instance_id = findtext(role_instance, \"InstanceId\")\n        role_config = find(role_instance, \"Configuration\")\n        self.role_config_name = findtext(role_config, \"ConfigName\")\n        container = find(xml_doc, \"Container\")\n        self.container_id = findtext(container, \"ContainerId\")\n        lbprobe_ports = find(xml_doc, \"LBProbePorts\")\n        self.load_balancer_probe_port = findtext(lbprobe_ports, \"Port\")\n        return self\n\n\nclass HostingEnv(object):\n    \"\"\"\n    parse Hosting enviromnet config and store in\n    HostingEnvironmentConfig.xml\n    \"\"\"\n\n    def __init__(self, xml_text):\n        if xml_text is None:\n            raise ValueError(\"HostingEnvironmentConfig.xml is None\")\n        logger.verbose(\"Load HostingEnvironmentConfig.xml\")\n        self.vm_name = None\n        self.role_name = None\n        self.deployment_name = None\n        self.xml_text = None\n        self.parse(xml_text)\n\n    def parse(self, xml_text):\n        \"\"\"\n        parse and create HostingEnvironmentConfig.xml.\n        \"\"\"\n        self.xml_text = xml_text\n        xml_doc = parse_doc(xml_text)\n        incarnation = find(xml_doc, \"Incarnation\")\n        self.vm_name = getattrib(incarnation, \"instance\")\n        role = find(xml_doc, \"Role\")\n        self.role_name = getattrib(role, \"name\")\n        deployment = find(xml_doc, \"Deployment\")\n        self.deployment_name = getattrib(deployment, \"name\")\n        return self\n\n\nclass SharedConfig(object):\n    \"\"\"\n    parse role endpoint server and goal state config.\n    \"\"\"\n\n    def __init__(self, xml_text):\n        logger.verbose(\"Load SharedConfig.xml\")\n        self.parse(xml_text)\n\n    def parse(self, xml_text):\n        \"\"\"\n        parse and write configuration to file SharedConfig.xml.\n        \"\"\"\n        # Not used currently\n        return self\n\n\nclass Certificates(object):\n    \"\"\"\n    Object containing certificates of host and provisioned user.\n    \"\"\"\n\n    def __init__(self, client, xml_text):\n        logger.verbose(\"Load Certificates.xml\")\n        self.client = client\n        self.cert_list = CertList()\n        self.parse(xml_text)\n\n    def parse(self, xml_text):\n        \"\"\"\n        Parse multiple certificates into seperate files.\n        \"\"\"\n        xml_doc = parse_doc(xml_text)\n        data = findtext(xml_doc, \"Data\")\n        if data is None:\n            return\n\n        cryptutil = CryptUtil(conf.get_openssl_cmd())\n        p7m_file = os.path.join(conf.get_lib_dir(), P7M_FILE_NAME)\n        p7m = (\"MIME-Version:1.0\\n\"\n               \"Content-Disposition: attachment; filename=\\\"{0}\\\"\\n\"\n               \"Content-Type: application/x-pkcs7-mime; name=\\\"{1}\\\"\\n\"\n               \"Content-Transfer-Encoding: base64\\n\"\n               \"\\n\"\n               \"{2}\").format(p7m_file, p7m_file, data)\n\n        self.client.save_cache(p7m_file, p7m)\n\n        trans_prv_file = os.path.join(conf.get_lib_dir(),\n                                      TRANSPORT_PRV_FILE_NAME)\n        trans_cert_file = os.path.join(conf.get_lib_dir(),\n                                       TRANSPORT_CERT_FILE_NAME)\n        pem_file = os.path.join(conf.get_lib_dir(), PEM_FILE_NAME)\n        # decrypt certificates\n        cryptutil.decrypt_p7m(p7m_file, trans_prv_file, trans_cert_file,\n                              pem_file)\n\n        # The parsing process use public key to match prv and crt.\n        buf = []\n        begin_crt = False\n        begin_prv = False\n        prvs = {}\n        thumbprints = {}\n        index = 0\n        v1_cert_list = []\n        with open(pem_file) as pem:\n            for line in pem.readlines():\n                buf.append(line)\n                if re.match(r'[-]+BEGIN.*KEY[-]+', line):\n                    begin_prv = True\n                elif re.match(r'[-]+BEGIN.*CERTIFICATE[-]+', line):\n                    begin_crt = True\n                elif re.match(r'[-]+END.*KEY[-]+', line):\n                    tmp_file = self.write_to_tmp_file(index, 'prv', buf)\n                    pub = cryptutil.get_pubkey_from_prv(tmp_file)\n                    prvs[pub] = tmp_file\n                    buf = []\n                    index += 1\n                    begin_prv = False\n                elif re.match(r'[-]+END.*CERTIFICATE[-]+', line):\n                    tmp_file = self.write_to_tmp_file(index, 'crt', buf)\n                    pub = cryptutil.get_pubkey_from_crt(tmp_file)\n                    thumbprint = cryptutil.get_thumbprint_from_crt(tmp_file)\n                    thumbprints[pub] = thumbprint\n                    # Rename crt with thumbprint as the file name\n                    crt = \"{0}.crt\".format(thumbprint)\n                    v1_cert_list.append({\n                        \"name\": None,\n                        \"thumbprint\": thumbprint\n                    })\n                    os.rename(tmp_file, os.path.join(conf.get_lib_dir(), crt))\n                    buf = []\n                    index += 1\n                    begin_crt = False\n\n        # Rename prv key with thumbprint as the file name\n        for pubkey in prvs:\n            thumbprint = thumbprints[pubkey]\n            if thumbprint:\n                tmp_file = prvs[pubkey]\n                prv = \"{0}.prv\".format(thumbprint)\n                os.rename(tmp_file, os.path.join(conf.get_lib_dir(), prv))\n\n        for v1_cert in v1_cert_list:\n            cert = Cert()\n            set_properties(\"certs\", cert, v1_cert)\n            self.cert_list.certificates.append(cert)\n\n    def write_to_tmp_file(self, index, suffix, buf):\n        file_name = os.path.join(conf.get_lib_dir(),\n                                 \"{0}.{1}\".format(index, suffix))\n        self.client.save_cache(file_name, \"\".join(buf))\n        return file_name\n\n\nclass ExtensionsConfig(object):\n    \"\"\"\n    parse ExtensionsConfig, downloading and unpacking them to /var/lib/waagent.\n    Install if <enabled>true</enabled>, remove if it is set to false.\n    \"\"\"\n\n    def __init__(self, xml_text):\n        logger.verbose(\"Load ExtensionsConfig.xml\")\n        self.ext_handlers = ExtHandlerList()\n        self.vmagent_manifests = VMAgentManifestList()\n        self.status_upload_blob = None\n        self.status_upload_blob_type = None\n        self.artifacts_profile_blob = None\n        if xml_text is not None:\n            self.parse(xml_text)\n\n    def parse(self, xml_text):\n        \"\"\"\n        Write configuration to file ExtensionsConfig.xml.\n        \"\"\"\n        xml_doc = parse_doc(xml_text)\n\n        ga_families_list = find(xml_doc, \"GAFamilies\")\n        ga_families = findall(ga_families_list, \"GAFamily\")\n\n        for ga_family in ga_families:\n            family = findtext(ga_family, \"Name\")\n            uris_list = find(ga_family, \"Uris\")\n            uris = findall(uris_list, \"Uri\")\n            manifest = VMAgentManifest()\n            manifest.family = family\n            for uri in uris:\n                manifestUri = VMAgentManifestUri(uri=gettext(uri))\n                manifest.versionsManifestUris.append(manifestUri)\n            self.vmagent_manifests.vmAgentManifests.append(manifest)\n\n        plugins_list = find(xml_doc, \"Plugins\")\n        plugins = findall(plugins_list, \"Plugin\")\n        plugin_settings_list = find(xml_doc, \"PluginSettings\")\n        plugin_settings = findall(plugin_settings_list, \"Plugin\")\n\n        for plugin in plugins:\n            ext_handler = self.parse_plugin(plugin)\n            self.ext_handlers.extHandlers.append(ext_handler)\n            self.parse_plugin_settings(ext_handler, plugin_settings)\n\n        self.status_upload_blob = findtext(xml_doc, \"StatusUploadBlob\")\n        self.artifacts_profile_blob = findtext(xml_doc, \"InVMArtifactsProfileBlob\")\n\n        status_upload_node = find(xml_doc, \"StatusUploadBlob\")\n        self.status_upload_blob_type = getattrib(status_upload_node,\n                                                 \"statusBlobType\")\n        logger.verbose(\"Extension config shows status blob type as [{0}]\",\n                       self.status_upload_blob_type)\n\n    def parse_plugin(self, plugin):\n        ext_handler = ExtHandler()\n        ext_handler.name = getattrib(plugin, \"name\")\n        ext_handler.properties.version = getattrib(plugin, \"version\")\n        ext_handler.properties.state = getattrib(plugin, \"state\")\n\n        ext_handler.properties.upgradeGuid = getattrib(plugin, \"upgradeGuid\")\n        if not ext_handler.properties.upgradeGuid:\n            ext_handler.properties.upgradeGuid = None\n\n        try:\n            ext_handler.properties.dependencyLevel = int(getattrib(plugin, \"dependencyLevel\"))\n        except ValueError:\n            ext_handler.properties.dependencyLevel = 0\n\n        auto_upgrade = getattrib(plugin, \"autoUpgrade\")\n        if auto_upgrade is not None and auto_upgrade.lower() == \"true\":\n            ext_handler.properties.upgradePolicy = \"auto\"\n        else:\n            ext_handler.properties.upgradePolicy = \"manual\"\n\n        location = getattrib(plugin, \"location\")\n        failover_location = getattrib(plugin, \"failoverlocation\")\n        for uri in [location, failover_location]:\n            version_uri = ExtHandlerVersionUri()\n            version_uri.uri = uri\n            ext_handler.versionUris.append(version_uri)\n        return ext_handler\n\n    def parse_plugin_settings(self, ext_handler, plugin_settings):\n        if plugin_settings is None:\n            return\n\n        name = ext_handler.name\n        version = ext_handler.properties.version\n        settings = [x for x in plugin_settings \\\n                    if getattrib(x, \"name\") == name and \\\n                    getattrib(x, \"version\") == version]\n\n        if settings is None or len(settings) == 0:\n            return\n\n        runtime_settings = None\n        runtime_settings_node = find(settings[0], \"RuntimeSettings\")\n        seqNo = getattrib(runtime_settings_node, \"seqNo\")\n        runtime_settings_str = gettext(runtime_settings_node)\n        try:\n            runtime_settings = json.loads(runtime_settings_str)\n        except ValueError as e:\n            logger.error(\"Invalid extension settings\")\n            return\n\n        for plugin_settings_list in runtime_settings[\"runtimeSettings\"]:\n            handler_settings = plugin_settings_list[\"handlerSettings\"]\n            ext = Extension()\n            # There is no \"extension name\" in wire protocol.\n            # Put\n            ext.name = ext_handler.name\n            ext.sequenceNumber = seqNo\n            ext.publicSettings = handler_settings.get(\"publicSettings\")\n            ext.protectedSettings = handler_settings.get(\"protectedSettings\")\n            thumbprint = handler_settings.get(\n                \"protectedSettingsCertThumbprint\")\n            ext.certificateThumbprint = thumbprint\n            ext_handler.properties.extensions.append(ext)\n\n\nclass ExtensionManifest(object):\n    def __init__(self, xml_text):\n        if xml_text is None:\n            raise ValueError(\"ExtensionManifest is None\")\n        logger.verbose(\"Load ExtensionManifest.xml\")\n        self.pkg_list = ExtHandlerPackageList()\n        self.allowed_versions = None\n        self.parse(xml_text)\n\n    def parse(self, xml_text):\n        xml_doc = parse_doc(xml_text)\n        self._handle_packages(findall(find(xml_doc,\n                                           \"Plugins\"),\n                                      \"Plugin\"),\n                              False)\n        self._handle_packages(findall(find(xml_doc,\n                                           \"InternalPlugins\"),\n                                      \"Plugin\"),\n                              True)\n\n    def _handle_packages(self, packages, isinternal):\n        for package in packages:\n            version = findtext(package, \"Version\")\n\n            disallow_major_upgrade = findtext(package,\n                                              \"DisallowMajorVersionUpgrade\")\n            if disallow_major_upgrade is None:\n                disallow_major_upgrade = ''\n            disallow_major_upgrade = disallow_major_upgrade.lower() == \"true\"\n\n            uris = find(package, \"Uris\")\n            uri_list = findall(uris, \"Uri\")\n            uri_list = [gettext(x) for x in uri_list]\n            pkg = ExtHandlerPackage()\n            pkg.version = version\n            pkg.disallow_major_upgrade = disallow_major_upgrade\n            for uri in uri_list:\n                pkg_uri = ExtHandlerVersionUri()\n                pkg_uri.uri = uri\n                pkg.uris.append(pkg_uri)\n\n            pkg.isinternal = isinternal\n            self.pkg_list.versions.append(pkg)\n\n\n# Do not extend this class\nclass InVMArtifactsProfile(object):\n    \"\"\"\n    deserialized json string of InVMArtifactsProfile.\n    It is expected to contain the following fields:\n    * inVMArtifactsProfileBlobSeqNo\n    * profileId (optional)\n    * onHold (optional)\n    * certificateThumbprint (optional)\n    * encryptedHealthChecks (optional)\n    * encryptedApplicationProfile (optional)\n    \"\"\"\n    def __init__(self, artifacts_profile):\n        if not textutil.is_str_none_or_whitespace(artifacts_profile):\n            self.__dict__.update(parse_json(artifacts_profile))\n\n    def is_on_hold(self):\n        # hasattr() is not available in Python 2.6\n        if 'onHold' in self.__dict__:\n            return self.onHold.lower() == 'true'\n        return False\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/sara0871/releases-/blob/2ee62b10bc3c51e642cb8ec2fe5e044b23c0fa46",
        "file_path": "/homeassistant/components/sensor/netatmo.py",
        "source": "\"\"\"\nSupport for the NetAtmo Weather Service.\n\nFor more details about this platform, please refer to the documentation at\nhttps://home-assistant.io/components/sensor.netatmo/\n\"\"\"\nimport logging\nfrom datetime import timedelta\n\nimport voluptuous as vol\n\nfrom homeassistant.components.sensor import PLATFORM_SCHEMA\nfrom homeassistant.const import (\n    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,\n    STATE_UNKNOWN)\nfrom homeassistant.helpers.entity import Entity\nfrom homeassistant.util import Throttle\nimport homeassistant.helpers.config_validation as cv\n\n_LOGGER = logging.getLogger(__name__)\n\nCONF_MODULES = 'modules'\nCONF_STATION = 'station'\n\nDEPENDENCIES = ['netatmo']\n\n# NetAtmo Data is uploaded to server every 10 minutes\nMIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n\nSENSOR_TYPES = {\n    'temperature': ['Temperature', TEMP_CELSIUS, None,\n                    DEVICE_CLASS_TEMPERATURE],\n    'co2': ['CO2', 'ppm', 'mdi:cloud', None],\n    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],\n    'noise': ['Noise', 'dB', 'mdi:volume-high', None],\n    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],\n    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],\n    'battery_vp': ['Battery', '', 'mdi:battery', None],\n    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],\n    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'windangle': ['Angle', '', 'mdi:compass', None],\n    'windangle_value': ['Angle Value', '', 'mdi:compass', None],\n    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],\n    'gustangle': ['Gust Angle', '', 'mdi:compass', None],\n    'gustangle_value': ['Gust Angle Value', '', 'mdi:compass', None],\n    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],\n    'rf_status': ['Radio', '', 'mdi:signal', None],\n    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n    'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n}\n\nMODULE_SCHEMA = vol.Schema({\n    vol.Required(cv.string):\n        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),\n})\n\nPLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({\n    vol.Optional(CONF_STATION): cv.string,\n    vol.Optional(CONF_MODULES): MODULE_SCHEMA,\n})\n\n\ndef setup_platform(hass, config, add_devices, discovery_info=None):\n    \"\"\"Set up the available Netatmo weather sensors.\"\"\"\n    netatmo = hass.components.netatmo\n    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))\n\n    dev = []\n    import pyatmo\n    try:\n        if CONF_MODULES in config:\n            # Iterate each module\n            for module_name, monitored_conditions in\\\n                    config[CONF_MODULES].items():\n                # Test if module exist \"\"\"\n                if module_name not in data.get_module_names():\n                    _LOGGER.error('Module name: \"%s\" not found', module_name)\n                    continue\n                # Only create sensor for monitored \"\"\"\n                for variable in monitored_conditions:\n                    dev.append(NetAtmoSensor(data, module_name, variable))\n        else:\n            for module_name in data.get_module_names():\n                for variable in\\\n                        data.station_data.monitoredConditions(module_name):\n                    if variable in SENSOR_TYPES.keys():\n                        dev.append(NetAtmoSensor(data, module_name, variable))\n                    else:\n                        _LOGGER.warning(\"Ignoring unknown var %s for mod %s\",\n                                        variable, module_name)\n    except pyatmo.NoDevice:\n        return None\n\n    add_devices(dev, True)\n\n\nclass NetAtmoSensor(Entity):\n    \"\"\"Implementation of a Netatmo sensor.\"\"\"\n\n    def __init__(self, netatmo_data, module_name, sensor_type):\n        \"\"\"Initialize the sensor.\"\"\"\n        self._name = 'Netatmo {} {}'.format(module_name,\n                                            SENSOR_TYPES[sensor_type][0])\n        self.netatmo_data = netatmo_data\n        self.module_name = module_name\n        self.type = sensor_type\n        self._state = None\n        self._device_class = SENSOR_TYPES[self.type][3]\n        self._icon = SENSOR_TYPES[self.type][2]\n        self._unit_of_measurement = SENSOR_TYPES[self.type][1]\n        module_id = self.netatmo_data.\\\n            station_data.moduleByName(module=module_name)['_id']\n        self.module_id = module_id[1]\n\n    @property\n    def name(self):\n        \"\"\"Return the name of the sensor.\"\"\"\n        return self._name\n\n    @property\n    def icon(self):\n        \"\"\"Icon to use in the frontend, if any.\"\"\"\n        return self._icon\n\n    @property\n    def device_class(self):\n        \"\"\"Return the device class of the sensor.\"\"\"\n        return self._device_class\n\n    @property\n    def state(self):\n        \"\"\"Return the state of the device.\"\"\"\n        return self._state\n\n    @property\n    def unit_of_measurement(self):\n        \"\"\"Return the unit of measurement of this entity, if any.\"\"\"\n        return self._unit_of_measurement\n\n    def update(self):\n        \"\"\"Get the latest data from NetAtmo API and updates the states.\"\"\"\n        self.netatmo_data.update()\n        data = self.netatmo_data.data.get(self.module_name)\n\n        if data is None:\n            _LOGGER.warning(\"No data found for %s\", self.module_name)\n            self._state = STATE_UNKNOWN\n            return\n\n        if self.type == 'temperature':\n            self._state = round(data['Temperature'], 1)\n        elif self.type == 'humidity':\n            self._state = data['Humidity']\n        elif self.type == 'rain':\n            self._state = data['Rain']\n        elif self.type == 'sum_rain_1':\n            self._state = data['sum_rain_1']\n        elif self.type == 'sum_rain_24':\n            self._state = data['sum_rain_24']\n        elif self.type == 'noise':\n            self._state = data['Noise']\n        elif self.type == 'co2':\n            self._state = data['CO2']\n        elif self.type == 'pressure':\n            self._state = round(data['Pressure'], 1)\n        elif self.type == 'battery_lvl':\n            self._state = data['battery_vp']\n        elif self.type == 'battery_vp' and self.module_id == '6':\n            if data['battery_vp'] >= 5590:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5180:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4770:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4360:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4360:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '5':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '3':\n            if data['battery_vp'] >= 5640:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5280:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4920:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4560:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4560:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '2':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'min_temp':\n            self._state = data['min_temp']\n        elif self.type == 'max_temp':\n            self._state = data['max_temp']\n        elif self.type == 'windangle_value':\n            self._state = data['WindAngle']\n        elif self.type == 'windangle':\n            if data['WindAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n        elif self.type == 'windstrength':\n            self._state = data['WindStrength']\n        elif self.type == 'gustangle_value':\n            self._state = data['GustAngle']\n        elif self.type == 'gustangle':\n            if data['GustAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n        elif self.type == 'guststrength':\n            self._state = data['GustStrength']\n        elif self.type == 'rf_status_lvl':\n            self._state = data['rf_status']\n        elif self.type == 'rf_status':\n            if data['rf_status'] >= 90:\n                self._state = \"Low\"\n            elif data['rf_status'] >= 76:\n                self._state = \"Medium\"\n            elif data['rf_status'] >= 60:\n                self._state = \"High\"\n            elif data['rf_status'] <= 59:\n                self._state = \"Full\"\n        elif self.type == 'wifi_status_lvl':\n            self._state = data['wifi_status']\n        elif self.type == 'wifi_status':\n            if data['wifi_status'] >= 86:\n                self._state = \"Low\"\n            elif data['wifi_status'] >= 71:\n                self._state = \"Medium\"\n            elif data['wifi_status'] >= 56:\n                self._state = \"High\"\n            elif data['wifi_status'] <= 55:\n                self._state = \"Full\"\n\n\nclass NetAtmoData(object):\n    \"\"\"Get the latest data from NetAtmo.\"\"\"\n\n    def __init__(self, auth, station):\n        \"\"\"Initialize the data object.\"\"\"\n        self.auth = auth\n        self.data = None\n        self.station_data = None\n        self.station = station\n\n    def get_module_names(self):\n        \"\"\"Return all module available on the API as a list.\"\"\"\n        self.update()\n        return self.data.keys()\n\n    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n    def update(self):\n        \"\"\"Call the Netatmo API to update the data.\"\"\"\n        import pyatmo\n        self.station_data = pyatmo.WeatherStationData(self.auth)\n\n        if self.station is not None:\n            self.data = self.station_data.lastData(\n                station=self.station, exclude=3600)\n        else:\n            self.data = self.station_data.lastData(exclude=3600)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/sara0871/master.zip/blob/2ee62b10bc3c51e642cb8ec2fe5e044b23c0fa46",
        "file_path": "/homeassistant/components/sensor/netatmo.py",
        "source": "\"\"\"\nSupport for the NetAtmo Weather Service.\n\nFor more details about this platform, please refer to the documentation at\nhttps://home-assistant.io/components/sensor.netatmo/\n\"\"\"\nimport logging\nfrom datetime import timedelta\n\nimport voluptuous as vol\n\nfrom homeassistant.components.sensor import PLATFORM_SCHEMA\nfrom homeassistant.const import (\n    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,\n    STATE_UNKNOWN)\nfrom homeassistant.helpers.entity import Entity\nfrom homeassistant.util import Throttle\nimport homeassistant.helpers.config_validation as cv\n\n_LOGGER = logging.getLogger(__name__)\n\nCONF_MODULES = 'modules'\nCONF_STATION = 'station'\n\nDEPENDENCIES = ['netatmo']\n\n# NetAtmo Data is uploaded to server every 10 minutes\nMIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n\nSENSOR_TYPES = {\n    'temperature': ['Temperature', TEMP_CELSIUS, None,\n                    DEVICE_CLASS_TEMPERATURE],\n    'co2': ['CO2', 'ppm', 'mdi:cloud', None],\n    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],\n    'noise': ['Noise', 'dB', 'mdi:volume-high', None],\n    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],\n    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],\n    'battery_vp': ['Battery', '', 'mdi:battery', None],\n    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],\n    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'windangle': ['Angle', '', 'mdi:compass', None],\n    'windangle_value': ['Angle Value', '', 'mdi:compass', None],\n    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],\n    'gustangle': ['Gust Angle', '', 'mdi:compass', None],\n    'gustangle_value': ['Gust Angle Value', '', 'mdi:compass', None],\n    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],\n    'rf_status': ['Radio', '', 'mdi:signal', None],\n    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n    'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n}\n\nMODULE_SCHEMA = vol.Schema({\n    vol.Required(cv.string):\n        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),\n})\n\nPLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({\n    vol.Optional(CONF_STATION): cv.string,\n    vol.Optional(CONF_MODULES): MODULE_SCHEMA,\n})\n\n\ndef setup_platform(hass, config, add_devices, discovery_info=None):\n    \"\"\"Set up the available Netatmo weather sensors.\"\"\"\n    netatmo = hass.components.netatmo\n    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))\n\n    dev = []\n    import pyatmo\n    try:\n        if CONF_MODULES in config:\n            # Iterate each module\n            for module_name, monitored_conditions in\\\n                    config[CONF_MODULES].items():\n                # Test if module exist \"\"\"\n                if module_name not in data.get_module_names():\n                    _LOGGER.error('Module name: \"%s\" not found', module_name)\n                    continue\n                # Only create sensor for monitored \"\"\"\n                for variable in monitored_conditions:\n                    dev.append(NetAtmoSensor(data, module_name, variable))\n        else:\n            for module_name in data.get_module_names():\n                for variable in\\\n                        data.station_data.monitoredConditions(module_name):\n                    if variable in SENSOR_TYPES.keys():\n                        dev.append(NetAtmoSensor(data, module_name, variable))\n                    else:\n                        _LOGGER.warning(\"Ignoring unknown var %s for mod %s\",\n                                        variable, module_name)\n    except pyatmo.NoDevice:\n        return None\n\n    add_devices(dev, True)\n\n\nclass NetAtmoSensor(Entity):\n    \"\"\"Implementation of a Netatmo sensor.\"\"\"\n\n    def __init__(self, netatmo_data, module_name, sensor_type):\n        \"\"\"Initialize the sensor.\"\"\"\n        self._name = 'Netatmo {} {}'.format(module_name,\n                                            SENSOR_TYPES[sensor_type][0])\n        self.netatmo_data = netatmo_data\n        self.module_name = module_name\n        self.type = sensor_type\n        self._state = None\n        self._device_class = SENSOR_TYPES[self.type][3]\n        self._icon = SENSOR_TYPES[self.type][2]\n        self._unit_of_measurement = SENSOR_TYPES[self.type][1]\n        module_id = self.netatmo_data.\\\n            station_data.moduleByName(module=module_name)['_id']\n        self.module_id = module_id[1]\n\n    @property\n    def name(self):\n        \"\"\"Return the name of the sensor.\"\"\"\n        return self._name\n\n    @property\n    def icon(self):\n        \"\"\"Icon to use in the frontend, if any.\"\"\"\n        return self._icon\n\n    @property\n    def device_class(self):\n        \"\"\"Return the device class of the sensor.\"\"\"\n        return self._device_class\n\n    @property\n    def state(self):\n        \"\"\"Return the state of the device.\"\"\"\n        return self._state\n\n    @property\n    def unit_of_measurement(self):\n        \"\"\"Return the unit of measurement of this entity, if any.\"\"\"\n        return self._unit_of_measurement\n\n    def update(self):\n        \"\"\"Get the latest data from NetAtmo API and updates the states.\"\"\"\n        self.netatmo_data.update()\n        data = self.netatmo_data.data.get(self.module_name)\n\n        if data is None:\n            _LOGGER.warning(\"No data found for %s\", self.module_name)\n            self._state = STATE_UNKNOWN\n            return\n\n        if self.type == 'temperature':\n            self._state = round(data['Temperature'], 1)\n        elif self.type == 'humidity':\n            self._state = data['Humidity']\n        elif self.type == 'rain':\n            self._state = data['Rain']\n        elif self.type == 'sum_rain_1':\n            self._state = data['sum_rain_1']\n        elif self.type == 'sum_rain_24':\n            self._state = data['sum_rain_24']\n        elif self.type == 'noise':\n            self._state = data['Noise']\n        elif self.type == 'co2':\n            self._state = data['CO2']\n        elif self.type == 'pressure':\n            self._state = round(data['Pressure'], 1)\n        elif self.type == 'battery_lvl':\n            self._state = data['battery_vp']\n        elif self.type == 'battery_vp' and self.module_id == '6':\n            if data['battery_vp'] >= 5590:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5180:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4770:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4360:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4360:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '5':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '3':\n            if data['battery_vp'] >= 5640:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5280:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4920:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4560:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4560:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '2':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'min_temp':\n            self._state = data['min_temp']\n        elif self.type == 'max_temp':\n            self._state = data['max_temp']\n        elif self.type == 'windangle_value':\n            self._state = data['WindAngle']\n        elif self.type == 'windangle':\n            if data['WindAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n        elif self.type == 'windstrength':\n            self._state = data['WindStrength']\n        elif self.type == 'gustangle_value':\n            self._state = data['GustAngle']\n        elif self.type == 'gustangle':\n            if data['GustAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n        elif self.type == 'guststrength':\n            self._state = data['GustStrength']\n        elif self.type == 'rf_status_lvl':\n            self._state = data['rf_status']\n        elif self.type == 'rf_status':\n            if data['rf_status'] >= 90:\n                self._state = \"Low\"\n            elif data['rf_status'] >= 76:\n                self._state = \"Medium\"\n            elif data['rf_status'] >= 60:\n                self._state = \"High\"\n            elif data['rf_status'] <= 59:\n                self._state = \"Full\"\n        elif self.type == 'wifi_status_lvl':\n            self._state = data['wifi_status']\n        elif self.type == 'wifi_status':\n            if data['wifi_status'] >= 86:\n                self._state = \"Low\"\n            elif data['wifi_status'] >= 71:\n                self._state = \"Medium\"\n            elif data['wifi_status'] >= 56:\n                self._state = \"High\"\n            elif data['wifi_status'] <= 55:\n                self._state = \"Full\"\n\n\nclass NetAtmoData(object):\n    \"\"\"Get the latest data from NetAtmo.\"\"\"\n\n    def __init__(self, auth, station):\n        \"\"\"Initialize the data object.\"\"\"\n        self.auth = auth\n        self.data = None\n        self.station_data = None\n        self.station = station\n\n    def get_module_names(self):\n        \"\"\"Return all module available on the API as a list.\"\"\"\n        self.update()\n        return self.data.keys()\n\n    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n    def update(self):\n        \"\"\"Call the Netatmo API to update the data.\"\"\"\n        import pyatmo\n        self.station_data = pyatmo.WeatherStationData(self.auth)\n\n        if self.station is not None:\n            self.data = self.station_data.lastData(\n                station=self.station, exclude=3600)\n        else:\n            self.data = self.station_data.lastData(exclude=3600)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/eric-erki/Home-assistant/blob/2ee62b10bc3c51e642cb8ec2fe5e044b23c0fa46",
        "file_path": "/homeassistant/components/sensor/netatmo.py",
        "source": "\"\"\"\nSupport for the NetAtmo Weather Service.\n\nFor more details about this platform, please refer to the documentation at\nhttps://home-assistant.io/components/sensor.netatmo/\n\"\"\"\nimport logging\nfrom datetime import timedelta\n\nimport voluptuous as vol\n\nfrom homeassistant.components.sensor import PLATFORM_SCHEMA\nfrom homeassistant.const import (\n    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,\n    STATE_UNKNOWN)\nfrom homeassistant.helpers.entity import Entity\nfrom homeassistant.util import Throttle\nimport homeassistant.helpers.config_validation as cv\n\n_LOGGER = logging.getLogger(__name__)\n\nCONF_MODULES = 'modules'\nCONF_STATION = 'station'\n\nDEPENDENCIES = ['netatmo']\n\n# NetAtmo Data is uploaded to server every 10 minutes\nMIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n\nSENSOR_TYPES = {\n    'temperature': ['Temperature', TEMP_CELSIUS, None,\n                    DEVICE_CLASS_TEMPERATURE],\n    'co2': ['CO2', 'ppm', 'mdi:cloud', None],\n    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],\n    'noise': ['Noise', 'dB', 'mdi:volume-high', None],\n    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],\n    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],\n    'battery_vp': ['Battery', '', 'mdi:battery', None],\n    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],\n    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'windangle': ['Angle', '', 'mdi:compass', None],\n    'windangle_value': ['Angle Value', '', 'mdi:compass', None],\n    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],\n    'gustangle': ['Gust Angle', '', 'mdi:compass', None],\n    'gustangle_value': ['Gust Angle Value', '', 'mdi:compass', None],\n    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],\n    'rf_status': ['Radio', '', 'mdi:signal', None],\n    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n    'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n}\n\nMODULE_SCHEMA = vol.Schema({\n    vol.Required(cv.string):\n        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),\n})\n\nPLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({\n    vol.Optional(CONF_STATION): cv.string,\n    vol.Optional(CONF_MODULES): MODULE_SCHEMA,\n})\n\n\ndef setup_platform(hass, config, add_devices, discovery_info=None):\n    \"\"\"Set up the available Netatmo weather sensors.\"\"\"\n    netatmo = hass.components.netatmo\n    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))\n\n    dev = []\n    import pyatmo\n    try:\n        if CONF_MODULES in config:\n            # Iterate each module\n            for module_name, monitored_conditions in\\\n                    config[CONF_MODULES].items():\n                # Test if module exist \"\"\"\n                if module_name not in data.get_module_names():\n                    _LOGGER.error('Module name: \"%s\" not found', module_name)\n                    continue\n                # Only create sensor for monitored \"\"\"\n                for variable in monitored_conditions:\n                    dev.append(NetAtmoSensor(data, module_name, variable))\n        else:\n            for module_name in data.get_module_names():\n                for variable in\\\n                        data.station_data.monitoredConditions(module_name):\n                    if variable in SENSOR_TYPES.keys():\n                        dev.append(NetAtmoSensor(data, module_name, variable))\n                    else:\n                        _LOGGER.warning(\"Ignoring unknown var %s for mod %s\",\n                                        variable, module_name)\n    except pyatmo.NoDevice:\n        return None\n\n    add_devices(dev, True)\n\n\nclass NetAtmoSensor(Entity):\n    \"\"\"Implementation of a Netatmo sensor.\"\"\"\n\n    def __init__(self, netatmo_data, module_name, sensor_type):\n        \"\"\"Initialize the sensor.\"\"\"\n        self._name = 'Netatmo {} {}'.format(module_name,\n                                            SENSOR_TYPES[sensor_type][0])\n        self.netatmo_data = netatmo_data\n        self.module_name = module_name\n        self.type = sensor_type\n        self._state = None\n        self._device_class = SENSOR_TYPES[self.type][3]\n        self._icon = SENSOR_TYPES[self.type][2]\n        self._unit_of_measurement = SENSOR_TYPES[self.type][1]\n        module_id = self.netatmo_data.\\\n            station_data.moduleByName(module=module_name)['_id']\n        self.module_id = module_id[1]\n\n    @property\n    def name(self):\n        \"\"\"Return the name of the sensor.\"\"\"\n        return self._name\n\n    @property\n    def icon(self):\n        \"\"\"Icon to use in the frontend, if any.\"\"\"\n        return self._icon\n\n    @property\n    def device_class(self):\n        \"\"\"Return the device class of the sensor.\"\"\"\n        return self._device_class\n\n    @property\n    def state(self):\n        \"\"\"Return the state of the device.\"\"\"\n        return self._state\n\n    @property\n    def unit_of_measurement(self):\n        \"\"\"Return the unit of measurement of this entity, if any.\"\"\"\n        return self._unit_of_measurement\n\n    def update(self):\n        \"\"\"Get the latest data from NetAtmo API and updates the states.\"\"\"\n        self.netatmo_data.update()\n        data = self.netatmo_data.data.get(self.module_name)\n\n        if data is None:\n            _LOGGER.warning(\"No data found for %s\", self.module_name)\n            self._state = STATE_UNKNOWN\n            return\n\n        if self.type == 'temperature':\n            self._state = round(data['Temperature'], 1)\n        elif self.type == 'humidity':\n            self._state = data['Humidity']\n        elif self.type == 'rain':\n            self._state = data['Rain']\n        elif self.type == 'sum_rain_1':\n            self._state = data['sum_rain_1']\n        elif self.type == 'sum_rain_24':\n            self._state = data['sum_rain_24']\n        elif self.type == 'noise':\n            self._state = data['Noise']\n        elif self.type == 'co2':\n            self._state = data['CO2']\n        elif self.type == 'pressure':\n            self._state = round(data['Pressure'], 1)\n        elif self.type == 'battery_lvl':\n            self._state = data['battery_vp']\n        elif self.type == 'battery_vp' and self.module_id == '6':\n            if data['battery_vp'] >= 5590:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5180:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4770:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4360:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4360:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '5':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '3':\n            if data['battery_vp'] >= 5640:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5280:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4920:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4560:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4560:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '2':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'min_temp':\n            self._state = data['min_temp']\n        elif self.type == 'max_temp':\n            self._state = data['max_temp']\n        elif self.type == 'windangle_value':\n            self._state = data['WindAngle']\n        elif self.type == 'windangle':\n            if data['WindAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n        elif self.type == 'windstrength':\n            self._state = data['WindStrength']\n        elif self.type == 'gustangle_value':\n            self._state = data['GustAngle']\n        elif self.type == 'gustangle':\n            if data['GustAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n        elif self.type == 'guststrength':\n            self._state = data['GustStrength']\n        elif self.type == 'rf_status_lvl':\n            self._state = data['rf_status']\n        elif self.type == 'rf_status':\n            if data['rf_status'] >= 90:\n                self._state = \"Low\"\n            elif data['rf_status'] >= 76:\n                self._state = \"Medium\"\n            elif data['rf_status'] >= 60:\n                self._state = \"High\"\n            elif data['rf_status'] <= 59:\n                self._state = \"Full\"\n        elif self.type == 'wifi_status_lvl':\n            self._state = data['wifi_status']\n        elif self.type == 'wifi_status':\n            if data['wifi_status'] >= 86:\n                self._state = \"Low\"\n            elif data['wifi_status'] >= 71:\n                self._state = \"Medium\"\n            elif data['wifi_status'] >= 56:\n                self._state = \"High\"\n            elif data['wifi_status'] <= 55:\n                self._state = \"Full\"\n\n\nclass NetAtmoData(object):\n    \"\"\"Get the latest data from NetAtmo.\"\"\"\n\n    def __init__(self, auth, station):\n        \"\"\"Initialize the data object.\"\"\"\n        self.auth = auth\n        self.data = None\n        self.station_data = None\n        self.station = station\n\n    def get_module_names(self):\n        \"\"\"Return all module available on the API as a list.\"\"\"\n        self.update()\n        return self.data.keys()\n\n    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n    def update(self):\n        \"\"\"Call the Netatmo API to update the data.\"\"\"\n        import pyatmo\n        self.station_data = pyatmo.WeatherStationData(self.auth)\n\n        if self.station is not None:\n            self.data = self.station_data.lastData(\n                station=self.station, exclude=3600)\n        else:\n            self.data = self.station_data.lastData(exclude=3600)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/MoshonkaKita/Golovastik/blob/2ee62b10bc3c51e642cb8ec2fe5e044b23c0fa46",
        "file_path": "/homeassistant/components/sensor/netatmo.py",
        "source": "\"\"\"\nSupport for the NetAtmo Weather Service.\n\nFor more details about this platform, please refer to the documentation at\nhttps://home-assistant.io/components/sensor.netatmo/\n\"\"\"\nimport logging\nfrom datetime import timedelta\n\nimport voluptuous as vol\n\nfrom homeassistant.components.sensor import PLATFORM_SCHEMA\nfrom homeassistant.const import (\n    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,\n    STATE_UNKNOWN)\nfrom homeassistant.helpers.entity import Entity\nfrom homeassistant.util import Throttle\nimport homeassistant.helpers.config_validation as cv\n\n_LOGGER = logging.getLogger(__name__)\n\nCONF_MODULES = 'modules'\nCONF_STATION = 'station'\n\nDEPENDENCIES = ['netatmo']\n\n# NetAtmo Data is uploaded to server every 10 minutes\nMIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n\nSENSOR_TYPES = {\n    'temperature': ['Temperature', TEMP_CELSIUS, None,\n                    DEVICE_CLASS_TEMPERATURE],\n    'co2': ['CO2', 'ppm', 'mdi:cloud', None],\n    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],\n    'noise': ['Noise', 'dB', 'mdi:volume-high', None],\n    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],\n    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],\n    'battery_vp': ['Battery', '', 'mdi:battery', None],\n    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],\n    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'windangle': ['Angle', '', 'mdi:compass', None],\n    'windangle_value': ['Angle Value', '', 'mdi:compass', None],\n    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],\n    'gustangle': ['Gust Angle', '', 'mdi:compass', None],\n    'gustangle_value': ['Gust Angle Value', '', 'mdi:compass', None],\n    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],\n    'rf_status': ['Radio', '', 'mdi:signal', None],\n    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n    'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n}\n\nMODULE_SCHEMA = vol.Schema({\n    vol.Required(cv.string):\n        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),\n})\n\nPLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({\n    vol.Optional(CONF_STATION): cv.string,\n    vol.Optional(CONF_MODULES): MODULE_SCHEMA,\n})\n\n\ndef setup_platform(hass, config, add_devices, discovery_info=None):\n    \"\"\"Set up the available Netatmo weather sensors.\"\"\"\n    netatmo = hass.components.netatmo\n    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))\n\n    dev = []\n    import pyatmo\n    try:\n        if CONF_MODULES in config:\n            # Iterate each module\n            for module_name, monitored_conditions in\\\n                    config[CONF_MODULES].items():\n                # Test if module exist \"\"\"\n                if module_name not in data.get_module_names():\n                    _LOGGER.error('Module name: \"%s\" not found', module_name)\n                    continue\n                # Only create sensor for monitored \"\"\"\n                for variable in monitored_conditions:\n                    dev.append(NetAtmoSensor(data, module_name, variable))\n        else:\n            for module_name in data.get_module_names():\n                for variable in\\\n                        data.station_data.monitoredConditions(module_name):\n                    if variable in SENSOR_TYPES.keys():\n                        dev.append(NetAtmoSensor(data, module_name, variable))\n                    else:\n                        _LOGGER.warning(\"Ignoring unknown var %s for mod %s\",\n                                        variable, module_name)\n    except pyatmo.NoDevice:\n        return None\n\n    add_devices(dev, True)\n\n\nclass NetAtmoSensor(Entity):\n    \"\"\"Implementation of a Netatmo sensor.\"\"\"\n\n    def __init__(self, netatmo_data, module_name, sensor_type):\n        \"\"\"Initialize the sensor.\"\"\"\n        self._name = 'Netatmo {} {}'.format(module_name,\n                                            SENSOR_TYPES[sensor_type][0])\n        self.netatmo_data = netatmo_data\n        self.module_name = module_name\n        self.type = sensor_type\n        self._state = None\n        self._device_class = SENSOR_TYPES[self.type][3]\n        self._icon = SENSOR_TYPES[self.type][2]\n        self._unit_of_measurement = SENSOR_TYPES[self.type][1]\n        module_id = self.netatmo_data.\\\n            station_data.moduleByName(module=module_name)['_id']\n        self.module_id = module_id[1]\n\n    @property\n    def name(self):\n        \"\"\"Return the name of the sensor.\"\"\"\n        return self._name\n\n    @property\n    def icon(self):\n        \"\"\"Icon to use in the frontend, if any.\"\"\"\n        return self._icon\n\n    @property\n    def device_class(self):\n        \"\"\"Return the device class of the sensor.\"\"\"\n        return self._device_class\n\n    @property\n    def state(self):\n        \"\"\"Return the state of the device.\"\"\"\n        return self._state\n\n    @property\n    def unit_of_measurement(self):\n        \"\"\"Return the unit of measurement of this entity, if any.\"\"\"\n        return self._unit_of_measurement\n\n    def update(self):\n        \"\"\"Get the latest data from NetAtmo API and updates the states.\"\"\"\n        self.netatmo_data.update()\n        data = self.netatmo_data.data.get(self.module_name)\n\n        if data is None:\n            _LOGGER.warning(\"No data found for %s\", self.module_name)\n            self._state = STATE_UNKNOWN\n            return\n\n        if self.type == 'temperature':\n            self._state = round(data['Temperature'], 1)\n        elif self.type == 'humidity':\n            self._state = data['Humidity']\n        elif self.type == 'rain':\n            self._state = data['Rain']\n        elif self.type == 'sum_rain_1':\n            self._state = data['sum_rain_1']\n        elif self.type == 'sum_rain_24':\n            self._state = data['sum_rain_24']\n        elif self.type == 'noise':\n            self._state = data['Noise']\n        elif self.type == 'co2':\n            self._state = data['CO2']\n        elif self.type == 'pressure':\n            self._state = round(data['Pressure'], 1)\n        elif self.type == 'battery_lvl':\n            self._state = data['battery_vp']\n        elif self.type == 'battery_vp' and self.module_id == '6':\n            if data['battery_vp'] >= 5590:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5180:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4770:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4360:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4360:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '5':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '3':\n            if data['battery_vp'] >= 5640:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5280:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4920:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4560:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4560:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '2':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'min_temp':\n            self._state = data['min_temp']\n        elif self.type == 'max_temp':\n            self._state = data['max_temp']\n        elif self.type == 'windangle_value':\n            self._state = data['WindAngle']\n        elif self.type == 'windangle':\n            if data['WindAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n        elif self.type == 'windstrength':\n            self._state = data['WindStrength']\n        elif self.type == 'gustangle_value':\n            self._state = data['GustAngle']\n        elif self.type == 'gustangle':\n            if data['GustAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n        elif self.type == 'guststrength':\n            self._state = data['GustStrength']\n        elif self.type == 'rf_status_lvl':\n            self._state = data['rf_status']\n        elif self.type == 'rf_status':\n            if data['rf_status'] >= 90:\n                self._state = \"Low\"\n            elif data['rf_status'] >= 76:\n                self._state = \"Medium\"\n            elif data['rf_status'] >= 60:\n                self._state = \"High\"\n            elif data['rf_status'] <= 59:\n                self._state = \"Full\"\n        elif self.type == 'wifi_status_lvl':\n            self._state = data['wifi_status']\n        elif self.type == 'wifi_status':\n            if data['wifi_status'] >= 86:\n                self._state = \"Low\"\n            elif data['wifi_status'] >= 71:\n                self._state = \"Medium\"\n            elif data['wifi_status'] >= 56:\n                self._state = \"High\"\n            elif data['wifi_status'] <= 55:\n                self._state = \"Full\"\n\n\nclass NetAtmoData(object):\n    \"\"\"Get the latest data from NetAtmo.\"\"\"\n\n    def __init__(self, auth, station):\n        \"\"\"Initialize the data object.\"\"\"\n        self.auth = auth\n        self.data = None\n        self.station_data = None\n        self.station = station\n\n    def get_module_names(self):\n        \"\"\"Return all module available on the API as a list.\"\"\"\n        self.update()\n        return self.data.keys()\n\n    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n    def update(self):\n        \"\"\"Call the Netatmo API to update the data.\"\"\"\n        import pyatmo\n        self.station_data = pyatmo.WeatherStationData(self.auth)\n\n        if self.station is not None:\n            self.data = self.station_data.lastData(\n                station=self.station, exclude=3600)\n        else:\n            self.data = self.station_data.lastData(exclude=3600)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/tianyabeef/gutMicrobiome/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/dag.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport textwrap\nimport time\nfrom collections import defaultdict, Counter\nfrom itertools import chain, combinations, filterfalse, product, groupby\nfrom functools import partial, lru_cache\nfrom operator import itemgetter, attrgetter\n\nfrom snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\nfrom snakemake.jobs import Job, Reason\nfrom snakemake.exceptions import RuleException, MissingInputException\nfrom snakemake.exceptions import MissingRuleException, AmbiguousRuleException\nfrom snakemake.exceptions import CyclicGraphException, MissingOutputException\nfrom snakemake.exceptions import IncompleteFilesException\nfrom snakemake.exceptions import PeriodicWildcardError\nfrom snakemake.exceptions import UnexpectedOutputException, InputFunctionException\nfrom snakemake.logging import logger\nfrom snakemake.output_index import OutputIndex\n\n\nclass DAG:\n    def __init__(self, workflow,\n                 rules=None,\n                 dryrun=False,\n                 targetfiles=None,\n                 targetrules=None,\n                 forceall=False,\n                 forcerules=None,\n                 forcefiles=None,\n                 priorityfiles=None,\n                 priorityrules=None,\n                 ignore_ambiguity=False,\n                 force_incomplete=False,\n                 ignore_incomplete=False,\n                 notemp=False):\n\n        self.dryrun = dryrun\n        self.dependencies = defaultdict(partial(defaultdict, set))\n        self.depending = defaultdict(partial(defaultdict, set))\n        self._needrun = set()\n        self._priority = dict()\n        self._downstream_size = dict()\n        self._reason = defaultdict(Reason)\n        self._finished = set()\n        self._dynamic = set()\n        self._len = 0\n        self.workflow = workflow\n        self.rules = set(rules)\n        self.ignore_ambiguity = ignore_ambiguity\n        self.targetfiles = targetfiles\n        self.targetrules = targetrules\n        self.priorityfiles = priorityfiles\n        self.priorityrules = priorityrules\n        self.targetjobs = set()\n        self.prioritytargetjobs = set()\n        self._ready_jobs = set()\n        self.notemp = notemp\n        self._jobid = dict()\n\n        self.forcerules = set()\n        self.forcefiles = set()\n        self.updated_subworkflow_files = set()\n        if forceall:\n            self.forcerules.update(self.rules)\n        elif forcerules:\n            self.forcerules.update(forcerules)\n        if forcefiles:\n            self.forcefiles.update(forcefiles)\n        self.omitforce = set()\n\n        self.force_incomplete = force_incomplete\n        self.ignore_incomplete = ignore_incomplete\n\n        self.periodic_wildcard_detector = PeriodicityDetector()\n\n        self.update_output_index()\n\n    def init(self):\n        \"\"\" Initialise the DAG. \"\"\"\n        for job in map(self.rule2job, self.targetrules):\n            job = self.update([job])\n            self.targetjobs.add(job)\n\n        for file in self.targetfiles:\n            job = self.update(self.file2jobs(file), file=file)\n            self.targetjobs.add(job)\n\n        self.update_needrun()\n\n    def update_output_index(self):\n        self.output_index = OutputIndex(self.rules)\n\n    def check_incomplete(self):\n        if not self.ignore_incomplete:\n            incomplete = self.incomplete_files\n            if incomplete:\n                if self.force_incomplete:\n                    logger.debug(\"Forcing incomplete files:\")\n                    logger.debug(\"\\t\" + \"\\n\\t\".join(incomplete))\n                    self.forcefiles.update(incomplete)\n                else:\n                    raise IncompleteFilesException(incomplete)\n\n    def check_dynamic(self):\n        for job in filter(lambda job: (\n            job.dynamic_output and not self.needrun(job)\n        ), self.jobs):\n            self.update_dynamic(job)\n\n    @property\n    def dynamic_output_jobs(self):\n        return (job for job in self.jobs if job.dynamic_output)\n\n    @property\n    def jobs(self):\n        \"\"\" All jobs in the DAG. \"\"\"\n        for job in self.bfs(self.dependencies, *self.targetjobs):\n            yield job\n\n    @property\n    def needrun_jobs(self):\n        \"\"\" Jobs that need to be executed. \"\"\"\n        for job in filter(self.needrun,\n                          self.bfs(self.dependencies, *self.targetjobs,\n                                   stop=self.noneedrun_finished)):\n            yield job\n\n    @property\n    def local_needrun_jobs(self):\n        return filter(lambda job: self.workflow.is_local(job.rule),\n                      self.needrun_jobs)\n\n    @property\n    def finished_jobs(self):\n        \"\"\" Jobs that have been executed. \"\"\"\n        for job in filter(self.finished, self.bfs(self.dependencies,\n                                                  *self.targetjobs)):\n            yield job\n\n    @property\n    def ready_jobs(self):\n        \"\"\" Jobs that are ready to execute. \"\"\"\n        return self._ready_jobs\n\n    def ready(self, job):\n        \"\"\" Return whether a given job is ready to execute. \"\"\"\n        return job in self._ready_jobs\n\n    def needrun(self, job):\n        \"\"\" Return whether a given job needs to be executed. \"\"\"\n        return job in self._needrun\n\n    def priority(self, job):\n        return self._priority[job]\n\n    def downstream_size(self, job):\n        return self._downstream_size[job]\n\n    def _job_values(self, jobs, values):\n        return [values[job] for job in jobs]\n\n    def priorities(self, jobs):\n        return self._job_values(jobs, self._priority)\n\n    def downstream_sizes(self, jobs):\n        return self._job_values(jobs, self._downstream_size)\n\n    def noneedrun_finished(self, job):\n        \"\"\"\n        Return whether a given job is finished or was not\n        required to run at all.\n        \"\"\"\n        return not self.needrun(job) or self.finished(job)\n\n    def reason(self, job):\n        \"\"\" Return the reason of the job execution. \"\"\"\n        return self._reason[job]\n\n    def finished(self, job):\n        \"\"\" Return whether a job is finished. \"\"\"\n        return job in self._finished\n\n    def dynamic(self, job):\n        \"\"\"\n        Return whether a job is dynamic (i.e. it is only a placeholder\n        for those that are created after the job with dynamic output has\n        finished.\n        \"\"\"\n        return job in self._dynamic\n\n    def requested_files(self, job):\n        \"\"\" Return the files a job requests. \"\"\"\n        return set(*self.depending[job].values())\n\n    @property\n    def incomplete_files(self):\n        return list(chain(*(\n            job.output for job in filter(self.workflow.persistence.incomplete,\n                                         filterfalse(self.needrun, self.jobs))\n        )))\n\n    @property\n    def newversion_files(self):\n        return list(chain(*(\n            job.output\n            for job in filter(self.workflow.persistence.newversion, self.jobs)\n        )))\n\n    def missing_temp(self, job):\n        \"\"\"\n        Return whether a temp file that is input of the given job is missing.\n        \"\"\"\n        for job_, files in self.depending[job].items():\n            if self.needrun(job_) and any(not f.exists for f in files):\n                return True\n        return False\n\n    def check_output(self, job, wait=3):\n        \"\"\" Raise exception if output files of job are missing. \"\"\"\n        try:\n            wait_for_files(job.expanded_output, latency_wait=wait)\n        except IOError as e:\n            raise MissingOutputException(str(e), rule=job.rule)\n\n        input_maxtime = job.input_maxtime\n        if input_maxtime is not None:\n            output_mintime = job.output_mintime\n            if output_mintime is not None and output_mintime < input_maxtime:\n                raise RuleException(\n                    \"Output files {} are older than input \"\n                    \"files. Did you extract an archive? Make sure that output \"\n                    \"files have a more recent modification date than the \"\n                    \"archive, e.g. by using 'touch'.\".format(\n                        \", \".join(job.expanded_output)),\n                    rule=job.rule)\n\n    def check_periodic_wildcards(self, job):\n        \"\"\" Raise an exception if a wildcard of the given job appears to be periodic,\n        indicating a cyclic dependency. \"\"\"\n        for wildcard, value in job.wildcards_dict.items():\n            periodic_substring = self.periodic_wildcard_detector.is_periodic(\n                value)\n            if periodic_substring is not None:\n                raise PeriodicWildcardError(\n                    \"The value {} in wildcard {} is periodically repeated ({}). \"\n                    \"This would lead to an infinite recursion. \"\n                    \"To avoid this, e.g. restrict the wildcards in this rule to certain values.\".format(\n                        periodic_substring, wildcard, value),\n                    rule=job.rule)\n\n    def handle_protected(self, job):\n        \"\"\" Write-protect output files that are marked with protected(). \"\"\"\n        for f in job.expanded_output:\n            if f in job.protected_output:\n                logger.info(\"Write-protecting output file {}.\".format(f))\n                f.protect()\n\n    def handle_touch(self, job):\n        \"\"\" Touches those output files that are marked for touching. \"\"\"\n        for f in job.expanded_output:\n            if f in job.touch_output:\n                logger.info(\"Touching output file {}.\".format(f))\n                f.touch_or_create()\n\n    def handle_temp(self, job):\n        \"\"\" Remove temp files if they are no longer needed. \"\"\"\n        if self.notemp:\n            return\n\n        needed = lambda job_, f: any(\n            f in files for j, files in self.depending[job_].items()\n            if not self.finished(j) and self.needrun(j) and j != job)\n\n        def unneeded_files():\n            for job_, files in self.dependencies[job].items():\n                for f in job_.temp_output & files:\n                    if not needed(job_, f):\n                        yield f\n            for f in filterfalse(partial(needed, job), job.temp_output):\n                if not f in self.targetfiles:\n                    yield f\n\n        for f in unneeded_files():\n            logger.info(\"Removing temporary output file {}.\".format(f))\n            f.remove()\n\n    def jobid(self, job):\n        if job not in self._jobid:\n            self._jobid[job] = len(self._jobid)\n        return self._jobid[job]\n\n    def update(self, jobs, file=None, visited=None, skip_until_dynamic=False):\n        \"\"\" Update the DAG by adding given jobs and their dependencies. \"\"\"\n        if visited is None:\n            visited = set()\n        producer = None\n        exceptions = list()\n        jobs = sorted(jobs, reverse=not self.ignore_ambiguity)\n        cycles = list()\n\n        for job in jobs:\n            if file in job.input:\n                cycles.append(job)\n                continue\n            if job in visited:\n                cycles.append(job)\n                continue\n            try:\n                self.check_periodic_wildcards(job)\n                self.update_(job,\n                             visited=set(visited),\n                             skip_until_dynamic=skip_until_dynamic)\n                # TODO this might fail if a rule discarded here is needed\n                # elsewhere\n                if producer:\n                    if job < producer or self.ignore_ambiguity:\n                        break\n                    elif producer is not None:\n                        raise AmbiguousRuleException(file, job, producer)\n                producer = job\n            except (MissingInputException, CyclicGraphException,\n                    PeriodicWildcardError) as ex:\n                exceptions.append(ex)\n        if producer is None:\n            if cycles:\n                job = cycles[0]\n                raise CyclicGraphException(job.rule, file, rule=job.rule)\n            if exceptions:\n                raise exceptions[0]\n        return producer\n\n    def update_(self, job, visited=None, skip_until_dynamic=False):\n        \"\"\" Update the DAG by adding the given job and its dependencies. \"\"\"\n        if job in self.dependencies:\n            return\n        if visited is None:\n            visited = set()\n        visited.add(job)\n        dependencies = self.dependencies[job]\n        potential_dependencies = self.collect_potential_dependencies(\n            job).items()\n\n        skip_until_dynamic = skip_until_dynamic and not job.dynamic_output\n\n        missing_input = job.missing_input\n        producer = dict()\n        exceptions = dict()\n        for file, jobs in potential_dependencies:\n            try:\n                producer[file] = self.update(\n                    jobs,\n                    file=file,\n                    visited=visited,\n                    skip_until_dynamic=skip_until_dynamic or file in\n                    job.dynamic_input)\n            except (MissingInputException, CyclicGraphException,\n                    PeriodicWildcardError) as ex:\n                if file in missing_input:\n                    self.delete_job(job,\n                                    recursive=False)  # delete job from tree\n                    raise ex\n\n        for file, job_ in producer.items():\n            dependencies[job_].add(file)\n            self.depending[job_][job].add(file)\n\n        missing_input -= producer.keys()\n        if missing_input:\n            self.delete_job(job, recursive=False)  # delete job from tree\n            raise MissingInputException(job.rule, missing_input)\n\n        if skip_until_dynamic:\n            self._dynamic.add(job)\n\n    def update_needrun(self):\n        \"\"\" Update the information whether a job needs to be executed. \"\"\"\n\n        def output_mintime(job):\n            for job_ in self.bfs(self.depending, job):\n                t = job_.output_mintime\n                if t:\n                    return t\n\n        def needrun(job):\n            reason = self.reason(job)\n            noinitreason = not reason\n            updated_subworkflow_input = self.updated_subworkflow_files.intersection(\n                job.input)\n            if (job not in self.omitforce and job.rule in self.forcerules or\n                not self.forcefiles.isdisjoint(job.output)):\n                reason.forced = True\n            elif updated_subworkflow_input:\n                reason.updated_input.update(updated_subworkflow_input)\n            elif job in self.targetjobs:\n                # TODO find a way to handle added/removed input files here?\n                if not job.output and not job.benchmark:\n                    if job.input:\n                        if job.rule.norun:\n                            reason.updated_input_run.update([f\n                                                             for f in job.input\n                                                             if not f.exists])\n                        else:\n                            reason.nooutput = True\n                    else:\n                        reason.noio = True\n                else:\n                    if job.rule in self.targetrules:\n                        missing_output = job.missing_output()\n                    else:\n                        missing_output = job.missing_output(\n                            requested=set(chain(*self.depending[job].values()))\n                            | self.targetfiles)\n                    reason.missing_output.update(missing_output)\n            if not reason:\n                output_mintime_ = output_mintime(job)\n                if output_mintime_:\n                    updated_input = [\n                        f for f in job.input\n                        if f.exists and f.is_newer(output_mintime_)\n                    ]\n                    reason.updated_input.update(updated_input)\n            if noinitreason and reason:\n                reason.derived = False\n            return job\n\n        reason = self.reason\n        _needrun = self._needrun\n        dependencies = self.dependencies\n        depending = self.depending\n\n        _needrun.clear()\n        candidates = set(self.jobs)\n\n        queue = list(filter(reason, map(needrun, candidates)))\n        visited = set(queue)\n        while queue:\n            job = queue.pop(0)\n            _needrun.add(job)\n\n            for job_, files in dependencies[job].items():\n                missing_output = job_.missing_output(requested=files)\n                reason(job_).missing_output.update(missing_output)\n                if missing_output and not job_ in visited:\n                    visited.add(job_)\n                    queue.append(job_)\n\n            for job_, files in depending[job].items():\n                if job_ in candidates:\n                    reason(job_).updated_input_run.update(files)\n                    if not job_ in visited:\n                        visited.add(job_)\n                        queue.append(job_)\n\n        self._len = len(_needrun)\n\n    def update_priority(self):\n        \"\"\" Update job priorities. \"\"\"\n        prioritized = (lambda job: job.rule in self.priorityrules or\n                       not self.priorityfiles.isdisjoint(job.output))\n        for job in self.needrun_jobs:\n            self._priority[job] = job.rule.priority\n        for job in self.bfs(self.dependencies,\n                            *filter(prioritized, self.needrun_jobs),\n                            stop=self.noneedrun_finished):\n            self._priority[job] = Job.HIGHEST_PRIORITY\n\n    def update_ready(self):\n        \"\"\" Update information whether a job is ready to execute. \"\"\"\n        for job in filter(self.needrun, self.jobs):\n            if not self.finished(job) and self._ready(job):\n                self._ready_jobs.add(job)\n\n    def update_downstream_size(self):\n        for job in self.needrun_jobs:\n            self._downstream_size[job] = sum(\n                1 for _ in self.bfs(self.depending, job,\n                                    stop=self.noneedrun_finished)) - 1\n\n    def postprocess(self):\n        self.update_needrun()\n        self.update_priority()\n        self.update_ready()\n        self.update_downstream_size()\n\n    def _ready(self, job):\n        return self._finished.issuperset(\n            filter(self.needrun, self.dependencies[job]))\n\n    def finish(self, job, update_dynamic=True):\n        self._finished.add(job)\n        try:\n            self._ready_jobs.remove(job)\n        except KeyError:\n            pass\n        # mark depending jobs as ready\n        for job_ in self.depending[job]:\n            if self.needrun(job_) and self._ready(job_):\n                self._ready_jobs.add(job_)\n\n        if update_dynamic and job.dynamic_output:\n            logger.info(\"Dynamically updating jobs\")\n            newjob = self.update_dynamic(job)\n            if newjob:\n                # simulate that this job ran and was finished before\n                self.omitforce.add(newjob)\n                self._needrun.add(newjob)\n                self._finished.add(newjob)\n\n                self.postprocess()\n                self.handle_protected(newjob)\n                self.handle_touch(newjob)\n                # add finished jobs to len as they are not counted after new postprocess\n                self._len += len(self._finished)\n\n    def update_dynamic(self, job):\n        dynamic_wildcards = job.dynamic_wildcards\n        if not dynamic_wildcards:\n            # this happens e.g. in dryrun if output is not yet present\n            return\n\n        depending = list(filter(lambda job_: not self.finished(job_),\n                                self.bfs(self.depending, job)))\n        newrule, non_dynamic_wildcards = job.rule.dynamic_branch(\n            dynamic_wildcards,\n            input=False)\n        self.specialize_rule(job.rule, newrule)\n\n        # no targetfile needed for job\n        newjob = Job(newrule, self, format_wildcards=non_dynamic_wildcards)\n        self.replace_job(job, newjob)\n        for job_ in depending:\n            if job_.dynamic_input:\n                newrule_ = job_.rule.dynamic_branch(dynamic_wildcards)\n                if newrule_ is not None:\n                    self.specialize_rule(job_.rule, newrule_)\n                    if not self.dynamic(job_):\n                        logger.debug(\"Updating job {}.\".format(job_))\n                        newjob_ = Job(newrule_, self,\n                                      targetfile=job_.targetfile)\n\n                        unexpected_output = self.reason(\n                            job_).missing_output.intersection(\n                                newjob.existing_output)\n                        if unexpected_output:\n                            logger.warning(\n                                \"Warning: the following output files of rule {} were not \"\n                                \"present when the DAG was created:\\n{}\".format(\n                                    newjob_.rule, unexpected_output))\n\n                        self.replace_job(job_, newjob_)\n        return newjob\n\n    def delete_job(self, job, recursive=True):\n        for job_ in self.depending[job]:\n            del self.dependencies[job_][job]\n        del self.depending[job]\n        for job_ in self.dependencies[job]:\n            depending = self.depending[job_]\n            del depending[job]\n            if not depending and recursive:\n                self.delete_job(job_)\n        del self.dependencies[job]\n        if job in self._needrun:\n            self._len -= 1\n            self._needrun.remove(job)\n            del self._reason[job]\n        if job in self._finished:\n            self._finished.remove(job)\n        if job in self._dynamic:\n            self._dynamic.remove(job)\n        if job in self._ready_jobs:\n            self._ready_jobs.remove(job)\n\n    def replace_job(self, job, newjob):\n        depending = list(self.depending[job].items())\n        if self.finished(job):\n            self._finished.add(newjob)\n\n        self.delete_job(job)\n        self.update([newjob])\n\n        for job_, files in depending:\n            if not job_.dynamic_input:\n                self.dependencies[job_][newjob].update(files)\n                self.depending[newjob][job_].update(files)\n        if job in self.targetjobs:\n            self.targetjobs.remove(job)\n            self.targetjobs.add(newjob)\n\n    def specialize_rule(self, rule, newrule):\n        assert newrule is not None\n        self.rules.add(newrule)\n        self.update_output_index()\n\n    def collect_potential_dependencies(self, job):\n        dependencies = defaultdict(list)\n        # use a set to circumvent multiple jobs for the same file\n        # if user specified it twice\n        file2jobs = self.file2jobs\n        for file in set(job.input):\n            # omit the file if it comes from a subworkflow\n            if file in job.subworkflow_input:\n                continue\n            try:\n                if file in job.dependencies:\n                    jobs = [Job(job.dependencies[file], self, targetfile=file)]\n                else:\n                    jobs = file2jobs(file)\n                dependencies[file].extend(jobs)\n            except MissingRuleException as ex:\n                pass\n        return dependencies\n\n    def bfs(self, direction, *jobs, stop=lambda job: False):\n        queue = list(jobs)\n        visited = set(queue)\n        while queue:\n            job = queue.pop(0)\n            if stop(job):\n                # stop criterion reached for this node\n                continue\n            yield job\n            for job_, _ in direction[job].items():\n                if not job_ in visited:\n                    queue.append(job_)\n                    visited.add(job_)\n\n    def level_bfs(self, direction, *jobs, stop=lambda job: False):\n        queue = [(job, 0) for job in jobs]\n        visited = set(jobs)\n        while queue:\n            job, level = queue.pop(0)\n            if stop(job):\n                # stop criterion reached for this node\n                continue\n            yield level, job\n            level += 1\n            for job_, _ in direction[job].items():\n                if not job_ in visited:\n                    queue.append((job_, level))\n                    visited.add(job_)\n\n    def dfs(self, direction, *jobs, stop=lambda job: False, post=True):\n        visited = set()\n        for job in jobs:\n            for job_ in self._dfs(direction, job, visited,\n                                  stop=stop,\n                                  post=post):\n                yield job_\n\n    def _dfs(self, direction, job, visited, stop, post):\n        if stop(job):\n            return\n        if not post:\n            yield job\n        for job_ in direction[job]:\n            if not job_ in visited:\n                visited.add(job_)\n                for j in self._dfs(direction, job_, visited, stop, post):\n                    yield j\n        if post:\n            yield job\n\n    def is_isomorph(self, job1, job2):\n        if job1.rule != job2.rule:\n            return False\n        rule = lambda job: job.rule.name\n        queue1, queue2 = [job1], [job2]\n        visited1, visited2 = set(queue1), set(queue2)\n        while queue1 and queue2:\n            job1, job2 = queue1.pop(0), queue2.pop(0)\n            deps1 = sorted(self.dependencies[job1], key=rule)\n            deps2 = sorted(self.dependencies[job2], key=rule)\n            for job1_, job2_ in zip(deps1, deps2):\n                if job1_.rule != job2_.rule:\n                    return False\n                if not job1_ in visited1 and not job2_ in visited2:\n                    queue1.append(job1_)\n                    visited1.add(job1_)\n                    queue2.append(job2_)\n                    visited2.add(job2_)\n                elif not (job1_ in visited1 and job2_ in visited2):\n                    return False\n        return True\n\n    def all_longest_paths(self, *jobs):\n        paths = defaultdict(list)\n\n        def all_longest_paths(_jobs):\n            for job in _jobs:\n                if job in paths:\n                    continue\n                deps = self.dependencies[job]\n                if not deps:\n                    paths[job].append([job])\n                    continue\n                all_longest_paths(deps)\n                for _job in deps:\n                    paths[job].extend(path + [job] for path in paths[_job])\n\n        all_longest_paths(jobs)\n        return chain(*(paths[job] for job in jobs))\n\n    def new_wildcards(self, job):\n        new_wildcards = set(job.wildcards.items())\n        for job_ in self.dependencies[job]:\n            if not new_wildcards:\n                return set()\n            for wildcard in job_.wildcards.items():\n                new_wildcards.discard(wildcard)\n        return new_wildcards\n\n    def rule2job(self, targetrule):\n        return Job(targetrule, self)\n\n    def file2jobs(self, targetfile):\n        rules = self.output_index.match(targetfile)\n        jobs = []\n        exceptions = list()\n        for rule in rules:\n            if rule.is_producer(targetfile):\n                try:\n                    jobs.append(Job(rule, self, targetfile=targetfile))\n                except InputFunctionException as e:\n                    exceptions.append(e)\n        if not jobs:\n            if exceptions:\n                raise exceptions[0]\n            raise MissingRuleException(targetfile)\n        return jobs\n\n    def rule_dot2(self):\n        dag = defaultdict(list)\n        visited = set()\n        preselect = set()\n\n        def preselect_parents(job):\n            for parent in self.depending[job]:\n                if parent in preselect:\n                    continue\n                preselect.add(parent)\n                preselect_parents(parent)\n\n        def build_ruledag(job, key=lambda job: job.rule.name):\n            if job in visited:\n                return\n            visited.add(job)\n            deps = sorted(self.dependencies[job], key=key)\n            deps = [(group[0] if preselect.isdisjoint(group) else\n                     preselect.intersection(group).pop())\n                    for group in (list(g) for _, g in groupby(deps, key))]\n            dag[job].extend(deps)\n            preselect_parents(job)\n            for dep in deps:\n                build_ruledag(dep)\n\n        for job in self.targetjobs:\n            build_ruledag(job)\n\n        return self._dot(dag.keys(),\n                         print_wildcards=False,\n                         print_types=False,\n                         dag=dag)\n\n    def rule_dot(self):\n        graph = defaultdict(set)\n        for job in self.jobs:\n            graph[job.rule].update(dep.rule for dep in self.dependencies[job])\n        return self._dot(graph)\n\n    def dot(self):\n        def node2style(job):\n            if not self.needrun(job):\n                return \"rounded,dashed\"\n            if self.dynamic(job) or job.dynamic_input:\n                return \"rounded,dotted\"\n            return \"rounded\"\n\n        def format_wildcard(wildcard):\n            name, value = wildcard\n            if _IOFile.dynamic_fill in value:\n                value = \"...\"\n            return \"{}: {}\".format(name, value)\n\n        node2rule = lambda job: job.rule\n        node2label = lambda job: \"\\\\n\".join(chain([\n            job.rule.name\n        ], sorted(map(format_wildcard, self.new_wildcards(job)))))\n\n        dag = {job: self.dependencies[job] for job in self.jobs}\n\n        return self._dot(dag,\n                         node2rule=node2rule,\n                         node2style=node2style,\n                         node2label=node2label)\n\n    def _dot(self, graph,\n             node2rule=lambda node: node,\n             node2style=lambda node: \"rounded\",\n             node2label=lambda node: node):\n\n        # color rules\n        huefactor = 2 / (3 * len(self.rules))\n        rulecolor = {\n            rule: \"{:.2f} 0.6 0.85\".format(i * huefactor)\n            for i, rule in enumerate(self.rules)\n        }\n\n        # markup\n        node_markup = '\\t{}[label = \"{}\", color = \"{}\", style=\"{}\"];'.format\n        edge_markup = \"\\t{} -> {}\".format\n\n        # node ids\n        ids = {node: i for i, node in enumerate(graph)}\n\n        # calculate nodes\n        nodes = [node_markup(ids[node], node2label(node),\n                             rulecolor[node2rule(node)], node2style(node))\n                 for node in graph]\n        # calculate edges\n        edges = [edge_markup(ids[dep], ids[node])\n                 for node, deps in graph.items() for dep in deps]\n\n        return textwrap.dedent(\"\"\"\\\n            digraph snakemake_dag {{\n                graph[bgcolor=white, margin=0];\n                node[shape=box, style=rounded, fontname=sans, \\\n                fontsize=10, penwidth=2];\n                edge[penwidth=2, color=grey];\n            {items}\n            }}\\\n            \"\"\").format(items=\"\\n\".join(nodes + edges))\n\n    def summary(self, detailed=False):\n        if detailed:\n            yield \"output_file\\tdate\\trule\\tversion\\tinput_file(s)\\tshellcmd\\tstatus\\tplan\"\n        else:\n            yield \"output_file\\tdate\\trule\\tversion\\tstatus\\tplan\"\n\n        for job in self.jobs:\n            output = job.rule.output if self.dynamic(\n                job) else job.expanded_output\n            for f in output:\n                rule = self.workflow.persistence.rule(f)\n                rule = \"-\" if rule is None else rule\n\n                version = self.workflow.persistence.version(f)\n                version = \"-\" if version is None else str(version)\n\n                date = time.ctime(f.mtime) if f.exists else \"-\"\n\n                pending = \"update pending\" if self.reason(job) else \"no update\"\n\n                input = self.workflow.persistence.input(f)\n                input = \"-\" if input is None else \",\".join(input)\n\n                shellcmd = self.workflow.persistence.shellcmd(f)\n                shellcmd = \"-\" if shellcmd is None else shellcmd\n                # remove new line characters, leading and trailing whitespace\n                shellcmd = shellcmd.strip().replace(\"\\n\", \"; \")\n\n                status = \"ok\"\n                if not f.exists:\n                    status = \"missing\"\n                elif self.reason(job).updated_input:\n                    status = \"updated input files\"\n                elif self.workflow.persistence.version_changed(job, file=f):\n                    status = \"version changed to {}\".format(job.rule.version)\n                elif self.workflow.persistence.code_changed(job, file=f):\n                    status = \"rule implementation changed\"\n                elif self.workflow.persistence.input_changed(job, file=f):\n                    status = \"set of input files changed\"\n                elif self.workflow.persistence.params_changed(job, file=f):\n                    status = \"params changed\"\n                if detailed:\n                    yield \"\\t\".join((f, date, rule, version, input, shellcmd,\n                                     status, pending))\n                else:\n                    yield \"\\t\".join((f, date, rule, version, status, pending))\n\n    def d3dag(self, max_jobs=10000):\n        def node(job):\n            jobid = self.jobid(job)\n            return {\n                \"id\": jobid,\n                \"value\": {\n                    \"jobid\": jobid,\n                    \"label\": job.rule.name,\n                    \"rule\": job.rule.name\n                }\n            }\n\n        def edge(a, b):\n            return {\"u\": self.jobid(a), \"v\": self.jobid(b)}\n\n        jobs = list(self.jobs)\n\n        if len(jobs) > max_jobs:\n            logger.info(\n                \"Job-DAG is too large for visualization (>{} jobs).\".format(\n                    max_jobs))\n        else:\n            logger.d3dag(nodes=[node(job) for job in jobs],\n                         edges=[edge(dep, job) for job in jobs for dep in\n                                self.dependencies[job] if self.needrun(dep)])\n\n    def stats(self):\n        rules = Counter()\n        rules.update(job.rule for job in self.needrun_jobs)\n        rules.update(job.rule for job in self.finished_jobs)\n        yield \"Job counts:\"\n        yield \"\\tcount\\tjobs\"\n        for rule, count in sorted(rules.most_common(),\n                                  key=lambda item: item[0].name):\n            yield \"\\t{}\\t{}\".format(count, rule)\n        yield \"\\t{}\".format(len(self))\n\n    def __str__(self):\n        return self.dot()\n\n    def __len__(self):\n        return self._len\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/tianyabeef/gutMicrobiome/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/io.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport stat\nimport time\nimport json\nfrom itertools import product, chain\nfrom collections import Iterable, namedtuple\nfrom snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\nfrom snakemake.logging import logger\n\n\ndef lstat(f):\n    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n\n\ndef lutime(f, times):\n    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)\n\n\ndef lchmod(f, mode):\n    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)\n\n\ndef IOFile(file, rule=None):\n    f = _IOFile(file)\n    f.rule = rule\n    return f\n\n\nclass _IOFile(str):\n    \"\"\"\n    A file that is either input or output of a rule.\n    \"\"\"\n\n    dynamic_fill = \"__snakemake_dynamic__\"\n\n    def __new__(cls, file):\n        obj = str.__new__(cls, file)\n        obj._is_function = type(file).__name__ == \"function\"\n        obj._file = file\n        obj.rule = None\n        obj._regex = None\n        return obj\n\n    @property\n    def file(self):\n        if not self._is_function:\n            return self._file\n        else:\n            raise ValueError(\"This IOFile is specified as a function and \"\n                             \"may not be used directly.\")\n\n    @property\n    def exists(self):\n        return os.path.exists(self.file)\n\n    @property\n    def protected(self):\n        return self.exists and not os.access(self.file, os.W_OK)\n\n    @property\n    def mtime(self):\n        # do not follow symlinks for modification time\n        return lstat(self.file).st_mtime\n\n    @property\n    def size(self):\n        # follow symlinks but throw error if invalid\n        self.check_broken_symlink()\n        return os.path.getsize(self.file)\n\n    def check_broken_symlink(self):\n        \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n        if not self.exists and lstat(self.file):\n            raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n\n    def is_newer(self, time):\n        return self.mtime > time\n\n    def prepare(self):\n        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n        dir = os.path.dirname(path_until_wildcard)\n        if len(dir) > 0 and not os.path.exists(dir):\n            try:\n                os.makedirs(dir)\n            except OSError as e:\n                # ignore Errno 17 \"File exists\" (reason: multiprocessing)\n                if e.errno != 17:\n                    raise e\n\n    def protect(self):\n        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~\n                stat.S_IWOTH)\n        if os.path.isdir(self.file):\n            for root, dirs, files in os.walk(self.file):\n                for d in dirs:\n                    lchmod(os.path.join(self.file, d), mode)\n                for f in files:\n                    lchmod(os.path.join(self.file, f), mode)\n        else:\n            lchmod(self.file, mode)\n\n    def remove(self):\n        remove(self.file)\n\n    def touch(self):\n        try:\n            lutime(self.file, None)\n        except OSError as e:\n            if e.errno == 2:\n                raise MissingOutputException(\n                    \"Output file {} of rule {} shall be touched but \"\n                    \"does not exist.\".format(self.file, self.rule.name),\n                    lineno=self.rule.lineno,\n                    snakefile=self.rule.snakefile)\n            else:\n                raise e\n\n    def touch_or_create(self):\n        try:\n            self.touch()\n        except MissingOutputException:\n            # create empty file\n            with open(self.file, \"w\") as f:\n                pass\n\n    def apply_wildcards(self, wildcards,\n                        fill_missing=False,\n                        fail_dynamic=False):\n        f = self._file\n        if self._is_function:\n            f = self._file(Namedlist(fromdict=wildcards))\n\n        return IOFile(apply_wildcards(f, wildcards,\n                                      fill_missing=fill_missing,\n                                      fail_dynamic=fail_dynamic,\n                                      dynamic_fill=self.dynamic_fill),\n                      rule=self.rule)\n\n    def get_wildcard_names(self):\n        return get_wildcard_names(self.file)\n\n    def contains_wildcard(self):\n        return contains_wildcard(self.file)\n\n    def regex(self):\n        if self._regex is None:\n            # compile a regular expression\n            self._regex = re.compile(regex(self.file))\n        return self._regex\n\n    def constant_prefix(self):\n        first_wildcard = _wildcard_regex.search(self.file)\n        if first_wildcard:\n            return self.file[:first_wildcard.start()]\n        return self.file\n\n    def match(self, target):\n        return self.regex().match(target) or None\n\n    def format_dynamic(self):\n        return self.replace(self.dynamic_fill, \"{*}\")\n\n    def __eq__(self, other):\n        f = other._file if isinstance(other, _IOFile) else other\n        return self._file == f\n\n    def __hash__(self):\n        return self._file.__hash__()\n\n\n_wildcard_regex = re.compile(\n    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\")\n\n#    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>[^\\}]*))?\\s*\\}\")\n\n\ndef wait_for_files(files, latency_wait=3):\n    \"\"\"Wait for given files to be present in filesystem.\"\"\"\n    files = list(files)\n    get_missing = lambda: [f for f in files if not os.path.exists(f)]\n    missing = get_missing()\n    if missing:\n        logger.info(\"Waiting at most {} seconds for missing files.\".format(\n            latency_wait))\n        for _ in range(latency_wait):\n            if not get_missing():\n                return\n            time.sleep(1)\n        raise IOError(\"Missing files after {} seconds:\\n{}\".format(\n            latency_wait, \"\\n\".join(get_missing())))\n\n\ndef get_wildcard_names(pattern):\n    return set(match.group('name')\n               for match in _wildcard_regex.finditer(pattern))\n\n\ndef contains_wildcard(path):\n    return _wildcard_regex.search(path) is not None\n\n\ndef remove(file):\n    if os.path.exists(file):\n        if os.path.isdir(file):\n            try:\n                os.removedirs(file)\n            except OSError:\n                # ignore non empty directories\n                pass\n        else:\n            os.remove(file)\n\n\ndef regex(filepattern):\n    f = []\n    last = 0\n    wildcards = set()\n    for match in _wildcard_regex.finditer(filepattern):\n        f.append(re.escape(filepattern[last:match.start()]))\n        wildcard = match.group(\"name\")\n        if wildcard in wildcards:\n            if match.group(\"constraint\"):\n                raise ValueError(\n                    \"If multiple wildcards of the same name \"\n                    \"appear in a string, eventual constraints have to be defined \"\n                    \"at the first occurence and will be inherited by the others.\")\n            f.append(\"(?P={})\".format(wildcard))\n        else:\n            wildcards.add(wildcard)\n            f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if\n                                         match.group(\"constraint\") else \".+\"))\n        last = match.end()\n    f.append(re.escape(filepattern[last:]))\n    f.append(\"$\")  # ensure that the match spans the whole file\n    return \"\".join(f)\n\n\ndef apply_wildcards(pattern, wildcards,\n                    fill_missing=False,\n                    fail_dynamic=False,\n                    dynamic_fill=None,\n                    keep_dynamic=False):\n    def format_match(match):\n        name = match.group(\"name\")\n        try:\n            value = wildcards[name]\n            if fail_dynamic and value == dynamic_fill:\n                raise WildcardError(name)\n            return str(value)  # convert anything into a str\n        except KeyError as ex:\n            if keep_dynamic:\n                return \"{{{}}}\".format(name)\n            elif fill_missing:\n                return dynamic_fill\n            else:\n                raise WildcardError(str(ex))\n\n    return re.sub(_wildcard_regex, format_match, pattern)\n\n\ndef not_iterable(value):\n    return isinstance(value, str) or not isinstance(value, Iterable)\n\n\nclass AnnotatedString(str):\n    def __init__(self, value):\n        self.flags = dict()\n\n\ndef flag(value, flag_type, flag_value=True):\n    if isinstance(value, AnnotatedString):\n        value.flags[flag_type] = flag_value\n        return value\n    if not_iterable(value):\n        value = AnnotatedString(value)\n        value.flags[flag_type] = flag_value\n        return value\n    return [flag(v, flag_type, flag_value=flag_value) for v in value]\n\n\ndef is_flagged(value, flag):\n    if isinstance(value, AnnotatedString):\n        return flag in value.flags\n    return False\n\n\ndef temp(value):\n    \"\"\"\n    A flag for an input or output file that shall be removed after usage.\n    \"\"\"\n    if is_flagged(value, \"protected\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"temp\")\n\n\ndef temporary(value):\n    \"\"\" An alias for temp. \"\"\"\n    return temp(value)\n\n\ndef protected(value):\n    \"\"\" A flag for a file that shall be write protected after creation. \"\"\"\n    if is_flagged(value, \"temp\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"protected\")\n\n\ndef dynamic(value):\n    \"\"\"\n    A flag for a file that shall be dynamic, i.e. the multiplicity\n    (and wildcard values) will be expanded after a certain\n    rule has been run \"\"\"\n    annotated = flag(value, \"dynamic\")\n    tocheck = [annotated] if not_iterable(annotated) else annotated\n    for file in tocheck:\n        matches = list(_wildcard_regex.finditer(file))\n        #if len(matches) != 1:\n        #    raise SyntaxError(\"Dynamic files need exactly one wildcard.\")\n        for match in matches:\n            if match.group(\"constraint\"):\n                raise SyntaxError(\n                    \"The wildcards in dynamic files cannot be constrained.\")\n    return annotated\n\n\ndef touch(value):\n    return flag(value, \"touch\")\n\n\ndef expand(*args, **wildcards):\n    \"\"\"\n    Expand wildcards in given filepatterns.\n\n    Arguments\n    *args -- first arg: filepatterns as list or one single filepattern,\n        second arg (optional): a function to combine wildcard values\n        (itertools.product per default)\n    **wildcards -- the wildcards as keyword arguments\n        with their values as lists\n    \"\"\"\n    filepatterns = args[0]\n    if len(args) == 1:\n        combinator = product\n    elif len(args) == 2:\n        combinator = args[1]\n    if isinstance(filepatterns, str):\n        filepatterns = [filepatterns]\n\n    def flatten(wildcards):\n        for wildcard, values in wildcards.items():\n            if isinstance(values, str) or not isinstance(values, Iterable):\n                values = [values]\n            yield [(wildcard, value) for value in values]\n\n    try:\n        return [filepattern.format(**comb)\n                for comb in map(dict, combinator(*flatten(wildcards))) for\n                filepattern in filepatterns]\n    except KeyError as e:\n        raise WildcardError(\"No values given for wildcard {}.\".format(e))\n\n\ndef limit(pattern, **wildcards):\n    \"\"\"\n    Limit wildcards to the given values.\n\n    Arguments:\n    **wildcards -- the wildcards as keyword arguments\n                   with their values as lists\n    \"\"\"\n    return pattern.format(**{\n        wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values))\n        for wildcard, values in wildcards.items()\n    })\n\n\ndef glob_wildcards(pattern):\n    \"\"\"\n    Glob the values of the wildcards by matching the given pattern to the filesystem.\n    Returns a named tuple with a list of values for each wildcard.\n    \"\"\"\n    pattern = os.path.normpath(pattern)\n    first_wildcard = re.search(\"{[^{]\", pattern)\n    dirname = os.path.dirname(pattern[:first_wildcard.start(\n    )]) if first_wildcard else os.path.dirname(pattern)\n    if not dirname:\n        dirname = \".\"\n\n    names = [match.group('name')\n             for match in _wildcard_regex.finditer(pattern)]\n    Wildcards = namedtuple(\"Wildcards\", names)\n    wildcards = Wildcards(*[list() for name in names])\n\n    pattern = re.compile(regex(pattern))\n    for dirpath, dirnames, filenames in os.walk(dirname):\n        for f in chain(filenames, dirnames):\n            if dirpath != \".\":\n                f = os.path.join(dirpath, f)\n            match = re.match(pattern, f)\n            if match:\n                for name, value in match.groupdict().items():\n                    getattr(wildcards, name).append(value)\n    return wildcards\n\n\n# TODO rewrite Namedlist!\nclass Namedlist(list):\n    \"\"\"\n    A list that additionally provides functions to name items. Further,\n    it is hashable, however the hash does not consider the item names.\n    \"\"\"\n\n    def __init__(self, toclone=None, fromdict=None, plainstr=False):\n        \"\"\"\n        Create the object.\n\n        Arguments\n        toclone  -- another Namedlist that shall be cloned\n        fromdict -- a dict that shall be converted to a\n            Namedlist (keys become names)\n        \"\"\"\n        list.__init__(self)\n        self._names = dict()\n\n        if toclone:\n            self.extend(map(str, toclone) if plainstr else toclone)\n            if isinstance(toclone, Namedlist):\n                self.take_names(toclone.get_names())\n        if fromdict:\n            for key, item in fromdict.items():\n                self.append(item)\n                self.add_name(key)\n\n    def add_name(self, name):\n        \"\"\"\n        Add a name to the last item.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        self.set_name(name, len(self) - 1)\n\n    def set_name(self, name, index, end=None):\n        \"\"\"\n        Set the name of an item.\n\n        Arguments\n        name  -- a name\n        index -- the item index\n        \"\"\"\n        self._names[name] = (index, end)\n        if end is None:\n            setattr(self, name, self[index])\n        else:\n            setattr(self, name, Namedlist(toclone=self[index:end]))\n\n    def get_names(self):\n        \"\"\"\n        Get the defined names as (name, index) pairs.\n        \"\"\"\n        for name, index in self._names.items():\n            yield name, index\n\n    def take_names(self, names):\n        \"\"\"\n        Take over the given names.\n\n        Arguments\n        names -- the given names as (name, index) pairs\n        \"\"\"\n        for name, (i, j) in names:\n            self.set_name(name, i, end=j)\n\n    def items(self):\n        for name in self._names:\n            yield name, getattr(self, name)\n\n    def allitems(self):\n        next = 0\n        for name, index in sorted(self._names.items(),\n                                  key=lambda item: item[1][0]):\n            start, end = index\n            if end is None:\n                end = start + 1\n            if start > next:\n                for item in self[next:start]:\n                    yield None, item\n            yield name, getattr(self, name)\n            next = end\n        for item in self[next:]:\n            yield None, item\n\n    def insert_items(self, index, items):\n        self[index:index + 1] = items\n        add = len(items) - 1\n        for name, (i, j) in self._names.items():\n            if i > index:\n                self._names[name] = (i + add, j + add)\n            elif i == index:\n                self.set_name(name, i, end=i + len(items))\n\n    def keys(self):\n        return self._names\n\n    def plainstrings(self):\n        return self.__class__.__call__(toclone=self, plainstr=True)\n\n    def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)\n        except TypeError:\n            pass\n        return getattr(self, key)\n\n    def __hash__(self):\n        return hash(tuple(self))\n\n    def __str__(self):\n        return \" \".join(map(str, self))\n\n\nclass InputFiles(Namedlist):\n    pass\n\n\nclass OutputFiles(Namedlist):\n    pass\n\n\nclass Wildcards(Namedlist):\n    pass\n\n\nclass Params(Namedlist):\n    pass\n\n\nclass Resources(Namedlist):\n    pass\n\n\nclass Log(Namedlist):\n    pass\n\n\ndef _load_configfile(configpath):\n    \"Tries to load a configfile first as JSON, then as YAML, into a dict.\"\n    try:\n        with open(configpath) as f:\n            try:\n                return json.load(f)\n            except ValueError:\n                f.seek(0)  # try again\n            try:\n                import yaml\n            except ImportError:\n                raise WorkflowError(\"Config file is not valid JSON and PyYAML \"\n                                    \"has not been installed. Please install \"\n                                    \"PyYAML to use YAML config files.\")\n            try:\n                return yaml.load(f)\n            except yaml.YAMLError:\n                raise WorkflowError(\"Config file is not valid JSON or YAML.\")\n    except FileNotFoundError:\n        raise WorkflowError(\"Config file {} not found.\".format(configpath))\n\n\ndef load_configfile(configpath):\n    \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\"\n    config = _load_configfile(configpath)\n    if not isinstance(config, dict):\n        raise WorkflowError(\"Config file must be given as JSON or YAML \"\n                            \"with keys at top level.\")\n    return config\n\n##### Wildcard pumping detection #####\n\n\nclass PeriodicityDetector:\n    def __init__(self, min_repeat=50, max_repeat=100):\n        \"\"\"\n        Args:\n            max_len (int): The maximum length of the periodic substring.\n        \"\"\"\n        self.regex = re.compile(\n            \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format(\n                min_repeat=min_repeat - 1,\n                max_repeat=max_repeat - 1))\n\n    def is_periodic(self, value):\n        \"\"\"Returns the periodic substring or None if not periodic.\"\"\"\n        m = self.regex.search(value)  # search for a periodic suffix.\n        if m is not None:\n            return m.group(\"value\")\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/tianyabeef/gutMicrobiome/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/jobs.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport sys\nimport base64\nimport json\n\nfrom collections import defaultdict\nfrom itertools import chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.io import IOFile, Wildcards, Resources, _IOFile\nfrom snakemake.utils import format, listfiles\nfrom snakemake.exceptions import RuleException, ProtectedOutputException\nfrom snakemake.exceptions import UnexpectedOutputException\nfrom snakemake.logging import logger\n\n\ndef jobfiles(jobs, type):\n    return chain(*map(attrgetter(type), jobs))\n\n\nclass Job:\n    HIGHEST_PRIORITY = sys.maxsize\n\n    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):\n        self.rule = rule\n        self.dag = dag\n        self.targetfile = targetfile\n\n        self.wildcards_dict = self.rule.get_wildcards(targetfile)\n        self.wildcards = Wildcards(fromdict=self.wildcards_dict)\n        self._format_wildcards = (self.wildcards if format_wildcards is None\n                                  else Wildcards(fromdict=format_wildcards))\n\n        (self.input, self.output, self.params, self.log, self.benchmark,\n         self.ruleio,\n         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)\n\n        self.resources_dict = {\n            name: min(self.rule.workflow.global_resources.get(name, res), res)\n            for name, res in rule.resources.items()\n        }\n        self.threads = self.resources_dict[\"_cores\"]\n        self.resources = Resources(fromdict=self.resources_dict)\n        self._inputsize = None\n\n        self.dynamic_output, self.dynamic_input = set(), set()\n        self.temp_output, self.protected_output = set(), set()\n        self.touch_output = set()\n        self.subworkflow_input = dict()\n        for f in self.output:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_output:\n                self.dynamic_output.add(f)\n            if f_ in self.rule.temp_output:\n                self.temp_output.add(f)\n            if f_ in self.rule.protected_output:\n                self.protected_output.add(f)\n            if f_ in self.rule.touch_output:\n                self.touch_output.add(f)\n        for f in self.input:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_input:\n                self.dynamic_input.add(f)\n            if f_ in self.rule.subworkflow_input:\n                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]\n        self._hash = self.rule.__hash__()\n        if True or not self.dynamic_output:\n            for o in self.output:\n                self._hash ^= o.__hash__()\n\n    @property\n    def priority(self):\n        return self.dag.priority(self)\n\n    @property\n    def b64id(self):\n        return base64.b64encode((self.rule.name + \"\".join(self.output)\n                                 ).encode(\"utf-8\")).decode(\"utf-8\")\n\n    @property\n    def inputsize(self):\n        \"\"\"\n        Return the size of the input files.\n        Input files need to be present.\n        \"\"\"\n        if self._inputsize is None:\n            self._inputsize = sum(f.size for f in self.input)\n        return self._inputsize\n\n    @property\n    def message(self):\n        \"\"\" Return the message for this job. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.message) if\n                    self.rule.message else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable in message \"\n                                \"of shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def shellcmd(self):\n        \"\"\" Return the shell command. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.shellcmd) if\n                    self.rule.shellcmd else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable when printing \"\n                                \"shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def expanded_output(self):\n        \"\"\" Iterate over output files while dynamic output is expanded. \"\"\"\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                expansion = self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill)\n                if not expansion:\n                    yield f_\n                for f, _ in expansion:\n                    yield IOFile(f, self.rule)\n            else:\n                yield f\n\n    @property\n    def dynamic_wildcards(self):\n        \"\"\" Return all wildcard values determined from dynamic output. \"\"\"\n        combinations = set()\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                for f, w in self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill):\n                    combinations.add(tuple(w.items()))\n        wildcards = defaultdict(list)\n        for combination in combinations:\n            for name, value in combination:\n                wildcards[name].append(value)\n        return wildcards\n\n    @property\n    def missing_input(self):\n        \"\"\" Return missing input files. \"\"\"\n        # omit file if it comes from a subworkflow\n        return set(f for f in self.input\n                   if not f.exists and not f in self.subworkflow_input)\n\n    @property\n    def output_mintime(self):\n        \"\"\" Return oldest output file. \"\"\"\n        existing = [f.mtime for f in self.expanded_output if f.exists]\n        if self.benchmark and self.benchmark.exists:\n            existing.append(self.benchmark.mtime)\n        if existing:\n            return min(existing)\n        return None\n\n    @property\n    def input_maxtime(self):\n        \"\"\" Return newest input file. \"\"\"\n        existing = [f.mtime for f in self.input if f.exists]\n        if existing:\n            return max(existing)\n        return None\n\n    def missing_output(self, requested=None):\n        \"\"\" Return missing output files. \"\"\"\n        files = set()\n        if self.benchmark and (requested is None or\n                               self.benchmark in requested):\n            if not self.benchmark.exists:\n                files.add(self.benchmark)\n\n        for f, f_ in zip(self.output, self.rule.output):\n            if requested is None or f in requested:\n                if f in self.dynamic_output:\n                    if not self.expand_dynamic(\n                        f_,\n                        restriction=self.wildcards,\n                        omit_value=_IOFile.dynamic_fill):\n                        files.add(\"{} (dynamic)\".format(f_))\n                elif not f.exists:\n                    files.add(f)\n        return files\n\n    @property\n    def existing_output(self):\n        return filter(lambda f: f.exists, self.expanded_output)\n\n    def check_protected_output(self):\n        protected = list(filter(lambda f: f.protected, self.expanded_output))\n        if protected:\n            raise ProtectedOutputException(self.rule, protected)\n\n    def prepare(self):\n        \"\"\"\n        Prepare execution of job.\n        This includes creation of directories and deletion of previously\n        created dynamic files.\n        \"\"\"\n\n        self.check_protected_output()\n\n        unexpected_output = self.dag.reason(self).missing_output.intersection(\n            self.existing_output)\n        if unexpected_output:\n            logger.warning(\n                \"Warning: the following output files of rule {} were not \"\n                \"present when the DAG was created:\\n{}\".format(\n                    self.rule, unexpected_output))\n\n        if self.dynamic_output:\n            for f, _ in chain(*map(partial(self.expand_dynamic,\n                                           restriction=self.wildcards,\n                                           omit_value=_IOFile.dynamic_fill),\n                                   self.rule.dynamic_output)):\n                os.remove(f)\n        for f, f_ in zip(self.output, self.rule.output):\n            f.prepare()\n        for f in self.log:\n            f.prepare()\n        if self.benchmark:\n            self.benchmark.prepare()\n\n    def cleanup(self):\n        \"\"\" Cleanup output files. \"\"\"\n        to_remove = [f for f in self.expanded_output if f.exists]\n        if to_remove:\n            logger.info(\"Removing output files of failed job {}\"\n                        \" since they might be corrupted:\\n{}\".format(\n                            self, \", \".join(to_remove)))\n            for f in to_remove:\n                f.remove()\n\n    def format_wildcards(self, string, **variables):\n        \"\"\" Format a string with variables from the job. \"\"\"\n        _variables = dict()\n        _variables.update(self.rule.workflow.globals)\n        _variables.update(dict(input=self.input,\n                               output=self.output,\n                               params=self.params,\n                               wildcards=self._format_wildcards,\n                               threads=self.threads,\n                               resources=self.resources,\n                               log=self.log,\n                               version=self.rule.version,\n                               rule=self.rule.name, ))\n        _variables.update(variables)\n        try:\n            return format(string, **_variables)\n        except NameError as ex:\n            raise RuleException(\"NameError: \" + str(ex), rule=self.rule)\n        except IndexError as ex:\n            raise RuleException(\"IndexError: \" + str(ex), rule=self.rule)\n\n    def properties(self, omit_resources=\"_cores _nodes\".split()):\n        resources = {\n            name: res\n            for name, res in self.resources.items()\n            if name not in omit_resources\n        }\n        params = {name: value for name, value in self.params.items()}\n        properties = {\n            \"rule\": self.rule.name,\n            \"local\": self.dag.workflow.is_local(self.rule),\n            \"input\": self.input,\n            \"output\": self.output,\n            \"params\": params,\n            \"threads\": self.threads,\n            \"resources\": resources\n        }\n        return properties\n\n    def json(self):\n        return json.dumps(self.properties())\n\n    def __repr__(self):\n        return self.rule.name\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return self.rule == other.rule and (\n            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)\n\n    def __lt__(self, other):\n        return self.rule.__lt__(other.rule)\n\n    def __gt__(self, other):\n        return self.rule.__gt__(other.rule)\n\n    def __hash__(self):\n        return self._hash\n\n    @staticmethod\n    def expand_dynamic(pattern, restriction=None, omit_value=None):\n        \"\"\" Expand dynamic files. \"\"\"\n        return list(listfiles(pattern,\n                              restriction=restriction,\n                              omit_value=omit_value))\n\n\nclass Reason:\n    def __init__(self):\n        self.updated_input = set()\n        self.updated_input_run = set()\n        self.missing_output = set()\n        self.incomplete_output = set()\n        self.forced = False\n        self.noio = False\n        self.nooutput = False\n        self.derived = True\n\n    def __str__(self):\n        s = list()\n        if self.forced:\n            s.append(\"Forced execution\")\n        else:\n            if self.noio:\n                s.append(\"Rules with neither input nor \"\n                         \"output files are always executed.\")\n            elif self.nooutput:\n                s.append(\"Rules with a run or shell declaration but no output \"\n                         \"are always executed.\")\n            else:\n                if self.missing_output:\n                    s.append(\"Missing output files: {}\".format(\n                        \", \".join(self.missing_output)))\n                if self.incomplete_output:\n                    s.append(\"Incomplete output files: {}\".format(\n                        \", \".join(self.incomplete_output)))\n                updated_input = self.updated_input - self.updated_input_run\n                if updated_input:\n                    s.append(\"Updated input files: {}\".format(\n                        \", \".join(updated_input)))\n                if self.updated_input_run:\n                    s.append(\"Input files updated by another job: {}\".format(\n                        \", \".join(self.updated_input_run)))\n        s = \"; \".join(s)\n        return s\n\n    def __bool__(self):\n        return bool(self.updated_input or self.missing_output or self.forced or\n                    self.updated_input_run or self.noio or self.nooutput)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/tianyabeef/gutMicrobiome/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/rules.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport sys\nimport inspect\nimport sre_constants\nfrom collections import defaultdict\n\nfrom snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist\nfrom snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log\nfrom snakemake.io import apply_wildcards, is_flagged, not_iterable\nfrom snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException\n\n\nclass Rule:\n    def __init__(self, *args, lineno=None, snakefile=None):\n        \"\"\"\n        Create a rule\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if len(args) == 2:\n            name, workflow = args\n            self.name = name\n            self.workflow = workflow\n            self.docstring = None\n            self.message = None\n            self._input = InputFiles()\n            self._output = OutputFiles()\n            self._params = Params()\n            self.dependencies = dict()\n            self.dynamic_output = set()\n            self.dynamic_input = set()\n            self.temp_output = set()\n            self.protected_output = set()\n            self.touch_output = set()\n            self.subworkflow_input = dict()\n            self.resources = dict(_cores=1, _nodes=1)\n            self.priority = 0\n            self.version = None\n            self._log = Log()\n            self._benchmark = None\n            self.wildcard_names = set()\n            self.lineno = lineno\n            self.snakefile = snakefile\n            self.run_func = None\n            self.shellcmd = None\n            self.norun = False\n        elif len(args) == 1:\n            other = args[0]\n            self.name = other.name\n            self.workflow = other.workflow\n            self.docstring = other.docstring\n            self.message = other.message\n            self._input = InputFiles(other._input)\n            self._output = OutputFiles(other._output)\n            self._params = Params(other._params)\n            self.dependencies = dict(other.dependencies)\n            self.dynamic_output = set(other.dynamic_output)\n            self.dynamic_input = set(other.dynamic_input)\n            self.temp_output = set(other.temp_output)\n            self.protected_output = set(other.protected_output)\n            self.touch_output = set(other.touch_output)\n            self.subworkflow_input = dict(other.subworkflow_input)\n            self.resources = other.resources\n            self.priority = other.priority\n            self.version = other.version\n            self._log = other._log\n            self._benchmark = other._benchmark\n            self.wildcard_names = set(other.wildcard_names)\n            self.lineno = other.lineno\n            self.snakefile = other.snakefile\n            self.run_func = other.run_func\n            self.shellcmd = other.shellcmd\n            self.norun = other.norun\n\n    def dynamic_branch(self, wildcards, input=True):\n        def get_io(rule):\n            return (rule.input, rule.dynamic_input) if input else (\n                rule.output, rule.dynamic_output\n            )\n\n        io, dynamic_io = get_io(self)\n\n        branch = Rule(self)\n        io_, dynamic_io_ = get_io(branch)\n\n        expansion = defaultdict(list)\n        for i, f in enumerate(io):\n            if f in dynamic_io:\n                try:\n                    for e in reversed(expand(f, zip, **wildcards)):\n                        expansion[i].append(IOFile(e, rule=branch))\n                except KeyError:\n                    return None\n\n        # replace the dynamic files with the expanded files\n        replacements = [(i, io[i], e)\n                        for i, e in reversed(list(expansion.items()))]\n        for i, old, exp in replacements:\n            dynamic_io_.remove(old)\n            io_.insert_items(i, exp)\n\n        if not input:\n            for i, old, exp in replacements:\n                if old in branch.temp_output:\n                    branch.temp_output.discard(old)\n                    branch.temp_output.update(exp)\n                if old in branch.protected_output:\n                    branch.protected_output.discard(old)\n                    branch.protected_output.update(exp)\n                if old in branch.touch_output:\n                    branch.touch_output.discard(old)\n                    branch.touch_output.update(exp)\n\n            branch.wildcard_names.clear()\n            non_dynamic_wildcards = dict((name, values[0])\n                                         for name, values in wildcards.items()\n                                         if len(set(values)) == 1)\n            # TODO have a look into how to concretize dependencies here\n            (branch._input, branch._output, branch._params, branch._log,\n             branch._benchmark, _, branch.dependencies\n             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)\n            return branch, non_dynamic_wildcards\n        return branch\n\n    def has_wildcards(self):\n        \"\"\"\n        Return True if rule contains wildcards.\n        \"\"\"\n        return bool(self.wildcard_names)\n\n    @property\n    def benchmark(self):\n        return self._benchmark\n\n    @benchmark.setter\n    def benchmark(self, benchmark):\n        self._benchmark = IOFile(benchmark, rule=self)\n\n    @property\n    def input(self):\n        return self._input\n\n    def set_input(self, *input, **kwinput):\n        \"\"\"\n        Add a list of input files. Recursive lists are flattened.\n\n        Arguments\n        input -- the list of input files\n        \"\"\"\n        for item in input:\n            self._set_inoutput_item(item)\n        for name, item in kwinput.items():\n            self._set_inoutput_item(item, name=name)\n\n    @property\n    def output(self):\n        return self._output\n\n    @property\n    def products(self):\n        products = list(self.output)\n        if self.benchmark:\n            products.append(self.benchmark)\n        return products\n\n    def set_output(self, *output, **kwoutput):\n        \"\"\"\n        Add a list of output files. Recursive lists are flattened.\n\n        Arguments\n        output -- the list of output files\n        \"\"\"\n        for item in output:\n            self._set_inoutput_item(item, output=True)\n        for name, item in kwoutput.items():\n            self._set_inoutput_item(item, output=True, name=name)\n\n        for item in self.output:\n            if self.dynamic_output and item not in self.dynamic_output:\n                raise SyntaxError(\n                    \"A rule with dynamic output may not define any \"\n                    \"non-dynamic output files.\")\n            wildcards = item.get_wildcard_names()\n            if self.wildcard_names:\n                if self.wildcard_names != wildcards:\n                    raise SyntaxError(\n                        \"Not all output files of rule {} \"\n                        \"contain the same wildcards.\".format(self.name))\n            else:\n                self.wildcard_names = wildcards\n\n    def _set_inoutput_item(self, item, output=False, name=None):\n        \"\"\"\n        Set an item to be input or output.\n\n        Arguments\n        item     -- the item\n        inoutput -- either a Namedlist of input or output items\n        name     -- an optional name for the item\n        \"\"\"\n        inoutput = self.output if output else self.input\n        if isinstance(item, str):\n            # add the rule to the dependencies\n            if isinstance(item, _IOFile):\n                self.dependencies[item] = item.rule\n            _item = IOFile(item, rule=self)\n            if is_flagged(item, \"temp\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be temporary\")\n                self.temp_output.add(_item)\n            if is_flagged(item, \"protected\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be protected\")\n                self.protected_output.add(_item)\n            if is_flagged(item, \"touch\"):\n                if not output:\n                    raise SyntaxError(\n                        \"Only output files may be marked for touching.\")\n                self.touch_output.add(_item)\n            if is_flagged(item, \"dynamic\"):\n                if output:\n                    self.dynamic_output.add(_item)\n                else:\n                    self.dynamic_input.add(_item)\n            if is_flagged(item, \"subworkflow\"):\n                if output:\n                    raise SyntaxError(\n                        \"Only input files may refer to a subworkflow\")\n                else:\n                    # record the workflow this item comes from\n                    self.subworkflow_input[_item] = item.flags[\"subworkflow\"]\n            inoutput.append(_item)\n            if name:\n                inoutput.add_name(name)\n        elif callable(item):\n            if output:\n                raise SyntaxError(\n                    \"Only input files can be specified as functions\")\n            inoutput.append(item)\n            if name:\n                inoutput.add_name(name)\n        else:\n            try:\n                start = len(inoutput)\n                for i in item:\n                    self._set_inoutput_item(i, output=output)\n                if name:\n                    # if the list was named, make it accessible\n                    inoutput.set_name(name, start, end=len(inoutput))\n            except TypeError:\n                raise SyntaxError(\n                    \"Input and output files have to be specified as strings or lists of strings.\")\n\n    @property\n    def params(self):\n        return self._params\n\n    def set_params(self, *params, **kwparams):\n        for item in params:\n            self._set_params_item(item)\n        for name, item in kwparams.items():\n            self._set_params_item(item, name=name)\n\n    def _set_params_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.params.append(item)\n            if name:\n                self.params.add_name(name)\n        else:\n            try:\n                start = len(self.params)\n                for i in item:\n                    self._set_params_item(i)\n                if name:\n                    self.params.set_name(name, start, end=len(self.params))\n            except TypeError:\n                raise SyntaxError(\"Params have to be specified as strings.\")\n\n    @property\n    def log(self):\n        return self._log\n\n    def set_log(self, *logs, **kwlogs):\n        for item in logs:\n            self._set_log_item(item)\n        for name, item in kwlogs.items():\n            self._set_log_item(item, name=name)\n\n    def _set_log_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.log.append(IOFile(item,\n                                   rule=self)\n                            if isinstance(item, str) else item)\n            if name:\n                self.log.add_name(name)\n        else:\n            try:\n                start = len(self.log)\n                for i in item:\n                    self._set_log_item(i)\n                if name:\n                    self.log.set_name(name, start, end=len(self.log))\n            except TypeError:\n                raise SyntaxError(\"Log files have to be specified as strings.\")\n\n    def expand_wildcards(self, wildcards=None):\n        \"\"\"\n        Expand wildcards depending on the requested output\n        or given wildcards dict.\n        \"\"\"\n\n        def concretize_iofile(f, wildcards):\n            if not isinstance(f, _IOFile):\n                return IOFile(f, rule=self)\n            else:\n                return f.apply_wildcards(wildcards,\n                                         fill_missing=f in self.dynamic_input,\n                                         fail_dynamic=self.dynamic_output)\n\n        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,\n                             concretize=apply_wildcards,\n                             ruleio=None):\n            for name, item in olditems.allitems():\n                start = len(newitems)\n                is_iterable = True\n                if callable(item):\n                    try:\n                        item = item(wildcards_obj)\n                    except (Exception, BaseException) as e:\n                        raise InputFunctionException(e, rule=self)\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        if not isinstance(item_, str):\n                            raise RuleException(\n                                \"Input function did not return str or list of str.\",\n                                rule=self)\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                else:\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                if name:\n                    newitems.set_name(\n                        name, start,\n                        end=len(newitems) if is_iterable else None)\n\n        if wildcards is None:\n            wildcards = dict()\n        missing_wildcards = self.wildcard_names - set(wildcards.keys())\n\n        if missing_wildcards:\n            raise RuleException(\n                \"Could not resolve wildcards in rule {}:\\n{}\".format(\n                    self.name, \"\\n\".join(self.wildcard_names)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n        ruleio = dict()\n\n        try:\n            input = InputFiles()\n            wildcards_obj = Wildcards(fromdict=wildcards)\n            _apply_wildcards(input, self.input, wildcards, wildcards_obj,\n                             concretize=concretize_iofile,\n                             ruleio=ruleio)\n\n            params = Params()\n            _apply_wildcards(params, self.params, wildcards, wildcards_obj)\n\n            output = OutputFiles(o.apply_wildcards(wildcards)\n                                 for o in self.output)\n            output.take_names(self.output.get_names())\n\n            dependencies = {\n                None if f is None else f.apply_wildcards(wildcards): rule\n                for f, rule in self.dependencies.items()\n            }\n\n            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))\n\n            log = Log()\n            _apply_wildcards(log, self.log, wildcards, wildcards_obj,\n                             concretize=concretize_iofile)\n\n            benchmark = self.benchmark.apply_wildcards(\n                wildcards) if self.benchmark else None\n            return input, output, params, log, benchmark, ruleio, dependencies\n        except WildcardError as ex:\n            # this can only happen if an input contains an unresolved wildcard.\n            raise RuleException(\n                \"Wildcards in input, params, log or benchmark file of rule {} cannot be \"\n                \"determined from output files:\\n{}\".format(self, str(ex)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n    def is_producer(self, requested_output):\n        \"\"\"\n        Returns True if this rule is a producer of the requested output.\n        \"\"\"\n        try:\n            for o in self.products:\n                if o.match(requested_output):\n                    return True\n            return False\n        except sre_constants.error as ex:\n            raise IOFileException(\"{} in wildcard statement\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n        except ValueError as ex:\n            raise IOFileException(\"{}\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n\n    def get_wildcards(self, requested_output):\n        \"\"\"\n        Update the given wildcard dictionary by matching regular expression\n        output files to the requested concrete ones.\n\n        Arguments\n        wildcards -- a dictionary of wildcards\n        requested_output -- a concrete filepath\n        \"\"\"\n        if requested_output is None:\n            return dict()\n        bestmatchlen = 0\n        bestmatch = None\n\n        for o in self.products:\n            match = o.match(requested_output)\n            if match:\n                l = self.get_wildcard_len(match.groupdict())\n                if not bestmatch or bestmatchlen > l:\n                    bestmatch = match.groupdict()\n                    bestmatchlen = l\n        return bestmatch\n\n    @staticmethod\n    def get_wildcard_len(wildcards):\n        \"\"\"\n        Return the length of the given wildcard values.\n\n        Arguments\n        wildcards -- a dict of wildcards\n        \"\"\"\n        return sum(map(len, wildcards.values()))\n\n    def __lt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp < 0\n\n    def __gt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp > 0\n\n    def __str__(self):\n        return self.name\n\n    def __hash__(self):\n        return self.name.__hash__()\n\n    def __eq__(self, other):\n        return self.name == other.name\n\n\nclass Ruleorder:\n    def __init__(self):\n        self.order = list()\n\n    def add(self, *rulenames):\n        \"\"\"\n        Records the order of given rules as rule1 > rule2 > rule3, ...\n        \"\"\"\n        self.order.append(list(rulenames))\n\n    def compare(self, rule1, rule2):\n        \"\"\"\n        Return whether rule2 has a higher priority than rule1.\n        \"\"\"\n        # try the last clause first,\n        # i.e. clauses added later overwrite those before.\n        for clause in reversed(self.order):\n            try:\n                i = clause.index(rule1.name)\n                j = clause.index(rule2.name)\n                # rules with higher priority should have a smaller index\n                comp = j - i\n                if comp < 0:\n                    comp = -1\n                elif comp > 0:\n                    comp = 1\n                return comp\n            except ValueError:\n                pass\n\n        # if not ruleorder given, prefer rule without wildcards\n        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()\n        if wildcard_cmp != 0:\n            return wildcard_cmp\n\n        return 0\n\n    def __iter__(self):\n        return self.order.__iter__()\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/tianyabeef/gutMicrobiome/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/workflow.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport re\nimport os\nimport sys\nimport signal\nimport json\nimport urllib\nfrom collections import OrderedDict\nfrom itertools import filterfalse, chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.logging import logger, format_resources, format_resource_names\nfrom snakemake.rules import Rule, Ruleorder\nfrom snakemake.exceptions import RuleException, CreateRuleException, \\\n    UnknownRuleException, NoRulesException, print_exception, WorkflowError\nfrom snakemake.shell import shell\nfrom snakemake.dag import DAG\nfrom snakemake.scheduler import JobScheduler\nfrom snakemake.parser import parse\nimport snakemake.io\nfrom snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\nfrom snakemake.persistence import Persistence\nfrom snakemake.utils import update_config\n\n\nclass Workflow:\n    def __init__(self,\n                 snakefile=None,\n                 snakemakepath=None,\n                 jobscript=None,\n                 overwrite_shellcmd=None,\n                 overwrite_config=dict(),\n                 overwrite_workdir=None,\n                 overwrite_configfile=None,\n                 config_args=None,\n                 debug=False):\n        \"\"\"\n        Create the controller.\n        \"\"\"\n        self._rules = OrderedDict()\n        self.first_rule = None\n        self._workdir = None\n        self.overwrite_workdir = overwrite_workdir\n        self.workdir_init = os.path.abspath(os.curdir)\n        self._ruleorder = Ruleorder()\n        self._localrules = set()\n        self.linemaps = dict()\n        self.rule_count = 0\n        self.basedir = os.path.dirname(snakefile)\n        self.snakefile = os.path.abspath(snakefile)\n        self.snakemakepath = snakemakepath\n        self.included = []\n        self.included_stack = []\n        self.jobscript = jobscript\n        self.persistence = None\n        self.global_resources = None\n        self.globals = globals()\n        self._subworkflows = dict()\n        self.overwrite_shellcmd = overwrite_shellcmd\n        self.overwrite_config = overwrite_config\n        self.overwrite_configfile = overwrite_configfile\n        self.config_args = config_args\n        self._onsuccess = lambda log: None\n        self._onerror = lambda log: None\n        self.debug = debug\n\n        global config\n        config = dict()\n        config.update(self.overwrite_config)\n\n        global rules\n        rules = Rules()\n\n    @property\n    def subworkflows(self):\n        return self._subworkflows.values()\n\n    @property\n    def rules(self):\n        return self._rules.values()\n\n    @property\n    def concrete_files(self):\n        return (\n            file\n            for rule in self.rules for file in chain(rule.input, rule.output)\n            if not callable(file) and not file.contains_wildcard()\n        )\n\n    def check(self):\n        for clause in self._ruleorder:\n            for rulename in clause:\n                if not self.is_rule(rulename):\n                    raise UnknownRuleException(\n                        rulename,\n                        prefix=\"Error in ruleorder definition.\")\n\n    def add_rule(self, name=None, lineno=None, snakefile=None):\n        \"\"\"\n        Add a rule.\n        \"\"\"\n        if name is None:\n            name = str(len(self._rules) + 1)\n        if self.is_rule(name):\n            raise CreateRuleException(\n                \"The name {} is already used by another rule\".format(name))\n        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)\n        self._rules[rule.name] = rule\n        self.rule_count += 1\n        if not self.first_rule:\n            self.first_rule = rule.name\n        return name\n\n    def is_rule(self, name):\n        \"\"\"\n        Return True if name is the name of a rule.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        return name in self._rules\n\n    def get_rule(self, name):\n        \"\"\"\n        Get rule by name.\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if not self._rules:\n            raise NoRulesException()\n        if not name in self._rules:\n            raise UnknownRuleException(name)\n        return self._rules[name]\n\n    def list_rules(self, only_targets=False):\n        rules = self.rules\n        if only_targets:\n            rules = filterfalse(Rule.has_wildcards, rules)\n        for rule in rules:\n            logger.rule_info(name=rule.name, docstring=rule.docstring)\n\n    def list_resources(self):\n        for resource in set(\n            resource for rule in self.rules for resource in rule.resources):\n            if resource not in \"_cores _nodes\".split():\n                logger.info(resource)\n\n    def is_local(self, rule):\n        return rule.name in self._localrules or rule.norun\n\n    def execute(self,\n                targets=None,\n                dryrun=False,\n                touch=False,\n                cores=1,\n                nodes=1,\n                local_cores=1,\n                forcetargets=False,\n                forceall=False,\n                forcerun=None,\n                prioritytargets=None,\n                quiet=False,\n                keepgoing=False,\n                printshellcmds=False,\n                printreason=False,\n                printdag=False,\n                cluster=None,\n                cluster_config=None,\n                cluster_sync=None,\n                jobname=None,\n                immediate_submit=False,\n                ignore_ambiguity=False,\n                printrulegraph=False,\n                printd3dag=False,\n                drmaa=None,\n                stats=None,\n                force_incomplete=False,\n                ignore_incomplete=False,\n                list_version_changes=False,\n                list_code_changes=False,\n                list_input_changes=False,\n                list_params_changes=False,\n                summary=False,\n                detailed_summary=False,\n                latency_wait=3,\n                benchmark_repeats=3,\n                wait_for_files=None,\n                nolock=False,\n                unlock=False,\n                resources=None,\n                notemp=False,\n                nodeps=False,\n                cleanup_metadata=None,\n                subsnakemake=None,\n                updated_files=None,\n                keep_target_files=False,\n                allowed_rules=None,\n                greediness=1.0,\n                no_hooks=False):\n\n        self.global_resources = dict() if resources is None else resources\n        self.global_resources[\"_cores\"] = cores\n        self.global_resources[\"_nodes\"] = nodes\n\n        def rules(items):\n            return map(self._rules.__getitem__, filter(self.is_rule, items))\n\n        if keep_target_files:\n\n            def files(items):\n                return filterfalse(self.is_rule, items)\n        else:\n\n            def files(items):\n                return map(os.path.relpath, filterfalse(self.is_rule, items))\n\n        if not targets:\n            targets = [self.first_rule\n                       ] if self.first_rule is not None else list()\n        if prioritytargets is None:\n            prioritytargets = list()\n        if forcerun is None:\n            forcerun = list()\n\n        priorityrules = set(rules(prioritytargets))\n        priorityfiles = set(files(prioritytargets))\n        forcerules = set(rules(forcerun))\n        forcefiles = set(files(forcerun))\n        targetrules = set(chain(rules(targets),\n                                filterfalse(Rule.has_wildcards, priorityrules),\n                                filterfalse(Rule.has_wildcards, forcerules)))\n        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))\n        if forcetargets:\n            forcefiles.update(targetfiles)\n            forcerules.update(targetrules)\n\n        rules = self.rules\n        if allowed_rules:\n            rules = [rule for rule in rules if rule.name in set(allowed_rules)]\n\n        if wait_for_files is not None:\n            try:\n                snakemake.io.wait_for_files(wait_for_files,\n                                            latency_wait=latency_wait)\n            except IOError as e:\n                logger.error(str(e))\n                return False\n\n        dag = DAG(\n            self, rules,\n            dryrun=dryrun,\n            targetfiles=targetfiles,\n            targetrules=targetrules,\n            forceall=forceall,\n            forcefiles=forcefiles,\n            forcerules=forcerules,\n            priorityfiles=priorityfiles,\n            priorityrules=priorityrules,\n            ignore_ambiguity=ignore_ambiguity,\n            force_incomplete=force_incomplete,\n            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,\n            notemp=notemp)\n\n        self.persistence = Persistence(\n            nolock=nolock,\n            dag=dag,\n            warn_only=dryrun or printrulegraph or printdag or summary or\n            list_version_changes or list_code_changes or list_input_changes or\n            list_params_changes)\n\n        if cleanup_metadata:\n            for f in cleanup_metadata:\n                self.persistence.cleanup_metadata(f)\n            return True\n\n        dag.init()\n        dag.check_dynamic()\n\n        if unlock:\n            try:\n                self.persistence.cleanup_locks()\n                logger.info(\"Unlocking working directory.\")\n                return True\n            except IOError:\n                logger.error(\"Error: Unlocking the directory {} failed. Maybe \"\n                             \"you don't have the permissions?\")\n                return False\n        try:\n            self.persistence.lock()\n        except IOError:\n            logger.error(\n                \"Error: Directory cannot be locked. Please make \"\n                \"sure that no other Snakemake process is trying to create \"\n                \"the same files in the following directory:\\n{}\\n\"\n                \"If you are sure that no other \"\n                \"instances of snakemake are running on this directory, \"\n                \"the remaining lock was likely caused by a kill signal or \"\n                \"a power loss. It can be removed with \"\n                \"the --unlock argument.\".format(os.getcwd()))\n            return False\n\n        if self.subworkflows and not printdag and not printrulegraph:\n            # backup globals\n            globals_backup = dict(self.globals)\n            # execute subworkflows\n            for subworkflow in self.subworkflows:\n                subworkflow_targets = subworkflow.targets(dag)\n                updated = list()\n                if subworkflow_targets:\n                    logger.info(\n                        \"Executing subworkflow {}.\".format(subworkflow.name))\n                    if not subsnakemake(subworkflow.snakefile,\n                                        workdir=subworkflow.workdir,\n                                        targets=subworkflow_targets,\n                                        updated_files=updated):\n                        return False\n                    dag.updated_subworkflow_files.update(subworkflow.target(f)\n                                                         for f in updated)\n                else:\n                    logger.info(\"Subworkflow {}: Nothing to be done.\".format(\n                        subworkflow.name))\n            if self.subworkflows:\n                logger.info(\"Executing main workflow.\")\n            # rescue globals\n            self.globals.update(globals_backup)\n\n        dag.check_incomplete()\n        dag.postprocess()\n\n        if nodeps:\n            missing_input = [f for job in dag.targetjobs for f in job.input\n                             if dag.needrun(job) and not os.path.exists(f)]\n            if missing_input:\n                logger.error(\n                    \"Dependency resolution disabled (--nodeps) \"\n                    \"but missing input \"\n                    \"files detected. If this happens on a cluster, please make sure \"\n                    \"that you handle the dependencies yourself or turn of \"\n                    \"--immediate-submit. Missing input files:\\n{}\".format(\n                        \"\\n\".join(missing_input)))\n                return False\n\n        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)\n\n        if printd3dag:\n            dag.d3dag()\n            return True\n        elif printdag:\n            print(dag)\n            return True\n        elif printrulegraph:\n            print(dag.rule_dot())\n            return True\n        elif summary:\n            print(\"\\n\".join(dag.summary(detailed=False)))\n            return True\n        elif detailed_summary:\n            print(\"\\n\".join(dag.summary(detailed=True)))\n            return True\n        elif list_version_changes:\n            items = list(\n                chain(*map(self.persistence.version_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_code_changes:\n            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_input_changes:\n            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_params_changes:\n            items = list(\n                chain(*map(self.persistence.params_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n\n        scheduler = JobScheduler(self, dag, cores,\n                                 local_cores=local_cores,\n                                 dryrun=dryrun,\n                                 touch=touch,\n                                 cluster=cluster,\n                                 cluster_config=cluster_config,\n                                 cluster_sync=cluster_sync,\n                                 jobname=jobname,\n                                 immediate_submit=immediate_submit,\n                                 quiet=quiet,\n                                 keepgoing=keepgoing,\n                                 drmaa=drmaa,\n                                 printreason=printreason,\n                                 printshellcmds=printshellcmds,\n                                 latency_wait=latency_wait,\n                                 benchmark_repeats=benchmark_repeats,\n                                 greediness=greediness)\n\n        if not dryrun and not quiet:\n            if len(dag):\n                if cluster or cluster_sync or drmaa:\n                    logger.resources_info(\n                        \"Provided cluster nodes: {}\".format(nodes))\n                else:\n                    logger.resources_info(\"Provided cores: {}\".format(cores))\n                    logger.resources_info(\"Rules claiming more threads will be scaled down.\")\n                provided_resources = format_resources(resources)\n                if provided_resources:\n                    logger.resources_info(\n                        \"Provided resources: \" + provided_resources)\n                ignored_resources = format_resource_names(\n                    set(resource for job in dag.needrun_jobs for resource in\n                        job.resources_dict if resource not in resources))\n                if ignored_resources:\n                    logger.resources_info(\n                        \"Ignored resources: \" + ignored_resources)\n                logger.run_info(\"\\n\".join(dag.stats()))\n            else:\n                logger.info(\"Nothing to be done.\")\n        if dryrun and not len(dag):\n            logger.info(\"Nothing to be done.\")\n\n        success = scheduler.schedule()\n\n        if success:\n            if dryrun:\n                if not quiet and len(dag):\n                    logger.run_info(\"\\n\".join(dag.stats()))\n            elif stats:\n                scheduler.stats.to_json(stats)\n            if not dryrun and not no_hooks:\n                self._onsuccess(logger.get_logfile())\n            return True\n        else:\n            if not dryrun and not no_hooks:\n                self._onerror(logger.get_logfile())\n            return False\n\n    def include(self, snakefile,\n                overwrite_first_rule=False,\n                print_compilation=False,\n                overwrite_shellcmd=None):\n        \"\"\"\n        Include a snakefile.\n        \"\"\"\n        # check if snakefile is a path to the filesystem\n        if not urllib.parse.urlparse(snakefile).scheme:\n            if not os.path.isabs(snakefile) and self.included_stack:\n                current_path = os.path.dirname(self.included_stack[-1])\n                snakefile = os.path.join(current_path, snakefile)\n            snakefile = os.path.abspath(snakefile)\n        # else it could be an url.\n        # at least we don't want to modify the path for clarity.\n\n        if snakefile in self.included:\n            logger.info(\"Multiple include of {} ignored\".format(snakefile))\n            return\n        self.included.append(snakefile)\n        self.included_stack.append(snakefile)\n\n        global workflow\n\n        workflow = self\n\n        first_rule = self.first_rule\n        code, linemap = parse(snakefile,\n                              overwrite_shellcmd=self.overwrite_shellcmd)\n\n        if print_compilation:\n            print(code)\n\n        # insert the current directory into sys.path\n        # this allows to import modules from the workflow directory\n        sys.path.insert(0, os.path.dirname(snakefile))\n\n        self.linemaps[snakefile] = linemap\n        exec(compile(code, snakefile, \"exec\"), self.globals)\n        if not overwrite_first_rule:\n            self.first_rule = first_rule\n        self.included_stack.pop()\n\n    def onsuccess(self, func):\n        self._onsuccess = func\n\n    def onerror(self, func):\n        self._onerror = func\n\n    def workdir(self, workdir):\n        if self.overwrite_workdir is None:\n            if not os.path.exists(workdir):\n                os.makedirs(workdir)\n            self._workdir = workdir\n            os.chdir(workdir)\n\n    def configfile(self, jsonpath):\n        \"\"\" Update the global config with the given dictionary. \"\"\"\n        global config\n        c = snakemake.io.load_configfile(jsonpath)\n        update_config(config, c)\n        update_config(config, self.overwrite_config)\n\n    def ruleorder(self, *rulenames):\n        self._ruleorder.add(*rulenames)\n\n    def subworkflow(self, name, snakefile=None, workdir=None):\n        sw = Subworkflow(self, name, snakefile, workdir)\n        self._subworkflows[name] = sw\n        self.globals[name] = sw.target\n\n    def localrules(self, *rulenames):\n        self._localrules.update(rulenames)\n\n    def rule(self, name=None, lineno=None, snakefile=None):\n        name = self.add_rule(name, lineno, snakefile)\n        rule = self.get_rule(name)\n\n        def decorate(ruleinfo):\n            if ruleinfo.input:\n                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])\n            if ruleinfo.output:\n                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])\n            if ruleinfo.params:\n                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])\n            if ruleinfo.threads:\n                if not isinstance(ruleinfo.threads, int):\n                    raise RuleException(\"Threads value has to be an integer.\",\n                                        rule=rule)\n                rule.resources[\"_cores\"] = ruleinfo.threads\n            if ruleinfo.resources:\n                args, resources = ruleinfo.resources\n                if args:\n                    raise RuleException(\"Resources have to be named.\")\n                if not all(map(lambda r: isinstance(r, int),\n                               resources.values())):\n                    raise RuleException(\n                        \"Resources values have to be integers.\",\n                        rule=rule)\n                rule.resources.update(resources)\n            if ruleinfo.priority:\n                if (not isinstance(ruleinfo.priority, int) and\n                    not isinstance(ruleinfo.priority, float)):\n                    raise RuleException(\"Priority values have to be numeric.\",\n                                        rule=rule)\n                rule.priority = ruleinfo.priority\n            if ruleinfo.version:\n                rule.version = ruleinfo.version\n            if ruleinfo.log:\n                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])\n            if ruleinfo.message:\n                rule.message = ruleinfo.message\n            if ruleinfo.benchmark:\n                rule.benchmark = ruleinfo.benchmark\n            rule.norun = ruleinfo.norun\n            rule.docstring = ruleinfo.docstring\n            rule.run_func = ruleinfo.func\n            rule.shellcmd = ruleinfo.shellcmd\n            ruleinfo.func.__name__ = \"__{}\".format(name)\n            self.globals[ruleinfo.func.__name__] = ruleinfo.func\n            setattr(rules, name, rule)\n            return ruleinfo.func\n\n        return decorate\n\n    def docstring(self, string):\n        def decorate(ruleinfo):\n            ruleinfo.docstring = string\n            return ruleinfo\n\n        return decorate\n\n    def input(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.input = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def output(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.output = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def params(self, *params, **kwparams):\n        def decorate(ruleinfo):\n            ruleinfo.params = (params, kwparams)\n            return ruleinfo\n\n        return decorate\n\n    def message(self, message):\n        def decorate(ruleinfo):\n            ruleinfo.message = message\n            return ruleinfo\n\n        return decorate\n\n    def benchmark(self, benchmark):\n        def decorate(ruleinfo):\n            ruleinfo.benchmark = benchmark\n            return ruleinfo\n\n        return decorate\n\n    def threads(self, threads):\n        def decorate(ruleinfo):\n            ruleinfo.threads = threads\n            return ruleinfo\n\n        return decorate\n\n    def resources(self, *args, **resources):\n        def decorate(ruleinfo):\n            ruleinfo.resources = (args, resources)\n            return ruleinfo\n\n        return decorate\n\n    def priority(self, priority):\n        def decorate(ruleinfo):\n            ruleinfo.priority = priority\n            return ruleinfo\n\n        return decorate\n\n    def version(self, version):\n        def decorate(ruleinfo):\n            ruleinfo.version = version\n            return ruleinfo\n\n        return decorate\n\n    def log(self, *logs, **kwlogs):\n        def decorate(ruleinfo):\n            ruleinfo.log = (logs, kwlogs)\n            return ruleinfo\n\n        return decorate\n\n    def shellcmd(self, cmd):\n        def decorate(ruleinfo):\n            ruleinfo.shellcmd = cmd\n            return ruleinfo\n\n        return decorate\n\n    def norun(self):\n        def decorate(ruleinfo):\n            ruleinfo.norun = True\n            return ruleinfo\n\n        return decorate\n\n    def run(self, func):\n        return RuleInfo(func)\n\n    @staticmethod\n    def _empty_decorator(f):\n        return f\n\n\nclass RuleInfo:\n    def __init__(self, func):\n        self.func = func\n        self.shellcmd = None\n        self.norun = False\n        self.input = None\n        self.output = None\n        self.params = None\n        self.message = None\n        self.benchmark = None\n        self.threads = None\n        self.resources = None\n        self.priority = None\n        self.version = None\n        self.log = None\n        self.docstring = None\n\n\nclass Subworkflow:\n    def __init__(self, workflow, name, snakefile, workdir):\n        self.workflow = workflow\n        self.name = name\n        self._snakefile = snakefile\n        self._workdir = workdir\n\n    @property\n    def snakefile(self):\n        if self._snakefile is None:\n            return os.path.abspath(os.path.join(self.workdir, \"Snakefile\"))\n        if not os.path.isabs(self._snakefile):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                self._snakefile))\n        return self._snakefile\n\n    @property\n    def workdir(self):\n        workdir = \".\" if self._workdir is None else self._workdir\n        if not os.path.isabs(workdir):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                workdir))\n        return workdir\n\n    def target(self, paths):\n        if not_iterable(paths):\n            return flag(os.path.join(self.workdir, paths), \"subworkflow\", self)\n        return [self.target(path) for path in paths]\n\n    def targets(self, dag):\n        return [f for job in dag.jobs for f in job.subworkflow_input\n                if job.subworkflow_input[f] is self]\n\n\nclass Rules:\n    \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\"\n    pass\n\n\ndef srcdir(path):\n    \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\"\n    if not workflow.included_stack:\n        return None\n    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kyleabeauchamp/mirrorsnake/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/dag.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport textwrap\nimport time\nfrom collections import defaultdict, Counter\nfrom itertools import chain, combinations, filterfalse, product, groupby\nfrom functools import partial, lru_cache\nfrom operator import itemgetter, attrgetter\n\nfrom snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\nfrom snakemake.jobs import Job, Reason\nfrom snakemake.exceptions import RuleException, MissingInputException\nfrom snakemake.exceptions import MissingRuleException, AmbiguousRuleException\nfrom snakemake.exceptions import CyclicGraphException, MissingOutputException\nfrom snakemake.exceptions import IncompleteFilesException\nfrom snakemake.exceptions import PeriodicWildcardError\nfrom snakemake.exceptions import UnexpectedOutputException, InputFunctionException\nfrom snakemake.logging import logger\nfrom snakemake.output_index import OutputIndex\n\n\nclass DAG:\n    def __init__(self, workflow,\n                 rules=None,\n                 dryrun=False,\n                 targetfiles=None,\n                 targetrules=None,\n                 forceall=False,\n                 forcerules=None,\n                 forcefiles=None,\n                 priorityfiles=None,\n                 priorityrules=None,\n                 ignore_ambiguity=False,\n                 force_incomplete=False,\n                 ignore_incomplete=False,\n                 notemp=False):\n\n        self.dryrun = dryrun\n        self.dependencies = defaultdict(partial(defaultdict, set))\n        self.depending = defaultdict(partial(defaultdict, set))\n        self._needrun = set()\n        self._priority = dict()\n        self._downstream_size = dict()\n        self._reason = defaultdict(Reason)\n        self._finished = set()\n        self._dynamic = set()\n        self._len = 0\n        self.workflow = workflow\n        self.rules = set(rules)\n        self.ignore_ambiguity = ignore_ambiguity\n        self.targetfiles = targetfiles\n        self.targetrules = targetrules\n        self.priorityfiles = priorityfiles\n        self.priorityrules = priorityrules\n        self.targetjobs = set()\n        self.prioritytargetjobs = set()\n        self._ready_jobs = set()\n        self.notemp = notemp\n        self._jobid = dict()\n\n        self.forcerules = set()\n        self.forcefiles = set()\n        self.updated_subworkflow_files = set()\n        if forceall:\n            self.forcerules.update(self.rules)\n        elif forcerules:\n            self.forcerules.update(forcerules)\n        if forcefiles:\n            self.forcefiles.update(forcefiles)\n        self.omitforce = set()\n\n        self.force_incomplete = force_incomplete\n        self.ignore_incomplete = ignore_incomplete\n\n        self.periodic_wildcard_detector = PeriodicityDetector()\n\n        self.update_output_index()\n\n    def init(self):\n        \"\"\" Initialise the DAG. \"\"\"\n        for job in map(self.rule2job, self.targetrules):\n            job = self.update([job])\n            self.targetjobs.add(job)\n\n        for file in self.targetfiles:\n            job = self.update(self.file2jobs(file), file=file)\n            self.targetjobs.add(job)\n\n        self.update_needrun()\n\n    def update_output_index(self):\n        self.output_index = OutputIndex(self.rules)\n\n    def check_incomplete(self):\n        if not self.ignore_incomplete:\n            incomplete = self.incomplete_files\n            if incomplete:\n                if self.force_incomplete:\n                    logger.debug(\"Forcing incomplete files:\")\n                    logger.debug(\"\\t\" + \"\\n\\t\".join(incomplete))\n                    self.forcefiles.update(incomplete)\n                else:\n                    raise IncompleteFilesException(incomplete)\n\n    def check_dynamic(self):\n        for job in filter(lambda job: (\n            job.dynamic_output and not self.needrun(job)\n        ), self.jobs):\n            self.update_dynamic(job)\n\n    @property\n    def dynamic_output_jobs(self):\n        return (job for job in self.jobs if job.dynamic_output)\n\n    @property\n    def jobs(self):\n        \"\"\" All jobs in the DAG. \"\"\"\n        for job in self.bfs(self.dependencies, *self.targetjobs):\n            yield job\n\n    @property\n    def needrun_jobs(self):\n        \"\"\" Jobs that need to be executed. \"\"\"\n        for job in filter(self.needrun,\n                          self.bfs(self.dependencies, *self.targetjobs,\n                                   stop=self.noneedrun_finished)):\n            yield job\n\n    @property\n    def local_needrun_jobs(self):\n        return filter(lambda job: self.workflow.is_local(job.rule),\n                      self.needrun_jobs)\n\n    @property\n    def finished_jobs(self):\n        \"\"\" Jobs that have been executed. \"\"\"\n        for job in filter(self.finished, self.bfs(self.dependencies,\n                                                  *self.targetjobs)):\n            yield job\n\n    @property\n    def ready_jobs(self):\n        \"\"\" Jobs that are ready to execute. \"\"\"\n        return self._ready_jobs\n\n    def ready(self, job):\n        \"\"\" Return whether a given job is ready to execute. \"\"\"\n        return job in self._ready_jobs\n\n    def needrun(self, job):\n        \"\"\" Return whether a given job needs to be executed. \"\"\"\n        return job in self._needrun\n\n    def priority(self, job):\n        return self._priority[job]\n\n    def downstream_size(self, job):\n        return self._downstream_size[job]\n\n    def _job_values(self, jobs, values):\n        return [values[job] for job in jobs]\n\n    def priorities(self, jobs):\n        return self._job_values(jobs, self._priority)\n\n    def downstream_sizes(self, jobs):\n        return self._job_values(jobs, self._downstream_size)\n\n    def noneedrun_finished(self, job):\n        \"\"\"\n        Return whether a given job is finished or was not\n        required to run at all.\n        \"\"\"\n        return not self.needrun(job) or self.finished(job)\n\n    def reason(self, job):\n        \"\"\" Return the reason of the job execution. \"\"\"\n        return self._reason[job]\n\n    def finished(self, job):\n        \"\"\" Return whether a job is finished. \"\"\"\n        return job in self._finished\n\n    def dynamic(self, job):\n        \"\"\"\n        Return whether a job is dynamic (i.e. it is only a placeholder\n        for those that are created after the job with dynamic output has\n        finished.\n        \"\"\"\n        return job in self._dynamic\n\n    def requested_files(self, job):\n        \"\"\" Return the files a job requests. \"\"\"\n        return set(*self.depending[job].values())\n\n    @property\n    def incomplete_files(self):\n        return list(chain(*(\n            job.output for job in filter(self.workflow.persistence.incomplete,\n                                         filterfalse(self.needrun, self.jobs))\n        )))\n\n    @property\n    def newversion_files(self):\n        return list(chain(*(\n            job.output\n            for job in filter(self.workflow.persistence.newversion, self.jobs)\n        )))\n\n    def missing_temp(self, job):\n        \"\"\"\n        Return whether a temp file that is input of the given job is missing.\n        \"\"\"\n        for job_, files in self.depending[job].items():\n            if self.needrun(job_) and any(not f.exists for f in files):\n                return True\n        return False\n\n    def check_output(self, job, wait=3):\n        \"\"\" Raise exception if output files of job are missing. \"\"\"\n        try:\n            wait_for_files(job.expanded_output, latency_wait=wait)\n        except IOError as e:\n            raise MissingOutputException(str(e), rule=job.rule)\n\n        input_maxtime = job.input_maxtime\n        if input_maxtime is not None:\n            output_mintime = job.output_mintime\n            if output_mintime is not None and output_mintime < input_maxtime:\n                raise RuleException(\n                    \"Output files {} are older than input \"\n                    \"files. Did you extract an archive? Make sure that output \"\n                    \"files have a more recent modification date than the \"\n                    \"archive, e.g. by using 'touch'.\".format(\n                        \", \".join(job.expanded_output)),\n                    rule=job.rule)\n\n    def check_periodic_wildcards(self, job):\n        \"\"\" Raise an exception if a wildcard of the given job appears to be periodic,\n        indicating a cyclic dependency. \"\"\"\n        for wildcard, value in job.wildcards_dict.items():\n            periodic_substring = self.periodic_wildcard_detector.is_periodic(\n                value)\n            if periodic_substring is not None:\n                raise PeriodicWildcardError(\n                    \"The value {} in wildcard {} is periodically repeated ({}). \"\n                    \"This would lead to an infinite recursion. \"\n                    \"To avoid this, e.g. restrict the wildcards in this rule to certain values.\".format(\n                        periodic_substring, wildcard, value),\n                    rule=job.rule)\n\n    def handle_protected(self, job):\n        \"\"\" Write-protect output files that are marked with protected(). \"\"\"\n        for f in job.expanded_output:\n            if f in job.protected_output:\n                logger.info(\"Write-protecting output file {}.\".format(f))\n                f.protect()\n\n    def handle_touch(self, job):\n        \"\"\" Touches those output files that are marked for touching. \"\"\"\n        for f in job.expanded_output:\n            if f in job.touch_output:\n                logger.info(\"Touching output file {}.\".format(f))\n                f.touch_or_create()\n\n    def handle_temp(self, job):\n        \"\"\" Remove temp files if they are no longer needed. \"\"\"\n        if self.notemp:\n            return\n\n        needed = lambda job_, f: any(\n            f in files for j, files in self.depending[job_].items()\n            if not self.finished(j) and self.needrun(j) and j != job)\n\n        def unneeded_files():\n            for job_, files in self.dependencies[job].items():\n                for f in job_.temp_output & files:\n                    if not needed(job_, f):\n                        yield f\n            for f in filterfalse(partial(needed, job), job.temp_output):\n                if not f in self.targetfiles:\n                    yield f\n\n        for f in unneeded_files():\n            logger.info(\"Removing temporary output file {}.\".format(f))\n            f.remove()\n\n    def jobid(self, job):\n        if job not in self._jobid:\n            self._jobid[job] = len(self._jobid)\n        return self._jobid[job]\n\n    def update(self, jobs, file=None, visited=None, skip_until_dynamic=False):\n        \"\"\" Update the DAG by adding given jobs and their dependencies. \"\"\"\n        if visited is None:\n            visited = set()\n        producer = None\n        exceptions = list()\n        jobs = sorted(jobs, reverse=not self.ignore_ambiguity)\n        cycles = list()\n\n        for job in jobs:\n            if file in job.input:\n                cycles.append(job)\n                continue\n            if job in visited:\n                cycles.append(job)\n                continue\n            try:\n                self.check_periodic_wildcards(job)\n                self.update_(job,\n                             visited=set(visited),\n                             skip_until_dynamic=skip_until_dynamic)\n                # TODO this might fail if a rule discarded here is needed\n                # elsewhere\n                if producer:\n                    if job < producer or self.ignore_ambiguity:\n                        break\n                    elif producer is not None:\n                        raise AmbiguousRuleException(file, job, producer)\n                producer = job\n            except (MissingInputException, CyclicGraphException,\n                    PeriodicWildcardError) as ex:\n                exceptions.append(ex)\n        if producer is None:\n            if cycles:\n                job = cycles[0]\n                raise CyclicGraphException(job.rule, file, rule=job.rule)\n            if exceptions:\n                raise exceptions[0]\n        return producer\n\n    def update_(self, job, visited=None, skip_until_dynamic=False):\n        \"\"\" Update the DAG by adding the given job and its dependencies. \"\"\"\n        if job in self.dependencies:\n            return\n        if visited is None:\n            visited = set()\n        visited.add(job)\n        dependencies = self.dependencies[job]\n        potential_dependencies = self.collect_potential_dependencies(\n            job).items()\n\n        skip_until_dynamic = skip_until_dynamic and not job.dynamic_output\n\n        missing_input = job.missing_input\n        producer = dict()\n        exceptions = dict()\n        for file, jobs in potential_dependencies:\n            try:\n                producer[file] = self.update(\n                    jobs,\n                    file=file,\n                    visited=visited,\n                    skip_until_dynamic=skip_until_dynamic or file in\n                    job.dynamic_input)\n            except (MissingInputException, CyclicGraphException,\n                    PeriodicWildcardError) as ex:\n                if file in missing_input:\n                    self.delete_job(job,\n                                    recursive=False)  # delete job from tree\n                    raise ex\n\n        for file, job_ in producer.items():\n            dependencies[job_].add(file)\n            self.depending[job_][job].add(file)\n\n        missing_input -= producer.keys()\n        if missing_input:\n            self.delete_job(job, recursive=False)  # delete job from tree\n            raise MissingInputException(job.rule, missing_input)\n\n        if skip_until_dynamic:\n            self._dynamic.add(job)\n\n    def update_needrun(self):\n        \"\"\" Update the information whether a job needs to be executed. \"\"\"\n\n        def output_mintime(job):\n            for job_ in self.bfs(self.depending, job):\n                t = job_.output_mintime\n                if t:\n                    return t\n\n        def needrun(job):\n            reason = self.reason(job)\n            noinitreason = not reason\n            updated_subworkflow_input = self.updated_subworkflow_files.intersection(\n                job.input)\n            if (job not in self.omitforce and job.rule in self.forcerules or\n                not self.forcefiles.isdisjoint(job.output)):\n                reason.forced = True\n            elif updated_subworkflow_input:\n                reason.updated_input.update(updated_subworkflow_input)\n            elif job in self.targetjobs:\n                # TODO find a way to handle added/removed input files here?\n                if not job.output and not job.benchmark:\n                    if job.input:\n                        if job.rule.norun:\n                            reason.updated_input_run.update([f\n                                                             for f in job.input\n                                                             if not f.exists])\n                        else:\n                            reason.nooutput = True\n                    else:\n                        reason.noio = True\n                else:\n                    if job.rule in self.targetrules:\n                        missing_output = job.missing_output()\n                    else:\n                        missing_output = job.missing_output(\n                            requested=set(chain(*self.depending[job].values()))\n                            | self.targetfiles)\n                    reason.missing_output.update(missing_output)\n            if not reason:\n                output_mintime_ = output_mintime(job)\n                if output_mintime_:\n                    updated_input = [\n                        f for f in job.input\n                        if f.exists and f.is_newer(output_mintime_)\n                    ]\n                    reason.updated_input.update(updated_input)\n            if noinitreason and reason:\n                reason.derived = False\n            return job\n\n        reason = self.reason\n        _needrun = self._needrun\n        dependencies = self.dependencies\n        depending = self.depending\n\n        _needrun.clear()\n        candidates = set(self.jobs)\n\n        queue = list(filter(reason, map(needrun, candidates)))\n        visited = set(queue)\n        while queue:\n            job = queue.pop(0)\n            _needrun.add(job)\n\n            for job_, files in dependencies[job].items():\n                missing_output = job_.missing_output(requested=files)\n                reason(job_).missing_output.update(missing_output)\n                if missing_output and not job_ in visited:\n                    visited.add(job_)\n                    queue.append(job_)\n\n            for job_, files in depending[job].items():\n                if job_ in candidates:\n                    reason(job_).updated_input_run.update(files)\n                    if not job_ in visited:\n                        visited.add(job_)\n                        queue.append(job_)\n\n        self._len = len(_needrun)\n\n    def update_priority(self):\n        \"\"\" Update job priorities. \"\"\"\n        prioritized = (lambda job: job.rule in self.priorityrules or\n                       not self.priorityfiles.isdisjoint(job.output))\n        for job in self.needrun_jobs:\n            self._priority[job] = job.rule.priority\n        for job in self.bfs(self.dependencies,\n                            *filter(prioritized, self.needrun_jobs),\n                            stop=self.noneedrun_finished):\n            self._priority[job] = Job.HIGHEST_PRIORITY\n\n    def update_ready(self):\n        \"\"\" Update information whether a job is ready to execute. \"\"\"\n        for job in filter(self.needrun, self.jobs):\n            if not self.finished(job) and self._ready(job):\n                self._ready_jobs.add(job)\n\n    def update_downstream_size(self):\n        for job in self.needrun_jobs:\n            self._downstream_size[job] = sum(\n                1 for _ in self.bfs(self.depending, job,\n                                    stop=self.noneedrun_finished)) - 1\n\n    def postprocess(self):\n        self.update_needrun()\n        self.update_priority()\n        self.update_ready()\n        self.update_downstream_size()\n\n    def _ready(self, job):\n        return self._finished.issuperset(\n            filter(self.needrun, self.dependencies[job]))\n\n    def finish(self, job, update_dynamic=True):\n        self._finished.add(job)\n        try:\n            self._ready_jobs.remove(job)\n        except KeyError:\n            pass\n        # mark depending jobs as ready\n        for job_ in self.depending[job]:\n            if self.needrun(job_) and self._ready(job_):\n                self._ready_jobs.add(job_)\n\n        if update_dynamic and job.dynamic_output:\n            logger.info(\"Dynamically updating jobs\")\n            newjob = self.update_dynamic(job)\n            if newjob:\n                # simulate that this job ran and was finished before\n                self.omitforce.add(newjob)\n                self._needrun.add(newjob)\n                self._finished.add(newjob)\n\n                self.postprocess()\n                self.handle_protected(newjob)\n                self.handle_touch(newjob)\n                # add finished jobs to len as they are not counted after new postprocess\n                self._len += len(self._finished)\n\n    def update_dynamic(self, job):\n        dynamic_wildcards = job.dynamic_wildcards\n        if not dynamic_wildcards:\n            # this happens e.g. in dryrun if output is not yet present\n            return\n\n        depending = list(filter(lambda job_: not self.finished(job_),\n                                self.bfs(self.depending, job)))\n        newrule, non_dynamic_wildcards = job.rule.dynamic_branch(\n            dynamic_wildcards,\n            input=False)\n        self.specialize_rule(job.rule, newrule)\n\n        # no targetfile needed for job\n        newjob = Job(newrule, self, format_wildcards=non_dynamic_wildcards)\n        self.replace_job(job, newjob)\n        for job_ in depending:\n            if job_.dynamic_input:\n                newrule_ = job_.rule.dynamic_branch(dynamic_wildcards)\n                if newrule_ is not None:\n                    self.specialize_rule(job_.rule, newrule_)\n                    if not self.dynamic(job_):\n                        logger.debug(\"Updating job {}.\".format(job_))\n                        newjob_ = Job(newrule_, self,\n                                      targetfile=job_.targetfile)\n\n                        unexpected_output = self.reason(\n                            job_).missing_output.intersection(\n                                newjob.existing_output)\n                        if unexpected_output:\n                            logger.warning(\n                                \"Warning: the following output files of rule {} were not \"\n                                \"present when the DAG was created:\\n{}\".format(\n                                    newjob_.rule, unexpected_output))\n\n                        self.replace_job(job_, newjob_)\n        return newjob\n\n    def delete_job(self, job, recursive=True):\n        for job_ in self.depending[job]:\n            del self.dependencies[job_][job]\n        del self.depending[job]\n        for job_ in self.dependencies[job]:\n            depending = self.depending[job_]\n            del depending[job]\n            if not depending and recursive:\n                self.delete_job(job_)\n        del self.dependencies[job]\n        if job in self._needrun:\n            self._len -= 1\n            self._needrun.remove(job)\n            del self._reason[job]\n        if job in self._finished:\n            self._finished.remove(job)\n        if job in self._dynamic:\n            self._dynamic.remove(job)\n        if job in self._ready_jobs:\n            self._ready_jobs.remove(job)\n\n    def replace_job(self, job, newjob):\n        depending = list(self.depending[job].items())\n        if self.finished(job):\n            self._finished.add(newjob)\n\n        self.delete_job(job)\n        self.update([newjob])\n\n        for job_, files in depending:\n            if not job_.dynamic_input:\n                self.dependencies[job_][newjob].update(files)\n                self.depending[newjob][job_].update(files)\n        if job in self.targetjobs:\n            self.targetjobs.remove(job)\n            self.targetjobs.add(newjob)\n\n    def specialize_rule(self, rule, newrule):\n        assert newrule is not None\n        self.rules.add(newrule)\n        self.update_output_index()\n\n    def collect_potential_dependencies(self, job):\n        dependencies = defaultdict(list)\n        # use a set to circumvent multiple jobs for the same file\n        # if user specified it twice\n        file2jobs = self.file2jobs\n        for file in set(job.input):\n            # omit the file if it comes from a subworkflow\n            if file in job.subworkflow_input:\n                continue\n            try:\n                if file in job.dependencies:\n                    jobs = [Job(job.dependencies[file], self, targetfile=file)]\n                else:\n                    jobs = file2jobs(file)\n                dependencies[file].extend(jobs)\n            except MissingRuleException as ex:\n                pass\n        return dependencies\n\n    def bfs(self, direction, *jobs, stop=lambda job: False):\n        queue = list(jobs)\n        visited = set(queue)\n        while queue:\n            job = queue.pop(0)\n            if stop(job):\n                # stop criterion reached for this node\n                continue\n            yield job\n            for job_, _ in direction[job].items():\n                if not job_ in visited:\n                    queue.append(job_)\n                    visited.add(job_)\n\n    def level_bfs(self, direction, *jobs, stop=lambda job: False):\n        queue = [(job, 0) for job in jobs]\n        visited = set(jobs)\n        while queue:\n            job, level = queue.pop(0)\n            if stop(job):\n                # stop criterion reached for this node\n                continue\n            yield level, job\n            level += 1\n            for job_, _ in direction[job].items():\n                if not job_ in visited:\n                    queue.append((job_, level))\n                    visited.add(job_)\n\n    def dfs(self, direction, *jobs, stop=lambda job: False, post=True):\n        visited = set()\n        for job in jobs:\n            for job_ in self._dfs(direction, job, visited,\n                                  stop=stop,\n                                  post=post):\n                yield job_\n\n    def _dfs(self, direction, job, visited, stop, post):\n        if stop(job):\n            return\n        if not post:\n            yield job\n        for job_ in direction[job]:\n            if not job_ in visited:\n                visited.add(job_)\n                for j in self._dfs(direction, job_, visited, stop, post):\n                    yield j\n        if post:\n            yield job\n\n    def is_isomorph(self, job1, job2):\n        if job1.rule != job2.rule:\n            return False\n        rule = lambda job: job.rule.name\n        queue1, queue2 = [job1], [job2]\n        visited1, visited2 = set(queue1), set(queue2)\n        while queue1 and queue2:\n            job1, job2 = queue1.pop(0), queue2.pop(0)\n            deps1 = sorted(self.dependencies[job1], key=rule)\n            deps2 = sorted(self.dependencies[job2], key=rule)\n            for job1_, job2_ in zip(deps1, deps2):\n                if job1_.rule != job2_.rule:\n                    return False\n                if not job1_ in visited1 and not job2_ in visited2:\n                    queue1.append(job1_)\n                    visited1.add(job1_)\n                    queue2.append(job2_)\n                    visited2.add(job2_)\n                elif not (job1_ in visited1 and job2_ in visited2):\n                    return False\n        return True\n\n    def all_longest_paths(self, *jobs):\n        paths = defaultdict(list)\n\n        def all_longest_paths(_jobs):\n            for job in _jobs:\n                if job in paths:\n                    continue\n                deps = self.dependencies[job]\n                if not deps:\n                    paths[job].append([job])\n                    continue\n                all_longest_paths(deps)\n                for _job in deps:\n                    paths[job].extend(path + [job] for path in paths[_job])\n\n        all_longest_paths(jobs)\n        return chain(*(paths[job] for job in jobs))\n\n    def new_wildcards(self, job):\n        new_wildcards = set(job.wildcards.items())\n        for job_ in self.dependencies[job]:\n            if not new_wildcards:\n                return set()\n            for wildcard in job_.wildcards.items():\n                new_wildcards.discard(wildcard)\n        return new_wildcards\n\n    def rule2job(self, targetrule):\n        return Job(targetrule, self)\n\n    def file2jobs(self, targetfile):\n        rules = self.output_index.match(targetfile)\n        jobs = []\n        exceptions = list()\n        for rule in rules:\n            if rule.is_producer(targetfile):\n                try:\n                    jobs.append(Job(rule, self, targetfile=targetfile))\n                except InputFunctionException as e:\n                    exceptions.append(e)\n        if not jobs:\n            if exceptions:\n                raise exceptions[0]\n            raise MissingRuleException(targetfile)\n        return jobs\n\n    def rule_dot2(self):\n        dag = defaultdict(list)\n        visited = set()\n        preselect = set()\n\n        def preselect_parents(job):\n            for parent in self.depending[job]:\n                if parent in preselect:\n                    continue\n                preselect.add(parent)\n                preselect_parents(parent)\n\n        def build_ruledag(job, key=lambda job: job.rule.name):\n            if job in visited:\n                return\n            visited.add(job)\n            deps = sorted(self.dependencies[job], key=key)\n            deps = [(group[0] if preselect.isdisjoint(group) else\n                     preselect.intersection(group).pop())\n                    for group in (list(g) for _, g in groupby(deps, key))]\n            dag[job].extend(deps)\n            preselect_parents(job)\n            for dep in deps:\n                build_ruledag(dep)\n\n        for job in self.targetjobs:\n            build_ruledag(job)\n\n        return self._dot(dag.keys(),\n                         print_wildcards=False,\n                         print_types=False,\n                         dag=dag)\n\n    def rule_dot(self):\n        graph = defaultdict(set)\n        for job in self.jobs:\n            graph[job.rule].update(dep.rule for dep in self.dependencies[job])\n        return self._dot(graph)\n\n    def dot(self):\n        def node2style(job):\n            if not self.needrun(job):\n                return \"rounded,dashed\"\n            if self.dynamic(job) or job.dynamic_input:\n                return \"rounded,dotted\"\n            return \"rounded\"\n\n        def format_wildcard(wildcard):\n            name, value = wildcard\n            if _IOFile.dynamic_fill in value:\n                value = \"...\"\n            return \"{}: {}\".format(name, value)\n\n        node2rule = lambda job: job.rule\n        node2label = lambda job: \"\\\\n\".join(chain([\n            job.rule.name\n        ], sorted(map(format_wildcard, self.new_wildcards(job)))))\n\n        dag = {job: self.dependencies[job] for job in self.jobs}\n\n        return self._dot(dag,\n                         node2rule=node2rule,\n                         node2style=node2style,\n                         node2label=node2label)\n\n    def _dot(self, graph,\n             node2rule=lambda node: node,\n             node2style=lambda node: \"rounded\",\n             node2label=lambda node: node):\n\n        # color rules\n        huefactor = 2 / (3 * len(self.rules))\n        rulecolor = {\n            rule: \"{:.2f} 0.6 0.85\".format(i * huefactor)\n            for i, rule in enumerate(self.rules)\n        }\n\n        # markup\n        node_markup = '\\t{}[label = \"{}\", color = \"{}\", style=\"{}\"];'.format\n        edge_markup = \"\\t{} -> {}\".format\n\n        # node ids\n        ids = {node: i for i, node in enumerate(graph)}\n\n        # calculate nodes\n        nodes = [node_markup(ids[node], node2label(node),\n                             rulecolor[node2rule(node)], node2style(node))\n                 for node in graph]\n        # calculate edges\n        edges = [edge_markup(ids[dep], ids[node])\n                 for node, deps in graph.items() for dep in deps]\n\n        return textwrap.dedent(\"\"\"\\\n            digraph snakemake_dag {{\n                graph[bgcolor=white, margin=0];\n                node[shape=box, style=rounded, fontname=sans, \\\n                fontsize=10, penwidth=2];\n                edge[penwidth=2, color=grey];\n            {items}\n            }}\\\n            \"\"\").format(items=\"\\n\".join(nodes + edges))\n\n    def summary(self, detailed=False):\n        if detailed:\n            yield \"output_file\\tdate\\trule\\tversion\\tinput_file(s)\\tshellcmd\\tstatus\\tplan\"\n        else:\n            yield \"output_file\\tdate\\trule\\tversion\\tstatus\\tplan\"\n\n        for job in self.jobs:\n            output = job.rule.output if self.dynamic(\n                job) else job.expanded_output\n            for f in output:\n                rule = self.workflow.persistence.rule(f)\n                rule = \"-\" if rule is None else rule\n\n                version = self.workflow.persistence.version(f)\n                version = \"-\" if version is None else str(version)\n\n                date = time.ctime(f.mtime) if f.exists else \"-\"\n\n                pending = \"update pending\" if self.reason(job) else \"no update\"\n\n                input = self.workflow.persistence.input(f)\n                input = \"-\" if input is None else \",\".join(input)\n\n                shellcmd = self.workflow.persistence.shellcmd(f)\n                shellcmd = \"-\" if shellcmd is None else shellcmd\n                # remove new line characters, leading and trailing whitespace\n                shellcmd = shellcmd.strip().replace(\"\\n\", \"; \")\n\n                status = \"ok\"\n                if not f.exists:\n                    status = \"missing\"\n                elif self.reason(job).updated_input:\n                    status = \"updated input files\"\n                elif self.workflow.persistence.version_changed(job, file=f):\n                    status = \"version changed to {}\".format(job.rule.version)\n                elif self.workflow.persistence.code_changed(job, file=f):\n                    status = \"rule implementation changed\"\n                elif self.workflow.persistence.input_changed(job, file=f):\n                    status = \"set of input files changed\"\n                elif self.workflow.persistence.params_changed(job, file=f):\n                    status = \"params changed\"\n                if detailed:\n                    yield \"\\t\".join((f, date, rule, version, input, shellcmd,\n                                     status, pending))\n                else:\n                    yield \"\\t\".join((f, date, rule, version, status, pending))\n\n    def d3dag(self, max_jobs=10000):\n        def node(job):\n            jobid = self.jobid(job)\n            return {\n                \"id\": jobid,\n                \"value\": {\n                    \"jobid\": jobid,\n                    \"label\": job.rule.name,\n                    \"rule\": job.rule.name\n                }\n            }\n\n        def edge(a, b):\n            return {\"u\": self.jobid(a), \"v\": self.jobid(b)}\n\n        jobs = list(self.jobs)\n\n        if len(jobs) > max_jobs:\n            logger.info(\n                \"Job-DAG is too large for visualization (>{} jobs).\".format(\n                    max_jobs))\n        else:\n            logger.d3dag(nodes=[node(job) for job in jobs],\n                         edges=[edge(dep, job) for job in jobs for dep in\n                                self.dependencies[job] if self.needrun(dep)])\n\n    def stats(self):\n        rules = Counter()\n        rules.update(job.rule for job in self.needrun_jobs)\n        rules.update(job.rule for job in self.finished_jobs)\n        yield \"Job counts:\"\n        yield \"\\tcount\\tjobs\"\n        for rule, count in sorted(rules.most_common(),\n                                  key=lambda item: item[0].name):\n            yield \"\\t{}\\t{}\".format(count, rule)\n        yield \"\\t{}\".format(len(self))\n\n    def __str__(self):\n        return self.dot()\n\n    def __len__(self):\n        return self._len\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kyleabeauchamp/mirrorsnake/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/io.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport stat\nimport time\nimport json\nfrom itertools import product, chain\nfrom collections import Iterable, namedtuple\nfrom snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\nfrom snakemake.logging import logger\n\n\ndef lstat(f):\n    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n\n\ndef lutime(f, times):\n    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)\n\n\ndef lchmod(f, mode):\n    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)\n\n\ndef IOFile(file, rule=None):\n    f = _IOFile(file)\n    f.rule = rule\n    return f\n\n\nclass _IOFile(str):\n    \"\"\"\n    A file that is either input or output of a rule.\n    \"\"\"\n\n    dynamic_fill = \"__snakemake_dynamic__\"\n\n    def __new__(cls, file):\n        obj = str.__new__(cls, file)\n        obj._is_function = type(file).__name__ == \"function\"\n        obj._file = file\n        obj.rule = None\n        obj._regex = None\n        return obj\n\n    @property\n    def file(self):\n        if not self._is_function:\n            return self._file\n        else:\n            raise ValueError(\"This IOFile is specified as a function and \"\n                             \"may not be used directly.\")\n\n    @property\n    def exists(self):\n        return os.path.exists(self.file)\n\n    @property\n    def protected(self):\n        return self.exists and not os.access(self.file, os.W_OK)\n\n    @property\n    def mtime(self):\n        # do not follow symlinks for modification time\n        return lstat(self.file).st_mtime\n\n    @property\n    def size(self):\n        # follow symlinks but throw error if invalid\n        self.check_broken_symlink()\n        return os.path.getsize(self.file)\n\n    def check_broken_symlink(self):\n        \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n        if not self.exists and lstat(self.file):\n            raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n\n    def is_newer(self, time):\n        return self.mtime > time\n\n    def prepare(self):\n        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n        dir = os.path.dirname(path_until_wildcard)\n        if len(dir) > 0 and not os.path.exists(dir):\n            try:\n                os.makedirs(dir)\n            except OSError as e:\n                # ignore Errno 17 \"File exists\" (reason: multiprocessing)\n                if e.errno != 17:\n                    raise e\n\n    def protect(self):\n        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~\n                stat.S_IWOTH)\n        if os.path.isdir(self.file):\n            for root, dirs, files in os.walk(self.file):\n                for d in dirs:\n                    lchmod(os.path.join(self.file, d), mode)\n                for f in files:\n                    lchmod(os.path.join(self.file, f), mode)\n        else:\n            lchmod(self.file, mode)\n\n    def remove(self):\n        remove(self.file)\n\n    def touch(self):\n        try:\n            lutime(self.file, None)\n        except OSError as e:\n            if e.errno == 2:\n                raise MissingOutputException(\n                    \"Output file {} of rule {} shall be touched but \"\n                    \"does not exist.\".format(self.file, self.rule.name),\n                    lineno=self.rule.lineno,\n                    snakefile=self.rule.snakefile)\n            else:\n                raise e\n\n    def touch_or_create(self):\n        try:\n            self.touch()\n        except MissingOutputException:\n            # create empty file\n            with open(self.file, \"w\") as f:\n                pass\n\n    def apply_wildcards(self, wildcards,\n                        fill_missing=False,\n                        fail_dynamic=False):\n        f = self._file\n        if self._is_function:\n            f = self._file(Namedlist(fromdict=wildcards))\n\n        return IOFile(apply_wildcards(f, wildcards,\n                                      fill_missing=fill_missing,\n                                      fail_dynamic=fail_dynamic,\n                                      dynamic_fill=self.dynamic_fill),\n                      rule=self.rule)\n\n    def get_wildcard_names(self):\n        return get_wildcard_names(self.file)\n\n    def contains_wildcard(self):\n        return contains_wildcard(self.file)\n\n    def regex(self):\n        if self._regex is None:\n            # compile a regular expression\n            self._regex = re.compile(regex(self.file))\n        return self._regex\n\n    def constant_prefix(self):\n        first_wildcard = _wildcard_regex.search(self.file)\n        if first_wildcard:\n            return self.file[:first_wildcard.start()]\n        return self.file\n\n    def match(self, target):\n        return self.regex().match(target) or None\n\n    def format_dynamic(self):\n        return self.replace(self.dynamic_fill, \"{*}\")\n\n    def __eq__(self, other):\n        f = other._file if isinstance(other, _IOFile) else other\n        return self._file == f\n\n    def __hash__(self):\n        return self._file.__hash__()\n\n\n_wildcard_regex = re.compile(\n    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\")\n\n#    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>[^\\}]*))?\\s*\\}\")\n\n\ndef wait_for_files(files, latency_wait=3):\n    \"\"\"Wait for given files to be present in filesystem.\"\"\"\n    files = list(files)\n    get_missing = lambda: [f for f in files if not os.path.exists(f)]\n    missing = get_missing()\n    if missing:\n        logger.info(\"Waiting at most {} seconds for missing files.\".format(\n            latency_wait))\n        for _ in range(latency_wait):\n            if not get_missing():\n                return\n            time.sleep(1)\n        raise IOError(\"Missing files after {} seconds:\\n{}\".format(\n            latency_wait, \"\\n\".join(get_missing())))\n\n\ndef get_wildcard_names(pattern):\n    return set(match.group('name')\n               for match in _wildcard_regex.finditer(pattern))\n\n\ndef contains_wildcard(path):\n    return _wildcard_regex.search(path) is not None\n\n\ndef remove(file):\n    if os.path.exists(file):\n        if os.path.isdir(file):\n            try:\n                os.removedirs(file)\n            except OSError:\n                # ignore non empty directories\n                pass\n        else:\n            os.remove(file)\n\n\ndef regex(filepattern):\n    f = []\n    last = 0\n    wildcards = set()\n    for match in _wildcard_regex.finditer(filepattern):\n        f.append(re.escape(filepattern[last:match.start()]))\n        wildcard = match.group(\"name\")\n        if wildcard in wildcards:\n            if match.group(\"constraint\"):\n                raise ValueError(\n                    \"If multiple wildcards of the same name \"\n                    \"appear in a string, eventual constraints have to be defined \"\n                    \"at the first occurence and will be inherited by the others.\")\n            f.append(\"(?P={})\".format(wildcard))\n        else:\n            wildcards.add(wildcard)\n            f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if\n                                         match.group(\"constraint\") else \".+\"))\n        last = match.end()\n    f.append(re.escape(filepattern[last:]))\n    f.append(\"$\")  # ensure that the match spans the whole file\n    return \"\".join(f)\n\n\ndef apply_wildcards(pattern, wildcards,\n                    fill_missing=False,\n                    fail_dynamic=False,\n                    dynamic_fill=None,\n                    keep_dynamic=False):\n    def format_match(match):\n        name = match.group(\"name\")\n        try:\n            value = wildcards[name]\n            if fail_dynamic and value == dynamic_fill:\n                raise WildcardError(name)\n            return str(value)  # convert anything into a str\n        except KeyError as ex:\n            if keep_dynamic:\n                return \"{{{}}}\".format(name)\n            elif fill_missing:\n                return dynamic_fill\n            else:\n                raise WildcardError(str(ex))\n\n    return re.sub(_wildcard_regex, format_match, pattern)\n\n\ndef not_iterable(value):\n    return isinstance(value, str) or not isinstance(value, Iterable)\n\n\nclass AnnotatedString(str):\n    def __init__(self, value):\n        self.flags = dict()\n\n\ndef flag(value, flag_type, flag_value=True):\n    if isinstance(value, AnnotatedString):\n        value.flags[flag_type] = flag_value\n        return value\n    if not_iterable(value):\n        value = AnnotatedString(value)\n        value.flags[flag_type] = flag_value\n        return value\n    return [flag(v, flag_type, flag_value=flag_value) for v in value]\n\n\ndef is_flagged(value, flag):\n    if isinstance(value, AnnotatedString):\n        return flag in value.flags\n    return False\n\n\ndef temp(value):\n    \"\"\"\n    A flag for an input or output file that shall be removed after usage.\n    \"\"\"\n    if is_flagged(value, \"protected\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"temp\")\n\n\ndef temporary(value):\n    \"\"\" An alias for temp. \"\"\"\n    return temp(value)\n\n\ndef protected(value):\n    \"\"\" A flag for a file that shall be write protected after creation. \"\"\"\n    if is_flagged(value, \"temp\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"protected\")\n\n\ndef dynamic(value):\n    \"\"\"\n    A flag for a file that shall be dynamic, i.e. the multiplicity\n    (and wildcard values) will be expanded after a certain\n    rule has been run \"\"\"\n    annotated = flag(value, \"dynamic\")\n    tocheck = [annotated] if not_iterable(annotated) else annotated\n    for file in tocheck:\n        matches = list(_wildcard_regex.finditer(file))\n        #if len(matches) != 1:\n        #    raise SyntaxError(\"Dynamic files need exactly one wildcard.\")\n        for match in matches:\n            if match.group(\"constraint\"):\n                raise SyntaxError(\n                    \"The wildcards in dynamic files cannot be constrained.\")\n    return annotated\n\n\ndef touch(value):\n    return flag(value, \"touch\")\n\n\ndef expand(*args, **wildcards):\n    \"\"\"\n    Expand wildcards in given filepatterns.\n\n    Arguments\n    *args -- first arg: filepatterns as list or one single filepattern,\n        second arg (optional): a function to combine wildcard values\n        (itertools.product per default)\n    **wildcards -- the wildcards as keyword arguments\n        with their values as lists\n    \"\"\"\n    filepatterns = args[0]\n    if len(args) == 1:\n        combinator = product\n    elif len(args) == 2:\n        combinator = args[1]\n    if isinstance(filepatterns, str):\n        filepatterns = [filepatterns]\n\n    def flatten(wildcards):\n        for wildcard, values in wildcards.items():\n            if isinstance(values, str) or not isinstance(values, Iterable):\n                values = [values]\n            yield [(wildcard, value) for value in values]\n\n    try:\n        return [filepattern.format(**comb)\n                for comb in map(dict, combinator(*flatten(wildcards))) for\n                filepattern in filepatterns]\n    except KeyError as e:\n        raise WildcardError(\"No values given for wildcard {}.\".format(e))\n\n\ndef limit(pattern, **wildcards):\n    \"\"\"\n    Limit wildcards to the given values.\n\n    Arguments:\n    **wildcards -- the wildcards as keyword arguments\n                   with their values as lists\n    \"\"\"\n    return pattern.format(**{\n        wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values))\n        for wildcard, values in wildcards.items()\n    })\n\n\ndef glob_wildcards(pattern):\n    \"\"\"\n    Glob the values of the wildcards by matching the given pattern to the filesystem.\n    Returns a named tuple with a list of values for each wildcard.\n    \"\"\"\n    pattern = os.path.normpath(pattern)\n    first_wildcard = re.search(\"{[^{]\", pattern)\n    dirname = os.path.dirname(pattern[:first_wildcard.start(\n    )]) if first_wildcard else os.path.dirname(pattern)\n    if not dirname:\n        dirname = \".\"\n\n    names = [match.group('name')\n             for match in _wildcard_regex.finditer(pattern)]\n    Wildcards = namedtuple(\"Wildcards\", names)\n    wildcards = Wildcards(*[list() for name in names])\n\n    pattern = re.compile(regex(pattern))\n    for dirpath, dirnames, filenames in os.walk(dirname):\n        for f in chain(filenames, dirnames):\n            if dirpath != \".\":\n                f = os.path.join(dirpath, f)\n            match = re.match(pattern, f)\n            if match:\n                for name, value in match.groupdict().items():\n                    getattr(wildcards, name).append(value)\n    return wildcards\n\n\n# TODO rewrite Namedlist!\nclass Namedlist(list):\n    \"\"\"\n    A list that additionally provides functions to name items. Further,\n    it is hashable, however the hash does not consider the item names.\n    \"\"\"\n\n    def __init__(self, toclone=None, fromdict=None, plainstr=False):\n        \"\"\"\n        Create the object.\n\n        Arguments\n        toclone  -- another Namedlist that shall be cloned\n        fromdict -- a dict that shall be converted to a\n            Namedlist (keys become names)\n        \"\"\"\n        list.__init__(self)\n        self._names = dict()\n\n        if toclone:\n            self.extend(map(str, toclone) if plainstr else toclone)\n            if isinstance(toclone, Namedlist):\n                self.take_names(toclone.get_names())\n        if fromdict:\n            for key, item in fromdict.items():\n                self.append(item)\n                self.add_name(key)\n\n    def add_name(self, name):\n        \"\"\"\n        Add a name to the last item.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        self.set_name(name, len(self) - 1)\n\n    def set_name(self, name, index, end=None):\n        \"\"\"\n        Set the name of an item.\n\n        Arguments\n        name  -- a name\n        index -- the item index\n        \"\"\"\n        self._names[name] = (index, end)\n        if end is None:\n            setattr(self, name, self[index])\n        else:\n            setattr(self, name, Namedlist(toclone=self[index:end]))\n\n    def get_names(self):\n        \"\"\"\n        Get the defined names as (name, index) pairs.\n        \"\"\"\n        for name, index in self._names.items():\n            yield name, index\n\n    def take_names(self, names):\n        \"\"\"\n        Take over the given names.\n\n        Arguments\n        names -- the given names as (name, index) pairs\n        \"\"\"\n        for name, (i, j) in names:\n            self.set_name(name, i, end=j)\n\n    def items(self):\n        for name in self._names:\n            yield name, getattr(self, name)\n\n    def allitems(self):\n        next = 0\n        for name, index in sorted(self._names.items(),\n                                  key=lambda item: item[1][0]):\n            start, end = index\n            if end is None:\n                end = start + 1\n            if start > next:\n                for item in self[next:start]:\n                    yield None, item\n            yield name, getattr(self, name)\n            next = end\n        for item in self[next:]:\n            yield None, item\n\n    def insert_items(self, index, items):\n        self[index:index + 1] = items\n        add = len(items) - 1\n        for name, (i, j) in self._names.items():\n            if i > index:\n                self._names[name] = (i + add, j + add)\n            elif i == index:\n                self.set_name(name, i, end=i + len(items))\n\n    def keys(self):\n        return self._names\n\n    def plainstrings(self):\n        return self.__class__.__call__(toclone=self, plainstr=True)\n\n    def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)\n        except TypeError:\n            pass\n        return getattr(self, key)\n\n    def __hash__(self):\n        return hash(tuple(self))\n\n    def __str__(self):\n        return \" \".join(map(str, self))\n\n\nclass InputFiles(Namedlist):\n    pass\n\n\nclass OutputFiles(Namedlist):\n    pass\n\n\nclass Wildcards(Namedlist):\n    pass\n\n\nclass Params(Namedlist):\n    pass\n\n\nclass Resources(Namedlist):\n    pass\n\n\nclass Log(Namedlist):\n    pass\n\n\ndef _load_configfile(configpath):\n    \"Tries to load a configfile first as JSON, then as YAML, into a dict.\"\n    try:\n        with open(configpath) as f:\n            try:\n                return json.load(f)\n            except ValueError:\n                f.seek(0)  # try again\n            try:\n                import yaml\n            except ImportError:\n                raise WorkflowError(\"Config file is not valid JSON and PyYAML \"\n                                    \"has not been installed. Please install \"\n                                    \"PyYAML to use YAML config files.\")\n            try:\n                return yaml.load(f)\n            except yaml.YAMLError:\n                raise WorkflowError(\"Config file is not valid JSON or YAML.\")\n    except FileNotFoundError:\n        raise WorkflowError(\"Config file {} not found.\".format(configpath))\n\n\ndef load_configfile(configpath):\n    \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\"\n    config = _load_configfile(configpath)\n    if not isinstance(config, dict):\n        raise WorkflowError(\"Config file must be given as JSON or YAML \"\n                            \"with keys at top level.\")\n    return config\n\n##### Wildcard pumping detection #####\n\n\nclass PeriodicityDetector:\n    def __init__(self, min_repeat=50, max_repeat=100):\n        \"\"\"\n        Args:\n            max_len (int): The maximum length of the periodic substring.\n        \"\"\"\n        self.regex = re.compile(\n            \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format(\n                min_repeat=min_repeat - 1,\n                max_repeat=max_repeat - 1))\n\n    def is_periodic(self, value):\n        \"\"\"Returns the periodic substring or None if not periodic.\"\"\"\n        m = self.regex.search(value)  # search for a periodic suffix.\n        if m is not None:\n            return m.group(\"value\")\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kyleabeauchamp/mirrorsnake/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/jobs.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport sys\nimport base64\nimport json\n\nfrom collections import defaultdict\nfrom itertools import chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.io import IOFile, Wildcards, Resources, _IOFile\nfrom snakemake.utils import format, listfiles\nfrom snakemake.exceptions import RuleException, ProtectedOutputException\nfrom snakemake.exceptions import UnexpectedOutputException\nfrom snakemake.logging import logger\n\n\ndef jobfiles(jobs, type):\n    return chain(*map(attrgetter(type), jobs))\n\n\nclass Job:\n    HIGHEST_PRIORITY = sys.maxsize\n\n    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):\n        self.rule = rule\n        self.dag = dag\n        self.targetfile = targetfile\n\n        self.wildcards_dict = self.rule.get_wildcards(targetfile)\n        self.wildcards = Wildcards(fromdict=self.wildcards_dict)\n        self._format_wildcards = (self.wildcards if format_wildcards is None\n                                  else Wildcards(fromdict=format_wildcards))\n\n        (self.input, self.output, self.params, self.log, self.benchmark,\n         self.ruleio,\n         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)\n\n        self.resources_dict = {\n            name: min(self.rule.workflow.global_resources.get(name, res), res)\n            for name, res in rule.resources.items()\n        }\n        self.threads = self.resources_dict[\"_cores\"]\n        self.resources = Resources(fromdict=self.resources_dict)\n        self._inputsize = None\n\n        self.dynamic_output, self.dynamic_input = set(), set()\n        self.temp_output, self.protected_output = set(), set()\n        self.touch_output = set()\n        self.subworkflow_input = dict()\n        for f in self.output:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_output:\n                self.dynamic_output.add(f)\n            if f_ in self.rule.temp_output:\n                self.temp_output.add(f)\n            if f_ in self.rule.protected_output:\n                self.protected_output.add(f)\n            if f_ in self.rule.touch_output:\n                self.touch_output.add(f)\n        for f in self.input:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_input:\n                self.dynamic_input.add(f)\n            if f_ in self.rule.subworkflow_input:\n                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]\n        self._hash = self.rule.__hash__()\n        if True or not self.dynamic_output:\n            for o in self.output:\n                self._hash ^= o.__hash__()\n\n    @property\n    def priority(self):\n        return self.dag.priority(self)\n\n    @property\n    def b64id(self):\n        return base64.b64encode((self.rule.name + \"\".join(self.output)\n                                 ).encode(\"utf-8\")).decode(\"utf-8\")\n\n    @property\n    def inputsize(self):\n        \"\"\"\n        Return the size of the input files.\n        Input files need to be present.\n        \"\"\"\n        if self._inputsize is None:\n            self._inputsize = sum(f.size for f in self.input)\n        return self._inputsize\n\n    @property\n    def message(self):\n        \"\"\" Return the message for this job. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.message) if\n                    self.rule.message else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable in message \"\n                                \"of shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def shellcmd(self):\n        \"\"\" Return the shell command. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.shellcmd) if\n                    self.rule.shellcmd else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable when printing \"\n                                \"shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def expanded_output(self):\n        \"\"\" Iterate over output files while dynamic output is expanded. \"\"\"\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                expansion = self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill)\n                if not expansion:\n                    yield f_\n                for f, _ in expansion:\n                    yield IOFile(f, self.rule)\n            else:\n                yield f\n\n    @property\n    def dynamic_wildcards(self):\n        \"\"\" Return all wildcard values determined from dynamic output. \"\"\"\n        combinations = set()\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                for f, w in self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill):\n                    combinations.add(tuple(w.items()))\n        wildcards = defaultdict(list)\n        for combination in combinations:\n            for name, value in combination:\n                wildcards[name].append(value)\n        return wildcards\n\n    @property\n    def missing_input(self):\n        \"\"\" Return missing input files. \"\"\"\n        # omit file if it comes from a subworkflow\n        return set(f for f in self.input\n                   if not f.exists and not f in self.subworkflow_input)\n\n    @property\n    def output_mintime(self):\n        \"\"\" Return oldest output file. \"\"\"\n        existing = [f.mtime for f in self.expanded_output if f.exists]\n        if self.benchmark and self.benchmark.exists:\n            existing.append(self.benchmark.mtime)\n        if existing:\n            return min(existing)\n        return None\n\n    @property\n    def input_maxtime(self):\n        \"\"\" Return newest input file. \"\"\"\n        existing = [f.mtime for f in self.input if f.exists]\n        if existing:\n            return max(existing)\n        return None\n\n    def missing_output(self, requested=None):\n        \"\"\" Return missing output files. \"\"\"\n        files = set()\n        if self.benchmark and (requested is None or\n                               self.benchmark in requested):\n            if not self.benchmark.exists:\n                files.add(self.benchmark)\n\n        for f, f_ in zip(self.output, self.rule.output):\n            if requested is None or f in requested:\n                if f in self.dynamic_output:\n                    if not self.expand_dynamic(\n                        f_,\n                        restriction=self.wildcards,\n                        omit_value=_IOFile.dynamic_fill):\n                        files.add(\"{} (dynamic)\".format(f_))\n                elif not f.exists:\n                    files.add(f)\n        return files\n\n    @property\n    def existing_output(self):\n        return filter(lambda f: f.exists, self.expanded_output)\n\n    def check_protected_output(self):\n        protected = list(filter(lambda f: f.protected, self.expanded_output))\n        if protected:\n            raise ProtectedOutputException(self.rule, protected)\n\n    def prepare(self):\n        \"\"\"\n        Prepare execution of job.\n        This includes creation of directories and deletion of previously\n        created dynamic files.\n        \"\"\"\n\n        self.check_protected_output()\n\n        unexpected_output = self.dag.reason(self).missing_output.intersection(\n            self.existing_output)\n        if unexpected_output:\n            logger.warning(\n                \"Warning: the following output files of rule {} were not \"\n                \"present when the DAG was created:\\n{}\".format(\n                    self.rule, unexpected_output))\n\n        if self.dynamic_output:\n            for f, _ in chain(*map(partial(self.expand_dynamic,\n                                           restriction=self.wildcards,\n                                           omit_value=_IOFile.dynamic_fill),\n                                   self.rule.dynamic_output)):\n                os.remove(f)\n        for f, f_ in zip(self.output, self.rule.output):\n            f.prepare()\n        for f in self.log:\n            f.prepare()\n        if self.benchmark:\n            self.benchmark.prepare()\n\n    def cleanup(self):\n        \"\"\" Cleanup output files. \"\"\"\n        to_remove = [f for f in self.expanded_output if f.exists]\n        if to_remove:\n            logger.info(\"Removing output files of failed job {}\"\n                        \" since they might be corrupted:\\n{}\".format(\n                            self, \", \".join(to_remove)))\n            for f in to_remove:\n                f.remove()\n\n    def format_wildcards(self, string, **variables):\n        \"\"\" Format a string with variables from the job. \"\"\"\n        _variables = dict()\n        _variables.update(self.rule.workflow.globals)\n        _variables.update(dict(input=self.input,\n                               output=self.output,\n                               params=self.params,\n                               wildcards=self._format_wildcards,\n                               threads=self.threads,\n                               resources=self.resources,\n                               log=self.log,\n                               version=self.rule.version,\n                               rule=self.rule.name, ))\n        _variables.update(variables)\n        try:\n            return format(string, **_variables)\n        except NameError as ex:\n            raise RuleException(\"NameError: \" + str(ex), rule=self.rule)\n        except IndexError as ex:\n            raise RuleException(\"IndexError: \" + str(ex), rule=self.rule)\n\n    def properties(self, omit_resources=\"_cores _nodes\".split()):\n        resources = {\n            name: res\n            for name, res in self.resources.items()\n            if name not in omit_resources\n        }\n        params = {name: value for name, value in self.params.items()}\n        properties = {\n            \"rule\": self.rule.name,\n            \"local\": self.dag.workflow.is_local(self.rule),\n            \"input\": self.input,\n            \"output\": self.output,\n            \"params\": params,\n            \"threads\": self.threads,\n            \"resources\": resources\n        }\n        return properties\n\n    def json(self):\n        return json.dumps(self.properties())\n\n    def __repr__(self):\n        return self.rule.name\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return self.rule == other.rule and (\n            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)\n\n    def __lt__(self, other):\n        return self.rule.__lt__(other.rule)\n\n    def __gt__(self, other):\n        return self.rule.__gt__(other.rule)\n\n    def __hash__(self):\n        return self._hash\n\n    @staticmethod\n    def expand_dynamic(pattern, restriction=None, omit_value=None):\n        \"\"\" Expand dynamic files. \"\"\"\n        return list(listfiles(pattern,\n                              restriction=restriction,\n                              omit_value=omit_value))\n\n\nclass Reason:\n    def __init__(self):\n        self.updated_input = set()\n        self.updated_input_run = set()\n        self.missing_output = set()\n        self.incomplete_output = set()\n        self.forced = False\n        self.noio = False\n        self.nooutput = False\n        self.derived = True\n\n    def __str__(self):\n        s = list()\n        if self.forced:\n            s.append(\"Forced execution\")\n        else:\n            if self.noio:\n                s.append(\"Rules with neither input nor \"\n                         \"output files are always executed.\")\n            elif self.nooutput:\n                s.append(\"Rules with a run or shell declaration but no output \"\n                         \"are always executed.\")\n            else:\n                if self.missing_output:\n                    s.append(\"Missing output files: {}\".format(\n                        \", \".join(self.missing_output)))\n                if self.incomplete_output:\n                    s.append(\"Incomplete output files: {}\".format(\n                        \", \".join(self.incomplete_output)))\n                updated_input = self.updated_input - self.updated_input_run\n                if updated_input:\n                    s.append(\"Updated input files: {}\".format(\n                        \", \".join(updated_input)))\n                if self.updated_input_run:\n                    s.append(\"Input files updated by another job: {}\".format(\n                        \", \".join(self.updated_input_run)))\n        s = \"; \".join(s)\n        return s\n\n    def __bool__(self):\n        return bool(self.updated_input or self.missing_output or self.forced or\n                    self.updated_input_run or self.noio or self.nooutput)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kyleabeauchamp/mirrorsnake/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/rules.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport sys\nimport inspect\nimport sre_constants\nfrom collections import defaultdict\n\nfrom snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist\nfrom snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log\nfrom snakemake.io import apply_wildcards, is_flagged, not_iterable\nfrom snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException\n\n\nclass Rule:\n    def __init__(self, *args, lineno=None, snakefile=None):\n        \"\"\"\n        Create a rule\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if len(args) == 2:\n            name, workflow = args\n            self.name = name\n            self.workflow = workflow\n            self.docstring = None\n            self.message = None\n            self._input = InputFiles()\n            self._output = OutputFiles()\n            self._params = Params()\n            self.dependencies = dict()\n            self.dynamic_output = set()\n            self.dynamic_input = set()\n            self.temp_output = set()\n            self.protected_output = set()\n            self.touch_output = set()\n            self.subworkflow_input = dict()\n            self.resources = dict(_cores=1, _nodes=1)\n            self.priority = 0\n            self.version = None\n            self._log = Log()\n            self._benchmark = None\n            self.wildcard_names = set()\n            self.lineno = lineno\n            self.snakefile = snakefile\n            self.run_func = None\n            self.shellcmd = None\n            self.norun = False\n        elif len(args) == 1:\n            other = args[0]\n            self.name = other.name\n            self.workflow = other.workflow\n            self.docstring = other.docstring\n            self.message = other.message\n            self._input = InputFiles(other._input)\n            self._output = OutputFiles(other._output)\n            self._params = Params(other._params)\n            self.dependencies = dict(other.dependencies)\n            self.dynamic_output = set(other.dynamic_output)\n            self.dynamic_input = set(other.dynamic_input)\n            self.temp_output = set(other.temp_output)\n            self.protected_output = set(other.protected_output)\n            self.touch_output = set(other.touch_output)\n            self.subworkflow_input = dict(other.subworkflow_input)\n            self.resources = other.resources\n            self.priority = other.priority\n            self.version = other.version\n            self._log = other._log\n            self._benchmark = other._benchmark\n            self.wildcard_names = set(other.wildcard_names)\n            self.lineno = other.lineno\n            self.snakefile = other.snakefile\n            self.run_func = other.run_func\n            self.shellcmd = other.shellcmd\n            self.norun = other.norun\n\n    def dynamic_branch(self, wildcards, input=True):\n        def get_io(rule):\n            return (rule.input, rule.dynamic_input) if input else (\n                rule.output, rule.dynamic_output\n            )\n\n        io, dynamic_io = get_io(self)\n\n        branch = Rule(self)\n        io_, dynamic_io_ = get_io(branch)\n\n        expansion = defaultdict(list)\n        for i, f in enumerate(io):\n            if f in dynamic_io:\n                try:\n                    for e in reversed(expand(f, zip, **wildcards)):\n                        expansion[i].append(IOFile(e, rule=branch))\n                except KeyError:\n                    return None\n\n        # replace the dynamic files with the expanded files\n        replacements = [(i, io[i], e)\n                        for i, e in reversed(list(expansion.items()))]\n        for i, old, exp in replacements:\n            dynamic_io_.remove(old)\n            io_.insert_items(i, exp)\n\n        if not input:\n            for i, old, exp in replacements:\n                if old in branch.temp_output:\n                    branch.temp_output.discard(old)\n                    branch.temp_output.update(exp)\n                if old in branch.protected_output:\n                    branch.protected_output.discard(old)\n                    branch.protected_output.update(exp)\n                if old in branch.touch_output:\n                    branch.touch_output.discard(old)\n                    branch.touch_output.update(exp)\n\n            branch.wildcard_names.clear()\n            non_dynamic_wildcards = dict((name, values[0])\n                                         for name, values in wildcards.items()\n                                         if len(set(values)) == 1)\n            # TODO have a look into how to concretize dependencies here\n            (branch._input, branch._output, branch._params, branch._log,\n             branch._benchmark, _, branch.dependencies\n             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)\n            return branch, non_dynamic_wildcards\n        return branch\n\n    def has_wildcards(self):\n        \"\"\"\n        Return True if rule contains wildcards.\n        \"\"\"\n        return bool(self.wildcard_names)\n\n    @property\n    def benchmark(self):\n        return self._benchmark\n\n    @benchmark.setter\n    def benchmark(self, benchmark):\n        self._benchmark = IOFile(benchmark, rule=self)\n\n    @property\n    def input(self):\n        return self._input\n\n    def set_input(self, *input, **kwinput):\n        \"\"\"\n        Add a list of input files. Recursive lists are flattened.\n\n        Arguments\n        input -- the list of input files\n        \"\"\"\n        for item in input:\n            self._set_inoutput_item(item)\n        for name, item in kwinput.items():\n            self._set_inoutput_item(item, name=name)\n\n    @property\n    def output(self):\n        return self._output\n\n    @property\n    def products(self):\n        products = list(self.output)\n        if self.benchmark:\n            products.append(self.benchmark)\n        return products\n\n    def set_output(self, *output, **kwoutput):\n        \"\"\"\n        Add a list of output files. Recursive lists are flattened.\n\n        Arguments\n        output -- the list of output files\n        \"\"\"\n        for item in output:\n            self._set_inoutput_item(item, output=True)\n        for name, item in kwoutput.items():\n            self._set_inoutput_item(item, output=True, name=name)\n\n        for item in self.output:\n            if self.dynamic_output and item not in self.dynamic_output:\n                raise SyntaxError(\n                    \"A rule with dynamic output may not define any \"\n                    \"non-dynamic output files.\")\n            wildcards = item.get_wildcard_names()\n            if self.wildcard_names:\n                if self.wildcard_names != wildcards:\n                    raise SyntaxError(\n                        \"Not all output files of rule {} \"\n                        \"contain the same wildcards.\".format(self.name))\n            else:\n                self.wildcard_names = wildcards\n\n    def _set_inoutput_item(self, item, output=False, name=None):\n        \"\"\"\n        Set an item to be input or output.\n\n        Arguments\n        item     -- the item\n        inoutput -- either a Namedlist of input or output items\n        name     -- an optional name for the item\n        \"\"\"\n        inoutput = self.output if output else self.input\n        if isinstance(item, str):\n            # add the rule to the dependencies\n            if isinstance(item, _IOFile):\n                self.dependencies[item] = item.rule\n            _item = IOFile(item, rule=self)\n            if is_flagged(item, \"temp\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be temporary\")\n                self.temp_output.add(_item)\n            if is_flagged(item, \"protected\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be protected\")\n                self.protected_output.add(_item)\n            if is_flagged(item, \"touch\"):\n                if not output:\n                    raise SyntaxError(\n                        \"Only output files may be marked for touching.\")\n                self.touch_output.add(_item)\n            if is_flagged(item, \"dynamic\"):\n                if output:\n                    self.dynamic_output.add(_item)\n                else:\n                    self.dynamic_input.add(_item)\n            if is_flagged(item, \"subworkflow\"):\n                if output:\n                    raise SyntaxError(\n                        \"Only input files may refer to a subworkflow\")\n                else:\n                    # record the workflow this item comes from\n                    self.subworkflow_input[_item] = item.flags[\"subworkflow\"]\n            inoutput.append(_item)\n            if name:\n                inoutput.add_name(name)\n        elif callable(item):\n            if output:\n                raise SyntaxError(\n                    \"Only input files can be specified as functions\")\n            inoutput.append(item)\n            if name:\n                inoutput.add_name(name)\n        else:\n            try:\n                start = len(inoutput)\n                for i in item:\n                    self._set_inoutput_item(i, output=output)\n                if name:\n                    # if the list was named, make it accessible\n                    inoutput.set_name(name, start, end=len(inoutput))\n            except TypeError:\n                raise SyntaxError(\n                    \"Input and output files have to be specified as strings or lists of strings.\")\n\n    @property\n    def params(self):\n        return self._params\n\n    def set_params(self, *params, **kwparams):\n        for item in params:\n            self._set_params_item(item)\n        for name, item in kwparams.items():\n            self._set_params_item(item, name=name)\n\n    def _set_params_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.params.append(item)\n            if name:\n                self.params.add_name(name)\n        else:\n            try:\n                start = len(self.params)\n                for i in item:\n                    self._set_params_item(i)\n                if name:\n                    self.params.set_name(name, start, end=len(self.params))\n            except TypeError:\n                raise SyntaxError(\"Params have to be specified as strings.\")\n\n    @property\n    def log(self):\n        return self._log\n\n    def set_log(self, *logs, **kwlogs):\n        for item in logs:\n            self._set_log_item(item)\n        for name, item in kwlogs.items():\n            self._set_log_item(item, name=name)\n\n    def _set_log_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.log.append(IOFile(item,\n                                   rule=self)\n                            if isinstance(item, str) else item)\n            if name:\n                self.log.add_name(name)\n        else:\n            try:\n                start = len(self.log)\n                for i in item:\n                    self._set_log_item(i)\n                if name:\n                    self.log.set_name(name, start, end=len(self.log))\n            except TypeError:\n                raise SyntaxError(\"Log files have to be specified as strings.\")\n\n    def expand_wildcards(self, wildcards=None):\n        \"\"\"\n        Expand wildcards depending on the requested output\n        or given wildcards dict.\n        \"\"\"\n\n        def concretize_iofile(f, wildcards):\n            if not isinstance(f, _IOFile):\n                return IOFile(f, rule=self)\n            else:\n                return f.apply_wildcards(wildcards,\n                                         fill_missing=f in self.dynamic_input,\n                                         fail_dynamic=self.dynamic_output)\n\n        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,\n                             concretize=apply_wildcards,\n                             ruleio=None):\n            for name, item in olditems.allitems():\n                start = len(newitems)\n                is_iterable = True\n                if callable(item):\n                    try:\n                        item = item(wildcards_obj)\n                    except (Exception, BaseException) as e:\n                        raise InputFunctionException(e, rule=self)\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        if not isinstance(item_, str):\n                            raise RuleException(\n                                \"Input function did not return str or list of str.\",\n                                rule=self)\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                else:\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                if name:\n                    newitems.set_name(\n                        name, start,\n                        end=len(newitems) if is_iterable else None)\n\n        if wildcards is None:\n            wildcards = dict()\n        missing_wildcards = self.wildcard_names - set(wildcards.keys())\n\n        if missing_wildcards:\n            raise RuleException(\n                \"Could not resolve wildcards in rule {}:\\n{}\".format(\n                    self.name, \"\\n\".join(self.wildcard_names)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n        ruleio = dict()\n\n        try:\n            input = InputFiles()\n            wildcards_obj = Wildcards(fromdict=wildcards)\n            _apply_wildcards(input, self.input, wildcards, wildcards_obj,\n                             concretize=concretize_iofile,\n                             ruleio=ruleio)\n\n            params = Params()\n            _apply_wildcards(params, self.params, wildcards, wildcards_obj)\n\n            output = OutputFiles(o.apply_wildcards(wildcards)\n                                 for o in self.output)\n            output.take_names(self.output.get_names())\n\n            dependencies = {\n                None if f is None else f.apply_wildcards(wildcards): rule\n                for f, rule in self.dependencies.items()\n            }\n\n            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))\n\n            log = Log()\n            _apply_wildcards(log, self.log, wildcards, wildcards_obj,\n                             concretize=concretize_iofile)\n\n            benchmark = self.benchmark.apply_wildcards(\n                wildcards) if self.benchmark else None\n            return input, output, params, log, benchmark, ruleio, dependencies\n        except WildcardError as ex:\n            # this can only happen if an input contains an unresolved wildcard.\n            raise RuleException(\n                \"Wildcards in input, params, log or benchmark file of rule {} cannot be \"\n                \"determined from output files:\\n{}\".format(self, str(ex)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n    def is_producer(self, requested_output):\n        \"\"\"\n        Returns True if this rule is a producer of the requested output.\n        \"\"\"\n        try:\n            for o in self.products:\n                if o.match(requested_output):\n                    return True\n            return False\n        except sre_constants.error as ex:\n            raise IOFileException(\"{} in wildcard statement\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n        except ValueError as ex:\n            raise IOFileException(\"{}\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n\n    def get_wildcards(self, requested_output):\n        \"\"\"\n        Update the given wildcard dictionary by matching regular expression\n        output files to the requested concrete ones.\n\n        Arguments\n        wildcards -- a dictionary of wildcards\n        requested_output -- a concrete filepath\n        \"\"\"\n        if requested_output is None:\n            return dict()\n        bestmatchlen = 0\n        bestmatch = None\n\n        for o in self.products:\n            match = o.match(requested_output)\n            if match:\n                l = self.get_wildcard_len(match.groupdict())\n                if not bestmatch or bestmatchlen > l:\n                    bestmatch = match.groupdict()\n                    bestmatchlen = l\n        return bestmatch\n\n    @staticmethod\n    def get_wildcard_len(wildcards):\n        \"\"\"\n        Return the length of the given wildcard values.\n\n        Arguments\n        wildcards -- a dict of wildcards\n        \"\"\"\n        return sum(map(len, wildcards.values()))\n\n    def __lt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp < 0\n\n    def __gt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp > 0\n\n    def __str__(self):\n        return self.name\n\n    def __hash__(self):\n        return self.name.__hash__()\n\n    def __eq__(self, other):\n        return self.name == other.name\n\n\nclass Ruleorder:\n    def __init__(self):\n        self.order = list()\n\n    def add(self, *rulenames):\n        \"\"\"\n        Records the order of given rules as rule1 > rule2 > rule3, ...\n        \"\"\"\n        self.order.append(list(rulenames))\n\n    def compare(self, rule1, rule2):\n        \"\"\"\n        Return whether rule2 has a higher priority than rule1.\n        \"\"\"\n        # try the last clause first,\n        # i.e. clauses added later overwrite those before.\n        for clause in reversed(self.order):\n            try:\n                i = clause.index(rule1.name)\n                j = clause.index(rule2.name)\n                # rules with higher priority should have a smaller index\n                comp = j - i\n                if comp < 0:\n                    comp = -1\n                elif comp > 0:\n                    comp = 1\n                return comp\n            except ValueError:\n                pass\n\n        # if not ruleorder given, prefer rule without wildcards\n        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()\n        if wildcard_cmp != 0:\n            return wildcard_cmp\n\n        return 0\n\n    def __iter__(self):\n        return self.order.__iter__()\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kyleabeauchamp/mirrorsnake/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/workflow.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport re\nimport os\nimport sys\nimport signal\nimport json\nimport urllib\nfrom collections import OrderedDict\nfrom itertools import filterfalse, chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.logging import logger, format_resources, format_resource_names\nfrom snakemake.rules import Rule, Ruleorder\nfrom snakemake.exceptions import RuleException, CreateRuleException, \\\n    UnknownRuleException, NoRulesException, print_exception, WorkflowError\nfrom snakemake.shell import shell\nfrom snakemake.dag import DAG\nfrom snakemake.scheduler import JobScheduler\nfrom snakemake.parser import parse\nimport snakemake.io\nfrom snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\nfrom snakemake.persistence import Persistence\nfrom snakemake.utils import update_config\n\n\nclass Workflow:\n    def __init__(self,\n                 snakefile=None,\n                 snakemakepath=None,\n                 jobscript=None,\n                 overwrite_shellcmd=None,\n                 overwrite_config=dict(),\n                 overwrite_workdir=None,\n                 overwrite_configfile=None,\n                 config_args=None,\n                 debug=False):\n        \"\"\"\n        Create the controller.\n        \"\"\"\n        self._rules = OrderedDict()\n        self.first_rule = None\n        self._workdir = None\n        self.overwrite_workdir = overwrite_workdir\n        self.workdir_init = os.path.abspath(os.curdir)\n        self._ruleorder = Ruleorder()\n        self._localrules = set()\n        self.linemaps = dict()\n        self.rule_count = 0\n        self.basedir = os.path.dirname(snakefile)\n        self.snakefile = os.path.abspath(snakefile)\n        self.snakemakepath = snakemakepath\n        self.included = []\n        self.included_stack = []\n        self.jobscript = jobscript\n        self.persistence = None\n        self.global_resources = None\n        self.globals = globals()\n        self._subworkflows = dict()\n        self.overwrite_shellcmd = overwrite_shellcmd\n        self.overwrite_config = overwrite_config\n        self.overwrite_configfile = overwrite_configfile\n        self.config_args = config_args\n        self._onsuccess = lambda log: None\n        self._onerror = lambda log: None\n        self.debug = debug\n\n        global config\n        config = dict()\n        config.update(self.overwrite_config)\n\n        global rules\n        rules = Rules()\n\n    @property\n    def subworkflows(self):\n        return self._subworkflows.values()\n\n    @property\n    def rules(self):\n        return self._rules.values()\n\n    @property\n    def concrete_files(self):\n        return (\n            file\n            for rule in self.rules for file in chain(rule.input, rule.output)\n            if not callable(file) and not file.contains_wildcard()\n        )\n\n    def check(self):\n        for clause in self._ruleorder:\n            for rulename in clause:\n                if not self.is_rule(rulename):\n                    raise UnknownRuleException(\n                        rulename,\n                        prefix=\"Error in ruleorder definition.\")\n\n    def add_rule(self, name=None, lineno=None, snakefile=None):\n        \"\"\"\n        Add a rule.\n        \"\"\"\n        if name is None:\n            name = str(len(self._rules) + 1)\n        if self.is_rule(name):\n            raise CreateRuleException(\n                \"The name {} is already used by another rule\".format(name))\n        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)\n        self._rules[rule.name] = rule\n        self.rule_count += 1\n        if not self.first_rule:\n            self.first_rule = rule.name\n        return name\n\n    def is_rule(self, name):\n        \"\"\"\n        Return True if name is the name of a rule.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        return name in self._rules\n\n    def get_rule(self, name):\n        \"\"\"\n        Get rule by name.\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if not self._rules:\n            raise NoRulesException()\n        if not name in self._rules:\n            raise UnknownRuleException(name)\n        return self._rules[name]\n\n    def list_rules(self, only_targets=False):\n        rules = self.rules\n        if only_targets:\n            rules = filterfalse(Rule.has_wildcards, rules)\n        for rule in rules:\n            logger.rule_info(name=rule.name, docstring=rule.docstring)\n\n    def list_resources(self):\n        for resource in set(\n            resource for rule in self.rules for resource in rule.resources):\n            if resource not in \"_cores _nodes\".split():\n                logger.info(resource)\n\n    def is_local(self, rule):\n        return rule.name in self._localrules or rule.norun\n\n    def execute(self,\n                targets=None,\n                dryrun=False,\n                touch=False,\n                cores=1,\n                nodes=1,\n                local_cores=1,\n                forcetargets=False,\n                forceall=False,\n                forcerun=None,\n                prioritytargets=None,\n                quiet=False,\n                keepgoing=False,\n                printshellcmds=False,\n                printreason=False,\n                printdag=False,\n                cluster=None,\n                cluster_config=None,\n                cluster_sync=None,\n                jobname=None,\n                immediate_submit=False,\n                ignore_ambiguity=False,\n                printrulegraph=False,\n                printd3dag=False,\n                drmaa=None,\n                stats=None,\n                force_incomplete=False,\n                ignore_incomplete=False,\n                list_version_changes=False,\n                list_code_changes=False,\n                list_input_changes=False,\n                list_params_changes=False,\n                summary=False,\n                detailed_summary=False,\n                latency_wait=3,\n                benchmark_repeats=3,\n                wait_for_files=None,\n                nolock=False,\n                unlock=False,\n                resources=None,\n                notemp=False,\n                nodeps=False,\n                cleanup_metadata=None,\n                subsnakemake=None,\n                updated_files=None,\n                keep_target_files=False,\n                allowed_rules=None,\n                greediness=1.0,\n                no_hooks=False):\n\n        self.global_resources = dict() if resources is None else resources\n        self.global_resources[\"_cores\"] = cores\n        self.global_resources[\"_nodes\"] = nodes\n\n        def rules(items):\n            return map(self._rules.__getitem__, filter(self.is_rule, items))\n\n        if keep_target_files:\n\n            def files(items):\n                return filterfalse(self.is_rule, items)\n        else:\n\n            def files(items):\n                return map(os.path.relpath, filterfalse(self.is_rule, items))\n\n        if not targets:\n            targets = [self.first_rule\n                       ] if self.first_rule is not None else list()\n        if prioritytargets is None:\n            prioritytargets = list()\n        if forcerun is None:\n            forcerun = list()\n\n        priorityrules = set(rules(prioritytargets))\n        priorityfiles = set(files(prioritytargets))\n        forcerules = set(rules(forcerun))\n        forcefiles = set(files(forcerun))\n        targetrules = set(chain(rules(targets),\n                                filterfalse(Rule.has_wildcards, priorityrules),\n                                filterfalse(Rule.has_wildcards, forcerules)))\n        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))\n        if forcetargets:\n            forcefiles.update(targetfiles)\n            forcerules.update(targetrules)\n\n        rules = self.rules\n        if allowed_rules:\n            rules = [rule for rule in rules if rule.name in set(allowed_rules)]\n\n        if wait_for_files is not None:\n            try:\n                snakemake.io.wait_for_files(wait_for_files,\n                                            latency_wait=latency_wait)\n            except IOError as e:\n                logger.error(str(e))\n                return False\n\n        dag = DAG(\n            self, rules,\n            dryrun=dryrun,\n            targetfiles=targetfiles,\n            targetrules=targetrules,\n            forceall=forceall,\n            forcefiles=forcefiles,\n            forcerules=forcerules,\n            priorityfiles=priorityfiles,\n            priorityrules=priorityrules,\n            ignore_ambiguity=ignore_ambiguity,\n            force_incomplete=force_incomplete,\n            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,\n            notemp=notemp)\n\n        self.persistence = Persistence(\n            nolock=nolock,\n            dag=dag,\n            warn_only=dryrun or printrulegraph or printdag or summary or\n            list_version_changes or list_code_changes or list_input_changes or\n            list_params_changes)\n\n        if cleanup_metadata:\n            for f in cleanup_metadata:\n                self.persistence.cleanup_metadata(f)\n            return True\n\n        dag.init()\n        dag.check_dynamic()\n\n        if unlock:\n            try:\n                self.persistence.cleanup_locks()\n                logger.info(\"Unlocking working directory.\")\n                return True\n            except IOError:\n                logger.error(\"Error: Unlocking the directory {} failed. Maybe \"\n                             \"you don't have the permissions?\")\n                return False\n        try:\n            self.persistence.lock()\n        except IOError:\n            logger.error(\n                \"Error: Directory cannot be locked. Please make \"\n                \"sure that no other Snakemake process is trying to create \"\n                \"the same files in the following directory:\\n{}\\n\"\n                \"If you are sure that no other \"\n                \"instances of snakemake are running on this directory, \"\n                \"the remaining lock was likely caused by a kill signal or \"\n                \"a power loss. It can be removed with \"\n                \"the --unlock argument.\".format(os.getcwd()))\n            return False\n\n        if self.subworkflows and not printdag and not printrulegraph:\n            # backup globals\n            globals_backup = dict(self.globals)\n            # execute subworkflows\n            for subworkflow in self.subworkflows:\n                subworkflow_targets = subworkflow.targets(dag)\n                updated = list()\n                if subworkflow_targets:\n                    logger.info(\n                        \"Executing subworkflow {}.\".format(subworkflow.name))\n                    if not subsnakemake(subworkflow.snakefile,\n                                        workdir=subworkflow.workdir,\n                                        targets=subworkflow_targets,\n                                        updated_files=updated):\n                        return False\n                    dag.updated_subworkflow_files.update(subworkflow.target(f)\n                                                         for f in updated)\n                else:\n                    logger.info(\"Subworkflow {}: Nothing to be done.\".format(\n                        subworkflow.name))\n            if self.subworkflows:\n                logger.info(\"Executing main workflow.\")\n            # rescue globals\n            self.globals.update(globals_backup)\n\n        dag.check_incomplete()\n        dag.postprocess()\n\n        if nodeps:\n            missing_input = [f for job in dag.targetjobs for f in job.input\n                             if dag.needrun(job) and not os.path.exists(f)]\n            if missing_input:\n                logger.error(\n                    \"Dependency resolution disabled (--nodeps) \"\n                    \"but missing input \"\n                    \"files detected. If this happens on a cluster, please make sure \"\n                    \"that you handle the dependencies yourself or turn of \"\n                    \"--immediate-submit. Missing input files:\\n{}\".format(\n                        \"\\n\".join(missing_input)))\n                return False\n\n        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)\n\n        if printd3dag:\n            dag.d3dag()\n            return True\n        elif printdag:\n            print(dag)\n            return True\n        elif printrulegraph:\n            print(dag.rule_dot())\n            return True\n        elif summary:\n            print(\"\\n\".join(dag.summary(detailed=False)))\n            return True\n        elif detailed_summary:\n            print(\"\\n\".join(dag.summary(detailed=True)))\n            return True\n        elif list_version_changes:\n            items = list(\n                chain(*map(self.persistence.version_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_code_changes:\n            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_input_changes:\n            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_params_changes:\n            items = list(\n                chain(*map(self.persistence.params_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n\n        scheduler = JobScheduler(self, dag, cores,\n                                 local_cores=local_cores,\n                                 dryrun=dryrun,\n                                 touch=touch,\n                                 cluster=cluster,\n                                 cluster_config=cluster_config,\n                                 cluster_sync=cluster_sync,\n                                 jobname=jobname,\n                                 immediate_submit=immediate_submit,\n                                 quiet=quiet,\n                                 keepgoing=keepgoing,\n                                 drmaa=drmaa,\n                                 printreason=printreason,\n                                 printshellcmds=printshellcmds,\n                                 latency_wait=latency_wait,\n                                 benchmark_repeats=benchmark_repeats,\n                                 greediness=greediness)\n\n        if not dryrun and not quiet:\n            if len(dag):\n                if cluster or cluster_sync or drmaa:\n                    logger.resources_info(\n                        \"Provided cluster nodes: {}\".format(nodes))\n                else:\n                    logger.resources_info(\"Provided cores: {}\".format(cores))\n                    logger.resources_info(\"Rules claiming more threads will be scaled down.\")\n                provided_resources = format_resources(resources)\n                if provided_resources:\n                    logger.resources_info(\n                        \"Provided resources: \" + provided_resources)\n                ignored_resources = format_resource_names(\n                    set(resource for job in dag.needrun_jobs for resource in\n                        job.resources_dict if resource not in resources))\n                if ignored_resources:\n                    logger.resources_info(\n                        \"Ignored resources: \" + ignored_resources)\n                logger.run_info(\"\\n\".join(dag.stats()))\n            else:\n                logger.info(\"Nothing to be done.\")\n        if dryrun and not len(dag):\n            logger.info(\"Nothing to be done.\")\n\n        success = scheduler.schedule()\n\n        if success:\n            if dryrun:\n                if not quiet and len(dag):\n                    logger.run_info(\"\\n\".join(dag.stats()))\n            elif stats:\n                scheduler.stats.to_json(stats)\n            if not dryrun and not no_hooks:\n                self._onsuccess(logger.get_logfile())\n            return True\n        else:\n            if not dryrun and not no_hooks:\n                self._onerror(logger.get_logfile())\n            return False\n\n    def include(self, snakefile,\n                overwrite_first_rule=False,\n                print_compilation=False,\n                overwrite_shellcmd=None):\n        \"\"\"\n        Include a snakefile.\n        \"\"\"\n        # check if snakefile is a path to the filesystem\n        if not urllib.parse.urlparse(snakefile).scheme:\n            if not os.path.isabs(snakefile) and self.included_stack:\n                current_path = os.path.dirname(self.included_stack[-1])\n                snakefile = os.path.join(current_path, snakefile)\n            snakefile = os.path.abspath(snakefile)\n        # else it could be an url.\n        # at least we don't want to modify the path for clarity.\n\n        if snakefile in self.included:\n            logger.info(\"Multiple include of {} ignored\".format(snakefile))\n            return\n        self.included.append(snakefile)\n        self.included_stack.append(snakefile)\n\n        global workflow\n\n        workflow = self\n\n        first_rule = self.first_rule\n        code, linemap = parse(snakefile,\n                              overwrite_shellcmd=self.overwrite_shellcmd)\n\n        if print_compilation:\n            print(code)\n\n        # insert the current directory into sys.path\n        # this allows to import modules from the workflow directory\n        sys.path.insert(0, os.path.dirname(snakefile))\n\n        self.linemaps[snakefile] = linemap\n        exec(compile(code, snakefile, \"exec\"), self.globals)\n        if not overwrite_first_rule:\n            self.first_rule = first_rule\n        self.included_stack.pop()\n\n    def onsuccess(self, func):\n        self._onsuccess = func\n\n    def onerror(self, func):\n        self._onerror = func\n\n    def workdir(self, workdir):\n        if self.overwrite_workdir is None:\n            if not os.path.exists(workdir):\n                os.makedirs(workdir)\n            self._workdir = workdir\n            os.chdir(workdir)\n\n    def configfile(self, jsonpath):\n        \"\"\" Update the global config with the given dictionary. \"\"\"\n        global config\n        c = snakemake.io.load_configfile(jsonpath)\n        update_config(config, c)\n        update_config(config, self.overwrite_config)\n\n    def ruleorder(self, *rulenames):\n        self._ruleorder.add(*rulenames)\n\n    def subworkflow(self, name, snakefile=None, workdir=None):\n        sw = Subworkflow(self, name, snakefile, workdir)\n        self._subworkflows[name] = sw\n        self.globals[name] = sw.target\n\n    def localrules(self, *rulenames):\n        self._localrules.update(rulenames)\n\n    def rule(self, name=None, lineno=None, snakefile=None):\n        name = self.add_rule(name, lineno, snakefile)\n        rule = self.get_rule(name)\n\n        def decorate(ruleinfo):\n            if ruleinfo.input:\n                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])\n            if ruleinfo.output:\n                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])\n            if ruleinfo.params:\n                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])\n            if ruleinfo.threads:\n                if not isinstance(ruleinfo.threads, int):\n                    raise RuleException(\"Threads value has to be an integer.\",\n                                        rule=rule)\n                rule.resources[\"_cores\"] = ruleinfo.threads\n            if ruleinfo.resources:\n                args, resources = ruleinfo.resources\n                if args:\n                    raise RuleException(\"Resources have to be named.\")\n                if not all(map(lambda r: isinstance(r, int),\n                               resources.values())):\n                    raise RuleException(\n                        \"Resources values have to be integers.\",\n                        rule=rule)\n                rule.resources.update(resources)\n            if ruleinfo.priority:\n                if (not isinstance(ruleinfo.priority, int) and\n                    not isinstance(ruleinfo.priority, float)):\n                    raise RuleException(\"Priority values have to be numeric.\",\n                                        rule=rule)\n                rule.priority = ruleinfo.priority\n            if ruleinfo.version:\n                rule.version = ruleinfo.version\n            if ruleinfo.log:\n                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])\n            if ruleinfo.message:\n                rule.message = ruleinfo.message\n            if ruleinfo.benchmark:\n                rule.benchmark = ruleinfo.benchmark\n            rule.norun = ruleinfo.norun\n            rule.docstring = ruleinfo.docstring\n            rule.run_func = ruleinfo.func\n            rule.shellcmd = ruleinfo.shellcmd\n            ruleinfo.func.__name__ = \"__{}\".format(name)\n            self.globals[ruleinfo.func.__name__] = ruleinfo.func\n            setattr(rules, name, rule)\n            return ruleinfo.func\n\n        return decorate\n\n    def docstring(self, string):\n        def decorate(ruleinfo):\n            ruleinfo.docstring = string\n            return ruleinfo\n\n        return decorate\n\n    def input(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.input = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def output(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.output = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def params(self, *params, **kwparams):\n        def decorate(ruleinfo):\n            ruleinfo.params = (params, kwparams)\n            return ruleinfo\n\n        return decorate\n\n    def message(self, message):\n        def decorate(ruleinfo):\n            ruleinfo.message = message\n            return ruleinfo\n\n        return decorate\n\n    def benchmark(self, benchmark):\n        def decorate(ruleinfo):\n            ruleinfo.benchmark = benchmark\n            return ruleinfo\n\n        return decorate\n\n    def threads(self, threads):\n        def decorate(ruleinfo):\n            ruleinfo.threads = threads\n            return ruleinfo\n\n        return decorate\n\n    def resources(self, *args, **resources):\n        def decorate(ruleinfo):\n            ruleinfo.resources = (args, resources)\n            return ruleinfo\n\n        return decorate\n\n    def priority(self, priority):\n        def decorate(ruleinfo):\n            ruleinfo.priority = priority\n            return ruleinfo\n\n        return decorate\n\n    def version(self, version):\n        def decorate(ruleinfo):\n            ruleinfo.version = version\n            return ruleinfo\n\n        return decorate\n\n    def log(self, *logs, **kwlogs):\n        def decorate(ruleinfo):\n            ruleinfo.log = (logs, kwlogs)\n            return ruleinfo\n\n        return decorate\n\n    def shellcmd(self, cmd):\n        def decorate(ruleinfo):\n            ruleinfo.shellcmd = cmd\n            return ruleinfo\n\n        return decorate\n\n    def norun(self):\n        def decorate(ruleinfo):\n            ruleinfo.norun = True\n            return ruleinfo\n\n        return decorate\n\n    def run(self, func):\n        return RuleInfo(func)\n\n    @staticmethod\n    def _empty_decorator(f):\n        return f\n\n\nclass RuleInfo:\n    def __init__(self, func):\n        self.func = func\n        self.shellcmd = None\n        self.norun = False\n        self.input = None\n        self.output = None\n        self.params = None\n        self.message = None\n        self.benchmark = None\n        self.threads = None\n        self.resources = None\n        self.priority = None\n        self.version = None\n        self.log = None\n        self.docstring = None\n\n\nclass Subworkflow:\n    def __init__(self, workflow, name, snakefile, workdir):\n        self.workflow = workflow\n        self.name = name\n        self._snakefile = snakefile\n        self._workdir = workdir\n\n    @property\n    def snakefile(self):\n        if self._snakefile is None:\n            return os.path.abspath(os.path.join(self.workdir, \"Snakefile\"))\n        if not os.path.isabs(self._snakefile):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                self._snakefile))\n        return self._snakefile\n\n    @property\n    def workdir(self):\n        workdir = \".\" if self._workdir is None else self._workdir\n        if not os.path.isabs(workdir):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                workdir))\n        return workdir\n\n    def target(self, paths):\n        if not_iterable(paths):\n            return flag(os.path.join(self.workdir, paths), \"subworkflow\", self)\n        return [self.target(path) for path in paths]\n\n    def targets(self, dag):\n        return [f for job in dag.jobs for f in job.subworkflow_input\n                if job.subworkflow_input[f] is self]\n\n\nclass Rules:\n    \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\"\n    pass\n\n\ndef srcdir(path):\n    \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\"\n    if not workflow.included_stack:\n        return None\n    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kdaily/snakemake/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/dag.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport textwrap\nimport time\nfrom collections import defaultdict, Counter\nfrom itertools import chain, combinations, filterfalse, product, groupby\nfrom functools import partial, lru_cache\nfrom operator import itemgetter, attrgetter\n\nfrom snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\nfrom snakemake.jobs import Job, Reason\nfrom snakemake.exceptions import RuleException, MissingInputException\nfrom snakemake.exceptions import MissingRuleException, AmbiguousRuleException\nfrom snakemake.exceptions import CyclicGraphException, MissingOutputException\nfrom snakemake.exceptions import IncompleteFilesException\nfrom snakemake.exceptions import PeriodicWildcardError\nfrom snakemake.exceptions import UnexpectedOutputException, InputFunctionException\nfrom snakemake.logging import logger\nfrom snakemake.output_index import OutputIndex\n\n\nclass DAG:\n    def __init__(self, workflow,\n                 rules=None,\n                 dryrun=False,\n                 targetfiles=None,\n                 targetrules=None,\n                 forceall=False,\n                 forcerules=None,\n                 forcefiles=None,\n                 priorityfiles=None,\n                 priorityrules=None,\n                 ignore_ambiguity=False,\n                 force_incomplete=False,\n                 ignore_incomplete=False,\n                 notemp=False):\n\n        self.dryrun = dryrun\n        self.dependencies = defaultdict(partial(defaultdict, set))\n        self.depending = defaultdict(partial(defaultdict, set))\n        self._needrun = set()\n        self._priority = dict()\n        self._downstream_size = dict()\n        self._reason = defaultdict(Reason)\n        self._finished = set()\n        self._dynamic = set()\n        self._len = 0\n        self.workflow = workflow\n        self.rules = set(rules)\n        self.ignore_ambiguity = ignore_ambiguity\n        self.targetfiles = targetfiles\n        self.targetrules = targetrules\n        self.priorityfiles = priorityfiles\n        self.priorityrules = priorityrules\n        self.targetjobs = set()\n        self.prioritytargetjobs = set()\n        self._ready_jobs = set()\n        self.notemp = notemp\n        self._jobid = dict()\n\n        self.forcerules = set()\n        self.forcefiles = set()\n        self.updated_subworkflow_files = set()\n        if forceall:\n            self.forcerules.update(self.rules)\n        elif forcerules:\n            self.forcerules.update(forcerules)\n        if forcefiles:\n            self.forcefiles.update(forcefiles)\n        self.omitforce = set()\n\n        self.force_incomplete = force_incomplete\n        self.ignore_incomplete = ignore_incomplete\n\n        self.periodic_wildcard_detector = PeriodicityDetector()\n\n        self.update_output_index()\n\n    def init(self):\n        \"\"\" Initialise the DAG. \"\"\"\n        for job in map(self.rule2job, self.targetrules):\n            job = self.update([job])\n            self.targetjobs.add(job)\n\n        for file in self.targetfiles:\n            job = self.update(self.file2jobs(file), file=file)\n            self.targetjobs.add(job)\n\n        self.update_needrun()\n\n    def update_output_index(self):\n        self.output_index = OutputIndex(self.rules)\n\n    def check_incomplete(self):\n        if not self.ignore_incomplete:\n            incomplete = self.incomplete_files\n            if incomplete:\n                if self.force_incomplete:\n                    logger.debug(\"Forcing incomplete files:\")\n                    logger.debug(\"\\t\" + \"\\n\\t\".join(incomplete))\n                    self.forcefiles.update(incomplete)\n                else:\n                    raise IncompleteFilesException(incomplete)\n\n    def check_dynamic(self):\n        for job in filter(lambda job: (\n            job.dynamic_output and not self.needrun(job)\n        ), self.jobs):\n            self.update_dynamic(job)\n\n    @property\n    def dynamic_output_jobs(self):\n        return (job for job in self.jobs if job.dynamic_output)\n\n    @property\n    def jobs(self):\n        \"\"\" All jobs in the DAG. \"\"\"\n        for job in self.bfs(self.dependencies, *self.targetjobs):\n            yield job\n\n    @property\n    def needrun_jobs(self):\n        \"\"\" Jobs that need to be executed. \"\"\"\n        for job in filter(self.needrun,\n                          self.bfs(self.dependencies, *self.targetjobs,\n                                   stop=self.noneedrun_finished)):\n            yield job\n\n    @property\n    def local_needrun_jobs(self):\n        return filter(lambda job: self.workflow.is_local(job.rule),\n                      self.needrun_jobs)\n\n    @property\n    def finished_jobs(self):\n        \"\"\" Jobs that have been executed. \"\"\"\n        for job in filter(self.finished, self.bfs(self.dependencies,\n                                                  *self.targetjobs)):\n            yield job\n\n    @property\n    def ready_jobs(self):\n        \"\"\" Jobs that are ready to execute. \"\"\"\n        return self._ready_jobs\n\n    def ready(self, job):\n        \"\"\" Return whether a given job is ready to execute. \"\"\"\n        return job in self._ready_jobs\n\n    def needrun(self, job):\n        \"\"\" Return whether a given job needs to be executed. \"\"\"\n        return job in self._needrun\n\n    def priority(self, job):\n        return self._priority[job]\n\n    def downstream_size(self, job):\n        return self._downstream_size[job]\n\n    def _job_values(self, jobs, values):\n        return [values[job] for job in jobs]\n\n    def priorities(self, jobs):\n        return self._job_values(jobs, self._priority)\n\n    def downstream_sizes(self, jobs):\n        return self._job_values(jobs, self._downstream_size)\n\n    def noneedrun_finished(self, job):\n        \"\"\"\n        Return whether a given job is finished or was not\n        required to run at all.\n        \"\"\"\n        return not self.needrun(job) or self.finished(job)\n\n    def reason(self, job):\n        \"\"\" Return the reason of the job execution. \"\"\"\n        return self._reason[job]\n\n    def finished(self, job):\n        \"\"\" Return whether a job is finished. \"\"\"\n        return job in self._finished\n\n    def dynamic(self, job):\n        \"\"\"\n        Return whether a job is dynamic (i.e. it is only a placeholder\n        for those that are created after the job with dynamic output has\n        finished.\n        \"\"\"\n        return job in self._dynamic\n\n    def requested_files(self, job):\n        \"\"\" Return the files a job requests. \"\"\"\n        return set(*self.depending[job].values())\n\n    @property\n    def incomplete_files(self):\n        return list(chain(*(\n            job.output for job in filter(self.workflow.persistence.incomplete,\n                                         filterfalse(self.needrun, self.jobs))\n        )))\n\n    @property\n    def newversion_files(self):\n        return list(chain(*(\n            job.output\n            for job in filter(self.workflow.persistence.newversion, self.jobs)\n        )))\n\n    def missing_temp(self, job):\n        \"\"\"\n        Return whether a temp file that is input of the given job is missing.\n        \"\"\"\n        for job_, files in self.depending[job].items():\n            if self.needrun(job_) and any(not f.exists for f in files):\n                return True\n        return False\n\n    def check_output(self, job, wait=3):\n        \"\"\" Raise exception if output files of job are missing. \"\"\"\n        try:\n            wait_for_files(job.expanded_output, latency_wait=wait)\n        except IOError as e:\n            raise MissingOutputException(str(e), rule=job.rule)\n\n        input_maxtime = job.input_maxtime\n        if input_maxtime is not None:\n            output_mintime = job.output_mintime\n            if output_mintime is not None and output_mintime < input_maxtime:\n                raise RuleException(\n                    \"Output files {} are older than input \"\n                    \"files. Did you extract an archive? Make sure that output \"\n                    \"files have a more recent modification date than the \"\n                    \"archive, e.g. by using 'touch'.\".format(\n                        \", \".join(job.expanded_output)),\n                    rule=job.rule)\n\n    def check_periodic_wildcards(self, job):\n        \"\"\" Raise an exception if a wildcard of the given job appears to be periodic,\n        indicating a cyclic dependency. \"\"\"\n        for wildcard, value in job.wildcards_dict.items():\n            periodic_substring = self.periodic_wildcard_detector.is_periodic(\n                value)\n            if periodic_substring is not None:\n                raise PeriodicWildcardError(\n                    \"The value {} in wildcard {} is periodically repeated ({}). \"\n                    \"This would lead to an infinite recursion. \"\n                    \"To avoid this, e.g. restrict the wildcards in this rule to certain values.\".format(\n                        periodic_substring, wildcard, value),\n                    rule=job.rule)\n\n    def handle_protected(self, job):\n        \"\"\" Write-protect output files that are marked with protected(). \"\"\"\n        for f in job.expanded_output:\n            if f in job.protected_output:\n                logger.info(\"Write-protecting output file {}.\".format(f))\n                f.protect()\n\n    def handle_touch(self, job):\n        \"\"\" Touches those output files that are marked for touching. \"\"\"\n        for f in job.expanded_output:\n            if f in job.touch_output:\n                logger.info(\"Touching output file {}.\".format(f))\n                f.touch_or_create()\n\n    def handle_temp(self, job):\n        \"\"\" Remove temp files if they are no longer needed. \"\"\"\n        if self.notemp:\n            return\n\n        needed = lambda job_, f: any(\n            f in files for j, files in self.depending[job_].items()\n            if not self.finished(j) and self.needrun(j) and j != job)\n\n        def unneeded_files():\n            for job_, files in self.dependencies[job].items():\n                for f in job_.temp_output & files:\n                    if not needed(job_, f):\n                        yield f\n            for f in filterfalse(partial(needed, job), job.temp_output):\n                if not f in self.targetfiles:\n                    yield f\n\n        for f in unneeded_files():\n            logger.info(\"Removing temporary output file {}.\".format(f))\n            f.remove()\n\n    def jobid(self, job):\n        if job not in self._jobid:\n            self._jobid[job] = len(self._jobid)\n        return self._jobid[job]\n\n    def update(self, jobs, file=None, visited=None, skip_until_dynamic=False):\n        \"\"\" Update the DAG by adding given jobs and their dependencies. \"\"\"\n        if visited is None:\n            visited = set()\n        producer = None\n        exceptions = list()\n        jobs = sorted(jobs, reverse=not self.ignore_ambiguity)\n        cycles = list()\n\n        for job in jobs:\n            if file in job.input:\n                cycles.append(job)\n                continue\n            if job in visited:\n                cycles.append(job)\n                continue\n            try:\n                self.check_periodic_wildcards(job)\n                self.update_(job,\n                             visited=set(visited),\n                             skip_until_dynamic=skip_until_dynamic)\n                # TODO this might fail if a rule discarded here is needed\n                # elsewhere\n                if producer:\n                    if job < producer or self.ignore_ambiguity:\n                        break\n                    elif producer is not None:\n                        raise AmbiguousRuleException(file, job, producer)\n                producer = job\n            except (MissingInputException, CyclicGraphException,\n                    PeriodicWildcardError) as ex:\n                exceptions.append(ex)\n        if producer is None:\n            if cycles:\n                job = cycles[0]\n                raise CyclicGraphException(job.rule, file, rule=job.rule)\n            if exceptions:\n                raise exceptions[0]\n        return producer\n\n    def update_(self, job, visited=None, skip_until_dynamic=False):\n        \"\"\" Update the DAG by adding the given job and its dependencies. \"\"\"\n        if job in self.dependencies:\n            return\n        if visited is None:\n            visited = set()\n        visited.add(job)\n        dependencies = self.dependencies[job]\n        potential_dependencies = self.collect_potential_dependencies(\n            job).items()\n\n        skip_until_dynamic = skip_until_dynamic and not job.dynamic_output\n\n        missing_input = job.missing_input\n        producer = dict()\n        exceptions = dict()\n        for file, jobs in potential_dependencies:\n            try:\n                producer[file] = self.update(\n                    jobs,\n                    file=file,\n                    visited=visited,\n                    skip_until_dynamic=skip_until_dynamic or file in\n                    job.dynamic_input)\n            except (MissingInputException, CyclicGraphException,\n                    PeriodicWildcardError) as ex:\n                if file in missing_input:\n                    self.delete_job(job,\n                                    recursive=False)  # delete job from tree\n                    raise ex\n\n        for file, job_ in producer.items():\n            dependencies[job_].add(file)\n            self.depending[job_][job].add(file)\n\n        missing_input -= producer.keys()\n        if missing_input:\n            self.delete_job(job, recursive=False)  # delete job from tree\n            raise MissingInputException(job.rule, missing_input)\n\n        if skip_until_dynamic:\n            self._dynamic.add(job)\n\n    def update_needrun(self):\n        \"\"\" Update the information whether a job needs to be executed. \"\"\"\n\n        def output_mintime(job):\n            for job_ in self.bfs(self.depending, job):\n                t = job_.output_mintime\n                if t:\n                    return t\n\n        def needrun(job):\n            reason = self.reason(job)\n            noinitreason = not reason\n            updated_subworkflow_input = self.updated_subworkflow_files.intersection(\n                job.input)\n            if (job not in self.omitforce and job.rule in self.forcerules or\n                not self.forcefiles.isdisjoint(job.output)):\n                reason.forced = True\n            elif updated_subworkflow_input:\n                reason.updated_input.update(updated_subworkflow_input)\n            elif job in self.targetjobs:\n                # TODO find a way to handle added/removed input files here?\n                if not job.output and not job.benchmark:\n                    if job.input:\n                        if job.rule.norun:\n                            reason.updated_input_run.update([f\n                                                             for f in job.input\n                                                             if not f.exists])\n                        else:\n                            reason.nooutput = True\n                    else:\n                        reason.noio = True\n                else:\n                    if job.rule in self.targetrules:\n                        missing_output = job.missing_output()\n                    else:\n                        missing_output = job.missing_output(\n                            requested=set(chain(*self.depending[job].values()))\n                            | self.targetfiles)\n                    reason.missing_output.update(missing_output)\n            if not reason:\n                output_mintime_ = output_mintime(job)\n                if output_mintime_:\n                    updated_input = [\n                        f for f in job.input\n                        if f.exists and f.is_newer(output_mintime_)\n                    ]\n                    reason.updated_input.update(updated_input)\n            if noinitreason and reason:\n                reason.derived = False\n            return job\n\n        reason = self.reason\n        _needrun = self._needrun\n        dependencies = self.dependencies\n        depending = self.depending\n\n        _needrun.clear()\n        candidates = set(self.jobs)\n\n        queue = list(filter(reason, map(needrun, candidates)))\n        visited = set(queue)\n        while queue:\n            job = queue.pop(0)\n            _needrun.add(job)\n\n            for job_, files in dependencies[job].items():\n                missing_output = job_.missing_output(requested=files)\n                reason(job_).missing_output.update(missing_output)\n                if missing_output and not job_ in visited:\n                    visited.add(job_)\n                    queue.append(job_)\n\n            for job_, files in depending[job].items():\n                if job_ in candidates:\n                    reason(job_).updated_input_run.update(files)\n                    if not job_ in visited:\n                        visited.add(job_)\n                        queue.append(job_)\n\n        self._len = len(_needrun)\n\n    def update_priority(self):\n        \"\"\" Update job priorities. \"\"\"\n        prioritized = (lambda job: job.rule in self.priorityrules or\n                       not self.priorityfiles.isdisjoint(job.output))\n        for job in self.needrun_jobs:\n            self._priority[job] = job.rule.priority\n        for job in self.bfs(self.dependencies,\n                            *filter(prioritized, self.needrun_jobs),\n                            stop=self.noneedrun_finished):\n            self._priority[job] = Job.HIGHEST_PRIORITY\n\n    def update_ready(self):\n        \"\"\" Update information whether a job is ready to execute. \"\"\"\n        for job in filter(self.needrun, self.jobs):\n            if not self.finished(job) and self._ready(job):\n                self._ready_jobs.add(job)\n\n    def update_downstream_size(self):\n        for job in self.needrun_jobs:\n            self._downstream_size[job] = sum(\n                1 for _ in self.bfs(self.depending, job,\n                                    stop=self.noneedrun_finished)) - 1\n\n    def postprocess(self):\n        self.update_needrun()\n        self.update_priority()\n        self.update_ready()\n        self.update_downstream_size()\n\n    def _ready(self, job):\n        return self._finished.issuperset(\n            filter(self.needrun, self.dependencies[job]))\n\n    def finish(self, job, update_dynamic=True):\n        self._finished.add(job)\n        try:\n            self._ready_jobs.remove(job)\n        except KeyError:\n            pass\n        # mark depending jobs as ready\n        for job_ in self.depending[job]:\n            if self.needrun(job_) and self._ready(job_):\n                self._ready_jobs.add(job_)\n\n        if update_dynamic and job.dynamic_output:\n            logger.info(\"Dynamically updating jobs\")\n            newjob = self.update_dynamic(job)\n            if newjob:\n                # simulate that this job ran and was finished before\n                self.omitforce.add(newjob)\n                self._needrun.add(newjob)\n                self._finished.add(newjob)\n\n                self.postprocess()\n                self.handle_protected(newjob)\n                self.handle_touch(newjob)\n                # add finished jobs to len as they are not counted after new postprocess\n                self._len += len(self._finished)\n\n    def update_dynamic(self, job):\n        dynamic_wildcards = job.dynamic_wildcards\n        if not dynamic_wildcards:\n            # this happens e.g. in dryrun if output is not yet present\n            return\n\n        depending = list(filter(lambda job_: not self.finished(job_),\n                                self.bfs(self.depending, job)))\n        newrule, non_dynamic_wildcards = job.rule.dynamic_branch(\n            dynamic_wildcards,\n            input=False)\n        self.specialize_rule(job.rule, newrule)\n\n        # no targetfile needed for job\n        newjob = Job(newrule, self, format_wildcards=non_dynamic_wildcards)\n        self.replace_job(job, newjob)\n        for job_ in depending:\n            if job_.dynamic_input:\n                newrule_ = job_.rule.dynamic_branch(dynamic_wildcards)\n                if newrule_ is not None:\n                    self.specialize_rule(job_.rule, newrule_)\n                    if not self.dynamic(job_):\n                        logger.debug(\"Updating job {}.\".format(job_))\n                        newjob_ = Job(newrule_, self,\n                                      targetfile=job_.targetfile)\n\n                        unexpected_output = self.reason(\n                            job_).missing_output.intersection(\n                                newjob.existing_output)\n                        if unexpected_output:\n                            logger.warning(\n                                \"Warning: the following output files of rule {} were not \"\n                                \"present when the DAG was created:\\n{}\".format(\n                                    newjob_.rule, unexpected_output))\n\n                        self.replace_job(job_, newjob_)\n        return newjob\n\n    def delete_job(self, job, recursive=True):\n        for job_ in self.depending[job]:\n            del self.dependencies[job_][job]\n        del self.depending[job]\n        for job_ in self.dependencies[job]:\n            depending = self.depending[job_]\n            del depending[job]\n            if not depending and recursive:\n                self.delete_job(job_)\n        del self.dependencies[job]\n        if job in self._needrun:\n            self._len -= 1\n            self._needrun.remove(job)\n            del self._reason[job]\n        if job in self._finished:\n            self._finished.remove(job)\n        if job in self._dynamic:\n            self._dynamic.remove(job)\n        if job in self._ready_jobs:\n            self._ready_jobs.remove(job)\n\n    def replace_job(self, job, newjob):\n        depending = list(self.depending[job].items())\n        if self.finished(job):\n            self._finished.add(newjob)\n\n        self.delete_job(job)\n        self.update([newjob])\n\n        for job_, files in depending:\n            if not job_.dynamic_input:\n                self.dependencies[job_][newjob].update(files)\n                self.depending[newjob][job_].update(files)\n        if job in self.targetjobs:\n            self.targetjobs.remove(job)\n            self.targetjobs.add(newjob)\n\n    def specialize_rule(self, rule, newrule):\n        assert newrule is not None\n        self.rules.add(newrule)\n        self.update_output_index()\n\n    def collect_potential_dependencies(self, job):\n        dependencies = defaultdict(list)\n        # use a set to circumvent multiple jobs for the same file\n        # if user specified it twice\n        file2jobs = self.file2jobs\n        for file in set(job.input):\n            # omit the file if it comes from a subworkflow\n            if file in job.subworkflow_input:\n                continue\n            try:\n                if file in job.dependencies:\n                    jobs = [Job(job.dependencies[file], self, targetfile=file)]\n                else:\n                    jobs = file2jobs(file)\n                dependencies[file].extend(jobs)\n            except MissingRuleException as ex:\n                pass\n        return dependencies\n\n    def bfs(self, direction, *jobs, stop=lambda job: False):\n        queue = list(jobs)\n        visited = set(queue)\n        while queue:\n            job = queue.pop(0)\n            if stop(job):\n                # stop criterion reached for this node\n                continue\n            yield job\n            for job_, _ in direction[job].items():\n                if not job_ in visited:\n                    queue.append(job_)\n                    visited.add(job_)\n\n    def level_bfs(self, direction, *jobs, stop=lambda job: False):\n        queue = [(job, 0) for job in jobs]\n        visited = set(jobs)\n        while queue:\n            job, level = queue.pop(0)\n            if stop(job):\n                # stop criterion reached for this node\n                continue\n            yield level, job\n            level += 1\n            for job_, _ in direction[job].items():\n                if not job_ in visited:\n                    queue.append((job_, level))\n                    visited.add(job_)\n\n    def dfs(self, direction, *jobs, stop=lambda job: False, post=True):\n        visited = set()\n        for job in jobs:\n            for job_ in self._dfs(direction, job, visited,\n                                  stop=stop,\n                                  post=post):\n                yield job_\n\n    def _dfs(self, direction, job, visited, stop, post):\n        if stop(job):\n            return\n        if not post:\n            yield job\n        for job_ in direction[job]:\n            if not job_ in visited:\n                visited.add(job_)\n                for j in self._dfs(direction, job_, visited, stop, post):\n                    yield j\n        if post:\n            yield job\n\n    def is_isomorph(self, job1, job2):\n        if job1.rule != job2.rule:\n            return False\n        rule = lambda job: job.rule.name\n        queue1, queue2 = [job1], [job2]\n        visited1, visited2 = set(queue1), set(queue2)\n        while queue1 and queue2:\n            job1, job2 = queue1.pop(0), queue2.pop(0)\n            deps1 = sorted(self.dependencies[job1], key=rule)\n            deps2 = sorted(self.dependencies[job2], key=rule)\n            for job1_, job2_ in zip(deps1, deps2):\n                if job1_.rule != job2_.rule:\n                    return False\n                if not job1_ in visited1 and not job2_ in visited2:\n                    queue1.append(job1_)\n                    visited1.add(job1_)\n                    queue2.append(job2_)\n                    visited2.add(job2_)\n                elif not (job1_ in visited1 and job2_ in visited2):\n                    return False\n        return True\n\n    def all_longest_paths(self, *jobs):\n        paths = defaultdict(list)\n\n        def all_longest_paths(_jobs):\n            for job in _jobs:\n                if job in paths:\n                    continue\n                deps = self.dependencies[job]\n                if not deps:\n                    paths[job].append([job])\n                    continue\n                all_longest_paths(deps)\n                for _job in deps:\n                    paths[job].extend(path + [job] for path in paths[_job])\n\n        all_longest_paths(jobs)\n        return chain(*(paths[job] for job in jobs))\n\n    def new_wildcards(self, job):\n        new_wildcards = set(job.wildcards.items())\n        for job_ in self.dependencies[job]:\n            if not new_wildcards:\n                return set()\n            for wildcard in job_.wildcards.items():\n                new_wildcards.discard(wildcard)\n        return new_wildcards\n\n    def rule2job(self, targetrule):\n        return Job(targetrule, self)\n\n    def file2jobs(self, targetfile):\n        rules = self.output_index.match(targetfile)\n        jobs = []\n        exceptions = list()\n        for rule in rules:\n            if rule.is_producer(targetfile):\n                try:\n                    jobs.append(Job(rule, self, targetfile=targetfile))\n                except InputFunctionException as e:\n                    exceptions.append(e)\n        if not jobs:\n            if exceptions:\n                raise exceptions[0]\n            raise MissingRuleException(targetfile)\n        return jobs\n\n    def rule_dot2(self):\n        dag = defaultdict(list)\n        visited = set()\n        preselect = set()\n\n        def preselect_parents(job):\n            for parent in self.depending[job]:\n                if parent in preselect:\n                    continue\n                preselect.add(parent)\n                preselect_parents(parent)\n\n        def build_ruledag(job, key=lambda job: job.rule.name):\n            if job in visited:\n                return\n            visited.add(job)\n            deps = sorted(self.dependencies[job], key=key)\n            deps = [(group[0] if preselect.isdisjoint(group) else\n                     preselect.intersection(group).pop())\n                    for group in (list(g) for _, g in groupby(deps, key))]\n            dag[job].extend(deps)\n            preselect_parents(job)\n            for dep in deps:\n                build_ruledag(dep)\n\n        for job in self.targetjobs:\n            build_ruledag(job)\n\n        return self._dot(dag.keys(),\n                         print_wildcards=False,\n                         print_types=False,\n                         dag=dag)\n\n    def rule_dot(self):\n        graph = defaultdict(set)\n        for job in self.jobs:\n            graph[job.rule].update(dep.rule for dep in self.dependencies[job])\n        return self._dot(graph)\n\n    def dot(self):\n        def node2style(job):\n            if not self.needrun(job):\n                return \"rounded,dashed\"\n            if self.dynamic(job) or job.dynamic_input:\n                return \"rounded,dotted\"\n            return \"rounded\"\n\n        def format_wildcard(wildcard):\n            name, value = wildcard\n            if _IOFile.dynamic_fill in value:\n                value = \"...\"\n            return \"{}: {}\".format(name, value)\n\n        node2rule = lambda job: job.rule\n        node2label = lambda job: \"\\\\n\".join(chain([\n            job.rule.name\n        ], sorted(map(format_wildcard, self.new_wildcards(job)))))\n\n        dag = {job: self.dependencies[job] for job in self.jobs}\n\n        return self._dot(dag,\n                         node2rule=node2rule,\n                         node2style=node2style,\n                         node2label=node2label)\n\n    def _dot(self, graph,\n             node2rule=lambda node: node,\n             node2style=lambda node: \"rounded\",\n             node2label=lambda node: node):\n\n        # color rules\n        huefactor = 2 / (3 * len(self.rules))\n        rulecolor = {\n            rule: \"{:.2f} 0.6 0.85\".format(i * huefactor)\n            for i, rule in enumerate(self.rules)\n        }\n\n        # markup\n        node_markup = '\\t{}[label = \"{}\", color = \"{}\", style=\"{}\"];'.format\n        edge_markup = \"\\t{} -> {}\".format\n\n        # node ids\n        ids = {node: i for i, node in enumerate(graph)}\n\n        # calculate nodes\n        nodes = [node_markup(ids[node], node2label(node),\n                             rulecolor[node2rule(node)], node2style(node))\n                 for node in graph]\n        # calculate edges\n        edges = [edge_markup(ids[dep], ids[node])\n                 for node, deps in graph.items() for dep in deps]\n\n        return textwrap.dedent(\"\"\"\\\n            digraph snakemake_dag {{\n                graph[bgcolor=white, margin=0];\n                node[shape=box, style=rounded, fontname=sans, \\\n                fontsize=10, penwidth=2];\n                edge[penwidth=2, color=grey];\n            {items}\n            }}\\\n            \"\"\").format(items=\"\\n\".join(nodes + edges))\n\n    def summary(self, detailed=False):\n        if detailed:\n            yield \"output_file\\tdate\\trule\\tversion\\tinput_file(s)\\tshellcmd\\tstatus\\tplan\"\n        else:\n            yield \"output_file\\tdate\\trule\\tversion\\tstatus\\tplan\"\n\n        for job in self.jobs:\n            output = job.rule.output if self.dynamic(\n                job) else job.expanded_output\n            for f in output:\n                rule = self.workflow.persistence.rule(f)\n                rule = \"-\" if rule is None else rule\n\n                version = self.workflow.persistence.version(f)\n                version = \"-\" if version is None else str(version)\n\n                date = time.ctime(f.mtime) if f.exists else \"-\"\n\n                pending = \"update pending\" if self.reason(job) else \"no update\"\n\n                input = self.workflow.persistence.input(f)\n                input = \"-\" if input is None else \",\".join(input)\n\n                shellcmd = self.workflow.persistence.shellcmd(f)\n                shellcmd = \"-\" if shellcmd is None else shellcmd\n                # remove new line characters, leading and trailing whitespace\n                shellcmd = shellcmd.strip().replace(\"\\n\", \"; \")\n\n                status = \"ok\"\n                if not f.exists:\n                    status = \"missing\"\n                elif self.reason(job).updated_input:\n                    status = \"updated input files\"\n                elif self.workflow.persistence.version_changed(job, file=f):\n                    status = \"version changed to {}\".format(job.rule.version)\n                elif self.workflow.persistence.code_changed(job, file=f):\n                    status = \"rule implementation changed\"\n                elif self.workflow.persistence.input_changed(job, file=f):\n                    status = \"set of input files changed\"\n                elif self.workflow.persistence.params_changed(job, file=f):\n                    status = \"params changed\"\n                if detailed:\n                    yield \"\\t\".join((f, date, rule, version, input, shellcmd,\n                                     status, pending))\n                else:\n                    yield \"\\t\".join((f, date, rule, version, status, pending))\n\n    def d3dag(self, max_jobs=10000):\n        def node(job):\n            jobid = self.jobid(job)\n            return {\n                \"id\": jobid,\n                \"value\": {\n                    \"jobid\": jobid,\n                    \"label\": job.rule.name,\n                    \"rule\": job.rule.name\n                }\n            }\n\n        def edge(a, b):\n            return {\"u\": self.jobid(a), \"v\": self.jobid(b)}\n\n        jobs = list(self.jobs)\n\n        if len(jobs) > max_jobs:\n            logger.info(\n                \"Job-DAG is too large for visualization (>{} jobs).\".format(\n                    max_jobs))\n        else:\n            logger.d3dag(nodes=[node(job) for job in jobs],\n                         edges=[edge(dep, job) for job in jobs for dep in\n                                self.dependencies[job] if self.needrun(dep)])\n\n    def stats(self):\n        rules = Counter()\n        rules.update(job.rule for job in self.needrun_jobs)\n        rules.update(job.rule for job in self.finished_jobs)\n        yield \"Job counts:\"\n        yield \"\\tcount\\tjobs\"\n        for rule, count in sorted(rules.most_common(),\n                                  key=lambda item: item[0].name):\n            yield \"\\t{}\\t{}\".format(count, rule)\n        yield \"\\t{}\".format(len(self))\n\n    def __str__(self):\n        return self.dot()\n\n    def __len__(self):\n        return self._len\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kdaily/snakemake/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/io.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport stat\nimport time\nimport json\nfrom itertools import product, chain\nfrom collections import Iterable, namedtuple\nfrom snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\nfrom snakemake.logging import logger\n\n\ndef lstat(f):\n    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n\n\ndef lutime(f, times):\n    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)\n\n\ndef lchmod(f, mode):\n    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)\n\n\ndef IOFile(file, rule=None):\n    f = _IOFile(file)\n    f.rule = rule\n    return f\n\n\nclass _IOFile(str):\n    \"\"\"\n    A file that is either input or output of a rule.\n    \"\"\"\n\n    dynamic_fill = \"__snakemake_dynamic__\"\n\n    def __new__(cls, file):\n        obj = str.__new__(cls, file)\n        obj._is_function = type(file).__name__ == \"function\"\n        obj._file = file\n        obj.rule = None\n        obj._regex = None\n        return obj\n\n    @property\n    def file(self):\n        if not self._is_function:\n            return self._file\n        else:\n            raise ValueError(\"This IOFile is specified as a function and \"\n                             \"may not be used directly.\")\n\n    @property\n    def exists(self):\n        return os.path.exists(self.file)\n\n    @property\n    def protected(self):\n        return self.exists and not os.access(self.file, os.W_OK)\n\n    @property\n    def mtime(self):\n        # do not follow symlinks for modification time\n        return lstat(self.file).st_mtime\n\n    @property\n    def size(self):\n        # follow symlinks but throw error if invalid\n        self.check_broken_symlink()\n        return os.path.getsize(self.file)\n\n    def check_broken_symlink(self):\n        \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n        if not self.exists and lstat(self.file):\n            raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n\n    def is_newer(self, time):\n        return self.mtime > time\n\n    def prepare(self):\n        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n        dir = os.path.dirname(path_until_wildcard)\n        if len(dir) > 0 and not os.path.exists(dir):\n            try:\n                os.makedirs(dir)\n            except OSError as e:\n                # ignore Errno 17 \"File exists\" (reason: multiprocessing)\n                if e.errno != 17:\n                    raise e\n\n    def protect(self):\n        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~\n                stat.S_IWOTH)\n        if os.path.isdir(self.file):\n            for root, dirs, files in os.walk(self.file):\n                for d in dirs:\n                    lchmod(os.path.join(self.file, d), mode)\n                for f in files:\n                    lchmod(os.path.join(self.file, f), mode)\n        else:\n            lchmod(self.file, mode)\n\n    def remove(self):\n        remove(self.file)\n\n    def touch(self):\n        try:\n            lutime(self.file, None)\n        except OSError as e:\n            if e.errno == 2:\n                raise MissingOutputException(\n                    \"Output file {} of rule {} shall be touched but \"\n                    \"does not exist.\".format(self.file, self.rule.name),\n                    lineno=self.rule.lineno,\n                    snakefile=self.rule.snakefile)\n            else:\n                raise e\n\n    def touch_or_create(self):\n        try:\n            self.touch()\n        except MissingOutputException:\n            # create empty file\n            with open(self.file, \"w\") as f:\n                pass\n\n    def apply_wildcards(self, wildcards,\n                        fill_missing=False,\n                        fail_dynamic=False):\n        f = self._file\n        if self._is_function:\n            f = self._file(Namedlist(fromdict=wildcards))\n\n        return IOFile(apply_wildcards(f, wildcards,\n                                      fill_missing=fill_missing,\n                                      fail_dynamic=fail_dynamic,\n                                      dynamic_fill=self.dynamic_fill),\n                      rule=self.rule)\n\n    def get_wildcard_names(self):\n        return get_wildcard_names(self.file)\n\n    def contains_wildcard(self):\n        return contains_wildcard(self.file)\n\n    def regex(self):\n        if self._regex is None:\n            # compile a regular expression\n            self._regex = re.compile(regex(self.file))\n        return self._regex\n\n    def constant_prefix(self):\n        first_wildcard = _wildcard_regex.search(self.file)\n        if first_wildcard:\n            return self.file[:first_wildcard.start()]\n        return self.file\n\n    def match(self, target):\n        return self.regex().match(target) or None\n\n    def format_dynamic(self):\n        return self.replace(self.dynamic_fill, \"{*}\")\n\n    def __eq__(self, other):\n        f = other._file if isinstance(other, _IOFile) else other\n        return self._file == f\n\n    def __hash__(self):\n        return self._file.__hash__()\n\n\n_wildcard_regex = re.compile(\n    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\")\n\n#    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>[^\\}]*))?\\s*\\}\")\n\n\ndef wait_for_files(files, latency_wait=3):\n    \"\"\"Wait for given files to be present in filesystem.\"\"\"\n    files = list(files)\n    get_missing = lambda: [f for f in files if not os.path.exists(f)]\n    missing = get_missing()\n    if missing:\n        logger.info(\"Waiting at most {} seconds for missing files.\".format(\n            latency_wait))\n        for _ in range(latency_wait):\n            if not get_missing():\n                return\n            time.sleep(1)\n        raise IOError(\"Missing files after {} seconds:\\n{}\".format(\n            latency_wait, \"\\n\".join(get_missing())))\n\n\ndef get_wildcard_names(pattern):\n    return set(match.group('name')\n               for match in _wildcard_regex.finditer(pattern))\n\n\ndef contains_wildcard(path):\n    return _wildcard_regex.search(path) is not None\n\n\ndef remove(file):\n    if os.path.exists(file):\n        if os.path.isdir(file):\n            try:\n                os.removedirs(file)\n            except OSError:\n                # ignore non empty directories\n                pass\n        else:\n            os.remove(file)\n\n\ndef regex(filepattern):\n    f = []\n    last = 0\n    wildcards = set()\n    for match in _wildcard_regex.finditer(filepattern):\n        f.append(re.escape(filepattern[last:match.start()]))\n        wildcard = match.group(\"name\")\n        if wildcard in wildcards:\n            if match.group(\"constraint\"):\n                raise ValueError(\n                    \"If multiple wildcards of the same name \"\n                    \"appear in a string, eventual constraints have to be defined \"\n                    \"at the first occurence and will be inherited by the others.\")\n            f.append(\"(?P={})\".format(wildcard))\n        else:\n            wildcards.add(wildcard)\n            f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if\n                                         match.group(\"constraint\") else \".+\"))\n        last = match.end()\n    f.append(re.escape(filepattern[last:]))\n    f.append(\"$\")  # ensure that the match spans the whole file\n    return \"\".join(f)\n\n\ndef apply_wildcards(pattern, wildcards,\n                    fill_missing=False,\n                    fail_dynamic=False,\n                    dynamic_fill=None,\n                    keep_dynamic=False):\n    def format_match(match):\n        name = match.group(\"name\")\n        try:\n            value = wildcards[name]\n            if fail_dynamic and value == dynamic_fill:\n                raise WildcardError(name)\n            return str(value)  # convert anything into a str\n        except KeyError as ex:\n            if keep_dynamic:\n                return \"{{{}}}\".format(name)\n            elif fill_missing:\n                return dynamic_fill\n            else:\n                raise WildcardError(str(ex))\n\n    return re.sub(_wildcard_regex, format_match, pattern)\n\n\ndef not_iterable(value):\n    return isinstance(value, str) or not isinstance(value, Iterable)\n\n\nclass AnnotatedString(str):\n    def __init__(self, value):\n        self.flags = dict()\n\n\ndef flag(value, flag_type, flag_value=True):\n    if isinstance(value, AnnotatedString):\n        value.flags[flag_type] = flag_value\n        return value\n    if not_iterable(value):\n        value = AnnotatedString(value)\n        value.flags[flag_type] = flag_value\n        return value\n    return [flag(v, flag_type, flag_value=flag_value) for v in value]\n\n\ndef is_flagged(value, flag):\n    if isinstance(value, AnnotatedString):\n        return flag in value.flags\n    return False\n\n\ndef temp(value):\n    \"\"\"\n    A flag for an input or output file that shall be removed after usage.\n    \"\"\"\n    if is_flagged(value, \"protected\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"temp\")\n\n\ndef temporary(value):\n    \"\"\" An alias for temp. \"\"\"\n    return temp(value)\n\n\ndef protected(value):\n    \"\"\" A flag for a file that shall be write protected after creation. \"\"\"\n    if is_flagged(value, \"temp\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"protected\")\n\n\ndef dynamic(value):\n    \"\"\"\n    A flag for a file that shall be dynamic, i.e. the multiplicity\n    (and wildcard values) will be expanded after a certain\n    rule has been run \"\"\"\n    annotated = flag(value, \"dynamic\")\n    tocheck = [annotated] if not_iterable(annotated) else annotated\n    for file in tocheck:\n        matches = list(_wildcard_regex.finditer(file))\n        #if len(matches) != 1:\n        #    raise SyntaxError(\"Dynamic files need exactly one wildcard.\")\n        for match in matches:\n            if match.group(\"constraint\"):\n                raise SyntaxError(\n                    \"The wildcards in dynamic files cannot be constrained.\")\n    return annotated\n\n\ndef touch(value):\n    return flag(value, \"touch\")\n\n\ndef expand(*args, **wildcards):\n    \"\"\"\n    Expand wildcards in given filepatterns.\n\n    Arguments\n    *args -- first arg: filepatterns as list or one single filepattern,\n        second arg (optional): a function to combine wildcard values\n        (itertools.product per default)\n    **wildcards -- the wildcards as keyword arguments\n        with their values as lists\n    \"\"\"\n    filepatterns = args[0]\n    if len(args) == 1:\n        combinator = product\n    elif len(args) == 2:\n        combinator = args[1]\n    if isinstance(filepatterns, str):\n        filepatterns = [filepatterns]\n\n    def flatten(wildcards):\n        for wildcard, values in wildcards.items():\n            if isinstance(values, str) or not isinstance(values, Iterable):\n                values = [values]\n            yield [(wildcard, value) for value in values]\n\n    try:\n        return [filepattern.format(**comb)\n                for comb in map(dict, combinator(*flatten(wildcards))) for\n                filepattern in filepatterns]\n    except KeyError as e:\n        raise WildcardError(\"No values given for wildcard {}.\".format(e))\n\n\ndef limit(pattern, **wildcards):\n    \"\"\"\n    Limit wildcards to the given values.\n\n    Arguments:\n    **wildcards -- the wildcards as keyword arguments\n                   with their values as lists\n    \"\"\"\n    return pattern.format(**{\n        wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values))\n        for wildcard, values in wildcards.items()\n    })\n\n\ndef glob_wildcards(pattern):\n    \"\"\"\n    Glob the values of the wildcards by matching the given pattern to the filesystem.\n    Returns a named tuple with a list of values for each wildcard.\n    \"\"\"\n    pattern = os.path.normpath(pattern)\n    first_wildcard = re.search(\"{[^{]\", pattern)\n    dirname = os.path.dirname(pattern[:first_wildcard.start(\n    )]) if first_wildcard else os.path.dirname(pattern)\n    if not dirname:\n        dirname = \".\"\n\n    names = [match.group('name')\n             for match in _wildcard_regex.finditer(pattern)]\n    Wildcards = namedtuple(\"Wildcards\", names)\n    wildcards = Wildcards(*[list() for name in names])\n\n    pattern = re.compile(regex(pattern))\n    for dirpath, dirnames, filenames in os.walk(dirname):\n        for f in chain(filenames, dirnames):\n            if dirpath != \".\":\n                f = os.path.join(dirpath, f)\n            match = re.match(pattern, f)\n            if match:\n                for name, value in match.groupdict().items():\n                    getattr(wildcards, name).append(value)\n    return wildcards\n\n\n# TODO rewrite Namedlist!\nclass Namedlist(list):\n    \"\"\"\n    A list that additionally provides functions to name items. Further,\n    it is hashable, however the hash does not consider the item names.\n    \"\"\"\n\n    def __init__(self, toclone=None, fromdict=None, plainstr=False):\n        \"\"\"\n        Create the object.\n\n        Arguments\n        toclone  -- another Namedlist that shall be cloned\n        fromdict -- a dict that shall be converted to a\n            Namedlist (keys become names)\n        \"\"\"\n        list.__init__(self)\n        self._names = dict()\n\n        if toclone:\n            self.extend(map(str, toclone) if plainstr else toclone)\n            if isinstance(toclone, Namedlist):\n                self.take_names(toclone.get_names())\n        if fromdict:\n            for key, item in fromdict.items():\n                self.append(item)\n                self.add_name(key)\n\n    def add_name(self, name):\n        \"\"\"\n        Add a name to the last item.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        self.set_name(name, len(self) - 1)\n\n    def set_name(self, name, index, end=None):\n        \"\"\"\n        Set the name of an item.\n\n        Arguments\n        name  -- a name\n        index -- the item index\n        \"\"\"\n        self._names[name] = (index, end)\n        if end is None:\n            setattr(self, name, self[index])\n        else:\n            setattr(self, name, Namedlist(toclone=self[index:end]))\n\n    def get_names(self):\n        \"\"\"\n        Get the defined names as (name, index) pairs.\n        \"\"\"\n        for name, index in self._names.items():\n            yield name, index\n\n    def take_names(self, names):\n        \"\"\"\n        Take over the given names.\n\n        Arguments\n        names -- the given names as (name, index) pairs\n        \"\"\"\n        for name, (i, j) in names:\n            self.set_name(name, i, end=j)\n\n    def items(self):\n        for name in self._names:\n            yield name, getattr(self, name)\n\n    def allitems(self):\n        next = 0\n        for name, index in sorted(self._names.items(),\n                                  key=lambda item: item[1][0]):\n            start, end = index\n            if end is None:\n                end = start + 1\n            if start > next:\n                for item in self[next:start]:\n                    yield None, item\n            yield name, getattr(self, name)\n            next = end\n        for item in self[next:]:\n            yield None, item\n\n    def insert_items(self, index, items):\n        self[index:index + 1] = items\n        add = len(items) - 1\n        for name, (i, j) in self._names.items():\n            if i > index:\n                self._names[name] = (i + add, j + add)\n            elif i == index:\n                self.set_name(name, i, end=i + len(items))\n\n    def keys(self):\n        return self._names\n\n    def plainstrings(self):\n        return self.__class__.__call__(toclone=self, plainstr=True)\n\n    def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)\n        except TypeError:\n            pass\n        return getattr(self, key)\n\n    def __hash__(self):\n        return hash(tuple(self))\n\n    def __str__(self):\n        return \" \".join(map(str, self))\n\n\nclass InputFiles(Namedlist):\n    pass\n\n\nclass OutputFiles(Namedlist):\n    pass\n\n\nclass Wildcards(Namedlist):\n    pass\n\n\nclass Params(Namedlist):\n    pass\n\n\nclass Resources(Namedlist):\n    pass\n\n\nclass Log(Namedlist):\n    pass\n\n\ndef _load_configfile(configpath):\n    \"Tries to load a configfile first as JSON, then as YAML, into a dict.\"\n    try:\n        with open(configpath) as f:\n            try:\n                return json.load(f)\n            except ValueError:\n                f.seek(0)  # try again\n            try:\n                import yaml\n            except ImportError:\n                raise WorkflowError(\"Config file is not valid JSON and PyYAML \"\n                                    \"has not been installed. Please install \"\n                                    \"PyYAML to use YAML config files.\")\n            try:\n                return yaml.load(f)\n            except yaml.YAMLError:\n                raise WorkflowError(\"Config file is not valid JSON or YAML.\")\n    except FileNotFoundError:\n        raise WorkflowError(\"Config file {} not found.\".format(configpath))\n\n\ndef load_configfile(configpath):\n    \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\"\n    config = _load_configfile(configpath)\n    if not isinstance(config, dict):\n        raise WorkflowError(\"Config file must be given as JSON or YAML \"\n                            \"with keys at top level.\")\n    return config\n\n##### Wildcard pumping detection #####\n\n\nclass PeriodicityDetector:\n    def __init__(self, min_repeat=50, max_repeat=100):\n        \"\"\"\n        Args:\n            max_len (int): The maximum length of the periodic substring.\n        \"\"\"\n        self.regex = re.compile(\n            \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format(\n                min_repeat=min_repeat - 1,\n                max_repeat=max_repeat - 1))\n\n    def is_periodic(self, value):\n        \"\"\"Returns the periodic substring or None if not periodic.\"\"\"\n        m = self.regex.search(value)  # search for a periodic suffix.\n        if m is not None:\n            return m.group(\"value\")\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kdaily/snakemake/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/jobs.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport sys\nimport base64\nimport json\n\nfrom collections import defaultdict\nfrom itertools import chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.io import IOFile, Wildcards, Resources, _IOFile\nfrom snakemake.utils import format, listfiles\nfrom snakemake.exceptions import RuleException, ProtectedOutputException\nfrom snakemake.exceptions import UnexpectedOutputException\nfrom snakemake.logging import logger\n\n\ndef jobfiles(jobs, type):\n    return chain(*map(attrgetter(type), jobs))\n\n\nclass Job:\n    HIGHEST_PRIORITY = sys.maxsize\n\n    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):\n        self.rule = rule\n        self.dag = dag\n        self.targetfile = targetfile\n\n        self.wildcards_dict = self.rule.get_wildcards(targetfile)\n        self.wildcards = Wildcards(fromdict=self.wildcards_dict)\n        self._format_wildcards = (self.wildcards if format_wildcards is None\n                                  else Wildcards(fromdict=format_wildcards))\n\n        (self.input, self.output, self.params, self.log, self.benchmark,\n         self.ruleio,\n         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)\n\n        self.resources_dict = {\n            name: min(self.rule.workflow.global_resources.get(name, res), res)\n            for name, res in rule.resources.items()\n        }\n        self.threads = self.resources_dict[\"_cores\"]\n        self.resources = Resources(fromdict=self.resources_dict)\n        self._inputsize = None\n\n        self.dynamic_output, self.dynamic_input = set(), set()\n        self.temp_output, self.protected_output = set(), set()\n        self.touch_output = set()\n        self.subworkflow_input = dict()\n        for f in self.output:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_output:\n                self.dynamic_output.add(f)\n            if f_ in self.rule.temp_output:\n                self.temp_output.add(f)\n            if f_ in self.rule.protected_output:\n                self.protected_output.add(f)\n            if f_ in self.rule.touch_output:\n                self.touch_output.add(f)\n        for f in self.input:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_input:\n                self.dynamic_input.add(f)\n            if f_ in self.rule.subworkflow_input:\n                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]\n        self._hash = self.rule.__hash__()\n        if True or not self.dynamic_output:\n            for o in self.output:\n                self._hash ^= o.__hash__()\n\n    @property\n    def priority(self):\n        return self.dag.priority(self)\n\n    @property\n    def b64id(self):\n        return base64.b64encode((self.rule.name + \"\".join(self.output)\n                                 ).encode(\"utf-8\")).decode(\"utf-8\")\n\n    @property\n    def inputsize(self):\n        \"\"\"\n        Return the size of the input files.\n        Input files need to be present.\n        \"\"\"\n        if self._inputsize is None:\n            self._inputsize = sum(f.size for f in self.input)\n        return self._inputsize\n\n    @property\n    def message(self):\n        \"\"\" Return the message for this job. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.message) if\n                    self.rule.message else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable in message \"\n                                \"of shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def shellcmd(self):\n        \"\"\" Return the shell command. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.shellcmd) if\n                    self.rule.shellcmd else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable when printing \"\n                                \"shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def expanded_output(self):\n        \"\"\" Iterate over output files while dynamic output is expanded. \"\"\"\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                expansion = self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill)\n                if not expansion:\n                    yield f_\n                for f, _ in expansion:\n                    yield IOFile(f, self.rule)\n            else:\n                yield f\n\n    @property\n    def dynamic_wildcards(self):\n        \"\"\" Return all wildcard values determined from dynamic output. \"\"\"\n        combinations = set()\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                for f, w in self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill):\n                    combinations.add(tuple(w.items()))\n        wildcards = defaultdict(list)\n        for combination in combinations:\n            for name, value in combination:\n                wildcards[name].append(value)\n        return wildcards\n\n    @property\n    def missing_input(self):\n        \"\"\" Return missing input files. \"\"\"\n        # omit file if it comes from a subworkflow\n        return set(f for f in self.input\n                   if not f.exists and not f in self.subworkflow_input)\n\n    @property\n    def output_mintime(self):\n        \"\"\" Return oldest output file. \"\"\"\n        existing = [f.mtime for f in self.expanded_output if f.exists]\n        if self.benchmark and self.benchmark.exists:\n            existing.append(self.benchmark.mtime)\n        if existing:\n            return min(existing)\n        return None\n\n    @property\n    def input_maxtime(self):\n        \"\"\" Return newest input file. \"\"\"\n        existing = [f.mtime for f in self.input if f.exists]\n        if existing:\n            return max(existing)\n        return None\n\n    def missing_output(self, requested=None):\n        \"\"\" Return missing output files. \"\"\"\n        files = set()\n        if self.benchmark and (requested is None or\n                               self.benchmark in requested):\n            if not self.benchmark.exists:\n                files.add(self.benchmark)\n\n        for f, f_ in zip(self.output, self.rule.output):\n            if requested is None or f in requested:\n                if f in self.dynamic_output:\n                    if not self.expand_dynamic(\n                        f_,\n                        restriction=self.wildcards,\n                        omit_value=_IOFile.dynamic_fill):\n                        files.add(\"{} (dynamic)\".format(f_))\n                elif not f.exists:\n                    files.add(f)\n        return files\n\n    @property\n    def existing_output(self):\n        return filter(lambda f: f.exists, self.expanded_output)\n\n    def check_protected_output(self):\n        protected = list(filter(lambda f: f.protected, self.expanded_output))\n        if protected:\n            raise ProtectedOutputException(self.rule, protected)\n\n    def prepare(self):\n        \"\"\"\n        Prepare execution of job.\n        This includes creation of directories and deletion of previously\n        created dynamic files.\n        \"\"\"\n\n        self.check_protected_output()\n\n        unexpected_output = self.dag.reason(self).missing_output.intersection(\n            self.existing_output)\n        if unexpected_output:\n            logger.warning(\n                \"Warning: the following output files of rule {} were not \"\n                \"present when the DAG was created:\\n{}\".format(\n                    self.rule, unexpected_output))\n\n        if self.dynamic_output:\n            for f, _ in chain(*map(partial(self.expand_dynamic,\n                                           restriction=self.wildcards,\n                                           omit_value=_IOFile.dynamic_fill),\n                                   self.rule.dynamic_output)):\n                os.remove(f)\n        for f, f_ in zip(self.output, self.rule.output):\n            f.prepare()\n        for f in self.log:\n            f.prepare()\n        if self.benchmark:\n            self.benchmark.prepare()\n\n    def cleanup(self):\n        \"\"\" Cleanup output files. \"\"\"\n        to_remove = [f for f in self.expanded_output if f.exists]\n        if to_remove:\n            logger.info(\"Removing output files of failed job {}\"\n                        \" since they might be corrupted:\\n{}\".format(\n                            self, \", \".join(to_remove)))\n            for f in to_remove:\n                f.remove()\n\n    def format_wildcards(self, string, **variables):\n        \"\"\" Format a string with variables from the job. \"\"\"\n        _variables = dict()\n        _variables.update(self.rule.workflow.globals)\n        _variables.update(dict(input=self.input,\n                               output=self.output,\n                               params=self.params,\n                               wildcards=self._format_wildcards,\n                               threads=self.threads,\n                               resources=self.resources,\n                               log=self.log,\n                               version=self.rule.version,\n                               rule=self.rule.name, ))\n        _variables.update(variables)\n        try:\n            return format(string, **_variables)\n        except NameError as ex:\n            raise RuleException(\"NameError: \" + str(ex), rule=self.rule)\n        except IndexError as ex:\n            raise RuleException(\"IndexError: \" + str(ex), rule=self.rule)\n\n    def properties(self, omit_resources=\"_cores _nodes\".split()):\n        resources = {\n            name: res\n            for name, res in self.resources.items()\n            if name not in omit_resources\n        }\n        params = {name: value for name, value in self.params.items()}\n        properties = {\n            \"rule\": self.rule.name,\n            \"local\": self.dag.workflow.is_local(self.rule),\n            \"input\": self.input,\n            \"output\": self.output,\n            \"params\": params,\n            \"threads\": self.threads,\n            \"resources\": resources\n        }\n        return properties\n\n    def json(self):\n        return json.dumps(self.properties())\n\n    def __repr__(self):\n        return self.rule.name\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return self.rule == other.rule and (\n            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)\n\n    def __lt__(self, other):\n        return self.rule.__lt__(other.rule)\n\n    def __gt__(self, other):\n        return self.rule.__gt__(other.rule)\n\n    def __hash__(self):\n        return self._hash\n\n    @staticmethod\n    def expand_dynamic(pattern, restriction=None, omit_value=None):\n        \"\"\" Expand dynamic files. \"\"\"\n        return list(listfiles(pattern,\n                              restriction=restriction,\n                              omit_value=omit_value))\n\n\nclass Reason:\n    def __init__(self):\n        self.updated_input = set()\n        self.updated_input_run = set()\n        self.missing_output = set()\n        self.incomplete_output = set()\n        self.forced = False\n        self.noio = False\n        self.nooutput = False\n        self.derived = True\n\n    def __str__(self):\n        s = list()\n        if self.forced:\n            s.append(\"Forced execution\")\n        else:\n            if self.noio:\n                s.append(\"Rules with neither input nor \"\n                         \"output files are always executed.\")\n            elif self.nooutput:\n                s.append(\"Rules with a run or shell declaration but no output \"\n                         \"are always executed.\")\n            else:\n                if self.missing_output:\n                    s.append(\"Missing output files: {}\".format(\n                        \", \".join(self.missing_output)))\n                if self.incomplete_output:\n                    s.append(\"Incomplete output files: {}\".format(\n                        \", \".join(self.incomplete_output)))\n                updated_input = self.updated_input - self.updated_input_run\n                if updated_input:\n                    s.append(\"Updated input files: {}\".format(\n                        \", \".join(updated_input)))\n                if self.updated_input_run:\n                    s.append(\"Input files updated by another job: {}\".format(\n                        \", \".join(self.updated_input_run)))\n        s = \"; \".join(s)\n        return s\n\n    def __bool__(self):\n        return bool(self.updated_input or self.missing_output or self.forced or\n                    self.updated_input_run or self.noio or self.nooutput)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kdaily/snakemake/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/rules.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport sys\nimport inspect\nimport sre_constants\nfrom collections import defaultdict\n\nfrom snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist\nfrom snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log\nfrom snakemake.io import apply_wildcards, is_flagged, not_iterable\nfrom snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException\n\n\nclass Rule:\n    def __init__(self, *args, lineno=None, snakefile=None):\n        \"\"\"\n        Create a rule\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if len(args) == 2:\n            name, workflow = args\n            self.name = name\n            self.workflow = workflow\n            self.docstring = None\n            self.message = None\n            self._input = InputFiles()\n            self._output = OutputFiles()\n            self._params = Params()\n            self.dependencies = dict()\n            self.dynamic_output = set()\n            self.dynamic_input = set()\n            self.temp_output = set()\n            self.protected_output = set()\n            self.touch_output = set()\n            self.subworkflow_input = dict()\n            self.resources = dict(_cores=1, _nodes=1)\n            self.priority = 0\n            self.version = None\n            self._log = Log()\n            self._benchmark = None\n            self.wildcard_names = set()\n            self.lineno = lineno\n            self.snakefile = snakefile\n            self.run_func = None\n            self.shellcmd = None\n            self.norun = False\n        elif len(args) == 1:\n            other = args[0]\n            self.name = other.name\n            self.workflow = other.workflow\n            self.docstring = other.docstring\n            self.message = other.message\n            self._input = InputFiles(other._input)\n            self._output = OutputFiles(other._output)\n            self._params = Params(other._params)\n            self.dependencies = dict(other.dependencies)\n            self.dynamic_output = set(other.dynamic_output)\n            self.dynamic_input = set(other.dynamic_input)\n            self.temp_output = set(other.temp_output)\n            self.protected_output = set(other.protected_output)\n            self.touch_output = set(other.touch_output)\n            self.subworkflow_input = dict(other.subworkflow_input)\n            self.resources = other.resources\n            self.priority = other.priority\n            self.version = other.version\n            self._log = other._log\n            self._benchmark = other._benchmark\n            self.wildcard_names = set(other.wildcard_names)\n            self.lineno = other.lineno\n            self.snakefile = other.snakefile\n            self.run_func = other.run_func\n            self.shellcmd = other.shellcmd\n            self.norun = other.norun\n\n    def dynamic_branch(self, wildcards, input=True):\n        def get_io(rule):\n            return (rule.input, rule.dynamic_input) if input else (\n                rule.output, rule.dynamic_output\n            )\n\n        io, dynamic_io = get_io(self)\n\n        branch = Rule(self)\n        io_, dynamic_io_ = get_io(branch)\n\n        expansion = defaultdict(list)\n        for i, f in enumerate(io):\n            if f in dynamic_io:\n                try:\n                    for e in reversed(expand(f, zip, **wildcards)):\n                        expansion[i].append(IOFile(e, rule=branch))\n                except KeyError:\n                    return None\n\n        # replace the dynamic files with the expanded files\n        replacements = [(i, io[i], e)\n                        for i, e in reversed(list(expansion.items()))]\n        for i, old, exp in replacements:\n            dynamic_io_.remove(old)\n            io_.insert_items(i, exp)\n\n        if not input:\n            for i, old, exp in replacements:\n                if old in branch.temp_output:\n                    branch.temp_output.discard(old)\n                    branch.temp_output.update(exp)\n                if old in branch.protected_output:\n                    branch.protected_output.discard(old)\n                    branch.protected_output.update(exp)\n                if old in branch.touch_output:\n                    branch.touch_output.discard(old)\n                    branch.touch_output.update(exp)\n\n            branch.wildcard_names.clear()\n            non_dynamic_wildcards = dict((name, values[0])\n                                         for name, values in wildcards.items()\n                                         if len(set(values)) == 1)\n            # TODO have a look into how to concretize dependencies here\n            (branch._input, branch._output, branch._params, branch._log,\n             branch._benchmark, _, branch.dependencies\n             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)\n            return branch, non_dynamic_wildcards\n        return branch\n\n    def has_wildcards(self):\n        \"\"\"\n        Return True if rule contains wildcards.\n        \"\"\"\n        return bool(self.wildcard_names)\n\n    @property\n    def benchmark(self):\n        return self._benchmark\n\n    @benchmark.setter\n    def benchmark(self, benchmark):\n        self._benchmark = IOFile(benchmark, rule=self)\n\n    @property\n    def input(self):\n        return self._input\n\n    def set_input(self, *input, **kwinput):\n        \"\"\"\n        Add a list of input files. Recursive lists are flattened.\n\n        Arguments\n        input -- the list of input files\n        \"\"\"\n        for item in input:\n            self._set_inoutput_item(item)\n        for name, item in kwinput.items():\n            self._set_inoutput_item(item, name=name)\n\n    @property\n    def output(self):\n        return self._output\n\n    @property\n    def products(self):\n        products = list(self.output)\n        if self.benchmark:\n            products.append(self.benchmark)\n        return products\n\n    def set_output(self, *output, **kwoutput):\n        \"\"\"\n        Add a list of output files. Recursive lists are flattened.\n\n        Arguments\n        output -- the list of output files\n        \"\"\"\n        for item in output:\n            self._set_inoutput_item(item, output=True)\n        for name, item in kwoutput.items():\n            self._set_inoutput_item(item, output=True, name=name)\n\n        for item in self.output:\n            if self.dynamic_output and item not in self.dynamic_output:\n                raise SyntaxError(\n                    \"A rule with dynamic output may not define any \"\n                    \"non-dynamic output files.\")\n            wildcards = item.get_wildcard_names()\n            if self.wildcard_names:\n                if self.wildcard_names != wildcards:\n                    raise SyntaxError(\n                        \"Not all output files of rule {} \"\n                        \"contain the same wildcards.\".format(self.name))\n            else:\n                self.wildcard_names = wildcards\n\n    def _set_inoutput_item(self, item, output=False, name=None):\n        \"\"\"\n        Set an item to be input or output.\n\n        Arguments\n        item     -- the item\n        inoutput -- either a Namedlist of input or output items\n        name     -- an optional name for the item\n        \"\"\"\n        inoutput = self.output if output else self.input\n        if isinstance(item, str):\n            # add the rule to the dependencies\n            if isinstance(item, _IOFile):\n                self.dependencies[item] = item.rule\n            _item = IOFile(item, rule=self)\n            if is_flagged(item, \"temp\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be temporary\")\n                self.temp_output.add(_item)\n            if is_flagged(item, \"protected\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be protected\")\n                self.protected_output.add(_item)\n            if is_flagged(item, \"touch\"):\n                if not output:\n                    raise SyntaxError(\n                        \"Only output files may be marked for touching.\")\n                self.touch_output.add(_item)\n            if is_flagged(item, \"dynamic\"):\n                if output:\n                    self.dynamic_output.add(_item)\n                else:\n                    self.dynamic_input.add(_item)\n            if is_flagged(item, \"subworkflow\"):\n                if output:\n                    raise SyntaxError(\n                        \"Only input files may refer to a subworkflow\")\n                else:\n                    # record the workflow this item comes from\n                    self.subworkflow_input[_item] = item.flags[\"subworkflow\"]\n            inoutput.append(_item)\n            if name:\n                inoutput.add_name(name)\n        elif callable(item):\n            if output:\n                raise SyntaxError(\n                    \"Only input files can be specified as functions\")\n            inoutput.append(item)\n            if name:\n                inoutput.add_name(name)\n        else:\n            try:\n                start = len(inoutput)\n                for i in item:\n                    self._set_inoutput_item(i, output=output)\n                if name:\n                    # if the list was named, make it accessible\n                    inoutput.set_name(name, start, end=len(inoutput))\n            except TypeError:\n                raise SyntaxError(\n                    \"Input and output files have to be specified as strings or lists of strings.\")\n\n    @property\n    def params(self):\n        return self._params\n\n    def set_params(self, *params, **kwparams):\n        for item in params:\n            self._set_params_item(item)\n        for name, item in kwparams.items():\n            self._set_params_item(item, name=name)\n\n    def _set_params_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.params.append(item)\n            if name:\n                self.params.add_name(name)\n        else:\n            try:\n                start = len(self.params)\n                for i in item:\n                    self._set_params_item(i)\n                if name:\n                    self.params.set_name(name, start, end=len(self.params))\n            except TypeError:\n                raise SyntaxError(\"Params have to be specified as strings.\")\n\n    @property\n    def log(self):\n        return self._log\n\n    def set_log(self, *logs, **kwlogs):\n        for item in logs:\n            self._set_log_item(item)\n        for name, item in kwlogs.items():\n            self._set_log_item(item, name=name)\n\n    def _set_log_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.log.append(IOFile(item,\n                                   rule=self)\n                            if isinstance(item, str) else item)\n            if name:\n                self.log.add_name(name)\n        else:\n            try:\n                start = len(self.log)\n                for i in item:\n                    self._set_log_item(i)\n                if name:\n                    self.log.set_name(name, start, end=len(self.log))\n            except TypeError:\n                raise SyntaxError(\"Log files have to be specified as strings.\")\n\n    def expand_wildcards(self, wildcards=None):\n        \"\"\"\n        Expand wildcards depending on the requested output\n        or given wildcards dict.\n        \"\"\"\n\n        def concretize_iofile(f, wildcards):\n            if not isinstance(f, _IOFile):\n                return IOFile(f, rule=self)\n            else:\n                return f.apply_wildcards(wildcards,\n                                         fill_missing=f in self.dynamic_input,\n                                         fail_dynamic=self.dynamic_output)\n\n        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,\n                             concretize=apply_wildcards,\n                             ruleio=None):\n            for name, item in olditems.allitems():\n                start = len(newitems)\n                is_iterable = True\n                if callable(item):\n                    try:\n                        item = item(wildcards_obj)\n                    except (Exception, BaseException) as e:\n                        raise InputFunctionException(e, rule=self)\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        if not isinstance(item_, str):\n                            raise RuleException(\n                                \"Input function did not return str or list of str.\",\n                                rule=self)\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                else:\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                if name:\n                    newitems.set_name(\n                        name, start,\n                        end=len(newitems) if is_iterable else None)\n\n        if wildcards is None:\n            wildcards = dict()\n        missing_wildcards = self.wildcard_names - set(wildcards.keys())\n\n        if missing_wildcards:\n            raise RuleException(\n                \"Could not resolve wildcards in rule {}:\\n{}\".format(\n                    self.name, \"\\n\".join(self.wildcard_names)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n        ruleio = dict()\n\n        try:\n            input = InputFiles()\n            wildcards_obj = Wildcards(fromdict=wildcards)\n            _apply_wildcards(input, self.input, wildcards, wildcards_obj,\n                             concretize=concretize_iofile,\n                             ruleio=ruleio)\n\n            params = Params()\n            _apply_wildcards(params, self.params, wildcards, wildcards_obj)\n\n            output = OutputFiles(o.apply_wildcards(wildcards)\n                                 for o in self.output)\n            output.take_names(self.output.get_names())\n\n            dependencies = {\n                None if f is None else f.apply_wildcards(wildcards): rule\n                for f, rule in self.dependencies.items()\n            }\n\n            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))\n\n            log = Log()\n            _apply_wildcards(log, self.log, wildcards, wildcards_obj,\n                             concretize=concretize_iofile)\n\n            benchmark = self.benchmark.apply_wildcards(\n                wildcards) if self.benchmark else None\n            return input, output, params, log, benchmark, ruleio, dependencies\n        except WildcardError as ex:\n            # this can only happen if an input contains an unresolved wildcard.\n            raise RuleException(\n                \"Wildcards in input, params, log or benchmark file of rule {} cannot be \"\n                \"determined from output files:\\n{}\".format(self, str(ex)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n    def is_producer(self, requested_output):\n        \"\"\"\n        Returns True if this rule is a producer of the requested output.\n        \"\"\"\n        try:\n            for o in self.products:\n                if o.match(requested_output):\n                    return True\n            return False\n        except sre_constants.error as ex:\n            raise IOFileException(\"{} in wildcard statement\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n        except ValueError as ex:\n            raise IOFileException(\"{}\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n\n    def get_wildcards(self, requested_output):\n        \"\"\"\n        Update the given wildcard dictionary by matching regular expression\n        output files to the requested concrete ones.\n\n        Arguments\n        wildcards -- a dictionary of wildcards\n        requested_output -- a concrete filepath\n        \"\"\"\n        if requested_output is None:\n            return dict()\n        bestmatchlen = 0\n        bestmatch = None\n\n        for o in self.products:\n            match = o.match(requested_output)\n            if match:\n                l = self.get_wildcard_len(match.groupdict())\n                if not bestmatch or bestmatchlen > l:\n                    bestmatch = match.groupdict()\n                    bestmatchlen = l\n        return bestmatch\n\n    @staticmethod\n    def get_wildcard_len(wildcards):\n        \"\"\"\n        Return the length of the given wildcard values.\n\n        Arguments\n        wildcards -- a dict of wildcards\n        \"\"\"\n        return sum(map(len, wildcards.values()))\n\n    def __lt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp < 0\n\n    def __gt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp > 0\n\n    def __str__(self):\n        return self.name\n\n    def __hash__(self):\n        return self.name.__hash__()\n\n    def __eq__(self, other):\n        return self.name == other.name\n\n\nclass Ruleorder:\n    def __init__(self):\n        self.order = list()\n\n    def add(self, *rulenames):\n        \"\"\"\n        Records the order of given rules as rule1 > rule2 > rule3, ...\n        \"\"\"\n        self.order.append(list(rulenames))\n\n    def compare(self, rule1, rule2):\n        \"\"\"\n        Return whether rule2 has a higher priority than rule1.\n        \"\"\"\n        # try the last clause first,\n        # i.e. clauses added later overwrite those before.\n        for clause in reversed(self.order):\n            try:\n                i = clause.index(rule1.name)\n                j = clause.index(rule2.name)\n                # rules with higher priority should have a smaller index\n                comp = j - i\n                if comp < 0:\n                    comp = -1\n                elif comp > 0:\n                    comp = 1\n                return comp\n            except ValueError:\n                pass\n\n        # if not ruleorder given, prefer rule without wildcards\n        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()\n        if wildcard_cmp != 0:\n            return wildcard_cmp\n\n        return 0\n\n    def __iter__(self):\n        return self.order.__iter__()\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kdaily/snakemake/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/workflow.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport re\nimport os\nimport sys\nimport signal\nimport json\nimport urllib\nfrom collections import OrderedDict\nfrom itertools import filterfalse, chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.logging import logger, format_resources, format_resource_names\nfrom snakemake.rules import Rule, Ruleorder\nfrom snakemake.exceptions import RuleException, CreateRuleException, \\\n    UnknownRuleException, NoRulesException, print_exception, WorkflowError\nfrom snakemake.shell import shell\nfrom snakemake.dag import DAG\nfrom snakemake.scheduler import JobScheduler\nfrom snakemake.parser import parse\nimport snakemake.io\nfrom snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\nfrom snakemake.persistence import Persistence\nfrom snakemake.utils import update_config\n\n\nclass Workflow:\n    def __init__(self,\n                 snakefile=None,\n                 snakemakepath=None,\n                 jobscript=None,\n                 overwrite_shellcmd=None,\n                 overwrite_config=dict(),\n                 overwrite_workdir=None,\n                 overwrite_configfile=None,\n                 config_args=None,\n                 debug=False):\n        \"\"\"\n        Create the controller.\n        \"\"\"\n        self._rules = OrderedDict()\n        self.first_rule = None\n        self._workdir = None\n        self.overwrite_workdir = overwrite_workdir\n        self.workdir_init = os.path.abspath(os.curdir)\n        self._ruleorder = Ruleorder()\n        self._localrules = set()\n        self.linemaps = dict()\n        self.rule_count = 0\n        self.basedir = os.path.dirname(snakefile)\n        self.snakefile = os.path.abspath(snakefile)\n        self.snakemakepath = snakemakepath\n        self.included = []\n        self.included_stack = []\n        self.jobscript = jobscript\n        self.persistence = None\n        self.global_resources = None\n        self.globals = globals()\n        self._subworkflows = dict()\n        self.overwrite_shellcmd = overwrite_shellcmd\n        self.overwrite_config = overwrite_config\n        self.overwrite_configfile = overwrite_configfile\n        self.config_args = config_args\n        self._onsuccess = lambda log: None\n        self._onerror = lambda log: None\n        self.debug = debug\n\n        global config\n        config = dict()\n        config.update(self.overwrite_config)\n\n        global rules\n        rules = Rules()\n\n    @property\n    def subworkflows(self):\n        return self._subworkflows.values()\n\n    @property\n    def rules(self):\n        return self._rules.values()\n\n    @property\n    def concrete_files(self):\n        return (\n            file\n            for rule in self.rules for file in chain(rule.input, rule.output)\n            if not callable(file) and not file.contains_wildcard()\n        )\n\n    def check(self):\n        for clause in self._ruleorder:\n            for rulename in clause:\n                if not self.is_rule(rulename):\n                    raise UnknownRuleException(\n                        rulename,\n                        prefix=\"Error in ruleorder definition.\")\n\n    def add_rule(self, name=None, lineno=None, snakefile=None):\n        \"\"\"\n        Add a rule.\n        \"\"\"\n        if name is None:\n            name = str(len(self._rules) + 1)\n        if self.is_rule(name):\n            raise CreateRuleException(\n                \"The name {} is already used by another rule\".format(name))\n        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)\n        self._rules[rule.name] = rule\n        self.rule_count += 1\n        if not self.first_rule:\n            self.first_rule = rule.name\n        return name\n\n    def is_rule(self, name):\n        \"\"\"\n        Return True if name is the name of a rule.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        return name in self._rules\n\n    def get_rule(self, name):\n        \"\"\"\n        Get rule by name.\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if not self._rules:\n            raise NoRulesException()\n        if not name in self._rules:\n            raise UnknownRuleException(name)\n        return self._rules[name]\n\n    def list_rules(self, only_targets=False):\n        rules = self.rules\n        if only_targets:\n            rules = filterfalse(Rule.has_wildcards, rules)\n        for rule in rules:\n            logger.rule_info(name=rule.name, docstring=rule.docstring)\n\n    def list_resources(self):\n        for resource in set(\n            resource for rule in self.rules for resource in rule.resources):\n            if resource not in \"_cores _nodes\".split():\n                logger.info(resource)\n\n    def is_local(self, rule):\n        return rule.name in self._localrules or rule.norun\n\n    def execute(self,\n                targets=None,\n                dryrun=False,\n                touch=False,\n                cores=1,\n                nodes=1,\n                local_cores=1,\n                forcetargets=False,\n                forceall=False,\n                forcerun=None,\n                prioritytargets=None,\n                quiet=False,\n                keepgoing=False,\n                printshellcmds=False,\n                printreason=False,\n                printdag=False,\n                cluster=None,\n                cluster_config=None,\n                cluster_sync=None,\n                jobname=None,\n                immediate_submit=False,\n                ignore_ambiguity=False,\n                printrulegraph=False,\n                printd3dag=False,\n                drmaa=None,\n                stats=None,\n                force_incomplete=False,\n                ignore_incomplete=False,\n                list_version_changes=False,\n                list_code_changes=False,\n                list_input_changes=False,\n                list_params_changes=False,\n                summary=False,\n                detailed_summary=False,\n                latency_wait=3,\n                benchmark_repeats=3,\n                wait_for_files=None,\n                nolock=False,\n                unlock=False,\n                resources=None,\n                notemp=False,\n                nodeps=False,\n                cleanup_metadata=None,\n                subsnakemake=None,\n                updated_files=None,\n                keep_target_files=False,\n                allowed_rules=None,\n                greediness=1.0,\n                no_hooks=False):\n\n        self.global_resources = dict() if resources is None else resources\n        self.global_resources[\"_cores\"] = cores\n        self.global_resources[\"_nodes\"] = nodes\n\n        def rules(items):\n            return map(self._rules.__getitem__, filter(self.is_rule, items))\n\n        if keep_target_files:\n\n            def files(items):\n                return filterfalse(self.is_rule, items)\n        else:\n\n            def files(items):\n                return map(os.path.relpath, filterfalse(self.is_rule, items))\n\n        if not targets:\n            targets = [self.first_rule\n                       ] if self.first_rule is not None else list()\n        if prioritytargets is None:\n            prioritytargets = list()\n        if forcerun is None:\n            forcerun = list()\n\n        priorityrules = set(rules(prioritytargets))\n        priorityfiles = set(files(prioritytargets))\n        forcerules = set(rules(forcerun))\n        forcefiles = set(files(forcerun))\n        targetrules = set(chain(rules(targets),\n                                filterfalse(Rule.has_wildcards, priorityrules),\n                                filterfalse(Rule.has_wildcards, forcerules)))\n        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))\n        if forcetargets:\n            forcefiles.update(targetfiles)\n            forcerules.update(targetrules)\n\n        rules = self.rules\n        if allowed_rules:\n            rules = [rule for rule in rules if rule.name in set(allowed_rules)]\n\n        if wait_for_files is not None:\n            try:\n                snakemake.io.wait_for_files(wait_for_files,\n                                            latency_wait=latency_wait)\n            except IOError as e:\n                logger.error(str(e))\n                return False\n\n        dag = DAG(\n            self, rules,\n            dryrun=dryrun,\n            targetfiles=targetfiles,\n            targetrules=targetrules,\n            forceall=forceall,\n            forcefiles=forcefiles,\n            forcerules=forcerules,\n            priorityfiles=priorityfiles,\n            priorityrules=priorityrules,\n            ignore_ambiguity=ignore_ambiguity,\n            force_incomplete=force_incomplete,\n            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,\n            notemp=notemp)\n\n        self.persistence = Persistence(\n            nolock=nolock,\n            dag=dag,\n            warn_only=dryrun or printrulegraph or printdag or summary or\n            list_version_changes or list_code_changes or list_input_changes or\n            list_params_changes)\n\n        if cleanup_metadata:\n            for f in cleanup_metadata:\n                self.persistence.cleanup_metadata(f)\n            return True\n\n        dag.init()\n        dag.check_dynamic()\n\n        if unlock:\n            try:\n                self.persistence.cleanup_locks()\n                logger.info(\"Unlocking working directory.\")\n                return True\n            except IOError:\n                logger.error(\"Error: Unlocking the directory {} failed. Maybe \"\n                             \"you don't have the permissions?\")\n                return False\n        try:\n            self.persistence.lock()\n        except IOError:\n            logger.error(\n                \"Error: Directory cannot be locked. Please make \"\n                \"sure that no other Snakemake process is trying to create \"\n                \"the same files in the following directory:\\n{}\\n\"\n                \"If you are sure that no other \"\n                \"instances of snakemake are running on this directory, \"\n                \"the remaining lock was likely caused by a kill signal or \"\n                \"a power loss. It can be removed with \"\n                \"the --unlock argument.\".format(os.getcwd()))\n            return False\n\n        if self.subworkflows and not printdag and not printrulegraph:\n            # backup globals\n            globals_backup = dict(self.globals)\n            # execute subworkflows\n            for subworkflow in self.subworkflows:\n                subworkflow_targets = subworkflow.targets(dag)\n                updated = list()\n                if subworkflow_targets:\n                    logger.info(\n                        \"Executing subworkflow {}.\".format(subworkflow.name))\n                    if not subsnakemake(subworkflow.snakefile,\n                                        workdir=subworkflow.workdir,\n                                        targets=subworkflow_targets,\n                                        updated_files=updated):\n                        return False\n                    dag.updated_subworkflow_files.update(subworkflow.target(f)\n                                                         for f in updated)\n                else:\n                    logger.info(\"Subworkflow {}: Nothing to be done.\".format(\n                        subworkflow.name))\n            if self.subworkflows:\n                logger.info(\"Executing main workflow.\")\n            # rescue globals\n            self.globals.update(globals_backup)\n\n        dag.check_incomplete()\n        dag.postprocess()\n\n        if nodeps:\n            missing_input = [f for job in dag.targetjobs for f in job.input\n                             if dag.needrun(job) and not os.path.exists(f)]\n            if missing_input:\n                logger.error(\n                    \"Dependency resolution disabled (--nodeps) \"\n                    \"but missing input \"\n                    \"files detected. If this happens on a cluster, please make sure \"\n                    \"that you handle the dependencies yourself or turn of \"\n                    \"--immediate-submit. Missing input files:\\n{}\".format(\n                        \"\\n\".join(missing_input)))\n                return False\n\n        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)\n\n        if printd3dag:\n            dag.d3dag()\n            return True\n        elif printdag:\n            print(dag)\n            return True\n        elif printrulegraph:\n            print(dag.rule_dot())\n            return True\n        elif summary:\n            print(\"\\n\".join(dag.summary(detailed=False)))\n            return True\n        elif detailed_summary:\n            print(\"\\n\".join(dag.summary(detailed=True)))\n            return True\n        elif list_version_changes:\n            items = list(\n                chain(*map(self.persistence.version_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_code_changes:\n            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_input_changes:\n            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_params_changes:\n            items = list(\n                chain(*map(self.persistence.params_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n\n        scheduler = JobScheduler(self, dag, cores,\n                                 local_cores=local_cores,\n                                 dryrun=dryrun,\n                                 touch=touch,\n                                 cluster=cluster,\n                                 cluster_config=cluster_config,\n                                 cluster_sync=cluster_sync,\n                                 jobname=jobname,\n                                 immediate_submit=immediate_submit,\n                                 quiet=quiet,\n                                 keepgoing=keepgoing,\n                                 drmaa=drmaa,\n                                 printreason=printreason,\n                                 printshellcmds=printshellcmds,\n                                 latency_wait=latency_wait,\n                                 benchmark_repeats=benchmark_repeats,\n                                 greediness=greediness)\n\n        if not dryrun and not quiet:\n            if len(dag):\n                if cluster or cluster_sync or drmaa:\n                    logger.resources_info(\n                        \"Provided cluster nodes: {}\".format(nodes))\n                else:\n                    logger.resources_info(\"Provided cores: {}\".format(cores))\n                    logger.resources_info(\"Rules claiming more threads will be scaled down.\")\n                provided_resources = format_resources(resources)\n                if provided_resources:\n                    logger.resources_info(\n                        \"Provided resources: \" + provided_resources)\n                ignored_resources = format_resource_names(\n                    set(resource for job in dag.needrun_jobs for resource in\n                        job.resources_dict if resource not in resources))\n                if ignored_resources:\n                    logger.resources_info(\n                        \"Ignored resources: \" + ignored_resources)\n                logger.run_info(\"\\n\".join(dag.stats()))\n            else:\n                logger.info(\"Nothing to be done.\")\n        if dryrun and not len(dag):\n            logger.info(\"Nothing to be done.\")\n\n        success = scheduler.schedule()\n\n        if success:\n            if dryrun:\n                if not quiet and len(dag):\n                    logger.run_info(\"\\n\".join(dag.stats()))\n            elif stats:\n                scheduler.stats.to_json(stats)\n            if not dryrun and not no_hooks:\n                self._onsuccess(logger.get_logfile())\n            return True\n        else:\n            if not dryrun and not no_hooks:\n                self._onerror(logger.get_logfile())\n            return False\n\n    def include(self, snakefile,\n                overwrite_first_rule=False,\n                print_compilation=False,\n                overwrite_shellcmd=None):\n        \"\"\"\n        Include a snakefile.\n        \"\"\"\n        # check if snakefile is a path to the filesystem\n        if not urllib.parse.urlparse(snakefile).scheme:\n            if not os.path.isabs(snakefile) and self.included_stack:\n                current_path = os.path.dirname(self.included_stack[-1])\n                snakefile = os.path.join(current_path, snakefile)\n            snakefile = os.path.abspath(snakefile)\n        # else it could be an url.\n        # at least we don't want to modify the path for clarity.\n\n        if snakefile in self.included:\n            logger.info(\"Multiple include of {} ignored\".format(snakefile))\n            return\n        self.included.append(snakefile)\n        self.included_stack.append(snakefile)\n\n        global workflow\n\n        workflow = self\n\n        first_rule = self.first_rule\n        code, linemap = parse(snakefile,\n                              overwrite_shellcmd=self.overwrite_shellcmd)\n\n        if print_compilation:\n            print(code)\n\n        # insert the current directory into sys.path\n        # this allows to import modules from the workflow directory\n        sys.path.insert(0, os.path.dirname(snakefile))\n\n        self.linemaps[snakefile] = linemap\n        exec(compile(code, snakefile, \"exec\"), self.globals)\n        if not overwrite_first_rule:\n            self.first_rule = first_rule\n        self.included_stack.pop()\n\n    def onsuccess(self, func):\n        self._onsuccess = func\n\n    def onerror(self, func):\n        self._onerror = func\n\n    def workdir(self, workdir):\n        if self.overwrite_workdir is None:\n            if not os.path.exists(workdir):\n                os.makedirs(workdir)\n            self._workdir = workdir\n            os.chdir(workdir)\n\n    def configfile(self, jsonpath):\n        \"\"\" Update the global config with the given dictionary. \"\"\"\n        global config\n        c = snakemake.io.load_configfile(jsonpath)\n        update_config(config, c)\n        update_config(config, self.overwrite_config)\n\n    def ruleorder(self, *rulenames):\n        self._ruleorder.add(*rulenames)\n\n    def subworkflow(self, name, snakefile=None, workdir=None):\n        sw = Subworkflow(self, name, snakefile, workdir)\n        self._subworkflows[name] = sw\n        self.globals[name] = sw.target\n\n    def localrules(self, *rulenames):\n        self._localrules.update(rulenames)\n\n    def rule(self, name=None, lineno=None, snakefile=None):\n        name = self.add_rule(name, lineno, snakefile)\n        rule = self.get_rule(name)\n\n        def decorate(ruleinfo):\n            if ruleinfo.input:\n                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])\n            if ruleinfo.output:\n                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])\n            if ruleinfo.params:\n                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])\n            if ruleinfo.threads:\n                if not isinstance(ruleinfo.threads, int):\n                    raise RuleException(\"Threads value has to be an integer.\",\n                                        rule=rule)\n                rule.resources[\"_cores\"] = ruleinfo.threads\n            if ruleinfo.resources:\n                args, resources = ruleinfo.resources\n                if args:\n                    raise RuleException(\"Resources have to be named.\")\n                if not all(map(lambda r: isinstance(r, int),\n                               resources.values())):\n                    raise RuleException(\n                        \"Resources values have to be integers.\",\n                        rule=rule)\n                rule.resources.update(resources)\n            if ruleinfo.priority:\n                if (not isinstance(ruleinfo.priority, int) and\n                    not isinstance(ruleinfo.priority, float)):\n                    raise RuleException(\"Priority values have to be numeric.\",\n                                        rule=rule)\n                rule.priority = ruleinfo.priority\n            if ruleinfo.version:\n                rule.version = ruleinfo.version\n            if ruleinfo.log:\n                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])\n            if ruleinfo.message:\n                rule.message = ruleinfo.message\n            if ruleinfo.benchmark:\n                rule.benchmark = ruleinfo.benchmark\n            rule.norun = ruleinfo.norun\n            rule.docstring = ruleinfo.docstring\n            rule.run_func = ruleinfo.func\n            rule.shellcmd = ruleinfo.shellcmd\n            ruleinfo.func.__name__ = \"__{}\".format(name)\n            self.globals[ruleinfo.func.__name__] = ruleinfo.func\n            setattr(rules, name, rule)\n            return ruleinfo.func\n\n        return decorate\n\n    def docstring(self, string):\n        def decorate(ruleinfo):\n            ruleinfo.docstring = string\n            return ruleinfo\n\n        return decorate\n\n    def input(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.input = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def output(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.output = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def params(self, *params, **kwparams):\n        def decorate(ruleinfo):\n            ruleinfo.params = (params, kwparams)\n            return ruleinfo\n\n        return decorate\n\n    def message(self, message):\n        def decorate(ruleinfo):\n            ruleinfo.message = message\n            return ruleinfo\n\n        return decorate\n\n    def benchmark(self, benchmark):\n        def decorate(ruleinfo):\n            ruleinfo.benchmark = benchmark\n            return ruleinfo\n\n        return decorate\n\n    def threads(self, threads):\n        def decorate(ruleinfo):\n            ruleinfo.threads = threads\n            return ruleinfo\n\n        return decorate\n\n    def resources(self, *args, **resources):\n        def decorate(ruleinfo):\n            ruleinfo.resources = (args, resources)\n            return ruleinfo\n\n        return decorate\n\n    def priority(self, priority):\n        def decorate(ruleinfo):\n            ruleinfo.priority = priority\n            return ruleinfo\n\n        return decorate\n\n    def version(self, version):\n        def decorate(ruleinfo):\n            ruleinfo.version = version\n            return ruleinfo\n\n        return decorate\n\n    def log(self, *logs, **kwlogs):\n        def decorate(ruleinfo):\n            ruleinfo.log = (logs, kwlogs)\n            return ruleinfo\n\n        return decorate\n\n    def shellcmd(self, cmd):\n        def decorate(ruleinfo):\n            ruleinfo.shellcmd = cmd\n            return ruleinfo\n\n        return decorate\n\n    def norun(self):\n        def decorate(ruleinfo):\n            ruleinfo.norun = True\n            return ruleinfo\n\n        return decorate\n\n    def run(self, func):\n        return RuleInfo(func)\n\n    @staticmethod\n    def _empty_decorator(f):\n        return f\n\n\nclass RuleInfo:\n    def __init__(self, func):\n        self.func = func\n        self.shellcmd = None\n        self.norun = False\n        self.input = None\n        self.output = None\n        self.params = None\n        self.message = None\n        self.benchmark = None\n        self.threads = None\n        self.resources = None\n        self.priority = None\n        self.version = None\n        self.log = None\n        self.docstring = None\n\n\nclass Subworkflow:\n    def __init__(self, workflow, name, snakefile, workdir):\n        self.workflow = workflow\n        self.name = name\n        self._snakefile = snakefile\n        self._workdir = workdir\n\n    @property\n    def snakefile(self):\n        if self._snakefile is None:\n            return os.path.abspath(os.path.join(self.workdir, \"Snakefile\"))\n        if not os.path.isabs(self._snakefile):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                self._snakefile))\n        return self._snakefile\n\n    @property\n    def workdir(self):\n        workdir = \".\" if self._workdir is None else self._workdir\n        if not os.path.isabs(workdir):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                workdir))\n        return workdir\n\n    def target(self, paths):\n        if not_iterable(paths):\n            return flag(os.path.join(self.workdir, paths), \"subworkflow\", self)\n        return [self.target(path) for path in paths]\n\n    def targets(self, dag):\n        return [f for job in dag.jobs for f in job.subworkflow_input\n                if job.subworkflow_input[f] is self]\n\n\nclass Rules:\n    \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\"\n    pass\n\n\ndef srcdir(path):\n    \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\"\n    if not workflow.included_stack:\n        return None\n    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/nh13/snakemake/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/dag.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport textwrap\nimport time\nfrom collections import defaultdict, Counter\nfrom itertools import chain, combinations, filterfalse, product, groupby\nfrom functools import partial, lru_cache\nfrom operator import itemgetter, attrgetter\n\nfrom snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\nfrom snakemake.jobs import Job, Reason\nfrom snakemake.exceptions import RuleException, MissingInputException\nfrom snakemake.exceptions import MissingRuleException, AmbiguousRuleException\nfrom snakemake.exceptions import CyclicGraphException, MissingOutputException\nfrom snakemake.exceptions import IncompleteFilesException\nfrom snakemake.exceptions import PeriodicWildcardError\nfrom snakemake.exceptions import UnexpectedOutputException, InputFunctionException\nfrom snakemake.logging import logger\nfrom snakemake.output_index import OutputIndex\n\n\nclass DAG:\n    def __init__(self, workflow,\n                 rules=None,\n                 dryrun=False,\n                 targetfiles=None,\n                 targetrules=None,\n                 forceall=False,\n                 forcerules=None,\n                 forcefiles=None,\n                 priorityfiles=None,\n                 priorityrules=None,\n                 ignore_ambiguity=False,\n                 force_incomplete=False,\n                 ignore_incomplete=False,\n                 notemp=False):\n\n        self.dryrun = dryrun\n        self.dependencies = defaultdict(partial(defaultdict, set))\n        self.depending = defaultdict(partial(defaultdict, set))\n        self._needrun = set()\n        self._priority = dict()\n        self._downstream_size = dict()\n        self._reason = defaultdict(Reason)\n        self._finished = set()\n        self._dynamic = set()\n        self._len = 0\n        self.workflow = workflow\n        self.rules = set(rules)\n        self.ignore_ambiguity = ignore_ambiguity\n        self.targetfiles = targetfiles\n        self.targetrules = targetrules\n        self.priorityfiles = priorityfiles\n        self.priorityrules = priorityrules\n        self.targetjobs = set()\n        self.prioritytargetjobs = set()\n        self._ready_jobs = set()\n        self.notemp = notemp\n        self._jobid = dict()\n\n        self.forcerules = set()\n        self.forcefiles = set()\n        self.updated_subworkflow_files = set()\n        if forceall:\n            self.forcerules.update(self.rules)\n        elif forcerules:\n            self.forcerules.update(forcerules)\n        if forcefiles:\n            self.forcefiles.update(forcefiles)\n        self.omitforce = set()\n\n        self.force_incomplete = force_incomplete\n        self.ignore_incomplete = ignore_incomplete\n\n        self.periodic_wildcard_detector = PeriodicityDetector()\n\n        self.update_output_index()\n\n    def init(self):\n        \"\"\" Initialise the DAG. \"\"\"\n        for job in map(self.rule2job, self.targetrules):\n            job = self.update([job])\n            self.targetjobs.add(job)\n\n        for file in self.targetfiles:\n            job = self.update(self.file2jobs(file), file=file)\n            self.targetjobs.add(job)\n\n        self.update_needrun()\n\n    def update_output_index(self):\n        self.output_index = OutputIndex(self.rules)\n\n    def check_incomplete(self):\n        if not self.ignore_incomplete:\n            incomplete = self.incomplete_files\n            if incomplete:\n                if self.force_incomplete:\n                    logger.debug(\"Forcing incomplete files:\")\n                    logger.debug(\"\\t\" + \"\\n\\t\".join(incomplete))\n                    self.forcefiles.update(incomplete)\n                else:\n                    raise IncompleteFilesException(incomplete)\n\n    def check_dynamic(self):\n        for job in filter(lambda job: (\n            job.dynamic_output and not self.needrun(job)\n        ), self.jobs):\n            self.update_dynamic(job)\n\n    @property\n    def dynamic_output_jobs(self):\n        return (job for job in self.jobs if job.dynamic_output)\n\n    @property\n    def jobs(self):\n        \"\"\" All jobs in the DAG. \"\"\"\n        for job in self.bfs(self.dependencies, *self.targetjobs):\n            yield job\n\n    @property\n    def needrun_jobs(self):\n        \"\"\" Jobs that need to be executed. \"\"\"\n        for job in filter(self.needrun,\n                          self.bfs(self.dependencies, *self.targetjobs,\n                                   stop=self.noneedrun_finished)):\n            yield job\n\n    @property\n    def local_needrun_jobs(self):\n        return filter(lambda job: self.workflow.is_local(job.rule),\n                      self.needrun_jobs)\n\n    @property\n    def finished_jobs(self):\n        \"\"\" Jobs that have been executed. \"\"\"\n        for job in filter(self.finished, self.bfs(self.dependencies,\n                                                  *self.targetjobs)):\n            yield job\n\n    @property\n    def ready_jobs(self):\n        \"\"\" Jobs that are ready to execute. \"\"\"\n        return self._ready_jobs\n\n    def ready(self, job):\n        \"\"\" Return whether a given job is ready to execute. \"\"\"\n        return job in self._ready_jobs\n\n    def needrun(self, job):\n        \"\"\" Return whether a given job needs to be executed. \"\"\"\n        return job in self._needrun\n\n    def priority(self, job):\n        return self._priority[job]\n\n    def downstream_size(self, job):\n        return self._downstream_size[job]\n\n    def _job_values(self, jobs, values):\n        return [values[job] for job in jobs]\n\n    def priorities(self, jobs):\n        return self._job_values(jobs, self._priority)\n\n    def downstream_sizes(self, jobs):\n        return self._job_values(jobs, self._downstream_size)\n\n    def noneedrun_finished(self, job):\n        \"\"\"\n        Return whether a given job is finished or was not\n        required to run at all.\n        \"\"\"\n        return not self.needrun(job) or self.finished(job)\n\n    def reason(self, job):\n        \"\"\" Return the reason of the job execution. \"\"\"\n        return self._reason[job]\n\n    def finished(self, job):\n        \"\"\" Return whether a job is finished. \"\"\"\n        return job in self._finished\n\n    def dynamic(self, job):\n        \"\"\"\n        Return whether a job is dynamic (i.e. it is only a placeholder\n        for those that are created after the job with dynamic output has\n        finished.\n        \"\"\"\n        return job in self._dynamic\n\n    def requested_files(self, job):\n        \"\"\" Return the files a job requests. \"\"\"\n        return set(*self.depending[job].values())\n\n    @property\n    def incomplete_files(self):\n        return list(chain(*(\n            job.output for job in filter(self.workflow.persistence.incomplete,\n                                         filterfalse(self.needrun, self.jobs))\n        )))\n\n    @property\n    def newversion_files(self):\n        return list(chain(*(\n            job.output\n            for job in filter(self.workflow.persistence.newversion, self.jobs)\n        )))\n\n    def missing_temp(self, job):\n        \"\"\"\n        Return whether a temp file that is input of the given job is missing.\n        \"\"\"\n        for job_, files in self.depending[job].items():\n            if self.needrun(job_) and any(not f.exists for f in files):\n                return True\n        return False\n\n    def check_output(self, job, wait=3):\n        \"\"\" Raise exception if output files of job are missing. \"\"\"\n        try:\n            wait_for_files(job.expanded_output, latency_wait=wait)\n        except IOError as e:\n            raise MissingOutputException(str(e), rule=job.rule)\n\n        input_maxtime = job.input_maxtime\n        if input_maxtime is not None:\n            output_mintime = job.output_mintime\n            if output_mintime is not None and output_mintime < input_maxtime:\n                raise RuleException(\n                    \"Output files {} are older than input \"\n                    \"files. Did you extract an archive? Make sure that output \"\n                    \"files have a more recent modification date than the \"\n                    \"archive, e.g. by using 'touch'.\".format(\n                        \", \".join(job.expanded_output)),\n                    rule=job.rule)\n\n    def check_periodic_wildcards(self, job):\n        \"\"\" Raise an exception if a wildcard of the given job appears to be periodic,\n        indicating a cyclic dependency. \"\"\"\n        for wildcard, value in job.wildcards_dict.items():\n            periodic_substring = self.periodic_wildcard_detector.is_periodic(\n                value)\n            if periodic_substring is not None:\n                raise PeriodicWildcardError(\n                    \"The value {} in wildcard {} is periodically repeated ({}). \"\n                    \"This would lead to an infinite recursion. \"\n                    \"To avoid this, e.g. restrict the wildcards in this rule to certain values.\".format(\n                        periodic_substring, wildcard, value),\n                    rule=job.rule)\n\n    def handle_protected(self, job):\n        \"\"\" Write-protect output files that are marked with protected(). \"\"\"\n        for f in job.expanded_output:\n            if f in job.protected_output:\n                logger.info(\"Write-protecting output file {}.\".format(f))\n                f.protect()\n\n    def handle_touch(self, job):\n        \"\"\" Touches those output files that are marked for touching. \"\"\"\n        for f in job.expanded_output:\n            if f in job.touch_output:\n                logger.info(\"Touching output file {}.\".format(f))\n                f.touch_or_create()\n\n    def handle_temp(self, job):\n        \"\"\" Remove temp files if they are no longer needed. \"\"\"\n        if self.notemp:\n            return\n\n        needed = lambda job_, f: any(\n            f in files for j, files in self.depending[job_].items()\n            if not self.finished(j) and self.needrun(j) and j != job)\n\n        def unneeded_files():\n            for job_, files in self.dependencies[job].items():\n                for f in job_.temp_output & files:\n                    if not needed(job_, f):\n                        yield f\n            for f in filterfalse(partial(needed, job), job.temp_output):\n                if not f in self.targetfiles:\n                    yield f\n\n        for f in unneeded_files():\n            logger.info(\"Removing temporary output file {}.\".format(f))\n            f.remove()\n\n    def jobid(self, job):\n        if job not in self._jobid:\n            self._jobid[job] = len(self._jobid)\n        return self._jobid[job]\n\n    def update(self, jobs, file=None, visited=None, skip_until_dynamic=False):\n        \"\"\" Update the DAG by adding given jobs and their dependencies. \"\"\"\n        if visited is None:\n            visited = set()\n        producer = None\n        exceptions = list()\n        jobs = sorted(jobs, reverse=not self.ignore_ambiguity)\n        cycles = list()\n\n        for job in jobs:\n            if file in job.input:\n                cycles.append(job)\n                continue\n            if job in visited:\n                cycles.append(job)\n                continue\n            try:\n                self.check_periodic_wildcards(job)\n                self.update_(job,\n                             visited=set(visited),\n                             skip_until_dynamic=skip_until_dynamic)\n                # TODO this might fail if a rule discarded here is needed\n                # elsewhere\n                if producer:\n                    if job < producer or self.ignore_ambiguity:\n                        break\n                    elif producer is not None:\n                        raise AmbiguousRuleException(file, job, producer)\n                producer = job\n            except (MissingInputException, CyclicGraphException,\n                    PeriodicWildcardError) as ex:\n                exceptions.append(ex)\n        if producer is None:\n            if cycles:\n                job = cycles[0]\n                raise CyclicGraphException(job.rule, file, rule=job.rule)\n            if exceptions:\n                raise exceptions[0]\n        return producer\n\n    def update_(self, job, visited=None, skip_until_dynamic=False):\n        \"\"\" Update the DAG by adding the given job and its dependencies. \"\"\"\n        if job in self.dependencies:\n            return\n        if visited is None:\n            visited = set()\n        visited.add(job)\n        dependencies = self.dependencies[job]\n        potential_dependencies = self.collect_potential_dependencies(\n            job).items()\n\n        skip_until_dynamic = skip_until_dynamic and not job.dynamic_output\n\n        missing_input = job.missing_input\n        producer = dict()\n        exceptions = dict()\n        for file, jobs in potential_dependencies:\n            try:\n                producer[file] = self.update(\n                    jobs,\n                    file=file,\n                    visited=visited,\n                    skip_until_dynamic=skip_until_dynamic or file in\n                    job.dynamic_input)\n            except (MissingInputException, CyclicGraphException,\n                    PeriodicWildcardError) as ex:\n                if file in missing_input:\n                    self.delete_job(job,\n                                    recursive=False)  # delete job from tree\n                    raise ex\n\n        for file, job_ in producer.items():\n            dependencies[job_].add(file)\n            self.depending[job_][job].add(file)\n\n        missing_input -= producer.keys()\n        if missing_input:\n            self.delete_job(job, recursive=False)  # delete job from tree\n            raise MissingInputException(job.rule, missing_input)\n\n        if skip_until_dynamic:\n            self._dynamic.add(job)\n\n    def update_needrun(self):\n        \"\"\" Update the information whether a job needs to be executed. \"\"\"\n\n        def output_mintime(job):\n            for job_ in self.bfs(self.depending, job):\n                t = job_.output_mintime\n                if t:\n                    return t\n\n        def needrun(job):\n            reason = self.reason(job)\n            noinitreason = not reason\n            updated_subworkflow_input = self.updated_subworkflow_files.intersection(\n                job.input)\n            if (job not in self.omitforce and job.rule in self.forcerules or\n                not self.forcefiles.isdisjoint(job.output)):\n                reason.forced = True\n            elif updated_subworkflow_input:\n                reason.updated_input.update(updated_subworkflow_input)\n            elif job in self.targetjobs:\n                # TODO find a way to handle added/removed input files here?\n                if not job.output and not job.benchmark:\n                    if job.input:\n                        if job.rule.norun:\n                            reason.updated_input_run.update([f\n                                                             for f in job.input\n                                                             if not f.exists])\n                        else:\n                            reason.nooutput = True\n                    else:\n                        reason.noio = True\n                else:\n                    if job.rule in self.targetrules:\n                        missing_output = job.missing_output()\n                    else:\n                        missing_output = job.missing_output(\n                            requested=set(chain(*self.depending[job].values()))\n                            | self.targetfiles)\n                    reason.missing_output.update(missing_output)\n            if not reason:\n                output_mintime_ = output_mintime(job)\n                if output_mintime_:\n                    updated_input = [\n                        f for f in job.input\n                        if f.exists and f.is_newer(output_mintime_)\n                    ]\n                    reason.updated_input.update(updated_input)\n            if noinitreason and reason:\n                reason.derived = False\n            return job\n\n        reason = self.reason\n        _needrun = self._needrun\n        dependencies = self.dependencies\n        depending = self.depending\n\n        _needrun.clear()\n        candidates = set(self.jobs)\n\n        queue = list(filter(reason, map(needrun, candidates)))\n        visited = set(queue)\n        while queue:\n            job = queue.pop(0)\n            _needrun.add(job)\n\n            for job_, files in dependencies[job].items():\n                missing_output = job_.missing_output(requested=files)\n                reason(job_).missing_output.update(missing_output)\n                if missing_output and not job_ in visited:\n                    visited.add(job_)\n                    queue.append(job_)\n\n            for job_, files in depending[job].items():\n                if job_ in candidates:\n                    reason(job_).updated_input_run.update(files)\n                    if not job_ in visited:\n                        visited.add(job_)\n                        queue.append(job_)\n\n        self._len = len(_needrun)\n\n    def update_priority(self):\n        \"\"\" Update job priorities. \"\"\"\n        prioritized = (lambda job: job.rule in self.priorityrules or\n                       not self.priorityfiles.isdisjoint(job.output))\n        for job in self.needrun_jobs:\n            self._priority[job] = job.rule.priority\n        for job in self.bfs(self.dependencies,\n                            *filter(prioritized, self.needrun_jobs),\n                            stop=self.noneedrun_finished):\n            self._priority[job] = Job.HIGHEST_PRIORITY\n\n    def update_ready(self):\n        \"\"\" Update information whether a job is ready to execute. \"\"\"\n        for job in filter(self.needrun, self.jobs):\n            if not self.finished(job) and self._ready(job):\n                self._ready_jobs.add(job)\n\n    def update_downstream_size(self):\n        for job in self.needrun_jobs:\n            self._downstream_size[job] = sum(\n                1 for _ in self.bfs(self.depending, job,\n                                    stop=self.noneedrun_finished)) - 1\n\n    def postprocess(self):\n        self.update_needrun()\n        self.update_priority()\n        self.update_ready()\n        self.update_downstream_size()\n\n    def _ready(self, job):\n        return self._finished.issuperset(\n            filter(self.needrun, self.dependencies[job]))\n\n    def finish(self, job, update_dynamic=True):\n        self._finished.add(job)\n        try:\n            self._ready_jobs.remove(job)\n        except KeyError:\n            pass\n        # mark depending jobs as ready\n        for job_ in self.depending[job]:\n            if self.needrun(job_) and self._ready(job_):\n                self._ready_jobs.add(job_)\n\n        if update_dynamic and job.dynamic_output:\n            logger.info(\"Dynamically updating jobs\")\n            newjob = self.update_dynamic(job)\n            if newjob:\n                # simulate that this job ran and was finished before\n                self.omitforce.add(newjob)\n                self._needrun.add(newjob)\n                self._finished.add(newjob)\n\n                self.postprocess()\n                self.handle_protected(newjob)\n                self.handle_touch(newjob)\n                # add finished jobs to len as they are not counted after new postprocess\n                self._len += len(self._finished)\n\n    def update_dynamic(self, job):\n        dynamic_wildcards = job.dynamic_wildcards\n        if not dynamic_wildcards:\n            # this happens e.g. in dryrun if output is not yet present\n            return\n\n        depending = list(filter(lambda job_: not self.finished(job_),\n                                self.bfs(self.depending, job)))\n        newrule, non_dynamic_wildcards = job.rule.dynamic_branch(\n            dynamic_wildcards,\n            input=False)\n        self.specialize_rule(job.rule, newrule)\n\n        # no targetfile needed for job\n        newjob = Job(newrule, self, format_wildcards=non_dynamic_wildcards)\n        self.replace_job(job, newjob)\n        for job_ in depending:\n            if job_.dynamic_input:\n                newrule_ = job_.rule.dynamic_branch(dynamic_wildcards)\n                if newrule_ is not None:\n                    self.specialize_rule(job_.rule, newrule_)\n                    if not self.dynamic(job_):\n                        logger.debug(\"Updating job {}.\".format(job_))\n                        newjob_ = Job(newrule_, self,\n                                      targetfile=job_.targetfile)\n\n                        unexpected_output = self.reason(\n                            job_).missing_output.intersection(\n                                newjob.existing_output)\n                        if unexpected_output:\n                            logger.warning(\n                                \"Warning: the following output files of rule {} were not \"\n                                \"present when the DAG was created:\\n{}\".format(\n                                    newjob_.rule, unexpected_output))\n\n                        self.replace_job(job_, newjob_)\n        return newjob\n\n    def delete_job(self, job, recursive=True):\n        for job_ in self.depending[job]:\n            del self.dependencies[job_][job]\n        del self.depending[job]\n        for job_ in self.dependencies[job]:\n            depending = self.depending[job_]\n            del depending[job]\n            if not depending and recursive:\n                self.delete_job(job_)\n        del self.dependencies[job]\n        if job in self._needrun:\n            self._len -= 1\n            self._needrun.remove(job)\n            del self._reason[job]\n        if job in self._finished:\n            self._finished.remove(job)\n        if job in self._dynamic:\n            self._dynamic.remove(job)\n        if job in self._ready_jobs:\n            self._ready_jobs.remove(job)\n\n    def replace_job(self, job, newjob):\n        depending = list(self.depending[job].items())\n        if self.finished(job):\n            self._finished.add(newjob)\n\n        self.delete_job(job)\n        self.update([newjob])\n\n        for job_, files in depending:\n            if not job_.dynamic_input:\n                self.dependencies[job_][newjob].update(files)\n                self.depending[newjob][job_].update(files)\n        if job in self.targetjobs:\n            self.targetjobs.remove(job)\n            self.targetjobs.add(newjob)\n\n    def specialize_rule(self, rule, newrule):\n        assert newrule is not None\n        self.rules.add(newrule)\n        self.update_output_index()\n\n    def collect_potential_dependencies(self, job):\n        dependencies = defaultdict(list)\n        # use a set to circumvent multiple jobs for the same file\n        # if user specified it twice\n        file2jobs = self.file2jobs\n        for file in set(job.input):\n            # omit the file if it comes from a subworkflow\n            if file in job.subworkflow_input:\n                continue\n            try:\n                if file in job.dependencies:\n                    jobs = [Job(job.dependencies[file], self, targetfile=file)]\n                else:\n                    jobs = file2jobs(file)\n                dependencies[file].extend(jobs)\n            except MissingRuleException as ex:\n                pass\n        return dependencies\n\n    def bfs(self, direction, *jobs, stop=lambda job: False):\n        queue = list(jobs)\n        visited = set(queue)\n        while queue:\n            job = queue.pop(0)\n            if stop(job):\n                # stop criterion reached for this node\n                continue\n            yield job\n            for job_, _ in direction[job].items():\n                if not job_ in visited:\n                    queue.append(job_)\n                    visited.add(job_)\n\n    def level_bfs(self, direction, *jobs, stop=lambda job: False):\n        queue = [(job, 0) for job in jobs]\n        visited = set(jobs)\n        while queue:\n            job, level = queue.pop(0)\n            if stop(job):\n                # stop criterion reached for this node\n                continue\n            yield level, job\n            level += 1\n            for job_, _ in direction[job].items():\n                if not job_ in visited:\n                    queue.append((job_, level))\n                    visited.add(job_)\n\n    def dfs(self, direction, *jobs, stop=lambda job: False, post=True):\n        visited = set()\n        for job in jobs:\n            for job_ in self._dfs(direction, job, visited,\n                                  stop=stop,\n                                  post=post):\n                yield job_\n\n    def _dfs(self, direction, job, visited, stop, post):\n        if stop(job):\n            return\n        if not post:\n            yield job\n        for job_ in direction[job]:\n            if not job_ in visited:\n                visited.add(job_)\n                for j in self._dfs(direction, job_, visited, stop, post):\n                    yield j\n        if post:\n            yield job\n\n    def is_isomorph(self, job1, job2):\n        if job1.rule != job2.rule:\n            return False\n        rule = lambda job: job.rule.name\n        queue1, queue2 = [job1], [job2]\n        visited1, visited2 = set(queue1), set(queue2)\n        while queue1 and queue2:\n            job1, job2 = queue1.pop(0), queue2.pop(0)\n            deps1 = sorted(self.dependencies[job1], key=rule)\n            deps2 = sorted(self.dependencies[job2], key=rule)\n            for job1_, job2_ in zip(deps1, deps2):\n                if job1_.rule != job2_.rule:\n                    return False\n                if not job1_ in visited1 and not job2_ in visited2:\n                    queue1.append(job1_)\n                    visited1.add(job1_)\n                    queue2.append(job2_)\n                    visited2.add(job2_)\n                elif not (job1_ in visited1 and job2_ in visited2):\n                    return False\n        return True\n\n    def all_longest_paths(self, *jobs):\n        paths = defaultdict(list)\n\n        def all_longest_paths(_jobs):\n            for job in _jobs:\n                if job in paths:\n                    continue\n                deps = self.dependencies[job]\n                if not deps:\n                    paths[job].append([job])\n                    continue\n                all_longest_paths(deps)\n                for _job in deps:\n                    paths[job].extend(path + [job] for path in paths[_job])\n\n        all_longest_paths(jobs)\n        return chain(*(paths[job] for job in jobs))\n\n    def new_wildcards(self, job):\n        new_wildcards = set(job.wildcards.items())\n        for job_ in self.dependencies[job]:\n            if not new_wildcards:\n                return set()\n            for wildcard in job_.wildcards.items():\n                new_wildcards.discard(wildcard)\n        return new_wildcards\n\n    def rule2job(self, targetrule):\n        return Job(targetrule, self)\n\n    def file2jobs(self, targetfile):\n        rules = self.output_index.match(targetfile)\n        jobs = []\n        exceptions = list()\n        for rule in rules:\n            if rule.is_producer(targetfile):\n                try:\n                    jobs.append(Job(rule, self, targetfile=targetfile))\n                except InputFunctionException as e:\n                    exceptions.append(e)\n        if not jobs:\n            if exceptions:\n                raise exceptions[0]\n            raise MissingRuleException(targetfile)\n        return jobs\n\n    def rule_dot2(self):\n        dag = defaultdict(list)\n        visited = set()\n        preselect = set()\n\n        def preselect_parents(job):\n            for parent in self.depending[job]:\n                if parent in preselect:\n                    continue\n                preselect.add(parent)\n                preselect_parents(parent)\n\n        def build_ruledag(job, key=lambda job: job.rule.name):\n            if job in visited:\n                return\n            visited.add(job)\n            deps = sorted(self.dependencies[job], key=key)\n            deps = [(group[0] if preselect.isdisjoint(group) else\n                     preselect.intersection(group).pop())\n                    for group in (list(g) for _, g in groupby(deps, key))]\n            dag[job].extend(deps)\n            preselect_parents(job)\n            for dep in deps:\n                build_ruledag(dep)\n\n        for job in self.targetjobs:\n            build_ruledag(job)\n\n        return self._dot(dag.keys(),\n                         print_wildcards=False,\n                         print_types=False,\n                         dag=dag)\n\n    def rule_dot(self):\n        graph = defaultdict(set)\n        for job in self.jobs:\n            graph[job.rule].update(dep.rule for dep in self.dependencies[job])\n        return self._dot(graph)\n\n    def dot(self):\n        def node2style(job):\n            if not self.needrun(job):\n                return \"rounded,dashed\"\n            if self.dynamic(job) or job.dynamic_input:\n                return \"rounded,dotted\"\n            return \"rounded\"\n\n        def format_wildcard(wildcard):\n            name, value = wildcard\n            if _IOFile.dynamic_fill in value:\n                value = \"...\"\n            return \"{}: {}\".format(name, value)\n\n        node2rule = lambda job: job.rule\n        node2label = lambda job: \"\\\\n\".join(chain([\n            job.rule.name\n        ], sorted(map(format_wildcard, self.new_wildcards(job)))))\n\n        dag = {job: self.dependencies[job] for job in self.jobs}\n\n        return self._dot(dag,\n                         node2rule=node2rule,\n                         node2style=node2style,\n                         node2label=node2label)\n\n    def _dot(self, graph,\n             node2rule=lambda node: node,\n             node2style=lambda node: \"rounded\",\n             node2label=lambda node: node):\n\n        # color rules\n        huefactor = 2 / (3 * len(self.rules))\n        rulecolor = {\n            rule: \"{:.2f} 0.6 0.85\".format(i * huefactor)\n            for i, rule in enumerate(self.rules)\n        }\n\n        # markup\n        node_markup = '\\t{}[label = \"{}\", color = \"{}\", style=\"{}\"];'.format\n        edge_markup = \"\\t{} -> {}\".format\n\n        # node ids\n        ids = {node: i for i, node in enumerate(graph)}\n\n        # calculate nodes\n        nodes = [node_markup(ids[node], node2label(node),\n                             rulecolor[node2rule(node)], node2style(node))\n                 for node in graph]\n        # calculate edges\n        edges = [edge_markup(ids[dep], ids[node])\n                 for node, deps in graph.items() for dep in deps]\n\n        return textwrap.dedent(\"\"\"\\\n            digraph snakemake_dag {{\n                graph[bgcolor=white, margin=0];\n                node[shape=box, style=rounded, fontname=sans, \\\n                fontsize=10, penwidth=2];\n                edge[penwidth=2, color=grey];\n            {items}\n            }}\\\n            \"\"\").format(items=\"\\n\".join(nodes + edges))\n\n    def summary(self, detailed=False):\n        if detailed:\n            yield \"output_file\\tdate\\trule\\tversion\\tinput_file(s)\\tshellcmd\\tstatus\\tplan\"\n        else:\n            yield \"output_file\\tdate\\trule\\tversion\\tstatus\\tplan\"\n\n        for job in self.jobs:\n            output = job.rule.output if self.dynamic(\n                job) else job.expanded_output\n            for f in output:\n                rule = self.workflow.persistence.rule(f)\n                rule = \"-\" if rule is None else rule\n\n                version = self.workflow.persistence.version(f)\n                version = \"-\" if version is None else str(version)\n\n                date = time.ctime(f.mtime) if f.exists else \"-\"\n\n                pending = \"update pending\" if self.reason(job) else \"no update\"\n\n                input = self.workflow.persistence.input(f)\n                input = \"-\" if input is None else \",\".join(input)\n\n                shellcmd = self.workflow.persistence.shellcmd(f)\n                shellcmd = \"-\" if shellcmd is None else shellcmd\n                # remove new line characters, leading and trailing whitespace\n                shellcmd = shellcmd.strip().replace(\"\\n\", \"; \")\n\n                status = \"ok\"\n                if not f.exists:\n                    status = \"missing\"\n                elif self.reason(job).updated_input:\n                    status = \"updated input files\"\n                elif self.workflow.persistence.version_changed(job, file=f):\n                    status = \"version changed to {}\".format(job.rule.version)\n                elif self.workflow.persistence.code_changed(job, file=f):\n                    status = \"rule implementation changed\"\n                elif self.workflow.persistence.input_changed(job, file=f):\n                    status = \"set of input files changed\"\n                elif self.workflow.persistence.params_changed(job, file=f):\n                    status = \"params changed\"\n                if detailed:\n                    yield \"\\t\".join((f, date, rule, version, input, shellcmd,\n                                     status, pending))\n                else:\n                    yield \"\\t\".join((f, date, rule, version, status, pending))\n\n    def d3dag(self, max_jobs=10000):\n        def node(job):\n            jobid = self.jobid(job)\n            return {\n                \"id\": jobid,\n                \"value\": {\n                    \"jobid\": jobid,\n                    \"label\": job.rule.name,\n                    \"rule\": job.rule.name\n                }\n            }\n\n        def edge(a, b):\n            return {\"u\": self.jobid(a), \"v\": self.jobid(b)}\n\n        jobs = list(self.jobs)\n\n        if len(jobs) > max_jobs:\n            logger.info(\n                \"Job-DAG is too large for visualization (>{} jobs).\".format(\n                    max_jobs))\n        else:\n            logger.d3dag(nodes=[node(job) for job in jobs],\n                         edges=[edge(dep, job) for job in jobs for dep in\n                                self.dependencies[job] if self.needrun(dep)])\n\n    def stats(self):\n        rules = Counter()\n        rules.update(job.rule for job in self.needrun_jobs)\n        rules.update(job.rule for job in self.finished_jobs)\n        yield \"Job counts:\"\n        yield \"\\tcount\\tjobs\"\n        for rule, count in sorted(rules.most_common(),\n                                  key=lambda item: item[0].name):\n            yield \"\\t{}\\t{}\".format(count, rule)\n        yield \"\\t{}\".format(len(self))\n\n    def __str__(self):\n        return self.dot()\n\n    def __len__(self):\n        return self._len\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/nh13/snakemake/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/io.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport stat\nimport time\nimport json\nfrom itertools import product, chain\nfrom collections import Iterable, namedtuple\nfrom snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\nfrom snakemake.logging import logger\n\n\ndef lstat(f):\n    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n\n\ndef lutime(f, times):\n    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)\n\n\ndef lchmod(f, mode):\n    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)\n\n\ndef IOFile(file, rule=None):\n    f = _IOFile(file)\n    f.rule = rule\n    return f\n\n\nclass _IOFile(str):\n    \"\"\"\n    A file that is either input or output of a rule.\n    \"\"\"\n\n    dynamic_fill = \"__snakemake_dynamic__\"\n\n    def __new__(cls, file):\n        obj = str.__new__(cls, file)\n        obj._is_function = type(file).__name__ == \"function\"\n        obj._file = file\n        obj.rule = None\n        obj._regex = None\n        return obj\n\n    @property\n    def file(self):\n        if not self._is_function:\n            return self._file\n        else:\n            raise ValueError(\"This IOFile is specified as a function and \"\n                             \"may not be used directly.\")\n\n    @property\n    def exists(self):\n        return os.path.exists(self.file)\n\n    @property\n    def protected(self):\n        return self.exists and not os.access(self.file, os.W_OK)\n\n    @property\n    def mtime(self):\n        # do not follow symlinks for modification time\n        return lstat(self.file).st_mtime\n\n    @property\n    def size(self):\n        # follow symlinks but throw error if invalid\n        self.check_broken_symlink()\n        return os.path.getsize(self.file)\n\n    def check_broken_symlink(self):\n        \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n        if not self.exists and lstat(self.file):\n            raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n\n    def is_newer(self, time):\n        return self.mtime > time\n\n    def prepare(self):\n        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n        dir = os.path.dirname(path_until_wildcard)\n        if len(dir) > 0 and not os.path.exists(dir):\n            try:\n                os.makedirs(dir)\n            except OSError as e:\n                # ignore Errno 17 \"File exists\" (reason: multiprocessing)\n                if e.errno != 17:\n                    raise e\n\n    def protect(self):\n        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~\n                stat.S_IWOTH)\n        if os.path.isdir(self.file):\n            for root, dirs, files in os.walk(self.file):\n                for d in dirs:\n                    lchmod(os.path.join(self.file, d), mode)\n                for f in files:\n                    lchmod(os.path.join(self.file, f), mode)\n        else:\n            lchmod(self.file, mode)\n\n    def remove(self):\n        remove(self.file)\n\n    def touch(self):\n        try:\n            lutime(self.file, None)\n        except OSError as e:\n            if e.errno == 2:\n                raise MissingOutputException(\n                    \"Output file {} of rule {} shall be touched but \"\n                    \"does not exist.\".format(self.file, self.rule.name),\n                    lineno=self.rule.lineno,\n                    snakefile=self.rule.snakefile)\n            else:\n                raise e\n\n    def touch_or_create(self):\n        try:\n            self.touch()\n        except MissingOutputException:\n            # create empty file\n            with open(self.file, \"w\") as f:\n                pass\n\n    def apply_wildcards(self, wildcards,\n                        fill_missing=False,\n                        fail_dynamic=False):\n        f = self._file\n        if self._is_function:\n            f = self._file(Namedlist(fromdict=wildcards))\n\n        return IOFile(apply_wildcards(f, wildcards,\n                                      fill_missing=fill_missing,\n                                      fail_dynamic=fail_dynamic,\n                                      dynamic_fill=self.dynamic_fill),\n                      rule=self.rule)\n\n    def get_wildcard_names(self):\n        return get_wildcard_names(self.file)\n\n    def contains_wildcard(self):\n        return contains_wildcard(self.file)\n\n    def regex(self):\n        if self._regex is None:\n            # compile a regular expression\n            self._regex = re.compile(regex(self.file))\n        return self._regex\n\n    def constant_prefix(self):\n        first_wildcard = _wildcard_regex.search(self.file)\n        if first_wildcard:\n            return self.file[:first_wildcard.start()]\n        return self.file\n\n    def match(self, target):\n        return self.regex().match(target) or None\n\n    def format_dynamic(self):\n        return self.replace(self.dynamic_fill, \"{*}\")\n\n    def __eq__(self, other):\n        f = other._file if isinstance(other, _IOFile) else other\n        return self._file == f\n\n    def __hash__(self):\n        return self._file.__hash__()\n\n\n_wildcard_regex = re.compile(\n    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\")\n\n#    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>[^\\}]*))?\\s*\\}\")\n\n\ndef wait_for_files(files, latency_wait=3):\n    \"\"\"Wait for given files to be present in filesystem.\"\"\"\n    files = list(files)\n    get_missing = lambda: [f for f in files if not os.path.exists(f)]\n    missing = get_missing()\n    if missing:\n        logger.info(\"Waiting at most {} seconds for missing files.\".format(\n            latency_wait))\n        for _ in range(latency_wait):\n            if not get_missing():\n                return\n            time.sleep(1)\n        raise IOError(\"Missing files after {} seconds:\\n{}\".format(\n            latency_wait, \"\\n\".join(get_missing())))\n\n\ndef get_wildcard_names(pattern):\n    return set(match.group('name')\n               for match in _wildcard_regex.finditer(pattern))\n\n\ndef contains_wildcard(path):\n    return _wildcard_regex.search(path) is not None\n\n\ndef remove(file):\n    if os.path.exists(file):\n        if os.path.isdir(file):\n            try:\n                os.removedirs(file)\n            except OSError:\n                # ignore non empty directories\n                pass\n        else:\n            os.remove(file)\n\n\ndef regex(filepattern):\n    f = []\n    last = 0\n    wildcards = set()\n    for match in _wildcard_regex.finditer(filepattern):\n        f.append(re.escape(filepattern[last:match.start()]))\n        wildcard = match.group(\"name\")\n        if wildcard in wildcards:\n            if match.group(\"constraint\"):\n                raise ValueError(\n                    \"If multiple wildcards of the same name \"\n                    \"appear in a string, eventual constraints have to be defined \"\n                    \"at the first occurence and will be inherited by the others.\")\n            f.append(\"(?P={})\".format(wildcard))\n        else:\n            wildcards.add(wildcard)\n            f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if\n                                         match.group(\"constraint\") else \".+\"))\n        last = match.end()\n    f.append(re.escape(filepattern[last:]))\n    f.append(\"$\")  # ensure that the match spans the whole file\n    return \"\".join(f)\n\n\ndef apply_wildcards(pattern, wildcards,\n                    fill_missing=False,\n                    fail_dynamic=False,\n                    dynamic_fill=None,\n                    keep_dynamic=False):\n    def format_match(match):\n        name = match.group(\"name\")\n        try:\n            value = wildcards[name]\n            if fail_dynamic and value == dynamic_fill:\n                raise WildcardError(name)\n            return str(value)  # convert anything into a str\n        except KeyError as ex:\n            if keep_dynamic:\n                return \"{{{}}}\".format(name)\n            elif fill_missing:\n                return dynamic_fill\n            else:\n                raise WildcardError(str(ex))\n\n    return re.sub(_wildcard_regex, format_match, pattern)\n\n\ndef not_iterable(value):\n    return isinstance(value, str) or not isinstance(value, Iterable)\n\n\nclass AnnotatedString(str):\n    def __init__(self, value):\n        self.flags = dict()\n\n\ndef flag(value, flag_type, flag_value=True):\n    if isinstance(value, AnnotatedString):\n        value.flags[flag_type] = flag_value\n        return value\n    if not_iterable(value):\n        value = AnnotatedString(value)\n        value.flags[flag_type] = flag_value\n        return value\n    return [flag(v, flag_type, flag_value=flag_value) for v in value]\n\n\ndef is_flagged(value, flag):\n    if isinstance(value, AnnotatedString):\n        return flag in value.flags\n    return False\n\n\ndef temp(value):\n    \"\"\"\n    A flag for an input or output file that shall be removed after usage.\n    \"\"\"\n    if is_flagged(value, \"protected\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"temp\")\n\n\ndef temporary(value):\n    \"\"\" An alias for temp. \"\"\"\n    return temp(value)\n\n\ndef protected(value):\n    \"\"\" A flag for a file that shall be write protected after creation. \"\"\"\n    if is_flagged(value, \"temp\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"protected\")\n\n\ndef dynamic(value):\n    \"\"\"\n    A flag for a file that shall be dynamic, i.e. the multiplicity\n    (and wildcard values) will be expanded after a certain\n    rule has been run \"\"\"\n    annotated = flag(value, \"dynamic\")\n    tocheck = [annotated] if not_iterable(annotated) else annotated\n    for file in tocheck:\n        matches = list(_wildcard_regex.finditer(file))\n        #if len(matches) != 1:\n        #    raise SyntaxError(\"Dynamic files need exactly one wildcard.\")\n        for match in matches:\n            if match.group(\"constraint\"):\n                raise SyntaxError(\n                    \"The wildcards in dynamic files cannot be constrained.\")\n    return annotated\n\n\ndef touch(value):\n    return flag(value, \"touch\")\n\n\ndef expand(*args, **wildcards):\n    \"\"\"\n    Expand wildcards in given filepatterns.\n\n    Arguments\n    *args -- first arg: filepatterns as list or one single filepattern,\n        second arg (optional): a function to combine wildcard values\n        (itertools.product per default)\n    **wildcards -- the wildcards as keyword arguments\n        with their values as lists\n    \"\"\"\n    filepatterns = args[0]\n    if len(args) == 1:\n        combinator = product\n    elif len(args) == 2:\n        combinator = args[1]\n    if isinstance(filepatterns, str):\n        filepatterns = [filepatterns]\n\n    def flatten(wildcards):\n        for wildcard, values in wildcards.items():\n            if isinstance(values, str) or not isinstance(values, Iterable):\n                values = [values]\n            yield [(wildcard, value) for value in values]\n\n    try:\n        return [filepattern.format(**comb)\n                for comb in map(dict, combinator(*flatten(wildcards))) for\n                filepattern in filepatterns]\n    except KeyError as e:\n        raise WildcardError(\"No values given for wildcard {}.\".format(e))\n\n\ndef limit(pattern, **wildcards):\n    \"\"\"\n    Limit wildcards to the given values.\n\n    Arguments:\n    **wildcards -- the wildcards as keyword arguments\n                   with their values as lists\n    \"\"\"\n    return pattern.format(**{\n        wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values))\n        for wildcard, values in wildcards.items()\n    })\n\n\ndef glob_wildcards(pattern):\n    \"\"\"\n    Glob the values of the wildcards by matching the given pattern to the filesystem.\n    Returns a named tuple with a list of values for each wildcard.\n    \"\"\"\n    pattern = os.path.normpath(pattern)\n    first_wildcard = re.search(\"{[^{]\", pattern)\n    dirname = os.path.dirname(pattern[:first_wildcard.start(\n    )]) if first_wildcard else os.path.dirname(pattern)\n    if not dirname:\n        dirname = \".\"\n\n    names = [match.group('name')\n             for match in _wildcard_regex.finditer(pattern)]\n    Wildcards = namedtuple(\"Wildcards\", names)\n    wildcards = Wildcards(*[list() for name in names])\n\n    pattern = re.compile(regex(pattern))\n    for dirpath, dirnames, filenames in os.walk(dirname):\n        for f in chain(filenames, dirnames):\n            if dirpath != \".\":\n                f = os.path.join(dirpath, f)\n            match = re.match(pattern, f)\n            if match:\n                for name, value in match.groupdict().items():\n                    getattr(wildcards, name).append(value)\n    return wildcards\n\n\n# TODO rewrite Namedlist!\nclass Namedlist(list):\n    \"\"\"\n    A list that additionally provides functions to name items. Further,\n    it is hashable, however the hash does not consider the item names.\n    \"\"\"\n\n    def __init__(self, toclone=None, fromdict=None, plainstr=False):\n        \"\"\"\n        Create the object.\n\n        Arguments\n        toclone  -- another Namedlist that shall be cloned\n        fromdict -- a dict that shall be converted to a\n            Namedlist (keys become names)\n        \"\"\"\n        list.__init__(self)\n        self._names = dict()\n\n        if toclone:\n            self.extend(map(str, toclone) if plainstr else toclone)\n            if isinstance(toclone, Namedlist):\n                self.take_names(toclone.get_names())\n        if fromdict:\n            for key, item in fromdict.items():\n                self.append(item)\n                self.add_name(key)\n\n    def add_name(self, name):\n        \"\"\"\n        Add a name to the last item.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        self.set_name(name, len(self) - 1)\n\n    def set_name(self, name, index, end=None):\n        \"\"\"\n        Set the name of an item.\n\n        Arguments\n        name  -- a name\n        index -- the item index\n        \"\"\"\n        self._names[name] = (index, end)\n        if end is None:\n            setattr(self, name, self[index])\n        else:\n            setattr(self, name, Namedlist(toclone=self[index:end]))\n\n    def get_names(self):\n        \"\"\"\n        Get the defined names as (name, index) pairs.\n        \"\"\"\n        for name, index in self._names.items():\n            yield name, index\n\n    def take_names(self, names):\n        \"\"\"\n        Take over the given names.\n\n        Arguments\n        names -- the given names as (name, index) pairs\n        \"\"\"\n        for name, (i, j) in names:\n            self.set_name(name, i, end=j)\n\n    def items(self):\n        for name in self._names:\n            yield name, getattr(self, name)\n\n    def allitems(self):\n        next = 0\n        for name, index in sorted(self._names.items(),\n                                  key=lambda item: item[1][0]):\n            start, end = index\n            if end is None:\n                end = start + 1\n            if start > next:\n                for item in self[next:start]:\n                    yield None, item\n            yield name, getattr(self, name)\n            next = end\n        for item in self[next:]:\n            yield None, item\n\n    def insert_items(self, index, items):\n        self[index:index + 1] = items\n        add = len(items) - 1\n        for name, (i, j) in self._names.items():\n            if i > index:\n                self._names[name] = (i + add, j + add)\n            elif i == index:\n                self.set_name(name, i, end=i + len(items))\n\n    def keys(self):\n        return self._names\n\n    def plainstrings(self):\n        return self.__class__.__call__(toclone=self, plainstr=True)\n\n    def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)\n        except TypeError:\n            pass\n        return getattr(self, key)\n\n    def __hash__(self):\n        return hash(tuple(self))\n\n    def __str__(self):\n        return \" \".join(map(str, self))\n\n\nclass InputFiles(Namedlist):\n    pass\n\n\nclass OutputFiles(Namedlist):\n    pass\n\n\nclass Wildcards(Namedlist):\n    pass\n\n\nclass Params(Namedlist):\n    pass\n\n\nclass Resources(Namedlist):\n    pass\n\n\nclass Log(Namedlist):\n    pass\n\n\ndef _load_configfile(configpath):\n    \"Tries to load a configfile first as JSON, then as YAML, into a dict.\"\n    try:\n        with open(configpath) as f:\n            try:\n                return json.load(f)\n            except ValueError:\n                f.seek(0)  # try again\n            try:\n                import yaml\n            except ImportError:\n                raise WorkflowError(\"Config file is not valid JSON and PyYAML \"\n                                    \"has not been installed. Please install \"\n                                    \"PyYAML to use YAML config files.\")\n            try:\n                return yaml.load(f)\n            except yaml.YAMLError:\n                raise WorkflowError(\"Config file is not valid JSON or YAML.\")\n    except FileNotFoundError:\n        raise WorkflowError(\"Config file {} not found.\".format(configpath))\n\n\ndef load_configfile(configpath):\n    \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\"\n    config = _load_configfile(configpath)\n    if not isinstance(config, dict):\n        raise WorkflowError(\"Config file must be given as JSON or YAML \"\n                            \"with keys at top level.\")\n    return config\n\n##### Wildcard pumping detection #####\n\n\nclass PeriodicityDetector:\n    def __init__(self, min_repeat=50, max_repeat=100):\n        \"\"\"\n        Args:\n            max_len (int): The maximum length of the periodic substring.\n        \"\"\"\n        self.regex = re.compile(\n            \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format(\n                min_repeat=min_repeat - 1,\n                max_repeat=max_repeat - 1))\n\n    def is_periodic(self, value):\n        \"\"\"Returns the periodic substring or None if not periodic.\"\"\"\n        m = self.regex.search(value)  # search for a periodic suffix.\n        if m is not None:\n            return m.group(\"value\")\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/nh13/snakemake/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/jobs.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport sys\nimport base64\nimport json\n\nfrom collections import defaultdict\nfrom itertools import chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.io import IOFile, Wildcards, Resources, _IOFile\nfrom snakemake.utils import format, listfiles\nfrom snakemake.exceptions import RuleException, ProtectedOutputException\nfrom snakemake.exceptions import UnexpectedOutputException\nfrom snakemake.logging import logger\n\n\ndef jobfiles(jobs, type):\n    return chain(*map(attrgetter(type), jobs))\n\n\nclass Job:\n    HIGHEST_PRIORITY = sys.maxsize\n\n    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):\n        self.rule = rule\n        self.dag = dag\n        self.targetfile = targetfile\n\n        self.wildcards_dict = self.rule.get_wildcards(targetfile)\n        self.wildcards = Wildcards(fromdict=self.wildcards_dict)\n        self._format_wildcards = (self.wildcards if format_wildcards is None\n                                  else Wildcards(fromdict=format_wildcards))\n\n        (self.input, self.output, self.params, self.log, self.benchmark,\n         self.ruleio,\n         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)\n\n        self.resources_dict = {\n            name: min(self.rule.workflow.global_resources.get(name, res), res)\n            for name, res in rule.resources.items()\n        }\n        self.threads = self.resources_dict[\"_cores\"]\n        self.resources = Resources(fromdict=self.resources_dict)\n        self._inputsize = None\n\n        self.dynamic_output, self.dynamic_input = set(), set()\n        self.temp_output, self.protected_output = set(), set()\n        self.touch_output = set()\n        self.subworkflow_input = dict()\n        for f in self.output:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_output:\n                self.dynamic_output.add(f)\n            if f_ in self.rule.temp_output:\n                self.temp_output.add(f)\n            if f_ in self.rule.protected_output:\n                self.protected_output.add(f)\n            if f_ in self.rule.touch_output:\n                self.touch_output.add(f)\n        for f in self.input:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_input:\n                self.dynamic_input.add(f)\n            if f_ in self.rule.subworkflow_input:\n                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]\n        self._hash = self.rule.__hash__()\n        if True or not self.dynamic_output:\n            for o in self.output:\n                self._hash ^= o.__hash__()\n\n    @property\n    def priority(self):\n        return self.dag.priority(self)\n\n    @property\n    def b64id(self):\n        return base64.b64encode((self.rule.name + \"\".join(self.output)\n                                 ).encode(\"utf-8\")).decode(\"utf-8\")\n\n    @property\n    def inputsize(self):\n        \"\"\"\n        Return the size of the input files.\n        Input files need to be present.\n        \"\"\"\n        if self._inputsize is None:\n            self._inputsize = sum(f.size for f in self.input)\n        return self._inputsize\n\n    @property\n    def message(self):\n        \"\"\" Return the message for this job. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.message) if\n                    self.rule.message else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable in message \"\n                                \"of shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def shellcmd(self):\n        \"\"\" Return the shell command. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.shellcmd) if\n                    self.rule.shellcmd else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable when printing \"\n                                \"shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def expanded_output(self):\n        \"\"\" Iterate over output files while dynamic output is expanded. \"\"\"\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                expansion = self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill)\n                if not expansion:\n                    yield f_\n                for f, _ in expansion:\n                    yield IOFile(f, self.rule)\n            else:\n                yield f\n\n    @property\n    def dynamic_wildcards(self):\n        \"\"\" Return all wildcard values determined from dynamic output. \"\"\"\n        combinations = set()\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                for f, w in self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill):\n                    combinations.add(tuple(w.items()))\n        wildcards = defaultdict(list)\n        for combination in combinations:\n            for name, value in combination:\n                wildcards[name].append(value)\n        return wildcards\n\n    @property\n    def missing_input(self):\n        \"\"\" Return missing input files. \"\"\"\n        # omit file if it comes from a subworkflow\n        return set(f for f in self.input\n                   if not f.exists and not f in self.subworkflow_input)\n\n    @property\n    def output_mintime(self):\n        \"\"\" Return oldest output file. \"\"\"\n        existing = [f.mtime for f in self.expanded_output if f.exists]\n        if self.benchmark and self.benchmark.exists:\n            existing.append(self.benchmark.mtime)\n        if existing:\n            return min(existing)\n        return None\n\n    @property\n    def input_maxtime(self):\n        \"\"\" Return newest input file. \"\"\"\n        existing = [f.mtime for f in self.input if f.exists]\n        if existing:\n            return max(existing)\n        return None\n\n    def missing_output(self, requested=None):\n        \"\"\" Return missing output files. \"\"\"\n        files = set()\n        if self.benchmark and (requested is None or\n                               self.benchmark in requested):\n            if not self.benchmark.exists:\n                files.add(self.benchmark)\n\n        for f, f_ in zip(self.output, self.rule.output):\n            if requested is None or f in requested:\n                if f in self.dynamic_output:\n                    if not self.expand_dynamic(\n                        f_,\n                        restriction=self.wildcards,\n                        omit_value=_IOFile.dynamic_fill):\n                        files.add(\"{} (dynamic)\".format(f_))\n                elif not f.exists:\n                    files.add(f)\n        return files\n\n    @property\n    def existing_output(self):\n        return filter(lambda f: f.exists, self.expanded_output)\n\n    def check_protected_output(self):\n        protected = list(filter(lambda f: f.protected, self.expanded_output))\n        if protected:\n            raise ProtectedOutputException(self.rule, protected)\n\n    def prepare(self):\n        \"\"\"\n        Prepare execution of job.\n        This includes creation of directories and deletion of previously\n        created dynamic files.\n        \"\"\"\n\n        self.check_protected_output()\n\n        unexpected_output = self.dag.reason(self).missing_output.intersection(\n            self.existing_output)\n        if unexpected_output:\n            logger.warning(\n                \"Warning: the following output files of rule {} were not \"\n                \"present when the DAG was created:\\n{}\".format(\n                    self.rule, unexpected_output))\n\n        if self.dynamic_output:\n            for f, _ in chain(*map(partial(self.expand_dynamic,\n                                           restriction=self.wildcards,\n                                           omit_value=_IOFile.dynamic_fill),\n                                   self.rule.dynamic_output)):\n                os.remove(f)\n        for f, f_ in zip(self.output, self.rule.output):\n            f.prepare()\n        for f in self.log:\n            f.prepare()\n        if self.benchmark:\n            self.benchmark.prepare()\n\n    def cleanup(self):\n        \"\"\" Cleanup output files. \"\"\"\n        to_remove = [f for f in self.expanded_output if f.exists]\n        if to_remove:\n            logger.info(\"Removing output files of failed job {}\"\n                        \" since they might be corrupted:\\n{}\".format(\n                            self, \", \".join(to_remove)))\n            for f in to_remove:\n                f.remove()\n\n    def format_wildcards(self, string, **variables):\n        \"\"\" Format a string with variables from the job. \"\"\"\n        _variables = dict()\n        _variables.update(self.rule.workflow.globals)\n        _variables.update(dict(input=self.input,\n                               output=self.output,\n                               params=self.params,\n                               wildcards=self._format_wildcards,\n                               threads=self.threads,\n                               resources=self.resources,\n                               log=self.log,\n                               version=self.rule.version,\n                               rule=self.rule.name, ))\n        _variables.update(variables)\n        try:\n            return format(string, **_variables)\n        except NameError as ex:\n            raise RuleException(\"NameError: \" + str(ex), rule=self.rule)\n        except IndexError as ex:\n            raise RuleException(\"IndexError: \" + str(ex), rule=self.rule)\n\n    def properties(self, omit_resources=\"_cores _nodes\".split()):\n        resources = {\n            name: res\n            for name, res in self.resources.items()\n            if name not in omit_resources\n        }\n        params = {name: value for name, value in self.params.items()}\n        properties = {\n            \"rule\": self.rule.name,\n            \"local\": self.dag.workflow.is_local(self.rule),\n            \"input\": self.input,\n            \"output\": self.output,\n            \"params\": params,\n            \"threads\": self.threads,\n            \"resources\": resources\n        }\n        return properties\n\n    def json(self):\n        return json.dumps(self.properties())\n\n    def __repr__(self):\n        return self.rule.name\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return self.rule == other.rule and (\n            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)\n\n    def __lt__(self, other):\n        return self.rule.__lt__(other.rule)\n\n    def __gt__(self, other):\n        return self.rule.__gt__(other.rule)\n\n    def __hash__(self):\n        return self._hash\n\n    @staticmethod\n    def expand_dynamic(pattern, restriction=None, omit_value=None):\n        \"\"\" Expand dynamic files. \"\"\"\n        return list(listfiles(pattern,\n                              restriction=restriction,\n                              omit_value=omit_value))\n\n\nclass Reason:\n    def __init__(self):\n        self.updated_input = set()\n        self.updated_input_run = set()\n        self.missing_output = set()\n        self.incomplete_output = set()\n        self.forced = False\n        self.noio = False\n        self.nooutput = False\n        self.derived = True\n\n    def __str__(self):\n        s = list()\n        if self.forced:\n            s.append(\"Forced execution\")\n        else:\n            if self.noio:\n                s.append(\"Rules with neither input nor \"\n                         \"output files are always executed.\")\n            elif self.nooutput:\n                s.append(\"Rules with a run or shell declaration but no output \"\n                         \"are always executed.\")\n            else:\n                if self.missing_output:\n                    s.append(\"Missing output files: {}\".format(\n                        \", \".join(self.missing_output)))\n                if self.incomplete_output:\n                    s.append(\"Incomplete output files: {}\".format(\n                        \", \".join(self.incomplete_output)))\n                updated_input = self.updated_input - self.updated_input_run\n                if updated_input:\n                    s.append(\"Updated input files: {}\".format(\n                        \", \".join(updated_input)))\n                if self.updated_input_run:\n                    s.append(\"Input files updated by another job: {}\".format(\n                        \", \".join(self.updated_input_run)))\n        s = \"; \".join(s)\n        return s\n\n    def __bool__(self):\n        return bool(self.updated_input or self.missing_output or self.forced or\n                    self.updated_input_run or self.noio or self.nooutput)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/nh13/snakemake/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/rules.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport sys\nimport inspect\nimport sre_constants\nfrom collections import defaultdict\n\nfrom snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist\nfrom snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log\nfrom snakemake.io import apply_wildcards, is_flagged, not_iterable\nfrom snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException\n\n\nclass Rule:\n    def __init__(self, *args, lineno=None, snakefile=None):\n        \"\"\"\n        Create a rule\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if len(args) == 2:\n            name, workflow = args\n            self.name = name\n            self.workflow = workflow\n            self.docstring = None\n            self.message = None\n            self._input = InputFiles()\n            self._output = OutputFiles()\n            self._params = Params()\n            self.dependencies = dict()\n            self.dynamic_output = set()\n            self.dynamic_input = set()\n            self.temp_output = set()\n            self.protected_output = set()\n            self.touch_output = set()\n            self.subworkflow_input = dict()\n            self.resources = dict(_cores=1, _nodes=1)\n            self.priority = 0\n            self.version = None\n            self._log = Log()\n            self._benchmark = None\n            self.wildcard_names = set()\n            self.lineno = lineno\n            self.snakefile = snakefile\n            self.run_func = None\n            self.shellcmd = None\n            self.norun = False\n        elif len(args) == 1:\n            other = args[0]\n            self.name = other.name\n            self.workflow = other.workflow\n            self.docstring = other.docstring\n            self.message = other.message\n            self._input = InputFiles(other._input)\n            self._output = OutputFiles(other._output)\n            self._params = Params(other._params)\n            self.dependencies = dict(other.dependencies)\n            self.dynamic_output = set(other.dynamic_output)\n            self.dynamic_input = set(other.dynamic_input)\n            self.temp_output = set(other.temp_output)\n            self.protected_output = set(other.protected_output)\n            self.touch_output = set(other.touch_output)\n            self.subworkflow_input = dict(other.subworkflow_input)\n            self.resources = other.resources\n            self.priority = other.priority\n            self.version = other.version\n            self._log = other._log\n            self._benchmark = other._benchmark\n            self.wildcard_names = set(other.wildcard_names)\n            self.lineno = other.lineno\n            self.snakefile = other.snakefile\n            self.run_func = other.run_func\n            self.shellcmd = other.shellcmd\n            self.norun = other.norun\n\n    def dynamic_branch(self, wildcards, input=True):\n        def get_io(rule):\n            return (rule.input, rule.dynamic_input) if input else (\n                rule.output, rule.dynamic_output\n            )\n\n        io, dynamic_io = get_io(self)\n\n        branch = Rule(self)\n        io_, dynamic_io_ = get_io(branch)\n\n        expansion = defaultdict(list)\n        for i, f in enumerate(io):\n            if f in dynamic_io:\n                try:\n                    for e in reversed(expand(f, zip, **wildcards)):\n                        expansion[i].append(IOFile(e, rule=branch))\n                except KeyError:\n                    return None\n\n        # replace the dynamic files with the expanded files\n        replacements = [(i, io[i], e)\n                        for i, e in reversed(list(expansion.items()))]\n        for i, old, exp in replacements:\n            dynamic_io_.remove(old)\n            io_.insert_items(i, exp)\n\n        if not input:\n            for i, old, exp in replacements:\n                if old in branch.temp_output:\n                    branch.temp_output.discard(old)\n                    branch.temp_output.update(exp)\n                if old in branch.protected_output:\n                    branch.protected_output.discard(old)\n                    branch.protected_output.update(exp)\n                if old in branch.touch_output:\n                    branch.touch_output.discard(old)\n                    branch.touch_output.update(exp)\n\n            branch.wildcard_names.clear()\n            non_dynamic_wildcards = dict((name, values[0])\n                                         for name, values in wildcards.items()\n                                         if len(set(values)) == 1)\n            # TODO have a look into how to concretize dependencies here\n            (branch._input, branch._output, branch._params, branch._log,\n             branch._benchmark, _, branch.dependencies\n             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)\n            return branch, non_dynamic_wildcards\n        return branch\n\n    def has_wildcards(self):\n        \"\"\"\n        Return True if rule contains wildcards.\n        \"\"\"\n        return bool(self.wildcard_names)\n\n    @property\n    def benchmark(self):\n        return self._benchmark\n\n    @benchmark.setter\n    def benchmark(self, benchmark):\n        self._benchmark = IOFile(benchmark, rule=self)\n\n    @property\n    def input(self):\n        return self._input\n\n    def set_input(self, *input, **kwinput):\n        \"\"\"\n        Add a list of input files. Recursive lists are flattened.\n\n        Arguments\n        input -- the list of input files\n        \"\"\"\n        for item in input:\n            self._set_inoutput_item(item)\n        for name, item in kwinput.items():\n            self._set_inoutput_item(item, name=name)\n\n    @property\n    def output(self):\n        return self._output\n\n    @property\n    def products(self):\n        products = list(self.output)\n        if self.benchmark:\n            products.append(self.benchmark)\n        return products\n\n    def set_output(self, *output, **kwoutput):\n        \"\"\"\n        Add a list of output files. Recursive lists are flattened.\n\n        Arguments\n        output -- the list of output files\n        \"\"\"\n        for item in output:\n            self._set_inoutput_item(item, output=True)\n        for name, item in kwoutput.items():\n            self._set_inoutput_item(item, output=True, name=name)\n\n        for item in self.output:\n            if self.dynamic_output and item not in self.dynamic_output:\n                raise SyntaxError(\n                    \"A rule with dynamic output may not define any \"\n                    \"non-dynamic output files.\")\n            wildcards = item.get_wildcard_names()\n            if self.wildcard_names:\n                if self.wildcard_names != wildcards:\n                    raise SyntaxError(\n                        \"Not all output files of rule {} \"\n                        \"contain the same wildcards.\".format(self.name))\n            else:\n                self.wildcard_names = wildcards\n\n    def _set_inoutput_item(self, item, output=False, name=None):\n        \"\"\"\n        Set an item to be input or output.\n\n        Arguments\n        item     -- the item\n        inoutput -- either a Namedlist of input or output items\n        name     -- an optional name for the item\n        \"\"\"\n        inoutput = self.output if output else self.input\n        if isinstance(item, str):\n            # add the rule to the dependencies\n            if isinstance(item, _IOFile):\n                self.dependencies[item] = item.rule\n            _item = IOFile(item, rule=self)\n            if is_flagged(item, \"temp\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be temporary\")\n                self.temp_output.add(_item)\n            if is_flagged(item, \"protected\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be protected\")\n                self.protected_output.add(_item)\n            if is_flagged(item, \"touch\"):\n                if not output:\n                    raise SyntaxError(\n                        \"Only output files may be marked for touching.\")\n                self.touch_output.add(_item)\n            if is_flagged(item, \"dynamic\"):\n                if output:\n                    self.dynamic_output.add(_item)\n                else:\n                    self.dynamic_input.add(_item)\n            if is_flagged(item, \"subworkflow\"):\n                if output:\n                    raise SyntaxError(\n                        \"Only input files may refer to a subworkflow\")\n                else:\n                    # record the workflow this item comes from\n                    self.subworkflow_input[_item] = item.flags[\"subworkflow\"]\n            inoutput.append(_item)\n            if name:\n                inoutput.add_name(name)\n        elif callable(item):\n            if output:\n                raise SyntaxError(\n                    \"Only input files can be specified as functions\")\n            inoutput.append(item)\n            if name:\n                inoutput.add_name(name)\n        else:\n            try:\n                start = len(inoutput)\n                for i in item:\n                    self._set_inoutput_item(i, output=output)\n                if name:\n                    # if the list was named, make it accessible\n                    inoutput.set_name(name, start, end=len(inoutput))\n            except TypeError:\n                raise SyntaxError(\n                    \"Input and output files have to be specified as strings or lists of strings.\")\n\n    @property\n    def params(self):\n        return self._params\n\n    def set_params(self, *params, **kwparams):\n        for item in params:\n            self._set_params_item(item)\n        for name, item in kwparams.items():\n            self._set_params_item(item, name=name)\n\n    def _set_params_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.params.append(item)\n            if name:\n                self.params.add_name(name)\n        else:\n            try:\n                start = len(self.params)\n                for i in item:\n                    self._set_params_item(i)\n                if name:\n                    self.params.set_name(name, start, end=len(self.params))\n            except TypeError:\n                raise SyntaxError(\"Params have to be specified as strings.\")\n\n    @property\n    def log(self):\n        return self._log\n\n    def set_log(self, *logs, **kwlogs):\n        for item in logs:\n            self._set_log_item(item)\n        for name, item in kwlogs.items():\n            self._set_log_item(item, name=name)\n\n    def _set_log_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.log.append(IOFile(item,\n                                   rule=self)\n                            if isinstance(item, str) else item)\n            if name:\n                self.log.add_name(name)\n        else:\n            try:\n                start = len(self.log)\n                for i in item:\n                    self._set_log_item(i)\n                if name:\n                    self.log.set_name(name, start, end=len(self.log))\n            except TypeError:\n                raise SyntaxError(\"Log files have to be specified as strings.\")\n\n    def expand_wildcards(self, wildcards=None):\n        \"\"\"\n        Expand wildcards depending on the requested output\n        or given wildcards dict.\n        \"\"\"\n\n        def concretize_iofile(f, wildcards):\n            if not isinstance(f, _IOFile):\n                return IOFile(f, rule=self)\n            else:\n                return f.apply_wildcards(wildcards,\n                                         fill_missing=f in self.dynamic_input,\n                                         fail_dynamic=self.dynamic_output)\n\n        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,\n                             concretize=apply_wildcards,\n                             ruleio=None):\n            for name, item in olditems.allitems():\n                start = len(newitems)\n                is_iterable = True\n                if callable(item):\n                    try:\n                        item = item(wildcards_obj)\n                    except (Exception, BaseException) as e:\n                        raise InputFunctionException(e, rule=self)\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        if not isinstance(item_, str):\n                            raise RuleException(\n                                \"Input function did not return str or list of str.\",\n                                rule=self)\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                else:\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                if name:\n                    newitems.set_name(\n                        name, start,\n                        end=len(newitems) if is_iterable else None)\n\n        if wildcards is None:\n            wildcards = dict()\n        missing_wildcards = self.wildcard_names - set(wildcards.keys())\n\n        if missing_wildcards:\n            raise RuleException(\n                \"Could not resolve wildcards in rule {}:\\n{}\".format(\n                    self.name, \"\\n\".join(self.wildcard_names)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n        ruleio = dict()\n\n        try:\n            input = InputFiles()\n            wildcards_obj = Wildcards(fromdict=wildcards)\n            _apply_wildcards(input, self.input, wildcards, wildcards_obj,\n                             concretize=concretize_iofile,\n                             ruleio=ruleio)\n\n            params = Params()\n            _apply_wildcards(params, self.params, wildcards, wildcards_obj)\n\n            output = OutputFiles(o.apply_wildcards(wildcards)\n                                 for o in self.output)\n            output.take_names(self.output.get_names())\n\n            dependencies = {\n                None if f is None else f.apply_wildcards(wildcards): rule\n                for f, rule in self.dependencies.items()\n            }\n\n            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))\n\n            log = Log()\n            _apply_wildcards(log, self.log, wildcards, wildcards_obj,\n                             concretize=concretize_iofile)\n\n            benchmark = self.benchmark.apply_wildcards(\n                wildcards) if self.benchmark else None\n            return input, output, params, log, benchmark, ruleio, dependencies\n        except WildcardError as ex:\n            # this can only happen if an input contains an unresolved wildcard.\n            raise RuleException(\n                \"Wildcards in input, params, log or benchmark file of rule {} cannot be \"\n                \"determined from output files:\\n{}\".format(self, str(ex)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n    def is_producer(self, requested_output):\n        \"\"\"\n        Returns True if this rule is a producer of the requested output.\n        \"\"\"\n        try:\n            for o in self.products:\n                if o.match(requested_output):\n                    return True\n            return False\n        except sre_constants.error as ex:\n            raise IOFileException(\"{} in wildcard statement\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n        except ValueError as ex:\n            raise IOFileException(\"{}\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n\n    def get_wildcards(self, requested_output):\n        \"\"\"\n        Update the given wildcard dictionary by matching regular expression\n        output files to the requested concrete ones.\n\n        Arguments\n        wildcards -- a dictionary of wildcards\n        requested_output -- a concrete filepath\n        \"\"\"\n        if requested_output is None:\n            return dict()\n        bestmatchlen = 0\n        bestmatch = None\n\n        for o in self.products:\n            match = o.match(requested_output)\n            if match:\n                l = self.get_wildcard_len(match.groupdict())\n                if not bestmatch or bestmatchlen > l:\n                    bestmatch = match.groupdict()\n                    bestmatchlen = l\n        return bestmatch\n\n    @staticmethod\n    def get_wildcard_len(wildcards):\n        \"\"\"\n        Return the length of the given wildcard values.\n\n        Arguments\n        wildcards -- a dict of wildcards\n        \"\"\"\n        return sum(map(len, wildcards.values()))\n\n    def __lt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp < 0\n\n    def __gt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp > 0\n\n    def __str__(self):\n        return self.name\n\n    def __hash__(self):\n        return self.name.__hash__()\n\n    def __eq__(self, other):\n        return self.name == other.name\n\n\nclass Ruleorder:\n    def __init__(self):\n        self.order = list()\n\n    def add(self, *rulenames):\n        \"\"\"\n        Records the order of given rules as rule1 > rule2 > rule3, ...\n        \"\"\"\n        self.order.append(list(rulenames))\n\n    def compare(self, rule1, rule2):\n        \"\"\"\n        Return whether rule2 has a higher priority than rule1.\n        \"\"\"\n        # try the last clause first,\n        # i.e. clauses added later overwrite those before.\n        for clause in reversed(self.order):\n            try:\n                i = clause.index(rule1.name)\n                j = clause.index(rule2.name)\n                # rules with higher priority should have a smaller index\n                comp = j - i\n                if comp < 0:\n                    comp = -1\n                elif comp > 0:\n                    comp = 1\n                return comp\n            except ValueError:\n                pass\n\n        # if not ruleorder given, prefer rule without wildcards\n        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()\n        if wildcard_cmp != 0:\n            return wildcard_cmp\n\n        return 0\n\n    def __iter__(self):\n        return self.order.__iter__()\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/nh13/snakemake/blob/274d2e3c67e64d480bedcf211a61495f33120a13",
        "file_path": "/snakemake/workflow.py",
        "source": "__author__ = \"Johannes Kster\"\n__copyright__ = \"Copyright 2015, Johannes Kster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport re\nimport os\nimport sys\nimport signal\nimport json\nimport urllib\nfrom collections import OrderedDict\nfrom itertools import filterfalse, chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.logging import logger, format_resources, format_resource_names\nfrom snakemake.rules import Rule, Ruleorder\nfrom snakemake.exceptions import RuleException, CreateRuleException, \\\n    UnknownRuleException, NoRulesException, print_exception, WorkflowError\nfrom snakemake.shell import shell\nfrom snakemake.dag import DAG\nfrom snakemake.scheduler import JobScheduler\nfrom snakemake.parser import parse\nimport snakemake.io\nfrom snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\nfrom snakemake.persistence import Persistence\nfrom snakemake.utils import update_config\n\n\nclass Workflow:\n    def __init__(self,\n                 snakefile=None,\n                 snakemakepath=None,\n                 jobscript=None,\n                 overwrite_shellcmd=None,\n                 overwrite_config=dict(),\n                 overwrite_workdir=None,\n                 overwrite_configfile=None,\n                 config_args=None,\n                 debug=False):\n        \"\"\"\n        Create the controller.\n        \"\"\"\n        self._rules = OrderedDict()\n        self.first_rule = None\n        self._workdir = None\n        self.overwrite_workdir = overwrite_workdir\n        self.workdir_init = os.path.abspath(os.curdir)\n        self._ruleorder = Ruleorder()\n        self._localrules = set()\n        self.linemaps = dict()\n        self.rule_count = 0\n        self.basedir = os.path.dirname(snakefile)\n        self.snakefile = os.path.abspath(snakefile)\n        self.snakemakepath = snakemakepath\n        self.included = []\n        self.included_stack = []\n        self.jobscript = jobscript\n        self.persistence = None\n        self.global_resources = None\n        self.globals = globals()\n        self._subworkflows = dict()\n        self.overwrite_shellcmd = overwrite_shellcmd\n        self.overwrite_config = overwrite_config\n        self.overwrite_configfile = overwrite_configfile\n        self.config_args = config_args\n        self._onsuccess = lambda log: None\n        self._onerror = lambda log: None\n        self.debug = debug\n\n        global config\n        config = dict()\n        config.update(self.overwrite_config)\n\n        global rules\n        rules = Rules()\n\n    @property\n    def subworkflows(self):\n        return self._subworkflows.values()\n\n    @property\n    def rules(self):\n        return self._rules.values()\n\n    @property\n    def concrete_files(self):\n        return (\n            file\n            for rule in self.rules for file in chain(rule.input, rule.output)\n            if not callable(file) and not file.contains_wildcard()\n        )\n\n    def check(self):\n        for clause in self._ruleorder:\n            for rulename in clause:\n                if not self.is_rule(rulename):\n                    raise UnknownRuleException(\n                        rulename,\n                        prefix=\"Error in ruleorder definition.\")\n\n    def add_rule(self, name=None, lineno=None, snakefile=None):\n        \"\"\"\n        Add a rule.\n        \"\"\"\n        if name is None:\n            name = str(len(self._rules) + 1)\n        if self.is_rule(name):\n            raise CreateRuleException(\n                \"The name {} is already used by another rule\".format(name))\n        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)\n        self._rules[rule.name] = rule\n        self.rule_count += 1\n        if not self.first_rule:\n            self.first_rule = rule.name\n        return name\n\n    def is_rule(self, name):\n        \"\"\"\n        Return True if name is the name of a rule.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        return name in self._rules\n\n    def get_rule(self, name):\n        \"\"\"\n        Get rule by name.\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if not self._rules:\n            raise NoRulesException()\n        if not name in self._rules:\n            raise UnknownRuleException(name)\n        return self._rules[name]\n\n    def list_rules(self, only_targets=False):\n        rules = self.rules\n        if only_targets:\n            rules = filterfalse(Rule.has_wildcards, rules)\n        for rule in rules:\n            logger.rule_info(name=rule.name, docstring=rule.docstring)\n\n    def list_resources(self):\n        for resource in set(\n            resource for rule in self.rules for resource in rule.resources):\n            if resource not in \"_cores _nodes\".split():\n                logger.info(resource)\n\n    def is_local(self, rule):\n        return rule.name in self._localrules or rule.norun\n\n    def execute(self,\n                targets=None,\n                dryrun=False,\n                touch=False,\n                cores=1,\n                nodes=1,\n                local_cores=1,\n                forcetargets=False,\n                forceall=False,\n                forcerun=None,\n                prioritytargets=None,\n                quiet=False,\n                keepgoing=False,\n                printshellcmds=False,\n                printreason=False,\n                printdag=False,\n                cluster=None,\n                cluster_config=None,\n                cluster_sync=None,\n                jobname=None,\n                immediate_submit=False,\n                ignore_ambiguity=False,\n                printrulegraph=False,\n                printd3dag=False,\n                drmaa=None,\n                stats=None,\n                force_incomplete=False,\n                ignore_incomplete=False,\n                list_version_changes=False,\n                list_code_changes=False,\n                list_input_changes=False,\n                list_params_changes=False,\n                summary=False,\n                detailed_summary=False,\n                latency_wait=3,\n                benchmark_repeats=3,\n                wait_for_files=None,\n                nolock=False,\n                unlock=False,\n                resources=None,\n                notemp=False,\n                nodeps=False,\n                cleanup_metadata=None,\n                subsnakemake=None,\n                updated_files=None,\n                keep_target_files=False,\n                allowed_rules=None,\n                greediness=1.0,\n                no_hooks=False):\n\n        self.global_resources = dict() if resources is None else resources\n        self.global_resources[\"_cores\"] = cores\n        self.global_resources[\"_nodes\"] = nodes\n\n        def rules(items):\n            return map(self._rules.__getitem__, filter(self.is_rule, items))\n\n        if keep_target_files:\n\n            def files(items):\n                return filterfalse(self.is_rule, items)\n        else:\n\n            def files(items):\n                return map(os.path.relpath, filterfalse(self.is_rule, items))\n\n        if not targets:\n            targets = [self.first_rule\n                       ] if self.first_rule is not None else list()\n        if prioritytargets is None:\n            prioritytargets = list()\n        if forcerun is None:\n            forcerun = list()\n\n        priorityrules = set(rules(prioritytargets))\n        priorityfiles = set(files(prioritytargets))\n        forcerules = set(rules(forcerun))\n        forcefiles = set(files(forcerun))\n        targetrules = set(chain(rules(targets),\n                                filterfalse(Rule.has_wildcards, priorityrules),\n                                filterfalse(Rule.has_wildcards, forcerules)))\n        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))\n        if forcetargets:\n            forcefiles.update(targetfiles)\n            forcerules.update(targetrules)\n\n        rules = self.rules\n        if allowed_rules:\n            rules = [rule for rule in rules if rule.name in set(allowed_rules)]\n\n        if wait_for_files is not None:\n            try:\n                snakemake.io.wait_for_files(wait_for_files,\n                                            latency_wait=latency_wait)\n            except IOError as e:\n                logger.error(str(e))\n                return False\n\n        dag = DAG(\n            self, rules,\n            dryrun=dryrun,\n            targetfiles=targetfiles,\n            targetrules=targetrules,\n            forceall=forceall,\n            forcefiles=forcefiles,\n            forcerules=forcerules,\n            priorityfiles=priorityfiles,\n            priorityrules=priorityrules,\n            ignore_ambiguity=ignore_ambiguity,\n            force_incomplete=force_incomplete,\n            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,\n            notemp=notemp)\n\n        self.persistence = Persistence(\n            nolock=nolock,\n            dag=dag,\n            warn_only=dryrun or printrulegraph or printdag or summary or\n            list_version_changes or list_code_changes or list_input_changes or\n            list_params_changes)\n\n        if cleanup_metadata:\n            for f in cleanup_metadata:\n                self.persistence.cleanup_metadata(f)\n            return True\n\n        dag.init()\n        dag.check_dynamic()\n\n        if unlock:\n            try:\n                self.persistence.cleanup_locks()\n                logger.info(\"Unlocking working directory.\")\n                return True\n            except IOError:\n                logger.error(\"Error: Unlocking the directory {} failed. Maybe \"\n                             \"you don't have the permissions?\")\n                return False\n        try:\n            self.persistence.lock()\n        except IOError:\n            logger.error(\n                \"Error: Directory cannot be locked. Please make \"\n                \"sure that no other Snakemake process is trying to create \"\n                \"the same files in the following directory:\\n{}\\n\"\n                \"If you are sure that no other \"\n                \"instances of snakemake are running on this directory, \"\n                \"the remaining lock was likely caused by a kill signal or \"\n                \"a power loss. It can be removed with \"\n                \"the --unlock argument.\".format(os.getcwd()))\n            return False\n\n        if self.subworkflows and not printdag and not printrulegraph:\n            # backup globals\n            globals_backup = dict(self.globals)\n            # execute subworkflows\n            for subworkflow in self.subworkflows:\n                subworkflow_targets = subworkflow.targets(dag)\n                updated = list()\n                if subworkflow_targets:\n                    logger.info(\n                        \"Executing subworkflow {}.\".format(subworkflow.name))\n                    if not subsnakemake(subworkflow.snakefile,\n                                        workdir=subworkflow.workdir,\n                                        targets=subworkflow_targets,\n                                        updated_files=updated):\n                        return False\n                    dag.updated_subworkflow_files.update(subworkflow.target(f)\n                                                         for f in updated)\n                else:\n                    logger.info(\"Subworkflow {}: Nothing to be done.\".format(\n                        subworkflow.name))\n            if self.subworkflows:\n                logger.info(\"Executing main workflow.\")\n            # rescue globals\n            self.globals.update(globals_backup)\n\n        dag.check_incomplete()\n        dag.postprocess()\n\n        if nodeps:\n            missing_input = [f for job in dag.targetjobs for f in job.input\n                             if dag.needrun(job) and not os.path.exists(f)]\n            if missing_input:\n                logger.error(\n                    \"Dependency resolution disabled (--nodeps) \"\n                    \"but missing input \"\n                    \"files detected. If this happens on a cluster, please make sure \"\n                    \"that you handle the dependencies yourself or turn of \"\n                    \"--immediate-submit. Missing input files:\\n{}\".format(\n                        \"\\n\".join(missing_input)))\n                return False\n\n        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)\n\n        if printd3dag:\n            dag.d3dag()\n            return True\n        elif printdag:\n            print(dag)\n            return True\n        elif printrulegraph:\n            print(dag.rule_dot())\n            return True\n        elif summary:\n            print(\"\\n\".join(dag.summary(detailed=False)))\n            return True\n        elif detailed_summary:\n            print(\"\\n\".join(dag.summary(detailed=True)))\n            return True\n        elif list_version_changes:\n            items = list(\n                chain(*map(self.persistence.version_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_code_changes:\n            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_input_changes:\n            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_params_changes:\n            items = list(\n                chain(*map(self.persistence.params_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n\n        scheduler = JobScheduler(self, dag, cores,\n                                 local_cores=local_cores,\n                                 dryrun=dryrun,\n                                 touch=touch,\n                                 cluster=cluster,\n                                 cluster_config=cluster_config,\n                                 cluster_sync=cluster_sync,\n                                 jobname=jobname,\n                                 immediate_submit=immediate_submit,\n                                 quiet=quiet,\n                                 keepgoing=keepgoing,\n                                 drmaa=drmaa,\n                                 printreason=printreason,\n                                 printshellcmds=printshellcmds,\n                                 latency_wait=latency_wait,\n                                 benchmark_repeats=benchmark_repeats,\n                                 greediness=greediness)\n\n        if not dryrun and not quiet:\n            if len(dag):\n                if cluster or cluster_sync or drmaa:\n                    logger.resources_info(\n                        \"Provided cluster nodes: {}\".format(nodes))\n                else:\n                    logger.resources_info(\"Provided cores: {}\".format(cores))\n                    logger.resources_info(\"Rules claiming more threads will be scaled down.\")\n                provided_resources = format_resources(resources)\n                if provided_resources:\n                    logger.resources_info(\n                        \"Provided resources: \" + provided_resources)\n                ignored_resources = format_resource_names(\n                    set(resource for job in dag.needrun_jobs for resource in\n                        job.resources_dict if resource not in resources))\n                if ignored_resources:\n                    logger.resources_info(\n                        \"Ignored resources: \" + ignored_resources)\n                logger.run_info(\"\\n\".join(dag.stats()))\n            else:\n                logger.info(\"Nothing to be done.\")\n        if dryrun and not len(dag):\n            logger.info(\"Nothing to be done.\")\n\n        success = scheduler.schedule()\n\n        if success:\n            if dryrun:\n                if not quiet and len(dag):\n                    logger.run_info(\"\\n\".join(dag.stats()))\n            elif stats:\n                scheduler.stats.to_json(stats)\n            if not dryrun and not no_hooks:\n                self._onsuccess(logger.get_logfile())\n            return True\n        else:\n            if not dryrun and not no_hooks:\n                self._onerror(logger.get_logfile())\n            return False\n\n    def include(self, snakefile,\n                overwrite_first_rule=False,\n                print_compilation=False,\n                overwrite_shellcmd=None):\n        \"\"\"\n        Include a snakefile.\n        \"\"\"\n        # check if snakefile is a path to the filesystem\n        if not urllib.parse.urlparse(snakefile).scheme:\n            if not os.path.isabs(snakefile) and self.included_stack:\n                current_path = os.path.dirname(self.included_stack[-1])\n                snakefile = os.path.join(current_path, snakefile)\n            snakefile = os.path.abspath(snakefile)\n        # else it could be an url.\n        # at least we don't want to modify the path for clarity.\n\n        if snakefile in self.included:\n            logger.info(\"Multiple include of {} ignored\".format(snakefile))\n            return\n        self.included.append(snakefile)\n        self.included_stack.append(snakefile)\n\n        global workflow\n\n        workflow = self\n\n        first_rule = self.first_rule\n        code, linemap = parse(snakefile,\n                              overwrite_shellcmd=self.overwrite_shellcmd)\n\n        if print_compilation:\n            print(code)\n\n        # insert the current directory into sys.path\n        # this allows to import modules from the workflow directory\n        sys.path.insert(0, os.path.dirname(snakefile))\n\n        self.linemaps[snakefile] = linemap\n        exec(compile(code, snakefile, \"exec\"), self.globals)\n        if not overwrite_first_rule:\n            self.first_rule = first_rule\n        self.included_stack.pop()\n\n    def onsuccess(self, func):\n        self._onsuccess = func\n\n    def onerror(self, func):\n        self._onerror = func\n\n    def workdir(self, workdir):\n        if self.overwrite_workdir is None:\n            if not os.path.exists(workdir):\n                os.makedirs(workdir)\n            self._workdir = workdir\n            os.chdir(workdir)\n\n    def configfile(self, jsonpath):\n        \"\"\" Update the global config with the given dictionary. \"\"\"\n        global config\n        c = snakemake.io.load_configfile(jsonpath)\n        update_config(config, c)\n        update_config(config, self.overwrite_config)\n\n    def ruleorder(self, *rulenames):\n        self._ruleorder.add(*rulenames)\n\n    def subworkflow(self, name, snakefile=None, workdir=None):\n        sw = Subworkflow(self, name, snakefile, workdir)\n        self._subworkflows[name] = sw\n        self.globals[name] = sw.target\n\n    def localrules(self, *rulenames):\n        self._localrules.update(rulenames)\n\n    def rule(self, name=None, lineno=None, snakefile=None):\n        name = self.add_rule(name, lineno, snakefile)\n        rule = self.get_rule(name)\n\n        def decorate(ruleinfo):\n            if ruleinfo.input:\n                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])\n            if ruleinfo.output:\n                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])\n            if ruleinfo.params:\n                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])\n            if ruleinfo.threads:\n                if not isinstance(ruleinfo.threads, int):\n                    raise RuleException(\"Threads value has to be an integer.\",\n                                        rule=rule)\n                rule.resources[\"_cores\"] = ruleinfo.threads\n            if ruleinfo.resources:\n                args, resources = ruleinfo.resources\n                if args:\n                    raise RuleException(\"Resources have to be named.\")\n                if not all(map(lambda r: isinstance(r, int),\n                               resources.values())):\n                    raise RuleException(\n                        \"Resources values have to be integers.\",\n                        rule=rule)\n                rule.resources.update(resources)\n            if ruleinfo.priority:\n                if (not isinstance(ruleinfo.priority, int) and\n                    not isinstance(ruleinfo.priority, float)):\n                    raise RuleException(\"Priority values have to be numeric.\",\n                                        rule=rule)\n                rule.priority = ruleinfo.priority\n            if ruleinfo.version:\n                rule.version = ruleinfo.version\n            if ruleinfo.log:\n                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])\n            if ruleinfo.message:\n                rule.message = ruleinfo.message\n            if ruleinfo.benchmark:\n                rule.benchmark = ruleinfo.benchmark\n            rule.norun = ruleinfo.norun\n            rule.docstring = ruleinfo.docstring\n            rule.run_func = ruleinfo.func\n            rule.shellcmd = ruleinfo.shellcmd\n            ruleinfo.func.__name__ = \"__{}\".format(name)\n            self.globals[ruleinfo.func.__name__] = ruleinfo.func\n            setattr(rules, name, rule)\n            return ruleinfo.func\n\n        return decorate\n\n    def docstring(self, string):\n        def decorate(ruleinfo):\n            ruleinfo.docstring = string\n            return ruleinfo\n\n        return decorate\n\n    def input(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.input = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def output(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.output = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def params(self, *params, **kwparams):\n        def decorate(ruleinfo):\n            ruleinfo.params = (params, kwparams)\n            return ruleinfo\n\n        return decorate\n\n    def message(self, message):\n        def decorate(ruleinfo):\n            ruleinfo.message = message\n            return ruleinfo\n\n        return decorate\n\n    def benchmark(self, benchmark):\n        def decorate(ruleinfo):\n            ruleinfo.benchmark = benchmark\n            return ruleinfo\n\n        return decorate\n\n    def threads(self, threads):\n        def decorate(ruleinfo):\n            ruleinfo.threads = threads\n            return ruleinfo\n\n        return decorate\n\n    def resources(self, *args, **resources):\n        def decorate(ruleinfo):\n            ruleinfo.resources = (args, resources)\n            return ruleinfo\n\n        return decorate\n\n    def priority(self, priority):\n        def decorate(ruleinfo):\n            ruleinfo.priority = priority\n            return ruleinfo\n\n        return decorate\n\n    def version(self, version):\n        def decorate(ruleinfo):\n            ruleinfo.version = version\n            return ruleinfo\n\n        return decorate\n\n    def log(self, *logs, **kwlogs):\n        def decorate(ruleinfo):\n            ruleinfo.log = (logs, kwlogs)\n            return ruleinfo\n\n        return decorate\n\n    def shellcmd(self, cmd):\n        def decorate(ruleinfo):\n            ruleinfo.shellcmd = cmd\n            return ruleinfo\n\n        return decorate\n\n    def norun(self):\n        def decorate(ruleinfo):\n            ruleinfo.norun = True\n            return ruleinfo\n\n        return decorate\n\n    def run(self, func):\n        return RuleInfo(func)\n\n    @staticmethod\n    def _empty_decorator(f):\n        return f\n\n\nclass RuleInfo:\n    def __init__(self, func):\n        self.func = func\n        self.shellcmd = None\n        self.norun = False\n        self.input = None\n        self.output = None\n        self.params = None\n        self.message = None\n        self.benchmark = None\n        self.threads = None\n        self.resources = None\n        self.priority = None\n        self.version = None\n        self.log = None\n        self.docstring = None\n\n\nclass Subworkflow:\n    def __init__(self, workflow, name, snakefile, workdir):\n        self.workflow = workflow\n        self.name = name\n        self._snakefile = snakefile\n        self._workdir = workdir\n\n    @property\n    def snakefile(self):\n        if self._snakefile is None:\n            return os.path.abspath(os.path.join(self.workdir, \"Snakefile\"))\n        if not os.path.isabs(self._snakefile):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                self._snakefile))\n        return self._snakefile\n\n    @property\n    def workdir(self):\n        workdir = \".\" if self._workdir is None else self._workdir\n        if not os.path.isabs(workdir):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                workdir))\n        return workdir\n\n    def target(self, paths):\n        if not_iterable(paths):\n            return flag(os.path.join(self.workdir, paths), \"subworkflow\", self)\n        return [self.target(path) for path in paths]\n\n    def targets(self, dag):\n        return [f for job in dag.jobs for f in job.subworkflow_input\n                if job.subworkflow_input[f] is self]\n\n\nclass Rules:\n    \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\"\n    pass\n\n\ndef srcdir(path):\n    \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\"\n    if not workflow.included_stack:\n        return None\n    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/sordhlm/Kiwi_optimization/blob/3345d2e142c0fc1257c67088071ff889f19bd869",
        "file_path": "/tcms/core/ajax.py",
        "source": "# -*- coding: utf-8 -*-\n\"\"\"\nShared functions for plan/case/run.\n\nMost of these functions are use for Ajax.\n\"\"\"\nimport datetime\nimport sys\nimport json\nfrom distutils.util import strtobool\n\nfrom django import http\nfrom django.db.models import Q, Count\nfrom django.contrib.auth.models import User\nfrom django.core import serializers\nfrom django.core.exceptions import ObjectDoesNotExist\nfrom django.apps import apps\nfrom django.forms import ValidationError\nfrom django.http import Http404\nfrom django.http import HttpResponse\nfrom django.shortcuts import render\nfrom django.views.decorators.http import require_GET\nfrom django.views.decorators.http import require_POST\n\nfrom tcms.signals import POST_UPDATE_SIGNAL\nfrom tcms.management.models import Component, Build, Version\nfrom tcms.management.models import Priority\nfrom tcms.management.models import Tag\nfrom tcms.management.models import EnvGroup, EnvProperty, EnvValue\nfrom tcms.testcases.models import TestCase, Bug\nfrom tcms.testcases.models import Category\nfrom tcms.testcases.models import TestCaseStatus, TestCaseTag\nfrom tcms.testcases.views import plan_from_request_or_none\nfrom tcms.testplans.models import TestPlan, TestCasePlan, TestPlanTag\nfrom tcms.testruns.models import TestRun, TestCaseRun, TestCaseRunStatus, TestRunTag\nfrom tcms.core.helpers.comments import add_comment\nfrom tcms.core.utils.validations import validate_bug_id\n\n\ndef check_permission(request, ctype):\n    perm = '%s.change_%s' % tuple(ctype.split('.'))\n    if request.user.has_perm(perm):\n        return True\n    return False\n\n\ndef strip_parameters(request_dict, skip_parameters):\n    parameters = {}\n    for key, value in request_dict.items():\n        if key not in skip_parameters and value:\n            parameters[str(key)] = value\n\n    return parameters\n\n\n@require_GET\ndef info(request):\n    \"\"\"Ajax responder for misc information\"\"\"\n\n    objects = _InfoObjects(request=request, product_id=request.GET.get('product_id'))\n    info_type = getattr(objects, request.GET.get('info_type'))\n\n    if not info_type:\n        return HttpResponse('Unrecognizable info-type')\n\n    if request.GET.get('format') == 'ulli':\n        field = request.GET.get('field', default='name')\n\n        response_str = '<ul>'\n        for obj_value in info_type().values(field):\n            response_str += '<li>' + obj_value.get(field, None) + '</li>'\n        response_str += '</ul>'\n\n        return HttpResponse(response_str)\n\n    return HttpResponse(serializers.serialize('json', info_type(), fields=('name', 'value')))\n\n\nclass _InfoObjects(object):\n\n    def __init__(self, request, product_id=None):\n        self.request = request\n        try:\n            self.product_id = int(product_id)\n        except (ValueError, TypeError):\n            self.product_id = 0\n\n    def builds(self):\n        try:\n            is_active = strtobool(self.request.GET.get('is_active', default='False'))\n        except (ValueError, TypeError):\n            is_active = False\n\n        return Build.objects.filter(product_id=self.product_id, is_active=is_active)\n\n    def categories(self):\n        return Category.objects.filter(product__id=self.product_id)\n\n    def components(self):\n        return Component.objects.filter(product__id=self.product_id)\n\n    def env_groups(self):\n        return EnvGroup.objects.all()\n\n    def env_properties(self):\n        if self.request.GET.get('env_group_id'):\n            return EnvGroup.objects.get(id=self.request.GET['env_group_id']).property.all()\n        return EnvProperty.objects.all()\n\n    def env_values(self):\n        return EnvValue.objects.filter(property__id=self.request.GET.get('env_property_id'))\n\n    def users(self):\n        query = strip_parameters(self.request.GET, skip_parameters=('info_type', 'field', 'format'))\n        return User.objects.filter(**query)\n\n    def versions(self):\n        return Version.objects.filter(product__id=self.product_id)\n\n\n@require_GET\ndef form(request):\n    \"\"\"Response get form ajax call, most using in dialog\"\"\"\n\n    # The parameters in internal_parameters will delete from parameters\n    internal_parameters = ['app_form', 'format']\n    parameters = strip_parameters(request.GET, internal_parameters)\n    q_app_form = request.GET.get('app_form')\n    q_format = request.GET.get('format')\n    if not q_format:\n        q_format = 'p'\n\n    if not q_app_form:\n        return HttpResponse('Unrecognizable app_form')\n\n    # Get the form\n    q_app, q_form = q_app_form.split('.')[0], q_app_form.split('.')[1]\n    exec('from tcms.%s.forms import %s as form' % (q_app, q_form))\n    __import__('tcms.%s.forms' % q_app)\n    q_app_module = sys.modules['tcms.%s.forms' % q_app]\n    form_class = getattr(q_app_module, q_form)\n    form_params = form_class(initial=parameters)\n\n    # Generate the HTML and reponse\n    html = getattr(form_params, 'as_' + q_format)\n    return HttpResponse(html())\n\n\ndef tags(request):\n    \"\"\" Get tags for TestPlan, TestCase or TestRun \"\"\"\n\n    tag_objects = _TagObjects(request)\n    template_name, obj = tag_objects.get()\n\n    q_tag = request.GET.get('tags')\n    q_action = request.GET.get('a')\n\n    if q_action:\n        tag_actions = _TagActions(obj=obj, tag_name=q_tag)\n        getattr(tag_actions, q_action)()\n\n    all_tags = obj.tag.all().order_by('pk')\n    test_plan_tags = TestPlanTag.objects.filter(\n        tag__in=all_tags).values('tag').annotate(num_plans=Count('tag')).order_by('tag')\n    test_case_tags = TestCaseTag.objects.filter(\n        tag__in=all_tags).values('tag').annotate(num_cases=Count('tag')).order_by('tag')\n    test_run_tags = TestRunTag.objects.filter(\n        tag__in=all_tags).values('tag').annotate(num_runs=Count('tag')).order_by('tag')\n\n    plan_counter = _TagCounter('num_plans', test_plan_tags)\n    case_counter = _TagCounter('num_cases', test_case_tags)\n    run_counter = _TagCounter('num_runs', test_run_tags)\n\n    for tag in all_tags:\n        tag.num_plans = plan_counter.calculate_tag_count(tag)\n        tag.num_cases = case_counter.calculate_tag_count(tag)\n        tag.num_runs = run_counter.calculate_tag_count(tag)\n\n    context_data = {\n        'tags': all_tags,\n        'object': obj,\n    }\n    return render(request, template_name, context_data)\n\n\nclass _TagObjects(object):\n    \"\"\" Used for getting the chosen object(TestPlan, TestCase or TestRun) from the database \"\"\"\n\n    def __init__(self, request):\n        \"\"\"\n        :param request: An HTTP GET request, containing the primary key\n                        and the type of object to be selected\n        :type request: HttpRequest\n        \"\"\"\n        for obj in ['plan', 'case', 'run']:\n            if request.GET.get(obj):\n                self.object = obj\n                self.object_pk = request.GET.get(obj)\n                break\n\n    def get(self):\n        func = getattr(self, self.object)\n        return func()\n\n    def plan(self):\n        return 'management/get_tag.html', TestPlan.objects.get(pk=self.object_pk)\n\n    def case(self):\n        return 'management/get_tag.html', TestCase.objects.get(pk=self.object_pk)\n\n    def run(self):\n        return 'run/get_tag.html', TestRun.objects.get(pk=self.object_pk)\n\n\nclass _TagActions(object):\n    \"\"\" Used for performing the 'add' and 'remove' actions on a given tag \"\"\"\n\n    def __init__(self, obj, tag_name):\n        \"\"\"\n        :param obj: the object for which the tag actions would be performed\n        :type obj: either a :class:`tcms.testplans.models.TestPlan`,\n                          a :class:`tcms.testcases.models.TestCase` or\n                          a :class:`tcms.testruns.models.TestRun`\n        :param tag_name: The name of the tag to be manipulated\n        :type tag_name: str\n        \"\"\"\n        self.obj = obj\n        self.tag_name = tag_name\n\n    def add(self):\n        tag, _ = Tag.objects.get_or_create(name=self.tag_name)\n        self.obj.add_tag(tag)\n\n    def remove(self):\n        tag = Tag.objects.get(name=self.tag_name)\n        self.obj.remove_tag(tag)\n\n\nclass _TagCounter(object):\n    \"\"\" Used for counting the number of times a tag is assigned to TestRun/TestCase/TestPlan \"\"\"\n\n    def __init__(self, key, test_tags):\n        \"\"\"\n         :param key: either 'num_plans', 'num_cases', 'num_runs', depending on what you want count\n         :type key: str\n         :param test_tags: query set, containing the Tag->Object relationship, ordered by tag and\n                            annotated by key\n            e.g. TestPlanTag, TestCaseTag ot TestRunTag\n         :type test_tags: QuerySet\n        \"\"\"\n        self.key = key\n        self.test_tags = iter(test_tags)\n        self.counter = {'tag': 0}\n\n    def calculate_tag_count(self, tag):\n        \"\"\"\n        :param tag: the tag you do the counting for\n        :type tag: :class:`tcms.management.models.Tag`\n        :return: the number of times a tag is assigned to object\n        :rtype: int\n        \"\"\"\n        if self.counter['tag'] != tag.pk:\n            try:\n                self.counter = self.test_tags.__next__()\n            except StopIteration:\n                return 0\n\n        if tag.pk == self.counter['tag']:\n            return self.counter[self.key]\n        return 0\n\n\ndef get_value_by_type(val, v_type):\n    \"\"\"\n    Exampls:\n    1. get_value_by_type('True', 'bool')\n    (1, None)\n    2. get_value_by_type('19860624 123059', 'datetime')\n    (datetime.datetime(1986, 6, 24, 12, 30, 59), None)\n    3. get_value_by_type('5', 'int')\n    ('5', None)\n    4. get_value_by_type('string', 'str')\n    ('string', None)\n    5. get_value_by_type('everything', 'None')\n    (None, None)\n    6. get_value_by_type('buggy', 'buggy')\n    (None, 'Unsupported value type.')\n    7. get_value_by_type('string', 'int')\n    (None, \"invalid literal for int() with base 10: 'string'\")\n    \"\"\"\n    value = error = None\n\n    def get_time(time):\n        date_time = datetime.datetime\n        if time == 'NOW':\n            return date_time.now()\n        return date_time.strptime(time, '%Y%m%d %H%M%S')\n\n    pipes = {\n        # Temporary solution is convert all of data to str\n        # 'bool': lambda x: x == 'True',\n        'bool': lambda x: x == 'True' and 1 or 0,\n        'datetime': get_time,\n        'int': lambda x: str(int(x)),\n        'str': lambda x: str(x),\n        'None': lambda x: None,\n    }\n    pipe = pipes.get(v_type, None)\n    if pipe is None:\n        error = 'Unsupported value type.'\n    else:\n        try:\n            value = pipe(val)\n        except Exception as e:\n            error = str(e)\n    return value, error\n\n\ndef say_no(error_msg):\n    ajax_response = {'rc': 1, 'response': error_msg}\n    return HttpResponse(json.dumps(ajax_response))\n\n\ndef say_yes():\n    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))\n\n\n# Deprecated. Not flexible.\n@require_POST\ndef update(request):\n    \"\"\"\n    Generic approach to update a model,\\n\n    based on contenttype.\n    \"\"\"\n    now = datetime.datetime.now()\n\n    data = request.POST.copy()\n    ctype = data.get(\"content_type\")\n    vtype = data.get('value_type', 'str')\n    object_pk_str = data.get(\"object_pk\")\n    field = data.get('field')\n    value = data.get('value')\n\n    object_pk = [int(a) for a in object_pk_str.split(',')]\n\n    if not field or not value or not object_pk or not ctype:\n        return say_no(\n            'Following fields are required - content_type, '\n            'object_pk, field and value.')\n\n    # Convert the value type\n    # FIXME: Django bug here: update() keywords must be strings\n    field = str(field)\n\n    value, error = get_value_by_type(value, vtype)\n    if error:\n        return say_no(error)\n    has_perms = check_permission(request, ctype)\n    if not has_perms:\n        return say_no('Permission Dinied.')\n\n    model = apps.get_model(*ctype.split(\".\", 1))\n    targets = model._default_manager.filter(pk__in=object_pk)\n\n    if not targets:\n        return say_no('No record found')\n    if not hasattr(targets[0], field):\n        return say_no('%s has no field %s' % (ctype, field))\n\n    if hasattr(targets[0], 'log_action'):\n        for t in targets:\n            try:\n                t.log_action(\n                    who=request.user,\n                    action='Field %s changed from %s to %s.' % (\n                        field, getattr(t, field), value\n                    )\n                )\n            except (AttributeError, User.DoesNotExist):\n                pass\n    objects_update(targets, **{field: value})\n\n    if hasattr(model, 'mail_scene'):\n        mail_context = model.mail_scene(\n            objects=targets, field=field, value=value, ctype=ctype,\n            object_pk=object_pk,\n        )\n        if mail_context:\n            from tcms.core.utils.mailto import mailto\n\n            mail_context['context']['user'] = request.user\n            try:\n                mailto(**mail_context)\n            except Exception:  # nosec:B110:try_except_pass\n                pass\n\n    # Special hacking for updating test case run status\n    if ctype == 'testruns.testcaserun' and field == 'case_run_status':\n        for t in targets:\n            field = 'close_date'\n            t.log_action(\n                who=request.user,\n                action='Field %s changed from %s to %s.' % (\n                    field, getattr(t, field), now\n                )\n            )\n            if t.tested_by != request.user:\n                field = 'tested_by'\n                t.log_action(\n                    who=request.user,\n                    action='Field %s changed from %s to %s.' % (\n                        field, getattr(t, field), request.user\n                    )\n                )\n\n            field = 'assignee'\n            try:\n                assignee = t.assginee\n                if assignee != request.user:\n                    t.log_action(\n                        who=request.user,\n                        action='Field %s changed from %s to %s.' % (\n                            field, getattr(t, field), request.user\n                        )\n                    )\n                    # t.assignee = request.user\n                t.save()\n            except (AttributeError, User.DoesNotExist):\n                pass\n        targets.update(close_date=now, tested_by=request.user)\n    return say_yes()\n\n\n@require_POST\ndef update_case_run_status(request):\n    \"\"\"\n    Update Case Run status.\n    \"\"\"\n    now = datetime.datetime.now()\n\n    data = request.POST.copy()\n    ctype = data.get(\"content_type\")\n    vtype = data.get('value_type', 'str')\n    object_pk_str = data.get(\"object_pk\")\n    field = data.get('field')\n    value = data.get('value')\n\n    object_pk = [int(a) for a in object_pk_str.split(',')]\n\n    if not field or not value or not object_pk or not ctype:\n        return say_no(\n            'Following fields are required - content_type, '\n            'object_pk, field and value.')\n\n    # Convert the value type\n    # FIXME: Django bug here: update() keywords must be strings\n    field = str(field)\n\n    value, error = get_value_by_type(value, vtype)\n    if error:\n        return say_no(error)\n    has_perms = check_permission(request, ctype)\n    if not has_perms:\n        return say_no('Permission Dinied.')\n\n    model = apps.get_model(*ctype.split(\".\", 1))\n    targets = model._default_manager.filter(pk__in=object_pk)\n\n    if not targets:\n        return say_no('No record found')\n    if not hasattr(targets[0], field):\n        return say_no('%s has no field %s' % (ctype, field))\n\n    if hasattr(targets[0], 'log_action'):\n        for t in targets:\n            try:\n                t.log_action(\n                    who=request.user,\n                    action='Field {} changed from {} to {}.'.format(\n                        field,\n                        getattr(t, field),\n                        TestCaseRunStatus.id_to_string(value),\n                    )\n                )\n            except (AttributeError, User.DoesNotExist):\n                pass\n    objects_update(targets, **{field: value})\n\n    if hasattr(model, 'mail_scene'):\n        from tcms.core.utils.mailto import mailto\n\n        mail_context = model.mail_scene(\n            objects=targets, field=field, value=value, ctype=ctype,\n            object_pk=object_pk,\n        )\n        if mail_context:\n            mail_context['context']['user'] = request.user\n            try:\n                mailto(**mail_context)\n            except Exception:  # nosec:B110:try_except_pass\n                pass\n\n    # Special hacking for updating test case run status\n    if ctype == 'testruns.testcaserun' and field == 'case_run_status':\n        for t in targets:\n            field = 'close_date'\n            t.log_action(\n                who=request.user,\n                action='Field %s changed from %s to %s.' % (\n                    field, getattr(t, field), now\n                )\n            )\n            if t.tested_by != request.user:\n                field = 'tested_by'\n                t.log_action(\n                    who=request.user,\n                    action='Field %s changed from %s to %s.' % (\n                        field, getattr(t, field), request.user\n                    )\n                )\n\n            field = 'assignee'\n            try:\n                assignee = t.assginee\n                if assignee != request.user:\n                    t.log_action(\n                        who=request.user,\n                        action='Field %s changed from %s to %s.' % (\n                            field, getattr(t, field), request.user\n                        )\n                    )\n                    # t.assignee = request.user\n                t.save()\n            except (AttributeError, User.DoesNotExist):\n                pass\n        targets.update(close_date=now, tested_by=request.user)\n\n    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))\n\n\nclass ModelUpdateActions(object):\n    \"\"\"Abstract class defining interfaces to update a model properties\"\"\"\n\n\nclass TestCaseUpdateActions(ModelUpdateActions):\n    \"\"\"Actions to update each possible proprety of TestCases\n\n    Define your own method named _update_[property name] to hold specific\n    update logic.\n    \"\"\"\n\n    ctype = 'testcases.testcase'\n\n    def __init__(self, request):\n        self.request = request\n        self.target_field = request.POST.get('target_field')\n        self.new_value = request.POST.get('new_value')\n\n    def get_update_action(self):\n        return getattr(self, '_update_%s' % self.target_field, None)\n\n    def update(self):\n        has_perms = check_permission(self.request, self.ctype)\n        if not has_perms:\n            return say_no(\"You don't have enough permission to update TestCases.\")\n\n        action = self.get_update_action()\n        if action is not None:\n            try:\n                resp = action()\n                self._sendmail()\n            except ObjectDoesNotExist as err:\n                return say_no(str(err))\n            except Exception:\n                # TODO: besides this message to users, what happening should be\n                # recorded in the system log.\n                return say_no('Update failed. Please try again or request '\n                              'support from your organization.')\n            else:\n                if resp is None:\n                    resp = say_yes()\n                return resp\n        return say_no('Not know what to update.')\n\n    def get_update_targets(self):\n        \"\"\"Get selected cases to update their properties\"\"\"\n        case_ids = map(int, self.request.POST.getlist('case'))\n        self._update_objects = TestCase.objects.filter(pk__in=case_ids)\n        return self._update_objects\n\n    def get_plan(self, pk_enough=True):\n        try:\n            return plan_from_request_or_none(self.request, pk_enough)\n        except Http404:\n            return None\n\n    def _sendmail(self):\n        mail_context = TestCase.mail_scene(objects=self._update_objects,\n                                           field=self.target_field,\n                                           value=self.new_value)\n        if mail_context:\n            from tcms.core.utils.mailto import mailto\n\n            mail_context['context']['user'] = self.request.user\n            try:\n                mailto(**mail_context)\n            except Exception:  # nosec:B110:try_except_pass\n                pass\n\n    def _update_priority(self):\n        exists = Priority.objects.filter(pk=self.new_value).exists()\n        if not exists:\n            raise ObjectDoesNotExist('The priority you specified to change '\n                                     'does not exist.')\n        self.get_update_targets().update(**{str(self.target_field): self.new_value})\n\n    def _update_default_tester(self):\n        try:\n            user = User.objects.get(Q(username=self.new_value) | Q(email=self.new_value))\n        except User.DoesNotExist:\n            raise ObjectDoesNotExist('Default tester not found!')\n        self.get_update_targets().update(**{str(self.target_field): user.pk})\n\n    def _update_case_status(self):\n        try:\n            new_status = TestCaseStatus.objects.get(pk=self.new_value)\n        except TestCaseStatus.DoesNotExist:\n            raise ObjectDoesNotExist('The status you choose does not exist.')\n\n        update_object = self.get_update_targets()\n        if not update_object:\n            return say_no('No record(s) found')\n\n        for testcase in update_object:\n            if hasattr(testcase, 'log_action'):\n                testcase.log_action(\n                    who=self.request.user,\n                    action='Field %s changed from %s to %s.' % (\n                        self.target_field, testcase.case_status, new_status.name\n                    )\n                )\n        update_object.update(**{str(self.target_field): self.new_value})\n\n        # ###\n        # Case is moved between Cases and Reviewing Cases tabs accoding to the\n        # change of status. Meanwhile, the number of cases with each status\n        # should be updated also.\n\n        try:\n            plan = plan_from_request_or_none(self.request)\n        except Http404:\n            return say_no(\"No plan record found.\")\n        else:\n            if plan is None:\n                return say_no('No plan record found.')\n\n        confirm_status_name = 'CONFIRMED'\n        plan.run_case = plan.case.filter(case_status__name=confirm_status_name)\n        plan.review_case = plan.case.exclude(case_status__name=confirm_status_name)\n        run_case_count = plan.run_case.count()\n        case_count = plan.case.count()\n        # FIXME: why not calculate review_case_count or run_case_count by using\n        # substraction, which saves one SQL query.\n        review_case_count = plan.review_case.count()\n\n        return http.JsonResponse({\n            'rc': 0, 'response': 'ok',\n            'run_case_count': run_case_count,\n            'case_count': case_count,\n            'review_case_count': review_case_count,\n        })\n\n    def _update_sortkey(self):\n        try:\n            sortkey = int(self.new_value)\n            if sortkey < 0 or sortkey > 32300:\n                return say_no('New sortkey is out of range [0, 32300].')\n        except ValueError:\n            return say_no('New sortkey is not an integer.')\n        plan = plan_from_request_or_none(self.request, pk_enough=True)\n        if plan is None:\n            return say_no('No plan record found.')\n        update_targets = self.get_update_targets()\n\n        # ##\n        # MySQL does not allow to exeucte UPDATE statement that contains\n        # subquery querying from same table. In this case, OperationError will\n        # be raised.\n        offset = 0\n        step_length = 500\n        queryset_filter = TestCasePlan.objects.filter\n        data = {self.target_field: sortkey}\n        while 1:\n            sub_cases = update_targets[offset:offset + step_length]\n            case_pks = [case.pk for case in sub_cases]\n            if len(case_pks) == 0:\n                break\n            queryset_filter(plan=plan, case__in=case_pks).update(**data)\n            # Move to next batch of cases to change.\n            offset += step_length\n\n    def _update_reviewer(self):\n        reviewers = User.objects.filter(username=self.new_value).values_list('pk', flat=True)\n        if not reviewers:\n            err_msg = 'Reviewer %s is not found' % self.new_value\n            raise ObjectDoesNotExist(err_msg)\n        self.get_update_targets().update(**{str(self.target_field): reviewers[0]})\n\n\n# NOTE: what permission is necessary\n# FIXME: find a good chance to map all TestCase property change request to this\n@require_POST\ndef update_cases_default_tester(request):\n    \"\"\"Update default tester upon selected TestCases\"\"\"\n    proxy = TestCaseUpdateActions(request)\n    return proxy.update()\n\n\nupdate_cases_priority = update_cases_default_tester\nupdate_cases_case_status = update_cases_default_tester\nupdate_cases_sortkey = update_cases_default_tester\nupdate_cases_reviewer = update_cases_default_tester\n\n\n@require_POST\ndef comment_case_runs(request):\n    \"\"\"\n    Add comment to one or more caseruns at a time.\n    \"\"\"\n    data = request.POST.copy()\n    comment = data.get('comment', None)\n    if not comment:\n        return say_no('Comments needed')\n    run_ids = [i for i in data.get('run', '').split(',') if i]\n    if not run_ids:\n        return say_no('No runs selected.')\n    runs = TestCaseRun.objects.filter(pk__in=run_ids).only('pk')\n    if not runs:\n        return say_no('No caserun found.')\n    add_comment(runs, comment, request.user)\n    return say_yes()\n\n\ndef clean_bug_form(request):\n    \"\"\"\n    Verify the form data, return a tuple\\n\n    (None, ERROR_MSG) on failure\\n\n    or\\n\n    (data_dict, '') on success.\\n\n    \"\"\"\n    data = {}\n    try:\n        data['bugs'] = request.GET.get('bug_id', '').split(',')\n        data['runs'] = map(int, request.GET.get('case_runs', '').split(','))\n    except (TypeError, ValueError) as e:\n        return (None, 'Please specify only integers for bugs, '\n                      'caseruns(using comma to seperate IDs), '\n                      'and bug_system. (DEBUG INFO: %s)' % str(e))\n\n    data['bug_system_id'] = int(request.GET.get('bug_system_id', 1))\n\n    if request.GET.get('a') not in ('add', 'remove'):\n        return (None, 'Actions only allow \"add\" and \"remove\".')\n    else:\n        data['action'] = request.GET.get('a')\n    data['bz_external_track'] = True if request.GET.get('bz_external_track',\n                                                        False) else False\n\n    return (data, '')\n\n\ndef update_bugs_to_caseruns(request):\n    \"\"\"\n    Add one or more bugs to or remove that from\\n\n    one or more caserun at a time.\n    \"\"\"\n    data, error = clean_bug_form(request)\n    if error:\n        return say_no(error)\n    runs = TestCaseRun.objects.filter(pk__in=data['runs'])\n    bug_system_id = data['bug_system_id']\n    bug_ids = data['bugs']\n\n    try:\n        validate_bug_id(bug_ids, bug_system_id)\n    except ValidationError as e:\n        return say_no(str(e))\n\n    bz_external_track = data['bz_external_track']\n    action = data['action']\n    try:\n        if action == \"add\":\n            for run in runs:\n                for bug_id in bug_ids:\n                    run.add_bug(bug_id=bug_id,\n                                bug_system_id=bug_system_id,\n                                bz_external_track=bz_external_track)\n        else:\n            bugs = Bug.objects.filter(bug_id__in=bug_ids)\n            for run in runs:\n                for bug in bugs:\n                    if bug.case_run_id == run.pk:\n                        run.remove_bug(bug.bug_id, run.pk)\n    except Exception as e:\n        return say_no(str(e))\n    return say_yes()\n\n\ndef get_prod_related_objs(p_pks, target):\n    \"\"\"\n    Get Component, Version, Category, and Build\\n\n    Return [(id, name), (id, name)]\n    \"\"\"\n    ctypes = {\n        'component': (Component, 'name'),\n        'version': (Version, 'value'),\n        'build': (Build, 'name'),\n        'category': (Category, 'name'),\n    }\n    results = ctypes[target][0]._default_manager.filter(product__in=p_pks)\n    attr = ctypes[target][1]\n    results = [(r.pk, getattr(r, attr)) for r in results]\n    return results\n\n\ndef get_prod_related_obj_json(request):\n    \"\"\"\n    View for updating product drop-down\\n\n    in a Ajax way.\n    \"\"\"\n    data = request.GET.copy()\n    target = data.get('target', None)\n    p_pks = data.get('p_ids', None)\n    sep = data.get('sep', None)\n    # py2.6: all(*values) => boolean ANDs\n    if target and p_pks and sep:\n        p_pks = [k for k in p_pks.split(sep) if k]\n        res = get_prod_related_objs(p_pks, target)\n    else:\n        res = []\n    return HttpResponse(json.dumps(res))\n\n\ndef objects_update(objects, **kwargs):\n    objects.update(**kwargs)\n    kwargs['instances'] = objects\n    if objects.model.__name__ == TestCaseRun.__name__ and kwargs.get(\n            'case_run_status', None):\n        POST_UPDATE_SIGNAL.send(sender=None, **kwargs)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/sordhlm/Kiwi_optimization/blob/3345d2e142c0fc1257c67088071ff889f19bd869",
        "file_path": "/tcms/core/tests/test_views.py",
        "source": "# -*- coding: utf-8 -*-\n\nimport json\nfrom http import HTTPStatus\nfrom urllib.parse import urlencode\n\nfrom django import test\nfrom django.conf import settings\nfrom django.contrib.contenttypes.models import ContentType\nfrom django.core import serializers\nfrom django.urls import reverse\nfrom django_comments.models import Comment\n\nfrom tcms.management.models import Priority\nfrom tcms.management.models import EnvGroup\nfrom tcms.management.models import EnvProperty\nfrom tcms.testcases.forms import CaseAutomatedForm\nfrom tcms.testcases.forms import TestCase\nfrom tcms.testplans.models import TestPlan\nfrom tcms.testruns.models import TestCaseRun\nfrom tcms.testruns.models import TestCaseRunStatus\nfrom tcms.tests import BaseCaseRun\nfrom tcms.tests import BasePlanCase\nfrom tcms.tests import remove_perm_from_user\nfrom tcms.tests import user_should_have_perm\nfrom tcms.tests.factories import UserFactory\nfrom tcms.tests.factories import EnvGroupFactory\nfrom tcms.tests.factories import EnvGroupPropertyMapFactory\nfrom tcms.tests.factories import EnvPropertyFactory\n\n\nclass TestNavigation(test.TestCase):\n    @classmethod\n    def setUpTestData(cls):\n        super(TestNavigation, cls).setUpTestData()\n        cls.user = UserFactory(email='user+1@example.com')\n        cls.user.set_password('testing')\n        cls.user.save()\n\n    def test_urls_for_emails_with_pluses(self):\n        # test for https://github.com/Nitrate/Nitrate/issues/262\n        # when email contains + sign it needs to be properly urlencoded\n        # before passing it as query string argument to the search views\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.user.username,\n            password='testing')\n        response = self.client.get(reverse('iframe-navigation'))\n\n        self.assertContains(response, urlencode({'people': self.user.email}))\n        self.assertContains(response, urlencode({'author__email__startswith': self.user.email}))\n\n\nclass TestIndex(BaseCaseRun):\n    def test_when_not_logged_in_index_page_redirects_to_login(self):\n        response = self.client.get(reverse('core-views-index'))\n        self.assertRedirects(\n            response,\n            reverse('tcms-login'),\n            target_status_code=HTTPStatus.OK)\n\n    def test_when_logged_in_index_page_redirects_to_dashboard(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n        response = self.client.get(reverse('core-views-index'))\n        self.assertRedirects(\n            response,\n            reverse('tcms-recent', args=[self.tester.username]),\n            target_status_code=HTTPStatus.OK)\n\n\nclass TestCommentCaseRuns(BaseCaseRun):\n    \"\"\"Test case for ajax.comment_case_runs\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestCommentCaseRuns, cls).setUpTestData()\n        cls.many_comments_url = reverse('ajax-comment_case_runs')\n\n    def test_refuse_if_missing_comment(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.many_comments_url,\n                                    {'run': [self.case_run_1.pk, self.case_run_2.pk]})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'Comments needed'})\n\n    def test_refuse_if_missing_no_case_run_pk(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.many_comments_url,\n                                    {'comment': 'new comment', 'run': []})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'No runs selected.'})\n\n        response = self.client.post(self.many_comments_url,\n                                    {'comment': 'new comment'})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'No runs selected.'})\n\n    def test_refuse_if_passed_case_run_pks_not_exist(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.many_comments_url,\n                                    {'comment': 'new comment',\n                                     'run': '99999998,1009900'})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'No caserun found.'})\n\n    def test_add_comment_to_case_runs(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        new_comment = 'new comment'\n        response = self.client.post(\n            self.many_comments_url,\n            {'comment': new_comment,\n             'run': ','.join([str(self.case_run_1.pk),\n                              str(self.case_run_2.pk)])})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 0, 'response': 'ok'})\n\n        # Assert comments are added\n        case_run_ct = ContentType.objects.get_for_model(TestCaseRun)\n\n        for case_run_pk in (self.case_run_1.pk, self.case_run_2.pk):\n            comments = Comment.objects.filter(object_pk=case_run_pk,\n                                              content_type=case_run_ct)\n            self.assertEqual(new_comment, comments[0].comment)\n            self.assertEqual(self.tester, comments[0].user)\n\n\nclass TestUpdateObject(BasePlanCase):\n    \"\"\"Test case for update\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestUpdateObject, cls).setUpTestData()\n\n        cls.permission = 'testplans.change_testplan'\n        cls.update_url = reverse('ajax-update')\n\n    def setUp(self):\n        user_should_have_perm(self.tester, self.permission)\n\n    def test_refuse_if_missing_permission(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        remove_perm_from_user(self.tester, self.permission)\n\n        post_data = {\n            'content_type': 'testplans.testplan',\n            'object_pk': self.plan.pk,\n            'field': 'is_active',\n            'value': 'False',\n            'value_type': 'bool'\n        }\n\n        response = self.client.post(self.update_url, post_data)\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'Permission Dinied.'})\n\n    def test_update_plan_is_active(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        post_data = {\n            'content_type': 'testplans.testplan',\n            'object_pk': self.plan.pk,\n            'field': 'is_active',\n            'value': 'False',\n            'value_type': 'bool'\n        }\n\n        response = self.client.post(self.update_url, post_data)\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 0, 'response': 'ok'})\n        plan = TestPlan.objects.get(pk=self.plan.pk)\n        self.assertFalse(plan.is_active)\n\n\nclass TestUpdateCaseRunStatus(BaseCaseRun):\n    \"\"\"Test case for update_case_run_status\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestUpdateCaseRunStatus, cls).setUpTestData()\n\n        cls.permission = 'testruns.change_testcaserun'\n        cls.update_url = reverse('ajax-update_case_run_status')\n\n    def setUp(self):\n        user_should_have_perm(self.tester, self.permission)\n\n    def test_refuse_if_missing_permission(self):\n        remove_perm_from_user(self.tester, self.permission)\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.update_url, {\n            'content_type': 'testruns.testcaserun',\n            'object_pk': self.case_run_1.pk,\n            'field': 'case_run_status',\n            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),\n            'value_type': 'int',\n        })\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'Permission Dinied.'})\n\n    def test_change_case_run_status(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.update_url, {\n            'content_type': 'testruns.testcaserun',\n            'object_pk': self.case_run_1.pk,\n            'field': 'case_run_status',\n            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),\n            'value_type': 'int',\n        })\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 0, 'response': 'ok'})\n        self.assertEqual(\n            'PAUSED', TestCaseRun.objects.get(pk=self.case_run_1.pk).case_run_status.name)\n\n\nclass TestGetForm(test.TestCase):\n    \"\"\"Test case for form\"\"\"\n\n    def test_get_form(self):\n        response = self.client.get(reverse('ajax-form'),\n                                   {'app_form': 'testcases.CaseAutomatedForm'})\n        form = CaseAutomatedForm()\n        self.assertHTMLEqual(str(response.content, encoding=settings.DEFAULT_CHARSET), form.as_p())\n\n\nclass TestUpdateCasePriority(BasePlanCase):\n    \"\"\"Test case for update_cases_default_tester\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestUpdateCasePriority, cls).setUpTestData()\n\n        cls.permission = 'testcases.change_testcase'\n        cls.case_update_url = reverse('ajax-update_cases_default_tester')\n\n    def setUp(self):\n        user_should_have_perm(self.tester, self.permission)\n\n    def test_refuse_if_missing_permission(self):\n        remove_perm_from_user(self.tester, self.permission)\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(\n            self.case_update_url,\n            {\n                'target_field': 'priority',\n                'from_plan': self.plan.pk,\n                'case': [self.case_1.pk, self.case_3.pk],\n                'new_value': Priority.objects.get(value='P3').pk,\n            })\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': \"You don't have enough permission to \"\n                                  \"update TestCases.\"})\n\n    def test_update_case_priority(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(\n            self.case_update_url,\n            {\n                'target_field': 'priority',\n                'from_plan': self.plan.pk,\n                'case': [self.case_1.pk, self.case_3.pk],\n                'new_value': Priority.objects.get(value='P3').pk,\n            })\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 0, 'response': 'ok'})\n\n        for pk in (self.case_1.pk, self.case_3.pk):\n            self.assertEqual('P3', TestCase.objects.get(pk=pk).priority.value)\n\n\nclass TestGetObjectInfo(BasePlanCase):\n    \"\"\"Test case for info view method\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestGetObjectInfo, cls).setUpTestData()\n\n        cls.get_info_url = reverse('ajax-info')\n\n        cls.group_nitrate = EnvGroupFactory(name='nitrate')\n        cls.group_new = EnvGroupFactory(name='NewGroup')\n\n        cls.property_os = EnvPropertyFactory(name='os')\n        cls.property_python = EnvPropertyFactory(name='python')\n        cls.property_django = EnvPropertyFactory(name='django')\n\n        EnvGroupPropertyMapFactory(group=cls.group_nitrate,\n                                   property=cls.property_os)\n        EnvGroupPropertyMapFactory(group=cls.group_nitrate,\n                                   property=cls.property_python)\n        EnvGroupPropertyMapFactory(group=cls.group_new,\n                                   property=cls.property_django)\n\n    def test_get_env_properties(self):\n        response = self.client.get(self.get_info_url, {'info_type': 'env_properties'})\n\n        expected_json = json.loads(\n            serializers.serialize(\n                'json',\n                EnvProperty.objects.all(),\n                fields=('name', 'value')))\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            expected_json)\n\n    def test_get_env_properties_by_group(self):\n        response = self.client.get(self.get_info_url,\n                                   {'info_type': 'env_properties',\n                                    'env_group_id': self.group_new.pk})\n\n        group = EnvGroup.objects.get(pk=self.group_new.pk)\n        expected_json = json.loads(\n            serializers.serialize(\n                'json',\n                group.property.all(),\n                fields=('name', 'value')))\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            expected_json)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/XanaduAI/pennylane/blob/a04aad4c762a5abf314600f823c0d6c5b2a0af58",
        "file_path": "/openqml-pq/openqml_pq/projectq.py",
        "source": "# Copyright 2018 Xanadu Quantum Technologies Inc.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nr\"\"\"\nProjectQ plugin\n========================\n\n**Module name:** :mod:`openqml.plugins.projectq`\n\n.. currentmodule:: openqml.plugins.projectq\n\nThis plugin provides the interface between OpenQML and ProjecQ.\nIt enables OpenQML to optimize quantum circuits simulable with ProjectQ.\n\nProjecQ supports several different backends. Of those, the following are useful in the current context:\n\n- projectq.backends.Simulator([gate_fusion, ...])\tSimulator is a compiler engine which simulates a quantum computer using C++-based kernels.\n- projectq.backends.ClassicalSimulator()\t        A simple introspective simulator that only permits classical operations.\n- projectq.backends.IBMBackend([use_hardware, ...])\tThe IBM Backend class, which stores the circuit, transforms it to JSON QASM, and sends the circuit through the IBM API.\n\nSee PluginAPI._capabilities['backend'] for a list of backend options.\n\nFunctions\n---------\n\n.. autosummary::\n   init_plugin\n\nClasses\n-------\n\n.. autosummary::\n   Gate\n   Observable\n   PluginAPI\n\n----\n\"\"\"\nimport logging as log\nimport numpy as np\nfrom numpy.random import (randn,)\nfrom openqml import Device, DeviceError\nfrom openqml import Variable\n\nimport projectq as pq\nimport projectq.setups.ibm #todo only import this if necessary\n\n# import operations\nfrom projectq.ops import (HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, SqrtSwapGate, Rx, Ry, Rz, R)\nfrom .ops import (CNOT, CZ, Toffoli, AllZGate, Rot, Hermitian)\n\nfrom ._version import __version__\n\n\noperator_map = {\n    'PauliX': XGate,\n    'PauliY': YGate,\n    'PauliZ': ZGate,\n    'CNOT': CNOT,\n    'CZ': CZ,\n    'SWAP': SwapGate,\n    'RX': Rx,\n    'RY': Ry,\n    'RZ': Rz,\n    'Rot': Rot,\n    #'PhaseShift': #todo: implement\n    #'QubitStateVector': #todo: implement\n    #'QubitUnitary': #todo: implement\n    #: H, #todo: implement\n    #: S, #todo: implement\n    #: T, #todo: implement\n    #: SqrtX, #todo: implement\n    #: SqrtSwap, #todo: implement\n    #: R, #todo: implement\n    #'AllPauliZ': AllZGate, #todo: implement\n    #'Hermitian': #todo: implement\n}\n\nclass ProjectQDevice(Device):\n    \"\"\"ProjectQ device for OpenQML.\n\n    Args:\n       wires (int): The number of qubits of the device.\n\n    Keyword Args for Simulator backend:\n      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).\n      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).\n\n    Keyword Args for IBMBackend backend:\n      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)\n      num_runs (int): Number of runs to collect statistics. (default is 1024)\n      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).\n      user (string): IBM Quantum Experience user name\n      password (string): IBM Quantum Experience password\n      device (string): Device to use (ibmqx4, or ibmqx5) if use_hardware is set to True. Default is ibmqx4.\n      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).\n    \"\"\"\n    name = 'ProjectQ OpenQML plugin'\n    short_name = 'projectq'\n    api_version = '0.1.0'\n    plugin_version = __version__\n    author = 'Christian Gogolin'\n    _capabilities = {'backend': list([\"Simulator\", \"ClassicalSimulator\", \"IBMBackend\"])}\n\n    def __init__(self, wires, **kwargs):\n        kwargs.setdefault('shots', 0)\n        super().__init__(self.short_name, kwargs['shots'])\n\n        # translate some aguments\n        for k,v in {'log':'verbose'}.items():\n            if k in kwargs:\n                kwargs.setdefault(v, kwargs[k])\n\n        # clean some arguments\n        if 'num_runs' in kwargs:\n            if isinstance(kwargs['num_runs'], int) and kwargs['num_runs']>0:\n                self.n_eval = kwargs['num_runs']\n            else:\n                self.n_eval = 0\n                del(kwargs['num_runs'])\n\n        self.wires = wires\n        self.backend = kwargs['backend']\n        del(kwargs['backend'])\n        self.kwargs = kwargs\n        self.eng = None\n        self.reg = None\n        #self.reset() #the actual initialization is done in reset(), but we don't need to call this manually as Device does it for us during __enter__()\n\n    def reset(self):\n        self.reg = self.eng.allocate_qureg(self.wires)\n\n    def __repr__(self):\n        return super().__repr__() +'Backend: ' +self.backend +'\\n'\n\n    def __str__(self):\n        return super().__str__() +'Backend: ' +self.backend +'\\n'\n\n    # def __del__(self):\n    #     self._deallocate()\n\n    def execute(self):\n        \"\"\" \"\"\"\n        #todo: I hope this function will become superfluous, see https://github.com/XanaduAI/openqml/issues/18\n        self._out = self.execute_queued()\n\n    def execute_queued(self):\n        \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n        #expectation_values = {}\n        for operation in self._queue:\n            if operation.name not in operator_map:\n                raise DeviceError(\"{} not supported by device {}\".format(operation.name, self.short_name))\n\n            par = [x.val if isinstance(x, Variable) else x for x in operation.params]\n            #expectation_values[tuple(operation.wires)] = self.apply(operator_map[operation.name](*p), self.reg, operation.wires)\n            self.apply(operation.name, operation.wires, *par)\n\n        result = self.expectation(self._observe.name, self._observe.wires)\n        self._deallocate()\n        return result\n\n        # if self._observe.wires is not None:\n        #     if isinstance(self._observe.wires, int):\n        #         return expectation_values[tuple([self._observe.wires])]\n        #     else:\n        #         return np.array([expectation_values[tuple([idx])] for idx in self._observe.wires if tuple([idx]) in expectation_values])\n\n    def apply(self, gate_name, wires, *par):\n        if gate_name not in self._gates:\n            raise ValueError('Gate {} not supported on this backend'.format(gate))\n\n        gate = operator_map[gate_name](*par)\n        if isinstance(wires, int):\n            gate | self.reg[wires]\n        else:\n            gate | tuple([self.reg[i] for i in wires])\n\n    def expectation(self, observable, wires):\n        raise NotImplementedError(\"expectation() is not yet implemented for this backend\")\n\n    def shutdown(self):\n        \"\"\"Shutdown.\n\n        \"\"\"\n        pass\n\n    def _deallocate(self):\n        \"\"\"Deallocate all qubits to make ProjectQ happy\n\n        See also: https://github.com/ProjectQ-Framework/ProjectQ/issues/2\n\n        Drawback: This is probably rather resource intensive.\n        \"\"\"\n        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':\n            pq.ops.All(pq.ops.Measure) | self.reg #avoid an unfriendly error message: https://github.com/ProjectQ-Framework/ProjectQ/issues/2\n\n    def _deallocate2(self):\n        \"\"\"Another proposal for how to deallocate all qubits to make ProjectQ happy\n\n        Unsuitable because: Produces a segmentation fault.\n        \"\"\"\n        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':\n             for qubit in self.reg:\n                 self.eng.deallocate_qubit(qubit)\n\n    def _deallocate3(self):\n        \"\"\"Another proposal for how to deallocate all qubits to make ProjectQ happy\n\n        Unsuitable because: Throws an error if the probability for the given collapse is 0.\n        \"\"\"\n        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':\n            self.eng.flush()\n            self.eng.backend.collapse_wavefunction(self.reg, [0 for i in range(len(self.reg))])\n\n\n    # def requires_credentials(self):\n    #     \"\"\"Check whether this plugin requires credentials\n    #     \"\"\"\n    #     if self.backend == 'IBMBackend':\n    #         return True\n    #     else:\n    #         return False\n\n\n    def filter_kwargs_for_backend(self, kwargs):\n        return { key:value for key,value in kwargs.items() if key in self._backend_kwargs }\n\n\nclass ProjectQSimulator(ProjectQDevice):\n    \"\"\"ProjectQ Simulator device for OpenQML.\n\n    Args:\n       wires (int): The number of qubits of the device.\n\n    Keyword Args:\n      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).\n      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).\n    \"\"\"\n\n    short_name = 'projectq.simulator'\n    _gates = set(operator_map.keys())\n    _observables = set([ key for (key,val) in operator_map.items() if val in [XGate, YGate, ZGate, AllZGate, Hermitian] ])\n    _circuits = {}\n    _backend_kwargs = ['gate_fusion', 'rnd_seed']\n\n    def __init__(self, wires, **kwargs):\n        kwargs['backend'] = 'Simulator'\n        super().__init__(wires, **kwargs)\n\n    def reset(self):\n        \"\"\"Resets the engine and backend\n\n        After the reset the Device should be as if it was just constructed.\n        Most importantly the quantum state is reset to its initial value.\n        \"\"\"\n        backend = pq.backends.Simulator(**self.filter_kwargs_for_backend(self.kwargs))\n        self.eng = pq.MainEngine(backend)\n        super().reset()\n\n\n    def expectation(self, observable, wires):\n        self.eng.flush(deallocate_qubits=False)\n        if observable == 'PauliX' or observable == 'PauliY' or observable == 'PauliZ':\n            expectation_value = self.eng.backend.get_expectation_value(pq.ops.QubitOperator(str(observable)[-1]+'0'), self.reg)\n            variance = 1 - expectation_value**2\n        elif observable == 'AllPauliZ':\n            expectation_value = [ self.eng.backend.get_expectation_value(pq.ops.QubitOperator(\"Z\"+'0'), [qubit]) for qubit in self.reg]\n            variance = [1 - e**2 for e in expectation_value]\n        else:\n            raise NotImplementedError(\"Estimation of expectation values not yet implemented for the observable {} in backend {}.\".format(observable, self.backend))\n\n        return expectation_value#, variance\n\n\nclass ProjectQClassicalSimulator(ProjectQDevice):\n    \"\"\"ProjectQ ClassicalSimulator device for OpenQML.\n\n    Args:\n       wires (int): The number of qubits of the device.\n    \"\"\"\n\n    short_name = 'projectq.classicalsimulator'\n    _gates = set([ key for (key,val) in operator_map.items() if val in [XGate, CNOT] ])\n    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])\n    _circuits = {}\n    _backend_kwargs = []\n\n    def __init__(self, wires, **kwargs):\n        kwargs['backend'] = 'ClassicalSimulator'\n        super().__init__(wires, **kwargs)\n\n    def reset(self):\n        \"\"\"Resets the engine and backend\n\n        After the reset the Device should be as if it was just constructed.\n        Most importantly the quantum state is reset to its initial value.\n        \"\"\"\n        backend = pq.backends.ClassicalSimulator(**self.filter_kwargs_for_backend(self.kwargs))\n        self.eng = pq.MainEngine(backend)\n        super().reset()\n\nclass ProjectQIBMBackend(ProjectQDevice):\n    \"\"\"ProjectQ IBMBackend device for OpenQML.\n\n    Args:\n       wires (int): The number of qubits of the device.\n\n    Keyword Args:\n      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)\n      num_runs (int): Number of runs to collect statistics. (default is 1024)\n      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).\n      user (string): IBM Quantum Experience user name\n      password (string): IBM Quantum Experience password\n      device (string): Device to use (ibmqx4, or ibmqx5) if use_hardware is set to True. Default is ibmqx4.\n      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).\n    \"\"\"\n\n    short_name = 'projectq.ibmbackend'\n    _gates = set([ key for (key,val) in operator_map.items() if val in [HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, Rx, Ry, Rz, R, CNOT, CZ] ])\n    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])\n    _circuits = {}\n    _backend_kwargs = ['use_hardware', 'num_runs', 'verbose', 'user', 'password', 'device', 'retrieve_execution']\n\n    def __init__(self, wires, **kwargs):\n        # check that necessary arguments are given\n        if 'user' not in kwargs:\n            raise ValueError('An IBM Quantum Experience user name specified via the \"user\" keyword argument is required')\n        if 'password' not in kwargs:\n            raise ValueError('An IBM Quantum Experience password specified via the \"password\" keyword argument is required')\n\n        kwargs['backend'] = 'IBMBackend'\n        #kwargs['verbose'] = True #todo: remove when done testing\n        #kwargs['log'] = True #todo: remove when done testing\n        #kwargs['use_hardware'] = False #todo: remove when done testing\n        #kwargs['num_runs'] = 3 #todo: remove when done testing\n        super().__init__(wires, **kwargs)\n\n    def reset(self):\n        \"\"\"Resets the engine and backend\n\n        After the reset the Device should be as if it was just constructed.\n        Most importantly the quantum state is reset to its initial value.\n        \"\"\"\n        backend = pq.backends.IBMBackend(**self.filter_kwargs_for_backend(self.kwargs))\n        self.eng = pq.MainEngine(backend, engine_list=pq.setups.ibm.get_engine_list())\n        super().reset()\n\n    def expectation(self, observable, wires):\n        pq.ops.R(0) | self.reg[0]# todo:remove this once https://github.com/ProjectQ-Framework/ProjectQ/issues/259 is resolved\n\n        pq.ops.All(pq.ops.Measure) | self.reg\n        self.eng.flush()\n\n        if observable == 'PauliZ':\n            probabilities = self.eng.backend.get_probabilities([self.reg[wires]])\n            #print(\"IBM probabilities=\"+str(probabilities))\n            if '1' in probabilities:\n                expectation_value = 2*probabilities['1']-1\n            else:\n                expectation_value = -(2*probabilities['0']-1)\n            variance = 1 - expectation_value**2\n        elif observable == 'AllPauliZ':\n            probabilities = self.eng.backend.get_probabilities(self.reg)\n            #print(\"IBM all probabilities=\"+str(probabilities))\n            expectation_value = [ ((2*sum(p for (state,p) in probabilities.items() if state[i] == '1')-1)-(2*sum(p for (state,p) in probabilities.items() if state[i] == '0')-1)) for i in range(len(self.reg)) ]\n            variance = [1 - e**2 for e in expectation_value]\n        else:\n            raise NotImplementedError(\"Estimation of expectation values not yet implemented for the observable {} in backend {}.\".format(observable, self.backend))\n\n        return expectation_value#, variance\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/XanaduAI/pennylane/blob/a04aad4c762a5abf314600f823c0d6c5b2a0af58",
        "file_path": "/openqml-sf/openqml_sf/fock.py",
        "source": "# Copyright 2018 Xanadu Quantum Technologies Inc.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"This module contains the device class and context manager\"\"\"\nimport numpy as np\nfrom openqml import Device, DeviceError\nfrom openqml import Variable\n\nimport strawberryfields as sf\n\n#import state preparations\nfrom strawberryfields.ops import (Catstate, Coherent, DensityMatrix, DisplacedSqueezed,\n                                  Fock, Ket, Squeezed, Thermal, Gaussian)\n# import decompositions\nfrom strawberryfields.ops import (GaussianTransform, Interferometer)\n# import gates\nfrom strawberryfields.ops import (BSgate, CKgate, CXgate, CZgate, Dgate, Fouriergate,\n                                  Kgate, Pgate, Rgate, S2gate, Sgate, Vgate, Xgate, Zgate)\n# import measurements\nfrom strawberryfields.ops import (MeasureFock, MeasureHeterodyne, MeasureHomodyne)\n\n\nfrom ._version import __version__\n\n\noperator_map = {\n    'CatState:': Catstate,\n    'CoherentState': Coherent,\n    'FockDensityMatrix': DensityMatrix,\n    'DisplacedSqueezed': DisplacedSqueezed,\n    'FockState': Fock,\n    'FockStateVector': Ket,\n    'SqueezedState': Squeezed,\n    'ThermalState': Thermal,\n    'GaussianState': Gaussian,\n    'Beamsplitter': BSgate,\n    'CrossKerr': CKgate,\n    'ControlledAddition': CXgate,\n    'ControlledPhase': CZgate,\n    'Displacement': Dgate,\n    'Kerr': Kgate,\n    'QuadraticPhase': Pgate,\n    'Rotation': Rgate,\n    'TwoModeSqueezing': S2gate,\n    'Squeezing': Sgate,\n    'CubicPhase': Vgate,\n    # 'XDisplacement': Xgate,\n    # 'PDisplacement': Zgate,\n    # 'MeasureFock': MeasureFock,\n    # 'MeasureHomodyne': MeasureHomodyne\n}\n\n\nclass StrawberryFieldsFock(Device):\n    \"\"\"StrawberryFields Fock device for OpenQML.\n\n    wires (int): the number of modes to initialize the device in.\n    cutoff (int): the Fock space truncation. Must be specified before\n        applying a qfunc.\n    hbar (float): the convention chosen in the canonical commutation\n        relation [x, p] = i hbar. The default value is hbar=2.\n    \"\"\"\n    name = 'Strawberry Fields OpenQML plugin'\n    short_name = 'strawberryfields.fock'\n    api_version = '0.1.0'\n    version = __version__\n    author = 'Josh Izaac'\n    _gates = set(operator_map.keys())\n    _observables = {'Fock', 'X', 'P', 'Homodyne'}\n    _circuits = {}\n\n    def __init__(self, wires, *, shots=0, cutoff=None, hbar=2):\n        self.wires = wires\n        self.cutoff = cutoff\n        self.hbar = hbar\n        self.eng = None\n        self.state = None\n        super().__init__(self.short_name, shots)\n\n    def execute(self):\n        \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n        if self.eng:\n            self.eng.reset()\n            self.reset()\n\n        self.eng, q = sf.Engine(self.wires, hbar=self.hbar)\n\n        with self.eng:\n            for operation in self._queue:\n                if operation.name not in operator_map:\n                    raise DeviceError(\"{} not supported by device {}\".format(operation.name, self.short_name))\n\n                p = [x.val if isinstance(x, Variable) else x for x in operation.params]\n                op = operator_map[operation.name](*p)\n                if isinstance(operation.wires, int):\n                    op | q[operation.wires]\n                else:\n                    op | [q[i] for i in operation.wires]\n\n        self.state = self.eng.run('fock', cutoff_dim=self.cutoff)\n\n        # calculate expectation value\n        reg = self._observe.wires\n        if self._observe.name == 'Fock':\n            ex = self.state.mean_photon(reg)\n            var = 0\n        elif self._observe.name == 'X':\n            ex, var = self.state.quad_expectation(reg, 0)\n        elif self._observe.name == 'P':\n            ex, var = self.state.quad_expectation(reg, np.pi/2)\n        elif self._observe.name == 'Homodyne':\n            ex, var = self.state.quad_expectation(reg, *self.observe.params)\n\n        if self.shots != 0:\n            # estimate the expectation value\n            # use central limit theorem, sample normal distribution once, only ok\n            # if shots is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)\n            ex = np.random.normal(ex, np.sqrt(var / self.shots))\n\n        self._out = ex\n\n    def reset(self):\n        \"\"\"Reset the device\"\"\"\n        if self.eng is not None:\n            self.eng = None\n            self.state = None\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/XanaduAI/pennylane/blob/a04aad4c762a5abf314600f823c0d6c5b2a0af58",
        "file_path": "/openqml-sf/openqml_sf/gaussian.py",
        "source": "# Copyright 2018 Xanadu Quantum Technologies Inc.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"This module contains the device class and context manager\"\"\"\nimport numpy as np\nfrom openqml import Device, DeviceError\nfrom openqml import Variable\n\nimport strawberryfields as sf\n\n#import state preparations\nfrom strawberryfields.ops import (Catstate, Coherent, DensityMatrix, DisplacedSqueezed,\n                                  Fock, Ket, Squeezed, Thermal, Gaussian)\n# import decompositions\nfrom strawberryfields.ops import (GaussianTransform, Interferometer)\n# import gates\nfrom strawberryfields.ops import (BSgate, CKgate, CXgate, CZgate, Dgate, Fouriergate,\n                                  Kgate, Pgate, Rgate, S2gate, Sgate, Vgate, Xgate, Zgate)\n# import measurements\nfrom strawberryfields.ops import (MeasureFock, MeasureHeterodyne, MeasureHomodyne)\n\n\nfrom ._version import __version__\n\n\noperator_map = {\n    'CoherentState': Coherent,\n    'DisplacedSqueezed': DisplacedSqueezed,\n    'SqueezedState': Squeezed,\n    'ThermalState': Thermal,\n    'GaussianState': Gaussian,\n    'Beamsplitter': BSgate,\n    'ControlledAddition': CXgate,\n    'ControlledPhase': CZgate,\n    'Displacement': Dgate,\n    'QuadraticPhase': Pgate,\n    'Rotation': Rgate,\n    'TwoModeSqueezing': S2gate,\n    'Squeeze': Sgate,\n    # 'XDisplacement': Xgate,\n    # 'PDisplacement': Zgate,\n    # 'MeasureHomodyne': MeasureHomodyne,\n    # 'MeasureHeterodyne': MeasureHeterodyne\n}\n\n\n\nclass StrawberryFieldsGaussian(Device):\n    \"\"\"StrawberryFields Gaussian device for OpenQML.\n\n    wires (int): the number of modes to initialize the device in.\n    hbar (float): the convention chosen in the canonical commutation\n        relation [x, p] = i hbar. The default value is hbar=2.\n    \"\"\"\n    name = 'Strawberry Fields OpenQML plugin'\n    short_name = 'strawberryfields.fock'\n    api_version = '0.1.0'\n    version = __version__\n    author = 'Josh Izaac'\n    _gates = set(operator_map.keys())\n    _observables = {'Fock', 'X', 'P', 'Homodyne', 'Heterodyne'}\n    _circuits = {}\n\n    def __init__(self, wires, *, shots=0, hbar=2):\n        self.wires = wires\n        self.hbar = hbar\n        self.eng = None\n        self.state = None\n        super().__init__(self.short_name, shots)\n\n    def execute(self):\n        \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n        if self.eng:\n            self.eng.reset()\n            self.reset()\n\n        self.eng, q = sf.Engine(self.wires, hbar=self.hbar)\n\n        with self.eng:\n            for operation in self._queue:\n                if operation.name not in operator_map:\n                    raise DeviceError(\"{} not supported by device {}\".format(operation.name, self.short_name))\n\n                p = [x.val if isinstance(x, Variable) else x for x in operation.params]\n                op = operator_map[operation.name](*p)\n                if isinstance(operation.wires, int):\n                    op | q[operation.wires]\n                else:\n                    op | [q[i] for i in operation.wires]\n\n        self.state = self.eng.run('gaussian')\n\n        # calculate expectation value\n        reg = self._observe.wires\n        if self._observe.name == 'Fock':\n            ex = self.state.mean_photon(reg)\n            var = 0\n        elif self._observe.name == 'X':\n            ex, var = self.state.quad_expectation(reg, 0)\n        elif self._observe.name == 'P':\n            ex, var = self.state.quad_expectation(reg, np.pi/2)\n        elif self._observe.name == 'Homodyne':\n            ex, var = self.state.quad_expectation(reg, *self._observe.params)\n        elif self._observe.name == 'Displacement':\n            ex = self.state.displacement(modes=reg)\n\n        if self.shots != 0:\n            # estimate the expectation value\n            # use central limit theorem, sample normal distribution once, only ok\n            # if shots is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)\n            ex = np.random.normal(ex, np.sqrt(var / self.shots))\n\n        self._out = ex\n\n    def reset(self):\n        \"\"\"Reset the device\"\"\"\n        if self.eng is not None:\n            self.eng = None\n            self.state = None\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/XanaduAI/pennylane/blob/a04aad4c762a5abf314600f823c0d6c5b2a0af58",
        "file_path": "/openqml/device.py",
        "source": "# Copyright 2018 Xanadu Quantum Technologies Inc.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"This module contains the device class and context manager\"\"\"\n\nimport abc\nimport logging\n\n\nlogging.getLogger()\n\n\nclass MethodFactory(type):\n    \"\"\"Metaclass that allows derived classes to dynamically instantiate\n    new objects based on undefined methods. The dynamic methods pass their arguments\n    directly to __init__ of the inheriting class.\"\"\"\n    def __getattr__(cls, name):\n        \"\"\"Get the attribute call via name\"\"\"\n        def new_object(*args, **kwargs):\n            \"\"\"Return a new object of the same class, passing the attribute name\n            as the first parameter, along with any additional parameters.\"\"\"\n            return cls(name, *args, **kwargs)\n        return new_object\n\n\nclass DeviceError(Exception):\n    \"\"\"Exception raised by a :class:`Device` when it encounters an illegal\n    operation in the quantum circuit.\n    \"\"\"\n    pass\n\n\nclass Device(abc.ABC):\n    \"\"\"Abstract base class for devices.\"\"\"\n    _current_context = None\n    name = ''          #: str: official device plugin name\n    short_name = ''    #: str: name used to load device plugin\n    api_version = ''   #: str: version of OpenQML for which the plugin was made\n    version = ''       #: str: version of the device plugin itself\n    author = ''        #: str: plugin author(s)\n    _capabilities = {} #: dict[str->*]: plugin capabilities\n    _gates = {}        #: dict[str->GateSpec]: specifications for supported gates\n    _observables = {}  #: dict[str->GateSpec]: specifications for supported observables\n    _circuits = {}     #: dict[str->Circuit]: circuit templates associated with this API class\n\n    def __init__(self, name, shots):\n        self.name = name # the name of the device\n\n        # number of circuit evaluations used to estimate\n        # expectation values of observables. 0 means the exact ev is returned.\n        self.shots = shots\n\n        self._out = None  # this attribute stores the expectation output\n        self._queue = []  # this list stores the operations to be queued to the device\n        self._observe = None # the measurement operation to be performed\n\n    def __repr__(self):\n        \"\"\"String representation.\"\"\"\n        return self.__module__ +'.' +self.__class__.__name__ +'\\nInstance: ' +self.name\n\n    def __str__(self):\n        \"\"\"Verbose string representation.\"\"\"\n        return self.__repr__() +'\\nName: ' +self.name +'\\nAPI version: ' +self.api_version\\\n            +'\\nPlugin version: ' +self.version +'\\nAuthor: ' +self.author +'\\n'\n\n    def __enter__(self):\n        if Device._current_context is None:\n            Device._current_context = self\n            self.reset()\n        else:\n            raise DeviceError('Only one device can be active at a time.')\n        return self\n\n    def __exit__(self, exc_type, exc_value, tb):\n        if self._observe is None:\n            raise DeviceError('A qfunc must always conclude with a classical expectation value.')\n        Device._current_context = None\n        self.execute()\n\n    @property\n    def gates(self):\n        \"\"\"Get the supported gate set.\n\n        Returns:\n          dict[str->GateSpec]:\n        \"\"\"\n        return self._gates\n\n    @property\n    def observables(self):\n        \"\"\"Get the supported observables.\n\n        Returns:\n          dict[str->GateSpec]:\n        \"\"\"\n        return self._observables\n\n    @property\n    def templates(self):\n        \"\"\"Get the predefined circuit templates.\n\n        .. todo:: rename to circuits?\n\n        Returns:\n          dict[str->Circuit]: circuit templates\n        \"\"\"\n        return self._circuits\n\n    @property\n    def result(self):\n        \"\"\"Get the circuit result.\n\n        Returns:\n            float or int\n        \"\"\"\n        return self._out\n\n    @classmethod\n    def capabilities(cls):\n        \"\"\"Get the other capabilities of the plugin.\n\n        Measurements, batching etc.\n\n        Returns:\n          dict[str->*]: results\n        \"\"\"\n        return cls._capabilities\n\n    @abc.abstractmethod\n    def execute(self):\n        \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def reset(self):\n        \"\"\"Reset the backend state.\n\n        After the reset the backend should be as if it was just constructed.\n        Most importantly the quantum state is reset to its initial value.\n        \"\"\"\n        raise NotImplementedError\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/XanaduAI/pennylane/blob/a04aad4c762a5abf314600f823c0d6c5b2a0af58",
        "file_path": "/openqml/plugins/default.py",
        "source": "# Copyright 2018 Xanadu Quantum Technologies Inc.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"This module contains the device class and context manager\"\"\"\nimport numpy as np\nfrom scipy.linalg import expm, eigh\n\nimport openqml as qm\nfrom openqml import Device, DeviceError, qfunc, QNode, Variable, __version__\n\n\n# tolerance for numerical errors\ntolerance = 1e-10\n\n\n#========================================================\n#  utilities\n#========================================================\n\ndef spectral_decomposition_qubit(A):\n    r\"\"\"Spectral decomposition of a 2*2 Hermitian matrix.\n\n    Args:\n      A (array): 2*2 Hermitian matrix\n\n    Returns:\n      (vector[float], list[array[complex]]): (a, P): eigenvalues and hermitian projectors\n        such that :math:`A = \\sum_k a_k P_k`.\n    \"\"\"\n    d, v = eigh(A)\n    P = []\n    for k in range(2):\n        temp = v[:, k]\n        P.append(np.outer(temp.conj(), temp))\n    return d, P\n\n\n#========================================================\n#  fixed gates\n#========================================================\n\nI = np.eye(2)\n# Pauli matrices\nX = np.array([[0, 1], [1, 0]])\nY = np.array([[0, -1j], [1j, 0]])\nZ = np.array([[1, 0], [0, -1]])\nCNOT = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]])\nSWAP = np.array([[1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]])\n\n\n#========================================================\n#  parametrized gates\n#========================================================\n\n\ndef frx(theta):\n    r\"\"\"One-qubit rotation about the x axis.\n\n    Args:\n        theta (float): rotation angle\n    Returns:\n        array: unitary 2x2 rotation matrix :math:`e^{-i \\sigma_x \\theta/2}`\n    \"\"\"\n    return expm(-1j * theta/2 * X)\n\n\ndef fry(theta):\n    r\"\"\"One-qubit rotation about the y axis.\n\n    Args:\n        theta (float): rotation angle\n    Returns:\n        array: unitary 2x2 rotation matrix :math:`e^{-i \\sigma_y \\theta/2}`\n    \"\"\"\n    return expm(-1j * theta/2 * Y)\n\n\ndef frz(theta):\n    r\"\"\"One-qubit rotation about the z axis.\n\n    Args:\n        theta (float): rotation angle\n    Returns:\n        array: unitary 2x2 rotation matrix :math:`e^{-i \\sigma_z \\theta/2}`\n    \"\"\"\n    return expm(-1j * theta/2 * Z)\n\n\ndef fr3(a, b, c):\n    r\"\"\"Arbitrary one-qubit rotation using three Euler angles.\n\n    Args:\n        a,b,c (float): rotation angles\n    Returns:\n        array: unitary 2x2 rotation matrix rz(c) @ ry(b) @ rz(a)\n    \"\"\"\n    return frz(c) @ (fry(b) @ frz(a))\n\n\n#========================================================\n#  Arbitrary states and operators\n#========================================================\n\ndef ket(*args):\n    r\"\"\"Input validation for an arbitary state vector.\n\n    Args:\n        args (array): NumPy array.\n\n    Returns:\n        array: normalised array.\n    \"\"\"\n    state = np.asarray(args)\n    return state/np.linalg.norm(state)\n\n\ndef unitary(*args):\n    r\"\"\"Input validation for an arbitary unitary operation.\n\n    Args:\n        args (array): square unitary matrix.\n\n    Returns:\n        array: square unitary matrix.\n    \"\"\"\n    U = np.asarray(args[0])\n\n    if U.shape[0] != U.shape[1]:\n        raise ValueError(\"Operator must be a square matrix.\")\n\n    if not np.allclose(U @ U.conj().T, np.identity(U.shape[0]), atol=tolerance):\n        raise ValueError(\"Operator must be unitary.\")\n\n    return U\n\n\ndef hermitian(*args):\n    r\"\"\"Input validation for an arbitary Hermitian observable.\n\n    Args:\n        args (array): square hermitian matrix.\n\n    Returns:\n        array: square hermitian matrix.\n    \"\"\"\n    A = np.asarray(args[0])\n\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"Observable must be a square matrix.\")\n\n    if not np.allclose(A, A.conj().T, atol=tolerance):\n        raise ValueError(\"Observable must be Hermitian.\")\n    return A\n\n\n#========================================================\n#  operator map\n#========================================================\n\n\noperator_map = {\n    'QubitStateVector': ket,\n    'QubitUnitary': unitary,\n    'Hermitian': hermitian,\n    'Identity': I,\n    'PauliX': X,\n    'PauliY': Y,\n    'PauliZ': Z,\n    'CNOT': CNOT,\n    'SWAP': SWAP,\n    'RX': frx,\n    'RY': fry,\n    'RZ': frz,\n    'Rot': fr3\n}\n\n\n#========================================================\n#  device\n#========================================================\n\n\nclass DefaultQubit(Device):\n    \"\"\"Default qubit device for OpenQML.\n\n    wires (int): the number of modes to initialize the device in.\n    cutoff (int): the Fock space truncation. Must be specified before\n        applying a qfunc.\n    hbar (float): the convention chosen in the canonical commutation\n        relation [x, p] = i hbar. The default value is hbar=2.\n    \"\"\"\n    name = 'Default OpenQML plugin'\n    short_name = 'default.qubit'\n    api_version = '0.1.0'\n    version = '0.1.0'\n    author = 'Xanadu Inc.'\n    _gates = set(operator_map.keys())\n    _observables = {}\n    _circuits = {}\n\n    def __init__(self, wires, *, shots=0):\n        self.wires = wires\n        self.eng = None\n        self._state = None\n        super().__init__(self.short_name, shots)\n\n    def execute(self):\n        \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n        if self._state is None:\n            # init the state vector to |00..0>\n            self._state = np.zeros(2**self.wires, dtype=complex)\n            self._state[0] = 1\n            self._out = np.full(self.wires, np.nan)\n\n        # apply unitary operations U\n        for operation in self._queue:\n            if operation.name == 'QubitStateVector':\n                state = np.asarray(operation.params[0])\n                if state.ndim == 1 and state.shape[0] == 2**self.wires:\n                    self._state = state\n                else:\n                    raise ValueError('State vector must be of length 2**wires.')\n                continue\n\n            U = DefaultQubit._get_operator_matrix(operation)\n\n            if len(operation.wires) == 1:\n                U = self.expand_one(U, operation.wires)\n            elif len(operation.wires) == 2:\n                U = self.expand_two(U, operation.wires)\n            else:\n                raise ValueError('This plugin supports only one- and two-qubit gates.')\n            self._state = U @ self._state\n\n        # measurement/expectation value <psi|A|psi>\n        A = DefaultQubit._get_operator_matrix(self._observe)\n        if self.shots == 0:\n            # exact expectation value\n            ev = self.ev(A, [self._observe.wires])\n        else:\n            # estimate the ev\n            if 0:\n                # use central limit theorem, sample normal distribution once, only ok if n_eval is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)\n                ev = self.ev(A, self._observe.wires)\n                var = self.ev(A**2, self._observe.wires) - ev**2  # variance\n                ev = np.random.normal(ev, np.sqrt(var / self.shots))\n            else:\n                # sample Bernoulli distribution n_eval times / binomial distribution once\n                a, P = spectral_decomposition_qubit(A)\n                p0 = self.ev(P[0], self._observe.wires)  # probability of measuring a[0]\n                n0 = np.random.binomial(self.shots, p0)\n                ev = (n0*a[0] +(self.shots-n0)*a[1]) / self.shots\n\n        self._out = ev  # store the result\n\n    @classmethod\n    def _get_operator_matrix(cls, A):\n        \"\"\"Get the operator matrix for a given operation.\n\n        Args:\n            A (openqml.Operation or openqml.Expectation): operation/observable.\n\n        Returns:\n            array: matrix representation.\n        \"\"\"\n        if A.name not in operator_map:\n            raise DeviceError(\"{} not supported by device {}\".format(A.name, cls.short_name))\n\n        if not callable(operator_map[A.name]):\n            return operator_map[A.name]\n\n        # unpack variables\n        p = [x.val if isinstance(x, Variable) else x for x in A.params]\n        return operator_map[A.name](*p)\n\n    def ev(self, A, wires):\n        r\"\"\"Expectation value of a one-qubit observable in the current state.\n\n        Args:\n          A (array): 2*2 hermitian matrix corresponding to the observable\n          wires (Sequence[int]): target subsystem\n\n        Returns:\n          float: expectation value :math:`\\expect{A} = \\bra{\\psi}A\\ket{\\psi}`\n        \"\"\"\n        if A.shape != (2, 2):\n            raise ValueError('2x2 matrix required.')\n\n        A = self.expand_one(A, wires)\n        expectation = np.vdot(self._state, A @ self._state)\n\n        if np.abs(expectation.imag) > tolerance:\n            log.warning('Nonvanishing imaginary part {} in expectation value.'.format(expectation.imag))\n        return expectation.real\n\n    def reset(self):\n        \"\"\"Reset the device\"\"\"\n        self._state  = None  #: array: state vector\n        self._out = None  #: array: measurement results\n\n    def expand_one(self, U, wires):\n        \"\"\"Expand a one-qubit operator into a full system operator.\n\n        Args:\n          U (array): 2*2 matrix\n          wires (Sequence[int]): target subsystem\n\n        Returns:\n          array: 2^n*2^n matrix\n        \"\"\"\n        if U.shape != (2, 2):\n            raise ValueError('2x2 matrix required.')\n        if len(wires) != 1:\n            raise ValueError('One target subsystem required.')\n        wires = wires[0]\n        before = 2**wires\n        after  = 2**(self.wires-wires-1)\n        U = np.kron(np.kron(np.eye(before), U), np.eye(after))\n        return U\n\n    def expand_two(self, U, wires):\n        \"\"\"Expand a two-qubit operator into a full system operator.\n\n        Args:\n          U (array): 4x4 matrix\n          wires (Sequence[int]): two target subsystems (order matters!)\n\n        Returns:\n          array: 2^n*2^n matrix\n        \"\"\"\n        if U.shape != (4, 4):\n            raise ValueError('4x4 matrix required.')\n        if len(wires) != 2:\n            raise ValueError('Two target subsystems required.')\n        wires = np.asarray(wires)\n        if np.any(wires < 0) or np.any(wires >= self.wires) or wires[0] == wires[1]:\n            raise ValueError('Bad target subsystems.')\n\n        a = np.min(wires)\n        b = np.max(wires)\n        n_between = b-a-1  # number of qubits between a and b\n        # dimensions of the untouched subsystems\n        before  = 2**a\n        after   = 2**(self.wires-b-1)\n        between = 2**n_between\n\n        U = np.kron(U, np.eye(between))\n        # how U should be reordered\n        if wires[0] < wires[1]:\n            p = [0, 2, 1]\n        else:\n            p = [1, 2, 0]\n        dim = [2, 2, between]\n        p = np.array(p)\n        perm = np.r_[p, p+3]\n        # reshape U into another array which has one index per subsystem, permute dimensions, back into original-shape array\n        temp = np.prod(dim)\n        U = U.reshape(dim * 2).transpose(perm).reshape([temp, temp])\n        U = np.kron(np.kron(np.eye(before), U), np.eye(after))\n        return U\n\n\n#====================\n# Default circuits\n#====================\n\n\ndev = DefaultQubit(wires=2)\n\ndef node(x, y, z):\n    qm.RX(x, [0])\n    qm.CNOT([0, 1])\n    qm.RY(-1.6, [0])\n    qm.RY(y, [1])\n    qm.CNOT([1, 0])\n    qm.RX(z, [0])\n    qm.CNOT([0, 1])\n    qm.expectation.Hermitian(np.array([[0, 1], [1, 0]]), 0)\n\ncircuits = {'demo_ev': QNode(node, dev)}\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/XanaduAI/pennylane-pq/blob/79e179360fd370982a1bfc174d7fdbaff752c45f",
        "file_path": "/openqml_pq/projectq.py",
        "source": "# Copyright 2018 Xanadu Quantum Technologies Inc.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nr\"\"\"\nProjectQ plugin\n========================\n\n**Module name:** :mod:`openqml.plugins.projectq`\n\n.. currentmodule:: openqml.plugins.projectq\n\nThis plugin provides the interface between OpenQML and ProjecQ.\nIt enables OpenQML to optimize quantum circuits simulable with ProjectQ.\n\nProjecQ supports several different backends. Of those, the following are useful in the current context:\n\n- projectq.backends.Simulator([gate_fusion, ...])\tSimulator is a compiler engine which simulates a quantum computer using C++-based kernels.\n- projectq.backends.ClassicalSimulator()\t        A simple introspective simulator that only permits classical operations.\n- projectq.backends.IBMBackend([use_hardware, ...])\tThe IBM Backend class, which stores the circuit, transforms it to JSON QASM, and sends the circuit through the IBM API.\n\nSee PluginAPI._capabilities['backend'] for a list of backend options.\n\nFunctions\n---------\n\n.. autosummary::\n   init_plugin\n\nClasses\n-------\n\n.. autosummary::\n   Gate\n   Observable\n   PluginAPI\n\n----\n\"\"\"\nimport logging as log\nimport numpy as np\nfrom numpy.random import (randn,)\nfrom openqml import Device, DeviceError\nfrom openqml import Variable\n\nimport projectq as pq\nimport projectq.setups.ibm #todo only import this if necessary\n\n# import operations\nfrom projectq.ops import (HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, SqrtSwapGate, Rx, Ry, Rz, R)\nfrom .ops import (CNOT, CZ, Toffoli, AllZGate, Rot, Hermitian)\n\nfrom ._version import __version__\n\n\noperator_map = {\n    'PauliX': XGate,\n    'PauliY': YGate,\n    'PauliZ': ZGate,\n    'CNOT': CNOT,\n    'CZ': CZ,\n    'SWAP': SwapGate,\n    'RX': Rx,\n    'RY': Ry,\n    'RZ': Rz,\n    'Rot': Rot,\n    #'PhaseShift': #todo: implement\n    #'QubitStateVector': #todo: implement\n    #'QubitUnitary': #todo: implement\n    #: H, #todo: implement\n    #: S, #todo: implement\n    #: T, #todo: implement\n    #: SqrtX, #todo: implement\n    #: SqrtSwap, #todo: implement\n    #: R, #todo: implement\n    #'AllPauliZ': AllZGate, #todo: implement\n    #'Hermitian': #todo: implement\n}\n\nclass ProjectQDevice(Device):\n    \"\"\"ProjectQ device for OpenQML.\n\n    Args:\n       wires (int): The number of qubits of the device.\n\n    Keyword Args for Simulator backend:\n      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).\n      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).\n\n    Keyword Args for IBMBackend backend:\n      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)\n      num_runs (int): Number of runs to collect statistics. (default is 1024)\n      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).\n      user (string): IBM Quantum Experience user name\n      password (string): IBM Quantum Experience password\n      device (string): Device to use (ibmqx4, or ibmqx5) if use_hardware is set to True. Default is ibmqx4.\n      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).\n    \"\"\"\n    name = 'ProjectQ OpenQML plugin'\n    short_name = 'projectq'\n    api_version = '0.1.0'\n    plugin_version = __version__\n    author = 'Christian Gogolin'\n    _capabilities = {'backend': list([\"Simulator\", \"ClassicalSimulator\", \"IBMBackend\"])}\n\n    def __init__(self, wires, **kwargs):\n        kwargs.setdefault('shots', 0)\n        super().__init__(self.short_name, kwargs['shots'])\n\n        # translate some aguments\n        for k,v in {'log':'verbose'}.items():\n            if k in kwargs:\n                kwargs.setdefault(v, kwargs[k])\n\n        # clean some arguments\n        if 'num_runs' in kwargs:\n            if isinstance(kwargs['num_runs'], int) and kwargs['num_runs']>0:\n                self.n_eval = kwargs['num_runs']\n            else:\n                self.n_eval = 0\n                del(kwargs['num_runs'])\n\n        self.wires = wires\n        self.backend = kwargs['backend']\n        del(kwargs['backend'])\n        self.kwargs = kwargs\n        self.eng = None\n        self.reg = None\n        #self.reset() #the actual initialization is done in reset(), but we don't need to call this manually as Device does it for us during __enter__()\n\n    def reset(self):\n        self.reg = self.eng.allocate_qureg(self.wires)\n\n    def __repr__(self):\n        return super().__repr__() +'Backend: ' +self.backend +'\\n'\n\n    def __str__(self):\n        return super().__str__() +'Backend: ' +self.backend +'\\n'\n\n    # def __del__(self):\n    #     self._deallocate()\n\n    def execute(self):\n        \"\"\" \"\"\"\n        #todo: I hope this function will become superfluous, see https://github.com/XanaduAI/openqml/issues/18\n        self._out = self.execute_queued()\n\n    def execute_queued(self):\n        \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n        #expectation_values = {}\n        for operation in self._queue:\n            if operation.name not in operator_map:\n                raise DeviceError(\"{} not supported by device {}\".format(operation.name, self.short_name))\n\n            par = [x.val if isinstance(x, Variable) else x for x in operation.params]\n            #expectation_values[tuple(operation.wires)] = self.apply(operator_map[operation.name](*p), self.reg, operation.wires)\n            self.apply(operation.name, operation.wires, *par)\n\n        result = self.expectation(self._observe.name, self._observe.wires)\n        self._deallocate()\n        return result\n\n        # if self._observe.wires is not None:\n        #     if isinstance(self._observe.wires, int):\n        #         return expectation_values[tuple([self._observe.wires])]\n        #     else:\n        #         return np.array([expectation_values[tuple([idx])] for idx in self._observe.wires if tuple([idx]) in expectation_values])\n\n    def apply(self, gate_name, wires, *par):\n        if gate_name not in self._gates:\n            raise ValueError('Gate {} not supported on this backend'.format(gate))\n\n        gate = operator_map[gate_name](*par)\n        if isinstance(wires, int):\n            gate | self.reg[wires]\n        else:\n            gate | tuple([self.reg[i] for i in wires])\n\n    def expectation(self, observable, wires):\n        raise NotImplementedError(\"expectation() is not yet implemented for this backend\")\n\n    def shutdown(self):\n        \"\"\"Shutdown.\n\n        \"\"\"\n        pass\n\n    def _deallocate(self):\n        \"\"\"Deallocate all qubits to make ProjectQ happy\n\n        See also: https://github.com/ProjectQ-Framework/ProjectQ/issues/2\n\n        Drawback: This is probably rather resource intensive.\n        \"\"\"\n        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':\n            pq.ops.All(pq.ops.Measure) | self.reg #avoid an unfriendly error message: https://github.com/ProjectQ-Framework/ProjectQ/issues/2\n\n    def _deallocate2(self):\n        \"\"\"Another proposal for how to deallocate all qubits to make ProjectQ happy\n\n        Unsuitable because: Produces a segmentation fault.\n        \"\"\"\n        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':\n             for qubit in self.reg:\n                 self.eng.deallocate_qubit(qubit)\n\n    def _deallocate3(self):\n        \"\"\"Another proposal for how to deallocate all qubits to make ProjectQ happy\n\n        Unsuitable because: Throws an error if the probability for the given collapse is 0.\n        \"\"\"\n        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':\n            self.eng.flush()\n            self.eng.backend.collapse_wavefunction(self.reg, [0 for i in range(len(self.reg))])\n\n\n    # def requires_credentials(self):\n    #     \"\"\"Check whether this plugin requires credentials\n    #     \"\"\"\n    #     if self.backend == 'IBMBackend':\n    #         return True\n    #     else:\n    #         return False\n\n\n    def filter_kwargs_for_backend(self, kwargs):\n        return { key:value for key,value in kwargs.items() if key in self._backend_kwargs }\n\n\nclass ProjectQSimulator(ProjectQDevice):\n    \"\"\"ProjectQ Simulator device for OpenQML.\n\n    Args:\n       wires (int): The number of qubits of the device.\n\n    Keyword Args:\n      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).\n      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).\n    \"\"\"\n\n    short_name = 'projectq.simulator'\n    _gates = set(operator_map.keys())\n    _observables = set([ key for (key,val) in operator_map.items() if val in [XGate, YGate, ZGate, AllZGate, Hermitian] ])\n    _circuits = {}\n    _backend_kwargs = ['gate_fusion', 'rnd_seed']\n\n    def __init__(self, wires, **kwargs):\n        kwargs['backend'] = 'Simulator'\n        super().__init__(wires, **kwargs)\n\n    def reset(self):\n        \"\"\"Resets the engine and backend\n\n        After the reset the Device should be as if it was just constructed.\n        Most importantly the quantum state is reset to its initial value.\n        \"\"\"\n        backend = pq.backends.Simulator(**self.filter_kwargs_for_backend(self.kwargs))\n        self.eng = pq.MainEngine(backend)\n        super().reset()\n\n\n    def expectation(self, observable, wires):\n        self.eng.flush(deallocate_qubits=False)\n        if observable == 'PauliX' or observable == 'PauliY' or observable == 'PauliZ':\n            expectation_value = self.eng.backend.get_expectation_value(pq.ops.QubitOperator(str(observable)[-1]+'0'), self.reg)\n            variance = 1 - expectation_value**2\n        elif observable == 'AllPauliZ':\n            expectation_value = [ self.eng.backend.get_expectation_value(pq.ops.QubitOperator(\"Z\"+'0'), [qubit]) for qubit in self.reg]\n            variance = [1 - e**2 for e in expectation_value]\n        else:\n            raise NotImplementedError(\"Estimation of expectation values not yet implemented for the observable {} in backend {}.\".format(observable, self.backend))\n\n        return expectation_value#, variance\n\n\nclass ProjectQClassicalSimulator(ProjectQDevice):\n    \"\"\"ProjectQ ClassicalSimulator device for OpenQML.\n\n    Args:\n       wires (int): The number of qubits of the device.\n    \"\"\"\n\n    short_name = 'projectq.classicalsimulator'\n    _gates = set([ key for (key,val) in operator_map.items() if val in [XGate, CNOT] ])\n    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])\n    _circuits = {}\n    _backend_kwargs = []\n\n    def __init__(self, wires, **kwargs):\n        kwargs['backend'] = 'ClassicalSimulator'\n        super().__init__(wires, **kwargs)\n\n    def reset(self):\n        \"\"\"Resets the engine and backend\n\n        After the reset the Device should be as if it was just constructed.\n        Most importantly the quantum state is reset to its initial value.\n        \"\"\"\n        backend = pq.backends.ClassicalSimulator(**self.filter_kwargs_for_backend(self.kwargs))\n        self.eng = pq.MainEngine(backend)\n        super().reset()\n\nclass ProjectQIBMBackend(ProjectQDevice):\n    \"\"\"ProjectQ IBMBackend device for OpenQML.\n\n    Args:\n       wires (int): The number of qubits of the device.\n\n    Keyword Args:\n      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)\n      num_runs (int): Number of runs to collect statistics. (default is 1024)\n      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).\n      user (string): IBM Quantum Experience user name\n      password (string): IBM Quantum Experience password\n      device (string): Device to use (ibmqx4, or ibmqx5) if use_hardware is set to True. Default is ibmqx4.\n      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).\n    \"\"\"\n\n    short_name = 'projectq.ibmbackend'\n    _gates = set([ key for (key,val) in operator_map.items() if val in [HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, Rx, Ry, Rz, R, CNOT, CZ] ])\n    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])\n    _circuits = {}\n    _backend_kwargs = ['use_hardware', 'num_runs', 'verbose', 'user', 'password', 'device', 'retrieve_execution']\n\n    def __init__(self, wires, **kwargs):\n        # check that necessary arguments are given\n        if 'user' not in kwargs:\n            raise ValueError('An IBM Quantum Experience user name specified via the \"user\" keyword argument is required')\n        if 'password' not in kwargs:\n            raise ValueError('An IBM Quantum Experience password specified via the \"password\" keyword argument is required')\n\n        kwargs['backend'] = 'IBMBackend'\n        #kwargs['verbose'] = True #todo: remove when done testing\n        #kwargs['log'] = True #todo: remove when done testing\n        #kwargs['use_hardware'] = False #todo: remove when done testing\n        #kwargs['num_runs'] = 3 #todo: remove when done testing\n        super().__init__(wires, **kwargs)\n\n    def reset(self):\n        \"\"\"Resets the engine and backend\n\n        After the reset the Device should be as if it was just constructed.\n        Most importantly the quantum state is reset to its initial value.\n        \"\"\"\n        backend = pq.backends.IBMBackend(**self.filter_kwargs_for_backend(self.kwargs))\n        self.eng = pq.MainEngine(backend, engine_list=pq.setups.ibm.get_engine_list())\n        super().reset()\n\n    def expectation(self, observable, wires):\n        pq.ops.R(0) | self.reg[0]# todo:remove this once https://github.com/ProjectQ-Framework/ProjectQ/issues/259 is resolved\n\n        pq.ops.All(pq.ops.Measure) | self.reg\n        self.eng.flush()\n\n        if observable == 'PauliZ':\n            probabilities = self.eng.backend.get_probabilities([self.reg[wires]])\n            #print(\"IBM probabilities=\"+str(probabilities))\n            if '1' in probabilities:\n                expectation_value = 2*probabilities['1']-1\n            else:\n                expectation_value = -(2*probabilities['0']-1)\n            variance = 1 - expectation_value**2\n        elif observable == 'AllPauliZ':\n            probabilities = self.eng.backend.get_probabilities(self.reg)\n            #print(\"IBM all probabilities=\"+str(probabilities))\n            expectation_value = [ ((2*sum(p for (state,p) in probabilities.items() if state[i] == '1')-1)-(2*sum(p for (state,p) in probabilities.items() if state[i] == '0')-1)) for i in range(len(self.reg)) ]\n            variance = [1 - e**2 for e in expectation_value]\n        else:\n            raise NotImplementedError(\"Estimation of expectation values not yet implemented for the observable {} in backend {}.\".format(observable, self.backend))\n\n        return expectation_value#, variance\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/bcgov/namex/blob/003189c4b23ce3672cab99b55ff02d8b1a73bdcc",
        "file_path": "/api/tests/python/end_points/test_sounds_like.py",
        "source": "from namex.models import User\nimport requests\nimport json\nimport pytest\nfrom tests.python import integration_solr, integration_synonym_api\nimport urllib\nfrom hamcrest import *\n\n\ntoken_header = {\n                \"alg\": \"RS256\",\n                \"typ\": \"JWT\",\n                \"kid\": \"flask-jwt-oidc-test-client\"\n               }\nclaims = {\n            \"iss\": \"https://sso-dev.pathfinder.gov.bc.ca/auth/realms/sbc\",\n            \"sub\": \"43e6a245-0bf7-4ccf-9bd0-e7fb85fd18cc\",\n            \"aud\": \"NameX-Dev\",\n            \"exp\": 31531718745,\n            \"iat\": 1531718745,\n            \"jti\": \"flask-jwt-oidc-test-support\",\n            \"typ\": \"Bearer\",\n            \"username\": \"test-user\",\n            \"realm_access\": {\n                \"roles\": [\n                    \"{}\".format(User.EDITOR),\n                    \"{}\".format(User.APPROVER),\n                    \"viewer\",\n                    \"user\"\n                ]\n            }\n         }\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef reload_schema(solr):\n    url = solr + '/solr/admin/cores?action=RELOAD&core=possible.conflicts&wt=json'\n    r = requests.get(url)\n\n    assert r.status_code == 200\n\n\n@integration_solr\ndef test_solr_available(solr, app, client, jwt):\n    url = solr + '/solr/possible.conflicts/admin/ping'\n    r = requests.get(url)\n\n    assert r.status_code == 200\n\n\ndef clean_database(solr):\n    url = solr + '/solr/possible.conflicts/update?commit=true'\n    headers = {'content-type': 'text/xml'}\n    data = '<delete><query>id:*</query></delete>'\n    r = requests.post(url, headers=headers, data=data)\n\n    assert r.status_code == 200\n\n\ndef seed_database_with(solr, name, id='1', source='CORP'):\n    url = solr + '/solr/possible.conflicts/update?commit=true'\n    headers = {'content-type': 'application/json'}\n    data = '[{\"source\":\"' + source + '\", \"name\":\"' + name + '\", \"id\":\"'+ id +'\"}]'\n    r = requests.post(url, headers=headers, data=data)\n\n    assert r.status_code == 200\n\n\ndef verify(data, expected):\n\n    print(\"Expected: \", expected)\n\n    # remove the search divider(s): ----<query term>\n    actual = [{ 'name':doc['name_info']['name'] } for doc in data['names']]\n\n    print(\"Actual: \", actual)\n\n    assert_that(len(actual), equal_to(len(expected)))\n    for i in range(len(actual)):\n        assert_that(actual[i]['name'], equal_to(expected[i]['name']))\n\n\ndef verify_results(client, jwt, query, expected):\n    data = search(client, jwt, query)\n    verify(data, expected)\n\n\ndef search(client, jwt, query):\n    token = jwt.create_jwt(claims, token_header)\n    headers = {'Authorization': 'Bearer ' + token}\n    url = '/api/v1/requests/phonetics/' + urllib.parse.quote(query) + '/*'\n    print(url)\n    rv = client.get(url, headers=headers)\n\n    assert rv.status_code == 200\n    return json.loads(rv.data)\n\n\n@integration_synonym_api\n@integration_solr\ndef test_all_good(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'GOLDSTREAM ELECTRICAL LTD')\n    verify_results(client, jwt,\n       query='GOLDSMITHS',\n       expected=[\n           {'name': '----GOLDSMITHS'},\n           {'name': 'GOLDSTREAM ELECTRICAL LTD'}\n       ]\n    )\n\n\n@pytest.mark.skip(reason=\"Rhyming not implemented yet\")\n@integration_synonym_api\n@integration_solr\ndef test_sounds_like(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'GAYLEDESIGNS INC.', id='1')\n    seed_database_with(solr, 'GOLDSTREAM ELECTRICAL CORP', id='2')\n    seed_database_with(solr, 'GLADSTONE JEWELLERY LTD', id='3')\n    seed_database_with(solr, 'GOLDSTEIN HOLDINGS INC.', id='4')\n    seed_database_with(solr, 'CLOUDSIDE INN INCORPORATED', id='5')\n    seed_database_with(solr, 'GOLDSPRING PROPERTIES LTD', id='6')\n    seed_database_with(solr, 'GOLDSTRIPES AVIATION INC', id='7')\n    seed_database_with(solr, 'GLADSTONE CAPITAL CORP', id='8')\n    seed_database_with(solr, 'KLETAS LAW CORPORATION', id='9')\n    seed_database_with(solr, 'COLDSTREAM VENTURES INC.', id='10')\n    seed_database_with(solr, 'BLABLA ANYTHING', id='11')\n    verify_results(client, jwt,\n       query='GOLDSMITHS',\n       expected=[\n           {'name': '----GOLDSMITHS'},\n           {'name': 'COLDSTREAM VENTURES INC.'},\n           {'name': 'GOLDSPRING PROPERTIES LTD'},\n           {'name': 'GOLDSTEIN HOLDINGS INC.'},\n           {'name': 'GOLDSTREAM ELECTRICAL CORP'},\n           {'name': 'GOLDSTRIPES AVIATION INC'},\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_liberti(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'LIBERTI', id='1')\n    verify_results(client, jwt,\n       query='LIBERTY',\n       expected=[\n           {'name': '----LIBERTY'},\n           {'name': 'LIBERTI'},\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_deeper(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'LABORATORY', id='1')\n    seed_database_with(solr, 'LAPORTE', id='2')\n    seed_database_with(solr, 'LIBERTI', id='3')\n    verify_results(client, jwt,\n       query='LIBERTY',\n       expected=[\n           {'name': '----LIBERTY'},\n           {'name': 'LIBERTI'},\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_jasmine(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'JASMINE', id='1')\n    verify_results(client, jwt,\n       query='OSMOND',\n       expected=[\n           {'name': '----OSMOND'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_fey(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'FEY', id='1')\n    verify_results(client, jwt,\n       query='FAY',\n       expected=[\n           {'name': '----FAY'},\n           {'name': 'FEY'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_venizia(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'VENIZIA', id='1')\n    seed_database_with(solr, 'VENEZIA', id='2')\n    seed_database_with(solr, 'VANSEA', id='3')\n    seed_database_with(solr, 'WENSO', id='4')\n    verify_results(client, jwt,\n       query='VENIZIA',\n       expected=[\n           {'name': '----VENIZIA'},\n           {'name': 'VENEZIA'},\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_ys_and_is(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'CRYSTAL', id='1')\n    verify_results(client, jwt,\n       query='CRISTAL',\n       expected=[\n           {'name': '----CRISTAL'},\n           {'name': 'CRYSTAL'},\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_cs_and_ks(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'KOLDSMITHS', id='1')\n    verify_results(client, jwt,\n       query='COLDSTREAM',\n       expected=[\n           {'name': '----COLDSTREAM'},\n           {'name': 'KOLDSMITHS'},\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_cs_and_ks_again(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'CRAZY', id='1')\n    seed_database_with(solr, 'KAIZEN', id='2')\n    verify_results(client, jwt,\n       query='CAYZEN',\n       expected=[\n           {'name': '----CAYZEN'},\n           {'name': 'KAIZEN'},\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_resist_short_word(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'FE', id='1')\n    verify_results(client, jwt,\n       query='FA',\n       expected=[\n           {'name': '----FA'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_resist_single_vowel(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'FEDS', id='1')\n    verify_results(client, jwt,\n       query='FADS',\n       expected=[\n           {'name': '----FADS'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_feel(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'FEEL', id='1')\n    verify_results(client, jwt,\n       query='FILL',\n       expected=[\n           {'name': '----FILL'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_bear(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'BEAR', id='1')\n    verify_results(client, jwt,\n       query='BARE',\n       expected=[\n           {'name': '----BARE'},\n           {'name': 'BEAR'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_ignore_corp(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'GLADSTONE CAPITAL corp', id='1')\n    verify_results(client, jwt,\n       query='GOLDSMITHS',\n       expected=[\n           {'name': '----GOLDSMITHS'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_designation_in_query_is_ignored(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'FINGER LIMATED', id='1')\n    verify_results(client, jwt,\n       query='SUN LIMITED',\n       expected=[\n           {'name': '----SUN'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef leak(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'LEAK', id='1')\n    verify_results(client, jwt,\n       query='LEEK',\n       expected=[\n           {'name': 'LEAK'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_plank(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'PLANCK', id='1')\n    verify_results(client, jwt,\n       query='PLANK',\n       expected=[\n           {'name': '----PLANK'},\n           {'name': 'PLANCK'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_krystal(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'KRYSTAL', id='1')\n    verify_results(client, jwt,\n       query='CRISTAL',\n       expected=[\n           {'name': '----CRISTAL'},\n           {'name': 'KRYSTAL'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_christal(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'KRYSTAL', id='1')\n    verify_results(client, jwt,\n       query='CHRISTAL',\n       expected=[\n           {'name': '----CHRISTAL'},\n           {'name': 'KRYSTAL'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_kl(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'KLASS', id='1')\n    verify_results(client, jwt,\n       query='CLASS',\n       expected=[\n           {'name': '----CLASS'},\n           {'name': 'KLASS'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_pheel(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'PHEEL', id='1')\n    verify_results(client, jwt,\n       query='FEEL',\n       expected=[\n           {'name': '----FEEL'},\n           {'name': 'PHEEL'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_ghable(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'GHABLE', id='1')\n    verify_results(client, jwt,\n       query='GABLE',\n       expected=[\n           {'name': '----GABLE'},\n           {'name': 'GHABLE'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_gnat(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'GNAT', id='1')\n    verify_results(client, jwt,\n       query='NAT',\n       expected=[\n           {'name': '----NAT'},\n           {'name': 'GNAT'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_kn(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'KNAT', id='1')\n    verify_results(client, jwt,\n       query='NAT',\n       expected=[\n           {'name': '----NAT'},\n           {'name': 'KNAT'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_pn(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'PNEU', id='1')\n    verify_results(client, jwt,\n       query='NEU',\n       expected=[\n           {'name': '----NEU'},\n           {'name': 'PNEU'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_wr(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'WREN', id='1')\n    verify_results(client, jwt,\n       query='REN',\n       expected=[\n           {'name': '----REN'},\n           {'name': 'WREN'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_rh(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'RHEN', id='1')\n    verify_results(client, jwt,\n       query='REN',\n       expected=[\n           {'name': '----REN'},\n           {'name': 'RHEN'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_soft_c_is_not_k(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'KIRK', id='1')\n    verify_results(client, jwt,\n       query='CIRCLE',\n       expected=[\n           {'name': '----CIRCLE'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_oi_oy(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'OYSTER', id='1')\n    verify_results(client, jwt,\n       query='OISTER',\n       expected=[\n           {'name': '----OISTER'},\n           {'name': 'OYSTER'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_dont_add_match_twice(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'RHEN GNAT', id='1')\n    verify_results(client, jwt,\n       query='REN NAT',\n       expected=[\n           {'name': '----REN NAT'},\n           {'name': 'RHEN GNAT'},\n           {'name': '----REN'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_neighbour(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'NEIGHBOUR', id='1')\n    verify_results(client, jwt,\n       query='NAYBOR',\n       expected=[\n           {'name': '----NAYBOR'},\n           {'name': 'NEIGHBOUR'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_mac_mc(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'MCGREGOR', id='1')\n    verify_results(client, jwt,\n       query='MACGREGOR',\n       expected=[\n           {'name': '----MACGREGOR'},\n           {'name': 'MCGREGOR'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_ex_x(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'EXTREME', id='1')\n    verify_results(client, jwt,\n       query='XTREME',\n       expected=[\n           {'name': '----XTREME'},\n           {'name': 'EXTREME'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_wh(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'WHITE', id='1')\n    verify_results(client, jwt,\n       query='WITE',\n       expected=[\n           {'name': '----WITE'},\n           {'name': 'WHITE'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_qu(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'KWIK', id='1')\n    verify_results(client, jwt,\n       query='QUICK',\n       expected=[\n           {'name': '----QUICK'},\n           {'name': 'KWIK'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_ps(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'PSYCHO', id='1')\n    verify_results(client, jwt,\n       query='SYCHO',\n       expected=[\n           {'name': '----SYCHO'},\n           {'name': 'PSYCHO'}\n       ]\n    )\n\n\n@pytest.mark.skip(reason=\"not handled yet\")\n@integration_synonym_api\n@integration_solr\ndef test_terra(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'TERRA', id='1')\n    verify_results(client, jwt,\n       query='TARA',\n       expected=[\n           {'name': 'TERRA'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_ayaan(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'AYAAN', id='1')\n    verify_results(client, jwt,\n       query='AYAN',\n       expected=[\n           {'name': '----AYAN'},\n           {'name': 'AYAAN'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_aggri(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'AGGRI', id='1')\n    verify_results(client, jwt,\n       query='AGRI',\n       expected=[\n           {'name': '----AGRI'},\n           {'name': 'AGGRI'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_kofi(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'KOFI', id='1')\n    verify_results(client, jwt,\n       query='COFFI',\n       expected=[\n           {'name': '----COFFI'},\n           {'name': 'KOFI'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_tru(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'TRU', id='1')\n    verify_results(client, jwt,\n       query='TRUE',\n       expected=[\n           {'name': '----TRUE'},\n           {'name': 'TRU'}\n       ]\n    )\n\n\n@pytest.mark.skip(reason=\"not handled yet\")\n@integration_synonym_api\n@integration_solr\ndef test_dymond(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'DYMOND', id='1')\n    verify_results(client, jwt,\n       query='DIAMOND',\n       expected=[\n           {'name': 'DYMOND'}\n       ]\n    )\n\n\n@pytest.mark.skip(reason=\"compound words not handled yet\")\n@integration_synonym_api\n@integration_solr\ndef test_bee_kleen(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'BEE KLEEN', id='1')\n    verify_results(client, jwt,\n       query='BE-CLEAN',\n       expected=[\n           {'name': 'BEE KLEEN'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_ignore_exact_match_keep_phonetic(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'BODY BLUEPRINT FITNESS INC.', id='1')\n    seed_database_with(solr, 'BLUEPRINT BEAUTEE', id='2')\n    verify_results(client, jwt,\n       query='BLUEPRINT BEAUTY',\n       expected=[\n           {'name': '----BLUEPRINT BEAUTY'},\n           {'name': 'BLUEPRINT BEAUTEE'},\n           {'name': '----BLUEPRINT synonyms:(BEAUTI)'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_match_both_words(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'ANDERSON BEHAVIOR CONSULTING', id='1')\n    verify_results(client, jwt,\n       query='INTERVENTION BEHAVIOUR',\n       expected=[\n           {'name': '----INTERVENTION BEHAVIOUR'},\n           {'name': '----INTERVENTION'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_match_at_right_level(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'ANDERSON BEHAVIOR CONSULTING INC.', id='1')\n    verify_results(client, jwt,\n       query='BEHAVIOUR INTERVENTION',\n       expected=[\n           {'name': '----BEHAVIOUR INTERVENTION'},\n           {'name': '----BEHAVIOUR'},\n           {'name': 'ANDERSON BEHAVIOR CONSULTING INC.'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_resists_qword_matching_several_words(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'ANDERSON BEHAVIOR BEHAVIOR', id='1')\n    verify_results(client, jwt,\n       query='BEHAVIOUR INTERVENTION',\n       expected=[\n           {'name': '----BEHAVIOUR INTERVENTION'},\n           {'name': '----BEHAVIOUR'},\n           {'name': 'ANDERSON BEHAVIOR BEHAVIOR'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_leading_vowel_a(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'AILEEN ENTERPRISES', id='1')\n    verify_results(client, jwt,\n       query='ALAN HARGREAVES CORPORATION',\n       expected=[\n           {'name': '----ALAN HARGREAVES'},\n           {'name': '----ALAN'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_leading_vowel_e(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'ACME', id='1')\n    verify_results(client, jwt,\n       query='EQUIOM',\n       expected=[\n           {'name': '----EQUIOM'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_leading_vowel_not_match_consonant(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'HELENAH WU & CO. INC.', id='1')\n    seed_database_with(solr, 'A BETTER WAY HERBALS LTD.', id='2')\n    verify_results(client, jwt,\n       query='EH',\n       expected=[\n           {'name': '----EH'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_unusual_result(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'DOUBLE J AVIATION LTD.', id='1')\n    verify_results(client, jwt,\n       query='TABLE',\n       expected=[\n           {'name': '----TABLE'}\n       ]\n    )\n\n@integration_synonym_api\n@integration_solr\ndef test_stack_ignores_wildcards(client, jwt, app):\n    verify_results(client, jwt,\n        query=\"TESTING* @WILDCARDS\",\n        expected=[\n            {'name': '----TESTING WILDCARDS'},\n            {'name': '----TESTING'}\n        ]\n    )\n\n@integration_synonym_api\n@integration_solr\n@pytest.mark.parametrize(\"query\", [\n    ('T.H.E.'),\n    ('COMPANY'),\n    ('ASSN'),\n    ('THAT'),\n    ('LIMITED CORP.'),\n])\ndef test_query_stripped_to_empty_string(solr,client, jwt, query):\n    clean_database(solr)\n    seed_database_with(solr, 'JM Van Damme inc', id='1')\n    seed_database_with(solr, 'SOME RANDOM NAME', id='2')\n    verify_results(client, jwt,\n        query=query,\n        expected=[{'name':'----*'}]\n    )\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/galaxyproject/galaxy-beta1/blob/3633c7dc116422da5689f35f3805cd4762b8874f",
        "file_path": "/lib/galaxy/jobs/runners/lwr.py",
        "source": "import logging\n\nfrom galaxy import model\nfrom galaxy.jobs.runners import AsynchronousJobState, AsynchronousJobRunner\nfrom galaxy.jobs import ComputeEnvironment\nfrom galaxy.jobs import JobDestination\nfrom galaxy.jobs.command_factory import build_command\nfrom galaxy.tools.deps import dependencies\nfrom galaxy.util import string_as_bool_or_none\nfrom galaxy.util.bunch import Bunch\n\nimport errno\nfrom time import sleep\nimport os\n\nfrom .lwr_client import build_client_manager\nfrom .lwr_client import url_to_destination_params\nfrom .lwr_client import finish_job as lwr_finish_job\nfrom .lwr_client import submit_job as lwr_submit_job\nfrom .lwr_client import ClientJobDescription\nfrom .lwr_client import LwrOutputs\nfrom .lwr_client import ClientOutputs\nfrom .lwr_client import PathMapper\n\nlog = logging.getLogger( __name__ )\n\n__all__ = [ 'LwrJobRunner' ]\n\nNO_REMOTE_GALAXY_FOR_METADATA_MESSAGE = \"LWR misconfiguration - LWR client configured to set metadata remotely, but remote LWR isn't properly configured with a galaxy_home directory.\"\nNO_REMOTE_DATATYPES_CONFIG = \"LWR client is configured to use remote datatypes configuration when setting metadata externally, but LWR is not configured with this information. Defaulting to datatypes_conf.xml.\"\n\n# Is there a good way to infer some default for this? Can only use\n# url_for from web threads. https://gist.github.com/jmchilton/9098762\nDEFAULT_GALAXY_URL = \"http://localhost:8080\"\n\n\nclass LwrJobRunner( AsynchronousJobRunner ):\n    \"\"\"\n    LWR Job Runner\n    \"\"\"\n    runner_name = \"LWRRunner\"\n\n    def __init__( self, app, nworkers, transport=None, cache=None, url=None, galaxy_url=DEFAULT_GALAXY_URL ):\n        \"\"\"Start the job runner \"\"\"\n        super( LwrJobRunner, self ).__init__( app, nworkers )\n        self.async_status_updates = dict()\n        self._init_monitor_thread()\n        self._init_worker_threads()\n        client_manager_kwargs = {'transport_type': transport, 'cache': string_as_bool_or_none(cache), \"url\": url}\n        self.galaxy_url = galaxy_url\n        self.client_manager = build_client_manager(**client_manager_kwargs)\n\n    def url_to_destination( self, url ):\n        \"\"\"Convert a legacy URL to a job destination\"\"\"\n        return JobDestination( runner=\"lwr\", params=url_to_destination_params( url ) )\n\n    def check_watched_item(self, job_state):\n        try:\n            client = self.get_client_from_state(job_state)\n\n            if hasattr(self.client_manager, 'ensure_has_status_update_callback'):\n                # Message queue implementation.\n\n                # TODO: Very hacky now, refactor after Dannon merges in his\n                # message queue work, runners need the ability to disable\n                # check_watched_item like this and instead a callback needs to\n                # be issued post job recovery allowing a message queue\n                # consumer to be setup.\n                self.client_manager.ensure_has_status_update_callback(self.__async_update)\n                return job_state\n\n            status = client.get_status()\n        except Exception:\n            # An orphaned job was put into the queue at app startup, so remote server went down\n            # either way we are done I guess.\n            self.mark_as_finished(job_state)\n            return None\n        job_state = self.__update_job_state_for_lwr_status(job_state, status)\n        return job_state\n\n    def __update_job_state_for_lwr_status(self, job_state, lwr_status):\n        if lwr_status == \"complete\":\n            self.mark_as_finished(job_state)\n            return None\n        if lwr_status == \"running\" and not job_state.running:\n            job_state.running = True\n            job_state.job_wrapper.change_state( model.Job.states.RUNNING )\n        return job_state\n\n    def __async_update( self, full_status ):\n        job_id = full_status[ \"job_id\" ]\n        job_state = self.__find_watched_job( job_id )\n        if not job_state:\n            # Probably finished too quickly, sleep and try again.\n            # Kind of a hack, why does monitor queue need to no wait\n            # get and sleep instead of doing a busy wait that would\n            # respond immediately.\n            sleep( 2 )\n            job_state = self.__find_watched_job( job_id )\n        if not job_state:\n            log.warn( \"Failed to find job corresponding to final status %s in %s\" % ( full_status, self.watched ) )\n        else:\n            self.__update_job_state_for_lwr_status(job_state, full_status[\"status\"])\n\n    def __find_watched_job( self, job_id ):\n        found_job = None\n        for async_job_state in self.watched:\n            if str( async_job_state.job_id ) == job_id:\n                found_job = async_job_state\n                break\n        return found_job\n\n    def queue_job(self, job_wrapper):\n        job_destination = job_wrapper.job_destination\n\n        command_line, client, remote_job_config, compute_environment = self.__prepare_job( job_wrapper, job_destination )\n\n        if not command_line:\n            return\n\n        try:\n            dependencies_description = LwrJobRunner.__dependencies_description( client, job_wrapper )\n            rewrite_paths = not LwrJobRunner.__rewrite_parameters( client )\n            unstructured_path_rewrites = {}\n            if compute_environment:\n                unstructured_path_rewrites = compute_environment.unstructured_path_rewrites\n\n            client_job_description = ClientJobDescription(\n                command_line=command_line,\n                input_files=self.get_input_files(job_wrapper),\n                client_outputs=self.__client_outputs(client, job_wrapper),\n                working_directory=job_wrapper.working_directory,\n                tool=job_wrapper.tool,\n                config_files=job_wrapper.extra_filenames,\n                dependencies_description=dependencies_description,\n                env=client.env,\n                rewrite_paths=rewrite_paths,\n                arbitrary_files=unstructured_path_rewrites,\n            )\n            job_id = lwr_submit_job(client, client_job_description, remote_job_config)\n            log.info(\"lwr job submitted with job_id %s\" % job_id)\n            job_wrapper.set_job_destination( job_destination, job_id )\n            job_wrapper.change_state( model.Job.states.QUEUED )\n        except Exception:\n            job_wrapper.fail( \"failure running job\", exception=True )\n            log.exception(\"failure running job %d\" % job_wrapper.job_id)\n            return\n\n        lwr_job_state = AsynchronousJobState()\n        lwr_job_state.job_wrapper = job_wrapper\n        lwr_job_state.job_id = job_id\n        lwr_job_state.old_state = True\n        lwr_job_state.running = False\n        lwr_job_state.job_destination = job_destination\n        self.monitor_job(lwr_job_state)\n\n    def __prepare_job(self, job_wrapper, job_destination):\n        \"\"\" Build command-line and LWR client for this job. \"\"\"\n        command_line = None\n        client = None\n        remote_job_config = None\n        compute_environment = None\n        try:\n            client = self.get_client_from_wrapper(job_wrapper)\n            tool = job_wrapper.tool\n            remote_job_config = client.setup(tool.id, tool.version)\n            rewrite_parameters = LwrJobRunner.__rewrite_parameters( client )\n            prepare_kwds = {}\n            if rewrite_parameters:\n                compute_environment = LwrComputeEnvironment( client, job_wrapper, remote_job_config )\n                prepare_kwds[ 'compute_environment' ] = compute_environment\n            job_wrapper.prepare( **prepare_kwds )\n            self.__prepare_input_files_locally(job_wrapper)\n            remote_metadata = LwrJobRunner.__remote_metadata( client )\n            remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )\n            dependency_resolution = LwrJobRunner.__dependency_resolution( client )\n            metadata_kwds = self.__build_metadata_configuration(client, job_wrapper, remote_metadata, remote_job_config)\n            remote_command_params = dict(\n                working_directory=remote_job_config['working_directory'],\n                metadata_kwds=metadata_kwds,\n                dependency_resolution=dependency_resolution,\n            )\n            command_line = build_command(\n                self,\n                job_wrapper=job_wrapper,\n                include_metadata=remote_metadata,\n                include_work_dir_outputs=remote_work_dir_copy,\n                remote_command_params=remote_command_params,\n            )\n        except Exception:\n            job_wrapper.fail( \"failure preparing job\", exception=True )\n            log.exception(\"failure running job %d\" % job_wrapper.job_id)\n\n        # If we were able to get a command line, run the job\n        if not command_line:\n            job_wrapper.finish( '', '' )\n\n        return command_line, client, remote_job_config, compute_environment\n\n    def __prepare_input_files_locally(self, job_wrapper):\n        \"\"\"Run task splitting commands locally.\"\"\"\n        prepare_input_files_cmds = getattr(job_wrapper, 'prepare_input_files_cmds', None)\n        if prepare_input_files_cmds is not None:\n            for cmd in prepare_input_files_cmds:  # run the commands to stage the input files\n                if 0 != os.system(cmd):\n                    raise Exception('Error running file staging command: %s' % cmd)\n            job_wrapper.prepare_input_files_cmds = None  # prevent them from being used in-line\n\n    def get_output_files(self, job_wrapper):\n        output_paths = job_wrapper.get_output_fnames()\n        return [ str( o ) for o in output_paths ]   # Force job_path from DatasetPath objects.\n\n    def get_input_files(self, job_wrapper):\n        input_paths = job_wrapper.get_input_paths()\n        return [ str( i ) for i in input_paths ]  # Force job_path from DatasetPath objects.\n\n    def get_client_from_wrapper(self, job_wrapper):\n        job_id = job_wrapper.job_id\n        if hasattr(job_wrapper, 'task_id'):\n            job_id = \"%s_%s\" % (job_id, job_wrapper.task_id)\n        params = job_wrapper.job_destination.params.copy()\n        for key, value in params.iteritems():\n            if value:\n                params[key] = model.User.expand_user_properties( job_wrapper.get_job().user, value )\n        env = getattr( job_wrapper.job_destination, \"env\", [] )\n        return self.get_client( params, job_id, env )\n\n    def get_client_from_state(self, job_state):\n        job_destination_params = job_state.job_destination.params\n        job_id = job_state.job_id\n        return self.get_client( job_destination_params, job_id )\n\n    def get_client( self, job_destination_params, job_id, env=[] ):\n        # Cannot use url_for outside of web thread.\n        #files_endpoint = url_for( controller=\"job_files\", job_id=encoded_job_id )\n\n        encoded_job_id = self.app.security.encode_id(job_id)\n        job_key = self.app.security.encode_id( job_id, kind=\"jobs_files\" )\n        files_endpoint = \"%s/api/jobs/%s/files?job_key=%s\" % (\n            self.galaxy_url,\n            encoded_job_id,\n            job_key\n        )\n        get_client_kwds = dict(\n            job_id=str( job_id ),\n            files_endpoint=files_endpoint,\n            env=env\n        )\n        return self.client_manager.get_client( job_destination_params, **get_client_kwds )\n\n    def finish_job( self, job_state ):\n        stderr = stdout = ''\n        job_wrapper = job_state.job_wrapper\n        try:\n            client = self.get_client_from_state(job_state)\n            run_results = client.full_status()\n\n            stdout = run_results.get('stdout', '')\n            stderr = run_results.get('stderr', '')\n            exit_code = run_results.get('returncode', None)\n            lwr_outputs = LwrOutputs.from_status_response(run_results)\n            # Use LWR client code to transfer/copy files back\n            # and cleanup job if needed.\n            completed_normally = \\\n                job_wrapper.get_state() not in [ model.Job.states.ERROR, model.Job.states.DELETED ]\n            cleanup_job = self.app.config.cleanup_job\n            client_outputs = self.__client_outputs(client, job_wrapper)\n            finish_args = dict( client=client,\n                                job_completed_normally=completed_normally,\n                                cleanup_job=cleanup_job,\n                                client_outputs=client_outputs,\n                                lwr_outputs=lwr_outputs )\n            failed = lwr_finish_job( **finish_args )\n\n            if failed:\n                job_wrapper.fail(\"Failed to find or download one or more job outputs from remote server.\", exception=True)\n        except Exception:\n            message = \"Failed to communicate with remote job server.\"\n            job_wrapper.fail( message, exception=True )\n            log.exception(\"failure finishing job %d\" % job_wrapper.job_id)\n            return\n        if not LwrJobRunner.__remote_metadata( client ):\n            self._handle_metadata_externally( job_wrapper, resolve_requirements=True )\n        # Finish the job\n        try:\n            job_wrapper.finish( stdout, stderr, exit_code )\n        except Exception:\n            log.exception(\"Job wrapper finish method failed\")\n            job_wrapper.fail(\"Unable to finish job\", exception=True)\n\n    def fail_job( self, job_state ):\n        \"\"\"\n        Seperated out so we can use the worker threads for it.\n        \"\"\"\n        self.stop_job( self.sa_session.query( self.app.model.Job ).get( job_state.job_wrapper.job_id ) )\n        job_state.job_wrapper.fail( job_state.fail_message )\n\n    def check_pid( self, pid ):\n        try:\n            os.kill( pid, 0 )\n            return True\n        except OSError, e:\n            if e.errno == errno.ESRCH:\n                log.debug( \"check_pid(): PID %d is dead\" % pid )\n            else:\n                log.warning( \"check_pid(): Got errno %s when attempting to check PID %d: %s\" % ( errno.errorcode[e.errno], pid, e.strerror ) )\n            return False\n\n    def stop_job( self, job ):\n        #if our local job has JobExternalOutputMetadata associated, then our primary job has to have already finished\n        job_ext_output_metadata = job.get_external_output_metadata()\n        if job_ext_output_metadata:\n            pid = job_ext_output_metadata[0].job_runner_external_pid  # every JobExternalOutputMetadata has a pid set, we just need to take from one of them\n            if pid in [ None, '' ]:\n                log.warning( \"stop_job(): %s: no PID in database for job, unable to stop\" % job.id )\n                return\n            pid = int( pid )\n            if not self.check_pid( pid ):\n                log.warning( \"stop_job(): %s: PID %d was already dead or can't be signaled\" % ( job.id, pid ) )\n                return\n            for sig in [ 15, 9 ]:\n                try:\n                    os.killpg( pid, sig )\n                except OSError, e:\n                    log.warning( \"stop_job(): %s: Got errno %s when attempting to signal %d to PID %d: %s\" % ( job.id, errno.errorcode[e.errno], sig, pid, e.strerror ) )\n                    return  # give up\n                sleep( 2 )\n                if not self.check_pid( pid ):\n                    log.debug( \"stop_job(): %s: PID %d successfully killed with signal %d\" % ( job.id, pid, sig ) )\n                    return\n                else:\n                    log.warning( \"stop_job(): %s: PID %d refuses to die after signaling TERM/KILL\" % ( job.id, pid ) )\n        else:\n            # Remote kill\n            lwr_url = job.job_runner_name\n            job_id = job.job_runner_external_id\n            log.debug(\"Attempt remote lwr kill of job with url %s and id %s\" % (lwr_url, job_id))\n            client = self.get_client(job.destination_params, job_id)\n            client.kill()\n\n    def recover( self, job, job_wrapper ):\n        \"\"\"Recovers jobs stuck in the queued/running state when Galaxy started\"\"\"\n        job_state = AsynchronousJobState()\n        job_state.job_id = str( job.get_job_runner_external_id() )\n        job_state.runner_url = job_wrapper.get_job_runner_url()\n        job_state.job_destination = job_wrapper.job_destination\n        job_wrapper.command_line = job.get_command_line()\n        job_state.job_wrapper = job_wrapper\n        state = job.get_state()\n        if state in [model.Job.states.RUNNING, model.Job.states.QUEUED]:\n            log.debug( \"(LWR/%s) is still in running state, adding to the LWR queue\" % ( job.get_id()) )\n            job_state.old_state = True\n            job_state.running = state == model.Job.states.RUNNING\n            self.monitor_queue.put( job_state )\n\n    def shutdown( self ):\n        super( LwrJobRunner, self ).shutdown()\n        self.client_manager.shutdown()\n\n    def __client_outputs( self, client, job_wrapper ):\n        remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )\n        if not remote_work_dir_copy:\n            work_dir_outputs = self.get_work_dir_outputs( job_wrapper )\n        else:\n            # They have already been copied over to look like regular outputs remotely,\n            # no need to handle them differently here.\n            work_dir_outputs = []\n        output_files = self.get_output_files( job_wrapper )\n        client_outputs = ClientOutputs(\n            working_directory=job_wrapper.working_directory,\n            work_dir_outputs=work_dir_outputs,\n            output_files=output_files,\n            version_file=job_wrapper.get_version_string_path(),\n        )\n        return client_outputs\n\n    @staticmethod\n    def __dependencies_description( lwr_client, job_wrapper ):\n        dependency_resolution = LwrJobRunner.__dependency_resolution( lwr_client )\n        remote_dependency_resolution = dependency_resolution == \"remote\"\n        if not remote_dependency_resolution:\n            return None\n        requirements = job_wrapper.tool.requirements or []\n        installed_tool_dependencies = job_wrapper.tool.installed_tool_dependencies or []\n        return dependencies.DependenciesDescription(\n            requirements=requirements,\n            installed_tool_dependencies=installed_tool_dependencies,\n        )\n\n    @staticmethod\n    def __dependency_resolution( lwr_client ):\n        dependency_resolution = lwr_client.destination_params.get( \"dependency_resolution\", \"local\" )\n        if dependency_resolution not in [\"none\", \"local\", \"remote\"]:\n            raise Exception(\"Unknown dependency_resolution value encountered %s\" % dependency_resolution)\n        return dependency_resolution\n\n    @staticmethod\n    def __remote_metadata( lwr_client ):\n        remote_metadata = string_as_bool_or_none( lwr_client.destination_params.get( \"remote_metadata\", False ) )\n        return remote_metadata\n\n    @staticmethod\n    def __remote_work_dir_copy( lwr_client ):\n        # Right now remote metadata handling assumes from_work_dir outputs\n        # have been copied over before it runs. So do that remotely. This is\n        # not the default though because adding it to the command line is not\n        # cross-platform (no cp on Windows) and it's un-needed work outside\n        # the context of metadata settting (just as easy to download from\n        # either place.)\n        return LwrJobRunner.__remote_metadata( lwr_client )\n\n    @staticmethod\n    def __use_remote_datatypes_conf( lwr_client ):\n        \"\"\" When setting remote metadata, use integrated datatypes from this\n        Galaxy instance or use the datatypes config configured via the remote\n        LWR.\n\n        Both options are broken in different ways for same reason - datatypes\n        may not match. One can push the local datatypes config to the remote\n        server - but there is no guarentee these datatypes will be defined\n        there. Alternatively, one can use the remote datatype config - but\n        there is no guarentee that it will contain all the datatypes available\n        to this Galaxy.\n        \"\"\"\n        use_remote_datatypes = string_as_bool_or_none( lwr_client.destination_params.get( \"use_remote_datatypes\", False ) )\n        return use_remote_datatypes\n\n    @staticmethod\n    def __rewrite_parameters( lwr_client ):\n        return string_as_bool_or_none( lwr_client.destination_params.get( \"rewrite_parameters\", False ) ) or False\n\n    def __build_metadata_configuration(self, client, job_wrapper, remote_metadata, remote_job_config):\n        metadata_kwds = {}\n        if remote_metadata:\n            remote_system_properties = remote_job_config.get(\"system_properties\", {})\n            remote_galaxy_home = remote_system_properties.get(\"galaxy_home\", None)\n            if not remote_galaxy_home:\n                raise Exception(NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE)\n            metadata_kwds['exec_dir'] = remote_galaxy_home\n            outputs_directory = remote_job_config['outputs_directory']\n            configs_directory = remote_job_config['configs_directory']\n            working_directory = remote_job_config['working_directory']\n            outputs = [Bunch(false_path=os.path.join(outputs_directory, os.path.basename(path)), real_path=path) for path in self.get_output_files(job_wrapper)]\n            metadata_kwds['output_fnames'] = outputs\n            metadata_kwds['compute_tmp_dir'] = working_directory\n            metadata_kwds['config_root'] = remote_galaxy_home\n            default_config_file = os.path.join(remote_galaxy_home, 'universe_wsgi.ini')\n            metadata_kwds['config_file'] = remote_system_properties.get('galaxy_config_file', default_config_file)\n            metadata_kwds['dataset_files_path'] = remote_system_properties.get('galaxy_dataset_files_path', None)\n            if LwrJobRunner.__use_remote_datatypes_conf( client ):\n                remote_datatypes_config = remote_system_properties.get('galaxy_datatypes_config_file', None)\n                if not remote_datatypes_config:\n                    log.warn(NO_REMOTE_DATATYPES_CONFIG)\n                    remote_datatypes_config = os.path.join(remote_galaxy_home, 'datatypes_conf.xml')\n                metadata_kwds['datatypes_config'] = remote_datatypes_config\n            else:\n                integrates_datatypes_config = self.app.datatypes_registry.integrated_datatypes_configs\n                # Ensure this file gets pushed out to the remote config dir.\n                job_wrapper.extra_filenames.append(integrates_datatypes_config)\n\n                metadata_kwds['datatypes_config'] = os.path.join(configs_directory, os.path.basename(integrates_datatypes_config))\n        return metadata_kwds\n\n\nclass LwrComputeEnvironment( ComputeEnvironment ):\n\n    def __init__( self, lwr_client, job_wrapper, remote_job_config ):\n        self.lwr_client = lwr_client\n        self.job_wrapper = job_wrapper\n        self.local_path_config = job_wrapper.default_compute_environment()\n        self.unstructured_path_rewrites = {}\n        # job_wrapper.prepare is going to expunge the job backing the following\n        # computations, so precalculate these paths.\n        self._wrapper_input_paths = self.local_path_config.input_paths()\n        self._wrapper_output_paths = self.local_path_config.output_paths()\n        self.path_mapper = PathMapper(lwr_client, remote_job_config, self.local_path_config.working_directory())\n        self._config_directory = remote_job_config[ \"configs_directory\" ]\n        self._working_directory = remote_job_config[ \"working_directory\" ]\n        self._sep = remote_job_config[ \"system_properties\" ][ \"separator\" ]\n        self._tool_dir = remote_job_config[ \"tools_directory\" ]\n        version_path = self.local_path_config.version_path()\n        new_version_path = self.path_mapper.remote_version_path_rewrite(version_path)\n        if new_version_path:\n            version_path = new_version_path\n        self._version_path = version_path\n\n    def output_paths( self ):\n        local_output_paths = self._wrapper_output_paths\n\n        results = []\n        for local_output_path in local_output_paths:\n            wrapper_path = str( local_output_path )\n            remote_path = self.path_mapper.remote_output_path_rewrite( wrapper_path )\n            results.append( self._dataset_path( local_output_path, remote_path ) )\n        return results\n\n    def input_paths( self ):\n        local_input_paths = self._wrapper_input_paths\n\n        results = []\n        for local_input_path in local_input_paths:\n            wrapper_path = str( local_input_path )\n            # This will over-copy in some cases. For instance in the case of task\n            # splitting, this input will be copied even though only the work dir\n            # input will actually be used.\n            remote_path = self.path_mapper.remote_input_path_rewrite( wrapper_path )\n            results.append( self._dataset_path( local_input_path, remote_path ) )\n        return results\n\n    def _dataset_path( self, local_dataset_path, remote_path ):\n        remote_extra_files_path = None\n        if remote_path:\n            remote_extra_files_path = \"%s_files\" % remote_path[ 0:-len( \".dat\" ) ]\n        return local_dataset_path.with_path_for_job( remote_path, remote_extra_files_path )\n\n    def working_directory( self ):\n        return self._working_directory\n\n    def config_directory( self ):\n        return self._config_directory\n\n    def new_file_path( self ):\n        return self.working_directory()  # Problems with doing this?\n\n    def sep( self ):\n        return self._sep\n\n    def version_path( self ):\n        return self._version_path\n\n    def rewriter( self, parameter_value ):\n        unstructured_path_rewrites = self.unstructured_path_rewrites\n        if parameter_value in unstructured_path_rewrites:\n            # Path previously mapped, use previous mapping.\n            return unstructured_path_rewrites[ parameter_value ]\n        if parameter_value in unstructured_path_rewrites.itervalues():\n            # Path is a rewritten remote path (this might never occur,\n            # consider dropping check...)\n            return parameter_value\n\n        rewrite, new_unstructured_path_rewrites = self.path_mapper.check_for_arbitrary_rewrite( parameter_value )\n        if rewrite:\n            unstructured_path_rewrites.update(new_unstructured_path_rewrites)\n            return rewrite\n        else:\n            # Did need to rewrite, use original path or value.\n            return parameter_value\n\n    def unstructured_path_rewriter( self ):\n        return self.rewriter\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kcgthb/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/CommandRegistry.py",
        "source": "# CommandRegistry.py -- Shine commands registry\n# Copyright (C) 2007, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n# Base command class definition\nfrom Base.Command import Command\n\n# Import list of enabled commands (defined in the module __init__.py)\nfrom Shine.Commands import commandList\n\nfrom Exceptions import *\n\n\n# ----------------------------------------------------------------------\n# Command Registry\n# ----------------------------------------------------------------------\n\n\nclass CommandRegistry:\n    \"\"\"Container object to deal with commands.\"\"\"\n\n    def __init__(self):\n        self.cmd_list = []\n        self.cmd_dict = {}\n        self.cmd_optargs = {}\n\n        # Autoload commands\n        self._load()\n\n    def __len__(self):\n        \"Return the number of commands.\"\n        return len(self.cmd_list)\n\n    def __iter__(self):\n        \"Iterate over available commands.\"\n        for cmd in self.cmd_list:\n            yield cmd\n\n    # Private methods\n\n    def _load(self):\n        for cmdobj in commandList:\n            self.register(cmdobj())\n\n    # Public methods\n\n    def get(self, name):\n        return self.cmd_dict[name]\n\n    def register(self, cmd):\n        \"Register a new command.\"\n        assert isinstance(cmd, Command)\n\n        self.cmd_list.append(cmd)\n        self.cmd_dict[cmd.get_name()] = cmd\n\n        # Keep an eye on ALL option arguments, this is to insure a global\n        # options coherency within shine and allow us to intermix options and\n        # command -- see execute() below.\n        opt_len = len(cmd.getopt_string)\n        for i in range(0, opt_len):\n            c = cmd.getopt_string[i]\n            if c == ':':\n                continue\n            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')\n            if c in self.cmd_optargs:\n                assert self.cmd_optargs[c] == has_arg, \"Incoherency in option arguments\"\n            else:\n                self.cmd_optargs[c] = has_arg \n\n    def execute(self, args):\n        \"\"\"\n        Execute a shine script command.\n        \"\"\"\n        # Get command and options. Options and command may be intermixed.\n        command = None\n        new_args = []\n        try:\n            # Find command through options...\n            next_is_arg = False\n            for opt in args:\n                if opt.startswith('-'):\n                    new_args.append(opt)\n                    next_is_arg = self.cmd_optargs[opt[-1:]]\n                elif next_is_arg:\n                    new_args.append(opt)\n                    next_is_arg = False\n                else:\n                    if command:\n                        # Command has already been found, so?\n                        if command.has_subcommand():\n                            # The command supports subcommand: keep it in new_args.\n                            new_args.append(opt)\n                        else:\n                            raise CommandHelpException(\"Syntax error.\", command)\n                    else:\n                        command = self.get(opt)\n                    next_is_arg = False\n        except KeyError, e:\n            raise CommandNotFoundError(opt)\n\n        # Parse\n        command.parse(new_args)\n\n        # Execute\n        return command.execute()\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kcgthb/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Install.py",
        "source": "# Install.py -- File system installation commands\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \n\nfrom Shine.FSUtils import create_lustrefs\n\nfrom Base.Command import Command\nfrom Base.Support.LMF import LMF\nfrom Base.Support.Nodes import Nodes\n\n\nclass Install(Command):\n    \"\"\"\n    shine install -f /path/to/model.lmf\n    \"\"\"\n    \n    def __init__(self):\n        Command.__init__(self)\n\n        self.lmf_support = LMF(self)\n        self.nodes_support = Nodes(self)\n\n    def get_name(self):\n        return \"install\"\n\n    def get_desc(self):\n        return \"Install a new file system.\"\n\n    def execute(self):\n        if not self.opt_m:\n            print \"Bad argument\"\n        else:\n            # Use this Shine.FSUtils convenience function.\n            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),\n                    event_handler=self)\n\n            install_nodes = self.nodes_support.get_nodeset()\n\n            # Install file system configuration files; normally, this should\n            # not be done by the Shine.Lustre.FileSystem object itself, but as\n            # all proxy methods are currently handled by it, it is more\n            # convenient this way...\n            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)\n\n            if install_nodes:\n                nodestr = \" on %s\" %  install_nodes\n            else:\n                nodestr = \"\"\n\n            print \"Configuration files for file system %s have been installed \" \\\n                    \"successfully%s.\" % (fs_conf.get_fs_name(), nodestr)\n\n            if not install_nodes:\n                # Print short file system summary.\n                print\n                print \"Lustre targets summary:\"\n                print \"\\t%d MGT on %s\" % (fs.mgt_count, fs.mgt_servers)\n                print \"\\t%d MDT on %s\" % (fs.mdt_count, fs.mdt_servers)\n                print \"\\t%d OST on %s\" % (fs.ost_count, fs.ost_servers)\n                print\n\n                # Give pointer to next user step.\n                print \"Use `shine format -f %s' to initialize the file system.\" % \\\n                        fs_conf.get_fs_name()\n\n            return 0\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kcgthb/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Mount.py",
        "source": "# Mount.py -- Mount file system on clients\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `mount' command classes.\n\nThe mount command aims to start Lustre filesystem clients.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSClientLiveCommand import FSClientLiveCommand\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\nfrom Exceptions import CommandException\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.FileSystem import *\n\nclass GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_startclient_start(self, node, client):\n        if self.verbose > 1:\n            print \"%s: Mounting %s on %s ...\" % (node, client.fs.fs_name, client.mount_path)\n\n    def ev_startclient_done(self, node, client):\n        if self.verbose > 1:\n            if client.status_info:\n                print \"%s: Mount: %s\" % (node, client.status_info)\n            else:\n                print \"%s: FS %s succesfully mounted on %s\" % (node,\n                        client.fs.fs_name, client.mount_path)\n\n    def ev_startclient_failed(self, node, client, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to mount FS %s on %s: %s\" % \\\n                (node, client.fs.fs_name, client.mount_path, strerr)\n        if rc:\n            print message\n\n\nclass Mount(FSClientLiveCommand):\n    \"\"\"\n    \"\"\"\n\n    def __init__(self):\n        FSClientLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"mount\"\n\n    def get_desc(self):\n        return \"Mount file system clients.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_OK,\n            RECOVERING : RC_FAILURE,\n            OFFLINE : RC_FAILURE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None,\n                    GlobalMountEventHandler(vlevel))\n\n            nodes = self.nodes_support.get_nodeset()\n\n            fs_conf, fs = open_lustrefs(fsname, None,\n                    nodes=nodes,\n                    indexes=None,\n                    event_handler=eh)\n\n            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):\n                raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\\n                        (nodes - fs_conf.get_client_nodes(), fsname))\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status = fs.mount(mount_options=fs_conf.get_mount_options())\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Mount successful.\"\n            elif rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n        return result\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kcgthb/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Preinstall.py",
        "source": "# Preinstall.py -- File system installation commands\n# Copyright (C) 2007, 2008 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\nfrom Shine.FSUtils import create_lustrefs\n\nfrom Base.RemoteCommand import RemoteCommand\nfrom Base.Support.FS import FS\n\nimport os\n\nclass Preinstall(RemoteCommand):\n    \"\"\"\n    shine preinstall -f <filesystem name> -R\n    \"\"\"\n    \n    def __init__(self):\n        RemoteCommand.__init__(self)\n        self.fs_support = FS(self)\n\n    def get_name(self):\n        return \"preinstall\"\n\n    def get_desc(self):\n        return \"Preinstall a new file system.\"\n\n    def is_hidden(self):\n        return True\n\n    def execute(self):\n        try:\n            conf_dir_path = Globals().get_conf_dir()\n            if not os.path.exists(conf_dir_path):\n                os.makedirs(conf_dir_path, 0755)\n        except OSError, ex:\n            print \"OSError\"\n            raise\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kcgthb/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Start.py",
        "source": "# Start.py -- Start file system\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `start' command classes.\n\nThe start command aims to start Lustre filesystem servers or just some\nof the filesystem targets on local or remote servers. It is available\nfor any filesystems previously installed and formatted.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\nfrom Shine.Commands.Status import Status\nfrom Shine.Commands.Tune import Tune\n\n# Command base class\nfrom Base.FSLiveCommand import FSLiveCommand\nfrom Base.FSEventHandler import FSGlobalEventHandler\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\n\n# Shine Proxy Protocol\nfrom Shine.Lustre.Actions.Proxies.ProxyAction import *\nfrom Shine.Lustre.FileSystem import *\n\n\nclass GlobalStartEventHandler(FSGlobalEventHandler):\n\n    def __init__(self, verbose=1):\n        FSGlobalEventHandler.__init__(self, verbose)\n\n    def handle_pre(self, fs):\n        if self.verbose > 0:\n            print \"Starting %d targets on %s\" % (fs.target_count,\n                    fs.target_servers)\n\n    def handle_post(self, fs):\n        if self.verbose > 0:\n            Status.status_view_fs(fs, show_clients=False)\n\n    def ev_starttarget_start(self, node, target):\n        # start/restart timer if needed (we might be running a new runloop)\n        if self.verbose > 1:\n            print \"%s: Starting %s %s (%s)...\" % (node, \\\n                    target.type.upper(), target.get_id(), target.dev)\n        self.update()\n\n    def ev_starttarget_done(self, node, target):\n        self.status_changed = True\n        if self.verbose > 1:\n            if target.status_info:\n                print \"%s: Start of %s %s (%s): %s\" % \\\n                        (node, target.type.upper(), target.get_id(), target.dev,\n                                target.status_info)\n            else:\n                print \"%s: Start of %s %s (%s) succeeded\" % \\\n                        (node, target.type.upper(), target.get_id(), target.dev)\n        self.update()\n\n    def ev_starttarget_failed(self, node, target, rc, message):\n        self.status_changed = True\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to start %s %s (%s): %s\" % \\\n                (node, target.type.upper(), target.get_id(), target.dev,\n                        strerr)\n        if rc:\n            print message\n        self.update()\n\n\nclass LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_starttarget_start(self, node, target):\n        if self.verbose > 1:\n            print \"Starting %s %s (%s)...\" % (target.type.upper(),\n                    target.get_id(), target.dev)\n\n    def ev_starttarget_done(self, node, target):\n        if self.verbose > 1:\n            if target.status_info:\n                print \"Start of %s %s (%s): %s\" % (target.type.upper(),\n                        target.get_id(), target.dev, target.status_info)\n            else:\n                print \"Start of %s %s (%s) succeeded\" % (target.type.upper(),\n                        target.get_id(), target.dev)\n\n    def ev_starttarget_failed(self, node, target, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"Failed to start %s %s (%s): %s\" % (target.type.upper(),\n                target.get_id(), target.dev, strerr)\n        if rc:\n            print message\n\n\nclass Start(FSLiveCommand):\n    \"\"\"\n    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]\n    \"\"\"\n\n    def __init__(self):\n        FSLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"start\"\n\n    def get_desc(self):\n        return \"Start file system servers.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_OK,\n            RECOVERING : RC_OK,\n            OFFLINE : RC_FAILURE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        target = self.target_support.get_target()\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),\n                    GlobalStartEventHandler(vlevel))\n\n            # Open configuration and instantiate a Lustre FS.\n            fs_conf, fs = open_lustrefs(fsname, target,\n                    nodes=self.nodes_support.get_nodeset(),\n                    indexes=self.indexes_support.get_rangeset(),\n                    event_handler=eh)\n\n            # Prepare options...\n            mount_options = {}\n            mount_paths = {}\n            for target_type in [ 'mgt', 'mdt', 'ost' ]:\n                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)\n                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            # Will call the handle_pre() method defined by the event handler.\n            if hasattr(eh, 'pre'):\n                eh.pre(fs)\n                \n            status = fs.start(mount_options=mount_options,\n                              mount_paths=mount_paths)\n\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Start successful.\"\n                tuning = Tune.get_tuning(fs_conf)\n                status = fs.tune(tuning)\n                if status == RUNTIME_ERROR:\n                    rc = RC_RUNTIME_ERROR\n                # XXX improve tuning on start error handling\n\n            if rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n            if hasattr(eh, 'post'):\n                eh.post(fs)\n\n            return rc\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kcgthb/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Status.py",
        "source": "# Status.py -- Check remote filesystem servers and targets status\n# Copyright (C) 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `status' command classes.\n\nThe status command aims to return the real state of a Lustre filesystem\nand its components, depending of the requested \"view\". Status views let\nthe Lustre administrator to either stand back and get a global status\nof the filesystem, or if needed, to enquire about filesystem components\ndetailed states.\n\"\"\"\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSLiveCommand import FSLiveCommand\nfrom Base.CommandRCDefs import *\n# Additional options\nfrom Base.Support.View import View\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n\n# Error handling\nfrom Exceptions import CommandBadParameterError\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Command output formatting\nfrom Shine.Utilities.AsciiTable import *\n\n# Lustre events and errors\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.Disk import *\nfrom Shine.Lustre.FileSystem import *\n\nfrom ClusterShell.NodeSet import NodeSet\n\nimport os\n\n\n(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)\n\n\nclass GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_statustarget_start(self, node, target):\n        pass\n\n    def ev_statustarget_done(self, node, target):\n        pass\n\n    def ev_statustarget_failed(self, node, target, rc, message):\n        print \"%s: Failed to status %s %s (%s)\" % (node, target.type.upper(), \\\n                target.get_id(), target.dev)\n        print \">> %s\" % message\n\n    def ev_statusclient_start(self, node, client):\n        pass\n\n    def ev_statusclient_done(self, node, client):\n        pass\n\n    def ev_statusclient_failed(self, node, client, rc, message):\n        print \"%s: Failed to status of FS %s\" % (node, client.fs.fs_name)\n        print \">> %s\" % message\n\n\nclass Status(FSLiveCommand):\n    \"\"\"\n    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]\n    \"\"\"\n\n    def __init__(self):\n        FSLiveCommand.__init__(self)\n        self.view_support = View(self)\n\n    def get_name(self):\n        return \"status\"\n\n    def get_desc(self):\n        return \"Check for file system target status.\"\n\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_ST_ONLINE,\n            RECOVERING : RC_ST_RECOVERING,\n            OFFLINE : RC_ST_OFFLINE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n\n        result = -1\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        target = self.target_support.get_target()\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))\n\n            fs_conf, fs = open_lustrefs(fsname, target,\n                    nodes=self.nodes_support.get_nodeset(),\n                    indexes=self.indexes_support.get_rangeset(),\n                    event_handler=eh)\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status_flags = STATUS_ANY\n            view = self.view_support.get_view()\n\n            # default view\n            if view is None:\n                view = \"fs\"\n            else:\n                view = view.lower()\n\n            # disable client checks when not requested\n            if view.startswith(\"disk\") or view.startswith(\"target\"):\n                status_flags &= ~STATUS_CLIENTS\n            # disable servers checks when not requested\n            if view.startswith(\"client\"):\n                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)\n\n            statusdict = fs.status(status_flags)\n\n            if RUNTIME_ERROR in statusdict:\n                # get targets that couldn't be checked\n                defect_targets = statusdict[RUNTIME_ERROR]\n\n                for nodes, msg in fs.proxy_errors:\n                    print nodes\n                    print '-' * 15\n                    print msg\n                print\n\n            else:\n                defect_targets = []\n\n            rc = self.fs_status_to_rc(max(statusdict.keys()))\n            if rc > result:\n                result = rc\n\n            if view == \"fs\":\n                self.status_view_fs(fs)\n            elif view.startswith(\"target\"):\n                self.status_view_targets(fs)\n            elif view.startswith(\"disk\"):\n                self.status_view_disks(fs)\n            else:\n                raise CommandBadParameterError(self.view_support.get_view(),\n                        \"fs, targets, disks\")\n        return result\n\n    def status_view_targets(self, fs):\n        \"\"\"\n        View: lustre targets\n        \"\"\"\n        print \"FILESYSTEM TARGETS (%s)\" % fs.fs_name\n\n        # override dict to allow target sorting by index\n        class target_dict(dict):\n            def __lt__(self, other):\n                return self[\"index\"] < other[\"index\"]\n\n        ldic = []\n        for type, (all_targets, enabled_targets) in fs.targets_by_type():\n            for target in enabled_targets:\n\n                if target.state == OFFLINE:\n                    status = \"offline\"\n                elif target.state == TARGET_ERROR:\n                    status = \"ERROR\"\n                elif target.state == RECOVERING:\n                    status = \"recovering %s\" % target.status_info\n                elif target.state == MOUNTED:\n                    status = \"online\"\n                else:\n                    status = \"UNKNOWN\"\n\n                ldic.append(target_dict([[\"target\", target.get_id()],\n                    [\"type\", target.type.upper()],\n                    [\"nodes\", NodeSet.fromlist(target.servers)],\n                    [\"device\", target.dev],\n                    [\"index\", target.index],\n                    [\"status\", status]]))\n\n        ldic.sort()\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        layout.set_column(\"target\", 0, AsciiTableLayout.LEFT, \"target id\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"type\", 1, AsciiTableLayout.LEFT, \"type\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"index\", 2, AsciiTableLayout.RIGHT, \"idx\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"nodes\", 3, AsciiTableLayout.LEFT, \"nodes\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"device\", 4, AsciiTableLayout.LEFT, \"device\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"status\", 5, AsciiTableLayout.LEFT, \"status\",\n                AsciiTableLayout.CENTER)\n\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n\n    def status_view_fs(cls, fs, show_clients=True):\n        \"\"\"\n        View: lustre FS summary\n        \"\"\"\n        ldic = []\n\n        # targets\n        for type, (a_targets, e_targets) in fs.targets_by_type():\n            nodes = NodeSet()\n            t_offline = []\n            t_error = []\n            t_recovering = []\n            t_online = []\n            t_runtime = []\n            t_unknown = []\n            for target in a_targets:\n                nodes.add(target.servers[0])\n\n                # check target status\n                if target.state == OFFLINE:\n                    t_offline.append(target)\n                elif target.state == TARGET_ERROR:\n                    t_error.append(target)\n                elif target.state == RECOVERING:\n                    t_recovering.append(target)\n                elif target.state == MOUNTED:\n                    t_online.append(target)\n                elif target.state == RUNTIME_ERROR:\n                    t_runtime.append(target)\n                else:\n                    t_unknown.append(target)\n\n            status = []\n            if len(t_offline) > 0:\n                status.append(\"offline (%d)\" % len(t_offline))\n            if len(t_error) > 0:\n                status.append(\"ERROR (%d)\" % len(t_error))\n            if len(t_recovering) > 0:\n                status.append(\"recovering (%d) for %s\" % (len(t_recovering),\n                    t_recovering[0].status_info))\n            if len(t_online) > 0:\n                status.append(\"online (%d)\" % len(t_online))\n            if len(t_runtime) > 0:\n                status.append(\"CHECK FAILURE (%d)\" % len(t_runtime))\n            if len(t_unknown) > 0:\n                status.append(\"not checked (%d)\" % len(t_unknown))\n\n            if len(t_unknown) < len(a_targets):\n                ldic.append(dict([[\"type\", \"%s\" % type.upper()],\n                    [\"count\", len(a_targets)], [\"nodes\", nodes],\n                    [\"status\", ', '.join(status)]]))\n\n        # clients\n        if show_clients:\n            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()\n            status = []\n            if c_ign > 0:\n                status.append(\"not checked (%d)\" % c_ign)\n            if c_offline > 0:\n                status.append(\"offline (%d)\" % c_offline)\n            if c_error > 0:\n                status.append(\"ERROR (%d)\" % c_error)\n            if c_runtime > 0:\n                status.append(\"CHECK FAILURE (%d)\" % c_runtime)\n            if c_mounted > 0:\n                status.append(\"mounted (%d)\" % c_mounted)\n\n            ldic.append(dict([[\"type\", \"CLI\"], [\"count\", len(fs.clients)],\n                [\"nodes\", \"%s\" % fs.get_client_servers()], [\"status\", ', '.join(status)]]))\n\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        layout.set_column(\"type\", 0, AsciiTableLayout.CENTER, \"type\", AsciiTableLayout.CENTER)\n        layout.set_column(\"count\", 1, AsciiTableLayout.RIGHT, \"#\", AsciiTableLayout.CENTER)\n        layout.set_column(\"nodes\", 2, AsciiTableLayout.LEFT, \"nodes\", AsciiTableLayout.CENTER)\n        layout.set_column(\"status\", 3, AsciiTableLayout.LEFT, \"status\", AsciiTableLayout.CENTER)\n\n        print \"FILESYSTEM COMPONENTS STATUS (%s)\" % fs.fs_name\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n    status_view_fs = classmethod(status_view_fs)\n\n\n    def status_view_disks(self, fs):\n        \"\"\"\n        View: lustre disks\n        \"\"\"\n\n        print \"FILESYSTEM DISKS (%s)\" % fs.fs_name\n\n        # override dict to allow target sorting by index\n        class target_dict(dict):\n            def __lt__(self, other):\n                return self[\"index\"] < other[\"index\"] \n        ldic = []\n        jdev_col_enabled = False\n        tag_col_enabled = False\n        for type, (all_targets, enabled_targets) in fs.targets_by_type():\n            for target in enabled_targets:\n\n                if target.state == OFFLINE:\n                    status = \"offline\"\n                elif target.state == RECOVERING:\n                    status = \"recovering %s\" % target.status_info\n                elif target.state == MOUNTED:\n                    status = \"online\"\n                elif target.state == TARGET_ERROR:\n                    status = \"ERROR\"\n                elif target.state == RUNTIME_ERROR:\n                    status = \"CHECK FAILURE\"\n                else:\n                    status = \"UNKNOWN\"\n\n                if target.dev_size >= TERA:\n                    dev_size = \"%.1fT\" % (target.dev_size/TERA)\n                elif target.dev_size >= GIGA:\n                    dev_size = \"%.1fG\" % (target.dev_size/GIGA)\n                elif target.dev_size >= MEGA:\n                    dev_size = \"%.1fM\" % (target.dev_size/MEGA)\n                elif target.dev_size >= KILO:\n                    dev_size = \"%.1fK\" % (target.dev_size/KILO)\n                else:\n                    dev_size = \"%d\" % target.dev_size\n\n                if target.jdev:\n                    jdev_col_enabled = True\n                    jdev = target.jdev\n                else:\n                    jdev = \"\"\n\n                if target.tag:\n                    tag_col_enabled = True\n                    tag = target.tag\n                else:\n                    tag = \"\"\n\n                flags = []\n                if target.has_need_index_flag():\n                    flags.append(\"need_index\")\n                if target.has_first_time_flag():\n                    flags.append(\"first_time\")\n                if target.has_update_flag():\n                    flags.append(\"update\")\n                if target.has_rewrite_ldd_flag():\n                    flags.append(\"rewrite_ldd\")\n                if target.has_writeconf_flag():\n                    flags.append(\"writeconf\")\n                if target.has_upgrade14_flag():\n                    flags.append(\"upgrade14\")\n                if target.has_param_flag():\n                    flags.append(\"conf_param\")\n\n                ldic.append(target_dict([\\\n                    [\"nodes\", NodeSet.fromlist(target.servers)],\n                    [\"dev\", target.dev],\n                    [\"size\", dev_size],\n                    [\"jdev\", jdev],\n                    [\"type\", target.type.upper()],\n                    [\"index\", target.index],\n                    [\"tag\", tag],\n                    [\"label\", target.label],\n                    [\"flags\", ' '.join(flags)],\n                    [\"fsname\", target.fs.fs_name],\n                    [\"status\", status]]))\n\n        ldic.sort()\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        i = 0\n        layout.set_column(\"dev\", i, AsciiTableLayout.LEFT, \"device\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"nodes\", i, AsciiTableLayout.LEFT, \"node(s)\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"size\", i, AsciiTableLayout.RIGHT, \"dev size\",\n                AsciiTableLayout.CENTER)\n        if jdev_col_enabled:\n            i += 1\n            layout.set_column(\"jdev\", i, AsciiTableLayout.RIGHT, \"journal device\",\n                    AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"type\", i, AsciiTableLayout.LEFT, \"type\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"index\", i, AsciiTableLayout.RIGHT, \"index\",\n                AsciiTableLayout.CENTER)\n        if tag_col_enabled:\n            i += 1\n            layout.set_column(\"tag\", i, AsciiTableLayout.LEFT, \"tag\",\n                    AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"label\", i, AsciiTableLayout.LEFT, \"label\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"flags\", i, AsciiTableLayout.LEFT, \"ldd flags\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"fsname\", i, AsciiTableLayout.LEFT, \"fsname\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"status\", i, AsciiTableLayout.LEFT, \"status\",\n                AsciiTableLayout.CENTER)\n\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kcgthb/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Umount.py",
        "source": "# Umount.py -- Unmount file system on clients\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `umount' command classes.\n\nThe umount command aims to stop Lustre filesystem clients.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSClientLiveCommand import FSClientLiveCommand\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.FileSystem import *\n\n\nclass GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_stopclient_start(self, node, client):\n        if self.verbose > 1:\n            print \"%s: Unmounting %s on %s ...\" % (node, client.fs.fs_name, client.mount_path)\n\n    def ev_stopclient_done(self, node, client):\n        if self.verbose > 1:\n            if client.status_info:\n                print \"%s: Umount: %s\" % (node, client.status_info)\n            else:\n                print \"%s: FS %s succesfully unmounted from %s\" % (node,\n                        client.fs.fs_name, client.mount_path)\n\n    def ev_stopclient_failed(self, node, client, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to unmount FS %s from %s: %s\" % \\\n                (node, client.fs.fs_name, client.mount_path, strerr)\n        if rc:\n            print message\n\n\nclass Umount(FSClientLiveCommand):\n    \"\"\"\n    shine umount\n    \"\"\"\n\n    def __init__(self):\n        FSClientLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"umount\"\n\n    def get_desc(self):\n        return \"Unmount file system clients.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_FAILURE,\n            RECOVERING : RC_FAILURE,\n            OFFLINE : RC_OK,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None,\n                    GlobalUmountEventHandler(vlevel))\n\n            nodes = self.nodes_support.get_nodeset()\n\n            fs_conf, fs = open_lustrefs(fsname, None,\n                    nodes=nodes,\n                    indexes=None,\n                    event_handler=eh)\n\n            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):\n                raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\\n                        (nodes - fs_conf.get_client_nodes(), fsname))\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status = fs.umount()\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Unmount successful.\"\n            elif rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n        return result\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kcgthb/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Controller.py",
        "source": "# Controller.py -- Controller class\n# Copyright (C) 2007 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Configuration.Globals import Globals\nfrom Commands.CommandRegistry import CommandRegistry\n\nfrom Configuration.ModelFile import ModelFileException\nfrom Configuration.ModelFile import ModelFileIOError\n\nfrom Configuration.Exceptions import ConfigException\nfrom Commands.Exceptions import *\nfrom Commands.Base.CommandRCDefs import *\n\nfrom Lustre.FileSystem import FSRemoteError\n\nfrom ClusterShell.Task import *\nfrom ClusterShell.NodeSet import *\n\nimport getopt\nimport logging\nimport re\nimport sys\n\n\ndef print_csdebug(task, s):\n    m = re.search(\"(\\w+): SHINE:\\d:(\\w+):\", s)\n    if m:\n        print \"%s<pickle>\" % m.group(0)\n    else:\n        print s\n\n\nclass Controller:\n\n    def __init__(self):\n        self.logger = logging.getLogger(\"shine\")\n        #handler = logging.FileHandler(Globals().get_log_file())\n        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')\n        #handler.setFormatter(formatter)\n        #self.logger.addHandler(handler)\n        #self.logger.setLevel(Globals().get_log_level())\n        self.cmds = CommandRegistry()\n\n        #task_self().set_info(\"debug\", True)\n\n        task_self().set_info(\"print_debug\", print_csdebug)\n\n    def usage(self):\n        cmd_maxlen = 0\n\n        for cmd in self.cmds:\n            if not cmd.is_hidden():\n                if len(cmd.get_name()) > cmd_maxlen:\n                    cmd_maxlen = len(cmd.get_name())\n        for cmd in self.cmds:\n            if not cmd.is_hidden():\n                print \"  %-*s %s\" % (cmd_maxlen, cmd.get_name(),\n                    cmd.get_params_desc())\n\n    def print_error(self, errmsg):\n        print >>sys.stderr, \"Error:\", errmsg\n\n    def print_help(self, msg, cmd):\n        if msg:\n            print msg\n            print\n        print \"Usage: %s %s\" % (cmd.get_name(), cmd.get_params_desc())\n        print\n        print cmd.get_desc()\n\n    def run_command(self, cmd_args):\n\n        #self.logger.info(\"running %s\" % cmd_name)\n\n        try:\n            return self.cmds.execute(cmd_args)\n        except getopt.GetoptError, e:\n            print \"Syntax error: %s\" % e\n        except CommandHelpException, e:\n            self.print_help(e.message, e.cmd)\n        except CommandException, e:\n            self.print_error(e.message)\n            return RC_USER_ERROR\n        except ModelFileIOError, e:\n            print \"Error - %s\" % e.message\n        except ModelFileException, e:\n            print \"ModelFile: %s\" % e\n        except ConfigException, e:\n            print \"Configuration: %s\" % e\n            return RC_RUNTIME_ERROR\n        # file system\n        except FSRemoteError, e:\n            self.print_error(e)\n            return e.rc\n        except NodeSetParseError, e:\n            self.print_error(\"%s\" % e)\n            return RC_USER_ERROR\n        except RangeSetParseError, e:\n            self.print_error(\"%s\" % e)\n            return RC_USER_ERROR\n        except KeyError:\n            print \"Error - Unrecognized action\"\n            print\n            raise\n        \n        return 1\n\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/kcgthb/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py",
        "source": "# FSProxyAction.py -- Lustre generic FS proxy action class\n# Copyright (C) 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Globals import Globals\nfrom Shine.Configuration.Configuration import Configuration\n\nfrom ProxyAction import *\n\nfrom ClusterShell.NodeSet import NodeSet\n\n\nclass FSProxyAction(ProxyAction):\n    \"\"\"\n    Generic file system command proxy action class.\n    \"\"\"\n\n    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):\n        ProxyAction.__init__(self)\n        self.fs = fs\n        self.action = action\n        assert isinstance(nodes, NodeSet)\n        self.nodes = nodes\n        self.debug = debug\n        self.targets_type = targets_type\n        self.targets_indexes = targets_indexes\n\n        if self.fs.debug:\n            print \"FSProxyAction %s on %s\" % (action, nodes)\n\n    def launch(self):\n        \"\"\"\n        Launch FS proxy command.\n        \"\"\"\n        command = [\"%s\" % self.progpath]\n        command.append(self.action)\n        command.append(\"-f %s\" % self.fs.fs_name)\n        command.append(\"-R\")\n\n        if self.debug:\n            command.append(\"-d\")\n\n        if self.targets_type:\n            command.append(\"-t %s\" % self.targets_type)\n            if self.targets_indexes:\n                command.append(\"-i %s\" % self.targets_indexes)\n\n        # Schedule cluster command.\n        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)\n\n    def ev_read(self, worker):\n        node, buf = worker.last_read()\n        try:\n            event, params = self._shine_msg_unpack(buf)\n            self.fs._handle_shine_event(event, node, **params)\n        except ProxyActionUnpackError, e:\n            # ignore any non shine messages\n            pass\n\n    def ev_close(self, worker):\n        \"\"\"\n        End of proxy command.\n        \"\"\"\n        # Gather nodes by return code\n        for rc, nodes in worker.iter_retcodes():\n            # rc 127 = command not found\n            # rc 126 = found but not executable\n            if rc >= 126:\n                # Gather these nodes by buffer\n                for buffer, nodes in worker.iter_buffers(nodes):\n                    # Handle proxy command error which rc >= 127 and \n                    self.fs._handle_shine_proxy_error(nodes, \"Remote action %s failed: %s\" % \\\n                            (self.action, buffer))\n\n        self.fs.action_refcnt -= 1\n        if self.fs.action_refcnt == 0:\n            worker.task.abort()\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/cea-hpc/shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/CommandRegistry.py",
        "source": "# CommandRegistry.py -- Shine commands registry\n# Copyright (C) 2007, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n# Base command class definition\nfrom Base.Command import Command\n\n# Import list of enabled commands (defined in the module __init__.py)\nfrom Shine.Commands import commandList\n\nfrom Exceptions import *\n\n\n# ----------------------------------------------------------------------\n# Command Registry\n# ----------------------------------------------------------------------\n\n\nclass CommandRegistry:\n    \"\"\"Container object to deal with commands.\"\"\"\n\n    def __init__(self):\n        self.cmd_list = []\n        self.cmd_dict = {}\n        self.cmd_optargs = {}\n\n        # Autoload commands\n        self._load()\n\n    def __len__(self):\n        \"Return the number of commands.\"\n        return len(self.cmd_list)\n\n    def __iter__(self):\n        \"Iterate over available commands.\"\n        for cmd in self.cmd_list:\n            yield cmd\n\n    # Private methods\n\n    def _load(self):\n        for cmdobj in commandList:\n            self.register(cmdobj())\n\n    # Public methods\n\n    def get(self, name):\n        return self.cmd_dict[name]\n\n    def register(self, cmd):\n        \"Register a new command.\"\n        assert isinstance(cmd, Command)\n\n        self.cmd_list.append(cmd)\n        self.cmd_dict[cmd.get_name()] = cmd\n\n        # Keep an eye on ALL option arguments, this is to insure a global\n        # options coherency within shine and allow us to intermix options and\n        # command -- see execute() below.\n        opt_len = len(cmd.getopt_string)\n        for i in range(0, opt_len):\n            c = cmd.getopt_string[i]\n            if c == ':':\n                continue\n            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')\n            if c in self.cmd_optargs:\n                assert self.cmd_optargs[c] == has_arg, \"Incoherency in option arguments\"\n            else:\n                self.cmd_optargs[c] = has_arg \n\n    def execute(self, args):\n        \"\"\"\n        Execute a shine script command.\n        \"\"\"\n        # Get command and options. Options and command may be intermixed.\n        command = None\n        new_args = []\n        try:\n            # Find command through options...\n            next_is_arg = False\n            for opt in args:\n                if opt.startswith('-'):\n                    new_args.append(opt)\n                    next_is_arg = self.cmd_optargs[opt[-1:]]\n                elif next_is_arg:\n                    new_args.append(opt)\n                    next_is_arg = False\n                else:\n                    if command:\n                        # Command has already been found, so?\n                        if command.has_subcommand():\n                            # The command supports subcommand: keep it in new_args.\n                            new_args.append(opt)\n                        else:\n                            raise CommandHelpException(\"Syntax error.\", command)\n                    else:\n                        command = self.get(opt)\n                    next_is_arg = False\n        except KeyError, e:\n            raise CommandNotFoundError(opt)\n\n        # Parse\n        command.parse(new_args)\n\n        # Execute\n        return command.execute()\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/cea-hpc/shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Install.py",
        "source": "# Install.py -- File system installation commands\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \n\nfrom Shine.FSUtils import create_lustrefs\n\nfrom Base.Command import Command\nfrom Base.Support.LMF import LMF\nfrom Base.Support.Nodes import Nodes\n\n\nclass Install(Command):\n    \"\"\"\n    shine install -f /path/to/model.lmf\n    \"\"\"\n    \n    def __init__(self):\n        Command.__init__(self)\n\n        self.lmf_support = LMF(self)\n        self.nodes_support = Nodes(self)\n\n    def get_name(self):\n        return \"install\"\n\n    def get_desc(self):\n        return \"Install a new file system.\"\n\n    def execute(self):\n        if not self.opt_m:\n            print \"Bad argument\"\n        else:\n            # Use this Shine.FSUtils convenience function.\n            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),\n                    event_handler=self)\n\n            install_nodes = self.nodes_support.get_nodeset()\n\n            # Install file system configuration files; normally, this should\n            # not be done by the Shine.Lustre.FileSystem object itself, but as\n            # all proxy methods are currently handled by it, it is more\n            # convenient this way...\n            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)\n\n            if install_nodes:\n                nodestr = \" on %s\" %  install_nodes\n            else:\n                nodestr = \"\"\n\n            print \"Configuration files for file system %s have been installed \" \\\n                    \"successfully%s.\" % (fs_conf.get_fs_name(), nodestr)\n\n            if not install_nodes:\n                # Print short file system summary.\n                print\n                print \"Lustre targets summary:\"\n                print \"\\t%d MGT on %s\" % (fs.mgt_count, fs.mgt_servers)\n                print \"\\t%d MDT on %s\" % (fs.mdt_count, fs.mdt_servers)\n                print \"\\t%d OST on %s\" % (fs.ost_count, fs.ost_servers)\n                print\n\n                # Give pointer to next user step.\n                print \"Use `shine format -f %s' to initialize the file system.\" % \\\n                        fs_conf.get_fs_name()\n\n            return 0\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/cea-hpc/shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Mount.py",
        "source": "# Mount.py -- Mount file system on clients\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `mount' command classes.\n\nThe mount command aims to start Lustre filesystem clients.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSClientLiveCommand import FSClientLiveCommand\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\nfrom Exceptions import CommandException\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.FileSystem import *\n\nclass GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_startclient_start(self, node, client):\n        if self.verbose > 1:\n            print \"%s: Mounting %s on %s ...\" % (node, client.fs.fs_name, client.mount_path)\n\n    def ev_startclient_done(self, node, client):\n        if self.verbose > 1:\n            if client.status_info:\n                print \"%s: Mount: %s\" % (node, client.status_info)\n            else:\n                print \"%s: FS %s succesfully mounted on %s\" % (node,\n                        client.fs.fs_name, client.mount_path)\n\n    def ev_startclient_failed(self, node, client, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to mount FS %s on %s: %s\" % \\\n                (node, client.fs.fs_name, client.mount_path, strerr)\n        if rc:\n            print message\n\n\nclass Mount(FSClientLiveCommand):\n    \"\"\"\n    \"\"\"\n\n    def __init__(self):\n        FSClientLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"mount\"\n\n    def get_desc(self):\n        return \"Mount file system clients.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_OK,\n            RECOVERING : RC_FAILURE,\n            OFFLINE : RC_FAILURE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None,\n                    GlobalMountEventHandler(vlevel))\n\n            nodes = self.nodes_support.get_nodeset()\n\n            fs_conf, fs = open_lustrefs(fsname, None,\n                    nodes=nodes,\n                    indexes=None,\n                    event_handler=eh)\n\n            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):\n                raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\\n                        (nodes - fs_conf.get_client_nodes(), fsname))\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status = fs.mount(mount_options=fs_conf.get_mount_options())\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Mount successful.\"\n            elif rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n        return result\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/cea-hpc/shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Preinstall.py",
        "source": "# Preinstall.py -- File system installation commands\n# Copyright (C) 2007, 2008 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\nfrom Shine.FSUtils import create_lustrefs\n\nfrom Base.RemoteCommand import RemoteCommand\nfrom Base.Support.FS import FS\n\nimport os\n\nclass Preinstall(RemoteCommand):\n    \"\"\"\n    shine preinstall -f <filesystem name> -R\n    \"\"\"\n    \n    def __init__(self):\n        RemoteCommand.__init__(self)\n        self.fs_support = FS(self)\n\n    def get_name(self):\n        return \"preinstall\"\n\n    def get_desc(self):\n        return \"Preinstall a new file system.\"\n\n    def is_hidden(self):\n        return True\n\n    def execute(self):\n        try:\n            conf_dir_path = Globals().get_conf_dir()\n            if not os.path.exists(conf_dir_path):\n                os.makedirs(conf_dir_path, 0755)\n        except OSError, ex:\n            print \"OSError\"\n            raise\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/cea-hpc/shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Start.py",
        "source": "# Start.py -- Start file system\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `start' command classes.\n\nThe start command aims to start Lustre filesystem servers or just some\nof the filesystem targets on local or remote servers. It is available\nfor any filesystems previously installed and formatted.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\nfrom Shine.Commands.Status import Status\nfrom Shine.Commands.Tune import Tune\n\n# Command base class\nfrom Base.FSLiveCommand import FSLiveCommand\nfrom Base.FSEventHandler import FSGlobalEventHandler\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\n\n# Shine Proxy Protocol\nfrom Shine.Lustre.Actions.Proxies.ProxyAction import *\nfrom Shine.Lustre.FileSystem import *\n\n\nclass GlobalStartEventHandler(FSGlobalEventHandler):\n\n    def __init__(self, verbose=1):\n        FSGlobalEventHandler.__init__(self, verbose)\n\n    def handle_pre(self, fs):\n        if self.verbose > 0:\n            print \"Starting %d targets on %s\" % (fs.target_count,\n                    fs.target_servers)\n\n    def handle_post(self, fs):\n        if self.verbose > 0:\n            Status.status_view_fs(fs, show_clients=False)\n\n    def ev_starttarget_start(self, node, target):\n        # start/restart timer if needed (we might be running a new runloop)\n        if self.verbose > 1:\n            print \"%s: Starting %s %s (%s)...\" % (node, \\\n                    target.type.upper(), target.get_id(), target.dev)\n        self.update()\n\n    def ev_starttarget_done(self, node, target):\n        self.status_changed = True\n        if self.verbose > 1:\n            if target.status_info:\n                print \"%s: Start of %s %s (%s): %s\" % \\\n                        (node, target.type.upper(), target.get_id(), target.dev,\n                                target.status_info)\n            else:\n                print \"%s: Start of %s %s (%s) succeeded\" % \\\n                        (node, target.type.upper(), target.get_id(), target.dev)\n        self.update()\n\n    def ev_starttarget_failed(self, node, target, rc, message):\n        self.status_changed = True\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to start %s %s (%s): %s\" % \\\n                (node, target.type.upper(), target.get_id(), target.dev,\n                        strerr)\n        if rc:\n            print message\n        self.update()\n\n\nclass LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_starttarget_start(self, node, target):\n        if self.verbose > 1:\n            print \"Starting %s %s (%s)...\" % (target.type.upper(),\n                    target.get_id(), target.dev)\n\n    def ev_starttarget_done(self, node, target):\n        if self.verbose > 1:\n            if target.status_info:\n                print \"Start of %s %s (%s): %s\" % (target.type.upper(),\n                        target.get_id(), target.dev, target.status_info)\n            else:\n                print \"Start of %s %s (%s) succeeded\" % (target.type.upper(),\n                        target.get_id(), target.dev)\n\n    def ev_starttarget_failed(self, node, target, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"Failed to start %s %s (%s): %s\" % (target.type.upper(),\n                target.get_id(), target.dev, strerr)\n        if rc:\n            print message\n\n\nclass Start(FSLiveCommand):\n    \"\"\"\n    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]\n    \"\"\"\n\n    def __init__(self):\n        FSLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"start\"\n\n    def get_desc(self):\n        return \"Start file system servers.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_OK,\n            RECOVERING : RC_OK,\n            OFFLINE : RC_FAILURE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        target = self.target_support.get_target()\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),\n                    GlobalStartEventHandler(vlevel))\n\n            # Open configuration and instantiate a Lustre FS.\n            fs_conf, fs = open_lustrefs(fsname, target,\n                    nodes=self.nodes_support.get_nodeset(),\n                    indexes=self.indexes_support.get_rangeset(),\n                    event_handler=eh)\n\n            # Prepare options...\n            mount_options = {}\n            mount_paths = {}\n            for target_type in [ 'mgt', 'mdt', 'ost' ]:\n                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)\n                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            # Will call the handle_pre() method defined by the event handler.\n            if hasattr(eh, 'pre'):\n                eh.pre(fs)\n                \n            status = fs.start(mount_options=mount_options,\n                              mount_paths=mount_paths)\n\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Start successful.\"\n                tuning = Tune.get_tuning(fs_conf)\n                status = fs.tune(tuning)\n                if status == RUNTIME_ERROR:\n                    rc = RC_RUNTIME_ERROR\n                # XXX improve tuning on start error handling\n\n            if rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n            if hasattr(eh, 'post'):\n                eh.post(fs)\n\n            return rc\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/cea-hpc/shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Status.py",
        "source": "# Status.py -- Check remote filesystem servers and targets status\n# Copyright (C) 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `status' command classes.\n\nThe status command aims to return the real state of a Lustre filesystem\nand its components, depending of the requested \"view\". Status views let\nthe Lustre administrator to either stand back and get a global status\nof the filesystem, or if needed, to enquire about filesystem components\ndetailed states.\n\"\"\"\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSLiveCommand import FSLiveCommand\nfrom Base.CommandRCDefs import *\n# Additional options\nfrom Base.Support.View import View\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n\n# Error handling\nfrom Exceptions import CommandBadParameterError\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Command output formatting\nfrom Shine.Utilities.AsciiTable import *\n\n# Lustre events and errors\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.Disk import *\nfrom Shine.Lustre.FileSystem import *\n\nfrom ClusterShell.NodeSet import NodeSet\n\nimport os\n\n\n(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)\n\n\nclass GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_statustarget_start(self, node, target):\n        pass\n\n    def ev_statustarget_done(self, node, target):\n        pass\n\n    def ev_statustarget_failed(self, node, target, rc, message):\n        print \"%s: Failed to status %s %s (%s)\" % (node, target.type.upper(), \\\n                target.get_id(), target.dev)\n        print \">> %s\" % message\n\n    def ev_statusclient_start(self, node, client):\n        pass\n\n    def ev_statusclient_done(self, node, client):\n        pass\n\n    def ev_statusclient_failed(self, node, client, rc, message):\n        print \"%s: Failed to status of FS %s\" % (node, client.fs.fs_name)\n        print \">> %s\" % message\n\n\nclass Status(FSLiveCommand):\n    \"\"\"\n    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]\n    \"\"\"\n\n    def __init__(self):\n        FSLiveCommand.__init__(self)\n        self.view_support = View(self)\n\n    def get_name(self):\n        return \"status\"\n\n    def get_desc(self):\n        return \"Check for file system target status.\"\n\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_ST_ONLINE,\n            RECOVERING : RC_ST_RECOVERING,\n            OFFLINE : RC_ST_OFFLINE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n\n        result = -1\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        target = self.target_support.get_target()\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))\n\n            fs_conf, fs = open_lustrefs(fsname, target,\n                    nodes=self.nodes_support.get_nodeset(),\n                    indexes=self.indexes_support.get_rangeset(),\n                    event_handler=eh)\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status_flags = STATUS_ANY\n            view = self.view_support.get_view()\n\n            # default view\n            if view is None:\n                view = \"fs\"\n            else:\n                view = view.lower()\n\n            # disable client checks when not requested\n            if view.startswith(\"disk\") or view.startswith(\"target\"):\n                status_flags &= ~STATUS_CLIENTS\n            # disable servers checks when not requested\n            if view.startswith(\"client\"):\n                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)\n\n            statusdict = fs.status(status_flags)\n\n            if RUNTIME_ERROR in statusdict:\n                # get targets that couldn't be checked\n                defect_targets = statusdict[RUNTIME_ERROR]\n\n                for nodes, msg in fs.proxy_errors:\n                    print nodes\n                    print '-' * 15\n                    print msg\n                print\n\n            else:\n                defect_targets = []\n\n            rc = self.fs_status_to_rc(max(statusdict.keys()))\n            if rc > result:\n                result = rc\n\n            if view == \"fs\":\n                self.status_view_fs(fs)\n            elif view.startswith(\"target\"):\n                self.status_view_targets(fs)\n            elif view.startswith(\"disk\"):\n                self.status_view_disks(fs)\n            else:\n                raise CommandBadParameterError(self.view_support.get_view(),\n                        \"fs, targets, disks\")\n        return result\n\n    def status_view_targets(self, fs):\n        \"\"\"\n        View: lustre targets\n        \"\"\"\n        print \"FILESYSTEM TARGETS (%s)\" % fs.fs_name\n\n        # override dict to allow target sorting by index\n        class target_dict(dict):\n            def __lt__(self, other):\n                return self[\"index\"] < other[\"index\"]\n\n        ldic = []\n        for type, (all_targets, enabled_targets) in fs.targets_by_type():\n            for target in enabled_targets:\n\n                if target.state == OFFLINE:\n                    status = \"offline\"\n                elif target.state == TARGET_ERROR:\n                    status = \"ERROR\"\n                elif target.state == RECOVERING:\n                    status = \"recovering %s\" % target.status_info\n                elif target.state == MOUNTED:\n                    status = \"online\"\n                else:\n                    status = \"UNKNOWN\"\n\n                ldic.append(target_dict([[\"target\", target.get_id()],\n                    [\"type\", target.type.upper()],\n                    [\"nodes\", NodeSet.fromlist(target.servers)],\n                    [\"device\", target.dev],\n                    [\"index\", target.index],\n                    [\"status\", status]]))\n\n        ldic.sort()\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        layout.set_column(\"target\", 0, AsciiTableLayout.LEFT, \"target id\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"type\", 1, AsciiTableLayout.LEFT, \"type\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"index\", 2, AsciiTableLayout.RIGHT, \"idx\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"nodes\", 3, AsciiTableLayout.LEFT, \"nodes\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"device\", 4, AsciiTableLayout.LEFT, \"device\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"status\", 5, AsciiTableLayout.LEFT, \"status\",\n                AsciiTableLayout.CENTER)\n\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n\n    def status_view_fs(cls, fs, show_clients=True):\n        \"\"\"\n        View: lustre FS summary\n        \"\"\"\n        ldic = []\n\n        # targets\n        for type, (a_targets, e_targets) in fs.targets_by_type():\n            nodes = NodeSet()\n            t_offline = []\n            t_error = []\n            t_recovering = []\n            t_online = []\n            t_runtime = []\n            t_unknown = []\n            for target in a_targets:\n                nodes.add(target.servers[0])\n\n                # check target status\n                if target.state == OFFLINE:\n                    t_offline.append(target)\n                elif target.state == TARGET_ERROR:\n                    t_error.append(target)\n                elif target.state == RECOVERING:\n                    t_recovering.append(target)\n                elif target.state == MOUNTED:\n                    t_online.append(target)\n                elif target.state == RUNTIME_ERROR:\n                    t_runtime.append(target)\n                else:\n                    t_unknown.append(target)\n\n            status = []\n            if len(t_offline) > 0:\n                status.append(\"offline (%d)\" % len(t_offline))\n            if len(t_error) > 0:\n                status.append(\"ERROR (%d)\" % len(t_error))\n            if len(t_recovering) > 0:\n                status.append(\"recovering (%d) for %s\" % (len(t_recovering),\n                    t_recovering[0].status_info))\n            if len(t_online) > 0:\n                status.append(\"online (%d)\" % len(t_online))\n            if len(t_runtime) > 0:\n                status.append(\"CHECK FAILURE (%d)\" % len(t_runtime))\n            if len(t_unknown) > 0:\n                status.append(\"not checked (%d)\" % len(t_unknown))\n\n            if len(t_unknown) < len(a_targets):\n                ldic.append(dict([[\"type\", \"%s\" % type.upper()],\n                    [\"count\", len(a_targets)], [\"nodes\", nodes],\n                    [\"status\", ', '.join(status)]]))\n\n        # clients\n        if show_clients:\n            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()\n            status = []\n            if c_ign > 0:\n                status.append(\"not checked (%d)\" % c_ign)\n            if c_offline > 0:\n                status.append(\"offline (%d)\" % c_offline)\n            if c_error > 0:\n                status.append(\"ERROR (%d)\" % c_error)\n            if c_runtime > 0:\n                status.append(\"CHECK FAILURE (%d)\" % c_runtime)\n            if c_mounted > 0:\n                status.append(\"mounted (%d)\" % c_mounted)\n\n            ldic.append(dict([[\"type\", \"CLI\"], [\"count\", len(fs.clients)],\n                [\"nodes\", \"%s\" % fs.get_client_servers()], [\"status\", ', '.join(status)]]))\n\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        layout.set_column(\"type\", 0, AsciiTableLayout.CENTER, \"type\", AsciiTableLayout.CENTER)\n        layout.set_column(\"count\", 1, AsciiTableLayout.RIGHT, \"#\", AsciiTableLayout.CENTER)\n        layout.set_column(\"nodes\", 2, AsciiTableLayout.LEFT, \"nodes\", AsciiTableLayout.CENTER)\n        layout.set_column(\"status\", 3, AsciiTableLayout.LEFT, \"status\", AsciiTableLayout.CENTER)\n\n        print \"FILESYSTEM COMPONENTS STATUS (%s)\" % fs.fs_name\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n    status_view_fs = classmethod(status_view_fs)\n\n\n    def status_view_disks(self, fs):\n        \"\"\"\n        View: lustre disks\n        \"\"\"\n\n        print \"FILESYSTEM DISKS (%s)\" % fs.fs_name\n\n        # override dict to allow target sorting by index\n        class target_dict(dict):\n            def __lt__(self, other):\n                return self[\"index\"] < other[\"index\"] \n        ldic = []\n        jdev_col_enabled = False\n        tag_col_enabled = False\n        for type, (all_targets, enabled_targets) in fs.targets_by_type():\n            for target in enabled_targets:\n\n                if target.state == OFFLINE:\n                    status = \"offline\"\n                elif target.state == RECOVERING:\n                    status = \"recovering %s\" % target.status_info\n                elif target.state == MOUNTED:\n                    status = \"online\"\n                elif target.state == TARGET_ERROR:\n                    status = \"ERROR\"\n                elif target.state == RUNTIME_ERROR:\n                    status = \"CHECK FAILURE\"\n                else:\n                    status = \"UNKNOWN\"\n\n                if target.dev_size >= TERA:\n                    dev_size = \"%.1fT\" % (target.dev_size/TERA)\n                elif target.dev_size >= GIGA:\n                    dev_size = \"%.1fG\" % (target.dev_size/GIGA)\n                elif target.dev_size >= MEGA:\n                    dev_size = \"%.1fM\" % (target.dev_size/MEGA)\n                elif target.dev_size >= KILO:\n                    dev_size = \"%.1fK\" % (target.dev_size/KILO)\n                else:\n                    dev_size = \"%d\" % target.dev_size\n\n                if target.jdev:\n                    jdev_col_enabled = True\n                    jdev = target.jdev\n                else:\n                    jdev = \"\"\n\n                if target.tag:\n                    tag_col_enabled = True\n                    tag = target.tag\n                else:\n                    tag = \"\"\n\n                flags = []\n                if target.has_need_index_flag():\n                    flags.append(\"need_index\")\n                if target.has_first_time_flag():\n                    flags.append(\"first_time\")\n                if target.has_update_flag():\n                    flags.append(\"update\")\n                if target.has_rewrite_ldd_flag():\n                    flags.append(\"rewrite_ldd\")\n                if target.has_writeconf_flag():\n                    flags.append(\"writeconf\")\n                if target.has_upgrade14_flag():\n                    flags.append(\"upgrade14\")\n                if target.has_param_flag():\n                    flags.append(\"conf_param\")\n\n                ldic.append(target_dict([\\\n                    [\"nodes\", NodeSet.fromlist(target.servers)],\n                    [\"dev\", target.dev],\n                    [\"size\", dev_size],\n                    [\"jdev\", jdev],\n                    [\"type\", target.type.upper()],\n                    [\"index\", target.index],\n                    [\"tag\", tag],\n                    [\"label\", target.label],\n                    [\"flags\", ' '.join(flags)],\n                    [\"fsname\", target.fs.fs_name],\n                    [\"status\", status]]))\n\n        ldic.sort()\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        i = 0\n        layout.set_column(\"dev\", i, AsciiTableLayout.LEFT, \"device\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"nodes\", i, AsciiTableLayout.LEFT, \"node(s)\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"size\", i, AsciiTableLayout.RIGHT, \"dev size\",\n                AsciiTableLayout.CENTER)\n        if jdev_col_enabled:\n            i += 1\n            layout.set_column(\"jdev\", i, AsciiTableLayout.RIGHT, \"journal device\",\n                    AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"type\", i, AsciiTableLayout.LEFT, \"type\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"index\", i, AsciiTableLayout.RIGHT, \"index\",\n                AsciiTableLayout.CENTER)\n        if tag_col_enabled:\n            i += 1\n            layout.set_column(\"tag\", i, AsciiTableLayout.LEFT, \"tag\",\n                    AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"label\", i, AsciiTableLayout.LEFT, \"label\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"flags\", i, AsciiTableLayout.LEFT, \"ldd flags\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"fsname\", i, AsciiTableLayout.LEFT, \"fsname\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"status\", i, AsciiTableLayout.LEFT, \"status\",\n                AsciiTableLayout.CENTER)\n\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/cea-hpc/shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Umount.py",
        "source": "# Umount.py -- Unmount file system on clients\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `umount' command classes.\n\nThe umount command aims to stop Lustre filesystem clients.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSClientLiveCommand import FSClientLiveCommand\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.FileSystem import *\n\n\nclass GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_stopclient_start(self, node, client):\n        if self.verbose > 1:\n            print \"%s: Unmounting %s on %s ...\" % (node, client.fs.fs_name, client.mount_path)\n\n    def ev_stopclient_done(self, node, client):\n        if self.verbose > 1:\n            if client.status_info:\n                print \"%s: Umount: %s\" % (node, client.status_info)\n            else:\n                print \"%s: FS %s succesfully unmounted from %s\" % (node,\n                        client.fs.fs_name, client.mount_path)\n\n    def ev_stopclient_failed(self, node, client, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to unmount FS %s from %s: %s\" % \\\n                (node, client.fs.fs_name, client.mount_path, strerr)\n        if rc:\n            print message\n\n\nclass Umount(FSClientLiveCommand):\n    \"\"\"\n    shine umount\n    \"\"\"\n\n    def __init__(self):\n        FSClientLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"umount\"\n\n    def get_desc(self):\n        return \"Unmount file system clients.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_FAILURE,\n            RECOVERING : RC_FAILURE,\n            OFFLINE : RC_OK,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None,\n                    GlobalUmountEventHandler(vlevel))\n\n            nodes = self.nodes_support.get_nodeset()\n\n            fs_conf, fs = open_lustrefs(fsname, None,\n                    nodes=nodes,\n                    indexes=None,\n                    event_handler=eh)\n\n            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):\n                raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\\n                        (nodes - fs_conf.get_client_nodes(), fsname))\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status = fs.umount()\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Unmount successful.\"\n            elif rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n        return result\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/cea-hpc/shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Controller.py",
        "source": "# Controller.py -- Controller class\n# Copyright (C) 2007 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Configuration.Globals import Globals\nfrom Commands.CommandRegistry import CommandRegistry\n\nfrom Configuration.ModelFile import ModelFileException\nfrom Configuration.ModelFile import ModelFileIOError\n\nfrom Configuration.Exceptions import ConfigException\nfrom Commands.Exceptions import *\nfrom Commands.Base.CommandRCDefs import *\n\nfrom Lustre.FileSystem import FSRemoteError\n\nfrom ClusterShell.Task import *\nfrom ClusterShell.NodeSet import *\n\nimport getopt\nimport logging\nimport re\nimport sys\n\n\ndef print_csdebug(task, s):\n    m = re.search(\"(\\w+): SHINE:\\d:(\\w+):\", s)\n    if m:\n        print \"%s<pickle>\" % m.group(0)\n    else:\n        print s\n\n\nclass Controller:\n\n    def __init__(self):\n        self.logger = logging.getLogger(\"shine\")\n        #handler = logging.FileHandler(Globals().get_log_file())\n        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')\n        #handler.setFormatter(formatter)\n        #self.logger.addHandler(handler)\n        #self.logger.setLevel(Globals().get_log_level())\n        self.cmds = CommandRegistry()\n\n        #task_self().set_info(\"debug\", True)\n\n        task_self().set_info(\"print_debug\", print_csdebug)\n\n    def usage(self):\n        cmd_maxlen = 0\n\n        for cmd in self.cmds:\n            if not cmd.is_hidden():\n                if len(cmd.get_name()) > cmd_maxlen:\n                    cmd_maxlen = len(cmd.get_name())\n        for cmd in self.cmds:\n            if not cmd.is_hidden():\n                print \"  %-*s %s\" % (cmd_maxlen, cmd.get_name(),\n                    cmd.get_params_desc())\n\n    def print_error(self, errmsg):\n        print >>sys.stderr, \"Error:\", errmsg\n\n    def print_help(self, msg, cmd):\n        if msg:\n            print msg\n            print\n        print \"Usage: %s %s\" % (cmd.get_name(), cmd.get_params_desc())\n        print\n        print cmd.get_desc()\n\n    def run_command(self, cmd_args):\n\n        #self.logger.info(\"running %s\" % cmd_name)\n\n        try:\n            return self.cmds.execute(cmd_args)\n        except getopt.GetoptError, e:\n            print \"Syntax error: %s\" % e\n        except CommandHelpException, e:\n            self.print_help(e.message, e.cmd)\n        except CommandException, e:\n            self.print_error(e.message)\n            return RC_USER_ERROR\n        except ModelFileIOError, e:\n            print \"Error - %s\" % e.message\n        except ModelFileException, e:\n            print \"ModelFile: %s\" % e\n        except ConfigException, e:\n            print \"Configuration: %s\" % e\n            return RC_RUNTIME_ERROR\n        # file system\n        except FSRemoteError, e:\n            self.print_error(e)\n            return e.rc\n        except NodeSetParseError, e:\n            self.print_error(\"%s\" % e)\n            return RC_USER_ERROR\n        except RangeSetParseError, e:\n            self.print_error(\"%s\" % e)\n            return RC_USER_ERROR\n        except KeyError:\n            print \"Error - Unrecognized action\"\n            print\n            raise\n        \n        return 1\n\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/cea-hpc/shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py",
        "source": "# FSProxyAction.py -- Lustre generic FS proxy action class\n# Copyright (C) 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Globals import Globals\nfrom Shine.Configuration.Configuration import Configuration\n\nfrom ProxyAction import *\n\nfrom ClusterShell.NodeSet import NodeSet\n\n\nclass FSProxyAction(ProxyAction):\n    \"\"\"\n    Generic file system command proxy action class.\n    \"\"\"\n\n    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):\n        ProxyAction.__init__(self)\n        self.fs = fs\n        self.action = action\n        assert isinstance(nodes, NodeSet)\n        self.nodes = nodes\n        self.debug = debug\n        self.targets_type = targets_type\n        self.targets_indexes = targets_indexes\n\n        if self.fs.debug:\n            print \"FSProxyAction %s on %s\" % (action, nodes)\n\n    def launch(self):\n        \"\"\"\n        Launch FS proxy command.\n        \"\"\"\n        command = [\"%s\" % self.progpath]\n        command.append(self.action)\n        command.append(\"-f %s\" % self.fs.fs_name)\n        command.append(\"-R\")\n\n        if self.debug:\n            command.append(\"-d\")\n\n        if self.targets_type:\n            command.append(\"-t %s\" % self.targets_type)\n            if self.targets_indexes:\n                command.append(\"-i %s\" % self.targets_indexes)\n\n        # Schedule cluster command.\n        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)\n\n    def ev_read(self, worker):\n        node, buf = worker.last_read()\n        try:\n            event, params = self._shine_msg_unpack(buf)\n            self.fs._handle_shine_event(event, node, **params)\n        except ProxyActionUnpackError, e:\n            # ignore any non shine messages\n            pass\n\n    def ev_close(self, worker):\n        \"\"\"\n        End of proxy command.\n        \"\"\"\n        # Gather nodes by return code\n        for rc, nodes in worker.iter_retcodes():\n            # rc 127 = command not found\n            # rc 126 = found but not executable\n            if rc >= 126:\n                # Gather these nodes by buffer\n                for buffer, nodes in worker.iter_buffers(nodes):\n                    # Handle proxy command error which rc >= 127 and \n                    self.fs._handle_shine_proxy_error(nodes, \"Remote action %s failed: %s\" % \\\n                            (self.action, buffer))\n\n        self.fs.action_refcnt -= 1\n        if self.fs.action_refcnt == 0:\n            worker.task.abort()\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/bullxpfs/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/CommandRegistry.py",
        "source": "# CommandRegistry.py -- Shine commands registry\n# Copyright (C) 2007, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n# Base command class definition\nfrom Base.Command import Command\n\n# Import list of enabled commands (defined in the module __init__.py)\nfrom Shine.Commands import commandList\n\nfrom Exceptions import *\n\n\n# ----------------------------------------------------------------------\n# Command Registry\n# ----------------------------------------------------------------------\n\n\nclass CommandRegistry:\n    \"\"\"Container object to deal with commands.\"\"\"\n\n    def __init__(self):\n        self.cmd_list = []\n        self.cmd_dict = {}\n        self.cmd_optargs = {}\n\n        # Autoload commands\n        self._load()\n\n    def __len__(self):\n        \"Return the number of commands.\"\n        return len(self.cmd_list)\n\n    def __iter__(self):\n        \"Iterate over available commands.\"\n        for cmd in self.cmd_list:\n            yield cmd\n\n    # Private methods\n\n    def _load(self):\n        for cmdobj in commandList:\n            self.register(cmdobj())\n\n    # Public methods\n\n    def get(self, name):\n        return self.cmd_dict[name]\n\n    def register(self, cmd):\n        \"Register a new command.\"\n        assert isinstance(cmd, Command)\n\n        self.cmd_list.append(cmd)\n        self.cmd_dict[cmd.get_name()] = cmd\n\n        # Keep an eye on ALL option arguments, this is to insure a global\n        # options coherency within shine and allow us to intermix options and\n        # command -- see execute() below.\n        opt_len = len(cmd.getopt_string)\n        for i in range(0, opt_len):\n            c = cmd.getopt_string[i]\n            if c == ':':\n                continue\n            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')\n            if c in self.cmd_optargs:\n                assert self.cmd_optargs[c] == has_arg, \"Incoherency in option arguments\"\n            else:\n                self.cmd_optargs[c] = has_arg \n\n    def execute(self, args):\n        \"\"\"\n        Execute a shine script command.\n        \"\"\"\n        # Get command and options. Options and command may be intermixed.\n        command = None\n        new_args = []\n        try:\n            # Find command through options...\n            next_is_arg = False\n            for opt in args:\n                if opt.startswith('-'):\n                    new_args.append(opt)\n                    next_is_arg = self.cmd_optargs[opt[-1:]]\n                elif next_is_arg:\n                    new_args.append(opt)\n                    next_is_arg = False\n                else:\n                    if command:\n                        # Command has already been found, so?\n                        if command.has_subcommand():\n                            # The command supports subcommand: keep it in new_args.\n                            new_args.append(opt)\n                        else:\n                            raise CommandHelpException(\"Syntax error.\", command)\n                    else:\n                        command = self.get(opt)\n                    next_is_arg = False\n        except KeyError, e:\n            raise CommandNotFoundError(opt)\n\n        # Parse\n        command.parse(new_args)\n\n        # Execute\n        return command.execute()\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/bullxpfs/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Install.py",
        "source": "# Install.py -- File system installation commands\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \n\nfrom Shine.FSUtils import create_lustrefs\n\nfrom Base.Command import Command\nfrom Base.Support.LMF import LMF\nfrom Base.Support.Nodes import Nodes\n\n\nclass Install(Command):\n    \"\"\"\n    shine install -f /path/to/model.lmf\n    \"\"\"\n    \n    def __init__(self):\n        Command.__init__(self)\n\n        self.lmf_support = LMF(self)\n        self.nodes_support = Nodes(self)\n\n    def get_name(self):\n        return \"install\"\n\n    def get_desc(self):\n        return \"Install a new file system.\"\n\n    def execute(self):\n        if not self.opt_m:\n            print \"Bad argument\"\n        else:\n            # Use this Shine.FSUtils convenience function.\n            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),\n                    event_handler=self)\n\n            install_nodes = self.nodes_support.get_nodeset()\n\n            # Install file system configuration files; normally, this should\n            # not be done by the Shine.Lustre.FileSystem object itself, but as\n            # all proxy methods are currently handled by it, it is more\n            # convenient this way...\n            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)\n\n            if install_nodes:\n                nodestr = \" on %s\" %  install_nodes\n            else:\n                nodestr = \"\"\n\n            print \"Configuration files for file system %s have been installed \" \\\n                    \"successfully%s.\" % (fs_conf.get_fs_name(), nodestr)\n\n            if not install_nodes:\n                # Print short file system summary.\n                print\n                print \"Lustre targets summary:\"\n                print \"\\t%d MGT on %s\" % (fs.mgt_count, fs.mgt_servers)\n                print \"\\t%d MDT on %s\" % (fs.mdt_count, fs.mdt_servers)\n                print \"\\t%d OST on %s\" % (fs.ost_count, fs.ost_servers)\n                print\n\n                # Give pointer to next user step.\n                print \"Use `shine format -f %s' to initialize the file system.\" % \\\n                        fs_conf.get_fs_name()\n\n            return 0\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/bullxpfs/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Mount.py",
        "source": "# Mount.py -- Mount file system on clients\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `mount' command classes.\n\nThe mount command aims to start Lustre filesystem clients.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSClientLiveCommand import FSClientLiveCommand\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\nfrom Exceptions import CommandException\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.FileSystem import *\n\nclass GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_startclient_start(self, node, client):\n        if self.verbose > 1:\n            print \"%s: Mounting %s on %s ...\" % (node, client.fs.fs_name, client.mount_path)\n\n    def ev_startclient_done(self, node, client):\n        if self.verbose > 1:\n            if client.status_info:\n                print \"%s: Mount: %s\" % (node, client.status_info)\n            else:\n                print \"%s: FS %s succesfully mounted on %s\" % (node,\n                        client.fs.fs_name, client.mount_path)\n\n    def ev_startclient_failed(self, node, client, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to mount FS %s on %s: %s\" % \\\n                (node, client.fs.fs_name, client.mount_path, strerr)\n        if rc:\n            print message\n\n\nclass Mount(FSClientLiveCommand):\n    \"\"\"\n    \"\"\"\n\n    def __init__(self):\n        FSClientLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"mount\"\n\n    def get_desc(self):\n        return \"Mount file system clients.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_OK,\n            RECOVERING : RC_FAILURE,\n            OFFLINE : RC_FAILURE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None,\n                    GlobalMountEventHandler(vlevel))\n\n            nodes = self.nodes_support.get_nodeset()\n\n            fs_conf, fs = open_lustrefs(fsname, None,\n                    nodes=nodes,\n                    indexes=None,\n                    event_handler=eh)\n\n            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):\n                raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\\n                        (nodes - fs_conf.get_client_nodes(), fsname))\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status = fs.mount(mount_options=fs_conf.get_mount_options())\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Mount successful.\"\n            elif rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n        return result\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/bullxpfs/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Preinstall.py",
        "source": "# Preinstall.py -- File system installation commands\n# Copyright (C) 2007, 2008 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\nfrom Shine.FSUtils import create_lustrefs\n\nfrom Base.RemoteCommand import RemoteCommand\nfrom Base.Support.FS import FS\n\nimport os\n\nclass Preinstall(RemoteCommand):\n    \"\"\"\n    shine preinstall -f <filesystem name> -R\n    \"\"\"\n    \n    def __init__(self):\n        RemoteCommand.__init__(self)\n        self.fs_support = FS(self)\n\n    def get_name(self):\n        return \"preinstall\"\n\n    def get_desc(self):\n        return \"Preinstall a new file system.\"\n\n    def is_hidden(self):\n        return True\n\n    def execute(self):\n        try:\n            conf_dir_path = Globals().get_conf_dir()\n            if not os.path.exists(conf_dir_path):\n                os.makedirs(conf_dir_path, 0755)\n        except OSError, ex:\n            print \"OSError\"\n            raise\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/bullxpfs/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Start.py",
        "source": "# Start.py -- Start file system\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `start' command classes.\n\nThe start command aims to start Lustre filesystem servers or just some\nof the filesystem targets on local or remote servers. It is available\nfor any filesystems previously installed and formatted.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\nfrom Shine.Commands.Status import Status\nfrom Shine.Commands.Tune import Tune\n\n# Command base class\nfrom Base.FSLiveCommand import FSLiveCommand\nfrom Base.FSEventHandler import FSGlobalEventHandler\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\n\n# Shine Proxy Protocol\nfrom Shine.Lustre.Actions.Proxies.ProxyAction import *\nfrom Shine.Lustre.FileSystem import *\n\n\nclass GlobalStartEventHandler(FSGlobalEventHandler):\n\n    def __init__(self, verbose=1):\n        FSGlobalEventHandler.__init__(self, verbose)\n\n    def handle_pre(self, fs):\n        if self.verbose > 0:\n            print \"Starting %d targets on %s\" % (fs.target_count,\n                    fs.target_servers)\n\n    def handle_post(self, fs):\n        if self.verbose > 0:\n            Status.status_view_fs(fs, show_clients=False)\n\n    def ev_starttarget_start(self, node, target):\n        # start/restart timer if needed (we might be running a new runloop)\n        if self.verbose > 1:\n            print \"%s: Starting %s %s (%s)...\" % (node, \\\n                    target.type.upper(), target.get_id(), target.dev)\n        self.update()\n\n    def ev_starttarget_done(self, node, target):\n        self.status_changed = True\n        if self.verbose > 1:\n            if target.status_info:\n                print \"%s: Start of %s %s (%s): %s\" % \\\n                        (node, target.type.upper(), target.get_id(), target.dev,\n                                target.status_info)\n            else:\n                print \"%s: Start of %s %s (%s) succeeded\" % \\\n                        (node, target.type.upper(), target.get_id(), target.dev)\n        self.update()\n\n    def ev_starttarget_failed(self, node, target, rc, message):\n        self.status_changed = True\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to start %s %s (%s): %s\" % \\\n                (node, target.type.upper(), target.get_id(), target.dev,\n                        strerr)\n        if rc:\n            print message\n        self.update()\n\n\nclass LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_starttarget_start(self, node, target):\n        if self.verbose > 1:\n            print \"Starting %s %s (%s)...\" % (target.type.upper(),\n                    target.get_id(), target.dev)\n\n    def ev_starttarget_done(self, node, target):\n        if self.verbose > 1:\n            if target.status_info:\n                print \"Start of %s %s (%s): %s\" % (target.type.upper(),\n                        target.get_id(), target.dev, target.status_info)\n            else:\n                print \"Start of %s %s (%s) succeeded\" % (target.type.upper(),\n                        target.get_id(), target.dev)\n\n    def ev_starttarget_failed(self, node, target, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"Failed to start %s %s (%s): %s\" % (target.type.upper(),\n                target.get_id(), target.dev, strerr)\n        if rc:\n            print message\n\n\nclass Start(FSLiveCommand):\n    \"\"\"\n    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]\n    \"\"\"\n\n    def __init__(self):\n        FSLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"start\"\n\n    def get_desc(self):\n        return \"Start file system servers.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_OK,\n            RECOVERING : RC_OK,\n            OFFLINE : RC_FAILURE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        target = self.target_support.get_target()\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),\n                    GlobalStartEventHandler(vlevel))\n\n            # Open configuration and instantiate a Lustre FS.\n            fs_conf, fs = open_lustrefs(fsname, target,\n                    nodes=self.nodes_support.get_nodeset(),\n                    indexes=self.indexes_support.get_rangeset(),\n                    event_handler=eh)\n\n            # Prepare options...\n            mount_options = {}\n            mount_paths = {}\n            for target_type in [ 'mgt', 'mdt', 'ost' ]:\n                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)\n                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            # Will call the handle_pre() method defined by the event handler.\n            if hasattr(eh, 'pre'):\n                eh.pre(fs)\n                \n            status = fs.start(mount_options=mount_options,\n                              mount_paths=mount_paths)\n\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Start successful.\"\n                tuning = Tune.get_tuning(fs_conf)\n                status = fs.tune(tuning)\n                if status == RUNTIME_ERROR:\n                    rc = RC_RUNTIME_ERROR\n                # XXX improve tuning on start error handling\n\n            if rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n            if hasattr(eh, 'post'):\n                eh.post(fs)\n\n            return rc\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/bullxpfs/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Status.py",
        "source": "# Status.py -- Check remote filesystem servers and targets status\n# Copyright (C) 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `status' command classes.\n\nThe status command aims to return the real state of a Lustre filesystem\nand its components, depending of the requested \"view\". Status views let\nthe Lustre administrator to either stand back and get a global status\nof the filesystem, or if needed, to enquire about filesystem components\ndetailed states.\n\"\"\"\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSLiveCommand import FSLiveCommand\nfrom Base.CommandRCDefs import *\n# Additional options\nfrom Base.Support.View import View\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n\n# Error handling\nfrom Exceptions import CommandBadParameterError\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Command output formatting\nfrom Shine.Utilities.AsciiTable import *\n\n# Lustre events and errors\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.Disk import *\nfrom Shine.Lustre.FileSystem import *\n\nfrom ClusterShell.NodeSet import NodeSet\n\nimport os\n\n\n(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)\n\n\nclass GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_statustarget_start(self, node, target):\n        pass\n\n    def ev_statustarget_done(self, node, target):\n        pass\n\n    def ev_statustarget_failed(self, node, target, rc, message):\n        print \"%s: Failed to status %s %s (%s)\" % (node, target.type.upper(), \\\n                target.get_id(), target.dev)\n        print \">> %s\" % message\n\n    def ev_statusclient_start(self, node, client):\n        pass\n\n    def ev_statusclient_done(self, node, client):\n        pass\n\n    def ev_statusclient_failed(self, node, client, rc, message):\n        print \"%s: Failed to status of FS %s\" % (node, client.fs.fs_name)\n        print \">> %s\" % message\n\n\nclass Status(FSLiveCommand):\n    \"\"\"\n    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]\n    \"\"\"\n\n    def __init__(self):\n        FSLiveCommand.__init__(self)\n        self.view_support = View(self)\n\n    def get_name(self):\n        return \"status\"\n\n    def get_desc(self):\n        return \"Check for file system target status.\"\n\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_ST_ONLINE,\n            RECOVERING : RC_ST_RECOVERING,\n            OFFLINE : RC_ST_OFFLINE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n\n        result = -1\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        target = self.target_support.get_target()\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))\n\n            fs_conf, fs = open_lustrefs(fsname, target,\n                    nodes=self.nodes_support.get_nodeset(),\n                    indexes=self.indexes_support.get_rangeset(),\n                    event_handler=eh)\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status_flags = STATUS_ANY\n            view = self.view_support.get_view()\n\n            # default view\n            if view is None:\n                view = \"fs\"\n            else:\n                view = view.lower()\n\n            # disable client checks when not requested\n            if view.startswith(\"disk\") or view.startswith(\"target\"):\n                status_flags &= ~STATUS_CLIENTS\n            # disable servers checks when not requested\n            if view.startswith(\"client\"):\n                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)\n\n            statusdict = fs.status(status_flags)\n\n            if RUNTIME_ERROR in statusdict:\n                # get targets that couldn't be checked\n                defect_targets = statusdict[RUNTIME_ERROR]\n\n                for nodes, msg in fs.proxy_errors:\n                    print nodes\n                    print '-' * 15\n                    print msg\n                print\n\n            else:\n                defect_targets = []\n\n            rc = self.fs_status_to_rc(max(statusdict.keys()))\n            if rc > result:\n                result = rc\n\n            if view == \"fs\":\n                self.status_view_fs(fs)\n            elif view.startswith(\"target\"):\n                self.status_view_targets(fs)\n            elif view.startswith(\"disk\"):\n                self.status_view_disks(fs)\n            else:\n                raise CommandBadParameterError(self.view_support.get_view(),\n                        \"fs, targets, disks\")\n        return result\n\n    def status_view_targets(self, fs):\n        \"\"\"\n        View: lustre targets\n        \"\"\"\n        print \"FILESYSTEM TARGETS (%s)\" % fs.fs_name\n\n        # override dict to allow target sorting by index\n        class target_dict(dict):\n            def __lt__(self, other):\n                return self[\"index\"] < other[\"index\"]\n\n        ldic = []\n        for type, (all_targets, enabled_targets) in fs.targets_by_type():\n            for target in enabled_targets:\n\n                if target.state == OFFLINE:\n                    status = \"offline\"\n                elif target.state == TARGET_ERROR:\n                    status = \"ERROR\"\n                elif target.state == RECOVERING:\n                    status = \"recovering %s\" % target.status_info\n                elif target.state == MOUNTED:\n                    status = \"online\"\n                else:\n                    status = \"UNKNOWN\"\n\n                ldic.append(target_dict([[\"target\", target.get_id()],\n                    [\"type\", target.type.upper()],\n                    [\"nodes\", NodeSet.fromlist(target.servers)],\n                    [\"device\", target.dev],\n                    [\"index\", target.index],\n                    [\"status\", status]]))\n\n        ldic.sort()\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        layout.set_column(\"target\", 0, AsciiTableLayout.LEFT, \"target id\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"type\", 1, AsciiTableLayout.LEFT, \"type\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"index\", 2, AsciiTableLayout.RIGHT, \"idx\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"nodes\", 3, AsciiTableLayout.LEFT, \"nodes\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"device\", 4, AsciiTableLayout.LEFT, \"device\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"status\", 5, AsciiTableLayout.LEFT, \"status\",\n                AsciiTableLayout.CENTER)\n\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n\n    def status_view_fs(cls, fs, show_clients=True):\n        \"\"\"\n        View: lustre FS summary\n        \"\"\"\n        ldic = []\n\n        # targets\n        for type, (a_targets, e_targets) in fs.targets_by_type():\n            nodes = NodeSet()\n            t_offline = []\n            t_error = []\n            t_recovering = []\n            t_online = []\n            t_runtime = []\n            t_unknown = []\n            for target in a_targets:\n                nodes.add(target.servers[0])\n\n                # check target status\n                if target.state == OFFLINE:\n                    t_offline.append(target)\n                elif target.state == TARGET_ERROR:\n                    t_error.append(target)\n                elif target.state == RECOVERING:\n                    t_recovering.append(target)\n                elif target.state == MOUNTED:\n                    t_online.append(target)\n                elif target.state == RUNTIME_ERROR:\n                    t_runtime.append(target)\n                else:\n                    t_unknown.append(target)\n\n            status = []\n            if len(t_offline) > 0:\n                status.append(\"offline (%d)\" % len(t_offline))\n            if len(t_error) > 0:\n                status.append(\"ERROR (%d)\" % len(t_error))\n            if len(t_recovering) > 0:\n                status.append(\"recovering (%d) for %s\" % (len(t_recovering),\n                    t_recovering[0].status_info))\n            if len(t_online) > 0:\n                status.append(\"online (%d)\" % len(t_online))\n            if len(t_runtime) > 0:\n                status.append(\"CHECK FAILURE (%d)\" % len(t_runtime))\n            if len(t_unknown) > 0:\n                status.append(\"not checked (%d)\" % len(t_unknown))\n\n            if len(t_unknown) < len(a_targets):\n                ldic.append(dict([[\"type\", \"%s\" % type.upper()],\n                    [\"count\", len(a_targets)], [\"nodes\", nodes],\n                    [\"status\", ', '.join(status)]]))\n\n        # clients\n        if show_clients:\n            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()\n            status = []\n            if c_ign > 0:\n                status.append(\"not checked (%d)\" % c_ign)\n            if c_offline > 0:\n                status.append(\"offline (%d)\" % c_offline)\n            if c_error > 0:\n                status.append(\"ERROR (%d)\" % c_error)\n            if c_runtime > 0:\n                status.append(\"CHECK FAILURE (%d)\" % c_runtime)\n            if c_mounted > 0:\n                status.append(\"mounted (%d)\" % c_mounted)\n\n            ldic.append(dict([[\"type\", \"CLI\"], [\"count\", len(fs.clients)],\n                [\"nodes\", \"%s\" % fs.get_client_servers()], [\"status\", ', '.join(status)]]))\n\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        layout.set_column(\"type\", 0, AsciiTableLayout.CENTER, \"type\", AsciiTableLayout.CENTER)\n        layout.set_column(\"count\", 1, AsciiTableLayout.RIGHT, \"#\", AsciiTableLayout.CENTER)\n        layout.set_column(\"nodes\", 2, AsciiTableLayout.LEFT, \"nodes\", AsciiTableLayout.CENTER)\n        layout.set_column(\"status\", 3, AsciiTableLayout.LEFT, \"status\", AsciiTableLayout.CENTER)\n\n        print \"FILESYSTEM COMPONENTS STATUS (%s)\" % fs.fs_name\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n    status_view_fs = classmethod(status_view_fs)\n\n\n    def status_view_disks(self, fs):\n        \"\"\"\n        View: lustre disks\n        \"\"\"\n\n        print \"FILESYSTEM DISKS (%s)\" % fs.fs_name\n\n        # override dict to allow target sorting by index\n        class target_dict(dict):\n            def __lt__(self, other):\n                return self[\"index\"] < other[\"index\"] \n        ldic = []\n        jdev_col_enabled = False\n        tag_col_enabled = False\n        for type, (all_targets, enabled_targets) in fs.targets_by_type():\n            for target in enabled_targets:\n\n                if target.state == OFFLINE:\n                    status = \"offline\"\n                elif target.state == RECOVERING:\n                    status = \"recovering %s\" % target.status_info\n                elif target.state == MOUNTED:\n                    status = \"online\"\n                elif target.state == TARGET_ERROR:\n                    status = \"ERROR\"\n                elif target.state == RUNTIME_ERROR:\n                    status = \"CHECK FAILURE\"\n                else:\n                    status = \"UNKNOWN\"\n\n                if target.dev_size >= TERA:\n                    dev_size = \"%.1fT\" % (target.dev_size/TERA)\n                elif target.dev_size >= GIGA:\n                    dev_size = \"%.1fG\" % (target.dev_size/GIGA)\n                elif target.dev_size >= MEGA:\n                    dev_size = \"%.1fM\" % (target.dev_size/MEGA)\n                elif target.dev_size >= KILO:\n                    dev_size = \"%.1fK\" % (target.dev_size/KILO)\n                else:\n                    dev_size = \"%d\" % target.dev_size\n\n                if target.jdev:\n                    jdev_col_enabled = True\n                    jdev = target.jdev\n                else:\n                    jdev = \"\"\n\n                if target.tag:\n                    tag_col_enabled = True\n                    tag = target.tag\n                else:\n                    tag = \"\"\n\n                flags = []\n                if target.has_need_index_flag():\n                    flags.append(\"need_index\")\n                if target.has_first_time_flag():\n                    flags.append(\"first_time\")\n                if target.has_update_flag():\n                    flags.append(\"update\")\n                if target.has_rewrite_ldd_flag():\n                    flags.append(\"rewrite_ldd\")\n                if target.has_writeconf_flag():\n                    flags.append(\"writeconf\")\n                if target.has_upgrade14_flag():\n                    flags.append(\"upgrade14\")\n                if target.has_param_flag():\n                    flags.append(\"conf_param\")\n\n                ldic.append(target_dict([\\\n                    [\"nodes\", NodeSet.fromlist(target.servers)],\n                    [\"dev\", target.dev],\n                    [\"size\", dev_size],\n                    [\"jdev\", jdev],\n                    [\"type\", target.type.upper()],\n                    [\"index\", target.index],\n                    [\"tag\", tag],\n                    [\"label\", target.label],\n                    [\"flags\", ' '.join(flags)],\n                    [\"fsname\", target.fs.fs_name],\n                    [\"status\", status]]))\n\n        ldic.sort()\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        i = 0\n        layout.set_column(\"dev\", i, AsciiTableLayout.LEFT, \"device\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"nodes\", i, AsciiTableLayout.LEFT, \"node(s)\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"size\", i, AsciiTableLayout.RIGHT, \"dev size\",\n                AsciiTableLayout.CENTER)\n        if jdev_col_enabled:\n            i += 1\n            layout.set_column(\"jdev\", i, AsciiTableLayout.RIGHT, \"journal device\",\n                    AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"type\", i, AsciiTableLayout.LEFT, \"type\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"index\", i, AsciiTableLayout.RIGHT, \"index\",\n                AsciiTableLayout.CENTER)\n        if tag_col_enabled:\n            i += 1\n            layout.set_column(\"tag\", i, AsciiTableLayout.LEFT, \"tag\",\n                    AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"label\", i, AsciiTableLayout.LEFT, \"label\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"flags\", i, AsciiTableLayout.LEFT, \"ldd flags\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"fsname\", i, AsciiTableLayout.LEFT, \"fsname\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"status\", i, AsciiTableLayout.LEFT, \"status\",\n                AsciiTableLayout.CENTER)\n\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/bullxpfs/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Commands/Umount.py",
        "source": "# Umount.py -- Unmount file system on clients\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `umount' command classes.\n\nThe umount command aims to stop Lustre filesystem clients.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSClientLiveCommand import FSClientLiveCommand\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.FileSystem import *\n\n\nclass GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_stopclient_start(self, node, client):\n        if self.verbose > 1:\n            print \"%s: Unmounting %s on %s ...\" % (node, client.fs.fs_name, client.mount_path)\n\n    def ev_stopclient_done(self, node, client):\n        if self.verbose > 1:\n            if client.status_info:\n                print \"%s: Umount: %s\" % (node, client.status_info)\n            else:\n                print \"%s: FS %s succesfully unmounted from %s\" % (node,\n                        client.fs.fs_name, client.mount_path)\n\n    def ev_stopclient_failed(self, node, client, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to unmount FS %s from %s: %s\" % \\\n                (node, client.fs.fs_name, client.mount_path, strerr)\n        if rc:\n            print message\n\n\nclass Umount(FSClientLiveCommand):\n    \"\"\"\n    shine umount\n    \"\"\"\n\n    def __init__(self):\n        FSClientLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"umount\"\n\n    def get_desc(self):\n        return \"Unmount file system clients.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_FAILURE,\n            RECOVERING : RC_FAILURE,\n            OFFLINE : RC_OK,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None,\n                    GlobalUmountEventHandler(vlevel))\n\n            nodes = self.nodes_support.get_nodeset()\n\n            fs_conf, fs = open_lustrefs(fsname, None,\n                    nodes=nodes,\n                    indexes=None,\n                    event_handler=eh)\n\n            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):\n                raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\\n                        (nodes - fs_conf.get_client_nodes(), fsname))\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status = fs.umount()\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Unmount successful.\"\n            elif rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n        return result\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/bullxpfs/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Controller.py",
        "source": "# Controller.py -- Controller class\n# Copyright (C) 2007 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Configuration.Globals import Globals\nfrom Commands.CommandRegistry import CommandRegistry\n\nfrom Configuration.ModelFile import ModelFileException\nfrom Configuration.ModelFile import ModelFileIOError\n\nfrom Configuration.Exceptions import ConfigException\nfrom Commands.Exceptions import *\nfrom Commands.Base.CommandRCDefs import *\n\nfrom Lustre.FileSystem import FSRemoteError\n\nfrom ClusterShell.Task import *\nfrom ClusterShell.NodeSet import *\n\nimport getopt\nimport logging\nimport re\nimport sys\n\n\ndef print_csdebug(task, s):\n    m = re.search(\"(\\w+): SHINE:\\d:(\\w+):\", s)\n    if m:\n        print \"%s<pickle>\" % m.group(0)\n    else:\n        print s\n\n\nclass Controller:\n\n    def __init__(self):\n        self.logger = logging.getLogger(\"shine\")\n        #handler = logging.FileHandler(Globals().get_log_file())\n        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')\n        #handler.setFormatter(formatter)\n        #self.logger.addHandler(handler)\n        #self.logger.setLevel(Globals().get_log_level())\n        self.cmds = CommandRegistry()\n\n        #task_self().set_info(\"debug\", True)\n\n        task_self().set_info(\"print_debug\", print_csdebug)\n\n    def usage(self):\n        cmd_maxlen = 0\n\n        for cmd in self.cmds:\n            if not cmd.is_hidden():\n                if len(cmd.get_name()) > cmd_maxlen:\n                    cmd_maxlen = len(cmd.get_name())\n        for cmd in self.cmds:\n            if not cmd.is_hidden():\n                print \"  %-*s %s\" % (cmd_maxlen, cmd.get_name(),\n                    cmd.get_params_desc())\n\n    def print_error(self, errmsg):\n        print >>sys.stderr, \"Error:\", errmsg\n\n    def print_help(self, msg, cmd):\n        if msg:\n            print msg\n            print\n        print \"Usage: %s %s\" % (cmd.get_name(), cmd.get_params_desc())\n        print\n        print cmd.get_desc()\n\n    def run_command(self, cmd_args):\n\n        #self.logger.info(\"running %s\" % cmd_name)\n\n        try:\n            return self.cmds.execute(cmd_args)\n        except getopt.GetoptError, e:\n            print \"Syntax error: %s\" % e\n        except CommandHelpException, e:\n            self.print_help(e.message, e.cmd)\n        except CommandException, e:\n            self.print_error(e.message)\n            return RC_USER_ERROR\n        except ModelFileIOError, e:\n            print \"Error - %s\" % e.message\n        except ModelFileException, e:\n            print \"ModelFile: %s\" % e\n        except ConfigException, e:\n            print \"Configuration: %s\" % e\n            return RC_RUNTIME_ERROR\n        # file system\n        except FSRemoteError, e:\n            self.print_error(e)\n            return e.rc\n        except NodeSetParseError, e:\n            self.print_error(\"%s\" % e)\n            return RC_USER_ERROR\n        except RangeSetParseError, e:\n            self.print_error(\"%s\" % e)\n            return RC_USER_ERROR\n        except KeyError:\n            print \"Error - Unrecognized action\"\n            print\n            raise\n        \n        return 1\n\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/bullxpfs/lustre-shine/blob/be7731dd9811a8106c53a7a6e47a4d174dc6cb7c",
        "file_path": "/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py",
        "source": "# FSProxyAction.py -- Lustre generic FS proxy action class\n# Copyright (C) 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Globals import Globals\nfrom Shine.Configuration.Configuration import Configuration\n\nfrom ProxyAction import *\n\nfrom ClusterShell.NodeSet import NodeSet\n\n\nclass FSProxyAction(ProxyAction):\n    \"\"\"\n    Generic file system command proxy action class.\n    \"\"\"\n\n    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):\n        ProxyAction.__init__(self)\n        self.fs = fs\n        self.action = action\n        assert isinstance(nodes, NodeSet)\n        self.nodes = nodes\n        self.debug = debug\n        self.targets_type = targets_type\n        self.targets_indexes = targets_indexes\n\n        if self.fs.debug:\n            print \"FSProxyAction %s on %s\" % (action, nodes)\n\n    def launch(self):\n        \"\"\"\n        Launch FS proxy command.\n        \"\"\"\n        command = [\"%s\" % self.progpath]\n        command.append(self.action)\n        command.append(\"-f %s\" % self.fs.fs_name)\n        command.append(\"-R\")\n\n        if self.debug:\n            command.append(\"-d\")\n\n        if self.targets_type:\n            command.append(\"-t %s\" % self.targets_type)\n            if self.targets_indexes:\n                command.append(\"-i %s\" % self.targets_indexes)\n\n        # Schedule cluster command.\n        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)\n\n    def ev_read(self, worker):\n        node, buf = worker.last_read()\n        try:\n            event, params = self._shine_msg_unpack(buf)\n            self.fs._handle_shine_event(event, node, **params)\n        except ProxyActionUnpackError, e:\n            # ignore any non shine messages\n            pass\n\n    def ev_close(self, worker):\n        \"\"\"\n        End of proxy command.\n        \"\"\"\n        # Gather nodes by return code\n        for rc, nodes in worker.iter_retcodes():\n            # rc 127 = command not found\n            # rc 126 = found but not executable\n            if rc >= 126:\n                # Gather these nodes by buffer\n                for buffer, nodes in worker.iter_buffers(nodes):\n                    # Handle proxy command error which rc >= 127 and \n                    self.fs._handle_shine_proxy_error(nodes, \"Remote action %s failed: %s\" % \\\n                            (self.action, buffer))\n\n        self.fs.action_refcnt -= 1\n        if self.fs.action_refcnt == 0:\n            worker.task.abort()\n\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/jlu5/PyLink/blob/40fa4f71bcd4946e5578beabedd30e308d1e1cdd",
        "file_path": "/classes.py",
        "source": "\"\"\"\nclasses.py - Base classes for PyLink IRC Services.\n\nThis module contains the base classes used by PyLink, including threaded IRC\nconnections and objects used to represent IRC servers, users, and channels.\n\nHere be dragons.\n\"\"\"\n\nimport threading\nimport time\nimport socket\nimport ssl\nimport hashlib\nfrom copy import deepcopy\nimport inspect\nimport re\nfrom collections import defaultdict, deque\nimport ipaddress\n\ntry:\n    import ircmatch\nexcept ImportError:\n    raise ImportError(\"PyLink requires ircmatch to function; please install it and try again.\")\n\nfrom . import world, utils, structures, conf, __version__\nfrom .log import *\n\n### Exceptions\n\nclass ProtocolError(RuntimeError):\n    pass\n\n### Internal classes (users, servers, channels)\n\nclass Irc(utils.DeprecatedAttributesObject):\n    \"\"\"Base IRC object for PyLink.\"\"\"\n\n    def __init__(self, netname, proto, conf):\n        \"\"\"\n        Initializes an IRC object. This takes 3 variables: the network name\n        (a string), the name of the protocol module to use for this connection,\n        and a configuration object.\n        \"\"\"\n        self.deprecated_attributes = {\n            'conf': 'Deprecated since 1.2; consider switching to conf.conf',\n            'botdata': \"Deprecated since 1.2; consider switching to conf.conf['bot']\",\n        }\n\n        self.loghandlers = []\n        self.name = netname\n        self.conf = conf\n        self.sid = None\n        self.serverdata = conf['servers'][netname]\n        self.botdata = conf['bot']\n        self.protoname = proto.__name__.split('.')[-1]  # Remove leading pylinkirc.protocols.\n        self.proto = proto.Class(self)\n        self.pingfreq = self.serverdata.get('pingfreq') or 90\n        self.pingtimeout = self.pingfreq * 2\n\n        self.queue = deque()\n\n        self.connected = threading.Event()\n        self.aborted = threading.Event()\n        self.reply_lock = threading.Lock()\n\n        self.pingTimer = None\n\n        # Sets the multiplier for autoconnect delay (grows with time).\n        self.autoconnect_active_multiplier = 1\n\n        self.initVars()\n\n        if world.testing:\n            # HACK: Don't thread if we're running tests.\n            self.connect()\n        else:\n            self.connection_thread = threading.Thread(target=self.connect,\n                                                      name=\"Listener for %s\" %\n                                                      self.name)\n            self.connection_thread.start()\n\n    def logSetup(self):\n        \"\"\"\n        Initializes any channel loggers defined for the current network.\n        \"\"\"\n        try:\n            channels = conf.conf['logging']['channels'][self.name]\n        except KeyError:  # Not set up; just ignore.\n            return\n\n        log.debug('(%s) Setting up channel logging to channels %r', self.name,\n                  channels)\n\n        if not self.loghandlers:\n            # Only create handlers if they haven't already been set up.\n\n            for channel, chandata in channels.items():\n                # Fetch the log level for this channel block.\n                level = None\n                if chandata is not None:\n                    level = chandata.get('loglevel')\n\n                handler = PyLinkChannelLogger(self, channel, level=level)\n                self.loghandlers.append(handler)\n                log.addHandler(handler)\n\n    def initVars(self):\n        \"\"\"\n        (Re)sets an IRC object to its default state. This should be called when\n        an IRC object is first created, and on every reconnection to a network.\n        \"\"\"\n        self.pingfreq = self.serverdata.get('pingfreq') or 90\n        self.pingtimeout = self.pingfreq * 3\n\n        self.pseudoclient = None\n        self.lastping = time.time()\n\n        self.queue.clear()\n\n        # Internal variable to set the place and caller of the last command (in PM\n        # or in a channel), used by fantasy command support.\n        self.called_by = None\n        self.called_in = None\n\n        # Intialize the server, channel, and user indexes to be populated by\n        # our protocol module. For the server index, we can add ourselves right\n        # now.\n        self.servers = {}\n        self.users = {}\n        self.channels = structures.KeyedDefaultdict(IrcChannel)\n\n        # This sets the list of supported channel and user modes: the default\n        # RFC1459 modes are implied. Named modes are used here to make\n        # protocol-independent code easier to write, as mode chars vary by\n        # IRCd.\n        # Protocol modules should add to and/or replace this with what their\n        # protocol supports. This can be a hardcoded list or something\n        # negotiated on connect, depending on the nature of their protocol.\n        self.cmodes = {'op': 'o', 'secret': 's', 'private': 'p',\n                       'noextmsg': 'n', 'moderated': 'm', 'inviteonly': 'i',\n                       'topiclock': 't', 'limit': 'l', 'ban': 'b',\n                       'voice': 'v', 'key': 'k',\n                       # This fills in the type of mode each mode character is.\n                       # A-type modes are list modes (i.e. bans, ban exceptions, etc.),\n                       # B-type modes require an argument to both set and unset,\n                       #   but there can only be one value at a time\n                       #   (i.e. cmode +k).\n                       # C-type modes require an argument to set but not to unset\n                       #   (one sets \"+l limit\" and # \"-l\"),\n                       # and D-type modes take no arguments at all.\n                       '*A': 'b',\n                       '*B': 'k',\n                       '*C': 'l',\n                       '*D': 'imnpstr'}\n        self.umodes = {'invisible': 'i', 'snomask': 's', 'wallops': 'w',\n                       'oper': 'o',\n                       '*A': '', '*B': '', '*C': '', '*D': 'iosw'}\n\n        # This max nick length starts off as the config value, but may be\n        # overwritten later by the protocol module if such information is\n        # received. It defaults to 30.\n        self.maxnicklen = self.serverdata.get('maxnicklen', 30)\n\n        # Defines a list of supported prefix modes.\n        self.prefixmodes = {'o': '@', 'v': '+'}\n\n        # Defines the uplink SID (to be filled in by protocol module).\n        self.uplink = None\n        self.start_ts = int(time.time())\n\n        # Set up channel logging for the network\n        self.logSetup()\n\n    def processQueue(self):\n        \"\"\"Loop to process outgoing queue data.\"\"\"\n        while not self.aborted.is_set():\n            if self.queue:  # Only process if there's data.\n                data = self.queue.popleft()\n                self._send(data)\n            throttle_time = self.serverdata.get('throttle_time', 0.005)\n            self.aborted.wait(throttle_time)\n        log.debug('(%s) Stopping queue thread as aborted is set', self.name)\n\n    def connect(self):\n        \"\"\"\n        Runs the connect loop for the IRC object. This is usually called by\n        __init__ in a separate thread to allow multiple concurrent connections.\n        \"\"\"\n        while True:\n\n            self.aborted.clear()\n            self.initVars()\n\n            try:\n                self.proto.validateServerConf()\n            except AssertionError as e:\n                log.exception(\"(%s) Configuration error: %s\", self.name, e)\n                return\n\n            ip = self.serverdata[\"ip\"]\n            port = self.serverdata[\"port\"]\n            checks_ok = True\n            try:\n                # Set the socket type (IPv6 or IPv4).\n                stype = socket.AF_INET6 if self.serverdata.get(\"ipv6\") else socket.AF_INET\n\n                # Creat the socket.\n                self.socket = socket.socket(stype)\n                self.socket.setblocking(0)\n\n                # Set the socket bind if applicable.\n                if 'bindhost' in self.serverdata:\n                    self.socket.bind((self.serverdata['bindhost'], 0))\n\n                # Set the connection timeouts. Initial connection timeout is a\n                # lot smaller than the timeout after we've connected; this is\n                # intentional.\n                self.socket.settimeout(self.pingfreq)\n\n                # Resolve hostnames if it's not an IP address already.\n                old_ip = ip\n                ip = socket.getaddrinfo(ip, port, stype)[0][-1][0]\n                log.debug('(%s) Resolving address %s to %s', self.name, old_ip, ip)\n\n                # Enable SSL if set to do so. This requires a valid keyfile and\n                # certfile to be present.\n                self.ssl = self.serverdata.get('ssl')\n                if self.ssl:\n                    log.info('(%s) Attempting SSL for this connection...', self.name)\n                    certfile = self.serverdata.get('ssl_certfile')\n                    keyfile = self.serverdata.get('ssl_keyfile')\n\n                    context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\n                    # Disable SSLv2 and SSLv3 - these are insecure\n                    context.options |= ssl.OP_NO_SSLv2\n                    context.options |= ssl.OP_NO_SSLv3\n\n                    if certfile and keyfile:\n                        try:\n                            context.load_cert_chain(certfile, keyfile)\n                        except OSError:\n                             log.exception('(%s) Caught OSError trying to '\n                                           'initialize the SSL connection; '\n                                           'are \"ssl_certfile\" and '\n                                           '\"ssl_keyfile\" set correctly?',\n                                           self.name)\n                             checks_ok = False\n\n                    self.socket = context.wrap_socket(self.socket)\n\n                log.info(\"Connecting to network %r on %s:%s\", self.name, ip, port)\n                self.socket.connect((ip, port))\n                self.socket.settimeout(self.pingtimeout)\n\n                # If SSL was enabled, optionally verify the certificate\n                # fingerprint for some added security. I don't bother to check\n                # the entire certificate for validity, since most IRC networks\n                # self-sign their certificates anyways.\n                if self.ssl and checks_ok:\n                    peercert = self.socket.getpeercert(binary_form=True)\n\n                    # Hash type is configurable using the ssl_fingerprint_type\n                    # value, and defaults to sha256.\n                    hashtype = self.serverdata.get('ssl_fingerprint_type', 'sha256').lower()\n\n                    try:\n                        hashfunc = getattr(hashlib, hashtype)\n                    except AttributeError:\n                        log.error('(%s) Unsupported SSL certificate fingerprint type %r given, disconnecting...',\n                                  self.name, hashtype)\n                        checks_ok = False\n                    else:\n                        fp = hashfunc(peercert).hexdigest()\n                        expected_fp = self.serverdata.get('ssl_fingerprint')\n\n                        if expected_fp and checks_ok:\n                            if fp != expected_fp:\n                                # SSL Fingerprint doesn't match; break.\n                                log.error('(%s) Uplink\\'s SSL certificate '\n                                          'fingerprint (%s) does not match the '\n                                          'one configured: expected %r, got %r; '\n                                          'disconnecting...', self.name, hashtype,\n                                          expected_fp, fp)\n                                checks_ok = False\n                            else:\n                                log.info('(%s) Uplink SSL certificate fingerprint '\n                                         '(%s) verified: %r', self.name, hashtype,\n                                         fp)\n                        else:\n                            log.info('(%s) Uplink\\'s SSL certificate fingerprint (%s) '\n                                     'is %r. You can enhance the security of your '\n                                     'link by specifying this in a \"ssl_fingerprint\"'\n                                     ' option in your server block.', self.name,\n                                     hashtype, fp)\n\n                if checks_ok:\n\n                    self.queue_thread = threading.Thread(name=\"Queue thread for %s\" % self.name,\n                                                         target=self.processQueue, daemon=True)\n                    self.queue_thread.start()\n\n                    self.sid = self.serverdata.get(\"sid\")\n                    # All our checks passed, get the protocol module to connect and run the listen\n                    # loop. This also updates any SID values should the protocol module do so.\n                    self.proto.connect()\n\n                    log.info('(%s) Enumerating our own SID %s', self.name, self.sid)\n                    host = self.hostname()\n\n                    self.servers[self.sid] = IrcServer(None, host, internal=True,\n                            desc=self.serverdata.get('serverdesc')\n                            or conf.conf['bot']['serverdesc'])\n\n                    log.info('(%s) Starting ping schedulers....', self.name)\n                    self.schedulePing()\n                    log.info('(%s) Server ready; listening for data.', self.name)\n                    self.autoconnect_active_multiplier = 1  # Reset any extra autoconnect delays\n                    self.run()\n                else:  # Configuration error :(\n                    log.error('(%s) A configuration error was encountered '\n                              'trying to set up this connection. Please check'\n                              ' your configuration file and try again.',\n                              self.name)\n            # self.run() or the protocol module it called raised an exception, meaning we've disconnected!\n            # Note: socket.error, ConnectionError, IOError, etc. are included in OSError since Python 3.3,\n            # so we don't need to explicitly catch them here.\n            # We also catch SystemExit here as a way to abort out connection threads properly, and stop the\n            # IRC connection from freezing instead.\n            except (OSError, RuntimeError, SystemExit) as e:\n                log.error('(%s) Disconnected from IRC: %s: %s',\n                          self.name, type(e).__name__, str(e))\n\n            self.disconnect()\n\n            # If autoconnect is enabled, loop back to the start. Otherwise,\n            # return and stop.\n            autoconnect = self.serverdata.get('autoconnect')\n\n            # Sets the autoconnect growth multiplier (e.g. a value of 2 multiplies the autoconnect\n            # time by 2 on every failure, etc.)\n            autoconnect_multiplier = self.serverdata.get('autoconnect_multiplier', 2)\n            autoconnect_max = self.serverdata.get('autoconnect_max', 1800)\n            # These values must at least be 1.\n            autoconnect_multiplier = max(autoconnect_multiplier, 1)\n            autoconnect_max = max(autoconnect_max, 1)\n\n            log.debug('(%s) Autoconnect delay set to %s seconds.', self.name, autoconnect)\n            if autoconnect is not None and autoconnect >= 1:\n                log.debug('(%s) Multiplying autoconnect delay %s by %s.', self.name, autoconnect, self.autoconnect_active_multiplier)\n                autoconnect *= self.autoconnect_active_multiplier\n                # Add a cap on the max. autoconnect delay, so that we don't go on forever...\n                autoconnect = min(autoconnect, autoconnect_max)\n\n                log.info('(%s) Going to auto-reconnect in %s seconds.', self.name, autoconnect)\n                # Continue when either self.aborted is set or the autoconnect time passes.\n                # Compared to time.sleep(), this allows us to stop connections quicker if we\n                # break while while for autoconnect.\n                self.aborted.clear()\n                self.aborted.wait(autoconnect)\n\n                # Store in the local state what the autoconnect multiplier currently is.\n                self.autoconnect_active_multiplier *= autoconnect_multiplier\n\n                if self not in world.networkobjects.values():\n                    log.debug('Stopping stale connect loop for old connection %r', self.name)\n                    return\n\n            else:\n                log.info('(%s) Stopping connect loop (autoconnect value %r is < 1).', self.name, autoconnect)\n                return\n\n    def disconnect(self):\n        \"\"\"Handle disconnects from the remote server.\"\"\"\n        was_successful = self.connected.is_set()\n        log.debug('(%s) disconnect: got %s for was_successful state', self.name, was_successful)\n\n        log.debug('(%s) disconnect: Clearing self.connected state.', self.name)\n        self.connected.clear()\n\n        log.debug('(%s) Removing channel logging handlers due to disconnect.', self.name)\n        while self.loghandlers:\n            log.removeHandler(self.loghandlers.pop())\n\n        try:\n            log.debug('(%s) disconnect: Shutting down socket.', self.name)\n            self.socket.shutdown(socket.SHUT_RDWR)\n        except:  # Socket timed out during creation; ignore\n            pass\n\n        self.socket.close()\n\n        if self.pingTimer:\n            log.debug('(%s) Canceling pingTimer at %s due to disconnect() call', self.name, time.time())\n            self.pingTimer.cancel()\n\n        log.debug('(%s) disconnect: Setting self.aborted to True.', self.name)\n        self.aborted.set()\n\n        # Internal hook signifying that a network has disconnected.\n        self.callHooks([None, 'PYLINK_DISCONNECT', {'was_successful': was_successful}])\n\n        log.debug('(%s) disconnect: Clearing state via initVars().', self.name)\n        self.initVars()\n\n    def run(self):\n        \"\"\"Main IRC loop which listens for messages.\"\"\"\n        # Some magic below cause this to work, though anything that's\n        # not encoded in UTF-8 doesn't work very well.\n        buf = b\"\"\n        data = b\"\"\n        while not self.aborted.is_set():\n\n            try:\n                data = self.socket.recv(2048)\n            except OSError:\n                # Suppress socket read warnings from lingering recv() calls if\n                # we've been told to shutdown.\n                if self.aborted.is_set():\n                    return\n                raise\n\n            buf += data\n            if not data:\n                log.error('(%s) No data received, disconnecting!', self.name)\n                return\n            elif (time.time() - self.lastping) > self.pingtimeout:\n                log.error('(%s) Connection timed out.', self.name)\n                return\n            while b'\\n' in buf:\n                line, buf = buf.split(b'\\n', 1)\n                line = line.strip(b'\\r')\n                # FIXME: respect other encodings?\n                line = line.decode(\"utf-8\", \"replace\")\n                self.runline(line)\n\n    def runline(self, line):\n        \"\"\"Sends a command to the protocol module.\"\"\"\n        log.debug(\"(%s) <- %s\", self.name, line)\n        try:\n            hook_args = self.proto.handle_events(line)\n        except Exception:\n            log.exception('(%s) Caught error in handle_events, disconnecting!', self.name)\n            log.error('(%s) The offending line was: <- %s', self.name, line)\n            self.aborted.set()\n            return\n        # Only call our hooks if there's data to process. Handlers that support\n        # hooks will return a dict of parsed arguments, which can be passed on\n        # to plugins and the like. For example, the JOIN handler will return\n        # something like: {'channel': '#whatever', 'users': ['UID1', 'UID2',\n        # 'UID3']}, etc.\n        if hook_args is not None:\n            self.callHooks(hook_args)\n\n        return hook_args\n\n    def callHooks(self, hook_args):\n        \"\"\"Calls a hook function with the given hook args.\"\"\"\n        numeric, command, parsed_args = hook_args\n        # Always make sure TS is sent.\n        if 'ts' not in parsed_args:\n            parsed_args['ts'] = int(time.time())\n        hook_cmd = command\n        hook_map = self.proto.hook_map\n\n        # If the hook name is present in the protocol module's hook_map, then we\n        # should set the hook name to the name that points to instead.\n        # For example, plugins will read SETHOST as CHGHOST, EOS (end of sync)\n        # as ENDBURST, etc.\n        if command in hook_map:\n            hook_cmd = hook_map[command]\n\n        # However, individual handlers can also return a 'parse_as' key to send\n        # their payload to a different hook. An example of this is \"/join 0\"\n        # being interpreted as leaving all channels (PART).\n        hook_cmd = parsed_args.get('parse_as') or hook_cmd\n\n        log.debug('(%s) Raw hook data: [%r, %r, %r] received from %s handler '\n                  '(calling hook %s)', self.name, numeric, hook_cmd, parsed_args,\n                  command, hook_cmd)\n\n        # Iterate over registered hook functions, catching errors accordingly.\n        for hook_func in world.hooks[hook_cmd]:\n            try:\n                log.debug('(%s) Calling hook function %s from plugin \"%s\"', self.name,\n                          hook_func, hook_func.__module__)\n                hook_func(self, numeric, command, parsed_args)\n            except Exception:\n                # We don't want plugins to crash our servers...\n                log.exception('(%s) Unhandled exception caught in hook %r from plugin \"%s\"',\n                              self.name, hook_func, hook_func.__module__)\n                log.error('(%s) The offending hook data was: %s', self.name,\n                          hook_args)\n                continue\n\n    def _send(self, data):\n        \"\"\"Sends raw text to the uplink server.\"\"\"\n        # Safeguard against newlines in input!! Otherwise, each line gets\n        # treated as a separate command, which is particularly nasty.\n        data = data.replace('\\n', ' ')\n        data = data.encode(\"utf-8\") + b\"\\n\"\n        stripped_data = data.decode(\"utf-8\").strip(\"\\n\")\n        log.debug(\"(%s) -> %s\", self.name, stripped_data)\n\n        try:\n            self.socket.send(data)\n        except (OSError, AttributeError):\n            log.debug(\"(%s) Dropping message %r; network isn't connected!\", self.name, stripped_data)\n\n    def send(self, data, queue=True):\n        \"\"\"send() wrapper with optional queueing support.\"\"\"\n        if queue:\n            self.queue.append(data)\n        else:\n            self._send(data)\n\n    def schedulePing(self):\n        \"\"\"Schedules periodic pings in a loop.\"\"\"\n        self.proto.ping()\n\n        self.pingTimer = threading.Timer(self.pingfreq, self.schedulePing)\n        self.pingTimer.daemon = True\n        self.pingTimer.name = 'Ping timer loop for %s' % self.name\n        self.pingTimer.start()\n\n        log.debug('(%s) Ping scheduled at %s', self.name, time.time())\n\n    def __repr__(self):\n        return \"<classes.Irc object for %r>\" % self.name\n\n    ### General utility functions\n    def callCommand(self, source, text):\n        \"\"\"\n        Calls a PyLink bot command. source is the caller's UID, and text is the\n        full, unparsed text of the message.\n        \"\"\"\n        world.services['pylink'].call_cmd(self, source, text)\n\n    def msg(self, target, text, notice=None, source=None, loopback=True):\n        \"\"\"Handy function to send messages/notices to clients. Source\n        is optional, and defaults to the main PyLink client if not specified.\"\"\"\n        if not text:\n            return\n\n        if not (source or self.pseudoclient):\n            # No explicit source set and our main client wasn't available; abort.\n            return\n        source = source or self.pseudoclient.uid\n\n        if notice:\n            self.proto.notice(source, target, text)\n            cmd = 'PYLINK_SELF_NOTICE'\n        else:\n            self.proto.message(source, target, text)\n            cmd = 'PYLINK_SELF_PRIVMSG'\n\n        if loopback:\n            # Determines whether we should send a hook for this msg(), to relay things like services\n            # replies across relay.\n            self.callHooks([source, cmd, {'target': target, 'text': text}])\n\n    def reply(self, text, notice=None, source=None, private=None, force_privmsg_in_private=False,\n            loopback=True):\n        \"\"\"Replies to the last caller in the right context (channel or PM).\"\"\"\n\n        with self.reply_lock:\n            if private is None:\n                # Allow using private replies as the default, if no explicit setting was given.\n                private = conf.conf['bot'].get(\"prefer_private_replies\")\n\n            # Private reply is enabled, or the caller was originally a PM\n            if private or (self.called_in in self.users):\n                if not force_privmsg_in_private:\n                    # For private replies, the default is to override the notice=True/False argument,\n                    # and send replies as notices regardless. This is standard behaviour for most\n                    # IRC services, but can be disabled if force_privmsg_in_private is given.\n                    notice = True\n                target = self.called_by\n            else:\n                target = self.called_in\n\n            self.msg(target, text, notice=notice, source=source, loopback=loopback)\n\n    def error(self, text, **kwargs):\n        \"\"\"Replies with an error to the last caller in the right context (channel or PM).\"\"\"\n        # This is a stub to alias error to reply\n        self.reply(\"Error: %s\" % text, **kwargs)\n\n    def toLower(self, text):\n        \"\"\"Returns a lowercase representation of text based on the IRC object's\n        casemapping (rfc1459 or ascii).\"\"\"\n        if self.proto.casemapping == 'rfc1459':\n            text = text.replace('{', '[')\n            text = text.replace('}', ']')\n            text = text.replace('|', '\\\\')\n            text = text.replace('~', '^')\n        # Encode the text as bytes first, and then lowercase it so that only ASCII characters are\n        # changed. Unicode in channel names, etc. is case sensitive because IRC is just that old of\n        # a protocol!!!\n        return text.encode().lower().decode()\n\n    def parseModes(self, target, args):\n        \"\"\"Parses a modestring list into a list of (mode, argument) tuples.\n        ['+mitl-o', '3', 'person'] => [('+m', None), ('+i', None), ('+t', None), ('+l', '3'), ('-o', 'person')]\n        \"\"\"\n        # http://www.irc.org/tech_docs/005.html\n        # A = Mode that adds or removes a nick or address to a list. Always has a parameter.\n        # B = Mode that changes a setting and always has a parameter.\n        # C = Mode that changes a setting and only has a parameter when set.\n        # D = Mode that changes a setting and never has a parameter.\n\n        if type(args) == str:\n            # If the modestring was given as a string, split it into a list.\n            args = args.split()\n\n        assert args, 'No valid modes were supplied!'\n        usermodes = not utils.isChannel(target)\n        prefix = ''\n        modestring = args[0]\n        args = args[1:]\n        if usermodes:\n            log.debug('(%s) Using self.umodes for this query: %s', self.name, self.umodes)\n\n            if target not in self.users:\n                log.debug('(%s) Possible desync! Mode target %s is not in the users index.', self.name, target)\n                return []  # Return an empty mode list\n\n            supported_modes = self.umodes\n            oldmodes = self.users[target].modes\n        else:\n            log.debug('(%s) Using self.cmodes for this query: %s', self.name, self.cmodes)\n\n            supported_modes = self.cmodes\n            oldmodes = self.channels[target].modes\n        res = []\n        for mode in modestring:\n            if mode in '+-':\n                prefix = mode\n            else:\n                if not prefix:\n                    prefix = '+'\n                arg = None\n                log.debug('Current mode: %s%s; args left: %s', prefix, mode, args)\n                try:\n                    if mode in self.prefixmodes and not usermodes:\n                        # We're setting a prefix mode on someone (e.g. +o user1)\n                        log.debug('Mode %s: This mode is a prefix mode.', mode)\n                        arg = args.pop(0)\n                        # Convert nicks to UIDs implicitly; most IRCds will want\n                        # this already.\n                        arg = self.nickToUid(arg) or arg\n                        if arg not in self.users:  # Target doesn't exist, skip it.\n                            log.debug('(%s) Skipping setting mode \"%s %s\"; the '\n                                      'target doesn\\'t seem to exist!', self.name,\n                                      mode, arg)\n                            continue\n                    elif mode in (supported_modes['*A'] + supported_modes['*B']):\n                        # Must have parameter.\n                        log.debug('Mode %s: This mode must have parameter.', mode)\n                        arg = args.pop(0)\n                        if prefix == '-':\n                            if mode in supported_modes['*B'] and arg == '*':\n                                # Charybdis allows unsetting +k without actually\n                                # knowing the key by faking the argument when unsetting\n                                # as a single \"*\".\n                                # We'd need to know the real argument of +k for us to\n                                # be able to unset the mode.\n                                oldarg = dict(oldmodes).get(mode)\n                                if oldarg:\n                                    # Set the arg to the old one on the channel.\n                                    arg = oldarg\n                                    log.debug(\"Mode %s: coersing argument of '*' to %r.\", mode, arg)\n\n                            log.debug('(%s) parseModes: checking if +%s %s is in old modes list: %s', self.name, mode, arg, oldmodes)\n\n                            if (mode, arg) not in oldmodes:\n                                # Ignore attempts to unset bans that don't exist.\n                                log.debug(\"(%s) parseModes(): ignoring removal of non-existent list mode +%s %s\", self.name, mode, arg)\n                                continue\n\n                    elif prefix == '+' and mode in supported_modes['*C']:\n                        # Only has parameter when setting.\n                        log.debug('Mode %s: Only has parameter when setting.', mode)\n                        arg = args.pop(0)\n                except IndexError:\n                    log.warning('(%s/%s) Error while parsing mode %r: mode requires an '\n                                'argument but none was found. (modestring: %r)',\n                                self.name, target, mode, modestring)\n                    continue  # Skip this mode; don't error out completely.\n                res.append((prefix + mode, arg))\n        return res\n\n    def applyModes(self, target, changedmodes):\n        \"\"\"Takes a list of parsed IRC modes, and applies them on the given target.\n\n        The target can be either a channel or a user; this is handled automatically.\"\"\"\n        usermodes = not utils.isChannel(target)\n        log.debug('(%s) Using usermodes for this query? %s', self.name, usermodes)\n\n        try:\n            if usermodes:\n                old_modelist = self.users[target].modes\n                supported_modes = self.umodes\n            else:\n                old_modelist = self.channels[target].modes\n                supported_modes = self.cmodes\n        except KeyError:\n            log.warning('(%s) Possible desync? Mode target %s is unknown.', self.name, target)\n            return\n\n        modelist = set(old_modelist)\n        log.debug('(%s) Applying modes %r on %s (initial modelist: %s)', self.name, changedmodes, target, modelist)\n        for mode in changedmodes:\n            # Chop off the +/- part that parseModes gives; it's meaningless for a mode list.\n            try:\n                real_mode = (mode[0][1], mode[1])\n            except IndexError:\n                real_mode = mode\n\n            if not usermodes:\n                # We only handle +qaohv for now. Iterate over every supported mode:\n                # if the IRCd supports this mode and it is the one being set, add/remove\n                # the person from the corresponding prefix mode list (e.g. c.prefixmodes['op']\n                # for ops).\n                for pmode, pmodelist in self.channels[target].prefixmodes.items():\n                    if pmode in self.cmodes and real_mode[0] == self.cmodes[pmode]:\n                        log.debug('(%s) Initial prefixmodes list: %s', self.name, pmodelist)\n                        if mode[0][0] == '+':\n                            pmodelist.add(mode[1])\n                        else:\n                            pmodelist.discard(mode[1])\n\n                        log.debug('(%s) Final prefixmodes list: %s', self.name, pmodelist)\n\n                if real_mode[0] in self.prefixmodes:\n                    # Don't add prefix modes to IrcChannel.modes; they belong in the\n                    # prefixmodes mapping handled above.\n                    log.debug('(%s) Not adding mode %s to IrcChannel.modes because '\n                              'it\\'s a prefix mode.', self.name, str(mode))\n                    continue\n\n            if mode[0][0] != '-':\n                # We're adding a mode\n                existing = [m for m in modelist if m[0] == real_mode[0] and m[1] != real_mode[1]]\n                if existing and real_mode[1] and real_mode[0] not in self.cmodes['*A']:\n                    # The mode we're setting takes a parameter, but is not a list mode (like +beI).\n                    # Therefore, only one version of it can exist at a time, and we must remove\n                    # any old modepairs using the same letter. Otherwise, we'll get duplicates when,\n                    # for example, someone sets mode \"+l 30\" on a channel already set \"+l 25\".\n                    log.debug('(%s) Old modes for mode %r exist on %s, removing them: %s',\n                              self.name, real_mode, target, str(existing))\n                    [modelist.discard(oldmode) for oldmode in existing]\n                modelist.add(real_mode)\n                log.debug('(%s) Adding mode %r on %s', self.name, real_mode, target)\n            else:\n                log.debug('(%s) Removing mode %r on %s', self.name, real_mode, target)\n                # We're removing a mode\n                if real_mode[1] is None:\n                    # We're removing a mode that only takes arguments when setting.\n                    # Remove all mode entries that use the same letter as the one\n                    # we're unsetting.\n                    for oldmode in modelist.copy():\n                        if oldmode[0] == real_mode[0]:\n                            modelist.discard(oldmode)\n                else:\n                    modelist.discard(real_mode)\n        log.debug('(%s) Final modelist: %s', self.name, modelist)\n        try:\n            if usermodes:\n                self.users[target].modes = modelist\n            else:\n                self.channels[target].modes = modelist\n        except KeyError:\n            log.warning(\"(%s) Invalid MODE target %s (usermodes=%s)\", self.name, target, usermodes)\n\n    @staticmethod\n    def _flip(mode):\n        \"\"\"Flips a mode character.\"\"\"\n        # Make it a list first, strings don't support item assignment\n        mode = list(mode)\n        if mode[0] == '-':  # Query is something like \"-n\"\n            mode[0] = '+'  # Change it to \"+n\"\n        elif mode[0] == '+':\n            mode[0] = '-'\n        else:  # No prefix given, assume +\n            mode.insert(0, '-')\n        return ''.join(mode)\n\n    def reverseModes(self, target, modes, oldobj=None):\n        \"\"\"Reverses/Inverts the mode string or mode list given.\n\n        Optionally, an oldobj argument can be given to look at an earlier state of\n        a channel/user object, e.g. for checking the op status of a mode setter\n        before their modes are processed and added to the channel state.\n\n        This function allows both mode strings or mode lists. Example uses:\n            \"+mi-lk test => \"-mi+lk test\"\n            \"mi-k test => \"-mi+k test\"\n            [('+m', None), ('+r', None), ('+l', '3'), ('-o', 'person')\n             => {('-m', None), ('-r', None), ('-l', None), ('+o', 'person')})\n            {('s', None), ('+o', 'whoever') => {('-s', None), ('-o', 'whoever')})\n        \"\"\"\n        origtype = type(modes)\n        # If the query is a string, we have to parse it first.\n        if origtype == str:\n            modes = self.parseModes(target, modes.split(\" \"))\n        # Get the current mode list first.\n        if utils.isChannel(target):\n            c = oldobj or self.channels[target]\n            oldmodes = c.modes.copy()\n            possible_modes = self.cmodes.copy()\n            # For channels, this also includes the list of prefix modes.\n            possible_modes['*A'] += ''.join(self.prefixmodes)\n            for name, userlist in c.prefixmodes.items():\n                try:\n                    oldmodes.update([(self.cmodes[name], u) for u in userlist])\n                except KeyError:\n                    continue\n        else:\n            oldmodes = self.users[target].modes\n            possible_modes = self.umodes\n        newmodes = []\n        log.debug('(%s) reverseModes: old/current mode list for %s is: %s', self.name,\n                   target, oldmodes)\n        for char, arg in modes:\n            # Mode types:\n            # A = Mode that adds or removes a nick or address to a list. Always has a parameter.\n            # B = Mode that changes a setting and always has a parameter.\n            # C = Mode that changes a setting and only has a parameter when set.\n            # D = Mode that changes a setting and never has a parameter.\n            mchar = char[-1]\n            if mchar in possible_modes['*B'] + possible_modes['*C']:\n                # We need to find the current mode list, so we can reset arguments\n                # for modes that have arguments. For example, setting +l 30 on a channel\n                # that had +l 50 set should give \"+l 30\", not \"-l\".\n                oldarg = [m for m in oldmodes if m[0] == mchar]\n                if oldarg:  # Old mode argument for this mode existed, use that.\n                    oldarg = oldarg[0]\n                    mpair = ('+%s' % oldarg[0], oldarg[1])\n                else:  # Not found, flip the mode then.\n                    # Mode takes no arguments when unsetting.\n                    if mchar in possible_modes['*C'] and char[0] != '-':\n                        arg = None\n                    mpair = (self._flip(char), arg)\n            else:\n                mpair = (self._flip(char), arg)\n            if char[0] != '-' and (mchar, arg) in oldmodes:\n                # Mode is already set.\n                log.debug(\"(%s) reverseModes: skipping reversing '%s %s' with %s since we're \"\n                          \"setting a mode that's already set.\", self.name, char, arg, mpair)\n                continue\n            elif char[0] == '-' and (mchar, arg) not in oldmodes and mchar in possible_modes['*A']:\n                # We're unsetting a prefixmode that was never set - don't set it in response!\n                # Charybdis lacks verification for this server-side.\n                log.debug(\"(%s) reverseModes: skipping reversing '%s %s' with %s since it \"\n                          \"wasn't previously set.\", self.name, char, arg, mpair)\n                continue\n            newmodes.append(mpair)\n\n        log.debug('(%s) reverseModes: new modes: %s', self.name, newmodes)\n        if origtype == str:\n            # If the original query is a string, send it back as a string.\n            return self.joinModes(newmodes)\n        else:\n            return set(newmodes)\n\n    @staticmethod\n    def joinModes(modes, sort=False):\n        \"\"\"Takes a list of (mode, arg) tuples in parseModes() format, and\n        joins them into a string.\n\n        See testJoinModes in tests/test_utils.py for some examples.\"\"\"\n        prefix = '+'  # Assume we're adding modes unless told otherwise\n        modelist = ''\n        args = []\n\n        # Sort modes alphabetically like a conventional IRCd.\n        if sort:\n            modes = sorted(modes)\n\n        for modepair in modes:\n            mode, arg = modepair\n            assert len(mode) in (1, 2), \"Incorrect length of a mode (received %r)\" % mode\n            try:\n                # If the mode has a prefix, use that.\n                curr_prefix, mode = mode\n            except ValueError:\n                # If not, the current prefix stays the same; move on to the next\n                # modepair.\n                pass\n            else:\n                # If the prefix of this mode isn't the same as the last one, add\n                # the prefix to the modestring. This prevents '+nt-lk' from turning\n                # into '+n+t-l-k' or '+ntlk'.\n                if prefix != curr_prefix:\n                    modelist += curr_prefix\n                    prefix = curr_prefix\n            modelist += mode\n            if arg is not None:\n                args.append(arg)\n        if not modelist.startswith(('+', '-')):\n            # Our starting mode didn't have a prefix with it. Assume '+'.\n            modelist = '+' + modelist\n        if args:\n            # Add the args if there are any.\n            modelist += ' %s' % ' '.join(args)\n        return modelist\n\n    @classmethod\n    def wrapModes(cls, modes, limit, max_modes_per_msg=0):\n        \"\"\"\n        Takes a list of modes and wraps it across multiple lines.\n        \"\"\"\n        strings = []\n\n        # This process is slightly trickier than just wrapping arguments, because modes create\n        # positional arguments that can't be separated from its character.\n        queued_modes = []\n        total_length = 0\n\n        last_prefix = '+'\n        orig_modes = modes.copy()\n        modes = list(modes)\n        while modes:\n            # PyLink mode lists come in the form [('+t', None), ('-b', '*!*@someone'), ('+l', 3)]\n            # The +/- part is optional depending on context, and should either:\n            # 1) The prefix of the last mode.\n            # 2) + (adding modes), if no prefix was ever given\n            next_mode = modes.pop(0)\n\n            modechar, arg = next_mode\n            prefix = modechar[0]\n            if prefix not in '+-':\n                prefix = last_prefix\n                # Explicitly add the prefix to the mode character to prevent\n                # ambiguity when passing it to joinModes().\n                modechar = prefix + modechar\n                # XXX: because tuples are immutable, we have to replace the entire modepair..\n                next_mode = (modechar, arg)\n\n            # Figure out the length that the next mode will add to the buffer. If we're changing\n            # from + to - (setting to removing modes) or vice versa, we'll need two characters\n            # (\"+\" or \"-\") plus the mode char itself.\n            next_length = 1\n            if prefix != last_prefix:\n                next_length += 1\n\n            # Replace the last_prefix with the current one for the next iteration.\n            last_prefix = prefix\n\n            if arg:\n                # This mode has an argument, so add the length of that and a space.\n                next_length += 1\n                next_length += len(arg)\n\n            assert next_length <= limit, \\\n                \"wrapModes: Mode %s is too long for the given length %s\" % (next_mode, limit)\n\n            # Check both message length and max. modes per msg if enabled.\n            if (next_length + total_length) <= limit and ((not max_modes_per_msg) or len(queued_modes) < max_modes_per_msg):\n                # We can fit this mode in the next message; add it.\n                total_length += next_length\n                log.debug('wrapModes: Adding mode %s to queued modes', str(next_mode))\n                queued_modes.append(next_mode)\n                log.debug('wrapModes: queued modes: %s', queued_modes)\n            else:\n                # Otherwise, create a new message by joining the previous queue.\n                # Then, add our current mode.\n                strings.append(cls.joinModes(queued_modes))\n                queued_modes.clear()\n\n                log.debug('wrapModes: cleared queue (length %s) and now adding %s', limit, str(next_mode))\n                queued_modes.append(next_mode)\n                total_length = next_length\n        else:\n            # Everything fit in one line, so just use that.\n            strings.append(cls.joinModes(queued_modes))\n\n        log.debug('wrapModes: returning %s for %s', strings, orig_modes)\n        return strings\n\n    def version(self):\n        \"\"\"\n        Returns a detailed version string including the PyLink daemon version,\n        the protocol module in use, and the server hostname.\n        \"\"\"\n        fullversion = 'PyLink-%s. %s :[protocol:%s]' % (__version__, self.hostname(), self.protoname)\n        return fullversion\n\n    def hostname(self):\n        \"\"\"\n        Returns the server hostname used by PyLink on the given server.\n        \"\"\"\n        return self.serverdata.get('hostname', world.fallback_hostname)\n\n    ### State checking functions\n    def nickToUid(self, nick):\n        \"\"\"Looks up the UID of a user with the given nick, if one is present.\"\"\"\n        nick = self.toLower(nick)\n        for k, v in self.users.copy().items():\n            if self.toLower(v.nick) == nick:\n                return k\n\n    def isInternalClient(self, numeric):\n        \"\"\"\n        Returns whether the given client numeric (UID) is a PyLink client.\n        \"\"\"\n        sid = self.getServer(numeric)\n        if sid and self.servers[sid].internal:\n            return True\n        return False\n\n    def isInternalServer(self, sid):\n        \"\"\"Returns whether the given SID is an internal PyLink server.\"\"\"\n        return (sid in self.servers and self.servers[sid].internal)\n\n    def getServer(self, numeric):\n        \"\"\"Finds the SID of the server a user is on.\"\"\"\n        userobj = self.users.get(numeric)\n        if userobj:\n            return userobj.server\n\n    def isManipulatableClient(self, uid):\n        \"\"\"\n        Returns whether the given user is marked as an internal, manipulatable\n        client. Usually, automatically spawned services clients should have this\n        set True to prevent interactions with opers (like mode changes) from\n        causing desyncs.\n        \"\"\"\n        return self.isInternalClient(uid) and self.users[uid].manipulatable\n\n    def getServiceBot(self, uid):\n        \"\"\"\n        Checks whether the given UID is a registered service bot. If True,\n        returns the cooresponding ServiceBot object.\n        \"\"\"\n        userobj = self.users.get(uid)\n        if not userobj:\n            return False\n\n        # Look for the \"service\" attribute in the IrcUser object, if one exists.\n        try:\n            sname = userobj.service\n            # Warn if the service name we fetched isn't a registered service.\n            if sname not in world.services.keys():\n                log.warning(\"(%s) User %s / %s had a service bot record to a service that doesn't \"\n                            \"exist (%s)!\", self.name, uid, userobj.nick, sname)\n            return world.services.get(sname)\n        except AttributeError:\n            return False\n\n    def getHostmask(self, user, realhost=False, ip=False):\n        \"\"\"\n        Returns the hostmask of the given user, if present. If the realhost option\n        is given, return the real host of the user instead of the displayed host.\n        If the ip option is given, return the IP address of the user (this overrides\n        realhost).\"\"\"\n        userobj = self.users.get(user)\n\n        try:\n            nick = userobj.nick\n        except AttributeError:\n            nick = '<unknown-nick>'\n\n        try:\n            ident = userobj.ident\n        except AttributeError:\n            ident = '<unknown-ident>'\n\n        try:\n            if ip:\n                host = userobj.ip\n            elif realhost:\n                host = userobj.realhost\n            else:\n                host = userobj.host\n        except AttributeError:\n            host = '<unknown-host>'\n\n        return '%s!%s@%s' % (nick, ident, host)\n\n    def getFriendlyName(self, entityid):\n        \"\"\"\n        Returns the friendly name of a SID or UID (server name for SIDs, nick for UID).\n        \"\"\"\n        if entityid in self.servers:\n            return self.servers[entityid].name\n        elif entityid in self.users:\n            return self.users[entityid].nick\n        else:\n            raise KeyError(\"Unknown UID/SID %s\" % entityid)\n\n    def getFullNetworkName(self):\n        \"\"\"\n        Returns the full network name (as defined by the \"netname\" option), or the\n        short network name if that isn't defined.\n        \"\"\"\n        return self.serverdata.get('netname', self.name)\n\n    def isOper(self, uid, allowAuthed=True, allowOper=True):\n        \"\"\"\n        Returns whether the given user has operator status on PyLink. This can be achieved\n        by either identifying to PyLink as admin (if allowAuthed is True),\n        or having user mode +o set (if allowOper is True). At least one of\n        allowAuthed or allowOper must be True for this to give any meaningful\n        results.\n        \"\"\"\n        if uid in self.users:\n            if allowOper and (\"o\", None) in self.users[uid].modes:\n                return True\n            elif allowAuthed and self.users[uid].account:\n                return True\n        return False\n\n    def checkAuthenticated(self, uid, allowAuthed=True, allowOper=True):\n        \"\"\"\n        Checks whether the given user has operator status on PyLink, raising\n        NotAuthorizedError and logging the access denial if not.\n        \"\"\"\n        log.warning(\"(%s) Irc.checkAuthenticated() is deprecated as of PyLink 1.2 and may be \"\n                    \"removed in a future relase. Consider migrating to the PyLink Permissions API.\",\n                    self.name)\n        lastfunc = inspect.stack()[1][3]\n        if not self.isOper(uid, allowAuthed=allowAuthed, allowOper=allowOper):\n            log.warning('(%s) Access denied for %s calling %r', self.name,\n                        self.getHostmask(uid), lastfunc)\n            raise utils.NotAuthorizedError(\"You are not authenticated!\")\n        return True\n\n    def matchHost(self, glob, target, ip=True, realhost=True):\n        \"\"\"\n        Checks whether the given host, or given UID's hostmask matches the given nick!user@host\n        glob.\n\n        If the target given is a UID, and the 'ip' or 'realhost' options are True, this will also\n        match against the target's IP address and real host, respectively.\n\n        This function respects IRC casemappings (rfc1459 and ascii). If the given target is a UID,\n        and the 'ip' option is enabled, the host portion of the glob is also matched as a CIDR\n        range.\n        \"\"\"\n        # Get the corresponding casemapping value used by ircmatch.\n        if self.proto.casemapping == 'rfc1459':\n            casemapping = 0\n        else:\n            casemapping = 1\n\n        # Try to convert target into a UID. If this fails, it's probably a hostname.\n        target = self.nickToUid(target) or target\n\n        # Prepare a list of hosts to check against.\n        if target in self.users:\n            if glob.startswith(('$', '!$')):\n                # !$exttarget inverts the given match.\n                invert = glob.startswith('!$')\n\n                # Exttargets start with $. Skip regular ban matching and find the matching ban handler.\n                glob = glob.lstrip('$!')\n                exttargetname = glob.split(':', 1)[0]\n                handler = world.exttarget_handlers.get(exttargetname)\n\n                if handler:\n                    # Handler exists. Return what it finds.\n                    result = handler(self, glob, target)\n                    log.debug('(%s) Got %s from exttarget %s in matchHost() glob $%s for target %s',\n                              self.name, result, exttargetname, glob, target)\n                    if invert:  # Anti-exttarget was specified.\n                        result = not result\n                    return result\n                else:\n                    log.debug('(%s) Unknown exttarget %s in matchHost() glob $%s', self.name,\n                              exttargetname, glob)\n                    return False\n\n            hosts = {self.getHostmask(target)}\n\n            if ip:\n                hosts.add(self.getHostmask(target, ip=True))\n\n                # HACK: support CIDR hosts in the hosts portion\n                try:\n                    header, cidrtarget = glob.split('@', 1)\n                    log.debug('(%s) Processing CIDRs for %s (full host: %s)', self.name,\n                              cidrtarget, glob)\n                    # Try to parse the host portion as a CIDR range\n                    network = ipaddress.ip_network(cidrtarget)\n\n                    log.debug('(%s) Found CIDR for %s, replacing target host with IP %s', self.name,\n                              realhost, target)\n                    real_ip = self.users[target].ip\n                    if ipaddress.ip_address(real_ip) in network:\n                        # If the CIDR matches, hack around the host matcher by pretending that\n                        # the lookup target was the IP and not the CIDR range!\n                        glob = '@'.join((header, real_ip))\n                except ValueError:\n                    pass\n\n            if realhost:\n                hosts.add(self.getHostmask(target, realhost=True))\n\n        else:  # We were given a host, use that.\n            hosts = [target]\n\n        # Iterate over the hosts to match using ircmatch.\n        for host in hosts:\n            if ircmatch.match(casemapping, glob, host):\n                return True\n\n        return False\n\nclass IrcUser():\n    \"\"\"PyLink IRC user class.\"\"\"\n    def __init__(self, nick, ts, uid, server, ident='null', host='null',\n                 realname='PyLink dummy client', realhost='null',\n                 ip='0.0.0.0', manipulatable=False, opertype='IRC Operator'):\n        self.nick = nick\n        self.ts = ts\n        self.uid = uid\n        self.ident = ident\n        self.host = host\n        self.realhost = realhost\n        self.ip = ip\n        self.realname = realname\n        self.modes = set()  # Tracks user modes\n        self.server = server\n\n        # Tracks PyLink identification status\n        self.account = ''\n\n        # Tracks oper type (for display only)\n        self.opertype = opertype\n\n        # Tracks external services identification status\n        self.services_account = ''\n\n        # Tracks channels the user is in\n        self.channels = set()\n\n        # Tracks away message status\n        self.away = ''\n\n        # This sets whether the client should be marked as manipulatable.\n        # Plugins like bots.py's commands should take caution against\n        # manipulating these \"protected\" clients, to prevent desyncs and such.\n        # For \"serious\" service clients, this should always be False.\n        self.manipulatable = manipulatable\n\n    def __repr__(self):\n        return 'IrcUser(%s/%s)' % (self.uid, self.nick)\n\nclass IrcServer():\n    \"\"\"PyLink IRC server class.\n\n    uplink: The SID of this IrcServer instance's uplink. This is set to None\n            for the main PyLink PseudoServer!\n    name: The name of the server.\n    internal: Whether the server is an internal PyLink PseudoServer.\n    \"\"\"\n\n    def __init__(self, uplink, name, internal=False, desc=\"(None given)\"):\n        self.uplink = uplink\n        self.users = set()\n        self.internal = internal\n        self.name = name.lower()\n        self.desc = desc\n\n    def __repr__(self):\n        return 'IrcServer(%s)' % self.name\n\nclass IrcChannel():\n    \"\"\"PyLink IRC channel class.\"\"\"\n    def __init__(self, name=None):\n        # Initialize variables, such as the topic, user list, TS, who's opped, etc.\n        self.users = set()\n        self.modes = set()\n        self.topic = ''\n        self.ts = int(time.time())\n        self.prefixmodes = {'op': set(), 'halfop': set(), 'voice': set(),\n                            'owner': set(), 'admin': set()}\n\n        # Determines whether a topic has been set here or not. Protocol modules\n        # should set this.\n        self.topicset = False\n\n        # Saves the channel name (may be useful to plugins, etc.)\n        self.name = name\n\n    def __repr__(self):\n        return 'IrcChannel(%s)' % self.name\n\n    def removeuser(self, target):\n        \"\"\"Removes a user from a channel.\"\"\"\n        for s in self.prefixmodes.values():\n            s.discard(target)\n        self.users.discard(target)\n\n    def deepcopy(self):\n        \"\"\"Returns a deep copy of the channel object.\"\"\"\n        return deepcopy(self)\n\n    def isVoice(self, uid):\n        \"\"\"Returns whether the given user is voice in the channel.\"\"\"\n        return uid in self.prefixmodes['voice']\n\n    def isHalfop(self, uid):\n        \"\"\"Returns whether the given user is halfop in the channel.\"\"\"\n        return uid in self.prefixmodes['halfop']\n\n    def isOp(self, uid):\n        \"\"\"Returns whether the given user is op in the channel.\"\"\"\n        return uid in self.prefixmodes['op']\n\n    def isAdmin(self, uid):\n        \"\"\"Returns whether the given user is admin (&) in the channel.\"\"\"\n        return uid in self.prefixmodes['admin']\n\n    def isOwner(self, uid):\n        \"\"\"Returns whether the given user is owner (~) in the channel.\"\"\"\n        return uid in self.prefixmodes['owner']\n\n    def isVoicePlus(self, uid):\n        \"\"\"Returns whether the given user is voice or above in the channel.\"\"\"\n        # If the user has any prefix mode, it has to be voice or greater.\n        return bool(self.getPrefixModes(uid))\n\n    def isHalfopPlus(self, uid):\n        \"\"\"Returns whether the given user is halfop or above in the channel.\"\"\"\n        for mode in ('halfop', 'op', 'admin', 'owner'):\n            if uid in self.prefixmodes[mode]:\n                return True\n        return False\n\n    def isOpPlus(self, uid):\n        \"\"\"Returns whether the given user is op or above in the channel.\"\"\"\n        for mode in ('op', 'admin', 'owner'):\n            if uid in self.prefixmodes[mode]:\n                return True\n        return False\n\n    @staticmethod\n    def sortPrefixes(key):\n        \"\"\"\n        Implements a sorted()-compatible sorter for prefix modes, giving each one a\n        numeric value.\n        \"\"\"\n        values = {'owner': 100, 'admin': 10, 'op': 5, 'halfop': 4, 'voice': 3}\n\n        # Default to highest value (1000) for unknown modes, should we choose to\n        # support them.\n        return values.get(key, 1000)\n\n    def getPrefixModes(self, uid, prefixmodes=None):\n        \"\"\"Returns a list of all named prefix modes the given user has in the channel.\n\n        Optionally, a prefixmodes argument can be given to look at an earlier state of\n        the channel's prefix modes mapping, e.g. for checking the op status of a mode\n        setter before their modes are processed and added to the channel state.\n        \"\"\"\n\n        if uid not in self.users:\n            raise KeyError(\"User %s does not exist or is not in the channel\" % uid)\n\n        result = []\n        prefixmodes = prefixmodes or self.prefixmodes\n\n        for mode, modelist in prefixmodes.items():\n            if uid in modelist:\n                result.append(mode)\n\n        return sorted(result, key=self.sortPrefixes)\n\nclass Protocol():\n    \"\"\"Base Protocol module class for PyLink.\"\"\"\n    def __init__(self, irc):\n        self.irc = irc\n        self.casemapping = 'rfc1459'\n        self.hook_map = {}\n\n        # Lock for updateTS to make sure only one thread can change the channel TS at one time.\n        self.ts_lock = threading.Lock()\n\n        # Lists required conf keys for the server block.\n        self.conf_keys = {'ip', 'port', 'hostname', 'sid', 'sidrange', 'protocol', 'sendpass',\n                          'recvpass'}\n\n        # Defines a set of PyLink protocol capabilities\n        self.protocol_caps = set()\n\n    def validateServerConf(self):\n        \"\"\"Validates that the server block given contains the required keys.\"\"\"\n        for k in self.conf_keys:\n            assert k in self.irc.serverdata, \"Missing option %r in server block for network %s.\" % (k, self.irc.name)\n\n        port = self.irc.serverdata['port']\n        assert type(port) == int and 0 < port < 65535, \"Invalid port %r for network %s\" % (port, self.irc.name)\n\n    @staticmethod\n    def parseArgs(args):\n        \"\"\"\n        Parses a string or list of of RFC1459-style arguments, where \":\" may\n        be used for multi-word arguments that last until the end of a line.\n        \"\"\"\n        if isinstance(args, str):\n            args = args.split(' ')\n\n        real_args = []\n        for idx, arg in enumerate(args):\n            if arg.startswith(':') and idx != 0:\n                # \":\" is used to begin multi-word arguments that last until the end of the message.\n                # Use list splicing here to join them into one argument, and then add it to our list of args.\n                joined_arg = ' '.join(args[idx:])[1:]  # Cut off the leading : as well\n                real_args.append(joined_arg)\n                break\n            real_args.append(arg)\n\n        return real_args\n\n    def hasCap(self, capab):\n        \"\"\"\n        Returns whether this protocol module instance has the requested capability.\n        \"\"\"\n        return capab.lower() in self.protocol_caps\n\n    def removeClient(self, numeric):\n        \"\"\"Internal function to remove a client from our internal state.\"\"\"\n        for c, v in self.irc.channels.copy().items():\n            v.removeuser(numeric)\n            # Clear empty non-permanent channels.\n            if not (self.irc.channels[c].users or ((self.irc.cmodes.get('permanent'), None) in self.irc.channels[c].modes)):\n                del self.irc.channels[c]\n            assert numeric not in v.users, \"IrcChannel's removeuser() is broken!\"\n\n        sid = self.irc.getServer(numeric)\n        log.debug('Removing client %s from self.irc.users', numeric)\n        del self.irc.users[numeric]\n        log.debug('Removing client %s from self.irc.servers[%s].users', numeric, sid)\n        self.irc.servers[sid].users.discard(numeric)\n\n    def updateTS(self, sender, channel, their_ts, modes=[]):\n        \"\"\"\n        Merges modes of a channel given the remote TS and a list of modes.\n        \"\"\"\n\n        # Okay, so the situation is that we have 6 possible TS/sender combinations:\n\n        #                       | our TS lower | TS equal | their TS lower\n        # mode origin is us     |   OVERWRITE  |   MERGE  |    IGNORE\n        # mode origin is uplink |    IGNORE    |   MERGE  |   OVERWRITE\n\n        def _clear():\n            log.debug(\"(%s) Clearing local modes from channel %s due to TS change\", self.irc.name,\n                      channel)\n            self.irc.channels[channel].modes.clear()\n            for p in self.irc.channels[channel].prefixmodes.values():\n                for user in p.copy():\n                    if not self.irc.isInternalClient(user):\n                        p.discard(user)\n\n        def _apply():\n            if modes:\n                log.debug(\"(%s) Applying modes on channel %s (TS ok)\", self.irc.name,\n                          channel)\n                self.irc.applyModes(channel, modes)\n\n        # Use a lock so only one thread can change a channel's TS at once: this prevents race\n        # conditions from desyncing the channel list.\n        with self.ts_lock:\n            our_ts = self.irc.channels[channel].ts\n            assert type(our_ts) == int, \"Wrong type for our_ts (expected int, got %s)\" % type(our_ts)\n            assert type(their_ts) == int, \"Wrong type for their_ts (expected int, got %s)\" % type(their_ts)\n\n            # Check if we're the mode sender based on the UID / SID given.\n            our_mode = self.irc.isInternalClient(sender) or self.irc.isInternalServer(sender)\n\n            log.debug(\"(%s/%s) our_ts: %s; their_ts: %s; is the mode origin us? %s\", self.irc.name,\n                      channel, our_ts, their_ts, our_mode)\n\n            if their_ts == our_ts:\n                log.debug(\"(%s/%s) remote TS of %s is equal to our %s; mode query %s\",\n                          self.irc.name, channel, their_ts, our_ts, modes)\n                # Their TS is equal to ours. Merge modes.\n                _apply()\n\n            elif (their_ts < our_ts):\n                if their_ts < 750000:\n                    log.warning('(%s) Possible desync? Not setting bogus TS %s on channel %s', self.irc.name, their_ts, channel)\n                else:\n                    log.debug('(%s) Resetting channel TS of %s from %s to %s (remote has lower TS)',\n                              self.irc.name, channel, our_ts, their_ts)\n                    self.irc.channels[channel].ts = their_ts\n\n                # Remote TS was lower and we're receiving modes. Clear the modelist and apply theirs.\n\n                _clear()\n                _apply()\n\n    def _getSid(self, sname):\n        \"\"\"Returns the SID of a server with the given name, if present.\"\"\"\n        name = sname.lower()\n        for k, v in self.irc.servers.items():\n            if v.name.lower() == name:\n                return k\n        else:\n            return sname  # Fall back to given text instead of None\n\n    def _getUid(self, target):\n        \"\"\"Converts a nick argument to its matching UID. This differs from irc.nickToUid()\n        in that it returns the original text instead of None, if no matching nick is found.\"\"\"\n        target = self.irc.nickToUid(target) or target\n        return target\n\n    @classmethod\n    def parsePrefixedArgs(cls, args):\n        \"\"\"Similar to parseArgs(), but stripping leading colons from the first argument\n        of a line (usually the sender field).\"\"\"\n        args = cls.parseArgs(args)\n        args[0] = args[0].split(':', 1)[1]\n        return args\n\n    def _squit(self, numeric, command, args):\n        \"\"\"Handles incoming SQUITs.\"\"\"\n\n        split_server = self._getSid(args[0])\n\n        # Normally we'd only need to check for our SID as the SQUIT target, but Nefarious\n        # actually uses the uplink server as the SQUIT target.\n        # <- ABAAE SQ nefarious.midnight.vpn 0 :test\n        if split_server in (self.irc.sid, self.irc.uplink):\n            raise ProtocolError('SQUIT received: (reason: %s)' % args[-1])\n\n        affected_users = []\n        affected_nicks = defaultdict(list)\n        log.debug('(%s) Splitting server %s (reason: %s)', self.irc.name, split_server, args[-1])\n\n        if split_server not in self.irc.servers:\n            log.warning(\"(%s) Tried to split a server (%s) that didn't exist!\", self.irc.name, split_server)\n            return\n\n        # Prevent RuntimeError: dictionary changed size during iteration\n        old_servers = self.irc.servers.copy()\n        old_channels = self.irc.channels.copy()\n\n        # Cycle through our list of servers. If any server's uplink is the one that is being SQUIT,\n        # remove them and all their users too.\n        for sid, data in old_servers.items():\n            if data.uplink == split_server:\n                log.debug('Server %s also hosts server %s, removing those users too...', split_server, sid)\n                # Recursively run SQUIT on any other hubs this server may have been connected to.\n                args = self._squit(sid, 'SQUIT', [sid, \"0\",\n                                   \"PyLink: Automatically splitting leaf servers of %s\" % sid])\n                affected_users += args['users']\n\n        for user in self.irc.servers[split_server].users.copy():\n            affected_users.append(user)\n            nick = self.irc.users[user].nick\n\n            # Nicks affected is channel specific for SQUIT:. This makes Clientbot's SQUIT relaying\n            # much easier to implement.\n            for name, cdata in old_channels.items():\n                if user in cdata.users:\n                    affected_nicks[name].append(nick)\n\n            log.debug('Removing client %s (%s)', user, nick)\n            self.removeClient(user)\n\n        serverdata = self.irc.servers[split_server]\n        sname = serverdata.name\n        uplink = serverdata.uplink\n\n        del self.irc.servers[split_server]\n        log.debug('(%s) Netsplit affected users: %s', self.irc.name, affected_users)\n\n        return {'target': split_server, 'users': affected_users, 'name': sname,\n                'uplink': uplink, 'nicks': affected_nicks, 'serverdata': serverdata,\n                'channeldata': old_channels}\n\n    @staticmethod\n    def parseCapabilities(args, fallback=''):\n        \"\"\"\n        Parses a string of capabilities in the 005 / RPL_ISUPPORT format.\n        \"\"\"\n\n        if type(args) == str:\n            args = args.split(' ')\n\n        caps = {}\n        for cap in args:\n            try:\n                # Try to split it as a KEY=VALUE pair.\n                key, value = cap.split('=', 1)\n            except ValueError:\n                key = cap\n                value = fallback\n            caps[key] = value\n\n        return caps\n\n    @staticmethod\n    def parsePrefixes(args):\n        \"\"\"\n        Separates prefixes field like \"(qaohv)~&@%+\" into a dict mapping mode characters to mode\n        prefixes.\n        \"\"\"\n        prefixsearch = re.search(r'\\(([A-Za-z]+)\\)(.*)', args)\n        return dict(zip(prefixsearch.group(1), prefixsearch.group(2)))\n\n    def handle_error(self, numeric, command, args):\n        \"\"\"Handles ERROR messages - these mean that our uplink has disconnected us!\"\"\"\n        raise ProtocolError('Received an ERROR, disconnecting!')\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/jlu5/PyLink/blob/40fa4f71bcd4946e5578beabedd30e308d1e1cdd",
        "file_path": "/plugins/networks.py",
        "source": "\"\"\"Networks plugin - allows you to manipulate connections to various configured networks.\"\"\"\nimport importlib\nimport types\n\nfrom pylinkirc import utils, world, conf, classes\nfrom pylinkirc.log import log\nfrom pylinkirc.coremods import control, permissions\n\n@utils.add_cmd\ndef disconnect(irc, source, args):\n    \"\"\"<network>\n\n    Disconnects the network <network>. When all networks are disconnected, PyLink will automatically exit.\n\n    To reconnect a network disconnected using this command, use REHASH to reload the networks list.\"\"\"\n    permissions.checkPermissions(irc, source, ['networks.disconnect'])\n    try:\n        netname = args[0]\n        network = world.networkobjects[netname]\n    except IndexError:  # No argument given.\n        irc.error('Not enough arguments (needs 1: network name (case sensitive)).')\n        return\n    except KeyError:  # Unknown network.\n        irc.error('No such network \"%s\" (case sensitive).' % netname)\n        return\n    irc.reply(\"Done. If you want to reconnect this network, use the 'rehash' command.\")\n\n    control.remove_network(network)\n\n@utils.add_cmd\ndef autoconnect(irc, source, args):\n    \"\"\"<network> <seconds>\n\n    Sets the autoconnect time for <network> to <seconds>.\n    You can disable autoconnect for a network by setting <seconds> to a negative value.\"\"\"\n    permissions.checkPermissions(irc, source, ['networks.autoconnect'])\n    try:\n        netname = args[0]\n        seconds = float(args[1])\n        network = world.networkobjects[netname]\n    except IndexError:  # Arguments not given.\n        irc.error('Not enough arguments (needs 2: network name (case sensitive), autoconnect time (in seconds)).')\n        return\n    except KeyError:  # Unknown network.\n        irc.error('No such network \"%s\" (case sensitive).' % netname)\n        return\n    except ValueError:\n        irc.error('Invalid argument \"%s\" for <seconds>.' % seconds)\n        return\n    network.serverdata['autoconnect'] = seconds\n    irc.reply(\"Done.\")\n\nremote_parser = utils.IRCParser()\nremote_parser.add_argument('network')\nremote_parser.add_argument('--service', type=str, default='pylink')\nremote_parser.add_argument('command', nargs=utils.IRCParser.REMAINDER)\n@utils.add_cmd\ndef remote(irc, source, args):\n    \"\"\"<network> [--service <service name>] <command>\n\n    Runs <command> on the remote network <network>. Plugin responses sent using irc.reply() are\n    supported and returned here, but others are dropped due to protocol limitations.\"\"\"\n    permissions.checkPermissions(irc, source, ['networks.remote'])\n\n    args = remote_parser.parse_args(args)\n    netname = args.network\n\n    if netname == irc.name:\n        # This would actually throw _remote_reply() into a loop, so check for it here...\n        # XXX: properly fix this.\n        irc.error(\"Cannot remote-send a command to the local network; use a normal command!\")\n        return\n\n    try:\n        remoteirc = world.networkobjects[netname]\n    except KeyError:  # Unknown network.\n        irc.error('No such network \"%s\" (case sensitive).' % netname)\n        return\n\n    if args.service not in world.services:\n        irc.error('Unknown service %r.' % args.service)\n        return\n\n    # Force remoteirc.called_in to something private in order to prevent\n    # accidental information leakage from replies.\n    remoteirc.called_in = remoteirc.called_by = remoteirc.pseudoclient.uid\n\n    # Set the identification override to the caller's account.\n    remoteirc.pseudoclient.account = irc.users[source].account\n\n    def _remote_reply(placeholder_self, text, **kwargs):\n        \"\"\"\n        reply() rerouter for the 'remote' command.\n        \"\"\"\n        assert irc.name != placeholder_self.name, \\\n            \"Refusing to route reply back to the same \" \\\n            \"network, as this would cause a recursive loop\"\n        log.debug('(%s) networks.remote: re-routing reply %r from network %s', irc.name,\n                  text, placeholder_self.name)\n\n        # Override the source option to make sure the source is valid on the local network.\n        if 'source' in kwargs:\n            del kwargs['source']\n        irc.reply(text, source=irc.pseudoclient.uid, **kwargs)\n\n    old_reply = remoteirc.reply\n\n    with remoteirc.reply_lock:\n        try:  # Remotely call the command (use the PyLink client as a dummy user).\n            # Override the remote irc.reply() to send replies HERE.\n            log.debug('(%s) networks.remote: overriding reply() of IRC object %s', irc.name, netname)\n            remoteirc.reply = types.MethodType(_remote_reply, remoteirc)\n            world.services[args.service].call_cmd(remoteirc, remoteirc.pseudoclient.uid,\n                                                  ' '.join(args.command))\n        finally:\n            # Restore the original remoteirc.reply()\n            log.debug('(%s) networks.remote: restoring reply() of IRC object %s', irc.name, netname)\n            remoteirc.reply = old_reply\n            # Remove the identification override after we finish.\n            remoteirc.pseudoclient.account = ''\n\n@utils.add_cmd\ndef reloadproto(irc, source, args):\n    \"\"\"<protocol module name>\n\n    Reloads the given protocol module without restart. You will have to manually disconnect and reconnect any network using the module for changes to apply.\"\"\"\n    permissions.checkPermissions(irc, source, ['networks.reloadproto'])\n    try:\n        name = args[0]\n    except IndexError:\n        irc.error('Not enough arguments (needs 1: protocol module name)')\n        return\n\n    proto = utils.getProtocolModule(name)\n    importlib.reload(proto)\n\n    irc.reply(\"Done. You will have to manually disconnect and reconnect any network using the %r module for changes to apply.\" % name)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/ornl-oxford/genomics-benchmarks/blob/d3e3e490f4c38e4af73d7c78bf697cf906e8a1b6",
        "file_path": "/benchmark/cli.py",
        "source": "\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, \ndetermines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,\nruns the benchmarks, and records the timer results. \"\"\"\n\nimport argparse  # for command line parsing\nimport time  # for benchmark timer\nimport csv  # for writing results\nimport logging\nimport sys\nimport shutil\nfrom benchmark import config, data_service\n\n\ndef get_cli_arguments():\n    \"\"\" Returns command line arguments. \n\n    Returns:\n    args object from an ArgumentParses for fetch data (boolean, from a server), label (optional, for naming the benchmark run), \n    and config argument for where is the config file. \"\"\"\n\n    logging.debug('Getting cli arguments')\n\n    parser = argparse.ArgumentParser(description=\"A benchmark for genomics routines in Python.\")\n\n    # Enable three exclusive groups of options (using subparsers)\n    # https://stackoverflow.com/questions/17909294/python-argparse-mutual-exclusive-group/17909525\n\n    subparser = parser.add_subparsers(title=\"commands\", dest=\"command\")\n    subparser.required = True\n\n    config_parser = subparser.add_parser(\"config\",\n                                         help='Setting up the default configuration of the benchmark. It creates the default configuration file.')\n    config_parser.add_argument(\"--output_config\", type=str, required=True,\n                               help=\"Specify the output path to a configuration file.\", metavar=\"FILEPATH\")\n    config_parser.add_argument(\"-f\", action=\"store_true\", help=\"Overwrite the destination file if it already exists.\")\n\n    data_setup_parser = subparser.add_parser(\"setup\",\n                                             help='Preparation and setting up of the data for the benchmark. It requires a configuration file.')\n    data_setup_parser.add_argument(\"--config_file\", required=True, help=\"Location of the configuration file\",\n                                   metavar=\"FILEPATH\")\n\n    benchmark_exec_parser = subparser.add_parser(\"exec\",\n                                                 help='Execution of the benchmark modes. It requires a configuration file.')\n    # TODO: use run_(timestamp) as default\n    benchmark_exec_parser.add_argument(\"--label\", type=str, default=\"run\", metavar=\"RUN_LABEL\",\n                                       help=\"Label for the benchmark run.\")\n    benchmark_exec_parser.add_argument(\"--config_file\", type=str, required=True,\n                                       help=\"Specify the path to a configuration file.\", metavar=\"FILEPATH\")\n\n    runtime_configuration = vars(parser.parse_args())\n    return runtime_configuration\n\n\ndef _main():\n    input_directory = \"./data/input/\"\n    download_directory = input_directory + \"download/\"\n    temp_directory = \"./data/temp/\"\n    vcf_directory = \"./data/vcf/\"\n    zarr_directory_setup = \"./data/zarr/\"\n    zarr_directory_benchmark = \"./data/zarr_benchmark/\"\n\n    cli_arguments = get_cli_arguments()\n\n    command = cli_arguments[\"command\"]\n    if command == \"config\":\n        output_config_location = cli_arguments[\"output_config\"]\n        overwrite_mode = cli_arguments[\"f\"]\n        config.generate_default_config_file(output_location=output_config_location,\n                                            overwrite=overwrite_mode)\n    elif command == \"setup\":\n        print(\"[Setup] Setting up benchmark data.\")\n\n        # Clear out existing files in VCF and Zarr directories\n        data_service.remove_directory_tree(vcf_directory)\n        data_service.remove_directory_tree(zarr_directory_setup)\n\n        # Get runtime config from specified location\n        runtime_config = config.read_configuration(location=cli_arguments[\"config_file\"])\n\n        # Get FTP module settings from runtime config\n        ftp_config = config.FTPConfigurationRepresentation(runtime_config)\n\n        if ftp_config.enabled:\n            print(\"[Setup][FTP] FTP module enabled. Running FTP download...\")\n            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=download_directory)\n        else:\n            print(\"[Setup][FTP] FTP module disabled. Skipping FTP download...\")\n\n        # Process/Organize downloaded files\n        data_service.process_data_files(input_dir=input_directory,\n                                        temp_dir=temp_directory,\n                                        output_dir=vcf_directory)\n\n        # Convert VCF files to Zarr format if the module is enabled\n        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)\n        if vcf_to_zarr_config.enabled:\n            data_service.setup_vcf_to_zarr(input_vcf_dir=vcf_directory,\n                                           output_zarr_dir=zarr_directory_setup,\n                                           conversion_config=vcf_to_zarr_config)\n    elif command == \"exec\":\n        print(\"[Exec] Executing benchmark tool.\")\n\n        # Get runtime config from specified location\n        runtime_config = config.read_configuration(location=cli_arguments[\"config_file\"])\n\n        # Get VCF to Zarr conversion settings from runtime config\n        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)\n\n        # TODO: Convert necessary VCF files to Zarr format\n        # data_service.convert_to_zarr(\"./data/vcf/chr22.1000.vcf\", \"./data/zarr/chr22.1000.zarr\", vcf_to_zarr_config)\n    else:\n        print(\"Error: Unexpected command specified. Exiting...\")\n        sys.exit(1)\n\n\ndef main():\n    try:\n        _main()\n    except KeyboardInterrupt:\n        print(\"Program interrupted. Exiting...\")\n        sys.exit(1)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/ornl-oxford/genomics-benchmarks/blob/d3e3e490f4c38e4af73d7c78bf697cf906e8a1b6",
        "file_path": "/benchmark/core.py",
        "source": "\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, \ndetermines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,\nruns the benchmarks, and records the timer results. \"\"\"\n\nimport time  # for benchmark timer\nimport csv  # for writing results\nimport logging\n\n\ndef run_benchmark(bench_conf):\n    pass\n\n\ndef run_dynamic(ftp_location):\n    pass\n\n\ndef run_static():\n    pass\n\n\ndef get_remote_files(ftp_server, ftp_directory, files=None):\n    pass\n\n\ndef record_runtime(benchmark, timestamp):\n    pass\n\n\n# temporary here\ndef main():\n    pass\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/ornl-oxford/genomics-benchmarks/blob/d3e3e490f4c38e4af73d7c78bf697cf906e8a1b6",
        "file_path": "/benchmark/data_service.py",
        "source": "\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, \ndetermines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,\nruns the benchmarks, and records the timer results. \"\"\"\n\nimport urllib.request\nfrom ftplib import FTP, FTP_TLS, error_perm\nimport time  # for benchmark timer\nimport csv  # for writing results\nimport logging\nimport os.path\nimport pathlib\nimport allel\nimport sys\nimport functools\nimport numpy as np\nimport zarr\nimport numcodecs\nfrom numcodecs import Blosc, LZ4, LZMA\nfrom benchmark import config\n\nimport gzip\nimport shutil\n\n\ndef create_directory_tree(path):\n    \"\"\"\n    Creates directories for the path specified.\n    :param path: The path to create dirs/subdirs for\n    :type path: str\n    \"\"\"\n    path = str(path)  # Ensure path is in str format\n    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n\n\ndef remove_directory_tree(path):\n    \"\"\"\n    Removes the directory and all subdirectories/files within the path specified.\n    :param path: The path to the directory to remove\n    :type path: str\n    \"\"\"\n\n    if os.path.exists(path):\n        shutil.rmtree(path, ignore_errors=True)\n\n\ndef fetch_data_via_ftp(ftp_config, local_directory):\n    \"\"\" Get benchmarking data from a remote ftp server. \n    :type ftp_config: config.FTPConfigurationRepresentation\n    :type local_directory: str\n    \"\"\"\n    if ftp_config.enabled:\n        # Create local directory tree if it does not exist\n        create_directory_tree(local_directory)\n\n        # Login to FTP server\n        if ftp_config.use_tls:\n            ftp = FTP_TLS(ftp_config.server)\n            ftp.login(ftp_config.username, ftp_config.password)\n            ftp.prot_p()  # Request secure data connection for file retrieval\n        else:\n            ftp = FTP(ftp_config.server)\n            ftp.login(ftp_config.username, ftp_config.password)\n\n        if not ftp_config.files:  # Auto-download all files in directory\n            fetch_data_via_ftp_recursive(ftp=ftp,\n                                         local_directory=local_directory,\n                                         remote_directory=ftp_config.directory)\n        else:\n            ftp.cwd(ftp_config.directory)\n\n            file_counter = 1\n            file_list_total = len(ftp_config.files)\n\n            for remote_filename in ftp_config.files:\n                local_filename = remote_filename\n                filepath = os.path.join(local_directory, local_filename)\n                if not os.path.exists(filepath):\n                    with open(filepath, \"wb\") as local_file:\n                        try:\n                            ftp.retrbinary('RETR %s' % remote_filename, local_file.write)\n                            print(\"[Setup][FTP] ({}/{}) File downloaded: {}\".format(file_counter, file_list_total,\n                                                                                    filepath))\n                        except error_perm:\n                            # Error downloading file. Display error message and delete local file\n                            print(\"[Setup][FTP] ({}/{}) Error downloading file. Skipping: {}\".format(file_counter,\n                                                                                                     file_list_total,\n                                                                                                     filepath))\n                            local_file.close()\n                            os.remove(filepath)\n                else:\n                    print(\"[Setup][FTP] ({}/{}) File already exists. Skipping: {}\".format(file_counter, file_list_total,\n                                                                                          filepath))\n                file_counter = file_counter + 1\n        # Close FTP connection\n        ftp.close()\n\n\ndef fetch_data_via_ftp_recursive(ftp, local_directory, remote_directory, remote_subdirs_list=None):\n    \"\"\"\n    Recursive function that automatically downloads all files with a FTP directory, including subdirectories.\n    :type ftp: ftplib.FTP\n    :type local_directory: str\n    :type remote_directory: str\n    :type remote_subdirs_list: list\n    \"\"\"\n\n    if (remote_subdirs_list is not None) and (len(remote_subdirs_list) > 0):\n        remote_path_relative = \"/\".join(remote_subdirs_list)\n        remote_path_absolute = \"/\" + remote_directory + \"/\" + remote_path_relative + \"/\"\n    else:\n        remote_subdirs_list = []\n        remote_path_relative = \"\"\n        remote_path_absolute = \"/\" + remote_directory + \"/\"\n\n    try:\n        local_path = local_directory + \"/\" + remote_path_relative\n        os.mkdir(local_path)\n        print(\"[Setup][FTP] Created local folder: {}\".format(local_path))\n    except OSError:  # Folder already exists at destination. Do nothing.\n        pass\n    except error_perm:  # Invalid Entry\n        print(\"[Setup][FTP] Error: Could not change to: {}\".format(remote_path_absolute))\n\n    ftp.cwd(remote_path_absolute)\n\n    # Get list of remote files/folders in current directory\n    file_list = ftp.nlst()\n\n    file_counter = 1\n    file_list_total = len(file_list)\n\n    for file in file_list:\n        file_path_local = local_directory + \"/\" + remote_path_relative + \"/\" + file\n        if not os.path.isfile(file_path_local):\n            try:\n                # Determine if a file or folder\n                ftp.cwd(remote_path_absolute + file)\n                # Path is for a folder. Run recursive function in new folder\n                print(\"[Setup][FTP] Switching to directory: {}\".format(remote_path_relative + \"/\" + file))\n                new_remote_subdirs_list = remote_subdirs_list.copy()\n                new_remote_subdirs_list.append(file)\n                fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory,\n                                             remote_directory=remote_directory,\n                                             remote_subdirs_list=new_remote_subdirs_list)\n                # Return up one level since we are using recursion\n                ftp.cwd(remote_path_absolute)\n            except error_perm:\n                # file is an actual file. Download if it doesn't already exist on filesystem.\n                temp = ftp.nlst()\n                if not os.path.isfile(file_path_local):\n                    with open(file_path_local, \"wb\") as local_file:\n                        ftp.retrbinary('RETR {}'.format(file), local_file.write)\n                    print(\"[Setup][FTP] ({}/{}) File downloaded: {}\".format(file_counter, file_list_total,\n                                                                            file_path_local))\n        else:\n            print(\"[Setup][FTP] ({}/{}) File already exists. Skipping: {}\".format(file_counter, file_list_total,\n                                                                                  file_path_local))\n        file_counter = file_counter + 1\n\n\ndef fetch_file_from_url(url, local_file):\n    urllib.request.urlretrieve(url, local_file)\n\n\ndef decompress_gzip(local_file_gz, local_file):\n    with open(local_file, 'wb') as file_out, gzip.open(local_file_gz, 'rb') as file_in:\n        shutil.copyfileobj(file_in, file_out)\n\n\ndef process_data_files(input_dir, temp_dir, output_dir):\n    \"\"\"\n    Iterates through all files in input_dir and processes *.vcf.gz files to *.vcf, placed in output_dir.\n    Additionally moves *.vcf files to output_dir\n    Note: This method searches through all subdirectories within input_dir, and files are placed in root of output_dir.\n    :param input_dir: The input directory containing files to process\n    :param temp_dir: The temporary directory for unzipping *.gz files, etc.\n    :param output_dir: The output directory where processed *.vcf files should go\n    :type input_dir: str\n    :type temp_dir: str\n    :type output_dir: str\n    \"\"\"\n\n    # Ensure input, temp, and output directory paths are in str format, not pathlib\n    input_dir = str(input_dir)\n    temp_dir = str(temp_dir)\n    output_dir = str(output_dir)\n\n    # Create input, temp, and output directories if they do not exist\n    create_directory_tree(input_dir)\n    create_directory_tree(temp_dir)\n    create_directory_tree(output_dir)\n\n    # Iterate through all *.gz files in input directory and uncompress them to the temporary directory\n    pathlist_gz = pathlib.Path(input_dir).glob(\"**/*.gz\")\n    for path in pathlist_gz:\n        path_str = str(path)\n        file_output_str = path_leaf(path_str)\n        file_output_str = file_output_str[0:len(file_output_str) - 3]  # Truncate *.gz from input filename\n        path_temp_output = str(pathlib.Path(temp_dir, file_output_str))\n        print(\"[Setup][Data] Decompressing file: {}\".format(path_str))\n        print(\"  - Output: {}\".format(path_temp_output))\n\n        # Decompress the .gz file\n        decompress_gzip(path_str, path_temp_output)\n\n    # Iterate through all files in temporary directory and move *.vcf files to output directory\n    pathlist_vcf_temp = pathlib.Path(temp_dir).glob(\"**/*.vcf\")\n    for path in pathlist_vcf_temp:\n        path_temp_str = str(path)\n        filename_str = path_leaf(path_temp_str)  # Strip filename from path\n        path_vcf_str = str(pathlib.Path(output_dir, filename_str))\n\n        shutil.move(path_temp_str, path_vcf_str)\n\n    # Remove temporary directory\n    remove_directory_tree(temp_dir)\n\n    # Copy any *.vcf files already in input directory to the output directory\n    pathlist_vcf_input = pathlib.Path(input_dir).glob(\"**/*.vcf\")\n    for path in pathlist_vcf_input:\n        path_input_str = str(path)\n        filename_str = path_leaf(path_input_str)  # Strip filename from path\n        path_vcf_str = str(pathlib.Path(output_dir, filename_str))\n\n        shutil.copy(path_input_str, path_vcf_str)\n\n\ndef path_head(path):\n    head, tail = os.path.split(path)\n    return head\n\n\ndef path_leaf(path):\n    head, tail = os.path.split(path)\n    return tail or os.path.basename(head)\n\n\ndef read_file_contents(local_filepath):\n    if os.path.isfile(local_filepath):\n        with open(local_filepath) as f:\n            data = f.read()\n            return data\n    else:\n        return None\n\n\ndef setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config):\n    \"\"\"\n    Converts all VCF files in input directory to Zarr format, placed in output directory,\n    based on conversion configuration parameters\n    :param input_vcf_dir: The input directory where VCF files are located\n    :param output_zarr_dir: The output directory to place Zarr-formatted data\n    :param conversion_config: Configuration data for the conversion\n    :type input_vcf_dir: str\n    :type output_zarr_dir: str\n    :type conversion_config: config.VCFtoZarrConfigurationRepresentation\n    \"\"\"\n    # Ensure input and output directory paths are in str format, not pathlib\n    input_vcf_dir = str(input_vcf_dir)\n    output_zarr_dir = str(output_zarr_dir)\n\n    # Create input and output directories if they do not exist\n    create_directory_tree(input_vcf_dir)\n    create_directory_tree(output_zarr_dir)\n\n    # Iterate through all *.vcf files in input directory and convert to Zarr format\n    pathlist_vcf = pathlib.Path(input_vcf_dir).glob(\"**/*.vcf\")\n    for path in pathlist_vcf:\n        path_str = str(path)\n        file_output_str = path_leaf(path_str)\n        file_output_str = file_output_str[0:len(file_output_str) - 4]  # Truncate *.vcf from input filename\n        path_zarr_output = str(pathlib.Path(output_zarr_dir, file_output_str))\n        print(\"[Setup][Data] Converting VCF file to Zarr format: {}\".format(path_str))\n        print(\"  - Output: {}\".format(path_zarr_output))\n\n        # Convert to Zarr format\n        convert_to_zarr(input_vcf_path=path_str,\n                        output_zarr_path=path_zarr_output,\n                        conversion_config=conversion_config)\n\n\ndef convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config):\n    \"\"\" Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.\n    :param input_vcf_path: The input VCF file location\n    :param output_zarr_path: The desired Zarr output location\n    :param conversion_config: Configuration data for the conversion\n    :type input_vcf_path: str\n    :type output_zarr_path: str\n    :type conversion_config: config.VCFtoZarrConfigurationRepresentation\n    \"\"\"\n    if conversion_config is not None:\n        # Ensure var is string, not pathlib.Path\n        output_zarr_path = str(output_zarr_path)\n\n        # Get alt number\n        if conversion_config.alt_number is None:\n            print(\"[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file.\")\n            # Scan VCF file to find max number of alleles in any variant\n            callset = allel.read_vcf(input_vcf_path, fields=['numalt'], log=sys.stdout)\n            numalt = callset['variants/numalt']\n            alt_number = np.max(numalt)\n        else:\n            print(\"[VCF-Zarr] Using alt number provided in configuration.\")\n            # Use the configuration-provided alt number\n            alt_number = conversion_config.alt_number\n        print(\"[VCF-Zarr] Alt number: {}\".format(alt_number))\n\n        # Get chunk length\n        chunk_length = allel.vcf_read.DEFAULT_CHUNK_LENGTH\n        if conversion_config.chunk_length is not None:\n            chunk_length = conversion_config.chunk_length\n        print(\"[VCF-Zarr] Chunk length: {}\".format(chunk_length))\n\n        # Get chunk width\n        chunk_width = allel.vcf_read.DEFAULT_CHUNK_WIDTH\n        if conversion_config.chunk_width is not None:\n            chunk_width = conversion_config.chunk_width\n        print(\"[VCF-Zarr] Chunk width: {}\".format(chunk_width))\n\n        if conversion_config.compressor == \"Blosc\":\n            compressor = Blosc(cname=conversion_config.blosc_compression_algorithm,\n                               clevel=conversion_config.blosc_compression_level,\n                               shuffle=conversion_config.blosc_shuffle_mode)\n        else:\n            raise ValueError(\"Unexpected compressor type specified.\")\n\n        print(\"[VCF-Zarr] Using {} compressor.\".format(conversion_config.compressor))\n\n        print(\"[VCF-Zarr] Performing VCF to Zarr conversion...\")\n        # Perform the VCF to Zarr conversion\n        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True,\n                          log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)\n        print(\"[VCF-Zarr] Done.\")\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/ornl-oxford/genomics-benchmarks/blob/80398ea68be913a53f7629ec3f0c128434e517ee",
        "file_path": "/genomics_benchmarks/config.py",
        "source": "from configparser import ConfigParser\nfrom shutil import copyfile\nimport os.path\nfrom pkg_resources import resource_string\nfrom numcodecs import Blosc\n\n\ndef config_str_to_bool(input_str):\n    \"\"\"\n    :param input_str: The input string to convert to bool value\n    :type input_str: str\n    :return: bool\n    \"\"\"\n    return input_str.lower() in ['true', '1', 't', 'y', 'yes']\n\n\nclass DataDirectoriesConfigurationRepresentation:\n    input_dir = \"./data/input/\"\n    download_dir = input_dir + \"download/\"\n    temp_dir = \"./data/temp/\"\n    vcf_dir = \"./data/vcf/\"\n    zarr_dir_setup = \"./data/zarr/\"\n    zarr_dir_benchmark = \"./data/zarr_benchmark/\"\n\n\ndef isint(value):\n    try:\n        int(value)\n        return True\n    except ValueError:\n        return False\n\n\ndef isfloat(value):\n    try:\n        float(value)\n        return True\n    except ValueError:\n        return False\n\n\nclass ConfigurationRepresentation(object):\n    \"\"\" A small utility class for object representation of a standard config. file. \"\"\"\n\n    def __init__(self, file_name):\n        \"\"\" Initializes the configuration representation with a supplied file. \"\"\"\n        parser = ConfigParser()\n        parser.optionxform = str  # make option names case sensitive\n        found = parser.read(file_name)\n        if not found:\n            raise ValueError(\"Configuration file {0} not found\".format(file_name))\n        for name in parser.sections():\n            dict_section = {name: dict(parser.items(name))}  # create dictionary representation for section\n            self.__dict__.update(dict_section)  # add section dictionary to root dictionary\n\n\nclass FTPConfigurationRepresentation(object):\n    \"\"\" Utility class for object representation of FTP module configuration. \"\"\"\n    enabled = False  # Specifies whether the FTP module should be enabled or not\n    server = \"\"  # FTP server to connect to\n    username = \"\"  # Username to login with. Set username and password to blank for anonymous login\n    password = \"\"  # Password to login with. Set username and password to blank for anonymous login\n    use_tls = False  # Whether the connection should use TLS encryption\n    directory = \"\"  # Directory on FTP server to download files from\n    files = []  # List of files within directory to download. Set to empty list to download all files within directory\n\n    def __init__(self, runtime_config=None):\n        \"\"\"\n        Creates an object representation of FTP module configuration data.\n        :param runtime_config: runtime_config data to extract FTP settings from\n        :type runtime_config: ConfigurationRepresentation\n        \"\"\"\n        if runtime_config is not None:\n            # Check if [ftp] section exists in config\n            if hasattr(runtime_config, \"ftp\"):\n                # Extract relevant settings from config file\n                if \"enabled\" in runtime_config.ftp:\n                    self.enabled = config_str_to_bool(runtime_config.ftp[\"enabled\"])\n                if \"server\" in runtime_config.ftp:\n                    self.server = runtime_config.ftp[\"server\"]\n                if \"username\" in runtime_config.ftp:\n                    self.username = runtime_config.ftp[\"username\"]\n                if \"password\" in runtime_config.ftp:\n                    self.password = runtime_config.ftp[\"password\"]\n                if \"use_tls\" in runtime_config.ftp:\n                    self.use_tls = config_str_to_bool(runtime_config.ftp[\"use_tls\"])\n                if \"directory\" in runtime_config.ftp:\n                    self.directory = runtime_config.ftp[\"directory\"]\n\n                # Convert delimited list of files (string) to Python-style list\n                if \"file_delimiter\" in runtime_config.ftp:\n                    delimiter = runtime_config.ftp[\"file_delimiter\"]\n                else:\n                    delimiter = \"|\"\n\n                if \"files\" in runtime_config.ftp:\n                    files_str = str(runtime_config.ftp[\"files\"])\n                    if files_str == \"*\":\n                        self.files = []\n                    else:\n                        self.files = files_str.split(delimiter)\n\n\nvcf_to_zarr_compressor_types = [\"Blosc\"]\nvcf_to_zarr_blosc_algorithm_types = [\"zstd\", \"blosclz\", \"lz4\", \"lz4hc\", \"zlib\", \"snappy\"]\nvcf_to_zarr_blosc_shuffle_types = [Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE]\n\n\nclass VCFtoZarrConfigurationRepresentation:\n    \"\"\" Utility class for object representation of VCF to Zarr conversion module configuration. \"\"\"\n    enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not\n    fields = None\n    alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined\n    chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value\n    chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value\n    compressor = \"Blosc\"  # Specifies compressor type to use for Zarr conversion\n    blosc_compression_algorithm = \"zstd\"\n    blosc_compression_level = 1  # Level of compression to use for Zarr conversion\n    blosc_shuffle_mode = Blosc.AUTOSHUFFLE\n\n    def __init__(self, runtime_config=None):\n        \"\"\"\n        Creates an object representation of VCF to Zarr Conversion module configuration data.\n        :param runtime_config: runtime_config data to extract conversion configuration from\n        :type runtime_config: ConfigurationRepresentation\n        \"\"\"\n        if runtime_config is not None:\n            # Check if [vcf_to_zarr] section exists in config\n            if hasattr(runtime_config, \"vcf_to_zarr\"):\n                # Extract relevant settings from config file\n                if \"enabled\" in runtime_config.vcf_to_zarr:\n                    self.enabled = config_str_to_bool(runtime_config.vcf_to_zarr[\"enabled\"])\n                if \"alt_number\" in runtime_config.vcf_to_zarr:\n                    alt_number_str = runtime_config.vcf_to_zarr[\"alt_number\"]\n\n                    if str(alt_number_str).lower() == \"auto\":\n                        self.alt_number = None\n                    elif isint(alt_number_str):\n                        self.alt_number = int(alt_number_str)\n                    else:\n                        raise TypeError(\"Invalid value provided for alt_number in configuration.\\n\"\n                                        \"Expected: \\\"auto\\\" or integer value\")\n                if \"chunk_length\" in runtime_config.vcf_to_zarr:\n                    chunk_length_str = runtime_config.vcf_to_zarr[\"chunk_length\"]\n                    if chunk_length_str == \"default\":\n                        self.chunk_length = None\n                    elif isint(chunk_length_str):\n                        self.chunk_length = int(chunk_length_str)\n                    else:\n                        raise TypeError(\"Invalid value provided for chunk_length in configuration.\\n\"\n                                        \"Expected: \\\"default\\\" or integer value\")\n                if \"chunk_width\" in runtime_config.vcf_to_zarr:\n                    chunk_width_str = runtime_config.vcf_to_zarr[\"chunk_width\"]\n                    if chunk_width_str == \"default\":\n                        self.chunk_width = None\n                    elif isint(chunk_width_str):\n                        self.chunk_width = int(chunk_width_str)\n                    else:\n                        raise TypeError(\"Invalid value provided for chunk_width in configuration.\\n\"\n                                        \"Expected: \\\"default\\\" or integer value\")\n                if \"compressor\" in runtime_config.vcf_to_zarr:\n                    compressor_temp = runtime_config.vcf_to_zarr[\"compressor\"]\n                    # Ensure compressor type specified is valid\n                    if compressor_temp in vcf_to_zarr_compressor_types:\n                        self.compressor = compressor_temp\n                if \"blosc_compression_algorithm\" in runtime_config.vcf_to_zarr:\n                    blosc_compression_algorithm_temp = runtime_config.vcf_to_zarr[\"blosc_compression_algorithm\"]\n                    if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types:\n                        self.blosc_compression_algorithm = blosc_compression_algorithm_temp\n                if \"blosc_compression_level\" in runtime_config.vcf_to_zarr:\n                    blosc_compression_level_str = runtime_config.vcf_to_zarr[\"blosc_compression_level\"]\n                    if isint(blosc_compression_level_str):\n                        compression_level_int = int(blosc_compression_level_str)\n                        if (compression_level_int >= 0) and (compression_level_int <= 9):\n                            self.blosc_compression_level = compression_level_int\n                        else:\n                            raise ValueError(\"Invalid value for blosc_compression_level in configuration.\\n\"\n                                             \"blosc_compression_level must be between 0 and 9.\")\n                    else:\n                        raise TypeError(\"Invalid value for blosc_compression_level in configuration.\\n\"\n                                        \"blosc_compression_level could not be converted to integer.\")\n                if \"blosc_shuffle_mode\" in runtime_config.vcf_to_zarr:\n                    blosc_shuffle_mode_str = runtime_config.vcf_to_zarr[\"blosc_shuffle_mode\"]\n                    if isint(blosc_shuffle_mode_str):\n                        blosc_shuffle_mode_int = int(blosc_shuffle_mode_str)\n                        if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types:\n                            self.blosc_shuffle_mode = blosc_shuffle_mode_int\n                        else:\n                            raise ValueError(\"Invalid value for blosc_shuffle_mode in configuration.\\n\"\n                                             \"blosc_shuffle_mode must be a valid integer.\")\n                    else:\n                        raise TypeError(\"Invalid value for blosc_shuffle_mode in configuration.\\n\"\n                                        \"blosc_shuffle_mode could not be converted to integer.\")\n\n\nbenchmark_data_input_types = [\"vcf\", \"zarr\"]\n\n\nclass BenchmarkConfigurationRepresentation:\n    \"\"\" Utility class for object representation of the benchmark module's configuration. \"\"\"\n    benchmark_number_runs = 5\n    benchmark_data_input = \"vcf\"\n    benchmark_dataset = \"\"\n    benchmark_allele_count = False\n    benchmark_PCA = False\n    vcf_to_zarr_config = None\n\n    def __init__(self, runtime_config=None):\n        \"\"\"\n        Creates an object representation of the Benchmark module's configuration data.\n        :param runtime_config: runtime_config data to extract benchmark configuration from\n        :type runtime_config: ConfigurationRepresentation\n        \"\"\"\n        if runtime_config is not None:\n            if hasattr(runtime_config, \"benchmark\"):\n                # Extract relevant settings from config file\n                if \"benchmark_number_runs\" in runtime_config.benchmark:\n                    try:\n                        self.benchmark_number_runs = int(runtime_config.benchmark[\"benchmark_number_runs\"])\n                    except ValueError:\n                        pass\n                if \"benchmark_data_input\" in runtime_config.benchmark:\n                    benchmark_data_input_temp = runtime_config.benchmark[\"benchmark_data_input\"]\n                    if benchmark_data_input_temp in benchmark_data_input_types:\n                        self.benchmark_data_input = benchmark_data_input_temp\n                if \"benchmark_dataset\" in runtime_config.benchmark:\n                    self.benchmark_dataset = runtime_config.benchmark[\"benchmark_dataset\"]\n                if \"benchmark_allele_count\" in runtime_config.benchmark:\n                    self.benchmark_allele_count = config_str_to_bool(runtime_config.benchmark[\"benchmark_allele_count\"])\n                if \"benchmark_PCA\" in runtime_config.benchmark:\n                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[\"benchmark_PCA\"])\n\n            # Add the VCF to Zarr Conversion Configuration Data\n            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)\n\n\ndef read_configuration(location):\n    \"\"\"\n    Args: location of the configuration file, existing configuration dictionary\n    Returns: a dictionary of the form\n    <dict>.<section>[<option>] and the corresponding values.\n    \"\"\"\n    config = ConfigurationRepresentation(location)\n    return config\n\n\ndef generate_default_config_file(output_location, overwrite=False):\n    # Get Default Config File Data as Package Resource\n    default_config_file_data = resource_string(__name__, 'config/benchmark.conf.default')\n\n    if overwrite is None:\n        overwrite = False\n\n    if output_location is not None:\n        # Check if a file currently exists at the location\n        if os.path.exists(output_location) and not overwrite:\n            print(\n                \"[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled.\")\n            return\n\n        # Write the default configuration file to specified location\n        with open(output_location, 'wb') as output_file:\n            output_file.write(default_config_file_data)\n\n        # Check whether configuration file now exists and report status\n        if os.path.exists(output_location):\n            print(\"[Config] Configuration file has been generated successfully.\")\n        else:\n            print(\"[Config] Configuration file was not generated.\")\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/ornl-oxford/genomics-benchmarks/blob/18609fa3b8b1e8cca95f1021d60750628abf7433",
        "file_path": "/genomics_benchmarks/config.py",
        "source": "from configparser import ConfigParser\nfrom shutil import copyfile\nimport os.path\nfrom pkg_resources import resource_string\nfrom numcodecs import Blosc\n\n\ndef config_str_to_bool(input_str):\n    \"\"\"\n    :param input_str: The input string to convert to bool value\n    :type input_str: str\n    :return: bool\n    \"\"\"\n    return input_str.lower() in ['true', '1', 't', 'y', 'yes']\n\n\nclass DataDirectoriesConfigurationRepresentation:\n    input_dir = \"./data/input/\"\n    download_dir = input_dir + \"download/\"\n    temp_dir = \"./data/temp/\"\n    vcf_dir = \"./data/vcf/\"\n    zarr_dir_setup = \"./data/zarr/\"\n    zarr_dir_benchmark = \"./data/zarr_benchmark/\"\n\n\ndef isint(value):\n    try:\n        int(value)\n        return True\n    except ValueError:\n        return False\n\n\ndef isfloat(value):\n    try:\n        float(value)\n        return True\n    except ValueError:\n        return False\n\n\nclass ConfigurationRepresentation(object):\n    \"\"\" A small utility class for object representation of a standard config. file. \"\"\"\n\n    def __init__(self, file_name):\n        \"\"\" Initializes the configuration representation with a supplied file. \"\"\"\n        parser = ConfigParser()\n        parser.optionxform = str  # make option names case sensitive\n        found = parser.read(file_name)\n        if not found:\n            raise ValueError(\"Configuration file {0} not found\".format(file_name))\n        for name in parser.sections():\n            dict_section = {name: dict(parser.items(name))}  # create dictionary representation for section\n            self.__dict__.update(dict_section)  # add section dictionary to root dictionary\n\n\nclass FTPConfigurationRepresentation(object):\n    \"\"\" Utility class for object representation of FTP module configuration. \"\"\"\n    enabled = False  # Specifies whether the FTP module should be enabled or not\n    server = \"\"  # FTP server to connect to\n    username = \"\"  # Username to login with. Set username and password to blank for anonymous login\n    password = \"\"  # Password to login with. Set username and password to blank for anonymous login\n    use_tls = False  # Whether the connection should use TLS encryption\n    directory = \"\"  # Directory on FTP server to download files from\n    files = []  # List of files within directory to download. Set to empty list to download all files within directory\n\n    def __init__(self, runtime_config=None):\n        \"\"\"\n        Creates an object representation of FTP module configuration data.\n        :param runtime_config: runtime_config data to extract FTP settings from\n        :type runtime_config: ConfigurationRepresentation\n        \"\"\"\n        if runtime_config is not None:\n            # Check if [ftp] section exists in config\n            if hasattr(runtime_config, \"ftp\"):\n                # Extract relevant settings from config file\n                if \"enabled\" in runtime_config.ftp:\n                    self.enabled = config_str_to_bool(runtime_config.ftp[\"enabled\"])\n                if \"server\" in runtime_config.ftp:\n                    self.server = runtime_config.ftp[\"server\"]\n                if \"username\" in runtime_config.ftp:\n                    self.username = runtime_config.ftp[\"username\"]\n                if \"password\" in runtime_config.ftp:\n                    self.password = runtime_config.ftp[\"password\"]\n                if \"use_tls\" in runtime_config.ftp:\n                    self.use_tls = config_str_to_bool(runtime_config.ftp[\"use_tls\"])\n                if \"directory\" in runtime_config.ftp:\n                    self.directory = runtime_config.ftp[\"directory\"]\n\n                # Convert delimited list of files (string) to Python-style list\n                if \"file_delimiter\" in runtime_config.ftp:\n                    delimiter = runtime_config.ftp[\"file_delimiter\"]\n                else:\n                    delimiter = \"|\"\n\n                if \"files\" in runtime_config.ftp:\n                    files_str = str(runtime_config.ftp[\"files\"])\n                    if files_str == \"*\":\n                        self.files = []\n                    else:\n                        self.files = files_str.split(delimiter)\n\n\nvcf_to_zarr_compressor_types = [\"Blosc\"]\nvcf_to_zarr_blosc_algorithm_types = [\"zstd\", \"blosclz\", \"lz4\", \"lz4hc\", \"zlib\", \"snappy\"]\nvcf_to_zarr_blosc_shuffle_types = [Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE]\n\n\nclass VCFtoZarrConfigurationRepresentation:\n    \"\"\" Utility class for object representation of VCF to Zarr conversion module configuration. \"\"\"\n    enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not\n    fields = None\n    alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined\n    chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value\n    chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value\n    compressor = \"Blosc\"  # Specifies compressor type to use for Zarr conversion\n    blosc_compression_algorithm = \"zstd\"\n    blosc_compression_level = 1  # Level of compression to use for Zarr conversion\n    blosc_shuffle_mode = Blosc.AUTOSHUFFLE\n\n    def __init__(self, runtime_config=None):\n        \"\"\"\n        Creates an object representation of VCF to Zarr Conversion module configuration data.\n        :param runtime_config: runtime_config data to extract conversion configuration from\n        :type runtime_config: ConfigurationRepresentation\n        \"\"\"\n        if runtime_config is not None:\n            # Check if [vcf_to_zarr] section exists in config\n            if hasattr(runtime_config, \"vcf_to_zarr\"):\n                # Extract relevant settings from config file\n                if \"enabled\" in runtime_config.vcf_to_zarr:\n                    self.enabled = config_str_to_bool(runtime_config.vcf_to_zarr[\"enabled\"])\n                if \"alt_number\" in runtime_config.vcf_to_zarr:\n                    alt_number_str = runtime_config.vcf_to_zarr[\"alt_number\"]\n\n                    if str(alt_number_str).lower() == \"auto\":\n                        self.alt_number = None\n                    elif isint(alt_number_str):\n                        self.alt_number = int(alt_number_str)\n                    else:\n                        raise TypeError(\"Invalid value provided for alt_number in configuration.\\n\"\n                                        \"Expected: \\\"auto\\\" or integer value\")\n                if \"chunk_length\" in runtime_config.vcf_to_zarr:\n                    chunk_length_str = runtime_config.vcf_to_zarr[\"chunk_length\"]\n                    if chunk_length_str == \"default\":\n                        self.chunk_length = None\n                    elif isint(chunk_length_str):\n                        self.chunk_length = int(chunk_length_str)\n                    else:\n                        raise TypeError(\"Invalid value provided for chunk_length in configuration.\\n\"\n                                        \"Expected: \\\"default\\\" or integer value\")\n                if \"chunk_width\" in runtime_config.vcf_to_zarr:\n                    chunk_width_str = runtime_config.vcf_to_zarr[\"chunk_width\"]\n                    if chunk_width_str == \"default\":\n                        self.chunk_width = None\n                    elif isint(chunk_width_str):\n                        self.chunk_width = int(chunk_width_str)\n                    else:\n                        raise TypeError(\"Invalid value provided for chunk_width in configuration.\\n\"\n                                        \"Expected: \\\"default\\\" or integer value\")\n                if \"compressor\" in runtime_config.vcf_to_zarr:\n                    compressor_temp = runtime_config.vcf_to_zarr[\"compressor\"]\n                    # Ensure compressor type specified is valid\n                    if compressor_temp in vcf_to_zarr_compressor_types:\n                        self.compressor = compressor_temp\n                if \"blosc_compression_algorithm\" in runtime_config.vcf_to_zarr:\n                    blosc_compression_algorithm_temp = runtime_config.vcf_to_zarr[\"blosc_compression_algorithm\"]\n                    if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types:\n                        self.blosc_compression_algorithm = blosc_compression_algorithm_temp\n                if \"blosc_compression_level\" in runtime_config.vcf_to_zarr:\n                    blosc_compression_level_str = runtime_config.vcf_to_zarr[\"blosc_compression_level\"]\n                    if isint(blosc_compression_level_str):\n                        compression_level_int = int(blosc_compression_level_str)\n                        if (compression_level_int >= 0) and (compression_level_int <= 9):\n                            self.blosc_compression_level = compression_level_int\n                        else:\n                            raise ValueError(\"Invalid value for blosc_compression_level in configuration.\\n\"\n                                             \"blosc_compression_level must be between 0 and 9.\")\n                    else:\n                        raise TypeError(\"Invalid value for blosc_compression_level in configuration.\\n\"\n                                        \"blosc_compression_level could not be converted to integer.\")\n                if \"blosc_shuffle_mode\" in runtime_config.vcf_to_zarr:\n                    blosc_shuffle_mode_str = runtime_config.vcf_to_zarr[\"blosc_shuffle_mode\"]\n                    if isint(blosc_shuffle_mode_str):\n                        blosc_shuffle_mode_int = int(blosc_shuffle_mode_str)\n                        if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types:\n                            self.blosc_shuffle_mode = blosc_shuffle_mode_int\n                        else:\n                            raise ValueError(\"Invalid value for blosc_shuffle_mode in configuration.\\n\"\n                                             \"blosc_shuffle_mode must be a valid integer.\")\n                    else:\n                        raise TypeError(\"Invalid value for blosc_shuffle_mode in configuration.\\n\"\n                                        \"blosc_shuffle_mode could not be converted to integer.\")\n\n\nbenchmark_data_input_types = [\"vcf\", \"zarr\"]\n\n\nclass BenchmarkConfigurationRepresentation:\n    \"\"\" Utility class for object representation of the benchmark module's configuration. \"\"\"\n    benchmark_number_runs = 5\n    benchmark_data_input = \"vcf\"\n    benchmark_dataset = \"\"\n    benchmark_aggregations = False\n    benchmark_PCA = False\n    vcf_to_zarr_config = None\n\n    def __init__(self, runtime_config=None):\n        \"\"\"\n        Creates an object representation of the Benchmark module's configuration data.\n        :param runtime_config: runtime_config data to extract benchmark configuration from\n        :type runtime_config: ConfigurationRepresentation\n        \"\"\"\n        if runtime_config is not None:\n            if hasattr(runtime_config, \"benchmark\"):\n                # Extract relevant settings from config file\n                if \"benchmark_number_runs\" in runtime_config.benchmark:\n                    try:\n                        self.benchmark_number_runs = int(runtime_config.benchmark[\"benchmark_number_runs\"])\n                    except ValueError:\n                        pass\n                if \"benchmark_data_input\" in runtime_config.benchmark:\n                    benchmark_data_input_temp = runtime_config.benchmark[\"benchmark_data_input\"]\n                    if benchmark_data_input_temp in benchmark_data_input_types:\n                        self.benchmark_data_input = benchmark_data_input_temp\n                if \"benchmark_dataset\" in runtime_config.benchmark:\n                    self.benchmark_dataset = runtime_config.benchmark[\"benchmark_dataset\"]\n                if \"benchmark_aggregations\" in runtime_config.benchmark:\n                    self.benchmark_aggregations = config_str_to_bool(runtime_config.benchmark[\"benchmark_aggregations\"])\n                if \"benchmark_PCA\" in runtime_config.benchmark:\n                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[\"benchmark_PCA\"])\n\n            # Add the VCF to Zarr Conversion Configuration Data\n            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)\n\n\ndef read_configuration(location):\n    \"\"\"\n    Args: location of the configuration file, existing configuration dictionary\n    Returns: a dictionary of the form\n    <dict>.<section>[<option>] and the corresponding values.\n    \"\"\"\n    config = ConfigurationRepresentation(location)\n    return config\n\n\ndef generate_default_config_file(output_location, overwrite=False):\n    # Get Default Config File Data as Package Resource\n    default_config_file_data = resource_string(__name__, 'config/benchmark.conf.default')\n\n    if overwrite is None:\n        overwrite = False\n\n    if output_location is not None:\n        # Check if a file currently exists at the location\n        if os.path.exists(output_location) and not overwrite:\n            print(\n                \"[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled.\")\n            return\n\n        # Write the default configuration file to specified location\n        with open(output_location, 'wb') as output_file:\n            output_file.write(default_config_file_data)\n\n        # Check whether configuration file now exists and report status\n        if os.path.exists(output_location):\n            print(\"[Config] Configuration file has been generated successfully.\")\n        else:\n            print(\"[Config] Configuration file was not generated.\")\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/waipu/bakawipe/blob/bf406be158ab778917f85ed94a669c2e63380722",
        "file_path": "/evproxy.py",
        "source": "# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nimport wzrpc\nfrom sup.ticker import Ticker\n\nclass EvaluatorProxy:\n    def __init__(self, ev_init, *args, **kvargs):\n        super().__init__()\n        self.ev_init = ev_init\n        self.bind_kt_ticker = Ticker()\n        self.bind_kt = 5\n\n    def handle_evaluate(self, reqid, interface, method, data):\n        domain, page = data\n        self.p.log.info('Recvd page %s, working on', reqid)\n        res = self.ev.solve_capage(domain, page)\n        self.p.log.info('Done, sending answer: %s', res)\n        self.p.send_success_rep(reqid, [v.encode('utf-8') for v in res])\n\n    def send_keepalive(self):\n        msg = self.p.wz.make_req_msg(b'Router', b'bind-keepalive', [],\n            self.handle_keepalive_reply)\n        msg.insert(0, b'')\n        self.p.wz_sock.send_multipart(msg)\n\n    def handle_keepalive_reply(self, reqid, seqnum, status, data):\n        if status == wzrpc.status.success:\n            self.p.log.debug('Keepalive was successfull')\n        elif status == wzrpc.status.e_req_denied:\n            self.p.log.warn('Keepalive status {0}, reauthentificating and rebinding'.\n                format(wzrpc.name_status(status)))\n            self.p.auth_requests()\n            self.p.bind_methods()\n        elif status == wzrpc.status.e_timeout:\n            self.p.log.warn('Keepalive timeout')\n        else:\n            self.p.log.warn('Keepalive status {0}'.\n                format(wzrpc.name_status(status)))\n\n    def __call__(self, parent):\n        self.p = parent\n        self.p.wz_connect()\n        self.p.wz_auth_requests = [\n            (b'Router', b'auth-bind-route'),\n            (b'Router', b'auth-unbind-route'),\n            (b'Router', b'auth-set-route-type')]\n        self.p.wz_bind_methods = [\n            (b'Evaluator', b'evaluate', self.handle_evaluate, wzrpc.routetype.random)]\n        self.p.auth_requests()\n        self.p.bind_methods()\n        self.ev = self.ev_init()\n        self.bind_kt_ticker.tick()\n        while self.p.running.is_set():\n            socks = self.p.poll()\n            if self.bind_kt_ticker.elapsed(False) > self.bind_kt:\n                self.bind_kt_ticker.tick()\n                self.send_keepalive()\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/waipu/bakawipe/blob/bf406be158ab778917f85ed94a669c2e63380722",
        "file_path": "/lib/wzrpc/wzhandler.py",
        "source": "# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nfrom . import *\nfrom .wzbase import WZBase\n\nclass WZHandler(WZBase):\n    def __init__(self):\n        self.req_handlers = {}\n        self.response_handlers = {}\n        self.sig_handlers = {}\n        self.iden_reqid_map = BijectiveSetMap()\n\n    def set_req_handler(self, interface, method, fun):\n        self.req_handlers[(interface, method)] = fun\n\n    def set_response_handler(self, reqid, fun):\n        self.response_handlers[reqid] = fun\n\n    def set_sig_handler(self, interface, method, fun):\n        self.sig_handlers[(interface, method)] = fun\n    \n    def del_req_handler(self, interface, method):\n        del self.req_handlers[(interface, method)]\n\n    def del_response_handler(self, reqid):\n        del self.response_handlers[reqid]\n\n    def del_sig_handler(self, interface, method):\n        del self.sig_handlers[(interface, method)]\n\n    def _parse_req(self, iden, msg, reqid, interface, method):\n        try:\n            handler = self.req_handlers[(interface, method)]\n        except KeyError:\n            try:\n                handler = self.req_handlers[(interface, None)]\n            except KeyError:\n                raise WZENoReqHandler(iden, reqid,\n                    'No req handler for %s,%s'%(interface, method))\n        if iden:\n            self.iden_reqid_map.add_value(tuple(iden), reqid)\n        handler(reqid, interface, method, msg[1:])\n        return ()\n\n    def _parse_rep(self, iden, msg, reqid, seqnum, status):\n        try:\n            handler = self.response_handlers[reqid]\n            if seqnum == 0:\n                del self.response_handlers[reqid]\n        except KeyError:\n            raise WZENoHandler(iden, 'No rep handler for reqid')\n        handler(reqid, seqnum, status, msg[1:])\n        return ()\n\n    def _parse_sig(self, iden, msg, interface, method):\n        try:\n            handler = self.sig_handlers[(interface, method)]\n        except KeyError:\n            raise WZENoHandler(iden, 'No handler for sig %s,%s'%(interface, method))\n        handler(interface, method, msg[1:])\n        return ()\n\n    def make_req_msg(self, interface, method, args, fun, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        msg = make_req_msg(interface, method, args, reqid)\n        self.set_response_handler(reqid, fun)\n        return msg\n    \n    def make_router_req_msg(self, iden, interface, method, args, fun, reqid=None):\n        msg = iden[:]\n        msg.append(b'')\n        msg.extend(self.make_req_msg(interface, method, args, fun, reqid))\n        return msg\n    \n    def make_router_rep_msg(self, reqid, seqnum, status, answer):\n        iden = self.iden_reqid_map.get_key(reqid)\n        if seqnum == 0:\n            self.iden_reqid_map.del_value(iden, reqid)\n        msg = list(iden)\n        msg.append(b'')\n        msg.extend(make_rep_msg(reqid, seqnum, status, answer))\n        return msg\n\n    def get_iden(self, reqid):\n        return self.iden_reqid_map.get_key(reqid)\n\n    def get_reqids(self, iden):\n        return self.iden_reqid_map.get_values(iden)\n\n    def make_reqid(self):\n        while True:\n            reqid = random.randint(1, (2**64)-1)\n            if not reqid in self.response_handlers:\n                return reqid\n        \n    def make_auth_req_data(self, interface, method, key, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        args = [interface, method, make_auth_hash(interface, method, reqid, key)]\n        return (b'Router', b'auth-request', args, reqid)\n\n    def make_auth_bind_route_data(self, interface, method, key, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        \n        return (b'Router', b'auth-bind-route', args, reqid)\n\n    def make_auth_unbind_route_data(self, interface, method, key, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        \n        return (b'Router', b'auth-unbind-route', args, reqid)\n\n    def make_auth_set_route_type_data(self, interface, method, type_, key, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        args = [interface, method, struct.pack('!B', type_),\n                make_auth_hash(interface, method, reqid, key)]\n        return (b'Router', b'auth-set-route-type', args, reqid)\n\n    def make_auth_clear_data(self, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        return (b'Router', b'auth-clear', [], reqid)\n\n    def req_from_data(self, d, fun):\n        return self.make_req_msg(d[0], d[1], d[2], fun, d[3])\n  \n    def _parse_err(self, iden, msg, status):\n        pass\n\n    def _handle_nil(self, iden, msg):\n        pass\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/waipu/bakawipe/blob/bf406be158ab778917f85ed94a669c2e63380722",
        "file_path": "/lib/wzworkers.py",
        "source": "import zmq\nimport threading, multiprocessing\nimport logging\nfrom sup.ticker import Ticker\n# from sup import split_frames\nimport wzrpc\nfrom wzrpc.wzhandler import WZHandler\nimport wzauth_data\n\nclass WorkerInterrupt(Exception):\n    '''Exception to raise when self.running is cleared'''\n    def __init__(self):\n        super().__init__('Worker was interrupted at runtime')\n\nclass Suspend(Exception):\n    # if we need this at all.\n    '''Exception to raise on suspend signal'''\n    def __init__(self, interval, *args, **kvargs):\n        self.interval = interval\n        super().__init__(*args, **kvargs)\n\nclass Resume(Exception):\n    '''Exception to raise when suspend sleep is interrupted'''\n\nclass WZWorkerBase:\n    def __init__(self, wz_addr, fun, args=(), kvargs={},\n            name=None, start_timer=None, poll_timeout=None,\n            pargs=(), pkvargs={}):\n        super().__init__(*pargs, **pkvargs)\n        self.name = name if name else type(self).__name__\n        self.start_timer = start_timer\n        self.poll_timeout = poll_timeout if poll_timeout else 5*1000\n        self.call = (fun, args, kvargs)\n\n        self.wz_addr = wz_addr\n        self.wz_auth_requests = []\n        self.wz_bind_methods = []\n        self.wz_poll_timeout = 30\n\n    def __sinit__(self):\n        '''Initializes thread-local interface on startup'''\n        self.log = logging.getLogger(self.name)\n        self.running = threading.Event()\n        self.sleep_ticker = Ticker()\n        self.poller = zmq.Poller()\n\n        s = self.ctx.socket(zmq.SUB)\n        self.poller.register(s, zmq.POLLIN)\n        s.setsockopt(zmq.IPV6, True)\n        s.connect(self.sig_addr)\n        s.setsockopt(zmq.SUBSCRIBE, b'GLOBAL')\n        s.setsockopt(zmq.SUBSCRIBE, b'WZWorker')\n        s.setsockopt(zmq.SUBSCRIBE, bytes(self.name, 'utf-8'))\n        self.sig_sock = s\n\n        s = self.ctx.socket(zmq.DEALER)\n        self.poller.register(s, zmq.POLLIN)\n        s.setsockopt(zmq.IPV6, True)\n        self.wz_sock = s\n\n        self.wz = WZHandler()\n\n        def term_handler(interface, method, data):\n            self.log.info(\n                'Termination signal %s recieved',\n                repr((interface, method, data)))\n            self.term()\n            raise WorkerInterrupt()\n        self.wz.set_sig_handler(b'WZWorker', b'terminate', term_handler)\n\n        def resumehandler(interface, method, data):\n            self.log.info('Resume signal %s recieved',\n                repr((interface, method, data)))\n            raise Resume()\n\n        self.wz.set_sig_handler(b'WZWorker', b'resume', term_handler)\n        self.running.set()\n\n    def wz_connect(self):\n        self.wz_sock.connect(self.wz_addr)\n\n    def wz_wait_reply(self, fun, interface, method, data, reqid=None, timeout=None):\n        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz\n        timeout = timeout if timeout else self.wz_poll_timeout\n        rs = wzrpc.RequestState(fun)\n        msg = self.wz.make_req_msg(interface, method, data,\n                                   rs.accept, reqid)\n        msg.insert(0, b'')\n        s.send_multipart(msg)\n        t.tick()\n        while self.running.is_set():\n            p(timeout*1000)\n            if rs.finished:\n                if rs.retry:\n                    msg = self.wz.make_req_msg(interface, method, data,\n                        rs.accept, reqid)\n                    msg.insert(0, b'')\n                    s.send_multipart(msg)\n                    rs.finished = False\n                    rs.retry = False\n                    continue\n                return\n            elapsed = t.elapsed(False)\n            if elapsed >= timeout:\n                t.tick()\n                # Notify fun about the timeout\n                rs.accept(None, 0, 255, [elapsed])\n                # fun sets rs.retry = True if it wants to retry\n        raise WorkerInterrupt()\n    \n    def wz_multiwait(self, requests):\n        # TODO: rewrite the retry loop\n        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz\n        timeout = self.wz_poll_timeout\n        rslist = []\n        msgdict = {}\n        for request in requests:\n            rs = wzrpc.RequestState(request[0])\n            rslist.append(rs)\n            msg = self.wz.make_req_msg(request[1][0], request[1][1], request[1][2],\n                                    rs.accept, request[1][3])\n            msg.insert(0, b'')\n            msgdict[rs] = msg\n            s.send_multipart(msg)\n        while self.running.is_set():\n            flag = 0\n            for rs in rslist:\n                if rs.finished:\n                    if not rs.retry:\n                        del msgdict[rs]\n                        continue\n                    s.send_multipart(msgdict[rs])\n                    rs.finished = False\n                    rs.retry = False\n                flag = 1\n            if not flag:\n                return\n            # check rs before polling, since we don't want to notify finished one\n            # about the timeout\n            t.tick()\n            p(timeout*1000)\n            if t.elapsed(False) >= timeout:\n                for rs in rslist:\n                    if not rs.finished:\n                        rs.accept(None, 0, 255, []) # Notify fun about the timeout\n                        rs.finished = True # fun sets rs.retry = True if it wants to retry\n        raise WorkerInterrupt()\n\n    def auth_requests(self):\n        for i, m in self.wz_auth_requests:\n            def accept(that, reqid, seqnum, status, data):\n                if status == wzrpc.status.success:\n                    self.log.debug('Successfull auth for (%s, %s)', i, m)\n                elif status == wzrpc.status.e_auth_wrong_hash:\n                    raise beon.PermanentError(\n                        'Cannot authentificate for ({0}, {1}), {2}: {3}'.\\\n                        format(i, m, wzrpc.name_status(status), repr(data)))\n                elif wzrpc.status.e_timeout:\n                    self.log.warn('Timeout {0}, retrying'.format(data[0]))\n                    that.retry = True\n                else:\n                    self.log.warning('Recvd unknown reply for (%s, %s) %s: %s', i, m,\n                        wzrpc.name_status(status), repr(data))\n            self.wz_wait_reply(accept,\n                *self.wz.make_auth_req_data(i, m, wzauth_data.request[i, m]))\n\n\n    def bind_route(self, i, m, f):\n        self.log.debug('Binding %s,%s route', i, m)\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.wz.set_req_handler(i, m, f)\n                self.log.debug('Succesfully binded route (%s, %s)', i, m)\n            elif status == wzrpc.status.e_req_denied:\n                self.log.warn('Status {0}, reauthentificating'.\\\n                    format(wzrpc.name_status(status)))\n                self.auth_requests()\n            elif wzrpc.status.e_timeout:\n                self.log.warn('Timeout {0}, retrying'.format(data[0]))\n                that.retry = True\n            else:\n                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))\n                that.retry = True\n        return self.wz_wait_reply(accept,\n                *self.wz.make_auth_bind_route_data(i, m, wzauth_data.bind_route[i, m]))\n\n    def set_route_type(self, i, m, t):\n        self.log.debug('Setting %s,%s type to %d', i, m, t)\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.log.debug('Succesfully set route type for (%s, %s) to %s', i, m,\n                    wzrpc.name_route_type(t))\n            elif status == wzrpc.status.e_req_denied:\n                self.log.warn('Status {0}, reauthentificating'.\\\n                    format(wzrpc.name_status(status)))\n                self.auth_requests()\n            else:\n                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))\n                that.retry = True\n        return self.wz_wait_reply(accept,\n            *self.wz.make_auth_set_route_type_data(i, m, t,\n                wzauth_data.set_route_type[i, m]))\n\n    def unbind_route(self, i, m):\n        if not (i, m) in self.wz.req_handlers:\n            self.log.debug('Route %s,%s was not bound', i, m)\n            return\n        self.log.debug('Unbinding route %s,%s', i, m)\n        self.wz.del_req_handler(i, m)\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.log.debug('Route unbinded for (%s, %s)', i, m)\n            else:\n                self.log.warn('Status %s, passing', wzrpc.name_status(status))\n        return self.wz_wait_reply(accept,\n            *self.wz.make_auth_unbind_route_data(i, m, wzauth_data.bind_route[i, m]))\n    \n    def clear_auth(self):\n        self.log.debug('Clearing our auth records')\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.log.debug('Auth records on router were cleared')\n            else:\n                self.log.warn('Status %s, passing', wzrpc.name_status(status))\n        return self.wz_wait_reply(accept, *self.wz.make_auth_clear_data())\n\n    def bind_methods(self):\n        for i, m, f, t in self.wz_bind_methods:\n            self.set_route_type(i, m, t)\n            self.bind_route(i, m, f)\n    \n    def unbind_methods(self):  \n        for i, m, f, t in self.wz_bind_methods:\n            self.unbind_route(i, m)\n        #self.clear_auth()\n\n    def send_rep(self, reqid, seqnum, status, data):\n        self.wz_sock.send_multipart(\n            self.wz.make_router_rep_msg(reqid, seqnum, status, data))\n\n    def send_success_rep(self, reqid, data):\n        self.send_rep(reqid, 0, wzrpc.status.success, data)\n    \n    def send_error_rep(self, reqid, data):\n        self.send_rep(reqid, 0, wzrpc.status.error, data)\n\n    def send_wz_error(self, reqid, data, seqid=0):\n        msg = self.wz.make_dealer_rep_msg(\n            reqid, seqid, wzrpc.status.error, data)\n        self.wz_sock.send_multipart(msg)\n        \n    def send_to_router(self, msg):\n        msg.insert(0, b'')\n        self.wz_sock.send_multipart(msg)\n    \n    # def bind_sig_route(self, routetype, interface, method, fun):\n    #     self.log.info('Binding %s,%s as type %d signal route',\n    #                   interface, method, routetype)\n    #     self.wz.set_signal_handler(interface, method, fun)\n    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'bind-sig-route',\n    #                                       [interface, method],\n    #                                       self.accept_ok)\n    #     self.wz_sock.send_multipart(msg)\n\n    # def unbind_sig_route(self, interface, method):\n    #     self.log.info('Deleting %s,%s signal route', interface, method)\n    #     self.wz.del_signal_handler(interface, method)\n    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'unbind-sig-route',\n    #                                       [interface, method],\n    #                                       self.accept_ok)\n    #     self.wz_sock.send_multipart(msg)\n\n    def inter_sleep(self, timeout):\n        self.sleep_ticker.tick()\n        self.poll(timeout * 1000)\n        while self.sleep_ticker.elapsed(False) < timeout:\n            try:\n                self.poll(timeout * 1000)\n            except Resume as e:\n                return\n\n    def poll(self, timeout=None):\n        try:\n            socks = dict(self.poller.poll(timeout if timeout != None\n                else self.poll_timeout))\n        except zmq.ZMQError as e:\n            self.log.error(e)\n            return\n        if socks.get(self.sig_sock) == zmq.POLLIN:\n            # No special handling or same-socket replies are necessary for signals.\n            # Backwards socket replies may be added here.\n            frames = self.sig_sock.recv_multipart()\n            try:\n                self.wz.parse_msg(frames[0], frames[1:])\n            except wzrpc.WZError as e:\n                self.log.warn(e)\n        if socks.get(self.wz_sock) == zmq.POLLIN:\n            self.process_wz_msg(self.wz_sock.recv_multipart())\n        return socks\n\n    def process_wz_msg(self, frames):\n        try:\n            for nfr in self.wz.parse_router_msg(frames):\n                # Send replies from the handler, for cases when it's methods were rewritten.\n                self.wz_sock.send_multipart(nfr)\n        except wzrpc.WZErrorRep as e:\n            self.log.info(e)\n            self.wz_sock.send_multipart(e.rep_msg)\n        except wzrpc.WZError as e:\n            self.log.warn(e)\n\n    def run(self):\n        self.__sinit__()\n        if self.start_timer:\n            self.inter_sleep(self.start_timer)\n        if self.running:\n            self.log.info('Starting')\n            try:\n                self.child = self.call[0](*self.call[1], **self.call[2])\n                self.child(self)\n            except WorkerInterrupt as e:\n                self.log.warn(e)\n            except Exception as e:\n                self.log.exception(e)\n            self.log.info('Terminating')\n        else:\n            self.log.info('Aborted')\n        self.running.set() # wz_multiwait needs this to avoid another state check.\n        self.unbind_methods()\n        self.running.clear()\n        self.wz_sock.close()\n        self.sig_sock.close()\n    \n    def term(self):\n        self.running.clear()\n\n\nclass WZWorkerThread(WZWorkerBase, threading.Thread):\n    def start(self, ctx, sig_addr, *args, **kvargs):\n        self.ctx = ctx\n        self.sig_addr = sig_addr\n        threading.Thread.start(self, *args, **kvargs)\n\nclass WZWorkerProcess(WZWorkerBase, multiprocessing.Process):\n    def start(self, sig_addr, *args, **kvargs):\n        self.sig_addr = sig_addr\n        multiprocessing.Process.start(self, *args, **kvargs)\n    \n    def __sinit__(self):\n        self.ctx = zmq.Context()\n        super().__sinit__()\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/waipu/bakawipe/blob/bf406be158ab778917f85ed94a669c2e63380722",
        "file_path": "/unistart.py",
        "source": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nimport sys\nif 'lib' not in sys.path:\n    sys.path.append('lib')\nimport os, signal, logging, threading, re, traceback, time\nimport random\nimport zmq\nfrom queue import Queue\nimport sup\nimport wzworkers as workers\nfrom dataloader import DataLoader\nfrom uniwipe import UniWipe\nfrom wipeskel import *\nimport wzrpc\nfrom beon import regexp\nimport pickle\n\nfrom logging import config\nfrom logconfig import logging_config\nconfig.dictConfig(logging_config)\nlogger = logging.getLogger()\n\nctx = zmq.Context()\nsig_addr = 'ipc://signals'\nsig_sock = ctx.socket(zmq.PUB)\nsig_sock.bind(sig_addr)\n\n# Settings for you\ndomains = set() # d.witch_domains\ntargets = dict() # d.witch_targets\nprotected = set() # will be removed later\nforums = dict() # target forums\n\n# from lib import textgen\n# with open('data.txt', 'rt') as f:\n#     model = textgen.train(f.read())\n# def mesasge():\n#     while True:\n#         s = textgen.generate_sentence(model)\n#         try:\n#             s.encode('cp1251')\n#             break\n#         except Exception:\n#             continue\n#     return s\n\ndef message():\n    msg = []\n    # msg.append('[video-youtube-'+\n    #            random.choice(('3odl-KoNZwk', 'bu55q_3YtOY', '4YPiCeLwh5o',\n    #                           'eSBybJGZoCU', 'ZtWTUt2RZh0', 'VXa9tXcMhXQ',))\n    #            +']')\n    msg.append('[image-original-none-http://simg4.gelbooru.com/'\n               + '/images/db/1d/db1dfb62a40f5ced2043bb8966da9a98.png]')\n    msg.append('     .')\n    # msg.append('[video-youtube-'+random.choice(\n    #     # ('WdDb_RId-xU', 'EFL1-fL-WtM', 'uAOoiIkFQq4',\n    #     #  'eZO3K_4yceU', '1c1lT_HgJNo', 'WOkvVVaJ2Ks',\n    #     #  'KYq90TEdxIE', 'rWBM2whL0bI', '0PDy_MKYo4A'))\n    #     #('GabBLLOT6vw', 'qgvOpSquCAY', 'zUe-z9DZBNo', '4fCbfDEKZss', 'uIE-JgmkmdM'))\n    #     ('42JQYPioVo4', 'jD6j072Ep1M', 'mPyF5ovoIVs', 'cEEi1BHycb0', 'PuA1Wf8nkxw',\n    #      'ASJ9qlsPgHU', 'DP1ZDW9_xOo', 'bgSqH9LT-mI', ))\n    # +']')\n    # http://simg2.gelbooru.com//images/626/58ca1c9a8ffcdedd0e2eb6f33c9389cb7588f0d1.jpg\n    # msg.append('Enjoy the view!')\n    msg.append(str(random.randint(0, 9999999999)))\n    return '\\n'.join(msg)\n\ndef sbjfun():\n    # return 'Out of the darkness we will rise, into the light we will dwell'\n    return sup.randstr(1, 30)\n\n# End\nimport argparse\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument('--only-cache', '-C', action='store_true',\n    help=\"Disables any requests in DataLoader (includes Witch)\")\nparser.add_argument('--no-shell', '-N', action='store_true',\n    help=\"Sleep instead of starting the shell\")\nparser.add_argument('--tcount', '-t', type=int, default=10,\n    help='WipeThread count')\nparser.add_argument('--ecount', '-e', type=int, default=0,\n    help='EvaluatorProxy count')\nparser.add_argument('--upload-avatar', action='store_true', default=False,\n    help='Upload random avatar after registration')\nparser.add_argument('--av-dir', default='randav', help='Directory with avatars')\nparser.add_argument('--rp-timeout', '-T', type=int, default=10,\n    help='Default rp timeout in seconds')\nparser.add_argument('--conlimit', type=int, default=3,\n    help='http_request conlimit')\nparser.add_argument('--noproxy-timeout', type=int, default=5,\n    help='noproxy_rp timeout')\n\nparser.add_argument('--caprate_minp', type=int, default=5,\n    help='Cap rate minimum possible count for limit check')\nparser.add_argument('--caprate_limit', type=float, default=0.8,\n    help='Captcha rate limit')\n\nparser.add_argument('--comment_successtimeout', type=float, default=0.8,\n    help='Comment success timeout')\nparser.add_argument('--topic_successtimeout', type=float, default=0.1,\n    help='Topic success timeout')\nparser.add_argument('--errortimeout', type=float, default=3,\n    help='Error timeout')\n\n\nparser.add_argument('--stop-on-closed', action='store_true', default=False,\n    help='Forget about closed topics')\nparser.add_argument('--die-on-neterror', action='store_true', default=False,\n    help='Terminate spawn in case of too many NetErrors')\n\nc = parser.parse_args()\n\n# rps = {}\n\nnoproxy_rp = sup.net.RequestPerformer()\nnoproxy_rp.proxy = ''\nnoproxy_rp.timeout = c.noproxy_timeout\nnoproxy_rp.timeout = c.rp_timeout\n\n# rps[''] = noproxy_rp\n\n# Achtung: DataLoader probably isn't thread-safe.\nd = DataLoader(noproxy_rp, c.only_cache)\nc.router_addr = d.addrs['rpcrouter']\nnoproxy_rp.useragent = random.choice(d.ua_list)\n\ndef terminate():\n    logger.info('Shutdown initiated')\n    # send_passthrough([b'GLOBAL', b'WZWorker', b'terminate'])\n    send_to_wm([b'GLOBAL', b'WZWorker', b'terminate'])\n    for t in threading.enumerate():\n        if isinstance(t, threading.Timer):\n            t.cancel()\n    # try:\n    #     wm.term()\n    #     wm.join()\n    # except: # WM instance is not created yet.\n    #     pass\n    logger.info('Exiting')\n\ndef interrupt_handler(signal, frame):\n    pass # Just do nothing\n\ndef terminate_handler(signal, frame):\n    terminate()\n\nsignal.signal(signal.SIGINT, interrupt_handler)\nsignal.signal(signal.SIGTERM, terminate_handler)\n\ndef make_net(proxy, proxytype):\n    # if proxy in rps:\n    #     return rps[proxy]\n    net = sup.net.RequestPerformer()\n    net.proxy = proxy\n    if proxytype == 'HTTP' or proxytype == 'HTTPS':\n        net.proxy_type = sup.proxytype.http\n    elif proxytype == 'SOCKS4':\n        net.proxy_type = sup.proxytype.socks4\n    elif proxytype == 'SOCKS5':\n        net.proxy_type = sup.proxytype.socks5\n    else:\n        raise TypeError('Invalid proxytype %s' % proxytype)\n    # rps[proxy] = net\n    net.useragent = random.choice(d.ua_list)\n    net.timeout = c.rp_timeout\n    return net\n\n# UniWipe patching start\ndef upload_avatar(self, ud):\n    if ('avatar_uploaded' in ud[0] and\n        ud[0]['avatar_uploaded'] is True):\n        return\n    files = []\n    for sd in os.walk(c.av_dir):\n        files.extend(sd[2])\n    av = os.path.join(sd[0], random.choice(files))\n    self.log.info('Uploading %s as new avatar', av)\n    self.site.uploadavatar('0', av)\n    ud[0]['avatar'] = av\n    ud[0]['avatar_uploaded'] = True\n\nfrom lib.mailinator import Mailinator\n# from lib.tempmail import TempMail as Mailinator\n\n# Move this to WipeManager\ndef create_spawn(proxy, proxytype, pc, uq=None):\n    for domain in domains:\n        if domain in targets:\n            tlist = targets[domain]\n        else:\n            tlist = list()\n            targets[domain] = tlist\n        if domain in forums:\n            fset = forums[domain]\n        else:\n            fset = set()\n            forums[domain] = fset\n        net = make_net(proxy, proxytype)\n        net.cookiefname = (proxy if proxy else 'noproxy')+'_'+domain\n        w = UniWipe(fset, tlist, sbjfun, message, pc, net, domain, Mailinator,\n            uq(domain) if uq else None)\n        w.stoponclose = c.stop_on_closed\n        w.die_on_neterror = c.die_on_neterror\n        w.caprate_minp = c.caprate_minp\n        w.caprate_limit = c.caprate_limit\n        w.conlimit = c.conlimit\n        w.comment_successtimeout = 0.2\n        if c.upload_avatar:\n            w.hooks['post_login'].append(upload_avatar)\n        yield w\n\n# UniWipe patching end\n\nclass WipeManager:\n    def __init__(self, config, *args, **kvargs):\n        super().__init__(*args, **kvargs)\n        self.newproxyfile = 'newproxies.txt'\n        self.proxylist = set()\n        self.c = config\n        self.threads = []\n        self.processes = []\n        self.th_sa = 'inproc://wm-wth.sock'\n        self.th_ba = 'inproc://wm-back.sock'\n        self.pr_sa = 'ipc://wm-wpr.sock'\n        self.pr_ba = 'ipc://wm-back.sock'\n        self.userqueues = {}\n        self.usersfile = 'wm_users.pickle'\n        self.targetsfile = 'wm_targets.pickle'\n        self.bumplimitfile = 'wm_bumplimit.pickle'\n\n    def init_th_sock(self):\n        self.log.info(\n            'Initializing intraprocess signal socket %s', self.th_sa)\n        self.th_sock = self.p.ctx.socket(zmq.PUB)\n        self.th_sock.bind(self.th_sa)\n\n    def init_th_back_sock(self):\n        self.log.info(\n            'Initializing intraprocess backward socket %s', self.th_ba)\n        self.th_back_sock = self.p.ctx.socket(zmq.ROUTER)\n        self.th_back_sock.bind(self.th_ba)\n\n    def init_pr_sock(self):\n        self.log.info(\n            'Initializing interprocess signal socket %s', self.pr_sa)\n        self.pr_sock = self.p.ctx.socket(zmq.PUB)\n        self.pr_sock.bind(self.pr_sa)\n\n    def init_pr_back_sock(self):\n        self.log.info(\n            'Initializing interprocess backward socket %s', self.pr_ba)\n        self.pr_back_sock = self.p.ctx.socket(zmq.ROUTER)\n        self.pr_back_sock.bind(self.pr_ba)\n\n    def read_newproxies(self):\n        if not os.path.isfile(self.newproxyfile):\n            return\n        newproxies = set()\n        with open(self.newproxyfile, 'rt') as f:\n            for line in f:\n                try:\n                    line = line.rstrip('\\n')\n                    proxypair = tuple(line.split(' '))\n                    if len(proxypair) < 2:\n                        self.log.warning('Line %s has too few spaces', line)\n                        continue\n                    if len(proxypair) > 2:\n                        self.log.debug('Line %s has too much spaces', line)\n                        proxypair = (proxypair[0], proxypair[1])\n                    newproxies.add(proxypair)\n                except Exception as e:\n                    self.log.exception('Line %s raised exception %s', line, e)\n        # os.unlink(self.newproxyfile)\n        return newproxies.difference(self.proxylist)\n\n    def add_spawns(self, proxypairs):\n        while self.running.is_set():\n            try:\n                try:\n                    proxypair = proxypairs.pop()\n                except Exception:\n                    return\n                self.proxylist.add(proxypair)\n                for spawn in create_spawn(proxypair[0], proxypair[1], self.pc,\n                        self.get_userqueue):\n                    self.log.info('Created spawn %s', spawn.name)\n                    self.spawnqueue.put(spawn, False)\n            except Exception as e:\n                self.log.exception('Exception \"%s\" raised on create_spawn', e)\n\n    def spawn_workers(self, wclass, count, args=(), kvargs={}):\n        wname = str(wclass.__name__)\n        self.log.info('Starting %s(s)', wname)\n        if issubclass(wclass, workers.WZWorkerThread):\n            type_ = 0\n            if not hasattr(self, 'th_sock'):\n                self.init_th_sock()\n            if not hasattr(self, 'th_back_sock'):\n                self.init_th_back_sock()\n        elif issubclass(wclass, workers.WZWorkerProcess):\n            type_ = 1\n            if not hasattr(self, 'pr_sock'):\n                self.init_pr_sock()\n            if not hasattr(self, 'pr_back_sock'):\n                self.init_pr_back_sock()\n        else:\n            raise Exception('Unknown wclass type')\n        for i in range(count):\n            if not self.running.is_set():\n                break\n            try:\n                w = wclass(*args, name='.'.join(\n                    (wname, ('pr{0}' if type_ else 'th{0}').format(i))),\n                    **kvargs)\n                if type_ == 0:\n                    self.threads.append(w)\n                    w.start(self.p.ctx, self.th_sa)\n                elif type_ == 1:\n                    self.processes.append(w)\n                    w.start(self.pr_sa)\n            except Exception as e:\n                self.log.exception('Exception \"%s\" raised on %s spawn',\n                                   e, wname)\n\n    def spawn_nworkers(self, type_, fun, count, args=(), kvargs={}):\n        wname = str(fun.__name__)\n        self.log.info('Starting %s(s)', wname)\n        if type_ == 0:\n            if not hasattr(self, 'th_sock'):\n                self.init_th_sock()\n            if not hasattr(self, 'th_back_sock'):\n                self.init_th_back_sock()\n        elif type_ == 1:\n            if not hasattr(self, 'pr_sock'):\n                self.init_pr_sock()\n            if not hasattr(self, 'pr_back_sock'):\n                self.init_pr_back_sock()\n        else:\n            raise Exception('Unknown wclass type')\n        for i in range(count):\n            if not self.running.is_set():\n                break\n            try:\n                if type_ == 0:\n                    w = workers.WZWorkerThread(\n                        self.c.router_addr, fun, args, kvargs,\n                        name='.'.join((wname, 'th{0}'.format(i))))\n                    self.threads.append(w)\n                    w.start(self.p.ctx, self.th_sa)\n                elif type_ == 1:\n                    w = workers.WZWorkerProcess(self.c.router_addr, fun, args, kvargs,\n                        name='.'.join((wname, 'pr{0}'.format(i))))\n                    self.processes.append(w)\n                    w.start(self.pr_sa)\n            except Exception as e:\n                self.log.exception('Exception \"%s\" raised on %s spawn',\n                                   e, wname)\n\n    def spawn_wipethreads(self):\n        return self.spawn_nworkers(0, WipeThread, self.c.tcount,\n                                  (self.pc, self.spawnqueue))\n\n    def spawn_evaluators(self):\n        self.log.info('Initializing Evaluator')\n        from evproxy import EvaluatorProxy\n        def ev_init():\n            from lib.evaluators.PyQt4Evaluator import Evaluator\n            return Evaluator()\n        return self.spawn_nworkers(1, EvaluatorProxy, self.c.ecount,\n                                  (ev_init,))\n\n    def load_users(self):\n        if not os.path.isfile(self.usersfile):\n            return\n        with open(self.usersfile, 'rb') as f:\n            users = pickle.loads(f.read())\n        try:\n            for domain in users.keys():\n                uq = Queue()\n                for ud in users[domain]:\n                    self.log.debug('Loaded user %s:%s', domain, ud['login'])\n                    uq.put(ud)\n                self.userqueues[domain] = uq\n        except Exception as e:\n            self.log.exception(e)\n            self.log.error('Failed to load users')\n\n    def save_users(self):\n        users = {}\n        for d, uq in self.userqueues.items():\n            uqsize = uq.qsize()\n            uds = []\n            for i in range(uqsize):\n                uds.append(uq.get(False))\n            users[d] = uds\n        with open(self.usersfile, 'wb') as f:\n            f.write(pickle.dumps(users, pickle.HIGHEST_PROTOCOL))\n        self.log.info('Saved users')\n\n    def get_userqueue(self, domain):\n        try:\n            uq = self.userqueues[domain]\n        except KeyError:\n            self.log.info('Created userqueue for %s', domain)\n            uq = Queue()\n            self.userqueues[domain] = uq\n        return uq\n\n    def load_targets(self):\n        fname = self.targetsfile\n        if not os.path.isfile(fname):\n            return\n        with open(fname, 'rb') as f:\n            data = pickle.loads(f.read())\n        if 'targets' in data:\n            self.log.debug('Target list was loaded')\n            targets.update(data['targets'])\n        if 'forums' in data:\n            self.log.debug('Forum set was loaded')\n            forums.update(data['forums'])\n        if 'domains' in data:\n            self.log.debug('Domain set was loaded')\n            domains.update(data['domains'])\n        if 'sets' in data:\n            self.log.debug('Other sets were loaded')\n            self.pc.sets.update(data['sets'])\n\n    def load_bumplimit_set(self):\n        if not os.path.isfile(self.bumplimitfile):\n            return\n        with open(self.bumplimitfile, 'rb') as f:\n            self.pc.sets['bumplimit'].update(pickle.loads(f.read()))\n\n    def save_targets(self):\n        data = {\n            'targets': targets,\n            'forums': forums,\n            'domains': domains,\n            'sets': self.pc.sets,\n            }\n        with open(self.targetsfile, 'wb') as f:\n            f.write(pickle.dumps(data, pickle.HIGHEST_PROTOCOL))\n\n    def targets_from_witch(self):\n        for t in d.witch_targets:\n            if t['domain'] == 'beon.ru' and t['forum'] == 'anonymous':\n                try:\n                    add_target_exc(t['id'], t['user'])\n                except ValueError:\n                    pass\n\n    def terminate(self):\n        msg = [b'GLOBAL']\n        msg.extend(wzrpc.make_sig_msg(b'WZWorker', b'terminate', []))\n        if hasattr(self, 'th_sock'):\n            self.th_sock.send_multipart(msg)\n        if hasattr(self, 'pr_sock'):\n            self.pr_sock.send_multipart(msg)\n\n    def join_threads(self):\n        for t in self.threads:\n            t.join()\n\n    def send_passthrough(self, interface, method, frames):\n        msg = [frames[0]]\n        msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))\n        self.th_sock.send_multipart(msg)\n        self.pr_sock.send_multipart(msg)\n\n    def __call__(self, parent):\n        self.p = parent\n        self.log = parent.log\n        self.inter_sleep = parent.inter_sleep\n        self.running = parent.running\n        self.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeManager')\n        self.p.wz.set_sig_handler(b'WipeManager', b'passthrough', self.send_passthrough)\n        if self.c.tcount > 0:\n            self.pc = ProcessContext(self.p.name, self.p.ctx,\n                self.c.router_addr, noproxy_rp)\n            self.spawnqueue = Queue()\n            self.load_bumplimit_set()\n            self.load_targets()\n            self.load_users()\n            self.spawn_wipethreads()\n        if self.c.ecount > 0:\n            self.spawn_evaluators()\n        try:\n            while self.running.is_set():\n                # self.targets_from_witch()\n                if self.c.tcount == 0:\n                    self.inter_sleep(5)\n                    continue\n                self.pc.check_waiting()\n                new = self.read_newproxies()\n                if not new:\n                    self.inter_sleep(5)\n                    continue\n                self.add_spawns(new)\n        except WorkerInterrupt:\n            pass\n        except Exception as e:\n            self.log.exception(e)\n        self.terminate()\n        self.join_threads()\n        if self.c.tcount > 0:\n            self.save_users()\n            self.save_targets()\n\nwm = workers.WZWorkerThread(c.router_addr, WipeManager, (c,),\n    name='SpaghettiMonster')\nwm.start(ctx, sig_addr)\n\ndef add_target(domain, id_, tuser=None):\n    if domain not in targets:\n        targets[domain] = []\n    tlist = targets[domain]\n    id_ = str(id_)\n    tuser = tuser or ''\n    t = (tuser, id_)\n    logger.info('Appending %s to targets[%s]', repr(t), domain)\n    tlist.append(t)\n\ndef remove_target(domain, id_, tuser=None):\n    tlist = targets[domain]\n    id_ = str(id_)\n    tuser = tuser or ''\n    t = (tuser, id_)\n    logger.info('Removing %s from targets[%s]', repr(t), domain)\n    tlist.remove(t)\n\ndef add_target_exc(domain, id_, tuser=None):\n    if domain not in targets:\n        targets[domain] = []\n    tlist = targets[domain]\n    id_ = str(id_)\n    tuser = tuser or ''\n    t = (tuser, id_)\n    if t in protected:\n        raise ValueError('%s is protected' % repr(t))\n    if t not in tlist:\n        logger.info('Appending %s to targets[%s]', repr(t), domain)\n        tlist.append(t)\n\nr_di = re.compile(regexp.f_udi)\n\ndef atfu(urls):\n    for user, domain, id1, id2 in r_di.findall(urls):\n        id_ = id1+id2\n        add_target(domain, id_, user)\n\ndef rtfu(urls):\n    for user, domain, id1, id2 in r_di.findall(urls):\n        id_ = id1+id2\n        remove_target(domain, id_, user)\n\ndef get_forum_id(name):\n    id_ = d.bm_id_forum.get_key(name)\n    int(id_, 10)  # id is int with base 10\n    return id_\n\n# def aftw(name):\n#     id_ = get_forum_id(name)\n#     logger.info('Appending %s (%s) to forums', name, id_)\n#     forums.append(id_)\n\n# def rffw(name):\n#     id_ = get_forum_id(name)\n#     logger.info('Removing %s (%s) from forums', name, id_)\n#     forums.remove(id_)\n\n# def aftw(name):\n#     id_ = get_forum_id(name)\n#     logger.info('Appending %s to forums', name)\n#     forums.add(name)\n\n# def rffw(name):\n#     id_ = get_forum_id(name)\n#     logger.info('Removing %s from forums', name)\n#     forums.remove(name)\n\nr_udf = re.compile(regexp.udf_prefix)\n\ndef affu(urls):\n    for user, domain, forum in r_udf.findall(urls):\n        if domain not in forums:\n            forums[domain] = set()\n        if len(forum) > 0:\n            get_forum_id(forum)\n        logger.info('Appending %s:%s to forums[%s]', user, forum, domain)\n        forums[domain].add((user, forum))\n\ndef rffu(urls):\n    for user, domain, forum in r_udf.findall(urls):\n        if len(forum) > 0:\n            get_forum_id(forum)\n        logger.info('Removing %s:%s from forums[%s]', user, forum, domain)\n        forums[domain].remove((user, forum))\n\ndef add_user(domain, login, passwd):\n    uq = wm.get_userqueue(domain)\n    uq.put({'login': login, 'passwd': passwd}, False)\n\ndef send_to_wm(frames):\n    msg = [frames[0]]\n    msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))\n    sig_sock.send_multipart(msg)\n\ndef send_passthrough(frames):\n    msg = [b'WipeManager']\n    msg.extend(wzrpc.make_sig_msg(b'WipeManager', b'passthrough', frames))\n    sig_sock.send_multipart(msg)\n\ndef drop_users():\n    send_passthrough([b'WipeSkel', b'WipeSkel', b'drop-user'])\n\ndef log_spawn_name():\n    send_passthrough([b'WipeThread', b'WipeThread', b'log-spawn-name'])\n\nif c.no_shell:\n    while True:\n        time.sleep(1)\nelse:\n    try:\n        import IPython\n        IPython.embed()\n    except ImportError:\n        # fallback shell\n        while True:\n            try:\n                exec(input('> '))\n            except KeyboardInterrupt:\n                print(\"KeyboardInterrupt\")\n            except SystemExit:\n                break\n            except:\n                print(traceback.format_exc())\n\nterminate()\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/waipu/bakawipe/blob/bf406be158ab778917f85ed94a669c2e63380722",
        "file_path": "/uniwipe.py",
        "source": "# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nfrom sup.net import NetError\nfrom wzworkers import WorkerInterrupt\nfrom wipeskel import WipeSkel, WipeState, cstate\nfrom beon import exc, regexp\nimport re\n\nclass UniWipe(WipeSkel):\n    def __init__(self, forums, targets, sbjfun, msgfun, *args, **kvargs):\n        self.sbjfun = sbjfun\n        self.msgfun = msgfun\n        self.forums = forums\n        self.targets = (type(targets) == str and [('', targets)]\n                        or type(targets) == tuple and list(targets)\n                        or targets)\n        super().__init__(*args, **kvargs)\n\n    def on_caprate_limit(self, rate):\n        if not self.logined:\n            self._capdata = (0, 0)\n            return\n        self.log.warning('Caprate limit reached, calling dologin() for now')\n        self.dologin()\n        # super().on_caprate_limit(rate)\n\n    def comment_loop(self):\n        for t in self.targets:\n            self.schedule(self.add_comment, (t, self.msgfun()))\n        if len(self.targets) == 0:\n            self.schedule(self.scan_targets_loop)\n        else:\n            self.schedule(self.comment_loop)\n\n    def add_comment(self, t, msg):\n        # with cstate(self, WipeState.posting_comment):\n        if True: # Just a placeholder\n            try:\n                # self.counter_tick()\n                self.postmsg(t[1], msg, t[0])\n            except exc.Success as e:\n                self.counters['comments'] += 1\n                self.w.sleep(self.comment_successtimeout)\n            except exc.Antispam as e:\n                self.w.sleep(self.comment_successtimeout)\n                self.schedule(self.add_comment, (t, msg))\n            except (exc.Closed, exc.UserDeny) as e:\n                try:\n                    self.targets.remove(t)\n                except ValueError:\n                    pass\n                self.w.sleep(self.comment_successtimeout)\n            except exc.Captcha as e:\n                self.log.error('Too many wrong answers to CAPTCHA')\n                self.schedule(self.add_comment, (t, msg))\n            except exc.UnknownAnswer as e:\n                self.log.warn('%s: %s', e, e.answer)\n                self.schedule(self.add_comment, (t, msg))\n            except exc.Wait5Min as e:\n                self.schedule(self.add_comment, (t, msg))\n                self.schedule_first(self.switch_user)\n            except exc.EmptyAnswer as e:\n                self.log.info('Removing %s from targets', t)\n                try:\n                    self.targets.remove(t)\n                except ValueError as e:\n                    pass\n                self.w.sleep(self.errortimeout)\n            except exc.TemporaryError as e:\n                self.schedule(self.add_comment, (t, msg))\n                self.w.sleep(self.errortimeout)\n            except exc.PermanentError as e:\n                try:\n                    self.targets.remove(t)\n                except ValueError as e:\n                    pass\n                self.w.sleep(self.errortimeout)\n            except UnicodeDecodeError as e:\n                self.log.exception(e)\n                self.w.sleep(self.errortimeout)\n\n    def forumwipe_loop(self):\n        for f in self.forums:\n            self.counter_tick()\n            try:\n                self.addtopic(self.msgfun(), self.sbjfun(), f)\n            except exc.Success as e:\n                self.counters['topics'] += 1\n                self.w.sleep(self.topic_successtimeout)\n            except exc.Wait5Min as e:\n                self.topic_successtimeout = self.topic_successtimeout + 0.1\n                self.log.info('Wait5Min exc caught, topic_successtimeout + 0.1, cur: %f',\n                    self.topic_successtimeout)\n                self.w.sleep(self.topic_successtimeout)\n            except exc.Captcha as e:\n                self.log.error('Too many wrong answers to CAPTCHA')\n                self.long_sleep(10)\n            except exc.UnknownAnswer as e:\n                self.log.warning('%s: %s', e, e.answer)\n                self.w.sleep(self.errortimeout)\n            except exc.PermanentError as e:\n                self.log.error(e)\n                self.w.sleep(self.errortimeout)\n            except exc.TemporaryError as e:\n                self.log.warn(e)\n                self.w.sleep(self.errortimeout)\n\n    def get_targets(self):\n        found_count = 0\n        for user, forum in self.forums:\n            targets = []\n            self.log.debug('Scanning first page of the forum %s:%s', user, forum)\n            page = self.site.get_page('1', forum, user)\n            rxp = re.compile(regexp.f_sub_id.format(user, self.site.domain, forum))\n            found = set(map(lambda x: (user, x[0]+x[1]), rxp.findall(page)))\n            for t in found:\n                if (t in self.pc.sets['closed']\n                    or t in self.pc.sets['bumplimit']\n                    or t in self.targets):\n                    continue\n                targets.append(t)\n            lt = len(targets)\n            found_count += lt\n            if lt > 0:\n                self.log.info('Found %d new targets in forum %s:%s', lt, user, forum)\n            else:\n                self.log.debug('Found no new targets in forum %s:%s', user, forum)\n            self.targets.extend(targets)\n        return found_count\n\n    def scan_targets_loop(self):\n        with cstate(self, WipeState.scanning_for_targets):\n            while len(self.targets) == 0:\n                c = self.get_targets()\n                if c == 0:\n                    self.log.info('No targets found at all, sleeping for 30 seconds')\n                    self.long_sleep(30)\n            self.schedule(self.comment_loop)\n        if len(self.forums) == 0:\n            self.schedule(self.wait_loop)\n\n    def wait_loop(self):\n        if len(self.targets) > 0:\n            self.schedule(self.comment_loop)\n            return\n        if len(self.forums) == 0:\n            with cstate(self, WipeState.waiting_for_targets):\n                while len(self.forums) == 0:\n                    # To prevent a busy loop.\n                    self.counter_tick()\n                    self.w.sleep(1)\n        self.schedule(self.scan_targets_loop)\n\n    def _run(self):\n        self.schedule(self.dologin)\n        self.schedule(self.wait_loop)\n        self.schedule(self.counter_ticker.tick)\n        try:\n            self.perform_tasks()\n        except NetError as e:\n            self.log.error(e)\n        except WorkerInterrupt as e:\n            self.log.warning(e)\n        except Exception as e:\n            self.log.exception(e)\n        self.return_user()\n# tw_flag = False\n# if len(self.targets) > 0:\n#     with cstate(self, WipeState.posting_comment):\n#         while len(self.targets) > 0:\n#             self.threadwipe_loop()\n#     if not tw_flag:\n#         tw_flag = True\n# if tw_flag:\n#     # Sleep for topic_successtimeout after last comment\n#     # to prevent a timeout spike\n#     self.w.sleep(self.topic_successtimeout)\n#     tw_flag = False\n# with cstate(self, WipeState.posting_topic):\n# self.forumwipe_loop()\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/waipu/bakawipe/blob/bf406be158ab778917f85ed94a669c2e63380722",
        "file_path": "/wipeskel.py",
        "source": "# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nimport logging, re\nfrom queue import Queue, Empty\nimport zmq\nimport beon, sup, wzrpc\nfrom beon import regexp\nfrom wzworkers import WorkerInterrupt\nfrom ocr import OCRError, PermOCRError, TempOCRError\nfrom sup.ticker import Ticker\nfrom userdata import short_wordsgen\nfrom enum import Enum\nfrom collections import Counter, deque\n\nclass ProcessContext:\n    def __init__(self, name, ctx, wz_addr, noproxy_rp):\n        self.log = logging.getLogger('.'.join((name, type(self).__name__)))\n        self.zmq_ctx = ctx\n        self.ticker = Ticker()\n        self.sets = {}\n        self.sets['targets'] = set()\n        self.sets['waiting'] = dict()\n        self.sets['pending'] = set()\n        self.sets['closed'] = set()\n        self.sets['bumplimit'] = set()\n        self.sets['protected'] = set()\n        self.wz_addr = wz_addr\n        self.noproxy_rp = noproxy_rp\n\n    def make_wz_sock(self):\n        self.log.debug('Initializing WZRPC socket')\n        wz_sock = self.zmq_ctx.socket(zmq.DEALER)\n        wz_sock.setsockopt(zmq.IPV6, True)\n        wz_sock.connect(self.wz_addr)\n        return wz_sock\n\n    def check_waiting(self):\n        elapsed = self.ticker.elapsed()\n        waiting = self.sets['waiting']\n        for k, v in waiting.copy().items():\n            rem = v - elapsed\n            if rem <= 0:\n                del waiting[k]\n                self.log.info('Removing %s from %s', k[0], k[1])\n                try:\n                    self.sets[k[1]].remove(k[0])\n                except KeyError:\n                    self.log.error('No %s in %s', k[0], k[1])\n            else:\n                waiting[k] = rem\n\n    def add_waiting(self, sname, item, ttl):\n        self.sets['waiting'][(item, sname)] = ttl\n\nclass WTState(Enum):\n    null = 0\n    starting = 2\n    empty = 3\n    sleeping = 4\n    running = 5\n\nclass WipeState(Enum):\n    null = 0\n    starting = 2\n    terminating = 3\n    sleeping = 4\n    running = 5\n\n    logging_in = 6\n    post_login_hooks = 7\n    registering = 8\n    pre_register_hooks = 9\n    post_register_hooks = 10\n    deobfuscating_capage = 11\n    solving_captcha = 12\n    reporting_code = 13\n\n    operation = 50\n    waiting_for_targets = 51\n    scanning_for_targets = 52\n    posting_comment = 53\n    posting_topic = 54\n\nclass state:\n    def __init__(self, defstate):\n        self.defstate = defstate\n        self.state = defstate\n\n    def __call__(self, state):\n        self.state = state\n        return self\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, exception_type, exception_value, traceback):\n        self.state = self.defstate\n\n    @property\n    def name(self):\n        return self.state.name\n\n    @property\n    def value(self):\n        return self.state.value\n\nclass cstate:\n    def __init__(self, obj, state):\n        self.obj = obj\n        self.backstate = obj.state\n        self.newstate = state\n\n    def __enter__(self):\n        self.obj.log.info('Switching state to %s', repr(self.newstate))\n        self.obj.state = self.newstate\n\n    def __exit__(self, exception_type, exception_value, traceback):\n        self.obj.log.info('Switching state to %s', repr(self.backstate))\n        self.obj.state = self.backstate\n\n\nclass WipeThread:\n    def __init__(self, pc, spawnqueue, *args, **kvargs):\n        self.pc = pc\n        self.spawnqueue = spawnqueue\n        self.spawn = None\n        self.state = WTState.null\n        self.wz_reply = None\n\n    def deobfuscate_capage(self, domain, page):\n        result = []\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success or status == wzrpc.status.error:\n                result.extend(map(lambda x:x.decode('utf-8'), data))\n            elif status == wzrpc.status.e_req_denied:\n                self.log.warn('Status {0}, reauthentificating'.\\\n                    format(wzrpc.name_status(status)))\n                self.p.auth_requests()\n                that.retry = True\n            elif status == wzrpc.status.e_timeout:\n                self.log.warn('Timeout {0}, retrying'.format(data[0]))\n                that.retry = True\n            else:\n                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))\n                that.retry = True\n        self.p.wz_wait_reply(accept,\n            b'Evaluator', b'evaluate', (domain.encode('utf-8'), page.encode('utf-8')),\n            timeout=60)\n        return tuple(result)\n\n    def solve_captcha(self, img):\n        result = []\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success or status == wzrpc.status.error:\n                result.extend(map(lambda x:x.decode('utf-8'), data))\n            elif status == wzrpc.status.e_req_denied:\n                self.log.warn('Status {0}, reauthentificating'.\\\n                    format(wzrpc.name_status(status)))\n                self.p.auth_requests()\n                that.retry = True\n            elif status == wzrpc.status.e_timeout:\n                self.log.warn('Timeout {0}, retrying'.format(data[0]))\n                that.retry = True\n            else:\n                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))\n                that.retry = True\n        self.p.wz_wait_reply(accept,\n            b'Solver', b'solve', (b'inbound', img), timeout=300)\n        if len(result) == 2: # Lame and redundant check. Rewrite this part someday.\n            return result\n        else:\n            raise OCRError('Solver returned error %s', result)\n        return tuple(result)\n\n    def report_code(self, cid, status):\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.log.debug('Successfully reported captcha status')\n            elif status == wzrpc.status.error:\n                self.log.error('Solver returned error on report: %s', repr(data))\n            elif status == wzrpc.status.e_req_denied:\n                self.log.warn('Status {0}, reauthentificating'.\\\n                    format(wzrpc.name_status(status)))\n                self.p.auth_requests()\n            else:\n                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))\n                that.retry = True\n        self.p.wz_wait_reply(accept,\n            b'Solver', b'report', (status.encode('utf-8'), cid.encode('utf-8')))\n                \n    def __call__(self, parent):\n        self.p = parent\n        self.log = parent.log\n        self.running = parent.running\n        self.sleep = parent.inter_sleep\n        self.p.wz_auth_requests = [\n            (b'Evaluator', b'evaluate'),\n            (b'Solver', b'solve'),\n            (b'Solver', b'report')]\n        cst = cstate(self, WTState.starting)\n        cst.__enter__()\n        self.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeThread')\n        def handle_lsn(interface, method, data):\n            if hasattr(self, 'spawn') and self.spawn:\n                self.log.info('My current spawn is %s, state %s',\n                    self.spawn.name, self.spawn.state.name)\n            else:\n                self.log.debug('Currently I do not have spawn')\n        self.p.wz.set_sig_handler(b'WipeThread', b'log-spawn-name', handle_lsn)\n        def handle_te(interface, method, data):\n            if self.state is WTState.empty:\n                self.p.term()\n        self.p.wz.set_sig_handler(b'WipeThread', b'terminate-empty', handle_te)\n\n        try:\n            self.p.wz_connect()\n            self.p.auth_requests()\n        except WorkerInterrupt as e:\n            self.log.error(e)\n            return\n        with cstate(self, WTState.empty):\n            while self.running.is_set():\n                try:\n                    self.spawn = self.spawnqueue.get(False)\n                except Empty:\n                    self.sleep(1)\n                    continue\n                with cstate(self, WTState.running):\n                    try:\n                        self.spawn.run(self)\n                    except WorkerInterrupt as e:\n                        self.log.error(e)\n                    except Exception as e:\n                        self.log.exception('Spawn throwed exception %s, requesting new', e)\n                    del self.spawn\n                    self.spawn = None\n                    self.spawnqueue.task_done()\n        cst.__exit__(None, None, None)\n\nclass WipeSkel(object):\n    reglimit = 10\n    loglimit = 10\n    conlimit = 3\n    catrymax = 3\n    _capdata = (0, 0)\n    caprate = 0\n    caprate_minp = 10\n    caprate_limit = 0.9\n    successtimeout = 1\n    comment_successtimeout = 0\n    topic_successtimeout = 0.8\n    counter_report_interval = 60\n    errortimeout = 3\n    uqtimeout = 5  # Timeout for userqueue\n    stoponclose = True\n    die_on_neterror = False\n    def __init__(self, pc, rp, domain, mrc, userqueue=None):\n        self.pc = pc\n        self.rp = rp\n        self.state = WipeState.null\n        self.site = beon.Beon(domain, self.http_request)\n        self.name = '.'.join((\n            type(self).__name__,\n            self.rp.proxy.replace('.', '_') if self.rp.proxy\n            else 'noproxy',\n            self.site.domain.replace('.', '_')))\n        self.rp.default_encoding = 'cp1251'\n        self.rp.default_decoding = 'cp1251'\n        self.rp.def_referer = self.site.ref  # Referer for net.py\n        self.hooks = {\n            'pre_register_new_user': [],\n            'post_register_new_user': [],\n            'post_login': [],\n            'check_new_user': [],\n        }\n        self.counter_ticker = Ticker()\n        self.counters = Counter()\n        self.task_deque = deque()\n        self.logined = False\n        self.noproxy_rp = self.pc.noproxy_rp\n        self.mrc = mrc\n        if userqueue:\n            self.userqueue = userqueue\n        else:\n            self.userqueue = Queue()\n\n    def schedule(self, task, args=(), kvargs={}):\n        self.task_deque.appendleft((task, args, kvargs))\n\n    def schedule_first(self, task, args=(), kvargs={}):\n        self.task_deque.append((task, args, kvargs))\n\n    def perform_tasks(self):\n        with cstate(self, WipeState.running):\n            while self.w.running.is_set():\n                self.counter_tick()\n                try:\n                    t = self.task_deque.pop()\n                except IndexError:\n                    return\n                t[0](*t[1], **t[2])\n\n    def long_sleep(self, time):\n        time = int(time)\n        with cstate(self, WipeState.sleeping):\n            step = int(time/10 if time > 10 else 1)\n            for s in range(0, time, step):\n                self.w.sleep(step)\n                self.counter_tick()\n\n    def http_request(self, url, postdata=None, onlyjar=False, referer=None,\n                     encoding=None, decoding=None):\n        _conc = 0\n        while self.w.running.is_set():\n            _conc += 1\n            try:\n                return self.rp.http_req(\n                    url, postdata, onlyjar, referer, encoding, decoding)\n            except sup.NetError as e:\n                if isinstance(e, sup.ConnError):\n                    if self.die_on_neterror and _conc > self.conlimit:\n                        raise\n                    self.log.warn('%s, waiting. t: %s', e.args[0], _conc)\n                    self.w.sleep(self.errortimeout)\n                else:\n                    self.log.error('%d %s', e.ec, e.args[0])\n                    if self.die_on_neterror:\n                        raise\n                    else:\n                        self.w.sleep(10)\n        else:\n            raise WorkerInterrupt()\n\n    def gen_userdata(self):\n        return short_wordsgen()\n\n    def update_caprate(self, got):\n        p, g = self._capdata\n        p += 1\n        if got is True:\n            self.counters['captchas'] += 1\n            g += 1\n        if p >= 255:\n            p = p/2\n            g = g/2\n        self._capdata = (p, g)\n        self.caprate = g/p\n        self.log.debug('Caprate: pos:%f got:%f rate:%f',\n                       p, g, self.caprate)\n        if (self.caprate_limit > 0\n            and p > self.caprate_minp\n            and self.caprate > self.caprate_limit):\n            self.on_caprate_limit(self.caprate)\n            # if self.getuser() == 'guest':\n            #     self.log.info(\"lol, we were trying to post from guest\")\n            #     while not self.relogin(): self.w.sleep(self.errortimeout)\n            # else:\n            #     while not self.dologin(): self.w.sleep(self.errortimeout)\n\n    def counter_tick(self):\n        if self.counter_report_interval == 0:\n            return\n        e = self.counter_ticker.elapsed(False)\n        if e > self.counter_report_interval:\n            self.counter_ticker.tick()\n            ccount = self.counters['comments']\n            tcount = self.counters['topics']\n            if ccount > 0:\n                self.log.info('%d comments in %d seconds, %0.2f cps, %0.2f caprate',\n                    ccount, e, ccount/e, self.caprate)\n                self.counters['comments'] = 0\n            if tcount > 0:\n                self.log.info('%d topics in %d seconds, %0.2f tps, %0.2f caprate',\n                    tcount, e, tcount/e, self.caprate)\n                self.counters['topics'] = 0\n\n    def on_caprate_limit(self, rate):\n        if not self.logined:\n            self._capdata = (0, 0)\n            return\n        self.log.warn('Caprate %f is over the limit', rate)\n        raise Exception('Caprate limit reached')\n\n    def captcha_wrapper(self, inc_fun, fin_fun, *args, **kvargs):\n        # TODO: report codes after solving cycle instead of scheduling them.\n        try:\n            self.log.debug('captcha_wrapper: calling inc_fun %s', repr(inc_fun))\n            self.log.error('captcha_wrapper: inc_fun returned %s',\n                           repr(inc_fun(*args, **kvargs)))\n        except beon.Success as e:\n            self.update_caprate(False)\n            raise\n        except beon.Captcha as e:\n            self.log.warn(e)\n            _page = e.page\n            _catry = e.catry\n            # Don't update caprate with positives if not logined\n            if self.logined is True:\n                try:\n                    user = self.find_login(_page)\n                except beon.PermanentError:\n                    self.log.debug(e)\n                else:\n                    if user != self.site.ud['login']:\n                        self.log.warn('We were posting as %s, but our login is %s',\n                                      user, self.site.ud['login'])\n                        self.schedule_first(self.relogin)\n                        return\n            self.update_caprate(True)\n            reports = []\n            def r():\n                if len(reports) > 0:\n                    with cstate(self, WipeState.reporting_code):\n                        for cid, status in reports:\n                            self.report_code(cid, status)\n                    reports.clear()\n            while self.w.running.is_set():\n                _requested_new = False\n                try:\n                    with cstate(self, WipeState.solving_captcha):\n                        cahash, cacode, cid = self.solve_captcha(_page)\n                except TempOCRError as e:\n                    self.log.error('OCRError: %s, retrying', e)\n                    self.w.sleep(self.errortimeout)\n                    continue\n                except OCRError as e:\n                    self.log.error('OCRError: %s, requesting new captcha', e)\n                    _requested_new = True\n                    cahash, cacode, cid = e.cahash, '', None\n                else:\n                    self.log.info('code: %s', cacode)\n                try:\n                    self.log.debug('captcha_wrapper calling fin_fun %s', repr(fin_fun))\n                    self.log.error('captcha_wrapper: fin_fun returned %s',\n                        repr(fin_fun(cahash, cacode, *args, catry=_catry, **kvargs)))\n                    break\n                except beon.Success as e:\n                    self.counters['captchas_solved'] += 1\n                    if cid:\n                        reports.append((cid, 'good'))\n                    r()\n                    raise\n                except beon.Captcha as e:\n                    _catry = e.catry\n                    _page = e.page\n                    if _requested_new:\n                        self.log.warn('New captcha requested c:%d', _catry)\n                        continue\n                    self.log.warn('%s c:%d', e, _catry)\n                    self.counters['captchas_wrong'] += 1\n                    if cid:\n                        reports.append((cid, 'bad'))\n                    if _catry > self.catrymax:\n                        r()\n                        raise\n                except Exception as e:\n                    if cid:\n                        reports.append((cid, 'bad'))\n                    r()\n                    raise\n\n    def adaptive_timeout_wrapper(self, fun, *args, **kvargs):\n        try:\n            return fun(*args, **kvargs)\n        except beon.Antispam as e:\n            self.log.info('Antispam exc caught, successtimeout + 0.1, cur: %f',\n                          self.successtimeout)\n            self.successtimeout = self.successtimeout + 0.1\n            raise\n\n    def register_new_user(self):\n        with cstate(self, WipeState.registering):\n            _regcount = 0\n            while self.w.running.is_set():\n                self.w.p.poll()\n                ud = self.gen_userdata()\n                self.request_email(ud)\n                for c in self.hooks['pre_register_new_user']:\n                    c(self, ud)\n                self.log.info('Generated new userdata: %s, registering', ud['login'])\n                self.log.debug('Userdata: %s', repr(ud))\n                try:\n                    udc = ud.copy()\n                    if 0 in udc:\n                        del udc[0]\n                    self.register(**udc)\n                except beon.Success as e:\n                    self.validate_email(ud)\n                    for c in self.hooks['post_register_new_user']:\n                        c(self, ud)\n                    return ud\n                except (beon.EmptyAnswer, beon.Wait5Min) as e:\n                    self.log.error('%s, sleeping for 100 seconds', e)\n                    self.long_sleep(100)\n                except beon.Captcha as e:\n                    self.log.error('Too much wrong answers to CAPTCHA')\n                    continue\n                except beon.UnknownAnswer as e:\n                    _regcount += 1\n                    if not _regcount < self.reglimit:\n                        raise beon.RegRetryLimit('Cannot register new user')\n                    self.log.error('%s, userdata may be invalid, retrying c:%d',\n                                e, _regcount)\n                    self.w.sleep(self.errortimeout)\n            else:\n                raise WorkerInterrupt()\n\n    def get_new_user(self):\n        ud = self.userqueue.get(True, self.uqtimeout)\n        self.userqueue.task_done()\n        for c in self.hooks['check_new_user']:\n            c(self, ud)\n        return ud\n\n    def login(self, login, passwd, **kvargs):\n        if not self.site.login_lock.acquire(False):\n            with self.site.login_lock.acquire():\n                return\n        self.logined = False\n        try:\n            self.captcha_wrapper(self.site.logininc, self.site.loginfin,\n                                 login, passwd, **kvargs)\n        except beon.Success as e:\n            self.logined = True\n            self.counters['logged_in'] += 1\n            self.log.info(e)\n            raise\n        finally:\n            self.site.login_lock.release()\n\n    def find_login(self, rec):\n        try:\n            return re.findall(regexp.var_login, rec)[0]\n        except IndexError:\n            raise beon.PermanentError('No users in here')\n\n    def get_current_login(self):\n        return self.find_login(self.site.get_page('1'))\n\n    def dologin(self):\n        '''Choose user, do login and return it.'''\n        while self.w.running.is_set():\n            try:\n                self.site.ud = self.get_new_user()\n            except Empty:\n                self.log.info('No users in queue')\n                self.site.ud = self.register_new_user()\n                return\n            try:\n                with cstate(self, WipeState.logging_in):\n                    self.login(self.site.ud['login'], self.site.ud['passwd'])\n            except beon.Success as e:\n                self.site.postuser = self.site.ud['login']\n                self.site.postpass = self.site.ud['passwd']\n                self.validate_email(self.site.ud)\n                for c in self.hooks['post_login']:\n                    c(self, self.site.ud)\n                self.w.sleep(self.successtimeout)\n                return\n            except beon.Captcha as e:\n                self.log.error('Too many wrong answers to CAPTCHA')\n                self.schedule(self.long_sleep, (10,))\n                self.schedule(self.dologin)\n            except beon.InvalidLogin as e:\n                self.log.error(\"Invalid login, passing here\")\n                self.schedule(self.dologin)\n                self.w.sleep(self.errortimeout)\n            except beon.TemporaryError as e:\n                self.userqueue.put(self.site.ud)\n                self.log.warn(e)\n                self.schedule(self.dologin)\n                self.w.sleep(self.errortimeout)\n        # else:\n        #     pending = len(self.pc.sets['pending'])\n        #     self.log.warn(\"No more logins here, %s pending.\"%pending)\n        #     if pending == 0: return False\n\n    def relogin(self):\n        '''Relogin with current user or do login'''\n        if 'login' in self.site.ud:\n            while self.w.running.is_set():\n                try:\n                    with cstate(self, WipeState.logging_in):\n                        self.login(self.site.ud['login'], self.site.ud['passwd'])\n                except beon.Success as e:\n                    for c in self.hooks['post_login']:\n                        c(self, self.site.ud)\n                    self.w.sleep(self.successtimeout)\n                    return\n                except beon.InvalidLogin as e:\n                    self.log.error(e)\n                    self.w.sleep(self.errortimeout)\n                    break\n                except beon.TemporaryError as e:\n                    self.log.warn(e)\n                    self.w.sleep(self.errortimeout)\n                    continue\n        self.dologin()\n\n    def request_email(self, ud):\n        ud['email'] = self.mailrequester.gen_addr()\n        ud[0]['email_service'] = type(self.mailrequester).__name__\n        ud[0]['email_requested'] = False\n        ud[0]['email_validated'] = False\n\n    def validate_email(self, ud):\n        if ('email' not in ud or\n            'email_service' not in ud[0] or\n            'email_requested' not in ud[0] or\n            'email_validated' not in ud[0] or\n            not ud[0]['email_service'] == type(self.mailrequester).__name__\n            or ud[0]['email_validated'] is True):\n            return\n        if not ud[0]['email_requested']:\n            try:\n                self.site.validate_email_inc()\n            except beon.Success as e:\n                ud[0]['email_requested'] = True\n                self.log.info(e)\n        self.log.info('Requesting messages for %s', ud['email'])\n        messages = self.mailrequester.get_messages(ud['email'])\n        for msg in messages:\n            if not msg['mail_from'].find('<reminder@{0}>'.format(self.site.domain)):\n                continue\n            h = re.findall(regexp.hashinmail.format(self.site.domain),\n                msg['mail_html'])\n            if len(h) > 0:\n                try:\n                    self.site.validate_email_fin(h[0])\n                except beon.Success as e:\n                    ud[0]['email_validated'] = True\n                    self.log.info(e)\n\n    def switch_user(self):\n        '''Log in with new user, but return the previous one'''\n        if 'login' in self.site.ud:\n            self.log.info('Switching user %s', self.site.ud['login'])\n            self.return_user()\n        self.site.ud = self.register_new_user()\n\n    def return_user(self, ud=None):\n        if not ud:\n            if (hasattr(self.site, 'ud') and self.site.ud):\n                ud = self.site.ud\n                self.site.ud = None\n            else:\n                return\n        self.log.info('Returning user %s to userqueue', ud['login'])\n        self.userqueue.put(ud, False)\n\n    def postmsg(self, target, msg, tuser=None, **kvargs):\n        tpair = (tuser, target)\n        target = target.lstrip('0')\n        ptarget = (':'.join(tpair) if tuser else target)\n        try:\n            try:\n                self.site.ajax_addcomment(target, msg, tuser, **kvargs)\n            except beon.Success as e:\n                self.update_caprate(False)\n                raise\n            except beon.Redir as e:\n                self.log.warn(e)\n                self.log.warn('Using non-ajax addcomment')\n                self.captcha_wrapper(self.site.addcomment, self.site.addcommentfin,\n                                     target, msg, tuser, **kvargs)\n        except beon.Success as e:\n            self.counters['comments_added'] += 1\n            self.log.debug(e)\n            raise\n        except beon.Antispam as e:\n            self.counters['antispam'] += 1\n            self.comment_successtimeout = self.comment_successtimeout + 0.1\n            self.log.info('Antispam exc caught, comment_successtimeout + 0.1, cur: %f',\n                self.comment_successtimeout)\n            raise\n        except beon.GuestDeny as e:\n            self.counters['delogin'] += 1\n            self.log.warn('%s, trying to log in', e)\n            self.schedule_first(self.relogin)\n            raise\n        except beon.Bumplimit as e:\n            self.log.info(e)\n            self.pc.sets['bumplimit'].add(tpair)\n            raise\n        except (beon.Closed, beon.UserDeny) as e:\n            self.pc.sets['closed'].add(tpair)\n            if self.stoponclose:\n                self.log.info(e)\n                raise beon.PermClosed(\"%s:%s is closed\", tpair, e.answer)\n            else:\n                self.log.info('%s, starting 300s remove timer', e)\n                self.pc.add_waiting('closed', tpair, 300)\n                raise\n        except beon.Wait5Min as e:\n            self.counters['wait5mincount'] += 1\n            self.log.warn(e)\n            raise\n        except beon.TemporaryError as e:\n            self.log.warn(e)\n            raise\n        except beon.PermanentError as e:\n            self.log.error(e)\n            raise\n\n    def addtopic(self, msg, subj, forum='1', tuser=None, **kvargs):\n        try:\n            self.captcha_wrapper(self.site.addtopicinc, self.site.addtopicfin,\n                                 msg, forum, subj, tuser, **kvargs)\n        except beon.Success as e:\n            self.counters['topics_added'] += 1\n            self.log.debug(e)\n            raise\n        except beon.Wait5Min as e:\n            self.counters['wait5min'] += 1\n            raise\n            # self._bancount += 1\n            # if 'login' in self.site.ud:\n            #     self.log.warn(e)\n            #     self.log.warn('Trying to change user')\n            #     self.pc.sets['pending'].add(self.site.ud['login'])\n            #     self.pc.add_waiting('pending', self.site.ud['login'], 300)\n            #     self.dologin()\n            # else:\n            #     raise\n        except beon.GuestDeny as e:\n            if 'login' not in self.site.ud:\n                raise\n            self.counters['delogin'] += 1\n            self.log.warn('%s, trying to log in', e)\n            self.schedule_first(self.dologin)\n            raise\n\n    def register(self, login, passwd, name, email, **kvargs):\n        self.logined = False\n        try:\n            self.captcha_wrapper(self.site.reginc, self.site.regfin,\n                                 login, passwd, name, email, **kvargs)\n        except beon.Success as e:\n            self.log.info(e)\n            self.logined = True\n            self.counters['users_registered'] += 1\n            raise\n\n    def solve_captcha(self, page):\n        # with cstate(self, WipeState.deobfuscating_capage):\n        self.log.info('Deobfuscating capage')\n        capair = self.w.deobfuscate_capage(self.site.domain, page)\n        self.log.info('Answer: %s', repr(capair))\n        if len(capair) != 2:\n            raise PermOCRError('Invalid answer from Evaluator')\n        self.log.info('Downloading captcha image')\n        try:\n            img = self.http_request(capair[1])\n        except sup.net.HTTPError as e:\n            # check error code here\n            self.log.error(e)\n            raise PermOCRError('404 Not Found on caurl', cahash=capair[0])\n        self.log.info('Sending captcha image to solver')\n        try:\n            result, cid = self.w.solve_captcha(img)\n        except OCRError as e:\n            e.cahash = capair[0]\n            raise\n        return capair[0], result, cid\n    \n    def report_code(self, cid, status):\n        self.log.info('Reporting %s code for %s', status, cid)\n        self.w.report_code(cid, status)\n        self.counters['captcha_codes_reported'] += 1\n\n    def run(self, caller):\n        self.w = caller\n        self.log = logging.getLogger(self.name)\n        self.run_time = Ticker()\n        cst = cstate(self, WipeState.starting)\n        cst.__enter__()\n        self.mailrequester = self.mrc(self.noproxy_rp, self.w.running, self.w.sleep)\n\n        # Get our own logger here, or use worker's?\n        self.log.info('Starting')\n        self.run_time.tick()\n\n        def drop_user_handler(interface, method, data):\n            self.log.info('drop-user signal recieved')\n            self.dologin()\n\n        self.w.p.wz.set_sig_handler(b'WipeSkel', b'drop-user', drop_user_handler)\n        self.w.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeSkel')\n\n        try:\n            self._run()\n        except Exception as e:\n            self.log.exception(e)\n        cst.__exit__(None, None, None)\n        with cstate(self, WipeState.terminating):\n            self.w.p.sig_sock.setsockopt(zmq.UNSUBSCRIBE, b'WipeSkel')\n            self.w.p.wz.del_sig_handler(b'WipeSkel', b'drop-user')\n            self.log.info(repr(self.counters))\n        self.log.info('Terminating, runtime is %ds', self.run_time.elapsed(False))\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/resamsel/dbmanagr/blob/3ac898f92854936a30dc8e3b301e5d5e3bbef895",
        "file_path": "/src/dbnav/wrapper.py",
        "source": "# -*- coding: utf-8 -*-\n#\n# Copyright  2014 Ren Samselnig\n#\n# This file is part of Database Navigator.\n#\n# Database Navigator is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Database Navigator is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Database Navigator.  If not, see <http://www.gnu.org/licenses/>.\n#\n\nimport sys\nimport os\nimport logging\nimport pdb\nimport urllib2\nimport json\nimport ijson\n\nfrom dbnav.writer import Writer\nfrom dbnav import logger as log\nfrom dbnav.jsonable import from_json\n\nCOMMANDS = {\n    'dbdiff': 'differ',\n    'dbexec': 'executer',\n    'dbexport': 'exporter',\n    'dbgraph': 'grapher',\n    'dbnav': 'navigator'\n}\n\n\nclass Wrapper(object):\n    def __init__(self, options=None):\n        self.options = options\n\n    def write(self):\n        try:\n            sys.stdout.write(Writer.write(self.run()))\n        except BaseException as e:\n            log.logger.exception(e)\n            return -1\n        return 0\n\n    def execute(self):  # pragma: no cover\n        \"\"\"To be overridden by sub classes\"\"\"\n        pass\n\n    def run(self):\n        try:\n            if self.options is not None and self.options.daemon:\n                log.logger.debug('Executing remotely')\n                return self.executer(*sys.argv)\n\n            log.logger.debug('Executing locally')\n            return self.execute()\n        except BaseException as e:\n            log.logger.exception(e)\n            if log.logger.getEffectiveLevel() <= logging.DEBUG:\n                # Start post mortem debugging only when debugging is enabled\n                if os.getenv('UNITTEST', 'False') == 'True':\n                    raise\n                if self.options.trace:\n                    pdb.post_mortem(sys.exc_info()[2])  # pragma: no cover\n            else:\n                # Show the error message if log level is INFO or higher\n                log.log_error(e)  # pragma: no cover\n\n    def executer(self, *args):\n        \"\"\"Execute remotely\"\"\"\n\n        options = self.options\n\n        try:\n            # from dbnav import daemon\n            # if not daemon.is_running(options):\n            #     daemon.start_server(options)\n\n            url = 'http://{host}:{port}/{path}'.format(\n                host=options.host,\n                port=options.port,\n                path=COMMANDS[options.prog])\n            request = json.dumps(args[1:])\n\n            log.logger.debug('Request to %s:\\n%s', url, request)\n\n            response = urllib2.urlopen(url, request)\n\n            for i in ijson.items(response, 'item'):\n                yield from_json(i)\n        except urllib2.HTTPError as e:\n            raise from_json(json.load(e))\n        except urllib2.URLError as e:\n            log.logger.error('Daemon not available: %s', e)\n        except BaseException as e:\n            log.logger.exception(e)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/almandin/fuxploider/blob/339574105dce4736f942481d69eb28ab417e86dc",
        "file_path": "/UploadForm.py",
        "source": "import logging,concurrent.futures\nfrom utils import *\nfrom urllib.parse import urljoin,urlparse\nfrom threading import Lock\n\nclass UploadForm :\n\tdef __init__(self,notRegex,trueRegex,session,size,postData,uploadsFolder=None,formUrl=None,formAction=None,inputName=None) :\n\t\tself.logger = logging.getLogger(\"fuxploider\")\n\t\tself.postData = postData\n\t\tself.formUrl = formUrl\n\t\turl = urlparse(self.formUrl)\n\t\tself.schema = url.scheme\n\t\tself.host = url.netloc\n\t\tself.uploadUrl = urljoin(formUrl, formAction)\n\t\tself.session = session\n\t\tself.trueRegex = trueRegex\n\t\tself.notRegex = notRegex\n\t\tself.inputName = inputName\n\t\tself.uploadsFolder = uploadsFolder\n\t\tself.size = size\n\t\tself.validExtensions = []\n\t\tself.httpRequests = 0\n\t\tself.codeExecUrlPattern = None #pattern for code exec detection using true regex findings\n\t\tself.logLock = Lock()\n\t\tself.stopThreads = False\n\t\tself.shouldLog = True\n\n\t#searches for a valid html form containing an input file, sets object parameters correctly\n\tdef setup(self,initUrl) :\n\t\tself.formUrl = initUrl\n\t\turl = urlparse(self.formUrl)\n\t\tself.schema = url.scheme\n\t\tself.host = url.netloc\n\n\t\tself.httpRequests = 0\n\t\ttry :\n\t\t\tinitGet = self.session.get(self.formUrl,headers={\"Accept-Encoding\":None})\n\t\t\tself.httpRequests += 1\n\t\t\tif self.logger.verbosity > 1 :\n\t\t\t\tprintSimpleResponseObject(initGet)\n\t\t\tif self.logger.verbosity > 2 :\n\t\t\t\tprint(\"\\033[36m\"+initGet.text+\"\\033[m\")\n\t\t\tif initGet.status_code < 200 or initGet.status_code > 300 :\n\t\t\t\tself.logger.critical(\"Server responded with following status : %s - %s\",initGet.status_code,initGet.reason)\n\t\t\t\texit()\n\t\texcept Exception as e :\n\t\t\t\tself.logger.critical(\"%s : Host unreachable (%s)\",getHost(initUrl),e)\n\t\t\t\texit()\n\t\t#rcuprer le formulaire,le dtecter\n\t\tdetectedForms = detectForms(initGet.text)\n\t\tif len(detectedForms) == 0 :\n\t\t\tself.logger.critical(\"No HTML form found here\")\n\t\t\texit()\n\t\tif len(detectedForms) > 1 :\n\t\t\tself.logger.critical(\"%s forms found containing file upload inputs, no way to choose which one to test.\",len(detectedForms))\n\t\t\texit()\n\t\tif len(detectedForms[0][1]) > 1 :\n\t\t\tself.logger.critical(\"%s file inputs found inside the same form, no way to choose which one to test.\",len(detectedForms[0]))\n\t\t\texit()\n\n\t\tself.inputName = detectedForms[0][1][0][\"name\"]\n\t\tself.logger.debug(\"Found the following file upload input : %s\",self.inputName)\n\t\tformDestination = detectedForms[0][0]\n\n\t\ttry :\n\t\t\tself.action = formDestination[\"action\"]\n\t\texcept :\n\t\t\tself.action = \"\"\n\t\tself.uploadUrl = urljoin(self.formUrl,self.action)\n\n\t\tself.logger.debug(\"Using following URL for file upload : %s\",self.uploadUrl)\n\n\t\tif not self.uploadsFolder and not self.trueRegex :\n\t\t\tself.logger.warning(\"No uploads folder nor true regex defined, code execution detection will not be possible.\")\n\t\telif not self.uploadsFolder and self.trueRegex :\n\t\t\tprint(\"No uploads path provided, code detection can still be done using true regex capturing group.\")\n\t\t\tcont = input(\"Do you want to use the True Regex for code execution detection ? [Y/n] \")\n\t\t\tif cont.lower().startswith(\"y\") or cont == \"\" :\n\t\t\t\tpreffixPattern = input(\"Preffix capturing group of the true regex with : \")\n\t\t\t\tsuffixPattern = input(\"Suffix capturing group of the true regex with : \")\n\t\t\t\tself.codeExecUrlPattern = preffixPattern+\"$captGroup$\"+suffixPattern\n\t\t\telse :\n\t\t\t\tself.logger.warning(\"Code execution detection will not be possible as there is no path nor regex pattern configured.\")\n\t\telse :\n\t\t\tpass#uploads folder provided\n\n\t#tries to upload a file through the file upload form\n\tdef uploadFile(self,suffix,mime,payload) :\n\t\twith tempfile.NamedTemporaryFile(suffix=suffix) as fd :\n\t\t\tfd.write(payload)\n\t\t\tfd.flush()\n\t\t\tfd.seek(0)\n\t\t\tfilename = os.path.basename(fd.name)\n\t\t\tif self.shouldLog :\n\t\t\t\tself.logger.debug(\"Sending file %s with mime type : %s\",filename,mime)\n\t\t\tfu = self.session.post(self.uploadUrl,files={self.inputName:(filename,fd,mime)},data=self.postData)\n\t\t\tself.httpRequests += 1\n\t\t\tif self.shouldLog :\n\t\t\t\tif self.logger.verbosity > 1 :\n\t\t\t\t\tprintSimpleResponseObject(fu)\n\t\t\t\tif self.logger.verbosity > 2 :\n\t\t\t\t\tprint(\"\\033[36m\"+fu.text+\"\\033[m\")\n\t\t\t\n\t\treturn (fu,filename)\n\n\t#detects if a given html code represents an upload success or not\n\tdef isASuccessfulUpload(self,html) :\n\t\tresult = False\n\t\tvalidExt = False\n\t\tif self.notRegex :\n\t\t\tfileUploaded = re.search(self.notRegex,html)\n\t\t\tif fileUploaded == None :\n\t\t\t\tresult = True\n\t\t\t\tif self.trueRegex :\n\t\t\t\t\tmoreInfo = re.search(self.trueRegex,html)\n\t\t\t\t\tif moreInfo :\n\t\t\t\t\t\tresult = str(moreInfo.groups())\n\t\tif self.trueRegex and not result :\n\t\t\tfileUploaded = re.search(self.trueRegex,html)\n\t\t\tif fileUploaded :\n\t\t\t\ttry :\n\t\t\t\t\tresult = str(fileUploaded.group(1))\n\t\t\t\texcept :\n\t\t\t\t\tresult = str(fileUploaded.group(0))\n\t\treturn result\n\n\t#callback function for matching html text against regex in order to detect successful uploads\n\tdef detectValidExtension(self, future) :\n\t\tif not self.stopThreads :\n\t\t\thtml = future.result()[0].text\n\t\t\text = future.ext[0]\n\n\t\t\tr = self.isASuccessfulUpload(html)\n\t\t\tif r :\n\t\t\t\tself.validExtensions.append(ext)\n\t\t\t\tif self.shouldLog :\n\t\t\t\t\tself.logger.info(\"\\033[1m\\033[42mExtension %s seems valid for this form.\\033[m\", ext)\n\t\t\t\t\tif r != True :\n\t\t\t\t\t\tself.logger.info(\"\\033[1;32mTrue regex matched the following information : %s\\033[m\",r)\n\n\t\t\treturn r\n\t\telse :\n\t\t\treturn None\n\n\t#detects valid extensions for this upload form (sending legit files with legit mime types)\n\tdef detectValidExtensions(self,extensions,maxN,extList=None) :\n\t\tself.logger.info(\"### Starting detection of valid extensions ...\")\n\t\tn = 0\n\t\tif extList :\n\t\t\ttmpExtList = []\n\t\t\tfor e in extList :\n\t\t\t\ttmpExtList.append((e,getMime(extensions,e)))\n\t\telse :\n\t\t\ttmpExtList = extensions\n\t\tvalidExtensions = []\n\n\t\textensionsToTest = tmpExtList[0:maxN]\n\t\twith concurrent.futures.ThreadPoolExecutor(max_workers=self.threads) as executor :\n\t\t\tfutures = []\n\t\t\ttry :\n\t\t\t\tfor ext in extensionsToTest:\n\t\t\t\t\tf = executor.submit(self.uploadFile,\".\"+ext[0],ext[1],os.urandom(self.size))\n\t\t\t\t\tf.ext = ext\n\t\t\t\t\tf.add_done_callback(self.detectValidExtension)\n\t\t\t\t\tfutures.append(f)\n\t\t\t\tfor future in concurrent.futures.as_completed(futures) :\n\t\t\t\t\ta = future.result()\n\t\t\t\t\tn += 1\n\t\t\texcept KeyboardInterrupt :\n\t\t\t\tself.shouldLog = False\n\t\t\t\texecutor.shutdown(wait=False)\n\t\t\t\tself.stopThreads = True\n\t\t\t\texecutor._threads.clear()\n\t\t\t\tconcurrent.futures.thread._threads_queues.clear()\n\t\treturn n\n\n\t#detects if code execution is gained, given an url to request and a regex supposed to match the executed code output\n\tdef detectCodeExec(self,url,regex) :\n\t\tif self.shouldLog :\n\t\t\tif self.logger.verbosity > 0 :\n\t\t\t\tself.logger.debug(\"Requesting %s ...\",url)\n\t\t\n\t\tr = self.session.get(url)\n\t\tif self.shouldLog :\n\t\t\tif r.status_code >= 400 :\n\t\t\t\tself.logger.warning(\"Code exec detection returned an http code of %s.\",r.status_code)\n\t\t\tself.httpRequests += 1\n\t\t\tif self.logger.verbosity > 1 :\n\t\t\t\tprintSimpleResponseObject(r)\n\t\t\tif self.logger.verbosity > 2 :\n\t\t\t\tprint(\"\\033[36m\"+r.text+\"\\033[m\")\n\n\t\tres = re.search(regex,r.text)\n\t\tif res :\n\t\t\treturn True\n\t\telse :\n\t\t\treturn False\n\n\t#core function : generates a temporary file using a suffixed name, a mime type and content, uploads the temp file on the server and eventually try to detect\n\t#\tif code execution is gained through the uploaded file\n\tdef submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None) :\n\t\tfu = self.uploadFile(suffix,mime,payload)\n\t\tuploadRes = self.isASuccessfulUpload(fu[0].text)\n\t\tresult = {\"uploaded\":False,\"codeExec\":False}\n\t\tif uploadRes :\n\t\t\tresult[\"uploaded\"] = True\n\t\t\tif self.shouldLog :\n\t\t\t\tself.logger.info(\"\\033[1;32mUpload of '%s' with mime type %s successful\\033[m\",fu[1], mime)\n\t\t\t\n\t\t\tif uploadRes != True :\n\t\t\t\tif self.shouldLog :\n\t\t\t\t\tself.logger.info(\"\\033[1;32m\\tTrue regex matched the following information : %s\\033[m\",uploadRes)\n\n\t\t\tif codeExecRegex and valid_regex(codeExecRegex) and (self.uploadsFolder or self.trueRegex) :\n\t\t\t\turl = None\n\t\t\t\tsecondUrl = None\n\t\t\t\tif self.uploadsFolder :\n\t\t\t\t\turl = self.schema+\"://\"+self.host+\"/\"+self.uploadsFolder+\"/\"+fu[1]\n\t\t\t\t\tfilename = fu[1]\n\t\t\t\t\tsecondUrl = None\n\t\t\t\t\tfor b in getPoisoningBytes() :\n\t\t\t\t\t\tif b in filename :\n\t\t\t\t\t\t\tsecondUrl = b.join(url.split(b)[:-1])\n\t\t\t\telif self.codeExecUrlPattern :\n\t\t\t\t\t#code exec detection through true regex\n\t\t\t\t\turl = self.codeExecUrlPattern.replace(\"$captGroup$\",uploadRes)\n\t\t\t\telse :\n\t\t\t\t\tpass\n\t\t\t\t\t#self.logger.warning(\"Impossible to determine where to find the uploaded payload.\")\n\t\t\t\tif url :\n\t\t\t\t\texecutedCode = self.detectCodeExec(url,codeExecRegex)\n\t\t\t\t\tif executedCode :\n\t\t\t\t\t\tresult[\"codeExec\"] = True\n\t\t\t\tif secondUrl :\n\t\t\t\t\texecutedCode = self.detectCodeExec(secondUrl,codeExecRegex)\n\t\t\t\t\tif executedCode :\n\t\t\t\t\t\tresult[\"codeExec\"] = True\n\t\treturn result\n\n\t#detects html forms and returns a list of beautifulSoup objects (detected forms)\n\tdef detectForms(html) :\n\t\tsoup = BeautifulSoup(html,'html.parser')\n\t\tdetectedForms = soup.find_all(\"form\")\n\t\treturnForms = []\n\t\tif len(detectedForms) > 0 :\n\t\t\tfor f in detectedForms :\n\t\t\t\tfileInputs = f.findChildren(\"input\",{\"type\":\"file\"})\n\t\t\t\tif len(fileInputs) > 0 :\n\t\t\t\t\treturnForms.append((f,fileInputs))\n\n\t\treturn returnForms",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/almandin/fuxploider/blob/339574105dce4736f942481d69eb28ab417e86dc",
        "file_path": "/fuxploider.py",
        "source": "#!/usr/bin/python3\nimport re,requests,argparse,logging,os,coloredlogs,datetime,getpass,tempfile,itertools,json,concurrent.futures,random\nfrom utils import *\nfrom UploadForm import UploadForm\nfrom threading import Lock\n#signal.signal(signal.SIGINT, quitting)\nversion = \"0.5.0\"\nlogging.basicConfig(datefmt='[%m/%d/%Y-%H:%M:%S]')\nlogger = logging.getLogger(\"fuxploider\")\n\ncoloredlogs.install(logger=logger,fmt='%(asctime)s %(levelname)s - %(message)s',level=logging.INFO)\nlogging.getLogger(\"requests\").setLevel(logging.ERROR)\n\n#################### TEMPLATES DEFINITION HERE ######################\ntemplatesFolder = \"payloads\"\nwith open(\"templates.json\",\"r\") as fd :\n\ttemplates = json.loads(fd.read())\n#######################################################################\ntemplatesNames = [x[\"templateName\"] for x in templates]\ntemplatesSection = \"[TEMPLATES]\\nTemplates are malicious payloads meant to be uploaded on the scanned remote server. Code execution detection is done based on the expected output of the payload.\"\ntemplatesSection += \"\\n\\tDefault templates are the following (name - description) : \"\nfor t in templates :\n\ttemplatesSection+=\"\\n\\t  * '\"+t[\"templateName\"]+\"' - \"+t[\"description\"]\n\nparser = argparse.ArgumentParser(epilog=templatesSection,description=__doc__, formatter_class=argparse.RawTextHelpFormatter)\nparser.add_argument(\"-d\", \"--data\", metavar=\"postData\",dest=\"data\", help=\"Additionnal data to be transmitted via POST method. Example : -d \\\"key1=value1&key2=value2\\\"\", type=valid_postData)\nparser.add_argument(\"--proxy\", metavar=\"proxyUrl\", dest=\"proxy\", help=\"Proxy information. Example : --proxy \\\"user:password@proxy.host:8080\\\"\", type=valid_proxyString)\nparser.add_argument(\"--proxy-creds\",metavar=\"credentials\",nargs='?',const=True,dest=\"proxyCreds\",help=\"Prompt for proxy credentials at runtime. Format : 'user:pass'\",type=valid_proxyCreds)\nparser.add_argument(\"-f\",\"--filesize\",metavar=\"integer\",nargs=1,default=[\"10\"],dest=\"size\",help=\"File size to use for files to be created and uploaded (in kB).\")\nparser.add_argument(\"--cookies\",metavar=\"omnomnom\",nargs=1,dest=\"cookies\",help=\"Cookies to use with HTTP requests. Example : PHPSESSID=aef45aef45afeaef45aef45&JSESSID=AQSEJHQSQSG\",type=valid_postData)\nparser.add_argument(\"--uploads-path\",default=[None],metavar=\"path\",nargs=1,dest=\"uploadsPath\",help=\"Path on the remote server where uploads are put. Example : '/tmp/uploads/'\")\nparser.add_argument(\"-t\",\"--template\",metavar=\"templateName\",nargs=1,dest=\"template\",help=\"Malicious payload to use for code execution detection. Default is to use every known templates. For a complete list of templates, see the TEMPLATE section.\")\nparser.add_argument(\"-r\",\"--regex-override\",metavar=\"regex\",nargs=1,dest=\"regexOverride\",help=\"Specify a regular expression to detect code execution. Overrides the default code execution detection regex defined in the template in use.\",type=valid_regex)\nrequiredNamedArgs = parser.add_argument_group('Required named arguments')\nrequiredNamedArgs.add_argument(\"-u\",\"--url\", metavar=\"target\", dest=\"url\",required=True, help=\"Web page URL containing the file upload form to be tested. Example : http://test.com/index.html?action=upload\", type=valid_url)\nrequiredNamedArgs.add_argument(\"--not-regex\", metavar=\"regex\", help=\"Regex matching an upload failure\", type=valid_regex,dest=\"notRegex\")\nrequiredNamedArgs.add_argument(\"--true-regex\",metavar=\"regex\", help=\"Regex matching an upload success\", type=valid_regex, dest=\"trueRegex\")\n\nexclusiveArgs = parser.add_mutually_exclusive_group()\nexclusiveArgs.add_argument(\"-l\",\"--legit-extensions\",metavar=\"listOfExtensions\",dest=\"legitExtensions\",nargs=1,help=\"Legit extensions expected, for a normal use of the form, comma separated. Example : 'jpg,png,bmp'\")\nexclusiveArgs.add_argument(\"-n\",metavar=\"n\",nargs=1,default=[\"100\"],dest=\"n\",help=\"Number of common extensions to use. Example : -n 100\", type=valid_nArg)\n\nexclusiveVerbosityArgs = parser.add_mutually_exclusive_group()\nexclusiveVerbosityArgs.add_argument(\"-v\",action=\"store_true\",required=False,dest=\"verbose\",help=\"Verbose mode\")\nexclusiveVerbosityArgs.add_argument(\"-vv\",action=\"store_true\",required=False,dest=\"veryVerbose\",help=\"Very verbose mode\")\nexclusiveVerbosityArgs.add_argument(\"-vvv\",action=\"store_true\",required=False,dest=\"veryVeryVerbose\",help=\"Much verbose, very log, wow.\")\n\nparser.add_argument(\"-s\",\"--skip-recon\",action=\"store_true\",required=False,dest=\"skipRecon\",help=\"Skip recon phase, where fuxploider tries to determine what extensions are expected and filtered by the server. Needs -l switch.\")\nparser.add_argument(\"-y\",action=\"store_true\",required=False,dest=\"detectAllEntryPoints\",help=\"Force detection of every entry points. Will not stop at first code exec found.\")\nparser.add_argument(\"-T\",\"--threads\",metavar=\"Threads\",nargs=1,dest=\"nbThreads\",help=\"Number of parallel tasks (threads).\",type=int,default=[4])\n\nexclusiveUserAgentsArgs = parser.add_mutually_exclusive_group()\nexclusiveUserAgentsArgs.add_argument(\"-U\",\"--user-agent\",metavar=\"useragent\",nargs=1,dest=\"userAgent\",help=\"User-agent to use while requesting the target.\",type=str,default=[requests.utils.default_user_agent()])\nexclusiveUserAgentsArgs.add_argument(\"--random-user-agent\",action=\"store_true\",required=False,dest=\"randomUserAgent\",help=\"Use a random user-agent while requesting the target.\")\n\nmanualFormArgs = parser.add_argument_group('Manual Form Detection arguments')\nmanualFormArgs.add_argument(\"-m\",\"--manual-form-detection\",action=\"store_true\",dest=\"manualFormDetection\",help=\"Disable automatic form detection. Useful when automatic detection fails due to: (1) Form loaded using Javascript (2) Multiple file upload forms in URL.\")\nmanualFormArgs.add_argument(\"--input-name\",metavar=\"image\",dest=\"inputName\",help=\"Name of input for file. Example: <input type=\\\"file\\\" name=\\\"image\\\">\")\nmanualFormArgs.add_argument(\"--form-action\",default=\"\",metavar=\"upload.php\",dest=\"formAction\",help=\"Path of form action. Example: <form method=\\\"POST\\\" action=\\\"upload.php\\\">\")\n\nargs = parser.parse_args()\nargs.uploadsPath = args.uploadsPath[0]\nargs.nbThreads = args.nbThreads[0]\nargs.userAgent = args.userAgent[0]\n\nif args.randomUserAgent :\n\twith open(\"user-agents.txt\",\"r\") as fd :\n\t\tnb = 0\n\t\tfor l in fd :\n\t\t\tnb += 1\n\t\tfd.seek(0)\n\t\tnb = random.randint(0,nb)\n\t\tfor i in range(0,nb) :\n\t\t\targs.userAgent = fd.readline()[:-1]\n\nif args.template :\n\targs.template = args.template[0]\n\tif args.template not in templatesNames :\n\t\tlogging.warning(\"Unknown template : %s\",args.template)\n\t\tcont = input(\"Use default templates instead ? [Y/n]\")\n\t\tif not cont.lower().startswith(\"y\") :\n\t\t\texit()\n\telse :\n\t\ttemplates = [[x for x in templates if x[\"templateName\"] == args.template][0]]\nif args.regexOverride :\n\tfor t in templates :\n\t\tt[\"codeExecRegex\"] = args.regexOverride[0]\n\nargs.verbosity = 0\nif args.verbose :\n\targs.verbosity = 1\nif args.veryVerbose :\n\targs.verbosity = 2\nif args.veryVeryVerbose :\n\targs.verbosity = 3\nlogger.verbosity = args.verbosity\nif args.verbosity > 0 :\n\tcoloredlogs.install(logger=logger,fmt='%(asctime)s %(levelname)s - %(message)s',level=logging.DEBUG)\n\n\nif args.proxyCreds and args.proxy == None :\n\tparser.error(\"--proxy-creds must be used with --proxy.\")\n\nif args.skipRecon and args.legitExtensions == None :\n\tparser.error(\"-s switch needs -l switch. Cannot skip recon phase without any known entry point.\")\n\nargs.n = int(args.n[0])\nargs.size = int(args.size[0])\nargs.size = 1024*args.size\n\nif not args.notRegex and not args.trueRegex :\n\tparser.error(\"At least one detection method must be provided, either with --not-regex or with --true-regex.\")\n\nif args.legitExtensions :\n\targs.legitExtensions = args.legitExtensions[0].split(\",\")\n\nif args.cookies :\n\targs.cookies = postDataFromStringToJSON(args.cookies[0])\n\nif args.manualFormDetection and args.inputName is None:\n\tparser.error(\"--manual-form-detection requires --input-name\")\n\nprint(\"\"\"\\033[1;32m\n                                     \n ___             _     _   _         \n|  _|_ _ _ _ ___| |___|_|_| |___ ___ \n|  _| | |_'_| . | | . | | . | -_|  _|\n|_| |___|_,_|  _|_|___|_|___|___|_|  \n            |_|                      \n\n\\033[1m\\033[42m{version \"\"\"+version+\"\"\"}\\033[m\n\n\\033[m[!] legal disclaimer : Usage of fuxploider for attacking targets without prior mutual consent is illegal. It is the end user's responsibility to obey all applicable local, state and federal laws. Developers assume no liability and are not responsible for any misuse or damage caused by this program\n\t\"\"\")\nif args.proxyCreds == True :\n\targs.proxyCreds = {}\n\targs.proxyCreds[\"username\"] = input(\"Proxy username : \")\n\targs.proxyCreds[\"password\"] = getpass.getpass(\"Proxy password : \")\n\nnow = datetime.datetime.now()\n\nprint(\"[*] starting at \"+str(now.hour)+\":\"+str(now.minute)+\":\"+str(now.second))\n\n#mimeFile = \"mimeTypes.advanced\"\nmimeFile = \"mimeTypes.basic\"\nextensions = loadExtensions(\"file\",mimeFile)\ntmpLegitExt = []\nif args.legitExtensions :\n\targs.legitExtensions = [x.lower() for x in args.legitExtensions]\n\tfoundExt = [a[0] for a in extensions]\n\tfor b in args.legitExtensions :\n\t\tif b in foundExt :\n\t\t\ttmpLegitExt.append(b)\n\t\telse :\n\t\t\tlogging.warning(\"Extension %s can't be found as a valid/known extension with associated mime type.\",b)\nargs.legitExtensions = tmpLegitExt\n\npostData = postDataFromStringToJSON(args.data)\n\ns = requests.Session()\nif args.cookies :\n\tfor key in args.cookies.keys() :\n\t\ts.cookies[key] = args.cookies[key]\ns.headers = {'User-Agent':args.userAgent}\n##### PROXY HANDLING #####\ns.trust_env = False\nif args.proxy :\n\tif args.proxy[\"username\"] and args.proxy[\"password\"] and args.proxyCreds :\n\t\tlogging.warning(\"Proxy username and password provided by the --proxy-creds switch replaces credentials provided using the --proxy switch\")\n\tif args.proxyCreds :\n\t\tproxyUser = args.proxyCreds[\"username\"]\n\t\tproxyPass = args.proxyCreds[\"password\"]\n\telse :\n\t\tproxyUser = args.proxy[\"username\"]\n\t\tproxyPass = args.proxy[\"password\"]\n\tproxyProtocol = args.proxy[\"protocol\"]\n\tproxyHostname = args.proxy[\"hostname\"]\n\tproxyPort = args.proxy[\"port\"]\n\tproxy = \"\"\n\tif proxyProtocol != None :\n\t\tproxy += proxyProtocol+\"://\"\n\telse :\n\t\tproxy += \"http://\"\n\n\tif proxyUser != None and proxyPass != None :\n\t\tproxy += proxyUser+\":\"+proxyPass+\"@\"\n\n\tproxy += proxyHostname\n\tif proxyPort != None :\n\t\tproxy += \":\"+proxyPort\n\n\tif proxyProtocol == \"https\" :\n\t\tproxies = {\"https\":proxy}\n\telse :\n\t\tproxies = {\"http\":proxy,\"https\":proxy}\n\n\ts.proxies.update(proxies)\n#########################################################\n\nif args.manualFormDetection:\n\tif args.formAction == \"\":\n\t\tlogger.warning(\"Using Manual Form Detection and no action specified with --form-action. Defaulting to empty string - meaning form action will be set to --url parameter.\")\n\tup = UploadForm(args.notRegex,args.trueRegex,s,args.size,postData,args.uploadsPath,args.url,args.formAction,args.inputName)\nelse:\n\tup = UploadForm(args.notRegex,args.trueRegex,s,args.size,postData,args.uploadsPath)\n\tup.setup(args.url)\nup.threads = args.nbThreads\n#########################################################\n\n############################################################\nuploadURL = up.uploadUrl\nfileInput = {\"name\":up.inputName}\n\n###### VALID EXTENSIONS DETECTION FOR THIS FORM ######\n\na = datetime.datetime.now()\n\nif not args.skipRecon :\n\tif len(args.legitExtensions) > 0 :\n\t\tn = up.detectValidExtensions(extensions,args.n,args.legitExtensions)\n\telse :\n\t\tn = up.detectValidExtensions(extensions,args.n)\n\tlogger.info(\"### Tried %s extensions, %s are valid.\",n,len(up.validExtensions))\nelse :\n\tlogger.info(\"### Skipping detection of valid extensions, using provided extensions instead (%s)\",args.legitExtensions)\n\tup.validExtensions = args.legitExtensions\n\nif up.validExtensions == [] :\n\tlogger.error(\"No valid extension found.\")\n\texit()\n\nb = datetime.datetime.now()\nprint(\"Extensions detection : \"+str(b-a))\n\n\n##############################################################################################################################################\n##############################################################################################################################################\ncont = input(\"Start uploading payloads ? [Y/n] : \")\nup.shouldLog = True\nif cont.lower().startswith(\"y\") or cont == \"\" :\n\tpass\nelse :\n\texit(\"Exiting.\")\n\nentryPoints = []\nup.stopThreads = True\n\nwith open(\"techniques.json\",\"r\") as rawTechniques :\n\ttechniques = json.loads(rawTechniques.read())\nlogger.info(\"### Starting code execution detection (messing with file extensions and mime types...)\")\nc = datetime.datetime.now()\nnbOfEntryPointsFound = 0\nattempts = []\ntemplatesData = {}\n\nfor template in templates :\n\ttemplatefd = open(templatesFolder+\"/\"+template[\"filename\"],\"rb\")\n\ttemplatesData[template[\"templateName\"]] = templatefd.read()\n\ttemplatefd.close()\n\tnastyExt = template[\"nastyExt\"]\n\tnastyMime = getMime(extensions,nastyExt)\n\tnastyExtVariants = template[\"extVariants\"]\n\tfor t in techniques :\n\t\tfor nastyVariant in [nastyExt]+nastyExtVariants :\n\t\t\tfor legitExt in up.validExtensions :\n\t\t\t\tlegitMime = getMime(extensions,legitExt)\n\t\t\t\tmime = legitMime if t[\"mime\"] == \"legit\" else nastyMime\n\t\t\t\tsuffix = t[\"suffix\"].replace(\"$legitExt$\",legitExt).replace(\"$nastyExt$\",nastyVariant)\n\t\t\t\tattempts.append({\"suffix\":suffix,\"mime\":mime,\"templateName\":template[\"templateName\"]})\n\n\nstopThreads = False\n\nattemptsTested = 0\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=args.nbThreads) as executor :\n\tfutures = []\n\ttry :\n\t\tfor a in attempts :\n\t\t\tsuffix = a[\"suffix\"]\n\t\t\tmime = a[\"mime\"]\n\t\t\tpayload = templatesData[a[\"templateName\"]]\n\t\t\tcodeExecRegex = [t[\"codeExecRegex\"] for t in templates if t[\"templateName\"] == a[\"templateName\"]][0]\n\n\t\t\tf = executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex)\n\t\t\tf.a = a\n\t\t\tfutures.append(f)\n\n\t\tfor future in concurrent.futures.as_completed(futures) :\n\t\t\tres = future.result()\n\t\t\tattemptsTested += 1\n\t\t\tif not stopThreads :\n\t\t\t\tif res[\"codeExec\"] :\n\n\t\t\t\t\tfoundEntryPoint = future.a\n\t\t\t\t\tlogging.info(\"\\033[1m\\033[42mCode execution obtained ('%s','%s','%s')\\033[m\",foundEntryPoint[\"suffix\"],foundEntryPoint[\"mime\"],foundEntryPoint[\"templateName\"])\n\t\t\t\t\tnbOfEntryPointsFound += 1\n\t\t\t\t\tentryPoints.append(foundEntryPoint)\n\n\t\t\t\t\tif not args.detectAllEntryPoints :\n\t\t\t\t\t\traise KeyboardInterrupt\n\n\texcept KeyboardInterrupt :\n\t\tstopThreads = True\n\t\texecutor.shutdown(wait=False)\n\t\texecutor._threads.clear()\n\t\tconcurrent.futures.thread._threads_queues.clear()\n\t\tlogger.setLevel(logging.CRITICAL)\n\t\tlogger.verbosity = -1\n\n\n################################################################################################################################################\n################################################################################################################################################\nd = datetime.datetime.now()\n#print(\"Code exec detection : \"+str(d-c))\nprint()\nlogging.info(\"%s entry point(s) found using %s HTTP requests.\",nbOfEntryPointsFound,up.httpRequests)\nprint(\"Found the following entry points : \")\nprint(entryPoints)",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/chrisbarnettster/galaxy_interactive_testing/blob/f8e904a81d28aaedb06faa594ec32da5efdada99",
        "file_path": "/lib/galaxy/jobs/runners/lwr.py",
        "source": "import logging\n\nfrom galaxy import model\nfrom galaxy.jobs.runners import AsynchronousJobState, AsynchronousJobRunner\nfrom galaxy.jobs import ComputeEnvironment\nfrom galaxy.jobs import JobDestination\nfrom galaxy.jobs.command_factory import build_command\nfrom galaxy.tools.deps import dependencies\nfrom galaxy.util import string_as_bool_or_none\nfrom galaxy.util.bunch import Bunch\n\nimport errno\nfrom time import sleep\nimport os\n\nfrom .lwr_client import build_client_manager\nfrom .lwr_client import url_to_destination_params\nfrom .lwr_client import finish_job as lwr_finish_job\nfrom .lwr_client import submit_job as lwr_submit_job\nfrom .lwr_client import ClientJobDescription\nfrom .lwr_client import LwrOutputs\nfrom .lwr_client import ClientOutputs\nfrom .lwr_client import PathMapper\n\nlog = logging.getLogger( __name__ )\n\n__all__ = [ 'LwrJobRunner' ]\n\nNO_REMOTE_GALAXY_FOR_METADATA_MESSAGE = \"LWR misconfiguration - LWR client configured to set metadata remotely, but remote LWR isn't properly configured with a galaxy_home directory.\"\nNO_REMOTE_DATATYPES_CONFIG = \"LWR client is configured to use remote datatypes configuration when setting metadata externally, but LWR is not configured with this information. Defaulting to datatypes_conf.xml.\"\n\n# Is there a good way to infer some default for this? Can only use\n# url_for from web threads. https://gist.github.com/jmchilton/9098762\nDEFAULT_GALAXY_URL = \"http://localhost:8080\"\n\n\nclass LwrJobRunner( AsynchronousJobRunner ):\n    \"\"\"\n    LWR Job Runner\n    \"\"\"\n    runner_name = \"LWRRunner\"\n\n    def __init__( self, app, nworkers, transport=None, cache=None, url=None, galaxy_url=DEFAULT_GALAXY_URL ):\n        \"\"\"Start the job runner \"\"\"\n        super( LwrJobRunner, self ).__init__( app, nworkers )\n        self.async_status_updates = dict()\n        self._init_monitor_thread()\n        self._init_worker_threads()\n        client_manager_kwargs = {'transport_type': transport, 'cache': string_as_bool_or_none(cache), \"url\": url}\n        self.galaxy_url = galaxy_url\n        self.client_manager = build_client_manager(**client_manager_kwargs)\n\n    def url_to_destination( self, url ):\n        \"\"\"Convert a legacy URL to a job destination\"\"\"\n        return JobDestination( runner=\"lwr\", params=url_to_destination_params( url ) )\n\n    def check_watched_item(self, job_state):\n        try:\n            client = self.get_client_from_state(job_state)\n\n            if hasattr(self.client_manager, 'ensure_has_status_update_callback'):\n                # Message queue implementation.\n\n                # TODO: Very hacky now, refactor after Dannon merges in his\n                # message queue work, runners need the ability to disable\n                # check_watched_item like this and instead a callback needs to\n                # be issued post job recovery allowing a message queue\n                # consumer to be setup.\n                self.client_manager.ensure_has_status_update_callback(self.__async_update)\n                return job_state\n\n            status = client.get_status()\n        except Exception:\n            # An orphaned job was put into the queue at app startup, so remote server went down\n            # either way we are done I guess.\n            self.mark_as_finished(job_state)\n            return None\n        job_state = self.__update_job_state_for_lwr_status(job_state, status)\n        return job_state\n\n    def __update_job_state_for_lwr_status(self, job_state, lwr_status):\n        if lwr_status == \"complete\":\n            self.mark_as_finished(job_state)\n            return None\n        if lwr_status == \"running\" and not job_state.running:\n            job_state.running = True\n            job_state.job_wrapper.change_state( model.Job.states.RUNNING )\n        return job_state\n\n    def __async_update( self, full_status ):\n        job_id = full_status[ \"job_id\" ]\n        job_state = self.__find_watched_job( job_id )\n        if not job_state:\n            # Probably finished too quickly, sleep and try again.\n            # Kind of a hack, why does monitor queue need to no wait\n            # get and sleep instead of doing a busy wait that would\n            # respond immediately.\n            sleep( 2 )\n            job_state = self.__find_watched_job( job_id )\n        if not job_state:\n            log.warn( \"Failed to find job corresponding to final status %s in %s\" % ( full_status, self.watched ) )\n        else:\n            self.__update_job_state_for_lwr_status(job_state, full_status[\"status\"])\n\n    def __find_watched_job( self, job_id ):\n        found_job = None\n        for async_job_state in self.watched:\n            if str( async_job_state.job_id ) == job_id:\n                found_job = async_job_state\n                break\n        return found_job\n\n    def queue_job(self, job_wrapper):\n        job_destination = job_wrapper.job_destination\n\n        command_line, client, remote_job_config, compute_environment = self.__prepare_job( job_wrapper, job_destination )\n\n        if not command_line:\n            return\n\n        try:\n            dependencies_description = LwrJobRunner.__dependencies_description( client, job_wrapper )\n            rewrite_paths = not LwrJobRunner.__rewrite_parameters( client )\n            unstructured_path_rewrites = {}\n            if compute_environment:\n                unstructured_path_rewrites = compute_environment.unstructured_path_rewrites\n\n            client_job_description = ClientJobDescription(\n                command_line=command_line,\n                input_files=self.get_input_files(job_wrapper),\n                client_outputs=self.__client_outputs(client, job_wrapper),\n                working_directory=job_wrapper.working_directory,\n                tool=job_wrapper.tool,\n                config_files=job_wrapper.extra_filenames,\n                dependencies_description=dependencies_description,\n                env=client.env,\n                rewrite_paths=rewrite_paths,\n                arbitrary_files=unstructured_path_rewrites,\n            )\n            job_id = lwr_submit_job(client, client_job_description, remote_job_config)\n            log.info(\"lwr job submitted with job_id %s\" % job_id)\n            job_wrapper.set_job_destination( job_destination, job_id )\n            job_wrapper.change_state( model.Job.states.QUEUED )\n        except Exception:\n            job_wrapper.fail( \"failure running job\", exception=True )\n            log.exception(\"failure running job %d\" % job_wrapper.job_id)\n            return\n\n        lwr_job_state = AsynchronousJobState()\n        lwr_job_state.job_wrapper = job_wrapper\n        lwr_job_state.job_id = job_id\n        lwr_job_state.old_state = True\n        lwr_job_state.running = False\n        lwr_job_state.job_destination = job_destination\n        self.monitor_job(lwr_job_state)\n\n    def __prepare_job(self, job_wrapper, job_destination):\n        \"\"\" Build command-line and LWR client for this job. \"\"\"\n        command_line = None\n        client = None\n        remote_job_config = None\n        compute_environment = None\n        try:\n            client = self.get_client_from_wrapper(job_wrapper)\n            tool = job_wrapper.tool\n            remote_job_config = client.setup(tool.id, tool.version)\n            rewrite_parameters = LwrJobRunner.__rewrite_parameters( client )\n            prepare_kwds = {}\n            if rewrite_parameters:\n                compute_environment = LwrComputeEnvironment( client, job_wrapper, remote_job_config )\n                prepare_kwds[ 'compute_environment' ] = compute_environment\n            job_wrapper.prepare( **prepare_kwds )\n            self.__prepare_input_files_locally(job_wrapper)\n            remote_metadata = LwrJobRunner.__remote_metadata( client )\n            remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )\n            dependency_resolution = LwrJobRunner.__dependency_resolution( client )\n            metadata_kwds = self.__build_metadata_configuration(client, job_wrapper, remote_metadata, remote_job_config)\n            remote_command_params = dict(\n                working_directory=remote_job_config['working_directory'],\n                metadata_kwds=metadata_kwds,\n                dependency_resolution=dependency_resolution,\n            )\n            command_line = build_command(\n                self,\n                job_wrapper=job_wrapper,\n                include_metadata=remote_metadata,\n                include_work_dir_outputs=remote_work_dir_copy,\n                remote_command_params=remote_command_params,\n            )\n        except Exception:\n            job_wrapper.fail( \"failure preparing job\", exception=True )\n            log.exception(\"failure running job %d\" % job_wrapper.job_id)\n\n        # If we were able to get a command line, run the job\n        if not command_line:\n            job_wrapper.finish( '', '' )\n\n        return command_line, client, remote_job_config, compute_environment\n\n    def __prepare_input_files_locally(self, job_wrapper):\n        \"\"\"Run task splitting commands locally.\"\"\"\n        prepare_input_files_cmds = getattr(job_wrapper, 'prepare_input_files_cmds', None)\n        if prepare_input_files_cmds is not None:\n            for cmd in prepare_input_files_cmds:  # run the commands to stage the input files\n                if 0 != os.system(cmd):\n                    raise Exception('Error running file staging command: %s' % cmd)\n            job_wrapper.prepare_input_files_cmds = None  # prevent them from being used in-line\n\n    def get_output_files(self, job_wrapper):\n        output_paths = job_wrapper.get_output_fnames()\n        return [ str( o ) for o in output_paths ]   # Force job_path from DatasetPath objects.\n\n    def get_input_files(self, job_wrapper):\n        input_paths = job_wrapper.get_input_paths()\n        return [ str( i ) for i in input_paths ]  # Force job_path from DatasetPath objects.\n\n    def get_client_from_wrapper(self, job_wrapper):\n        job_id = job_wrapper.job_id\n        if hasattr(job_wrapper, 'task_id'):\n            job_id = \"%s_%s\" % (job_id, job_wrapper.task_id)\n        params = job_wrapper.job_destination.params.copy()\n        for key, value in params.iteritems():\n            if value:\n                params[key] = model.User.expand_user_properties( job_wrapper.get_job().user, value )\n        env = getattr( job_wrapper.job_destination, \"env\", [] )\n        return self.get_client( params, job_id, env )\n\n    def get_client_from_state(self, job_state):\n        job_destination_params = job_state.job_destination.params\n        job_id = job_state.job_id\n        return self.get_client( job_destination_params, job_id )\n\n    def get_client( self, job_destination_params, job_id, env=[] ):\n        # Cannot use url_for outside of web thread.\n        #files_endpoint = url_for( controller=\"job_files\", job_id=encoded_job_id )\n\n        encoded_job_id = self.app.security.encode_id(job_id)\n        job_key = self.app.security.encode_id( job_id, kind=\"jobs_files\" )\n        files_endpoint = \"%s/api/jobs/%s/files?job_key=%s\" % (\n            self.galaxy_url,\n            encoded_job_id,\n            job_key\n        )\n        get_client_kwds = dict(\n            job_id=str( job_id ),\n            files_endpoint=files_endpoint,\n            env=env\n        )\n        return self.client_manager.get_client( job_destination_params, **get_client_kwds )\n\n    def finish_job( self, job_state ):\n        stderr = stdout = ''\n        job_wrapper = job_state.job_wrapper\n        try:\n            client = self.get_client_from_state(job_state)\n            run_results = client.full_status()\n\n            stdout = run_results.get('stdout', '')\n            stderr = run_results.get('stderr', '')\n            exit_code = run_results.get('returncode', None)\n            lwr_outputs = LwrOutputs.from_status_response(run_results)\n            # Use LWR client code to transfer/copy files back\n            # and cleanup job if needed.\n            completed_normally = \\\n                job_wrapper.get_state() not in [ model.Job.states.ERROR, model.Job.states.DELETED ]\n            cleanup_job = self.app.config.cleanup_job\n            client_outputs = self.__client_outputs(client, job_wrapper)\n            finish_args = dict( client=client,\n                                job_completed_normally=completed_normally,\n                                cleanup_job=cleanup_job,\n                                client_outputs=client_outputs,\n                                lwr_outputs=lwr_outputs )\n            failed = lwr_finish_job( **finish_args )\n\n            if failed:\n                job_wrapper.fail(\"Failed to find or download one or more job outputs from remote server.\", exception=True)\n        except Exception:\n            message = \"Failed to communicate with remote job server.\"\n            job_wrapper.fail( message, exception=True )\n            log.exception(\"failure finishing job %d\" % job_wrapper.job_id)\n            return\n        if not LwrJobRunner.__remote_metadata( client ):\n            self._handle_metadata_externally( job_wrapper, resolve_requirements=True )\n        # Finish the job\n        try:\n            job_wrapper.finish( stdout, stderr, exit_code )\n        except Exception:\n            log.exception(\"Job wrapper finish method failed\")\n            job_wrapper.fail(\"Unable to finish job\", exception=True)\n\n    def fail_job( self, job_state ):\n        \"\"\"\n        Seperated out so we can use the worker threads for it.\n        \"\"\"\n        self.stop_job( self.sa_session.query( self.app.model.Job ).get( job_state.job_wrapper.job_id ) )\n        job_state.job_wrapper.fail( job_state.fail_message )\n\n    def check_pid( self, pid ):\n        try:\n            os.kill( pid, 0 )\n            return True\n        except OSError, e:\n            if e.errno == errno.ESRCH:\n                log.debug( \"check_pid(): PID %d is dead\" % pid )\n            else:\n                log.warning( \"check_pid(): Got errno %s when attempting to check PID %d: %s\" % ( errno.errorcode[e.errno], pid, e.strerror ) )\n            return False\n\n    def stop_job( self, job ):\n        #if our local job has JobExternalOutputMetadata associated, then our primary job has to have already finished\n        job_ext_output_metadata = job.get_external_output_metadata()\n        if job_ext_output_metadata:\n            pid = job_ext_output_metadata[0].job_runner_external_pid  # every JobExternalOutputMetadata has a pid set, we just need to take from one of them\n            if pid in [ None, '' ]:\n                log.warning( \"stop_job(): %s: no PID in database for job, unable to stop\" % job.id )\n                return\n            pid = int( pid )\n            if not self.check_pid( pid ):\n                log.warning( \"stop_job(): %s: PID %d was already dead or can't be signaled\" % ( job.id, pid ) )\n                return\n            for sig in [ 15, 9 ]:\n                try:\n                    os.killpg( pid, sig )\n                except OSError, e:\n                    log.warning( \"stop_job(): %s: Got errno %s when attempting to signal %d to PID %d: %s\" % ( job.id, errno.errorcode[e.errno], sig, pid, e.strerror ) )\n                    return  # give up\n                sleep( 2 )\n                if not self.check_pid( pid ):\n                    log.debug( \"stop_job(): %s: PID %d successfully killed with signal %d\" % ( job.id, pid, sig ) )\n                    return\n                else:\n                    log.warning( \"stop_job(): %s: PID %d refuses to die after signaling TERM/KILL\" % ( job.id, pid ) )\n        else:\n            # Remote kill\n            lwr_url = job.job_runner_name\n            job_id = job.job_runner_external_id\n            log.debug(\"Attempt remote lwr kill of job with url %s and id %s\" % (lwr_url, job_id))\n            client = self.get_client(job.destination_params, job_id)\n            client.kill()\n\n    def recover( self, job, job_wrapper ):\n        \"\"\"Recovers jobs stuck in the queued/running state when Galaxy started\"\"\"\n        job_state = AsynchronousJobState()\n        job_state.job_id = str( job.get_job_runner_external_id() )\n        job_state.runner_url = job_wrapper.get_job_runner_url()\n        job_state.job_destination = job_wrapper.job_destination\n        job_wrapper.command_line = job.get_command_line()\n        job_state.job_wrapper = job_wrapper\n        state = job.get_state()\n        if state in [model.Job.states.RUNNING, model.Job.states.QUEUED]:\n            log.debug( \"(LWR/%s) is still in running state, adding to the LWR queue\" % ( job.get_id()) )\n            job_state.old_state = True\n            job_state.running = state == model.Job.states.RUNNING\n            self.monitor_queue.put( job_state )\n\n    def shutdown( self ):\n        super( LwrJobRunner, self ).shutdown()\n        self.client_manager.shutdown()\n\n    def __client_outputs( self, client, job_wrapper ):\n        remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )\n        if not remote_work_dir_copy:\n            work_dir_outputs = self.get_work_dir_outputs( job_wrapper )\n        else:\n            # They have already been copied over to look like regular outputs remotely,\n            # no need to handle them differently here.\n            work_dir_outputs = []\n        output_files = self.get_output_files( job_wrapper )\n        client_outputs = ClientOutputs(\n            working_directory=job_wrapper.working_directory,\n            work_dir_outputs=work_dir_outputs,\n            output_files=output_files,\n            version_file=job_wrapper.get_version_string_path(),\n        )\n        return client_outputs\n\n    @staticmethod\n    def __dependencies_description( lwr_client, job_wrapper ):\n        dependency_resolution = LwrJobRunner.__dependency_resolution( lwr_client )\n        remote_dependency_resolution = dependency_resolution == \"remote\"\n        if not remote_dependency_resolution:\n            return None\n        requirements = job_wrapper.tool.requirements or []\n        installed_tool_dependencies = job_wrapper.tool.installed_tool_dependencies or []\n        return dependencies.DependenciesDescription(\n            requirements=requirements,\n            installed_tool_dependencies=installed_tool_dependencies,\n        )\n\n    @staticmethod\n    def __dependency_resolution( lwr_client ):\n        dependency_resolution = lwr_client.destination_params.get( \"dependency_resolution\", \"local\" )\n        if dependency_resolution not in [\"none\", \"local\", \"remote\"]:\n            raise Exception(\"Unknown dependency_resolution value encountered %s\" % dependency_resolution)\n        return dependency_resolution\n\n    @staticmethod\n    def __remote_metadata( lwr_client ):\n        remote_metadata = string_as_bool_or_none( lwr_client.destination_params.get( \"remote_metadata\", False ) )\n        return remote_metadata\n\n    @staticmethod\n    def __remote_work_dir_copy( lwr_client ):\n        # Right now remote metadata handling assumes from_work_dir outputs\n        # have been copied over before it runs. So do that remotely. This is\n        # not the default though because adding it to the command line is not\n        # cross-platform (no cp on Windows) and it's un-needed work outside\n        # the context of metadata settting (just as easy to download from\n        # either place.)\n        return LwrJobRunner.__remote_metadata( lwr_client )\n\n    @staticmethod\n    def __use_remote_datatypes_conf( lwr_client ):\n        \"\"\" When setting remote metadata, use integrated datatypes from this\n        Galaxy instance or use the datatypes config configured via the remote\n        LWR.\n\n        Both options are broken in different ways for same reason - datatypes\n        may not match. One can push the local datatypes config to the remote\n        server - but there is no guarentee these datatypes will be defined\n        there. Alternatively, one can use the remote datatype config - but\n        there is no guarentee that it will contain all the datatypes available\n        to this Galaxy.\n        \"\"\"\n        use_remote_datatypes = string_as_bool_or_none( lwr_client.destination_params.get( \"use_remote_datatypes\", False ) )\n        return use_remote_datatypes\n\n    @staticmethod\n    def __rewrite_parameters( lwr_client ):\n        return string_as_bool_or_none( lwr_client.destination_params.get( \"rewrite_parameters\", False ) ) or False\n\n    def __build_metadata_configuration(self, client, job_wrapper, remote_metadata, remote_job_config):\n        metadata_kwds = {}\n        if remote_metadata:\n            remote_system_properties = remote_job_config.get(\"system_properties\", {})\n            remote_galaxy_home = remote_system_properties.get(\"galaxy_home\", None)\n            if not remote_galaxy_home:\n                raise Exception(NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE)\n            metadata_kwds['exec_dir'] = remote_galaxy_home\n            outputs_directory = remote_job_config['outputs_directory']\n            configs_directory = remote_job_config['configs_directory']\n            working_directory = remote_job_config['working_directory']\n            outputs = [Bunch(false_path=os.path.join(outputs_directory, os.path.basename(path)), real_path=path) for path in self.get_output_files(job_wrapper)]\n            metadata_kwds['output_fnames'] = outputs\n            metadata_kwds['compute_tmp_dir'] = working_directory\n            metadata_kwds['config_root'] = remote_galaxy_home\n            default_config_file = os.path.join(remote_galaxy_home, 'universe_wsgi.ini')\n            metadata_kwds['config_file'] = remote_system_properties.get('galaxy_config_file', default_config_file)\n            metadata_kwds['dataset_files_path'] = remote_system_properties.get('galaxy_dataset_files_path', None)\n            if LwrJobRunner.__use_remote_datatypes_conf( client ):\n                remote_datatypes_config = remote_system_properties.get('galaxy_datatypes_config_file', None)\n                if not remote_datatypes_config:\n                    log.warn(NO_REMOTE_DATATYPES_CONFIG)\n                    remote_datatypes_config = os.path.join(remote_galaxy_home, 'datatypes_conf.xml')\n                metadata_kwds['datatypes_config'] = remote_datatypes_config\n            else:\n                integrates_datatypes_config = self.app.datatypes_registry.integrated_datatypes_configs\n                # Ensure this file gets pushed out to the remote config dir.\n                job_wrapper.extra_filenames.append(integrates_datatypes_config)\n\n                metadata_kwds['datatypes_config'] = os.path.join(configs_directory, os.path.basename(integrates_datatypes_config))\n        return metadata_kwds\n\n\nclass LwrComputeEnvironment( ComputeEnvironment ):\n\n    def __init__( self, lwr_client, job_wrapper, remote_job_config ):\n        self.lwr_client = lwr_client\n        self.job_wrapper = job_wrapper\n        self.local_path_config = job_wrapper.default_compute_environment()\n        self.unstructured_path_rewrites = {}\n        # job_wrapper.prepare is going to expunge the job backing the following\n        # computations, so precalculate these paths.\n        self._wrapper_input_paths = self.local_path_config.input_paths()\n        self._wrapper_output_paths = self.local_path_config.output_paths()\n        self.path_mapper = PathMapper(lwr_client, remote_job_config, self.local_path_config.working_directory())\n        self._config_directory = remote_job_config[ \"configs_directory\" ]\n        self._working_directory = remote_job_config[ \"working_directory\" ]\n        self._sep = remote_job_config[ \"system_properties\" ][ \"separator\" ]\n        self._tool_dir = remote_job_config[ \"tools_directory\" ]\n        version_path = self.local_path_config.version_path()\n        new_version_path = self.path_mapper.remote_version_path_rewrite(version_path)\n        if new_version_path:\n            version_path = new_version_path\n        self._version_path = version_path\n\n    def output_paths( self ):\n        local_output_paths = self._wrapper_output_paths\n\n        results = []\n        for local_output_path in local_output_paths:\n            wrapper_path = str( local_output_path )\n            remote_path = self.path_mapper.remote_output_path_rewrite( wrapper_path )\n            results.append( self._dataset_path( local_output_path, remote_path ) )\n        return results\n\n    def input_paths( self ):\n        local_input_paths = self._wrapper_input_paths\n\n        results = []\n        for local_input_path in local_input_paths:\n            wrapper_path = str( local_input_path )\n            # This will over-copy in some cases. For instance in the case of task\n            # splitting, this input will be copied even though only the work dir\n            # input will actually be used.\n            remote_path = self.path_mapper.remote_input_path_rewrite( wrapper_path )\n            results.append( self._dataset_path( local_input_path, remote_path ) )\n        return results\n\n    def _dataset_path( self, local_dataset_path, remote_path ):\n        remote_extra_files_path = None\n        if remote_path:\n            remote_extra_files_path = \"%s_files\" % remote_path[ 0:-len( \".dat\" ) ]\n        return local_dataset_path.with_path_for_job( remote_path, remote_extra_files_path )\n\n    def working_directory( self ):\n        return self._working_directory\n\n    def config_directory( self ):\n        return self._config_directory\n\n    def new_file_path( self ):\n        return self.working_directory()  # Problems with doing this?\n\n    def sep( self ):\n        return self._sep\n\n    def version_path( self ):\n        return self._version_path\n\n    def rewriter( self, parameter_value ):\n        unstructured_path_rewrites = self.unstructured_path_rewrites\n        if parameter_value in unstructured_path_rewrites:\n            # Path previously mapped, use previous mapping.\n            return unstructured_path_rewrites[ parameter_value ]\n        if parameter_value in unstructured_path_rewrites.itervalues():\n            # Path is a rewritten remote path (this might never occur,\n            # consider dropping check...)\n            return parameter_value\n\n        rewrite, new_unstructured_path_rewrites = self.path_mapper.check_for_arbitrary_rewrite( parameter_value )\n        if rewrite:\n            unstructured_path_rewrites.update(new_unstructured_path_rewrites)\n            return rewrite\n        else:\n            # Did need to rewrite, use original path or value.\n            return parameter_value\n\n    def unstructured_path_rewriter( self ):\n        return self.rewriter\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/fakeNetflix/twitter-repo-pants/blob/71467604c5d43df6000995346f245f3768441c03",
        "file_path": "/src/python/pants/backend/graph_info/tasks/cloc.py",
        "source": "# coding=utf-8\n# Copyright 2015 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport os\nfrom builtins import open\n\nfrom future.utils import text_type\n\nfrom pants.backend.graph_info.subsystems.cloc_binary import ClocBinary\nfrom pants.base.workunit import WorkUnitLabel\nfrom pants.engine.fs import FilesContent, PathGlobs, PathGlobsAndRoot\nfrom pants.engine.isolated_process import ExecuteProcessRequest\nfrom pants.task.console_task import ConsoleTask\nfrom pants.util.contextutil import temporary_dir\n\n\nclass CountLinesOfCode(ConsoleTask):\n  \"\"\"Print counts of lines of code.\"\"\"\n\n  @classmethod\n  def subsystem_dependencies(cls):\n    return super(CountLinesOfCode, cls).subsystem_dependencies() + (ClocBinary,)\n\n  @classmethod\n  def register_options(cls, register):\n    super(CountLinesOfCode, cls).register_options(register)\n    register('--transitive', type=bool, fingerprint=True, default=True,\n             help='Operate on the transitive dependencies of the specified targets.  '\n                  'Unset to operate only on the specified targets.')\n    register('--ignored', type=bool, fingerprint=True,\n             help='Show information about files ignored by cloc.')\n\n  def console_output(self, targets):\n    if not self.get_options().transitive:\n      targets = self.context.target_roots\n\n    input_snapshots = tuple(\n      target.sources_snapshot(scheduler=self.context._scheduler) for target in targets\n    )\n    input_files = {f.path for snapshot in input_snapshots for f in snapshot.files}\n\n    # TODO: Work out a nice library-like utility for writing an argfile, as this will be common.\n    with temporary_dir() as tmpdir:\n      list_file = os.path.join(tmpdir, 'input_files_list')\n      with open(list_file, 'w') as list_file_out:\n        for input_file in sorted(input_files):\n          list_file_out.write(input_file)\n          list_file_out.write('\\n')\n      list_file_snapshot = self.context._scheduler.capture_snapshots((\n        PathGlobsAndRoot(\n          PathGlobs(('input_files_list',)),\n          text_type(tmpdir),\n        ),\n      ))[0]\n\n    cloc_path, cloc_snapshot = ClocBinary.global_instance().hackily_snapshot(self.context)\n\n    directory_digest = self.context._scheduler.merge_directories(tuple(s.directory_digest for s in\n      input_snapshots + (\n      cloc_snapshot,\n      list_file_snapshot,\n    )))\n\n    cmd = (\n      '/usr/bin/perl',\n      cloc_path,\n      '--skip-uniqueness',\n      '--ignored=ignored',\n      '--list-file=input_files_list',\n      '--report-file=report',\n    )\n\n    # The cloc script reaches into $PATH to look up perl. Let's assume it's in /usr/bin.\n    req = ExecuteProcessRequest(\n      argv=cmd,\n      input_files=directory_digest,\n      output_files=('ignored', 'report'),\n      description='cloc',\n    )\n    exec_result = self.context.execute_process_synchronously(req, 'cloc', (WorkUnitLabel.TOOL,))\n\n    files_content_tuple = self.context._scheduler.product_request(\n      FilesContent,\n      [exec_result.output_directory_digest]\n    )[0].dependencies\n\n    files_content = {fc.path: fc.content.decode('utf-8') for fc in files_content_tuple}\n    for line in files_content['report'].split('\\n'):\n      yield line\n\n    if self.get_options().ignored:\n      yield 'Ignored the following files:'\n      for line in files_content['ignored'].split('\\n'):\n        yield line\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/fakeNetflix/twitter-repo-pants/blob/71467604c5d43df6000995346f245f3768441c03",
        "file_path": "/src/python/pants/backend/jvm/subsystems/scala_platform.py",
        "source": "# coding=utf-8\n# Copyright 2015 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nfrom collections import namedtuple\n\nfrom pants.backend.jvm.subsystems.jvm_tool_mixin import JvmToolMixin\nfrom pants.backend.jvm.subsystems.zinc_language_mixin import ZincLanguageMixin\nfrom pants.backend.jvm.targets.jar_library import JarLibrary\nfrom pants.build_graph.address import Address\nfrom pants.build_graph.injectables_mixin import InjectablesMixin\nfrom pants.java.jar.jar_dependency import JarDependency\nfrom pants.subsystem.subsystem import Subsystem\n\n\n# full_version - the full scala version to use.\nmajor_version_info = namedtuple('major_version_info', ['full_version'])\n\n\n# Note that the compiler has two roles here: as a tool (invoked by the compile task), and as a\n# runtime library (when compiling plugins, which require the compiler library as a dependency).\nscala_build_info = {\n  '2.10': major_version_info(full_version='2.10.6'),\n  '2.11': major_version_info(full_version='2.11.12'),\n  '2.12': major_version_info(full_version='2.12.4'),\n}\n\n\n# Because scalastyle inspects only the sources, it needn't match the platform version.\nscala_style_jar = JarDependency('org.scalastyle', 'scalastyle_2.11', '0.8.0')\n\n\n# TODO: Sort out JVM compile config model: https://github.com/pantsbuild/pants/issues/4483.\nclass ScalaPlatform(JvmToolMixin, ZincLanguageMixin, InjectablesMixin, Subsystem):\n  \"\"\"A scala platform.\n\n  :API: public\n  \"\"\"\n  options_scope = 'scala'\n\n  @classmethod\n  def _create_jardep(cls, name, version):\n    return JarDependency(org='org.scala-lang',\n                         name=name,\n                         rev=scala_build_info[version].full_version)\n\n  @classmethod\n  def _create_runtime_jardep(cls, version):\n    return cls._create_jardep('scala-library', version)\n\n  @classmethod\n  def _create_compiler_jardep(cls, version):\n    return cls._create_jardep('scala-compiler', version)\n\n  @classmethod\n  def _key_for_tool_version(cls, tool, version):\n    if version == 'custom':\n      return tool\n    else:\n      return '{}_{}'.format(tool, version.replace('.', '_'))\n\n  @classmethod\n  def register_options(cls, register):\n    def register_scala_compiler_tool(version):\n      cls.register_jvm_tool(register,\n                            cls._key_for_tool_version('scalac', version),\n                            classpath=[cls._create_compiler_jardep(version)])\n\n    def register_scala_repl_tool(version, with_jline=False):\n      classpath = [cls._create_compiler_jardep(version)]  # Note: the REPL is in the compiler jar.\n      if with_jline:\n        jline_dep = JarDependency(\n            org = 'org.scala-lang',\n            name = 'jline',\n            rev = scala_build_info[version].full_version\n        )\n        classpath.append(jline_dep)\n      cls.register_jvm_tool(register,\n                            cls._key_for_tool_version('scala-repl', version),\n                            classpath=classpath)\n\n    def register_style_tool(version):\n      cls.register_jvm_tool(register,\n                            cls._key_for_tool_version('scalastyle', version),\n                            classpath=[scala_style_jar])\n\n    super(ScalaPlatform, cls).register_options(register)\n\n    register('--scalac-plugins', advanced=True, type=list, fingerprint=True,\n            help='Use these scalac plugins.')\n    register('--scalac-plugin-args', advanced=True, type=dict, default={}, fingerprint=True,\n            help='Map from scalac plugin name to list of arguments for that plugin.')\n    cls.register_jvm_tool(register, 'scalac-plugin-dep', classpath=[],\n                        help='Search for scalac plugins here, as well as in any '\n                                'explicit dependencies.')\n\n    register('--version', advanced=True, default='2.12',\n             choices=['2.10', '2.11', '2.12', 'custom'], fingerprint=True,\n             help='The scala platform version. If --version=custom, the targets '\n                  '//:scala-library, //:scalac, //:scala-repl and //:scalastyle will be used, '\n                  'and must exist.  Otherwise, defaults for the specified version will be used.')\n\n    register('--suffix-version', advanced=True, default=None,\n             help='Scala suffix to be used in `scala_jar` definitions. For example, specifying '\n                  '`2.11` or `2.12.0-RC1` would cause `scala_jar` lookups for artifacts with '\n                  'those suffixes.')\n\n    # Register the fixed version tools.\n    register_scala_compiler_tool('2.10')\n    register_scala_repl_tool('2.10', with_jline=True)  # 2.10 repl requires jline.\n    register_style_tool('2.10')\n\n    register_scala_compiler_tool('2.11')\n    register_scala_repl_tool('2.11')\n    register_style_tool('2.11')\n\n    register_scala_compiler_tool('2.12')\n    register_scala_repl_tool('2.12')\n    register_style_tool('2.12')\n\n    # Register the custom tools. We provide a dummy classpath, so that register_jvm_tool won't\n    # require that a target with the given spec actually exist (not everyone will define custom\n    # scala platforms). However if the custom tool is actually resolved, we want that to\n    # fail with a useful error, hence the dummy jardep with rev=None.\n    def register_custom_tool(key):\n      dummy_jardep = JarDependency('missing spec', ' //:{}'.format(key))\n      cls.register_jvm_tool(register, cls._key_for_tool_version(key, 'custom'),\n                            classpath=[dummy_jardep])\n    register_custom_tool('scalac')\n    register_custom_tool('scala-repl')\n    register_custom_tool('scalastyle')\n\n  def _tool_classpath(self, tool, products):\n    \"\"\"Return the proper classpath based on products and scala version.\"\"\"\n    return self.tool_classpath_from_products(products,\n                                             self._key_for_tool_version(tool, self.version),\n                                             scope=self.options_scope)\n\n  def compiler_classpath(self, products):\n    return self._tool_classpath('scalac', products)\n\n  def style_classpath(self, products):\n    return self._tool_classpath('scalastyle', products)\n\n  @property\n  def version(self):\n    return self.get_options().version\n\n  def suffix_version(self, name):\n    \"\"\"Appends the platform version to the given artifact name.\n\n    Also validates that the name doesn't already end with the version.\n    \"\"\"\n    if self.version == 'custom':\n      suffix = self.get_options().suffix_version\n      if suffix:\n        return '{0}_{1}'.format(name, suffix)\n      else:\n        raise RuntimeError('Suffix version must be specified if using a custom scala version. '\n                           'Suffix version is used for bootstrapping jars.  If a custom '\n                           'scala version is not specified, then the version specified in '\n                           '--scala-suffix-version is used.  For example for Scala '\n                           '2.10.7 you would use the suffix version \"2.10\".')\n\n    elif name.endswith(self.version):\n      raise ValueError('The name \"{0}\" should not be suffixed with the scala platform version '\n                      '({1}): it will be added automatically.'.format(name, self.version))\n    return '{0}_{1}'.format(name, self.version)\n\n  @property\n  def repl(self):\n    \"\"\"Return the repl tool key.\"\"\"\n    return self._key_for_tool_version('scala-repl', self.version)\n\n  def injectables(self, build_graph):\n    if self.version == 'custom':\n      return\n\n    specs_to_create = [\n      ('scalac', self._create_compiler_jardep),\n      ('scala-library', self._create_runtime_jardep)\n    ]\n\n    for spec_key, create_jardep_func in specs_to_create:\n      spec = self.injectables_spec_for_key(spec_key)\n      target_address = Address.parse(spec)\n      if not build_graph.contains_address(target_address):\n        jars = [create_jardep_func(self.version)]\n        build_graph.inject_synthetic_target(target_address,\n                                           JarLibrary,\n                                           jars=jars,\n                                           scope='forced')\n      elif not build_graph.get_target(target_address).is_synthetic:\n        raise build_graph.ManualSyntheticTargetError(target_address)\n\n  @property\n  def injectables_spec_mapping(self):\n    maybe_suffix = '' if self.version == 'custom' else '-synthetic'\n    return {\n      # Target spec for the scala compiler library.\n      'scalac': ['//:scalac{}'.format(maybe_suffix)],\n      # Target spec for the scala runtime library.\n      'scala-library': ['//:scala-library{}'.format(maybe_suffix)]\n    }\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/fakeNetflix/twitter-repo-pants/blob/71467604c5d43df6000995346f245f3768441c03",
        "file_path": "/src/python/pants/backend/jvm/subsystems/zinc.py",
        "source": "# coding=utf-8\n# Copyright 2017 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nfrom builtins import object\n\nfrom pants.backend.jvm.subsystems.dependency_context import DependencyContext\nfrom pants.backend.jvm.subsystems.java import Java\nfrom pants.backend.jvm.subsystems.jvm_tool_mixin import JvmToolMixin\nfrom pants.backend.jvm.subsystems.scala_platform import ScalaPlatform\nfrom pants.backend.jvm.subsystems.shader import Shader\nfrom pants.backend.jvm.targets.scala_jar_dependency import ScalaJarDependency\nfrom pants.backend.jvm.tasks.classpath_products import ClasspathEntry\nfrom pants.backend.jvm.tasks.classpath_util import ClasspathUtil\nfrom pants.base.build_environment import get_buildroot\nfrom pants.engine.fs import PathGlobs, PathGlobsAndRoot\nfrom pants.java.jar.jar_dependency import JarDependency\nfrom pants.subsystem.subsystem import Subsystem\nfrom pants.util.dirutil import fast_relpath\nfrom pants.util.memo import memoized_method, memoized_property\n\n\nclass Zinc(object):\n  \"\"\"Configuration for Pants' zinc wrapper tool.\"\"\"\n\n  ZINC_COMPILE_MAIN = 'org.pantsbuild.zinc.compiler.Main'\n  ZINC_EXTRACT_MAIN = 'org.pantsbuild.zinc.extractor.Main'\n  DEFAULT_CONFS = ['default']\n\n  ZINC_COMPILER_TOOL_NAME = 'zinc'\n  ZINC_EXTRACTOR_TOOL_NAME = 'zinc-extractor'\n\n  class Factory(Subsystem, JvmToolMixin):\n    options_scope = 'zinc'\n\n    @classmethod\n    def subsystem_dependencies(cls):\n      return super(Zinc.Factory, cls).subsystem_dependencies() + (DependencyContext,\n                                                                  Java,\n                                                                  ScalaPlatform)\n\n    @classmethod\n    def register_options(cls, register):\n      super(Zinc.Factory, cls).register_options(register)\n\n      zinc_rev = '1.0.3'\n\n      shader_rules = [\n          # The compiler-interface and compiler-bridge tool jars carry xsbt and\n          # xsbti interfaces that are used across the shaded tool jar boundary so\n          # we preserve these root packages wholesale along with the core scala\n          # APIs.\n          Shader.exclude_package('scala', recursive=True),\n          Shader.exclude_package('xsbt', recursive=True),\n          Shader.exclude_package('xsbti', recursive=True),\n          # Unfortunately, is loaded reflectively by the compiler.\n          Shader.exclude_package('org.apache.logging.log4j', recursive=True),\n        ]\n\n      cls.register_jvm_tool(register,\n                            Zinc.ZINC_COMPILER_TOOL_NAME,\n                            classpath=[\n                              JarDependency('org.pantsbuild', 'zinc-compiler_2.11', '0.0.7'),\n                            ],\n                            main=Zinc.ZINC_COMPILE_MAIN,\n                            custom_rules=shader_rules)\n\n      cls.register_jvm_tool(register,\n                            'compiler-bridge',\n                            classpath=[\n                              ScalaJarDependency(org='org.scala-sbt',\n                                                name='compiler-bridge',\n                                                rev=zinc_rev,\n                                                classifier='sources',\n                                                intransitive=True),\n                            ])\n      cls.register_jvm_tool(register,\n                            'compiler-interface',\n                            classpath=[\n                              JarDependency(org='org.scala-sbt',\n                                            name='compiler-interface',\n                                            rev=zinc_rev),\n                            ],\n                            # NB: We force a noop-jarjar'ing of the interface, since it is now\n                            # broken up into multiple jars, but zinc does not yet support a sequence\n                            # of jars for the interface.\n                            main='no.such.main.Main',\n                            custom_rules=shader_rules)\n\n      cls.register_jvm_tool(register,\n                            Zinc.ZINC_EXTRACTOR_TOOL_NAME,\n                            classpath=[\n                              JarDependency('org.pantsbuild', 'zinc-extractor_2.11', '0.0.4')\n                            ])\n\n    @classmethod\n    def _zinc(cls, products):\n      return cls.tool_jar_from_products(products, Zinc.ZINC_COMPILER_TOOL_NAME, cls.options_scope)\n\n    @classmethod\n    def _compiler_bridge(cls, products):\n      return cls.tool_jar_from_products(products, 'compiler-bridge', cls.options_scope)\n\n    @classmethod\n    def _compiler_interface(cls, products):\n      return cls.tool_jar_from_products(products, 'compiler-interface', cls.options_scope)\n\n    def create(self, products):\n      \"\"\"Create a Zinc instance from products active in the current Pants run.\n\n      :param products: The active Pants run products to pluck classpaths from.\n      :type products: :class:`pants.goal.products.Products`\n      :returns: A Zinc instance with access to relevant Zinc compiler wrapper jars and classpaths.\n      :rtype: :class:`Zinc`\n      \"\"\"\n      return Zinc(self, products)\n\n  def __init__(self, zinc_factory, products):\n    self._zinc_factory = zinc_factory\n    self._products = products\n\n  @memoized_property\n  def zinc(self):\n    \"\"\"Return the Zinc wrapper compiler classpath.\n\n    :rtype: list of str\n    \"\"\"\n    return self._zinc_factory._zinc(self._products)\n\n  @property\n  def dist(self):\n    \"\"\"Return the distribution selected for Zinc.\n\n    :rtype: list of str\n    \"\"\"\n    return self._zinc_factory.dist\n\n  @memoized_property\n  def compiler_bridge(self):\n    \"\"\"Return the path to the Zinc compiler-bridge jar.\n\n    :rtype: str\n    \"\"\"\n    return self._zinc_factory._compiler_bridge(self._products)\n\n  @memoized_property\n  def compiler_interface(self):\n    \"\"\"Return the path to the Zinc compiler-interface jar.\n\n    :rtype: str\n    \"\"\"\n    return self._zinc_factory._compiler_interface(self._products)\n\n  @memoized_method\n  def snapshot(self, scheduler):\n    buildroot = get_buildroot()\n    return scheduler.capture_snapshots((\n      PathGlobsAndRoot(\n        PathGlobs(\n          tuple(\n            fast_relpath(a, buildroot)\n            for a in (self.zinc, self.compiler_bridge, self.compiler_interface)\n          )\n        ),\n        buildroot,\n      ),\n    ))[0]\n\n  # TODO: Make rebase map work without needing to pass in absolute paths:\n  # https://github.com/pantsbuild/pants/issues/6434\n  @memoized_property\n  def rebase_map_args(self):\n    \"\"\"We rebase known stable paths in zinc analysis to make it portable across machines.\"\"\"\n    rebases = {\n        self.dist.real_home: '/dev/null/remapped_by_pants/java_home/',\n        get_buildroot(): '/dev/null/remapped_by_pants/buildroot/',\n        self._zinc_factory.get_options().pants_workdir: '/dev/null/remapped_by_pants/workdir/',\n      }\n    return (\n        '-rebase-map',\n        ','.join('{}:{}'.format(src, dst) for src, dst in rebases.items())\n      )\n\n  @memoized_method\n  def _compiler_plugins_cp_entries(self):\n    \"\"\"Any additional global compiletime classpath entries for compiler plugins.\"\"\"\n    java_options_src = Java.global_instance()\n    scala_options_src = ScalaPlatform.global_instance()\n\n    def cp(instance, toolname):\n      scope = instance.options_scope\n      return instance.tool_classpath_from_products(self._products, toolname, scope=scope)\n    classpaths = (cp(java_options_src, 'javac-plugin-dep') +\n                  cp(scala_options_src, 'scalac-plugin-dep'))\n    return [(conf, ClasspathEntry(jar)) for conf in self.DEFAULT_CONFS for jar in classpaths]\n\n  @memoized_property\n  def extractor(self):\n    return self._zinc_factory.tool_classpath_from_products(self._products,\n                                                           self.ZINC_EXTRACTOR_TOOL_NAME,\n                                                           scope=self._zinc_factory.options_scope)\n\n  def compile_classpath_entries(self, classpath_product_key, target, extra_cp_entries=None):\n    classpath_product = self._products.get_data(classpath_product_key)\n    if DependencyContext.global_instance().defaulted_property(target, lambda x: x.strict_deps):\n      dependencies = target.strict_dependencies(DependencyContext.global_instance())\n    else:\n      dependencies = DependencyContext.global_instance().all_dependencies(target)\n\n    all_extra_cp_entries = list(self._compiler_plugins_cp_entries())\n    if extra_cp_entries:\n      all_extra_cp_entries.extend(extra_cp_entries)\n\n    # TODO: We convert dependencies to an iterator here in order to _preserve_ a bug that will be\n    # fixed in https://github.com/pantsbuild/pants/issues/4874: `ClasspathUtil.compute_classpath`\n    # expects to receive a list, but had been receiving an iterator. In the context of an\n    # iterator, `excludes` are not applied\n    # in ClasspathProducts.get_product_target_mappings_for_targets.\n    return ClasspathUtil.compute_classpath_entries(iter(dependencies),\n      classpath_product,\n      all_extra_cp_entries,\n      self.DEFAULT_CONFS,\n    )\n\n  def compile_classpath(self, classpath_product_key, target, extra_cp_entries=None):\n    \"\"\"Compute the compile classpath for the given target.\"\"\"\n    return list(\n      entry.path\n      for entry in self.compile_classpath_entries(classpath_product_key, target, extra_cp_entries)\n    )\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/fakeNetflix/twitter-repo-pants/blob/71467604c5d43df6000995346f245f3768441c03",
        "file_path": "/src/python/pants/backend/jvm/tasks/jvm_compile/javac/javac_compile.py",
        "source": "# coding=utf-8\n# Copyright 2018 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport logging\nimport os\nfrom builtins import str\n\nfrom future.utils import text_type\n\nfrom pants.backend.jvm import argfile\nfrom pants.backend.jvm.subsystems.java import Java\nfrom pants.backend.jvm.subsystems.jvm_platform import JvmPlatform\nfrom pants.backend.jvm.targets.annotation_processor import AnnotationProcessor\nfrom pants.backend.jvm.targets.javac_plugin import JavacPlugin\nfrom pants.backend.jvm.targets.jvm_target import JvmTarget\nfrom pants.backend.jvm.tasks.jvm_compile.jvm_compile import JvmCompile\nfrom pants.base.exceptions import TaskError\nfrom pants.base.workunit import WorkUnit, WorkUnitLabel\nfrom pants.engine.fs import DirectoryToMaterialize\nfrom pants.engine.isolated_process import ExecuteProcessRequest\nfrom pants.java.distribution.distribution import DistributionLocator\nfrom pants.util.dirutil import safe_open\nfrom pants.util.process_handler import subprocess\n\n\n# Well known metadata file to register javac plugins.\n_JAVAC_PLUGIN_INFO_FILE = 'META-INF/services/com.sun.source.util.Plugin'\n\n# Well known metadata file to register annotation processors with a java 1.6+ compiler.\n_PROCESSOR_INFO_FILE = 'META-INF/services/javax.annotation.processing.Processor'\n\nlogger = logging.getLogger(__name__)\n\n\nclass JavacCompile(JvmCompile):\n  \"\"\"Compile Java code using Javac.\"\"\"\n\n  _name = 'java'\n\n  @staticmethod\n  def _write_javac_plugin_info(resources_dir, javac_plugin_target):\n    javac_plugin_info_file = os.path.join(resources_dir, _JAVAC_PLUGIN_INFO_FILE)\n    with safe_open(javac_plugin_info_file, 'w') as f:\n      f.write(javac_plugin_target.classname)\n\n  @classmethod\n  def get_args_default(cls, bootstrap_option_values):\n    return ('-encoding', 'UTF-8')\n\n  @classmethod\n  def get_warning_args_default(cls):\n    return ('-deprecation', '-Xlint:all', '-Xlint:-serial', '-Xlint:-path')\n\n  @classmethod\n  def get_no_warning_args_default(cls):\n    return ('-nowarn', '-Xlint:none', )\n\n  @classmethod\n  def get_fatal_warnings_enabled_args_default(cls):\n    return ('-Werror',)\n\n  @classmethod\n  def get_fatal_warnings_disabled_args_default(cls):\n    return ()\n\n  @classmethod\n  def register_options(cls, register):\n    super(JavacCompile, cls).register_options(register)\n\n  @classmethod\n  def subsystem_dependencies(cls):\n    return super(JavacCompile, cls).subsystem_dependencies() + (JvmPlatform,)\n\n  @classmethod\n  def prepare(cls, options, round_manager):\n    super(JavacCompile, cls).prepare(options, round_manager)\n\n  @classmethod\n  def product_types(cls):\n    return ['runtime_classpath']\n\n  def __init__(self, *args, **kwargs):\n    super(JavacCompile, self).__init__(*args, **kwargs)\n    self.set_distribution(jdk=True)\n\n  def select(self, target):\n    if not isinstance(target, JvmTarget):\n      return False\n    return target.has_sources('.java')\n\n  def select_source(self, source_file_path):\n    return source_file_path.endswith('.java')\n\n  def javac_classpath(self):\n    # Note that if this classpath is empty then Javac will automatically use the javac from\n    # the JDK it was invoked with.\n    return Java.global_javac_classpath(self.context.products)\n\n  def write_extra_resources(self, compile_context):\n    \"\"\"Override write_extra_resources to produce plugin and annotation processor files.\"\"\"\n    target = compile_context.target\n    if isinstance(target, JavacPlugin):\n      self._write_javac_plugin_info(compile_context.classes_dir, target)\n    elif isinstance(target, AnnotationProcessor) and target.processors:\n      processor_info_file = os.path.join(compile_context.classes_dir, _PROCESSOR_INFO_FILE)\n      self._write_processor_info(processor_info_file, target.processors)\n\n  def _write_processor_info(self, processor_info_file, processors):\n    with safe_open(processor_info_file, 'w') as f:\n      for processor in processors:\n        f.write('{}\\n'.format(processor.strip()))\n\n  def execute(self):\n    if JvmPlatform.global_instance().get_options().compiler == 'javac':\n      return super(JavacCompile, self).execute()\n\n  def compile(self, ctx, args, dependency_classpath, upstream_analysis,\n              settings, fatal_warnings, zinc_file_manager,\n              javac_plugin_map, scalac_plugin_map):\n    classpath = (ctx.classes_dir,) + tuple(ce.path for ce in dependency_classpath)\n\n    if self.get_options().capture_classpath:\n      self._record_compile_classpath(classpath, ctx.target, ctx.classes_dir)\n\n    try:\n      distribution = JvmPlatform.preferred_jvm_distribution([settings], strict=True)\n    except DistributionLocator.Error:\n      distribution = JvmPlatform.preferred_jvm_distribution([settings], strict=False)\n\n    javac_cmd = ['{}/bin/javac'.format(distribution.real_home)]\n\n    javac_cmd.extend([\n      '-classpath', ':'.join(classpath),\n    ])\n\n    if settings.args:\n      settings_args = settings.args\n      if any('$JAVA_HOME' in a for a in settings.args):\n        logger.debug('Substituting \"$JAVA_HOME\" with \"{}\" in jvm-platform args.'\n                     .format(distribution.home))\n        settings_args = (a.replace('$JAVA_HOME', distribution.home) for a in settings.args)\n      javac_cmd.extend(settings_args)\n\n      javac_cmd.extend([\n        # TODO: support -release\n        '-source', str(settings.source_level),\n        '-target', str(settings.target_level),\n      ])\n\n    if self.execution_strategy == self.HERMETIC:\n      javac_cmd.extend([\n        # We need to strip the source root from our output files. Outputting to a directory, and\n        # capturing that directory, does the job.\n        # Unfortunately, javac errors if the directory you pass to -d doesn't exist, and we don't\n        # have a convenient way of making a directory in the output tree, so let's just use the\n        # working directory as our output dir.\n        # This also has the benefit of not needing to strip leading directories from the returned\n        # snapshot.\n        '-d', '.',\n      ])\n    else:\n      javac_cmd.extend([\n        '-d', ctx.classes_dir,\n      ])\n\n    javac_cmd.extend(self._javac_plugin_args(javac_plugin_map))\n\n    javac_cmd.extend(args)\n\n    if fatal_warnings:\n      javac_cmd.extend(self.get_options().fatal_warnings_enabled_args)\n    else:\n      javac_cmd.extend(self.get_options().fatal_warnings_disabled_args)\n\n    with argfile.safe_args(ctx.sources, self.get_options()) as batched_sources:\n      javac_cmd.extend(batched_sources)\n\n      if self.execution_strategy == self.HERMETIC:\n        self._execute_hermetic_compile(javac_cmd, ctx)\n      else:\n        with self.context.new_workunit(name='javac',\n                                       cmd=' '.join(javac_cmd),\n                                       labels=[WorkUnitLabel.COMPILER]) as workunit:\n          self.context.log.debug('Executing {}'.format(' '.join(javac_cmd)))\n          p = subprocess.Popen(javac_cmd, stdout=workunit.output('stdout'), stderr=workunit.output('stderr'))\n          return_code = p.wait()\n          workunit.set_outcome(WorkUnit.FAILURE if return_code else WorkUnit.SUCCESS)\n          if return_code:\n            raise TaskError('javac exited with return code {rc}'.format(rc=return_code))\n\n  @classmethod\n  def _javac_plugin_args(cls, javac_plugin_map):\n    ret = []\n    for plugin, args in javac_plugin_map.items():\n      for arg in args:\n        if ' ' in arg:\n          # Note: Args are separated by spaces, and there is no way to escape embedded spaces, as\n          # javac's Main does a simple split on these strings.\n          raise TaskError('javac plugin args must not contain spaces '\n                          '(arg {} for plugin {})'.format(arg, plugin))\n      ret.append('-Xplugin:{} {}'.format(plugin, ' '.join(args)))\n    return ret\n\n  def _execute_hermetic_compile(self, cmd, ctx):\n    # For now, executing a compile remotely only works for targets that\n    # do not have any dependencies or inner classes\n\n    input_snapshot = ctx.target.sources_snapshot(scheduler=self.context._scheduler)\n    output_files = tuple(\n      # Assume no extra .class files to grab. We'll fix up that case soon.\n      # Drop the source_root from the file path.\n      # Assumes `-d .` has been put in the command.\n      os.path.relpath(f.path.replace('.java', '.class'), ctx.target.target_base)\n      for f in input_snapshot.files if f.path.endswith('.java')\n    )\n    exec_process_request = ExecuteProcessRequest(\n      argv=tuple(cmd),\n      input_files=input_snapshot.directory_digest,\n      output_files=output_files,\n      description='Compiling {} with javac'.format(ctx.target.address.spec),\n    )\n    exec_result = self.context.execute_process_synchronously(\n      exec_process_request,\n      'javac',\n      (WorkUnitLabel.TASK, WorkUnitLabel.JVM),\n    )\n\n    # Dump the output to the .pants.d directory where it's expected by downstream tasks.\n    classes_directory = ctx.classes_dir\n    self.context._scheduler.materialize_directories((\n      DirectoryToMaterialize(text_type(classes_directory), exec_result.output_directory_digest),\n    ))\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/fakeNetflix/twitter-repo-pants/blob/71467604c5d43df6000995346f245f3768441c03",
        "file_path": "/src/python/pants/backend/jvm/tasks/jvm_compile/zinc/zinc_compile.py",
        "source": "# coding=utf-8\n# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport errno\nimport logging\nimport os\nimport re\nimport textwrap\nfrom builtins import open\nfrom collections import defaultdict\nfrom contextlib import closing\nfrom hashlib import sha1\nfrom xml.etree import ElementTree\n\nfrom future.utils import PY3, text_type\n\nfrom pants.backend.jvm.subsystems.java import Java\nfrom pants.backend.jvm.subsystems.jvm_platform import JvmPlatform\nfrom pants.backend.jvm.subsystems.scala_platform import ScalaPlatform\nfrom pants.backend.jvm.subsystems.zinc import Zinc\nfrom pants.backend.jvm.targets.annotation_processor import AnnotationProcessor\nfrom pants.backend.jvm.targets.javac_plugin import JavacPlugin\nfrom pants.backend.jvm.targets.jvm_target import JvmTarget\nfrom pants.backend.jvm.targets.scalac_plugin import ScalacPlugin\nfrom pants.backend.jvm.tasks.classpath_util import ClasspathUtil\nfrom pants.backend.jvm.tasks.jvm_compile.jvm_compile import JvmCompile\nfrom pants.base.build_environment import get_buildroot\nfrom pants.base.exceptions import TaskError\nfrom pants.base.hash_utils import hash_file\nfrom pants.base.workunit import WorkUnitLabel\nfrom pants.engine.fs import DirectoryToMaterialize, PathGlobs, PathGlobsAndRoot\nfrom pants.engine.isolated_process import ExecuteProcessRequest\nfrom pants.java.distribution.distribution import DistributionLocator\nfrom pants.util.contextutil import open_zip\nfrom pants.util.dirutil import fast_relpath, safe_open\nfrom pants.util.memo import memoized_method, memoized_property\n\n\n# Well known metadata file required to register scalac plugins with nsc.\n_SCALAC_PLUGIN_INFO_FILE = 'scalac-plugin.xml'\n\n# Well known metadata file to register javac plugins.\n_JAVAC_PLUGIN_INFO_FILE = 'META-INF/services/com.sun.source.util.Plugin'\n\n# Well known metadata file to register annotation processors with a java 1.6+ compiler.\n_PROCESSOR_INFO_FILE = 'META-INF/services/javax.annotation.processing.Processor'\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseZincCompile(JvmCompile):\n  \"\"\"An abstract base class for zinc compilation tasks.\"\"\"\n\n  _name = 'zinc'\n\n  @staticmethod\n  def _write_scalac_plugin_info(resources_dir, scalac_plugin_target):\n    scalac_plugin_info_file = os.path.join(resources_dir, _SCALAC_PLUGIN_INFO_FILE)\n    with safe_open(scalac_plugin_info_file, 'w') as f:\n      f.write(textwrap.dedent(\"\"\"\n        <plugin>\n          <name>{}</name>\n          <classname>{}</classname>\n        </plugin>\n      \"\"\".format(scalac_plugin_target.plugin, scalac_plugin_target.classname)).strip())\n\n  @staticmethod\n  def _write_javac_plugin_info(resources_dir, javac_plugin_target):\n    javac_plugin_info_file = os.path.join(resources_dir, _JAVAC_PLUGIN_INFO_FILE)\n    with safe_open(javac_plugin_info_file, 'w') as f:\n      classname = javac_plugin_target.classname if PY3 else javac_plugin_target.classname.decode('utf-8')\n      f.write(classname)\n\n  @staticmethod\n  def validate_arguments(log, whitelisted_args, args):\n    \"\"\"Validate that all arguments match whitelisted regexes.\"\"\"\n    valid_patterns = {re.compile(p): v for p, v in whitelisted_args.items()}\n\n    def validate(idx):\n      arg = args[idx]\n      for pattern, has_argument in valid_patterns.items():\n        if pattern.match(arg):\n          return 2 if has_argument else 1\n      log.warn(\"Zinc argument '{}' is not supported, and is subject to change/removal!\".format(arg))\n      return 1\n\n    arg_index = 0\n    while arg_index < len(args):\n      arg_index += validate(arg_index)\n\n  @staticmethod\n  def _get_zinc_arguments(settings):\n    \"\"\"Extracts and formats the zinc arguments given in the jvm platform settings.\n\n    This is responsible for the symbol substitution which replaces $JAVA_HOME with the path to an\n    appropriate jvm distribution.\n\n    :param settings: The jvm platform settings from which to extract the arguments.\n    :type settings: :class:`JvmPlatformSettings`\n    \"\"\"\n    zinc_args = [\n      '-C-source', '-C{}'.format(settings.source_level),\n      '-C-target', '-C{}'.format(settings.target_level),\n    ]\n    if settings.args:\n      settings_args = settings.args\n      if any('$JAVA_HOME' in a for a in settings.args):\n        try:\n          distribution = JvmPlatform.preferred_jvm_distribution([settings], strict=True)\n        except DistributionLocator.Error:\n          distribution = JvmPlatform.preferred_jvm_distribution([settings], strict=False)\n        logger.debug('Substituting \"$JAVA_HOME\" with \"{}\" in jvm-platform args.'\n                     .format(distribution.home))\n        settings_args = (a.replace('$JAVA_HOME', distribution.home) for a in settings.args)\n      zinc_args.extend(settings_args)\n    return zinc_args\n\n  @classmethod\n  def implementation_version(cls):\n    return super(BaseZincCompile, cls).implementation_version() + [('BaseZincCompile', 7)]\n\n  @classmethod\n  def get_jvm_options_default(cls, bootstrap_option_values):\n    return ('-Dfile.encoding=UTF-8', '-Dzinc.analysis.cache.limit=1000',\n            '-Djava.awt.headless=true', '-Xmx2g')\n\n  @classmethod\n  def get_args_default(cls, bootstrap_option_values):\n    return ('-C-encoding', '-CUTF-8', '-S-encoding', '-SUTF-8', '-S-g:vars')\n\n  @classmethod\n  def get_warning_args_default(cls):\n    return ('-C-deprecation', '-C-Xlint:all', '-C-Xlint:-serial', '-C-Xlint:-path',\n            '-S-deprecation', '-S-unchecked', '-S-Xlint')\n\n  @classmethod\n  def get_no_warning_args_default(cls):\n    return ('-C-nowarn', '-C-Xlint:none', '-S-nowarn', '-S-Xlint:none', )\n\n  @classmethod\n  def get_fatal_warnings_enabled_args_default(cls):\n    return ('-S-Xfatal-warnings', '-C-Werror')\n\n  @classmethod\n  def get_fatal_warnings_disabled_args_default(cls):\n    return ()\n\n  @classmethod\n  def register_options(cls, register):\n    super(BaseZincCompile, cls).register_options(register)\n    register('--whitelisted-args', advanced=True, type=dict,\n             default={\n               '-S.*': False,\n               '-C.*': False,\n               '-file-filter': True,\n               '-msg-filter': True,\n               },\n             help='A dict of option regexes that make up pants\\' supported API for zinc. '\n                  'Options not listed here are subject to change/removal. The value of the dict '\n                  'indicates that an option accepts an argument.')\n\n    register('--incremental', advanced=True, type=bool, default=True,\n             help='When set, zinc will use sub-target incremental compilation, which dramatically '\n                  'improves compile performance while changing large targets. When unset, '\n                  'changed targets will be compiled with an empty output directory, as if after '\n                  'running clean-all.')\n\n    register('--incremental-caching', advanced=True, type=bool,\n             help='When set, the results of incremental compiles will be written to the cache. '\n                  'This is unset by default, because it is generally a good precaution to cache '\n                  'only clean/cold builds.')\n\n  @classmethod\n  def subsystem_dependencies(cls):\n    return super(BaseZincCompile, cls).subsystem_dependencies() + (Zinc.Factory, JvmPlatform,)\n\n  @classmethod\n  def prepare(cls, options, round_manager):\n    super(BaseZincCompile, cls).prepare(options, round_manager)\n    ScalaPlatform.prepare_tools(round_manager)\n\n  @property\n  def incremental(self):\n    \"\"\"Zinc implements incremental compilation.\n\n    Setting this property causes the task infrastructure to clone the previous\n    results_dir for a target into the new results_dir for a target.\n    \"\"\"\n    return self.get_options().incremental\n\n  @property\n  def cache_incremental(self):\n    \"\"\"Optionally write the results of incremental compiles to the cache.\"\"\"\n    return self.get_options().incremental_caching\n\n  @memoized_property\n  def _zinc(self):\n    return Zinc.Factory.global_instance().create(self.context.products)\n\n  def __init__(self, *args, **kwargs):\n    super(BaseZincCompile, self).__init__(*args, **kwargs)\n    # A directory to contain per-target subdirectories with apt processor info files.\n    self._processor_info_dir = os.path.join(self.workdir, 'apt-processor-info')\n\n    # Validate zinc options.\n    ZincCompile.validate_arguments(self.context.log, self.get_options().whitelisted_args,\n                                   self._args)\n    if self.execution_strategy == self.HERMETIC:\n      try:\n        fast_relpath(self.get_options().pants_workdir, get_buildroot())\n      except ValueError:\n        raise TaskError(\n          \"Hermetic zinc execution currently requires the workdir to be a child of the buildroot \"\n          \"but workdir was {} and buildroot was {}\".format(\n            self.get_options().pants_workdir,\n            get_buildroot(),\n          )\n        )\n\n      if self.get_options().use_classpath_jars:\n        # TODO: Make this work by capturing the correct DirectoryDigest and passing them around the\n        # right places.\n        # See https://github.com/pantsbuild/pants/issues/6432\n        raise TaskError(\"Hermetic zinc execution currently doesn't work with classpath jars\")\n\n  def select(self, target):\n    raise NotImplementedError()\n\n  def select_source(self, source_file_path):\n    raise NotImplementedError()\n\n  def register_extra_products_from_contexts(self, targets, compile_contexts):\n    compile_contexts = [self.select_runtime_context(compile_contexts[t]) for t in targets]\n    zinc_analysis = self.context.products.get_data('zinc_analysis')\n    zinc_args = self.context.products.get_data('zinc_args')\n\n    if zinc_analysis is not None:\n      for compile_context in compile_contexts:\n        zinc_analysis[compile_context.target] = (compile_context.classes_dir,\n        compile_context.jar_file,\n        compile_context.analysis_file)\n\n    if zinc_args is not None:\n      for compile_context in compile_contexts:\n        with open(compile_context.zinc_args_file, 'r') as fp:\n          args = fp.read().split()\n        zinc_args[compile_context.target] = args\n\n  def create_empty_extra_products(self):\n    if self.context.products.is_required_data('zinc_analysis'):\n      self.context.products.safe_create_data('zinc_analysis', dict)\n\n    if self.context.products.is_required_data('zinc_args'):\n      self.context.products.safe_create_data('zinc_args', lambda: defaultdict(list))\n\n  def javac_classpath(self):\n    # Note that if this classpath is empty then Zinc will automatically use the javac from\n    # the JDK it was invoked with.\n    return Java.global_javac_classpath(self.context.products)\n\n  def scalac_classpath(self):\n    return ScalaPlatform.global_instance().compiler_classpath(self.context.products)\n\n  def write_extra_resources(self, compile_context):\n    \"\"\"Override write_extra_resources to produce plugin and annotation processor files.\"\"\"\n    target = compile_context.target\n    if isinstance(target, ScalacPlugin):\n      self._write_scalac_plugin_info(compile_context.classes_dir, target)\n    elif isinstance(target, JavacPlugin):\n      self._write_javac_plugin_info(compile_context.classes_dir, target)\n    elif isinstance(target, AnnotationProcessor) and target.processors:\n      processor_info_file = os.path.join(compile_context.classes_dir, _PROCESSOR_INFO_FILE)\n      self._write_processor_info(processor_info_file, target.processors)\n\n  def _write_processor_info(self, processor_info_file, processors):\n    with safe_open(processor_info_file, 'w') as f:\n      for processor in processors:\n        f.write('{}\\n'.format(processor.strip()))\n\n  @memoized_property\n  def _zinc_cache_dir(self):\n    \"\"\"A directory where zinc can store compiled copies of the `compiler-bridge`.\n\n    The compiler-bridge is specific to each scala version, and is lazily computed by zinc if the\n    appropriate version does not exist. Eventually it would be great to just fetch this rather\n    than compiling it.\n    \"\"\"\n    hasher = sha1()\n    for cp_entry in [self._zinc.zinc, self._zinc.compiler_interface, self._zinc.compiler_bridge]:\n      hasher.update(os.path.relpath(cp_entry, self.get_options().pants_workdir))\n    key = hasher.hexdigest()[:12]\n    return os.path.join(self.get_options().pants_bootstrapdir, 'zinc', key)\n\n  def compile(self, ctx, args, dependency_classpath, upstream_analysis,\n              settings, compiler_option_sets, zinc_file_manager,\n              javac_plugin_map, scalac_plugin_map):\n    absolute_classpath = (ctx.classes_dir,) + tuple(ce.path for ce in dependency_classpath)\n\n    if self.get_options().capture_classpath:\n      self._record_compile_classpath(absolute_classpath, ctx.target, ctx.classes_dir)\n\n    # TODO: Allow use of absolute classpath entries with hermetic execution,\n    # specifically by using .jdk dir for Distributions:\n    # https://github.com/pantsbuild/pants/issues/6430\n    self._verify_zinc_classpath(absolute_classpath, allow_dist=(self.execution_strategy != self.HERMETIC))\n    # TODO: Investigate upstream_analysis for hermetic compiles\n    self._verify_zinc_classpath(upstream_analysis.keys())\n\n    def relative_to_exec_root(path):\n      # TODO: Support workdirs not nested under buildroot by path-rewriting.\n      return fast_relpath(path, get_buildroot())\n\n    scala_path = self.scalac_classpath()\n    compiler_interface = self._zinc.compiler_interface\n    compiler_bridge = self._zinc.compiler_bridge\n    classes_dir = ctx.classes_dir\n    analysis_cache = ctx.analysis_file\n\n    scala_path = tuple(relative_to_exec_root(c) for c in scala_path)\n    compiler_interface = relative_to_exec_root(compiler_interface)\n    compiler_bridge = relative_to_exec_root(compiler_bridge)\n    analysis_cache = relative_to_exec_root(analysis_cache)\n    classes_dir = relative_to_exec_root(classes_dir)\n    # TODO: Have these produced correctly, rather than having to relativize them here\n    relative_classpath = tuple(relative_to_exec_root(c) for c in absolute_classpath)\n\n    zinc_args = []\n    zinc_args.extend([\n      '-log-level', self.get_options().level,\n      '-analysis-cache', analysis_cache,\n      '-classpath', ':'.join(relative_classpath),\n      '-d', classes_dir,\n    ])\n    if not self.get_options().colors:\n      zinc_args.append('-no-color')\n\n    zinc_args.extend(['-compiler-interface', compiler_interface])\n    zinc_args.extend(['-compiler-bridge', compiler_bridge])\n    # TODO: Kill zinc-cache-dir: https://github.com/pantsbuild/pants/issues/6155\n    # But for now, this will probably fail remotely because the homedir probably doesn't exist.\n    zinc_args.extend(['-zinc-cache-dir', self._zinc_cache_dir])\n    zinc_args.extend(['-scala-path', ':'.join(scala_path)])\n\n    zinc_args.extend(self._javac_plugin_args(javac_plugin_map))\n    # Search for scalac plugins on the classpath.\n    # Note that:\n    # - We also search in the extra scalac plugin dependencies, if specified.\n    # - In scala 2.11 and up, the plugin's classpath element can be a dir, but for 2.10 it must be\n    #   a jar.  So in-repo plugins will only work with 2.10 if --use-classpath-jars is true.\n    # - We exclude our own classes_dir/jar_file, because if we're a plugin ourselves, then our\n    #   classes_dir doesn't have scalac-plugin.xml yet, and we don't want that fact to get\n    #   memoized (which in practice will only happen if this plugin uses some other plugin, thus\n    #   triggering the plugin search mechanism, which does the memoizing).\n    scalac_plugin_search_classpath = (\n      (set(absolute_classpath) | set(self.scalac_plugin_classpath_elements())) -\n      {ctx.classes_dir, ctx.jar_file}\n    )\n    zinc_args.extend(self._scalac_plugin_args(scalac_plugin_map, scalac_plugin_search_classpath))\n    if upstream_analysis:\n      zinc_args.extend(['-analysis-map',\n                        ','.join('{}:{}'.format(\n                          relative_to_exec_root(k),\n                          relative_to_exec_root(v)\n                        ) for k, v in upstream_analysis.items())])\n\n    zinc_args.extend(self._zinc.rebase_map_args)\n\n    zinc_args.extend(args)\n    zinc_args.extend(self._get_zinc_arguments(settings))\n    zinc_args.append('-transactional')\n\n    for option_set in compiler_option_sets:\n      enabled_args = self.get_options().compiler_option_sets_enabled_args.get(option_set, [])\n      if option_set == 'fatal_warnings':\n        enabled_args = self.get_options().fatal_warnings_enabled_args\n      zinc_args.extend(enabled_args)\n\n    for option_set, disabled_args in self.get_options().compiler_option_sets_disabled_args.items():\n      if option_set not in compiler_option_sets:\n        if option_set == 'fatal_warnings':\n          disabled_args = self.get_options().fatal_warnings_disabled_args\n        zinc_args.extend(disabled_args)\n\n    if not self._clear_invalid_analysis:\n      zinc_args.append('-no-clear-invalid-analysis')\n\n    if not zinc_file_manager:\n      zinc_args.append('-no-zinc-file-manager')\n\n    jvm_options = []\n\n    if self.javac_classpath():\n      # Make the custom javac classpath the first thing on the bootclasspath, to ensure that\n      # it's the one javax.tools.ToolProvider.getSystemJavaCompiler() loads.\n      # It will probably be loaded even on the regular classpath: If not found on the bootclasspath,\n      # getSystemJavaCompiler() constructs a classloader that loads from the JDK's tools.jar.\n      # That classloader will first delegate to its parent classloader, which will search the\n      # regular classpath.  However it's harder to guarantee that our javac will preceed any others\n      # on the classpath, so it's safer to prefix it to the bootclasspath.\n      jvm_options.extend(['-Xbootclasspath/p:{}'.format(':'.join(self.javac_classpath()))])\n\n    jvm_options.extend(self._jvm_options)\n\n    zinc_args.extend(ctx.sources)\n\n    self.log_zinc_file(ctx.analysis_file)\n    with open(ctx.zinc_args_file, 'wb') as fp:\n      for arg in zinc_args:\n        fp.write(arg)\n        fp.write(b'\\n')\n\n    if self.execution_strategy == self.HERMETIC:\n      zinc_relpath = fast_relpath(self._zinc.zinc, get_buildroot())\n\n      snapshots = [\n        self._zinc.snapshot(self.context._scheduler),\n        ctx.target.sources_snapshot(self.context._scheduler),\n      ]\n\n      directory_digests = tuple(\n        entry.directory_digest for entry in dependency_classpath if entry.directory_digest\n      )\n      if len(directory_digests) != len(dependency_classpath):\n        for dep in dependency_classpath:\n          if dep.directory_digest is None:\n            logger.warning(\n              \"ClasspathEntry {} didn't have a DirectoryDigest, so won't be present for hermetic \"\n              \"execution\".format(dep)\n            )\n\n      if scala_path:\n        # TODO: ScalaPlatform._tool_classpath should capture this and memoize it.\n        # See https://github.com/pantsbuild/pants/issues/6435\n        snapshots.append(\n          self.context._scheduler.capture_snapshots((PathGlobsAndRoot(\n            PathGlobs(scala_path),\n            get_buildroot(),\n          ),))[0]\n        )\n\n      merged_input_digest = self.context._scheduler.merge_directories(\n        tuple(s.directory_digest for s in (snapshots)) + directory_digests\n      )\n\n      # TODO: Extract something common from Executor._create_command to make the command line\n      # TODO: Lean on distribution for the bin/java appending here\n      argv = tuple(['.jdk/bin/java'] + jvm_options + ['-cp', zinc_relpath, Zinc.ZINC_COMPILE_MAIN] + zinc_args)\n      req = ExecuteProcessRequest(\n        argv=argv,\n        input_files=merged_input_digest,\n        output_files=(analysis_cache,),\n        output_directories=(classes_dir,),\n        description=\"zinc compile for {}\".format(ctx.target.address.spec),\n        # TODO: These should always be unicodes\n        jdk_home=text_type(self._zinc.dist.home),\n      )\n      res = self.context.execute_process_synchronously(req, self.name(), [WorkUnitLabel.COMPILER])\n      # TODO: Materialize as a batch in do_compile or somewhere\n      self.context._scheduler.materialize_directories((\n        DirectoryToMaterialize(get_buildroot(), res.output_directory_digest),\n      ))\n\n      # TODO: This should probably return a ClasspathEntry rather than a DirectoryDigest\n      return res.output_directory_digest\n    else:\n      if self.runjava(classpath=[self._zinc.zinc],\n                      main=Zinc.ZINC_COMPILE_MAIN,\n                      jvm_options=jvm_options,\n                      args=zinc_args,\n                      workunit_name=self.name(),\n                      workunit_labels=[WorkUnitLabel.COMPILER],\n                      dist=self._zinc.dist):\n        raise TaskError('Zinc compile failed.')\n\n  def _verify_zinc_classpath(self, classpath, allow_dist=True):\n    def is_outside(path, putative_parent):\n      return os.path.relpath(path, putative_parent).startswith(os.pardir)\n\n    dist = self._zinc.dist\n    for path in classpath:\n      if not os.path.isabs(path):\n        raise TaskError('Classpath entries provided to zinc should be absolute. '\n                        '{} is not.'.format(path))\n\n      if is_outside(path, self.get_options().pants_workdir) and (not allow_dist or is_outside(path, dist.home)):\n        raise TaskError('Classpath entries provided to zinc should be in working directory or '\n                        'part of the JDK. {} is not.'.format(path))\n      if path != os.path.normpath(path):\n        raise TaskError('Classpath entries provided to zinc should be normalized '\n                        '(i.e. without \"..\" and \".\"). {} is not.'.format(path))\n\n  def log_zinc_file(self, analysis_file):\n    self.context.log.debug('Calling zinc on: {} ({})'\n                           .format(analysis_file,\n                                   hash_file(analysis_file).upper()\n                                   if os.path.exists(analysis_file)\n                                   else 'nonexistent'))\n\n  @classmethod\n  def _javac_plugin_args(cls, javac_plugin_map):\n    ret = []\n    for plugin, args in javac_plugin_map.items():\n      for arg in args:\n        if ' ' in arg:\n          # Note: Args are separated by spaces, and there is no way to escape embedded spaces, as\n          # javac's Main does a simple split on these strings.\n          raise TaskError('javac plugin args must not contain spaces '\n                          '(arg {} for plugin {})'.format(arg, plugin))\n      ret.append('-C-Xplugin:{} {}'.format(plugin, ' '.join(args)))\n    return ret\n\n  def _scalac_plugin_args(self, scalac_plugin_map, classpath):\n    if not scalac_plugin_map:\n      return []\n\n    plugin_jar_map = self._find_scalac_plugins(list(scalac_plugin_map.keys()), classpath)\n    ret = []\n    for name, cp_entries in plugin_jar_map.items():\n      # Note that the first element in cp_entries is the one containing the plugin's metadata,\n      # meaning that this is the plugin that will be loaded, even if there happen to be other\n      # plugins in the list of entries (e.g., because this plugin depends on another plugin).\n      ret.append('-S-Xplugin:{}'.format(':'.join(cp_entries)))\n      for arg in scalac_plugin_map[name]:\n        ret.append('-S-P:{}:{}'.format(name, arg))\n    return ret\n\n  def _find_scalac_plugins(self, scalac_plugins, classpath):\n    \"\"\"Returns a map from plugin name to list of plugin classpath entries.\n\n    The first entry in each list is the classpath entry containing the plugin metadata.\n    The rest are the internal transitive deps of the plugin.\n\n    This allows us to have in-repo plugins with dependencies (unlike javac, scalac doesn't load\n    plugins or their deps from the regular classpath, so we have to provide these entries\n    separately, in the -Xplugin: flag).\n\n    Note that we don't currently support external plugins with dependencies, as we can't know which\n    external classpath elements are required, and we'd have to put the entire external classpath\n    on each -Xplugin: flag, which seems excessive.\n    Instead, external plugins should be published as \"fat jars\" (which appears to be the norm,\n    since SBT doesn't support plugins with dependencies anyway).\n    \"\"\"\n    # Allow multiple flags and also comma-separated values in a single flag.\n    plugin_names = {p for val in scalac_plugins for p in val.split(',')}\n    if not plugin_names:\n      return {}\n\n    active_plugins = {}\n    buildroot = get_buildroot()\n\n    cp_product = self.context.products.get_data('runtime_classpath')\n    for classpath_element in classpath:\n      name = self._maybe_get_plugin_name(classpath_element)\n      if name in plugin_names:\n        plugin_target_closure = self._plugin_targets('scalac').get(name, [])\n        # It's important to use relative paths, as the compiler flags get embedded in the zinc\n        # analysis file, and we port those between systems via the artifact cache.\n        rel_classpath_elements = [\n          os.path.relpath(cpe, buildroot) for cpe in\n          ClasspathUtil.internal_classpath(plugin_target_closure, cp_product, self._confs)]\n        # If the plugin is external then rel_classpath_elements will be empty, so we take\n        # just the external jar itself.\n        rel_classpath_elements = rel_classpath_elements or [classpath_element]\n        # Some classpath elements may be repeated, so we allow for that here.\n        if active_plugins.get(name, rel_classpath_elements) != rel_classpath_elements:\n          raise TaskError('Plugin {} defined in {} and in {}'.format(name, active_plugins[name],\n                                                                     classpath_element))\n        active_plugins[name] = rel_classpath_elements\n        if len(active_plugins) == len(plugin_names):\n          # We've found all the plugins, so return now to spare us from processing\n          # of the rest of the classpath for no reason.\n          return active_plugins\n\n    # If we get here we must have unresolved plugins.\n    unresolved_plugins = plugin_names - set(active_plugins.keys())\n    raise TaskError('Could not find requested plugins: {}'.format(list(unresolved_plugins)))\n\n  @classmethod\n  @memoized_method\n  def _maybe_get_plugin_name(cls, classpath_element):\n    \"\"\"If classpath_element is a scalac plugin, returns its name.\n\n    Returns None otherwise.\n    \"\"\"\n    def process_info_file(cp_elem, info_file):\n      plugin_info = ElementTree.parse(info_file).getroot()\n      if plugin_info.tag != 'plugin':\n        raise TaskError('File {} in {} is not a valid scalac plugin descriptor'.format(\n            _SCALAC_PLUGIN_INFO_FILE, cp_elem))\n      return plugin_info.find('name').text\n\n    if os.path.isdir(classpath_element):\n      try:\n        with open(os.path.join(classpath_element, _SCALAC_PLUGIN_INFO_FILE), 'r') as plugin_info_file:\n          return process_info_file(classpath_element, plugin_info_file)\n      except IOError as e:\n        if e.errno != errno.ENOENT:\n          raise\n    else:\n      with open_zip(classpath_element, 'r') as jarfile:\n        try:\n          with closing(jarfile.open(_SCALAC_PLUGIN_INFO_FILE, 'r')) as plugin_info_file:\n            return process_info_file(classpath_element, plugin_info_file)\n        except KeyError:\n          pass\n    return None\n\n\nclass ZincCompile(BaseZincCompile):\n  \"\"\"Compile Scala and Java code to classfiles using Zinc.\"\"\"\n\n  @classmethod\n  def product_types(cls):\n    return ['runtime_classpath', 'zinc_analysis', 'zinc_args']\n\n  def select(self, target):\n    # Require that targets are marked for JVM compilation, to differentiate from\n    # targets owned by the scalajs contrib module.\n    if not isinstance(target, JvmTarget):\n      return False\n    return target.has_sources('.java') or target.has_sources('.scala')\n\n  def select_source(self, source_file_path):\n    return source_file_path.endswith('.java') or source_file_path.endswith('.scala')\n\n  def execute(self):\n    if JvmPlatform.global_instance().get_options().compiler == 'zinc':\n      return super(ZincCompile, self).execute()\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/fakeNetflix/twitter-repo-pants/blob/71467604c5d43df6000995346f245f3768441c03",
        "file_path": "/src/python/pants/goal/context.py",
        "source": "# coding=utf-8\n# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport os\nimport sys\nfrom builtins import filter, object\nfrom collections import defaultdict\nfrom contextlib import contextmanager\n\nfrom twitter.common.collections import OrderedSet\n\nfrom pants.base.build_environment import get_buildroot, get_scm\nfrom pants.base.worker_pool import SubprocPool\nfrom pants.base.workunit import WorkUnit, WorkUnitLabel\nfrom pants.build_graph.target import Target\nfrom pants.engine.isolated_process import FallibleExecuteProcessResult\nfrom pants.goal.products import Products\nfrom pants.goal.workspace import ScmWorkspace\nfrom pants.process.lock import OwnerPrintingInterProcessFileLock\nfrom pants.reporting.report import Report\nfrom pants.source.source_root import SourceRootConfig\n\n\nclass Context(object):\n  \"\"\"Contains the context for a single run of pants.\n\n  Task implementations can access configuration data from pants.ini and any flags they have exposed\n  here as well as information about the targets involved in the run.\n\n  Advanced uses of the context include adding new targets to it for upstream or downstream goals to\n  operate on and mapping of products a goal creates to the targets the products are associated with.\n\n  :API: public\n  \"\"\"\n\n  class Log(object):\n    \"\"\"A logger facade that logs into the pants reporting framework.\"\"\"\n\n    def __init__(self, run_tracker):\n      self._run_tracker = run_tracker\n\n    def debug(self, *msg_elements):\n      self._run_tracker.log(Report.DEBUG, *msg_elements)\n\n    def info(self, *msg_elements):\n      self._run_tracker.log(Report.INFO, *msg_elements)\n\n    def warn(self, *msg_elements):\n      self._run_tracker.log(Report.WARN, *msg_elements)\n\n    def error(self, *msg_elements):\n      self._run_tracker.log(Report.ERROR, *msg_elements)\n\n    def fatal(self, *msg_elements):\n      self._run_tracker.log(Report.FATAL, *msg_elements)\n\n  # TODO: Figure out a more structured way to construct and use context than this big flat\n  # repository of attributes?\n  def __init__(self, options, run_tracker, target_roots,\n               requested_goals=None, target_base=None, build_graph=None,\n               build_file_parser=None, address_mapper=None, console_outstream=None, scm=None,\n               workspace=None, invalidation_report=None, scheduler=None):\n    self._options = options\n    self.build_graph = build_graph\n    self.build_file_parser = build_file_parser\n    self.address_mapper = address_mapper\n    self.run_tracker = run_tracker\n    self._log = self.Log(run_tracker)\n    self._target_base = target_base or Target\n    self._products = Products()\n    self._buildroot = get_buildroot()\n    self._source_roots = SourceRootConfig.global_instance().get_source_roots()\n    self._lock = OwnerPrintingInterProcessFileLock(os.path.join(self._buildroot, '.pants.workdir.file_lock'))\n    self._java_sysprops = None  # Computed lazily.\n    self.requested_goals = requested_goals or []\n    self._console_outstream = console_outstream or sys.stdout\n    self._scm = scm or get_scm()\n    self._workspace = workspace or (ScmWorkspace(self._scm) if self._scm else None)\n    self._replace_targets(target_roots)\n    self._invalidation_report = invalidation_report\n    self._scheduler = scheduler\n\n  @property\n  def options(self):\n    \"\"\"Returns the new-style options.\n\n    :API: public\n    \"\"\"\n    return self._options\n\n  @property\n  def log(self):\n    \"\"\"Returns the preferred logger for goals to use.\n\n    :API: public\n    \"\"\"\n    return self._log\n\n  @property\n  def products(self):\n    \"\"\"Returns the Products manager for the current run.\n\n    :API: public\n    \"\"\"\n    return self._products\n\n  @property\n  def source_roots(self):\n    \"\"\"Returns the :class:`pants.source.source_root.SourceRoots` instance for the current run.\n\n    :API: public\n    \"\"\"\n    return self._source_roots\n\n  @property\n  def target_roots(self):\n    \"\"\"Returns the targets specified on the command line.\n\n    This set is strictly a subset of all targets in play for the run as returned by self.targets().\n    Note that for a command line invocation that uses wildcard selectors : or ::, the targets\n    globbed by the wildcards are considered to be target roots.\n\n    :API: public\n    \"\"\"\n    return self._target_roots\n\n  @property\n  def console_outstream(self):\n    \"\"\"Returns the output stream to write console messages to.\n\n    :API: public\n    \"\"\"\n    return self._console_outstream\n\n  @property\n  def scm(self):\n    \"\"\"Returns the current workspace's scm, if any.\n\n    :API: public\n    \"\"\"\n    return self._scm\n\n  @property\n  def workspace(self):\n    \"\"\"Returns the current workspace, if any.\"\"\"\n    return self._workspace\n\n  @property\n  def invalidation_report(self):\n    return self._invalidation_report\n\n  def __str__(self):\n    ident = Target.identify(self.targets())\n    return 'Context(id:{}, targets:{})'.format(ident, self.targets())\n\n  @contextmanager\n  def executing(self):\n    \"\"\"A contextmanager that sets metrics in the context of a (v1) engine execution.\"\"\"\n    self._set_target_root_count_in_runtracker()\n    yield\n    self.run_tracker.pantsd_stats.set_scheduler_metrics(self._scheduler.metrics())\n    self._set_affected_target_count_in_runtracker()\n\n  def _set_target_root_count_in_runtracker(self):\n    \"\"\"Sets the target root count in the run tracker's daemon stats object.\"\"\"\n    # N.B. `self._target_roots` is always an expanded list of `Target` objects as\n    # provided by `GoalRunner`.\n    target_count = len(self._target_roots)\n    self.run_tracker.pantsd_stats.set_target_root_size(target_count)\n    return target_count\n\n  def _set_affected_target_count_in_runtracker(self):\n    \"\"\"Sets the realized target count in the run tracker's daemon stats object.\"\"\"\n    target_count = len(self.build_graph)\n    self.run_tracker.pantsd_stats.set_affected_targets_size(target_count)\n    return target_count\n\n  def submit_background_work_chain(self, work_chain, parent_workunit_name=None):\n    \"\"\"\n    :API: public\n    \"\"\"\n    background_root_workunit = self.run_tracker.get_background_root_workunit()\n    if parent_workunit_name:\n      # We have to keep this workunit alive until all its child work is done, so\n      # we manipulate the context manually instead of using it as a contextmanager.\n      # This is slightly funky, but the with-context usage is so pervasive and\n      # useful elsewhere that it's worth the funkiness in this one place.\n      workunit_parent_ctx = self.run_tracker.new_workunit_under_parent(\n        name=parent_workunit_name, labels=[WorkUnitLabel.MULTITOOL], parent=background_root_workunit)\n      workunit_parent = workunit_parent_ctx.__enter__()\n      done_hook = lambda: workunit_parent_ctx.__exit__(None, None, None)\n    else:\n      workunit_parent = background_root_workunit  # Run directly under the root.\n      done_hook = None\n    self.run_tracker.background_worker_pool().submit_async_work_chain(\n      work_chain, workunit_parent=workunit_parent, done_hook=done_hook)\n\n  def background_worker_pool(self):\n    \"\"\"Returns the pool to which tasks can submit background work.\n\n    :API: public\n    \"\"\"\n    return self.run_tracker.background_worker_pool()\n\n  def subproc_map(self, f, items):\n    \"\"\"Map function `f` over `items` in subprocesses and return the result.\n\n      :API: public\n\n      :param f: A multiproc-friendly (importable) work function.\n      :param items: A iterable of pickleable arguments to f.\n    \"\"\"\n    try:\n      # Pool.map (and async_map().get() w/o timeout) can miss SIGINT.\n      # See: http://stackoverflow.com/a/1408476, http://bugs.python.org/issue8844\n      # Instead, we map_async(...), wait *with a timeout* until ready, then .get()\n      # NB: in 2.x, wait() with timeout wakes up often to check, burning CPU. Oh well.\n      res = SubprocPool.foreground().map_async(f, items)\n      while not res.ready():\n        res.wait(60)  # Repeatedly wait for up to a minute.\n        if not res.ready():\n          self.log.debug('subproc_map result still not ready...')\n      return res.get()\n    except KeyboardInterrupt:\n      SubprocPool.shutdown(True)\n      raise\n\n  @contextmanager\n  def new_workunit(self, name, labels=None, cmd='', log_config=None):\n    \"\"\"Create a new workunit under the calling thread's current workunit.\n\n    :API: public\n    \"\"\"\n    with self.run_tracker.new_workunit(name=name, labels=labels, cmd=cmd, log_config=log_config) as workunit:\n      yield workunit\n\n  def acquire_lock(self):\n    \"\"\" Acquire the global lock for the root directory associated with this context. When\n    a goal requires serialization, it will call this to acquire the lock.\n\n    :API: public\n    \"\"\"\n    if self.options.for_global_scope().lock:\n      if not self._lock.acquired:\n        self._lock.acquire()\n\n  def release_lock(self):\n    \"\"\"Release the global lock if it's held.\n    Returns True if the lock was held before this call.\n\n    :API: public\n    \"\"\"\n    if not self._lock.acquired:\n      return False\n    else:\n      self._lock.release()\n      return True\n\n  def is_unlocked(self):\n    \"\"\"Whether the global lock object is actively holding the lock.\n\n    :API: public\n    \"\"\"\n    return not self._lock.acquired\n\n  def _replace_targets(self, target_roots):\n    # Replaces all targets in the context with the given roots and their transitive dependencies.\n    #\n    # If another task has already retrieved the current targets, mutable state may have been\n    # initialized somewhere, making it now unsafe to replace targets. Thus callers of this method\n    # must know what they're doing!\n    #\n    # TODO(John Sirois): This currently has only 1 use (outside ContextTest) in pantsbuild/pants and\n    # only 1 remaining known use case in the Foursquare codebase that will be able to go away with\n    # the post RoundEngine engine - kill the method at that time.\n    self._target_roots = list(target_roots)\n\n  def add_new_target(self, address, target_type, target_base=None, dependencies=None,\n                     derived_from=None, **kwargs):\n    \"\"\"Creates a new target, adds it to the context and returns it.\n\n    This method ensures the target resolves files against the given target_base, creating the\n    directory if needed and registering a source root.\n\n    :API: public\n    \"\"\"\n    rel_target_base = target_base or address.spec_path\n    abs_target_base = os.path.join(get_buildroot(), rel_target_base)\n    if not os.path.exists(abs_target_base):\n      os.makedirs(abs_target_base)\n      # TODO: Adding source roots on the fly like this is yucky, but hopefully this\n      # method will go away entirely under the new engine. It's primarily used for injecting\n      # synthetic codegen targets, and that isn't how codegen will work in the future.\n    if not self.source_roots.find_by_path(rel_target_base):\n      # TODO: Set the lang and root category (source/test/thirdparty) based on the target type?\n      self.source_roots.add_source_root(rel_target_base)\n    if dependencies:\n      dependencies = [dep.address for dep in dependencies]\n\n    self.build_graph.inject_synthetic_target(address=address,\n                                             target_type=target_type,\n                                             dependencies=dependencies,\n                                             derived_from=derived_from,\n                                             **kwargs)\n    new_target = self.build_graph.get_target(address)\n\n    return new_target\n\n  def targets(self, predicate=None, **kwargs):\n    \"\"\"Selects targets in-play in this run from the target roots and their transitive dependencies.\n\n    Also includes any new synthetic targets created from the target roots or their transitive\n    dependencies during the course of the run.\n\n    See Target.closure_for_targets for remaining parameters.\n\n    :API: public\n\n    :param predicate: If specified, the predicate will be used to narrow the scope of targets\n                      returned.\n    :param bool postorder: `True` to gather transitive dependencies with a postorder traversal;\n                          `False` or preorder by default.\n    :returns: A list of matching targets.\n    \"\"\"\n    target_set = self._collect_targets(self.target_roots, **kwargs)\n\n    synthetics = OrderedSet()\n    for synthetic_address in self.build_graph.synthetic_addresses:\n      if self.build_graph.get_concrete_derived_from(synthetic_address) in target_set:\n        synthetics.add(self.build_graph.get_target(synthetic_address))\n    target_set.update(self._collect_targets(synthetics, **kwargs))\n\n    return list(filter(predicate, target_set))\n\n  def _collect_targets(self, root_targets, **kwargs):\n    return Target.closure_for_targets(\n      target_roots=root_targets,\n      **kwargs\n    )\n\n  def dependents(self, on_predicate=None, from_predicate=None):\n    \"\"\"Returns  a map from targets that satisfy the from_predicate to targets they depend on that\n      satisfy the on_predicate.\n\n    :API: public\n    \"\"\"\n    core = set(self.targets(on_predicate))\n    dependees = defaultdict(set)\n    for target in self.targets(from_predicate):\n      for dependency in target.dependencies:\n        if dependency in core:\n          dependees[target].add(dependency)\n    return dependees\n\n  def resolve(self, spec):\n    \"\"\"Returns an iterator over the target(s) the given address points to.\n\n    :API: public\n    \"\"\"\n    return self.build_graph.resolve(spec)\n\n  def scan(self, root=None):\n    \"\"\"Scans and parses all BUILD files found under ``root``.\n\n    Only BUILD files found under ``root`` are parsed as roots in the graph, but any dependencies of\n    targets parsed in the root tree's BUILD files will be followed and this may lead to BUILD files\n    outside of ``root`` being parsed and included in the returned build graph.\n\n    :API: public\n\n    :param string root: The path to scan; by default, the build root.\n    :returns: A new build graph encapsulating the targets found.\n    \"\"\"\n    build_graph = self.build_graph.clone_new()\n    for address in self.address_mapper.scan_addresses(root):\n      build_graph.inject_address_closure(address)\n    return build_graph\n\n  def execute_process_synchronously(self, execute_process_request, name, labels=None):\n    \"\"\"Executes a process (possibly remotely), and returns information about its output.\n\n    :param execute_process_request: The ExecuteProcessRequest to run.\n    :param name: A descriptive name representing the process being executed.\n    :param labels: A tuple of WorkUnitLabels.\n    :return: An ExecuteProcessResult with information about the execution.\n\n    Note that this is an unstable, experimental API, which is subject to change with no notice.\n    \"\"\"\n    with self.new_workunit(\n      name=name,\n      labels=labels,\n      cmd=' '.join(execute_process_request.argv),\n    ) as workunit:\n      result = self._scheduler.product_request(FallibleExecuteProcessResult, [execute_process_request])[0]\n      workunit.output(\"stdout\").write(result.stdout)\n      workunit.output(\"stderr\").write(result.stderr)\n      workunit.set_outcome(WorkUnit.FAILURE if result.exit_code else WorkUnit.SUCCESS)\n      return result\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/fakeNetflix/twitter-repo-pants/blob/71467604c5d43df6000995346f245f3768441c03",
        "file_path": "/src/python/pants/java/distribution/distribution.py",
        "source": "# coding=utf-8\n# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport itertools\nimport logging\nimport os\nimport pkgutil\nimport plistlib\nfrom abc import abstractproperty\nfrom builtins import object, open, str\nfrom collections import namedtuple\nfrom contextlib import contextmanager\n\nfrom future.utils import PY3\nfrom six import string_types\n\nfrom pants.base.revision import Revision\nfrom pants.java.util import execute_java, execute_java_async\nfrom pants.subsystem.subsystem import Subsystem\nfrom pants.util.contextutil import temporary_dir\nfrom pants.util.memo import memoized_method, memoized_property\nfrom pants.util.meta import AbstractClass\nfrom pants.util.osutil import OS_ALIASES, normalize_os_name\nfrom pants.util.process_handler import subprocess\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef _parse_java_version(name, version):\n  # Java version strings have been well defined since release 1.3.1 as defined here:\n  #  http://www.oracle.com/technetwork/java/javase/versioning-naming-139433.html\n  # These version strings comply with semver except that the traditional pre-release semver\n  # slot (the 4th) can be delimited by an _ in the case of update releases of the jdk.\n  # We accommodate that difference here using lenient parsing.\n  # We also accommodate specification versions, which just have major and minor\n  # components; eg: `1.8`.  These are useful when specifying constraints a distribution must\n  # satisfy; eg: to pick any 1.8 java distribution: '1.8' <= version <= '1.8.99'\n  if isinstance(version, string_types):\n    version = Revision.lenient(version)\n  if version and not isinstance(version, Revision):\n    raise ValueError('{} must be a string or a Revision object, given: {}'.format(name, version))\n  return version\n\n\nclass Distribution(object):\n  \"\"\"Represents a java distribution - either a JRE or a JDK installed on the local system.\n\n  In particular provides access to the distribution's binaries; ie: java while ensuring basic\n  constraints are met.  For example a minimum version can be specified if you know need to compile\n  source code or run bytecode that exercise features only available in that version forward.\n\n  :API: public\n\n  TODO(John Sirois): This class has a broken API, its not reasonably useful with no methods exposed.\n  Expose reasonable methods: https://github.com/pantsbuild/pants/issues/3263\n  \"\"\"\n\n  class Error(Exception):\n    \"\"\"Indicates an invalid java distribution.\"\"\"\n\n  @staticmethod\n  def _is_executable(path):\n    return os.path.isfile(path) and os.access(path, os.X_OK)\n\n  def __init__(self, home_path=None, bin_path=None, minimum_version=None, maximum_version=None,\n               jdk=False):\n    \"\"\"Creates a distribution wrapping the given `home_path` or `bin_path`.\n\n    Only one of `home_path` or `bin_path` should be supplied.\n\n    :param string home_path: the path to the java distribution's home dir\n    :param string bin_path: the path to the java distribution's bin dir\n    :param minimum_version: a modified semantic version string or else a Revision object\n    :param maximum_version: a modified semantic version string or else a Revision object\n    :param bool jdk: ``True`` to require the distribution be a JDK vs a JRE\n    \"\"\"\n    if home_path and not os.path.isdir(home_path):\n      raise ValueError('The specified java home path is invalid: {}'.format(home_path))\n    if bin_path and not os.path.isdir(bin_path):\n      raise ValueError('The specified binary path is invalid: {}'.format(bin_path))\n    if not bool(home_path) ^ bool(bin_path):\n      raise ValueError('Exactly one of home path or bin path should be supplied, given: '\n                       'home_path={} bin_path={}'.format(home_path, bin_path))\n\n    self._home = home_path\n    self._bin_path = bin_path or (os.path.join(home_path, 'bin') if home_path else '/usr/bin')\n\n    self._minimum_version = _parse_java_version(\"minimum_version\", minimum_version)\n    self._maximum_version = _parse_java_version(\"maximum_version\", maximum_version)\n    self._jdk = jdk\n    self._is_jdk = False\n    self._system_properties = None\n    self._validated_binaries = {}\n\n  @property\n  def jdk(self):\n    self.validate()\n    return self._is_jdk\n\n  @property\n  def system_properties(self):\n    \"\"\"Returns a dict containing the system properties of this java distribution.\"\"\"\n    return dict(self._get_system_properties(self.java))\n\n  @property\n  def version(self):\n    \"\"\"Returns the distribution version.\n\n    Raises Distribution.Error if this distribution is not valid according to the configured\n    constraints.\n    \"\"\"\n    return self._get_version(self.java)\n\n  def find_libs(self, names):\n    \"\"\"Looks for jars in the distribution lib folder(s).\n\n    If the distribution is a JDK, both the `lib` and `jre/lib` dirs will be scanned.\n    The endorsed and extension dirs are not checked.\n\n    :param list names: jar file names\n    :return: list of paths to requested libraries\n    :raises: `Distribution.Error` if any of the jars could not be found.\n    \"\"\"\n    def collect_existing_libs():\n      def lib_paths():\n        yield os.path.join(self.home, 'lib')\n        if self.jdk:\n          yield os.path.join(self.home, 'jre', 'lib')\n\n      for name in names:\n        for path in lib_paths():\n          lib_path = os.path.join(path, name)\n          if os.path.exists(lib_path):\n            yield lib_path\n            break\n        else:\n          raise Distribution.Error('Failed to locate {} library'.format(name))\n\n    return list(collect_existing_libs())\n\n  @property\n  def home(self):\n    \"\"\"Returns the distribution JAVA_HOME.\"\"\"\n    if not self._home:\n      home = self._get_system_properties(self.java)['java.home']\n      # The `jre/bin/java` executable in a JDK distribution will report `java.home` as the jre dir,\n      # so we check for this and re-locate to the containing jdk dir when present.\n      if os.path.basename(home) == 'jre':\n        jdk_dir = os.path.dirname(home)\n        if self._is_executable(os.path.join(jdk_dir, 'bin', 'javac')):\n          home = jdk_dir\n      self._home = home\n    return self._home\n\n  @property\n  def real_home(self):\n    \"\"\"Real path to the distribution java.home (resolving links).\"\"\"\n    return os.path.realpath(self.home)\n\n  @property\n  def java(self):\n    \"\"\"Returns the path to this distribution's java command.\n\n    If this distribution has no valid java command raises Distribution.Error.\n    \"\"\"\n    return self.binary('java')\n\n  def binary(self, name):\n    \"\"\"Returns the path to the command of the given name for this distribution.\n\n    For example: ::\n\n        >>> d = Distribution()\n        >>> jar = d.binary('jar')\n        >>> jar\n        '/usr/bin/jar'\n        >>>\n\n    If this distribution has no valid command of the given name raises Distribution.Error.\n    If this distribution is a JDK checks both `bin` and `jre/bin` for the binary.\n    \"\"\"\n    if not isinstance(name, str):\n      raise ValueError('name must be a binary name, given {} of type {}'.format(name, type(name)))\n    self.validate()\n    return self._validated_executable(name)\n\n  def validate(self):\n    \"\"\"Validates this distribution against its configured constraints.\n\n    Raises Distribution.Error if this distribution is not valid according to the configured\n    constraints.\n    \"\"\"\n    if self._validated_binaries:\n      return\n\n    with self._valid_executable('java') as java:\n      if self._minimum_version:\n        version = self._get_version(java)\n        if version < self._minimum_version:\n          raise self.Error('The java distribution at {} is too old; expecting at least {} and'\n                           ' got {}'.format(java, self._minimum_version, version))\n      if self._maximum_version:\n        version = self._get_version(java)\n        if version > self._maximum_version:\n          raise self.Error('The java distribution at {} is too new; expecting no older than'\n                           ' {} and got {}'.format(java, self._maximum_version, version))\n\n    # We might be a JDK discovered by the embedded jre `java` executable.\n    # If so reset the bin path to the true JDK home dir for full access to all binaries.\n    self._bin_path = os.path.join(self.home, 'bin')\n\n    try:\n      self._validated_executable('javac')  # Calling purely for the check and cache side effects\n      self._is_jdk = True\n    except self.Error as e:\n      if self._jdk:\n        logger.debug('Failed to validate javac executable. Please check you have a JDK '\n                      'installed. Original error: {}'.format(e))\n        raise\n\n  def execute_java(self, *args, **kwargs):\n    return execute_java(*args, distribution=self, **kwargs)\n\n  def execute_java_async(self, *args, **kwargs):\n    return execute_java_async(*args, distribution=self, **kwargs)\n\n  @memoized_method\n  def _get_version(self, java):\n    return _parse_java_version('java.version', self._get_system_properties(java)['java.version'])\n\n  def _get_system_properties(self, java):\n    if not self._system_properties:\n      with temporary_dir() as classpath:\n        with open(os.path.join(classpath, 'SystemProperties.class'), 'w+b') as fp:\n          fp.write(pkgutil.get_data(__name__, 'SystemProperties.class'))\n        cmd = [java, '-cp', classpath, 'SystemProperties']\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = process.communicate()\n        if process.returncode != 0:\n          raise self.Error('Failed to determine java system properties for {} with {} - exit code'\n                           ' {}: {}'.format(java, ' '.join(cmd), process.returncode, stderr.decode('utf-8')))\n\n      props = {}\n      for line in stdout.decode('utf-8').split(os.linesep):\n        key, _, val = line.partition('=')\n        props[key] = val\n      self._system_properties = props\n\n    return self._system_properties\n\n  def _validate_executable(self, name):\n    def bin_paths():\n      yield self._bin_path\n      if self._is_jdk:\n        yield os.path.join(self.home, 'jre', 'bin')\n\n    for bin_path in bin_paths():\n      exe = os.path.join(bin_path, name)\n      if self._is_executable(exe):\n        return exe\n    raise self.Error('Failed to locate the {} executable, {} does not appear to be a'\n                     ' valid {} distribution'.format(name, self, 'JDK' if self._jdk else 'JRE'))\n\n  def _validated_executable(self, name):\n    exe = self._validated_binaries.get(name)\n    if not exe:\n      exe = self._validate_executable(name)\n      self._validated_binaries[name] = exe\n    return exe\n\n  @contextmanager\n  def _valid_executable(self, name):\n    exe = self._validate_executable(name)\n    yield exe\n    self._validated_binaries[name] = exe\n\n  def __repr__(self):\n    return ('Distribution({!r}, minimum_version={!r}, maximum_version={!r} jdk={!r})'.format(\n            self._bin_path, self._minimum_version, self._maximum_version, self._jdk))\n\n\nclass _DistributionEnvironment(AbstractClass):\n  class Location(namedtuple('Location', ['home_path', 'bin_path'])):\n    \"\"\"Represents the location of a java distribution.\"\"\"\n\n    @classmethod\n    def from_home(cls, home):\n      \"\"\"Creates a location given the JAVA_HOME directory.\n\n      :param string home: The path of the JAVA_HOME directory.\n      :returns: The java distribution location.\n      \"\"\"\n      return cls(home_path=home, bin_path=None)\n\n    @classmethod\n    def from_bin(cls, bin_path):\n      \"\"\"Creates a location given the `java` executable parent directory.\n\n      :param string bin_path: The parent path of the `java` executable.\n      :returns: The java distribution location.\n      \"\"\"\n      return cls(home_path=None, bin_path=bin_path)\n\n  @abstractproperty\n  def jvm_locations(self):\n    \"\"\"Return the jvm locations discovered in this environment.\n\n    :returns: An iterator over all discovered jvm locations.\n    :rtype: iterator of :class:`DistributionEnvironment.Location`\n    \"\"\"\n\n\nclass _EnvVarEnvironment(_DistributionEnvironment):\n  @property\n  def jvm_locations(self):\n    def env_home(home_env_var):\n      home = os.environ.get(home_env_var)\n      return self.Location.from_home(home) if home else None\n\n    jdk_home = env_home('JDK_HOME')\n    if jdk_home:\n      yield jdk_home\n\n    java_home = env_home('JAVA_HOME')\n    if java_home:\n      yield java_home\n\n    search_path = os.environ.get('PATH')\n    if search_path:\n      for bin_path in search_path.strip().split(os.pathsep):\n        yield self.Location.from_bin(bin_path)\n\n\nclass _OSXEnvironment(_DistributionEnvironment):\n  _OSX_JAVA_HOME_EXE = '/usr/libexec/java_home'\n\n  @classmethod\n  def standard(cls):\n    return cls(cls._OSX_JAVA_HOME_EXE)\n\n  def __init__(self, osx_java_home_exe):\n    self._osx_java_home_exe = osx_java_home_exe\n\n  @property\n  def jvm_locations(self):\n    # OSX will have a java_home tool that can be used to locate a unix-compatible java home dir.\n    #\n    # See:\n    #   https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man1/java_home.1.html\n    #\n    # The `--xml` output looks like so:\n    # <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    # <!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\"\n    #                        \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n    # <plist version=\"1.0\">\n    #   <array>\n    #     <dict>\n    #       ...\n    #       <key>JVMHomePath</key>\n    #       <string>/Library/Java/JavaVirtualMachines/jdk1.7.0_45.jdk/Contents/Home</string>\n    #       ...\n    #     </dict>\n    #     ...\n    #   </array>\n    # </plist>\n    if os.path.exists(self._osx_java_home_exe):\n      try:\n        plist = subprocess.check_output([self._osx_java_home_exe, '--failfast', '--xml'])\n        plist_results = plistlib.loads(plist) if PY3 else plistlib.readPlistFromString(plist)\n        for distribution in plist_results:\n          home = distribution['JVMHomePath']\n          yield self.Location.from_home(home)\n      except subprocess.CalledProcessError:\n        pass\n\n\nclass _LinuxEnvironment(_DistributionEnvironment):\n  # The `/usr/lib/jvm` dir is a common target of packages built for redhat and debian as well as\n  # other more exotic distributions.  SUSE uses lib64\n  _STANDARD_JAVA_DIST_DIRS = ('/usr/lib/jvm', '/usr/lib64/jvm')\n\n  @classmethod\n  def standard(cls):\n    return cls(*cls._STANDARD_JAVA_DIST_DIRS)\n\n  def __init__(self, *java_dist_dirs):\n    if len(java_dist_dirs) == 0:\n      raise ValueError('Expected at least 1 java dist dir.')\n    self._java_dist_dirs = java_dist_dirs\n\n  @property\n  def jvm_locations(self):\n    for java_dist_dir in self._java_dist_dirs:\n      if os.path.isdir(java_dist_dir):\n        for path in os.listdir(java_dist_dir):\n          home = os.path.join(java_dist_dir, path)\n          if os.path.isdir(home):\n            yield self.Location.from_home(home)\n\n\nclass _ExplicitEnvironment(_DistributionEnvironment):\n  def __init__(self, *homes):\n    self._homes = homes\n\n  @property\n  def jvm_locations(self):\n    for home in self._homes:\n      yield self.Location.from_home(home)\n\n\nclass _UnknownEnvironment(_DistributionEnvironment):\n  def __init__(self, *possible_environments):\n    super(_DistributionEnvironment, self).__init__()\n    if len(possible_environments) < 2:\n      raise ValueError('At least two possible environments must be supplied.')\n    self._possible_environments = possible_environments\n\n  @property\n  def jvm_locations(self):\n    return itertools.chain(*(pe.jvm_locations for pe in self._possible_environments))\n\n\nclass _Locator(object):\n  class Error(Distribution.Error):\n    \"\"\"Error locating a java distribution.\"\"\"\n\n  def __init__(self, distribution_environment, minimum_version=None, maximum_version=None):\n    self._cache = {}\n    self._distribution_environment = distribution_environment\n    self._minimum_version = minimum_version\n    self._maximum_version = maximum_version\n\n  def _scan_constraint_match(self, minimum_version, maximum_version, jdk):\n    \"\"\"Finds a cached version matching the specified constraints\n\n    :param Revision minimum_version: minimum jvm version to look for (eg, 1.7).\n    :param Revision maximum_version: maximum jvm version to look for (eg, 1.7.9999).\n    :param bool jdk: whether the found java distribution is required to have a jdk.\n    :return: the Distribution, or None if no matching distribution is in the cache.\n    :rtype: :class:`pants.java.distribution.Distribution`\n    \"\"\"\n\n    for dist in self._cache.values():\n      if minimum_version and dist.version < minimum_version:\n        continue\n      if maximum_version and dist.version > maximum_version:\n        continue\n      if jdk and not dist.jdk:\n        continue\n      return dist\n\n  def locate(self, minimum_version=None, maximum_version=None, jdk=False):\n    \"\"\"Finds a java distribution that meets the given constraints and returns it.\n\n    First looks for a cached version that was previously located, otherwise calls locate().\n    :param minimum_version: minimum jvm version to look for (eg, 1.7).\n                            The stricter of this and `--jvm-distributions-minimum-version` is used.\n    :param maximum_version: maximum jvm version to look for (eg, 1.7.9999).\n                            The stricter of this and `--jvm-distributions-maximum-version` is used.\n    :param bool jdk: whether the found java distribution is required to have a jdk.\n    :return: the Distribution.\n    :rtype: :class:`Distribution`\n    :raises: :class:`Distribution.Error` if no suitable java distribution could be found.\n    \"\"\"\n\n    def _get_stricter_version(a, b, name, stricter):\n      version_a = _parse_java_version(name, a)\n      version_b = _parse_java_version(name, b)\n      if version_a is None:\n        return version_b\n      if version_b is None:\n        return version_a\n      return stricter(version_a, version_b)\n\n    # Take the tighter constraint of method args and subsystem options.\n    minimum_version = _get_stricter_version(minimum_version,\n                                            self._minimum_version,\n                                            \"minimum_version\",\n                                            max)\n    maximum_version = _get_stricter_version(maximum_version,\n                                            self._maximum_version,\n                                            \"maximum_version\",\n                                            min)\n\n    key = (minimum_version, maximum_version, jdk)\n    dist = self._cache.get(key)\n    if not dist:\n      dist = self._scan_constraint_match(minimum_version, maximum_version, jdk)\n      if not dist:\n        dist = self._locate(minimum_version=minimum_version,\n                            maximum_version=maximum_version,\n                            jdk=jdk)\n      self._cache[key] = dist\n    return dist\n\n  def _locate(self, minimum_version=None, maximum_version=None, jdk=False):\n    \"\"\"Finds a java distribution that meets any given constraints and returns it.\n\n    :param minimum_version: minimum jvm version to look for (eg, 1.7).\n    :param maximum_version: maximum jvm version to look for (eg, 1.7.9999).\n    :param bool jdk: whether the found java distribution is required to have a jdk.\n    :return: the located Distribution.\n    :rtype: :class:`Distribution`\n    :raises: :class:`Distribution.Error` if no suitable java distribution could be found.\n    \"\"\"\n    for location in itertools.chain(self._distribution_environment.jvm_locations):\n      try:\n        dist = Distribution(home_path=location.home_path,\n                            bin_path=location.bin_path,\n                            minimum_version=minimum_version,\n                            maximum_version=maximum_version,\n                            jdk=jdk)\n        dist.validate()\n        logger.debug('Located {} for constraints: minimum_version {}, maximum_version {}, jdk {}'\n                     .format(dist, minimum_version, maximum_version, jdk))\n        return dist\n      except (ValueError, Distribution.Error) as e:\n        logger.debug('{} is not a valid distribution because: {}'\n                     .format(location.home_path, str(e)))\n        pass\n\n    if (minimum_version is not None\n        and maximum_version is not None\n        and maximum_version < minimum_version):\n      error_format = ('Pants configuration/options led to impossible constraints for {} '\n                      'distribution: minimum_version {}, maximum_version {}')\n    else:\n      error_format = ('Failed to locate a {} distribution with minimum_version {}, '\n                      'maximum_version {}')\n    raise self.Error(error_format.format('JDK' if jdk else 'JRE', minimum_version, maximum_version))\n\n\nclass DistributionLocator(Subsystem):\n  \"\"\"Subsystem that knows how to look up a java Distribution.\n\n  Distributions are searched for in the following order by default:\n\n  1. Paths listed for this operating system in the `--jvm-distributions-paths` map.\n  2. JDK_HOME/JAVA_HOME\n  3. PATH\n  4. Likely locations on the file system such as `/usr/lib/jvm` on Linux machines.\n\n  :API: public\n  \"\"\"\n\n  class Error(Distribution.Error):\n    \"\"\"Error locating a java distribution.\n\n    :API: public\n    \"\"\"\n\n  @classmethod\n  def cached(cls, minimum_version=None, maximum_version=None, jdk=False):\n    \"\"\"Finds a java distribution that meets the given constraints and returns it.\n\n    :API: public\n\n    First looks for a cached version that was previously located, otherwise calls locate().\n    :param minimum_version: minimum jvm version to look for (eg, 1.7).\n                            The stricter of this and `--jvm-distributions-minimum-version` is used.\n    :param maximum_version: maximum jvm version to look for (eg, 1.7.9999).\n                            The stricter of this and `--jvm-distributions-maximum-version` is used.\n    :param bool jdk: whether the found java distribution is required to have a jdk.\n    :return: the Distribution.\n    :rtype: :class:`Distribution`\n    :raises: :class:`Distribution.Error` if no suitable java distribution could be found.\n    \"\"\"\n    try:\n      return cls.global_instance()._locator().locate(\n          minimum_version=minimum_version,\n          maximum_version=maximum_version,\n          jdk=jdk)\n    except _Locator.Error as e:\n      raise cls.Error('Problem locating a java distribution: {}'.format(e))\n\n  options_scope = 'jvm-distributions'\n\n  @classmethod\n  def register_options(cls, register):\n    super(DistributionLocator, cls).register_options(register)\n    human_readable_os_aliases = ', '.join('{}: [{}]'.format(str(key), ', '.join(sorted(val)))\n                                          for key, val in OS_ALIASES.items())\n    register('--paths', advanced=True, type=dict,\n             help='Map of os names to lists of paths to jdks. These paths will be searched before '\n                  'everything else (before the JDK_HOME, JAVA_HOME, PATH environment variables) '\n                  'when locating a jvm to use. The same OS can be specified via several different '\n                  'aliases, according to this map: {}'.format(human_readable_os_aliases))\n    register('--minimum-version', advanced=True, help='Minimum version of the JVM pants will use')\n    register('--maximum-version', advanced=True, help='Maximum version of the JVM pants will use')\n\n  def all_jdk_paths(self):\n    \"\"\"Get all explicitly configured JDK paths.\n\n    :return: mapping of os name -> list of jdk_paths\n    :rtype: dict of string -> list of string\n    \"\"\"\n    return self._normalized_jdk_paths\n\n  @memoized_method\n  def _locator(self):\n    return self._create_locator()\n\n  @memoized_property\n  def _normalized_jdk_paths(self):\n    normalized = {}\n    jdk_paths = self.get_options().paths or {}\n    for name, paths in sorted(jdk_paths.items()):\n      rename = normalize_os_name(name)\n      if rename in normalized:\n        logger.warning('Multiple OS names alias to \"{}\"; combining results.'.format(rename))\n        normalized[rename].extend(paths)\n      else:\n        normalized[rename] = paths\n    return normalized\n\n  def _get_explicit_jdk_paths(self):\n    if not self._normalized_jdk_paths:\n      return ()\n    os_name = normalize_os_name(os.uname()[0].lower())\n    if os_name not in self._normalized_jdk_paths:\n      logger.warning('--jvm-distributions-paths was specified, but has no entry for \"{}\".'\n                     .format(os_name))\n    return self._normalized_jdk_paths.get(os_name, ())\n\n  def _create_locator(self):\n    homes = self._get_explicit_jdk_paths()\n    environment = _UnknownEnvironment(\n        _ExplicitEnvironment(*homes),\n        _UnknownEnvironment(\n            _EnvVarEnvironment(),\n            _LinuxEnvironment.standard(),\n            _OSXEnvironment.standard()\n        )\n    )\n    return _Locator(environment,\n                    self.get_options().minimum_version,\n                    self.get_options().maximum_version)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/fakeNetflix/twitter-repo-pants/blob/91a88ba0d3fe8731717cd8d64424dc2b90952162",
        "file_path": "/src/python/pants/backend/python/register.py",
        "source": "# coding=utf-8\n# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nfrom pants.backend.python.pants_requirement import PantsRequirement\nfrom pants.backend.python.python_artifact import PythonArtifact\nfrom pants.backend.python.python_requirement import PythonRequirement\nfrom pants.backend.python.python_requirements import PythonRequirements\nfrom pants.backend.python.rules import inject_init, python_test_runner\nfrom pants.backend.python.targets.python_app import PythonApp\nfrom pants.backend.python.targets.python_binary import PythonBinary\nfrom pants.backend.python.targets.python_distribution import PythonDistribution\nfrom pants.backend.python.targets.python_library import PythonLibrary\nfrom pants.backend.python.targets.python_requirement_library import PythonRequirementLibrary\nfrom pants.backend.python.targets.python_tests import PythonTests\nfrom pants.backend.python.targets.unpacked_whls import UnpackedWheels\nfrom pants.backend.python.tasks.build_local_python_distributions import \\\n  BuildLocalPythonDistributions\nfrom pants.backend.python.tasks.gather_sources import GatherSources\nfrom pants.backend.python.tasks.isort_prep import IsortPrep\nfrom pants.backend.python.tasks.isort_run import IsortRun\nfrom pants.backend.python.tasks.local_python_distribution_artifact import \\\n  LocalPythonDistributionArtifact\nfrom pants.backend.python.tasks.pytest_prep import PytestPrep\nfrom pants.backend.python.tasks.pytest_run import PytestRun\nfrom pants.backend.python.tasks.python_binary_create import PythonBinaryCreate\nfrom pants.backend.python.tasks.python_bundle import PythonBundle\nfrom pants.backend.python.tasks.python_repl import PythonRepl\nfrom pants.backend.python.tasks.python_run import PythonRun\nfrom pants.backend.python.tasks.resolve_requirements import ResolveRequirements\nfrom pants.backend.python.tasks.select_interpreter import SelectInterpreter\nfrom pants.backend.python.tasks.setup_py import SetupPy\nfrom pants.backend.python.tasks.unpack_wheels import UnpackWheels\nfrom pants.build_graph.build_file_aliases import BuildFileAliases\nfrom pants.build_graph.resources import Resources\nfrom pants.goal.task_registrar import TaskRegistrar as task\n\n\ndef build_file_aliases():\n  return BuildFileAliases(\n    targets={\n      PythonApp.alias(): PythonApp,\n      PythonBinary.alias(): PythonBinary,\n      PythonLibrary.alias(): PythonLibrary,\n      PythonTests.alias(): PythonTests,\n      PythonDistribution.alias(): PythonDistribution,\n      'python_requirement_library': PythonRequirementLibrary,\n      Resources.alias(): Resources,\n      UnpackedWheels.alias(): UnpackedWheels,\n    },\n    objects={\n      'python_requirement': PythonRequirement,\n      'python_artifact': PythonArtifact,\n      'setup_py': PythonArtifact,\n    },\n    context_aware_object_factories={\n      'python_requirements': PythonRequirements,\n      PantsRequirement.alias: PantsRequirement,\n    }\n  )\n\n\ndef register_goals():\n  task(name='interpreter', action=SelectInterpreter).install('pyprep')\n  task(name='build-local-dists', action=BuildLocalPythonDistributions).install('pyprep')\n  task(name='requirements', action=ResolveRequirements).install('pyprep')\n  task(name='sources', action=GatherSources).install('pyprep')\n  task(name='py', action=PythonRun).install('run')\n  task(name='pytest-prep', action=PytestPrep).install('test')\n  task(name='pytest', action=PytestRun).install('test')\n  task(name='py', action=PythonRepl).install('repl')\n  task(name='setup-py', action=SetupPy).install()\n  task(name='py', action=PythonBinaryCreate).install('binary')\n  task(name='py-wheels', action=LocalPythonDistributionArtifact).install('binary')\n  task(name='isort-prep', action=IsortPrep).install('fmt')\n  task(name='isort', action=IsortRun).install('fmt')\n  task(name='py', action=PythonBundle).install('bundle')\n  task(name='unpack-wheels', action=UnpackWheels).install()\n\n\ndef rules():\n  return inject_init.rules() + python_test_runner.rules()\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/fakeNetflix/twitter-repo-pants/blob/91a88ba0d3fe8731717cd8d64424dc2b90952162",
        "file_path": "/src/python/pants/backend/python/rules/python_test_runner.py",
        "source": "# coding=utf-8\n# Copyright 2018 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport os\nimport sys\nfrom builtins import str\n\nfrom future.utils import text_type\n\nfrom pants.backend.python.rules.inject_init import InjectedInitDigest\nfrom pants.backend.python.subsystems.pytest import PyTest\nfrom pants.backend.python.subsystems.python_setup import PythonSetup\nfrom pants.engine.fs import (Digest, DirectoriesToMerge, DirectoryWithPrefixToStrip, Snapshot,\n                             UrlToFetch)\nfrom pants.engine.isolated_process import (ExecuteProcessRequest, ExecuteProcessResult,\n                                           FallibleExecuteProcessResult)\nfrom pants.engine.legacy.graph import BuildFileAddresses, TransitiveHydratedTargets\nfrom pants.engine.legacy.structs import PythonTestsAdaptor\nfrom pants.engine.rules import UnionRule, optionable_rule, rule\nfrom pants.engine.selectors import Get\nfrom pants.rules.core.core_test_model import Status, TestResult, TestTarget\nfrom pants.source.source_root import SourceRootConfig\n\n\ndef parse_interpreter_constraints(python_setup, python_target_adaptors):\n  constraints = {\n    constraint\n    for target_adaptor in python_target_adaptors\n    for constraint in python_setup.compatibility_or_constraints(\n      getattr(target_adaptor, 'compatibility', None)\n    )\n  }\n  constraints_args = []\n  for constraint in sorted(constraints):\n    constraints_args.extend([\"--interpreter-constraint\", text_type(constraint)])\n  return constraints_args\n\n\n# TODO: Support resources\n# TODO(7697): Use a dedicated rule for removing the source root prefix, so that this rule\n# does not have to depend on SourceRootConfig.\n@rule(TestResult, [PythonTestsAdaptor, PyTest, PythonSetup, SourceRootConfig])\ndef run_python_test(test_target, pytest, python_setup, source_root_config):\n  \"\"\"Runs pytest for one target.\"\"\"\n\n  # TODO: Inject versions and digests here through some option, rather than hard-coding it.\n  url = 'https://github.com/pantsbuild/pex/releases/download/v1.6.6/pex'\n  digest = Digest('61bb79384db0da8c844678440bd368bcbfac17bbdb865721ad3f9cb0ab29b629', 1826945)\n  pex_snapshot = yield Get(Snapshot, UrlToFetch(url, digest))\n\n  # TODO(7726): replace this with a proper API to get the `closure` for a\n  # TransitiveHydratedTarget.\n  transitive_hydrated_targets = yield Get(\n    TransitiveHydratedTargets, BuildFileAddresses((test_target.address,))\n  )\n  all_targets = [t.adaptor for t in transitive_hydrated_targets.closure]\n\n  # Produce a pex containing pytest and all transitive 3rdparty requirements.\n  all_target_requirements = []\n  for maybe_python_req_lib in all_targets:\n    # This is a python_requirement()-like target.\n    if hasattr(maybe_python_req_lib, 'requirement'):\n      all_target_requirements.append(str(maybe_python_req_lib.requirement))\n    # This is a python_requirement_library()-like target.\n    if hasattr(maybe_python_req_lib, 'requirements'):\n      for py_req in maybe_python_req_lib.requirements:\n        all_target_requirements.append(str(py_req.requirement))\n\n  # Sort all user requirement strings to increase the chance of cache hits across invocations.\n  all_requirements = sorted(all_target_requirements + list(pytest.get_requirement_strings()))\n\n  # TODO(#7061): This str() can be removed after we drop py2!\n  python_binary = text_type(sys.executable)\n  interpreter_constraint_args = parse_interpreter_constraints(\n    python_setup, python_target_adaptors=all_targets\n  )\n\n  # TODO: This is non-hermetic because the requirements will be resolved on the fly by\n  # pex27, where it should be hermetically provided in some way.\n  output_pytest_requirements_pex_filename = 'pytest-with-requirements.pex'\n  requirements_pex_argv = [\n    python_binary,\n    './{}'.format(pex_snapshot.files[0]),\n    '-e', 'pytest:main',\n    '-o', output_pytest_requirements_pex_filename,\n  ] + interpreter_constraint_args + [\n    # TODO(#7061): This text_type() wrapping can be removed after we drop py2!\n    text_type(req) for req in all_requirements\n  ]\n  requirements_pex_request = ExecuteProcessRequest(\n    argv=tuple(requirements_pex_argv),\n    env={'PATH': text_type(os.pathsep.join(python_setup.interpreter_search_paths))},\n    input_files=pex_snapshot.directory_digest,\n    description='Resolve requirements: {}'.format(\", \".join(all_requirements)),\n    output_files=(output_pytest_requirements_pex_filename,),\n  )\n  requirements_pex_response = yield Get(\n    ExecuteProcessResult, ExecuteProcessRequest, requirements_pex_request)\n\n  source_roots = source_root_config.get_source_roots()\n\n  # Gather sources and adjust for the source root.\n  # TODO: make TargetAdaptor return a 'sources' field with an empty snapshot instead of raising to\n  # simplify the hasattr() checks here!\n  # TODO(7714): restore the full source name for the stdout of the Pytest run.\n  sources_snapshots_and_source_roots = []\n  for maybe_source_target in all_targets:\n    if hasattr(maybe_source_target, 'sources'):\n      tgt_snapshot = maybe_source_target.sources.snapshot\n      tgt_source_root = source_roots.find_by_path(maybe_source_target.address.spec_path)\n      sources_snapshots_and_source_roots.append((tgt_snapshot, tgt_source_root))\n  all_sources_digests = yield [\n    Get(\n      Digest,\n      DirectoryWithPrefixToStrip(\n        directory_digest=snapshot.directory_digest,\n        prefix=source_root.path\n      )\n    )\n    for snapshot, source_root\n    in sources_snapshots_and_source_roots\n  ]\n\n  sources_digest = yield Get(\n    Digest, DirectoriesToMerge(directories=tuple(all_sources_digests)),\n  )\n\n  inits_digest = yield Get(InjectedInitDigest, Digest, sources_digest)\n\n  all_input_digests = [\n    sources_digest,\n    inits_digest.directory_digest,\n    requirements_pex_response.output_directory_digest,\n  ]\n  merged_input_files = yield Get(\n    Digest,\n    DirectoriesToMerge,\n    DirectoriesToMerge(directories=tuple(all_input_digests)),\n  )\n\n  request = ExecuteProcessRequest(\n    argv=(python_binary, './{}'.format(output_pytest_requirements_pex_filename)),\n    env={'PATH': text_type(os.pathsep.join(python_setup.interpreter_search_paths))},\n    input_files=merged_input_files,\n    description='Run pytest for {}'.format(test_target.address.reference()),\n  )\n\n  result = yield Get(FallibleExecuteProcessResult, ExecuteProcessRequest, request)\n  status = Status.SUCCESS if result.exit_code == 0 else Status.FAILURE\n\n  yield TestResult(\n    status=status,\n    stdout=result.stdout.decode('utf-8'),\n    stderr=result.stderr.decode('utf-8'),\n  )\n\n\ndef rules():\n  return [\n      run_python_test,\n      UnionRule(TestTarget, PythonTestsAdaptor),\n      optionable_rule(PyTest),\n      optionable_rule(PythonSetup),\n      optionable_rule(SourceRootConfig),\n    ]\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/fakeNetflix/twitter-repo-pants/blob/91a88ba0d3fe8731717cd8d64424dc2b90952162",
        "file_path": "/src/python/pants/backend/python/subsystems/python_native_code.py",
        "source": "# coding=utf-8\n# Copyright 2017 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport logging\nfrom textwrap import dedent\n\nfrom pants.backend.native.subsystems.native_toolchain import NativeToolchain\nfrom pants.backend.native.targets.native_library import NativeLibrary\nfrom pants.backend.python.python_requirement import PythonRequirement\nfrom pants.backend.python.subsystems import pex_build_util\nfrom pants.backend.python.subsystems.python_setup import PythonSetup\nfrom pants.backend.python.targets.python_distribution import PythonDistribution\nfrom pants.base.exceptions import IncompatiblePlatformsError\nfrom pants.binaries.executable_pex_tool import ExecutablePexTool\nfrom pants.subsystem.subsystem import Subsystem\nfrom pants.util.memo import memoized_property\nfrom pants.util.objects import SubclassesOf\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass PythonNativeCode(Subsystem):\n  \"\"\"A subsystem which exposes components of the native backend to the python backend.\"\"\"\n\n  options_scope = 'python-native-code'\n\n  default_native_source_extensions = ['.c', '.cpp', '.cc']\n\n  class PythonNativeCodeError(Exception): pass\n\n  @classmethod\n  def register_options(cls, register):\n    super(PythonNativeCode, cls).register_options(register)\n\n    register('--native-source-extensions', type=list, default=cls.default_native_source_extensions,\n             fingerprint=True, advanced=True,\n             help='The extensions recognized for native source files in `python_dist()` sources.')\n\n  @classmethod\n  def subsystem_dependencies(cls):\n    return super(PythonNativeCode, cls).subsystem_dependencies() + (\n      NativeToolchain.scoped(cls),\n      PythonSetup,\n    )\n\n  @memoized_property\n  def _native_source_extensions(self):\n    return self.get_options().native_source_extensions\n\n  @memoized_property\n  def native_toolchain(self):\n    return NativeToolchain.scoped_instance(self)\n\n  @memoized_property\n  def _python_setup(self):\n    return PythonSetup.global_instance()\n\n  def pydist_has_native_sources(self, target):\n    return target.has_sources(extension=tuple(self._native_source_extensions))\n\n  @memoized_property\n  def _native_target_matchers(self):\n    return {\n      SubclassesOf(PythonDistribution): self.pydist_has_native_sources,\n      SubclassesOf(NativeLibrary): NativeLibrary.produces_ctypes_native_library,\n    }\n\n  def _any_targets_have_native_sources(self, targets):\n    # TODO(#5949): convert this to checking if the closure of python requirements has any\n    # platform-specific packages (maybe find the platforms there too?).\n    for tgt in targets:\n      for type_constraint, target_predicate in self._native_target_matchers.items():\n        if type_constraint.satisfied_by(tgt) and target_predicate(tgt):\n          return True\n    return False\n\n  def check_build_for_current_platform_only(self, targets):\n    \"\"\"\n    Performs a check of whether the current target closure has native sources and if so, ensures\n    that Pants is only targeting the current platform.\n\n    :param tgts: a list of :class:`Target` objects.\n    :return: a boolean value indicating whether the current target closure has native sources.\n    :raises: :class:`pants.base.exceptions.IncompatiblePlatformsError`\n    \"\"\"\n    # TODO(#5949): convert this to checking if the closure of python requirements has any\n    # platform-specific packages (maybe find the platforms there too?).\n    if not self._any_targets_have_native_sources(targets):\n      return False\n\n    platforms_with_sources = pex_build_util.targets_by_platform(targets, self._python_setup)\n    platform_names = list(platforms_with_sources.keys())\n\n    if not platform_names or platform_names == ['current']:\n      return True\n\n    bad_targets = set()\n    for platform, targets in platforms_with_sources.items():\n      if platform == 'current':\n        continue\n      bad_targets.update(targets)\n\n    raise IncompatiblePlatformsError(dedent(\"\"\"\\\n      Pants doesn't currently support cross-compiling native code.\n      The following targets set platforms arguments other than ['current'], which is unsupported for this reason.\n      Please either remove the platforms argument from these targets, or set them to exactly ['current'].\n      Bad targets:\n      {}\n      \"\"\".format('\\n'.join(sorted(target.address.reference() for target in bad_targets)))\n    ))\n\n\nclass BuildSetupRequiresPex(ExecutablePexTool):\n  options_scope = 'build-setup-requires-pex'\n\n  @classmethod\n  def register_options(cls, register):\n    super(BuildSetupRequiresPex, cls).register_options(register)\n    register('--setuptools-version', advanced=True, fingerprint=True, default='40.6.3',\n             help='The setuptools version to use when executing `setup.py` scripts.')\n    register('--wheel-version', advanced=True, fingerprint=True, default='0.32.3',\n             help='The wheel version to use when executing `setup.py` scripts.')\n\n  @property\n  def base_requirements(self):\n    return [\n      PythonRequirement('setuptools=={}'.format(self.get_options().setuptools_version)),\n      PythonRequirement('wheel=={}'.format(self.get_options().wheel_version)),\n    ]\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/fakeNetflix/twitter-repo-pants/blob/91a88ba0d3fe8731717cd8d64424dc2b90952162",
        "file_path": "/tests/python/pants_test/util/test_contextutil.py",
        "source": "# coding=utf-8\n# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport os\nimport pstats\nimport shutil\nimport signal\nimport sys\nimport unittest\nimport uuid\nimport zipfile\nfrom builtins import next, object, range, str\nfrom contextlib import contextmanager\n\nimport mock\nfrom future.utils import PY3\n\nfrom pants.util.contextutil import (InvalidZipPath, Timer, environment_as, exception_logging,\n                                    hermetic_environment_as, maybe_profiled, open_zip, pushd,\n                                    signal_handler_as, stdio_as, temporary_dir, temporary_file)\nfrom pants.util.process_handler import subprocess\n\n\nPATCH_OPTS = dict(autospec=True, spec_set=True)\n\n\nclass ContextutilTest(unittest.TestCase):\n\n  def test_empty_environment(self):\n    with environment_as():\n      pass\n\n  def test_override_single_variable(self):\n    with temporary_file(binary_mode=False) as output:\n      # test that the override takes place\n      with environment_as(HORK='BORK'):\n        subprocess.Popen([sys.executable, '-c', 'import os; print(os.environ[\"HORK\"])'],\n                         stdout=output).wait()\n        output.seek(0)\n        self.assertEqual('BORK\\n', output.read())\n\n      # test that the variable is cleared\n      with temporary_file(binary_mode=False) as new_output:\n        subprocess.Popen([sys.executable, '-c', 'import os; print(\"HORK\" in os.environ)'],\n                         stdout=new_output).wait()\n        new_output.seek(0)\n        self.assertEqual('False\\n', new_output.read())\n\n  def test_environment_negation(self):\n    with temporary_file(binary_mode=False) as output:\n      with environment_as(HORK='BORK'):\n        with environment_as(HORK=None):\n          # test that the variable is cleared\n          subprocess.Popen([sys.executable, '-c', 'import os; print(\"HORK\" in os.environ)'],\n                           stdout=output).wait()\n          output.seek(0)\n          self.assertEqual('False\\n', output.read())\n\n  def test_hermetic_environment(self):\n    self.assertIn('USER', os.environ)\n    with hermetic_environment_as(**{}):\n      self.assertNotIn('USER', os.environ)\n\n  def test_hermetic_environment_subprocesses(self):\n    self.assertIn('USER', os.environ)\n    with hermetic_environment_as(**dict(AAA='333')):\n      output = subprocess.check_output('env', shell=True).decode('utf-8')\n      self.assertNotIn('USER=', output)\n      self.assertIn('AAA', os.environ)\n      self.assertEqual(os.environ['AAA'], '333')\n    self.assertIn('USER', os.environ)\n    self.assertNotIn('AAA', os.environ)\n\n  def test_hermetic_environment_unicode(self):\n    UNICODE_CHAR = ''\n    ENCODED_CHAR = UNICODE_CHAR.encode('utf-8')\n    expected_output = UNICODE_CHAR if PY3 else ENCODED_CHAR\n    with environment_as(**dict(XXX=UNICODE_CHAR)):\n      self.assertEqual(os.environ['XXX'], expected_output)\n      with hermetic_environment_as(**dict(AAA=UNICODE_CHAR)):\n        self.assertIn('AAA', os.environ)\n        self.assertEqual(os.environ['AAA'], expected_output)\n      self.assertEqual(os.environ['XXX'], expected_output)\n\n  def test_simple_pushd(self):\n    pre_cwd = os.getcwd()\n    with temporary_dir() as tempdir:\n      with pushd(tempdir) as path:\n        self.assertEqual(tempdir, path)\n        self.assertEqual(os.path.realpath(tempdir), os.getcwd())\n      self.assertEqual(pre_cwd, os.getcwd())\n    self.assertEqual(pre_cwd, os.getcwd())\n\n  def test_nested_pushd(self):\n    pre_cwd = os.getcwd()\n    with temporary_dir() as tempdir1:\n      with pushd(tempdir1):\n        self.assertEqual(os.path.realpath(tempdir1), os.getcwd())\n        with temporary_dir(root_dir=tempdir1) as tempdir2:\n          with pushd(tempdir2):\n            self.assertEqual(os.path.realpath(tempdir2), os.getcwd())\n          self.assertEqual(os.path.realpath(tempdir1), os.getcwd())\n        self.assertEqual(os.path.realpath(tempdir1), os.getcwd())\n      self.assertEqual(pre_cwd, os.getcwd())\n    self.assertEqual(pre_cwd, os.getcwd())\n\n  def test_temporary_file_no_args(self):\n    with temporary_file() as fp:\n      self.assertTrue(os.path.exists(fp.name), 'Temporary file should exist within the context.')\n    self.assertTrue(os.path.exists(fp.name) == False,\n                    'Temporary file should not exist outside of the context.')\n\n  def test_temporary_file_without_cleanup(self):\n    with temporary_file(cleanup=False) as fp:\n      self.assertTrue(os.path.exists(fp.name), 'Temporary file should exist within the context.')\n    self.assertTrue(os.path.exists(fp.name),\n                    'Temporary file should exist outside of context if cleanup=False.')\n    os.unlink(fp.name)\n\n  def test_temporary_file_within_other_dir(self):\n    with temporary_dir() as path:\n      with temporary_file(root_dir=path) as f:\n        self.assertTrue(os.path.realpath(f.name).startswith(os.path.realpath(path)),\n                        'file should be created in root_dir if specified.')\n\n  def test_temporary_dir_no_args(self):\n    with temporary_dir() as path:\n      self.assertTrue(os.path.exists(path), 'Temporary dir should exist within the context.')\n      self.assertTrue(os.path.isdir(path), 'Temporary dir should be a dir and not a file.')\n    self.assertFalse(os.path.exists(path), 'Temporary dir should not exist outside of the context.')\n\n  def test_temporary_dir_without_cleanup(self):\n    with temporary_dir(cleanup=False) as path:\n      self.assertTrue(os.path.exists(path), 'Temporary dir should exist within the context.')\n    self.assertTrue(os.path.exists(path),\n                    'Temporary dir should exist outside of context if cleanup=False.')\n    shutil.rmtree(path)\n\n  def test_temporary_dir_with_root_dir(self):\n    with temporary_dir() as path1:\n      with temporary_dir(root_dir=path1) as path2:\n        self.assertTrue(os.path.realpath(path2).startswith(os.path.realpath(path1)),\n                        'Nested temporary dir should be created within outer dir.')\n\n  def test_timer(self):\n\n    class FakeClock(object):\n\n      def __init__(self):\n        self._time = 0.0\n\n      def time(self):\n        ret = self._time\n        self._time += 0.0001  # Force a little time to elapse.\n        return ret\n\n      def sleep(self, duration):\n        self._time += duration\n\n    clock = FakeClock()\n\n    # Note: to test with the real system clock, use this instead:\n    # import time\n    # clock = time\n\n    with Timer(clock=clock) as t:\n      self.assertLess(t.start, clock.time())\n      self.assertGreater(t.elapsed, 0)\n      clock.sleep(0.1)\n      self.assertGreater(t.elapsed, 0.1)\n      clock.sleep(0.1)\n      self.assertTrue(t.finish is None)\n    self.assertGreater(t.elapsed, 0.2)\n    self.assertLess(t.finish, clock.time())\n\n  def test_open_zipDefault(self):\n    with temporary_dir() as tempdir:\n      with open_zip(os.path.join(tempdir, 'test'), 'w') as zf:\n        self.assertTrue(zf._allowZip64)\n\n  def test_open_zipTrue(self):\n    with temporary_dir() as tempdir:\n      with open_zip(os.path.join(tempdir, 'test'), 'w', allowZip64=True) as zf:\n        self.assertTrue(zf._allowZip64)\n\n  def test_open_zipFalse(self):\n    with temporary_dir() as tempdir:\n      with open_zip(os.path.join(tempdir, 'test'), 'w', allowZip64=False) as zf:\n        self.assertFalse(zf._allowZip64)\n\n  def test_open_zip_raises_exception_on_falsey_paths(self):\n    falsey = (None, '', False)\n    for invalid in falsey:\n      with self.assertRaises(InvalidZipPath):\n        next(open_zip(invalid).gen)\n\n  def test_open_zip_returns_realpath_on_badzipfile(self):\n    # In case of file corruption, deleting a Pants-constructed symlink would not resolve the error.\n    with temporary_file() as not_zip:\n      with temporary_dir() as tempdir:\n        file_symlink = os.path.join(tempdir, 'foo')\n        os.symlink(not_zip.name, file_symlink)\n        self.assertEqual(os.path.realpath(file_symlink), os.path.realpath(not_zip.name))\n        with self.assertRaisesRegexp(zipfile.BadZipfile, r'{}'.format(not_zip.name)):\n          next(open_zip(file_symlink).gen)\n\n  @contextmanager\n  def _stdio_as_tempfiles(self):\n    \"\"\"Harness to replace `sys.std*` with tempfiles.\n\n    Validates that all files are read/written/flushed correctly, and acts as a\n    contextmanager to allow for recursive tests.\n    \"\"\"\n\n    # Prefix contents written within this instance with a unique string to differentiate\n    # them from other instances.\n    uuid_str = str(uuid.uuid4())\n    def u(string):\n      return '{}#{}'.format(uuid_str, string)\n    stdin_data = u('stdio')\n    stdout_data = u('stdout')\n    stderr_data = u('stderr')\n\n    with temporary_file(binary_mode=False) as tmp_stdin,\\\n         temporary_file(binary_mode=False) as tmp_stdout,\\\n         temporary_file(binary_mode=False) as tmp_stderr:\n      print(stdin_data, file=tmp_stdin)\n      tmp_stdin.seek(0)\n      # Read prepared content from stdin, and write content to stdout/stderr.\n      with stdio_as(stdout_fd=tmp_stdout.fileno(),\n                    stderr_fd=tmp_stderr.fileno(),\n                    stdin_fd=tmp_stdin.fileno()):\n        self.assertEqual(sys.stdin.fileno(), 0)\n        self.assertEqual(sys.stdout.fileno(), 1)\n        self.assertEqual(sys.stderr.fileno(), 2)\n\n        self.assertEqual(stdin_data, sys.stdin.read().strip())\n        print(stdout_data, file=sys.stdout)\n        yield\n        print(stderr_data, file=sys.stderr)\n\n      tmp_stdout.seek(0)\n      tmp_stderr.seek(0)\n      self.assertEqual(stdout_data, tmp_stdout.read().strip())\n      self.assertEqual(stderr_data, tmp_stderr.read().strip())\n\n  def test_stdio_as(self):\n    self.assertTrue(sys.stderr.fileno() > 2,\n                    \"Expected a pseudofile as stderr, got: {}\".format(sys.stderr))\n    old_stdout, old_stderr, old_stdin = sys.stdout, sys.stderr, sys.stdin\n\n    # The first level tests that when `sys.std*` are file-likes (in particular, the ones set up in\n    # pytest's harness) rather than actual files, we stash and restore them properly.\n    with self._stdio_as_tempfiles():\n      # The second level stashes the first level's actual file objects and then re-opens them.\n      with self._stdio_as_tempfiles():\n        pass\n\n      # Validate that after the second level completes, the first level still sees valid\n      # fds on `sys.std*`.\n      self.assertEqual(sys.stdin.fileno(), 0)\n      self.assertEqual(sys.stdout.fileno(), 1)\n      self.assertEqual(sys.stderr.fileno(), 2)\n\n    self.assertEqual(sys.stdout, old_stdout)\n    self.assertEqual(sys.stderr, old_stderr)\n    self.assertEqual(sys.stdin, old_stdin)\n\n  def test_stdio_as_dev_null(self):\n    # Capture output to tempfiles.\n    with self._stdio_as_tempfiles():\n      # Read/write from/to `/dev/null`, which will be validated by the harness as not\n      # affecting the tempfiles.\n      with stdio_as(stdout_fd=-1, stderr_fd=-1, stdin_fd=-1):\n        self.assertEqual('', sys.stdin.read())\n        print('garbage', file=sys.stdout)\n        print('garbage', file=sys.stderr)\n\n  def test_signal_handler_as(self):\n    mock_initial_handler = 1\n    mock_new_handler = 2\n    with mock.patch('signal.signal', **PATCH_OPTS) as mock_signal:\n      mock_signal.return_value = mock_initial_handler\n      try:\n        with signal_handler_as(signal.SIGUSR2, mock_new_handler):\n          raise NotImplementedError('blah')\n      except NotImplementedError:\n        pass\n    self.assertEqual(mock_signal.call_count, 2)\n    mock_signal.assert_has_calls([\n      mock.call(signal.SIGUSR2, mock_new_handler),\n      mock.call(signal.SIGUSR2, mock_initial_handler)\n    ])\n\n  def test_permissions(self):\n    with temporary_file(permissions=0o700) as f:\n      self.assertEqual(0o700, os.stat(f.name)[0] & 0o777)\n\n    with temporary_dir(permissions=0o644) as path:\n      self.assertEqual(0o644, os.stat(path)[0] & 0o777)\n\n  def test_exception_logging(self):\n    fake_logger = mock.Mock()\n\n    with self.assertRaises(AssertionError):\n      with exception_logging(fake_logger, 'error!'):\n        assert True is False\n\n    fake_logger.exception.assert_called_once_with('error!')\n\n  def test_maybe_profiled(self):\n    with temporary_dir() as td:\n      profile_path = os.path.join(td, 'profile.prof')\n\n      with maybe_profiled(profile_path):\n        for _ in range(5):\n          print('test')\n\n      # Ensure the profile data was written.\n      self.assertTrue(os.path.exists(profile_path))\n\n      # Ensure the profile data is valid.\n      pstats.Stats(profile_path).print_stats()\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/hyperion-start/hyperion-core/blob/9940b677381fcc38595a1452b1586480fd8a6146",
        "file_path": "/hyperion/hyperion.py",
        "source": "#! /usr/bin/env python\nfrom libtmux import Server\nfrom yaml import load, dump\nfrom setupParser import Loader\nfrom DepTree import Node, dep_resolve, CircularReferenceException\nimport logging\nimport os\nimport socket\nimport argparse\nfrom psutil import Process\nfrom subprocess import call\nfrom graphviz import Digraph\nfrom enum import Enum\nfrom time import sleep\n\nimport sys\nfrom PyQt4 import QtGui\nimport hyperGUI\n\nFORMAT = \"%(asctime)s: %(name)s [%(levelname)s]:\\t%(message)s\"\n\nlogging.basicConfig(level=logging.WARNING, format=FORMAT, datefmt='%I:%M:%S')\nTMP_SLAVE_DIR = \"/tmp/Hyperion/slave/components\"\nTMP_COMP_DIR = \"/tmp/Hyperion/components\"\nTMP_LOG_PATH = \"/tmp/Hyperion/log\"\n\nBASE_DIR = os.path.dirname(__file__)\nSCRIPT_CLONE_PATH = (\"%s/scripts/start_named_clone_session.sh\" % BASE_DIR)\n\n\nclass CheckState(Enum):\n    RUNNING = 0\n    STOPPED = 1\n    STOPPED_BUT_SUCCESSFUL = 2\n    STARTED_BY_HAND = 3\n    DEP_FAILED = 4\n\n\nclass ControlCenter:\n\n    def __init__(self, configfile=None):\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.DEBUG)\n        self.configfile = configfile\n        self.nodes = {}\n        self.server = []\n        self.host_list = []\n\n        if configfile:\n            self.load_config(configfile)\n            self.session_name = self.config[\"name\"]\n\n            # Debug write resulting yaml file\n            with open('debug-result.yml', 'w') as outfile:\n                dump(self.config, outfile, default_flow_style=False)\n            self.logger.debug(\"Loading config was successful\")\n\n            self.server = Server()\n\n            if self.server.has_session(self.session_name):\n                self.session = self.server.find_where({\n                    \"session_name\": self.session_name\n                })\n\n                self.logger.info('found running session by name \"%s\" on server' % self.session_name)\n            else:\n                self.logger.info('starting new session by name \"%s\" on server' % self.session_name)\n                self.session = self.server.new_session(\n                    session_name=self.session_name,\n                    window_name=\"Main\"\n                )\n        else:\n            self.config = None\n\n    ###################\n    # Setup\n    ###################\n    def load_config(self, filename=\"default.yaml\"):\n        with open(filename) as data_file:\n            self.config = load(data_file, Loader)\n\n    def init(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n\n        else:\n            for group in self.config['groups']:\n                for comp in group['components']:\n                    self.logger.debug(\"Checking component '%s' in group '%s' on host '%s'\" %\n                                      (comp['name'], group['name'], comp['host']))\n\n                    if comp['host'] != \"localhost\" and not self.run_on_localhost(comp):\n                        self.copy_component_to_remote(comp, comp['name'], comp['host'])\n\n            # Remove duplicate hosts\n            self.host_list = list(set(self.host_list))\n\n            self.set_dependencies(True)\n\n    def set_dependencies(self, exit_on_fail):\n        for group in self.config['groups']:\n            for comp in group['components']:\n                self.nodes[comp['name']] = Node(comp)\n\n        # Add a pseudo node that depends on all other nodes, to get a starting point to be able to iterate through all\n        # nodes with simple algorithms\n        master_node = Node({'name': 'master_node'})\n        for name in self.nodes:\n            node = self.nodes.get(name)\n\n            # Add edges from each node to pseudo node\n            master_node.addEdge(node)\n\n            # Add edges based on dependencies specified in the configuration\n            if \"depends\" in node.component:\n                for dep in node.component['depends']:\n                    if dep in self.nodes:\n                        node.addEdge(self.nodes[dep])\n                    else:\n                        self.logger.error(\"Unmet dependency: '%s' for component '%s'!\" % (dep, node.comp_name))\n                        if exit_on_fail:\n                            exit(1)\n        self.nodes['master_node'] = master_node\n\n        # Test if starting all components is possible\n        try:\n            node = self.nodes.get('master_node')\n            res = []\n            unres = []\n            dep_resolve(node, res, unres)\n            dep_string = \"\"\n            for node in res:\n                if node is not master_node:\n                    dep_string = \"%s -> %s\" % (dep_string, node.comp_name)\n            self.logger.debug(\"Dependency tree for start all: %s\" % dep_string)\n        except CircularReferenceException as ex:\n            self.logger.error(\"Detected circular dependency reference between %s and %s!\" % (ex.node1, ex.node2))\n            if exit_on_fail:\n                exit(1)\n\n    def copy_component_to_remote(self, infile, comp, host):\n        self.host_list.append(host)\n\n        self.logger.debug(\"Saving component to tmp\")\n        tmp_comp_path = ('%s/%s.yaml' % (TMP_COMP_DIR, comp))\n        ensure_dir(tmp_comp_path)\n        with open(tmp_comp_path, 'w') as outfile:\n            dump(infile, outfile, default_flow_style=False)\n\n        self.logger.debug('Copying component \"%s\" to remote host \"%s\"' % (comp, host))\n        cmd = (\"ssh %s 'mkdir -p %s' & scp %s %s:%s/%s.yaml\" %\n               (host, TMP_SLAVE_DIR, tmp_comp_path, host, TMP_SLAVE_DIR, comp))\n        self.logger.debug(cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Stop\n    ###################\n    def stop_component(self, comp):\n        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):\n            self.logger.debug(\"Stopping remote component '%s' on host '%s'\" % (comp['name'], comp['host']))\n            self.stop_remote_component(comp['name'], comp['host'])\n        else:\n            window = find_window(self.session, comp['name'])\n\n            if window:\n                self.logger.debug(\"window '%s' found running\" % comp['name'])\n                self.logger.info(\"Shutting down window...\")\n                kill_window(window)\n                self.logger.info(\"... done!\")\n\n    def stop_remote_component(self, comp_name, host):\n        # invoke Hyperion in slave mode on each remote host\n        cmd = (\"ssh %s 'hyperion --config %s/%s.yaml slave --kill'\" % (host, TMP_SLAVE_DIR, comp_name))\n        self.logger.debug(\"Run cmd:\\n%s\" % cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Start\n    ###################\n    def start_component(self, comp):\n\n        node = self.nodes.get(comp['name'])\n        res = []\n        unres = []\n        dep_resolve(node, res, unres)\n        for node in res:\n            self.logger.debug(\"node name '%s' vs. comp name '%s'\" % (node.comp_name, comp['name']))\n            if node.comp_name != comp['name']:\n                self.logger.debug(\"Checking and starting %s\" % node.comp_name)\n                state = self.check_component(node.component)\n                if (state is CheckState.STOPPED_BUT_SUCCESSFUL or\n                        state is CheckState.STARTED_BY_HAND or\n                        state is CheckState.RUNNING):\n                    self.logger.debug(\"Component %s is already running, skipping to next in line\" % comp['name'])\n                else:\n                    self.logger.debug(\"Start component '%s' as dependency of '%s'\" % (node.comp_name, comp['name']))\n                    self.start_component_without_deps(node.component)\n\n                    tries = 0\n                    while True:\n                        self.logger.debug(\"Checking %s resulted in checkstate %s\" % (node.comp_name, state))\n                        state = self.check_component(node.component)\n                        if (state is not CheckState.RUNNING or\n                           state is not CheckState.STOPPED_BUT_SUCCESSFUL):\n                            break\n                        if tries > 100:\n                            return False\n                        tries = tries + 1\n                        sleep(.5)\n\n        self.logger.debug(\"All dependencies satisfied, starting '%s'\" % (comp['name']))\n        state = self.check_component(node.component)\n        if (state is CheckState.STARTED_BY_HAND or\n                state is CheckState.RUNNING):\n            self.logger.debug(\"Component %s is already running. Skipping start\" % comp['name'])\n        else:\n            self.start_component_without_deps(comp)\n        return True\n\n    def start_component_without_deps(self, comp):\n        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):\n            self.logger.debug(\"Starting remote component '%s' on host '%s'\" % (comp['name'], comp['host']))\n            self.start_remote_component(comp['name'], comp['host'])\n        else:\n            log_file = (\"%s/%s\" % (TMP_LOG_PATH, comp['name']))\n            window = find_window(self.session, comp['name'])\n\n            if window:\n                self.logger.debug(\"Restarting '%s' in old window\" % comp['name'])\n                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])\n            else:\n                self.logger.info(\"creating window '%s'\" % comp['name'])\n                window = self.session.new_window(comp['name'])\n                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])\n\n    def start_remote_component(self, comp_name, host):\n        # invoke Hyperion in slave mode on each remote host\n        cmd = (\"ssh %s 'hyperion --config %s/%s.yaml slave'\" % (host, TMP_SLAVE_DIR, comp_name))\n        self.logger.debug(\"Run cmd:\\n%s\" % cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Check\n    ###################\n    def check_component(self, comp):\n        return check_component(comp, self.session, self.logger)\n\n    ###################\n    # Dependency management\n    ###################\n    def get_dep_list(self, comp):\n        node = self.nodes.get(comp['name'])\n        res = []\n        unres = []\n        dep_resolve(node, res, unres)\n        res.remove(node)\n\n        return res\n\n    ###################\n    # Host related checks\n    ###################\n    def is_localhost(self, hostname):\n        try:\n            hn_out = socket.gethostbyname(hostname)\n            if hn_out == '127.0.0.1' or hn_out == '::1':\n                self.logger.debug(\"Host '%s' is localhost\" % hostname)\n                return True\n            else:\n                self.logger.debug(\"Host '%s' is not localhost\" % hostname)\n                return False\n        except socket.gaierror:\n            sys.exit(\"Host '%s' is unknown! Update your /etc/hosts file!\" % hostname)\n\n    def run_on_localhost(self, comp):\n        return self.is_localhost(comp['host'])\n\n    ###################\n    # TMUX\n    ###################\n    def kill_remote_session_by_name(self, name, host):\n        cmd = \"ssh -t %s 'tmux kill-session -t %s'\" % (host, name)\n        send_main_session_command(self.session, cmd)\n\n    def start_clone_session(self, comp_name, session_name):\n        cmd = \"%s '%s' '%s'\" % (SCRIPT_CLONE_PATH, session_name, comp_name)\n        send_main_session_command(self.session, cmd)\n\n    def start_remote_clone_session(self, comp_name, session_name, hostname):\n        remote_cmd = (\"%s '%s' '%s'\" % (SCRIPT_CLONE_PATH, session_name, comp_name))\n        cmd = \"ssh %s 'bash -s' < %s\" % (hostname, remote_cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Visualisation\n    ###################\n    def draw_graph(self):\n        deps = Digraph(\"Deps\", strict=True)\n        deps.graph_attr.update(rankdir=\"BT\")\n        try:\n            node = self.nodes.get('master_node')\n\n            for current in node.depends_on:\n                deps.node(current.comp_name)\n\n                res = []\n                unres = []\n                dep_resolve(current, res, unres)\n                for node in res:\n                    if \"depends\" in node.component:\n                        for dep in node.component['depends']:\n                            if dep not in self.nodes:\n                                deps.node(dep, color=\"red\")\n                                deps.edge(node.comp_name, dep, \"missing\", color=\"red\")\n                            elif node.comp_name is not \"master_node\":\n                                deps.edge(node.comp_name, dep)\n\n        except CircularReferenceException as ex:\n            self.logger.error(\"Detected circular dependency reference between %s and %s!\" % (ex.node1, ex.node2))\n            deps.edge(ex.node1, ex.node2, \"circular error\", color=\"red\")\n            deps.edge(ex.node2, ex.node1, color=\"red\")\n\n        deps.view()\n\n\nclass SlaveLauncher:\n\n    def __init__(self, configfile=None, kill_mode=False, check_mode=False):\n        self.kill_mode = kill_mode\n        self.check_mode = check_mode\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.DEBUG)\n        self.config = None\n        self.session = None\n        if kill_mode:\n            self.logger.info(\"started slave with kill mode\")\n        if check_mode:\n            self.logger.info(\"started slave with check mode\")\n        self.server = Server()\n\n        if self.server.has_session(\"slave-session\"):\n            self.session = self.server.find_where({\n                \"session_name\": \"slave-session\"\n            })\n\n            self.logger.info('found running slave session on server')\n        elif not kill_mode and not check_mode:\n            self.logger.info('starting new slave session on server')\n            self.session = self.server.new_session(\n                session_name=\"slave-session\"\n            )\n\n        else:\n            self.logger.info(\"No slave session found on server. Aborting\")\n            exit(CheckState.STOPPED)\n\n        if configfile:\n            self.load_config(configfile)\n            self.window_name = self.config['name']\n            self.flag_path = (\"/tmp/Hyperion/slaves/%s\" % self.window_name)\n            self.log_file = (\"/tmp/Hyperion/log/%s\" % self.window_name)\n            ensure_dir(self.log_file)\n        else:\n            self.logger.error(\"No slave component config provided\")\n\n    def load_config(self, filename=\"default.yaml\"):\n        with open(filename) as data_file:\n            self.config = load(data_file, Loader)\n\n    def init(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n        elif not self.session:\n            self.logger.error(\" Init aborted. No session was found!\")\n        else:\n            self.logger.debug(self.config)\n            window = find_window(self.session, self.window_name)\n\n            if window:\n                self.logger.debug(\"window '%s' found running\" % self.window_name)\n                if self.kill_mode:\n                    self.logger.info(\"Shutting down window...\")\n                    kill_window(window)\n                    self.logger.info(\"... done!\")\n            elif not self.kill_mode:\n                self.logger.info(\"creating window '%s'\" % self.window_name)\n                window = self.session.new_window(self.window_name)\n                start_window(window, self.config['cmd'][0]['start'], self.log_file, self.window_name)\n\n            else:\n                self.logger.info(\"There is no component running by the name '%s'. Exiting kill mode\" %\n                                 self.window_name)\n\n    def run_check(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n            exit(CheckState.STOPPED.value)\n        elif not self.session:\n            self.logger.error(\" Init aborted. No session was found!\")\n            exit(CheckState.STOPPED.value)\n\n        check_state = check_component(self.config, self.session, self.logger)\n        exit(check_state.value)\n\n###################\n# Component Management\n###################\ndef run_component_check(comp):\n    if call(comp['cmd'][1]['check'], shell=True) == 0:\n        return True\n    else:\n        return False\n\n\ndef check_component(comp, session, logger):\n    logger.debug(\"Running component check for %s\" % comp['name'])\n    check_available = len(comp['cmd']) > 1 and 'check' in comp['cmd'][1]\n    window = find_window(session, comp['name'])\n    if window:\n        pid = get_window_pid(window)\n        logger.debug(\"Found window pid: %s\" % pid)\n\n        # May return more child pids if logging is done via tee (which then was started twice in the window too)\n        procs = []\n        for entry in pid:\n            procs.extend(Process(entry).children(recursive=True))\n        pids = [p.pid for p in procs]\n        logger.debug(\"Window is running %s child processes\" % len(pids))\n\n        # Two processes are tee logging\n        # TODO: Change this when more logging options are introduced\n        if len(pids) < 3:\n            logger.debug(\"Main window process has finished. Running custom check if available\")\n            if check_available and run_component_check(comp):\n                logger.debug(\"Process terminated but check was successful\")\n                return CheckState.STOPPED_BUT_SUCCESSFUL\n            else:\n                logger.debug(\"Check failed or no check available: returning false\")\n                return CheckState.STOPPED\n        elif check_available and run_component_check(comp):\n            logger.debug(\"Check succeeded\")\n            return CheckState.RUNNING\n        elif not check_available:\n            logger.debug(\"No custom check specified and got sufficient pid amount: returning true\")\n            return CheckState.RUNNING\n        else:\n            logger.debug(\"Check failed: returning false\")\n            return CheckState.STOPPED\n    else:\n        logger.debug(\"%s window is not running. Running custom check\" % comp['name'])\n        if check_available and run_component_check(comp):\n            logger.debug(\"Component was not started by Hyperion, but the check succeeded\")\n            return CheckState.STARTED_BY_HAND\n        else:\n            logger.debug(\"Window not running and no check command is available or it failed: returning false\")\n            return CheckState.STOPPED\n\n\ndef get_window_pid(window):\n    r = window.cmd('list-panes',\n                   \"-F #{pane_pid}\")\n    return [int(p) for p in r.stdout]\n\n###################\n# TMUX\n###################\ndef kill_session_by_name(server, name):\n    session = server.find_where({\n        \"session_name\": name\n    })\n    session.kill_session()\n\n\ndef kill_window(window):\n    window.cmd(\"send-keys\", \"\", \"C-c\")\n    window.kill_window()\n\n\ndef start_window(window, cmd, log_file, comp_name):\n    setup_log(window, log_file, comp_name)\n    window.cmd(\"send-keys\", cmd, \"Enter\")\n\n\ndef find_window(session, window_name):\n    window = session.find_where({\n        \"window_name\": window_name\n    })\n    return window\n\n\ndef send_main_session_command(session, cmd):\n    window = find_window(session, \"Main\")\n    window.cmd(\"send-keys\", cmd, \"Enter\")\n\n\n###################\n# Logging\n###################\ndef setup_log(window, file, comp_name):\n    clear_log(file)\n    # Reroute stderr to log file\n    window.cmd(\"send-keys\", \"exec 2> >(exec tee -i -a '%s')\" % file, \"Enter\")\n    # Reroute stdin to log file\n    window.cmd(\"send-keys\", \"exec 1> >(exec tee -i -a '%s')\" % file, \"Enter\")\n    window.cmd(\"send-keys\", ('echo \"#Hyperion component start: %s\\n$(date)\"' % comp_name), \"Enter\")\n\n\ndef clear_log(file_path):\n    if os.path.isfile(file_path):\n        os.remove(file_path)\n\n\ndef ensure_dir(file_path):\n    directory = os.path.dirname(file_path)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n###################\n# Startup\n###################\ndef main():\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n    parser = argparse.ArgumentParser()\n\n    # Create top level parser\n    parser.add_argument(\"--config\", '-c', type=str,\n                        default='test.yaml',\n                        help=\"YAML config file. see sample-config.yaml. Default: test.yaml\")\n    subparsers = parser.add_subparsers(dest=\"cmd\")\n\n    # Create parser for the editor command\n    subparser_editor = subparsers.add_parser('edit', help=\"Launches the editor to edit or create new systems and \"\n                                                          \"components\")\n    # Create parser for the run command\n    subparser_run = subparsers.add_parser('run', help=\"Launches the setup specified by the --config argument\")\n    # Create parser for validator\n    subparser_val = subparsers.add_parser('validate', help=\"Validate the setup specified by the --config argument\")\n\n    subparser_remote = subparsers.add_parser('slave', help=\"Run a component locally without controlling it. The \"\n                                                           \"control is taken care of the remote master invoking \"\n                                                           \"this command.\\nIf run with the --kill flag, the \"\n                                                           \"passed component will be killed\")\n\n    subparser_val.add_argument(\"--visual\", help=\"Generate and show a graph image\", action=\"store_true\")\n\n    remote_mutex = subparser_remote.add_mutually_exclusive_group(required=False)\n\n    remote_mutex.add_argument('-k', '--kill', help=\"switch to kill mode\", action=\"store_true\")\n    remote_mutex.add_argument('-c', '--check', help=\"Run a component check\", action=\"store_true\")\n\n    args = parser.parse_args()\n    logger.debug(args)\n\n    if args.cmd == 'edit':\n        logger.debug(\"Launching editor mode\")\n\n    elif args.cmd == 'run':\n        logger.debug(\"Launching runner mode\")\n\n        cc = ControlCenter(args.config)\n        cc.init()\n        start_gui(cc)\n\n    elif args.cmd == 'validate':\n        logger.debug(\"Launching validation mode\")\n        cc = ControlCenter(args.config)\n        if args.visual:\n            cc.set_dependencies(False)\n            cc.draw_graph()\n        else:\n            cc.set_dependencies(True)\n\n    elif args.cmd == 'slave':\n        logger.debug(\"Launching slave mode\")\n        sl = SlaveLauncher(args.config, args.kill, args.check)\n\n        if args.check:\n            sl.run_check()\n        else:\n            sl.init()\n\n\n###################\n# GUI\n###################\ndef start_gui(control_center):\n    app = QtGui.QApplication(sys.argv)\n    main_window = QtGui.QMainWindow()\n    ui = hyperGUI.UiMainWindow()\n    ui.ui_init(main_window, control_center)\n    main_window.show()\n    sys.exit(app.exec_())\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/ornl-oxford/genben/blob/d3e3e490f4c38e4af73d7c78bf697cf906e8a1b6",
        "file_path": "/benchmark/cli.py",
        "source": "\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, \ndetermines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,\nruns the benchmarks, and records the timer results. \"\"\"\n\nimport argparse  # for command line parsing\nimport time  # for benchmark timer\nimport csv  # for writing results\nimport logging\nimport sys\nimport shutil\nfrom benchmark import config, data_service\n\n\ndef get_cli_arguments():\n    \"\"\" Returns command line arguments. \n\n    Returns:\n    args object from an ArgumentParses for fetch data (boolean, from a server), label (optional, for naming the benchmark run), \n    and config argument for where is the config file. \"\"\"\n\n    logging.debug('Getting cli arguments')\n\n    parser = argparse.ArgumentParser(description=\"A benchmark for genomics routines in Python.\")\n\n    # Enable three exclusive groups of options (using subparsers)\n    # https://stackoverflow.com/questions/17909294/python-argparse-mutual-exclusive-group/17909525\n\n    subparser = parser.add_subparsers(title=\"commands\", dest=\"command\")\n    subparser.required = True\n\n    config_parser = subparser.add_parser(\"config\",\n                                         help='Setting up the default configuration of the benchmark. It creates the default configuration file.')\n    config_parser.add_argument(\"--output_config\", type=str, required=True,\n                               help=\"Specify the output path to a configuration file.\", metavar=\"FILEPATH\")\n    config_parser.add_argument(\"-f\", action=\"store_true\", help=\"Overwrite the destination file if it already exists.\")\n\n    data_setup_parser = subparser.add_parser(\"setup\",\n                                             help='Preparation and setting up of the data for the benchmark. It requires a configuration file.')\n    data_setup_parser.add_argument(\"--config_file\", required=True, help=\"Location of the configuration file\",\n                                   metavar=\"FILEPATH\")\n\n    benchmark_exec_parser = subparser.add_parser(\"exec\",\n                                                 help='Execution of the benchmark modes. It requires a configuration file.')\n    # TODO: use run_(timestamp) as default\n    benchmark_exec_parser.add_argument(\"--label\", type=str, default=\"run\", metavar=\"RUN_LABEL\",\n                                       help=\"Label for the benchmark run.\")\n    benchmark_exec_parser.add_argument(\"--config_file\", type=str, required=True,\n                                       help=\"Specify the path to a configuration file.\", metavar=\"FILEPATH\")\n\n    runtime_configuration = vars(parser.parse_args())\n    return runtime_configuration\n\n\ndef _main():\n    input_directory = \"./data/input/\"\n    download_directory = input_directory + \"download/\"\n    temp_directory = \"./data/temp/\"\n    vcf_directory = \"./data/vcf/\"\n    zarr_directory_setup = \"./data/zarr/\"\n    zarr_directory_benchmark = \"./data/zarr_benchmark/\"\n\n    cli_arguments = get_cli_arguments()\n\n    command = cli_arguments[\"command\"]\n    if command == \"config\":\n        output_config_location = cli_arguments[\"output_config\"]\n        overwrite_mode = cli_arguments[\"f\"]\n        config.generate_default_config_file(output_location=output_config_location,\n                                            overwrite=overwrite_mode)\n    elif command == \"setup\":\n        print(\"[Setup] Setting up benchmark data.\")\n\n        # Clear out existing files in VCF and Zarr directories\n        data_service.remove_directory_tree(vcf_directory)\n        data_service.remove_directory_tree(zarr_directory_setup)\n\n        # Get runtime config from specified location\n        runtime_config = config.read_configuration(location=cli_arguments[\"config_file\"])\n\n        # Get FTP module settings from runtime config\n        ftp_config = config.FTPConfigurationRepresentation(runtime_config)\n\n        if ftp_config.enabled:\n            print(\"[Setup][FTP] FTP module enabled. Running FTP download...\")\n            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=download_directory)\n        else:\n            print(\"[Setup][FTP] FTP module disabled. Skipping FTP download...\")\n\n        # Process/Organize downloaded files\n        data_service.process_data_files(input_dir=input_directory,\n                                        temp_dir=temp_directory,\n                                        output_dir=vcf_directory)\n\n        # Convert VCF files to Zarr format if the module is enabled\n        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)\n        if vcf_to_zarr_config.enabled:\n            data_service.setup_vcf_to_zarr(input_vcf_dir=vcf_directory,\n                                           output_zarr_dir=zarr_directory_setup,\n                                           conversion_config=vcf_to_zarr_config)\n    elif command == \"exec\":\n        print(\"[Exec] Executing benchmark tool.\")\n\n        # Get runtime config from specified location\n        runtime_config = config.read_configuration(location=cli_arguments[\"config_file\"])\n\n        # Get VCF to Zarr conversion settings from runtime config\n        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)\n\n        # TODO: Convert necessary VCF files to Zarr format\n        # data_service.convert_to_zarr(\"./data/vcf/chr22.1000.vcf\", \"./data/zarr/chr22.1000.zarr\", vcf_to_zarr_config)\n    else:\n        print(\"Error: Unexpected command specified. Exiting...\")\n        sys.exit(1)\n\n\ndef main():\n    try:\n        _main()\n    except KeyboardInterrupt:\n        print(\"Program interrupted. Exiting...\")\n        sys.exit(1)\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/ornl-oxford/genben/blob/d3e3e490f4c38e4af73d7c78bf697cf906e8a1b6",
        "file_path": "/benchmark/core.py",
        "source": "\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, \ndetermines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,\nruns the benchmarks, and records the timer results. \"\"\"\n\nimport time  # for benchmark timer\nimport csv  # for writing results\nimport logging\n\n\ndef run_benchmark(bench_conf):\n    pass\n\n\ndef run_dynamic(ftp_location):\n    pass\n\n\ndef run_static():\n    pass\n\n\ndef get_remote_files(ftp_server, ftp_directory, files=None):\n    pass\n\n\ndef record_runtime(benchmark, timestamp):\n    pass\n\n\n# temporary here\ndef main():\n    pass\n",
        "dataset": "plain_remote_code_execution.json"
    },
    {
        "html_url": " https://github.com/ornl-oxford/genben/blob/d3e3e490f4c38e4af73d7c78bf697cf906e8a1b6",
        "file_path": "/benchmark/data_service.py",
        "source": "\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, \ndetermines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,\nruns the benchmarks, and records the timer results. \"\"\"\n\nimport urllib.request\nfrom ftplib import FTP, FTP_TLS, error_perm\nimport time  # for benchmark timer\nimport csv  # for writing results\nimport logging\nimport os.path\nimport pathlib\nimport allel\nimport sys\nimport functools\nimport numpy as np\nimport zarr\nimport numcodecs\nfrom numcodecs import Blosc, LZ4, LZMA\nfrom benchmark import config\n\nimport gzip\nimport shutil\n\n\ndef create_directory_tree(path):\n    \"\"\"\n    Creates directories for the path specified.\n    :param path: The path to create dirs/subdirs for\n    :type path: str\n    \"\"\"\n    path = str(path)  # Ensure path is in str format\n    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n\n\ndef remove_directory_tree(path):\n    \"\"\"\n    Removes the directory and all subdirectories/files within the path specified.\n    :param path: The path to the directory to remove\n    :type path: str\n    \"\"\"\n\n    if os.path.exists(path):\n        shutil.rmtree(path, ignore_errors=True)\n\n\ndef fetch_data_via_ftp(ftp_config, local_directory):\n    \"\"\" Get benchmarking data from a remote ftp server. \n    :type ftp_config: config.FTPConfigurationRepresentation\n    :type local_directory: str\n    \"\"\"\n    if ftp_config.enabled:\n        # Create local directory tree if it does not exist\n        create_directory_tree(local_directory)\n\n        # Login to FTP server\n        if ftp_config.use_tls:\n            ftp = FTP_TLS(ftp_config.server)\n            ftp.login(ftp_config.username, ftp_config.password)\n            ftp.prot_p()  # Request secure data connection for file retrieval\n        else:\n            ftp = FTP(ftp_config.server)\n            ftp.login(ftp_config.username, ftp_config.password)\n\n        if not ftp_config.files:  # Auto-download all files in directory\n            fetch_data_via_ftp_recursive(ftp=ftp,\n                                         local_directory=local_directory,\n                                         remote_directory=ftp_config.directory)\n        else:\n            ftp.cwd(ftp_config.directory)\n\n            file_counter = 1\n            file_list_total = len(ftp_config.files)\n\n            for remote_filename in ftp_config.files:\n                local_filename = remote_filename\n                filepath = os.path.join(local_directory, local_filename)\n                if not os.path.exists(filepath):\n                    with open(filepath, \"wb\") as local_file:\n                        try:\n                            ftp.retrbinary('RETR %s' % remote_filename, local_file.write)\n                            print(\"[Setup][FTP] ({}/{}) File downloaded: {}\".format(file_counter, file_list_total,\n                                                                                    filepath))\n                        except error_perm:\n                            # Error downloading file. Display error message and delete local file\n                            print(\"[Setup][FTP] ({}/{}) Error downloading file. Skipping: {}\".format(file_counter,\n                                                                                                     file_list_total,\n                                                                                                     filepath))\n                            local_file.close()\n                            os.remove(filepath)\n                else:\n                    print(\"[Setup][FTP] ({}/{}) File already exists. Skipping: {}\".format(file_counter, file_list_total,\n                                                                                          filepath))\n                file_counter = file_counter + 1\n        # Close FTP connection\n        ftp.close()\n\n\ndef fetch_data_via_ftp_recursive(ftp, local_directory, remote_directory, remote_subdirs_list=None):\n    \"\"\"\n    Recursive function that automatically downloads all files with a FTP directory, including subdirectories.\n    :type ftp: ftplib.FTP\n    :type local_directory: str\n    :type remote_directory: str\n    :type remote_subdirs_list: list\n    \"\"\"\n\n    if (remote_subdirs_list is not None) and (len(remote_subdirs_list) > 0):\n        remote_path_relative = \"/\".join(remote_subdirs_list)\n        remote_path_absolute = \"/\" + remote_directory + \"/\" + remote_path_relative + \"/\"\n    else:\n        remote_subdirs_list = []\n        remote_path_relative = \"\"\n        remote_path_absolute = \"/\" + remote_directory + \"/\"\n\n    try:\n        local_path = local_directory + \"/\" + remote_path_relative\n        os.mkdir(local_path)\n        print(\"[Setup][FTP] Created local folder: {}\".format(local_path))\n    except OSError:  # Folder already exists at destination. Do nothing.\n        pass\n    except error_perm:  # Invalid Entry\n        print(\"[Setup][FTP] Error: Could not change to: {}\".format(remote_path_absolute))\n\n    ftp.cwd(remote_path_absolute)\n\n    # Get list of remote files/folders in current directory\n    file_list = ftp.nlst()\n\n    file_counter = 1\n    file_list_total = len(file_list)\n\n    for file in file_list:\n        file_path_local = local_directory + \"/\" + remote_path_relative + \"/\" + file\n        if not os.path.isfile(file_path_local):\n            try:\n                # Determine if a file or folder\n                ftp.cwd(remote_path_absolute + file)\n                # Path is for a folder. Run recursive function in new folder\n                print(\"[Setup][FTP] Switching to directory: {}\".format(remote_path_relative + \"/\" + file))\n                new_remote_subdirs_list = remote_subdirs_list.copy()\n                new_remote_subdirs_list.append(file)\n                fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory,\n                                             remote_directory=remote_directory,\n                                             remote_subdirs_list=new_remote_subdirs_list)\n                # Return up one level since we are using recursion\n                ftp.cwd(remote_path_absolute)\n            except error_perm:\n                # file is an actual file. Download if it doesn't already exist on filesystem.\n                temp = ftp.nlst()\n                if not os.path.isfile(file_path_local):\n                    with open(file_path_local, \"wb\") as local_file:\n                        ftp.retrbinary('RETR {}'.format(file), local_file.write)\n                    print(\"[Setup][FTP] ({}/{}) File downloaded: {}\".format(file_counter, file_list_total,\n                                                                            file_path_local))\n        else:\n            print(\"[Setup][FTP] ({}/{}) File already exists. Skipping: {}\".format(file_counter, file_list_total,\n                                                                                  file_path_local))\n        file_counter = file_counter + 1\n\n\ndef fetch_file_from_url(url, local_file):\n    urllib.request.urlretrieve(url, local_file)\n\n\ndef decompress_gzip(local_file_gz, local_file):\n    with open(local_file, 'wb') as file_out, gzip.open(local_file_gz, 'rb') as file_in:\n        shutil.copyfileobj(file_in, file_out)\n\n\ndef process_data_files(input_dir, temp_dir, output_dir):\n    \"\"\"\n    Iterates through all files in input_dir and processes *.vcf.gz files to *.vcf, placed in output_dir.\n    Additionally moves *.vcf files to output_dir\n    Note: This method searches through all subdirectories within input_dir, and files are placed in root of output_dir.\n    :param input_dir: The input directory containing files to process\n    :param temp_dir: The temporary directory for unzipping *.gz files, etc.\n    :param output_dir: The output directory where processed *.vcf files should go\n    :type input_dir: str\n    :type temp_dir: str\n    :type output_dir: str\n    \"\"\"\n\n    # Ensure input, temp, and output directory paths are in str format, not pathlib\n    input_dir = str(input_dir)\n    temp_dir = str(temp_dir)\n    output_dir = str(output_dir)\n\n    # Create input, temp, and output directories if they do not exist\n    create_directory_tree(input_dir)\n    create_directory_tree(temp_dir)\n    create_directory_tree(output_dir)\n\n    # Iterate through all *.gz files in input directory and uncompress them to the temporary directory\n    pathlist_gz = pathlib.Path(input_dir).glob(\"**/*.gz\")\n    for path in pathlist_gz:\n        path_str = str(path)\n        file_output_str = path_leaf(path_str)\n        file_output_str = file_output_str[0:len(file_output_str) - 3]  # Truncate *.gz from input filename\n        path_temp_output = str(pathlib.Path(temp_dir, file_output_str))\n        print(\"[Setup][Data] Decompressing file: {}\".format(path_str))\n        print(\"  - Output: {}\".format(path_temp_output))\n\n        # Decompress the .gz file\n        decompress_gzip(path_str, path_temp_output)\n\n    # Iterate through all files in temporary directory and move *.vcf files to output directory\n    pathlist_vcf_temp = pathlib.Path(temp_dir).glob(\"**/*.vcf\")\n    for path in pathlist_vcf_temp:\n        path_temp_str = str(path)\n        filename_str = path_leaf(path_temp_str)  # Strip filename from path\n        path_vcf_str = str(pathlib.Path(output_dir, filename_str))\n\n        shutil.move(path_temp_str, path_vcf_str)\n\n    # Remove temporary directory\n    remove_directory_tree(temp_dir)\n\n    # Copy any *.vcf files already in input directory to the output directory\n    pathlist_vcf_input = pathlib.Path(input_dir).glob(\"**/*.vcf\")\n    for path in pathlist_vcf_input:\n        path_input_str = str(path)\n        filename_str = path_leaf(path_input_str)  # Strip filename from path\n        path_vcf_str = str(pathlib.Path(output_dir, filename_str))\n\n        shutil.copy(path_input_str, path_vcf_str)\n\n\ndef path_head(path):\n    head, tail = os.path.split(path)\n    return head\n\n\ndef path_leaf(path):\n    head, tail = os.path.split(path)\n    return tail or os.path.basename(head)\n\n\ndef read_file_contents(local_filepath):\n    if os.path.isfile(local_filepath):\n        with open(local_filepath) as f:\n            data = f.read()\n            return data\n    else:\n        return None\n\n\ndef setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config):\n    \"\"\"\n    Converts all VCF files in input directory to Zarr format, placed in output directory,\n    based on conversion configuration parameters\n    :param input_vcf_dir: The input directory where VCF files are located\n    :param output_zarr_dir: The output directory to place Zarr-formatted data\n    :param conversion_config: Configuration data for the conversion\n    :type input_vcf_dir: str\n    :type output_zarr_dir: str\n    :type conversion_config: config.VCFtoZarrConfigurationRepresentation\n    \"\"\"\n    # Ensure input and output directory paths are in str format, not pathlib\n    input_vcf_dir = str(input_vcf_dir)\n    output_zarr_dir = str(output_zarr_dir)\n\n    # Create input and output directories if they do not exist\n    create_directory_tree(input_vcf_dir)\n    create_directory_tree(output_zarr_dir)\n\n    # Iterate through all *.vcf files in input directory and convert to Zarr format\n    pathlist_vcf = pathlib.Path(input_vcf_dir).glob(\"**/*.vcf\")\n    for path in pathlist_vcf:\n        path_str = str(path)\n        file_output_str = path_leaf(path_str)\n        file_output_str = file_output_str[0:len(file_output_str) - 4]  # Truncate *.vcf from input filename\n        path_zarr_output = str(pathlib.Path(output_zarr_dir, file_output_str))\n        print(\"[Setup][Data] Converting VCF file to Zarr format: {}\".format(path_str))\n        print(\"  - Output: {}\".format(path_zarr_output))\n\n        # Convert to Zarr format\n        convert_to_zarr(input_vcf_path=path_str,\n                        output_zarr_path=path_zarr_output,\n                        conversion_config=conversion_config)\n\n\ndef convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config):\n    \"\"\" Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.\n    :param input_vcf_path: The input VCF file location\n    :param output_zarr_path: The desired Zarr output location\n    :param conversion_config: Configuration data for the conversion\n    :type input_vcf_path: str\n    :type output_zarr_path: str\n    :type conversion_config: config.VCFtoZarrConfigurationRepresentation\n    \"\"\"\n    if conversion_config is not None:\n        # Ensure var is string, not pathlib.Path\n        output_zarr_path = str(output_zarr_path)\n\n        # Get alt number\n        if conversion_config.alt_number is None:\n            print(\"[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file.\")\n            # Scan VCF file to find max number of alleles in any variant\n            callset = allel.read_vcf(input_vcf_path, fields=['numalt'], log=sys.stdout)\n            numalt = callset['variants/numalt']\n            alt_number = np.max(numalt)\n        else:\n            print(\"[VCF-Zarr] Using alt number provided in configuration.\")\n            # Use the configuration-provided alt number\n            alt_number = conversion_config.alt_number\n        print(\"[VCF-Zarr] Alt number: {}\".format(alt_number))\n\n        # Get chunk length\n        chunk_length = allel.vcf_read.DEFAULT_CHUNK_LENGTH\n        if conversion_config.chunk_length is not None:\n            chunk_length = conversion_config.chunk_length\n        print(\"[VCF-Zarr] Chunk length: {}\".format(chunk_length))\n\n        # Get chunk width\n        chunk_width = allel.vcf_read.DEFAULT_CHUNK_WIDTH\n        if conversion_config.chunk_width is not None:\n            chunk_width = conversion_config.chunk_width\n        print(\"[VCF-Zarr] Chunk width: {}\".format(chunk_width))\n\n        if conversion_config.compressor == \"Blosc\":\n            compressor = Blosc(cname=conversion_config.blosc_compression_algorithm,\n                               clevel=conversion_config.blosc_compression_level,\n                               shuffle=conversion_config.blosc_shuffle_mode)\n        else:\n            raise ValueError(\"Unexpected compressor type specified.\")\n\n        print(\"[VCF-Zarr] Using {} compressor.\".format(conversion_config.compressor))\n\n        print(\"[VCF-Zarr] Performing VCF to Zarr conversion...\")\n        # Perform the VCF to Zarr conversion\n        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True,\n                          log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)\n        print(\"[VCF-Zarr] Done.\")\n",
        "dataset": "plain_remote_code_execution.json"
    }
]